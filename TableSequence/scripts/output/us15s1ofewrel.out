Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_15_seed_1', 'type': 'synthetic', 'model_size': 'large', 'with_train': False, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', data_dir='outputs/wrapper/fewrel/unseen_15_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [01:32<29:11, 92.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [01:52<14:53, 49.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [02:12<10:14, 36.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [02:31<07:55, 29.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [02:51<06:30, 26.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [03:09<05:26, 23.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [03:29<04:49, 22.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [03:51<04:24, 22.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [04:10<03:52, 21.14s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [04:30<03:28, 20.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [04:51<03:06, 20.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [05:09<02:40, 20.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [05:30<02:22, 20.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [05:48<01:56, 19.48s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [06:08<01:38, 19.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [06:27<01:18, 19.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [06:46<00:57, 19.31s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [07:08<00:40, 20.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [07:27<00:19, 19.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [07:45<00:00, 19.39s/it]Generating: 100%|██████████| 20/20 [07:45<00:00, 23.30s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 353, 'raw': 480}
{'target': 600, 'success': 378, 'raw': 512}
{'target': 600, 'success': 397, 'raw': 544}
{'target': 600, 'success': 420, 'raw': 576}
{'target': 600, 'success': 441, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 542, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7319711538461539, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 277, 'raw': 384}
{'target': 600, 'success': 300, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 413, 'raw': 576}
{'target': 600, 'success': 437, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 512, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 559, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 614, 'raw': 832}
{'prompt': 'Relation : genre .', 'success_rate': 0.7379807692307693, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 234, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 310, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 433, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 501, 'raw': 672}
{'target': 600, 'success': 525, 'raw': 704}
{'target': 600, 'success': 550, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 625, 'raw': 832}
{'prompt': 'Relation : head of government .', 'success_rate': 0.7512019230769231, 'errors': {'', "('Hans', 'head of government', '', 'He was succeeded by his brother , Dr . Hans .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 321, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 393, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 467, 'raw': 608}
{'target': 600, 'success': 492, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : military branch .', 'success_rate': 0.76375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', "('British ships', 'military branch', '', 'The Battle of Bataillon was the battle of the Battle of the Bastille , where British ships sank at least 1,400 French ships .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 181, 'raw': 256}
{'target': 600, 'success': 203, 'raw': 288}
{'target': 600, 'success': 226, 'raw': 320}
{'target': 600, 'success': 248, 'raw': 352}
{'target': 600, 'success': 274, 'raw': 384}
{'target': 600, 'success': 295, 'raw': 416}
{'target': 600, 'success': 317, 'raw': 448}
{'target': 600, 'success': 339, 'raw': 480}
{'target': 600, 'success': 361, 'raw': 512}
{'target': 600, 'success': 382, 'raw': 544}
{'target': 600, 'success': 405, 'raw': 576}
{'target': 600, 'success': 428, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 474, 'raw': 672}
{'target': 600, 'success': 496, 'raw': 704}
{'target': 600, 'success': 515, 'raw': 736}
{'target': 600, 'success': 538, 'raw': 768}
{'target': 600, 'success': 562, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 602, 'raw': 864}
{'prompt': 'Relation : winner .', 'success_rate': 0.6967592592592593, 'errors': {'', 'too many values to unpack (expected 2)', "('European Championships', 'winner', '', 'The previous year , he won the European Championships , and finished runner in 5th place in the category of medals .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8260869565217391, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : crosses . Context : Later in the year ( 1141–1231 ) he married Brigadier John B. Stoughton , sister of King James VI , the King of England . Head Entity : Robert Stoughton , Tail Entity : John B . Stoughton .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 473, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 574, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 616, 'raw': 800}
{'prompt': 'Relation : crosses .', 'success_rate': 0.77, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8301630434782609, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8020833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8342391304347826, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.845108695652174, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 403, 'raw': 512}
{'target': 600, 'success': 429, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 617, 'raw': 800}
{'prompt': 'Relation : occupation .', 'success_rate': 0.77125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8410326086956522, 'errors': {'', "('FreeBSD', 'operating system', '', 'The operating system is based on FreeBSD 2.1 .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 327, 'raw': 448}
{'target': 600, 'success': 352, 'raw': 480}
{'target': 600, 'success': 373, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 498, 'raw': 672}
{'target': 600, 'success': 519, 'raw': 704}
{'target': 600, 'success': 544, 'raw': 736}
{'target': 600, 'success': 568, 'raw': 768}
{'target': 600, 'success': 591, 'raw': 800}
{'target': 600, 'success': 617, 'raw': 832}
{'prompt': 'Relation : participant .', 'success_rate': 0.7415865384615384, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 165, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 361, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 410, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 566, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 621, 'raw': 800}
{'prompt': 'Relation : participating team .', 'success_rate': 0.77625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : platform .', 'success_rate': 0.8383152173913043, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : position played on team / speciality . Context : Later in 2008 , he played in the United States national team squad for the 2002 FIFA World Cup and 2010 FIFA World Cup games . Head Entity : Walter , Tail Entity : United States national team .\n']
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 99, 'raw': 160}
{'target': 600, 'success': 119, 'raw': 192}
{'target': 600, 'success': 141, 'raw': 224}
{'target': 600, 'success': 162, 'raw': 256}
{'target': 600, 'success': 183, 'raw': 288}
{'target': 600, 'success': 205, 'raw': 320}
{'target': 600, 'success': 224, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 266, 'raw': 416}
{'target': 600, 'success': 287, 'raw': 448}
{'target': 600, 'success': 312, 'raw': 480}
{'target': 600, 'success': 330, 'raw': 512}
{'target': 600, 'success': 348, 'raw': 544}
{'target': 600, 'success': 367, 'raw': 576}
{'target': 600, 'success': 386, 'raw': 608}
{'target': 600, 'success': 404, 'raw': 640}
{'target': 600, 'success': 421, 'raw': 672}
{'target': 600, 'success': 448, 'raw': 704}
{'target': 600, 'success': 465, 'raw': 736}
{'target': 600, 'success': 482, 'raw': 768}
{'target': 600, 'success': 499, 'raw': 800}
{'target': 600, 'success': 525, 'raw': 832}
{'target': 600, 'success': 553, 'raw': 864}
{'target': 600, 'success': 574, 'raw': 896}
{'target': 600, 'success': 594, 'raw': 928}
{'target': 600, 'success': 611, 'raw': 960}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.6364583333333333, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : publisher . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : season 2 , Tail Entity : the Walking Dead .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8233695652173914, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8288043478260869, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/synthetic/0_ext.jsonl'}}
estimate vocab size: 17454
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17554, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_1/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:43, 43.86s/it]Extractor Estimating: 2it [00:45, 19.25s/it]Extractor Estimating: 3it [00:46, 10.82s/it]Extractor Estimating: 4it [00:47,  6.86s/it]Extractor Estimating: 5it [00:48,  4.67s/it]Extractor Estimating: 6it [00:49,  3.35s/it]Extractor Estimating: 7it [00:49,  2.48s/it]Extractor Estimating: 8it [00:50,  1.94s/it]Extractor Estimating: 9it [00:51,  1.56s/it]Extractor Estimating: 10it [00:52,  1.32s/it]Extractor Estimating: 11it [00:52,  1.13s/it]Extractor Estimating: 12it [00:53,  1.04s/it]Extractor Estimating: 13it [00:54,  1.03it/s]Extractor Estimating: 14it [00:55,  1.12it/s]Extractor Estimating: 15it [00:55,  1.15it/s]Extractor Estimating: 16it [00:56,  1.20it/s]Extractor Estimating: 17it [00:57,  1.21it/s]Extractor Estimating: 18it [00:58,  1.25it/s]Extractor Estimating: 19it [00:58,  1.27it/s]Extractor Estimating: 20it [00:59,  1.35it/s]Extractor Estimating: 21it [01:00,  1.39it/s]Extractor Estimating: 22it [01:01,  1.06it/s]Extractor Estimating: 23it [01:02,  1.14it/s]Extractor Estimating: 24it [01:03,  1.22it/s]Extractor Estimating: 25it [01:03,  1.29it/s]Extractor Estimating: 26it [01:04,  1.24it/s]Extractor Estimating: 27it [01:05,  1.25it/s]Extractor Estimating: 28it [01:06,  1.02it/s]Extractor Estimating: 29it [01:08,  1.19s/it]Extractor Estimating: 30it [01:09,  1.08s/it]Extractor Estimating: 31it [01:10,  1.01it/s]Extractor Estimating: 32it [01:11,  1.04it/s]Extractor Estimating: 33it [01:11,  1.12it/s]Extractor Estimating: 34it [01:12,  1.12it/s]Extractor Estimating: 35it [01:13,  1.16it/s]Extractor Estimating: 36it [01:14,  1.18it/s]Extractor Estimating: 37it [01:15,  1.20it/s]Extractor Estimating: 38it [01:15,  1.24it/s]Extractor Estimating: 39it [01:16,  1.26it/s]Extractor Estimating: 40it [01:17,  1.26it/s]Extractor Estimating: 41it [01:18,  1.27it/s]Extractor Estimating: 42it [01:18,  1.25it/s]Extractor Estimating: 43it [01:19,  1.23it/s]Extractor Estimating: 44it [01:20,  1.26it/s]Extractor Estimating: 45it [01:21,  1.26it/s]Extractor Estimating: 46it [01:22,  1.22it/s]Extractor Estimating: 47it [01:22,  1.24it/s]Extractor Estimating: 48it [01:23,  1.24it/s]Extractor Estimating: 49it [01:24,  1.28it/s]Extractor Estimating: 50it [01:25,  1.27it/s]Extractor Estimating: 51it [01:26,  1.30it/s]Extractor Estimating: 52it [01:26,  1.29it/s]Extractor Estimating: 53it [01:28,  1.05it/s]Extractor Estimating: 54it [01:28,  1.12it/s]Extractor Estimating: 55it [01:29,  1.21it/s]Extractor Estimating: 56it [01:30,  1.30it/s]Extractor Estimating: 57it [01:30,  1.33it/s]Extractor Estimating: 58it [01:31,  1.34it/s]Extractor Estimating: 59it [01:32,  1.31it/s]Extractor Estimating: 60it [01:33,  1.36it/s]Extractor Estimating: 61it [01:33,  1.36it/s]Extractor Estimating: 62it [01:34,  1.38it/s]Extractor Estimating: 63it [01:35,  1.34it/s]Extractor Estimating: 64it [01:36,  1.31it/s]Extractor Estimating: 65it [01:36,  1.32it/s]Extractor Estimating: 66it [01:37,  1.37it/s]Extractor Estimating: 67it [01:38,  1.33it/s]Extractor Estimating: 68it [01:39,  1.32it/s]Extractor Estimating: 69it [01:40,  1.29it/s]Extractor Estimating: 70it [01:40,  1.34it/s]Extractor Estimating: 71it [01:41,  1.34it/s]Extractor Estimating: 72it [01:42,  1.33it/s]Extractor Estimating: 73it [01:43,  1.28it/s]Extractor Estimating: 74it [01:43,  1.29it/s]Extractor Estimating: 75it [01:44,  1.29it/s]Extractor Estimating: 76it [01:45,  1.27it/s]Extractor Estimating: 77it [01:46,  1.26it/s]Extractor Estimating: 78it [01:46,  1.28it/s]Extractor Estimating: 79it [01:47,  1.27it/s]Extractor Estimating: 80it [01:48,  1.25it/s]Extractor Estimating: 81it [01:49,  1.23it/s]Extractor Estimating: 82it [01:50,  1.27it/s]Extractor Estimating: 83it [01:50,  1.27it/s]Extractor Estimating: 84it [01:51,  1.26it/s]Extractor Estimating: 85it [01:52,  1.25it/s]Extractor Estimating: 86it [01:53,  1.28it/s]Extractor Estimating: 87it [01:54,  1.27it/s]Extractor Estimating: 88it [01:54,  1.29it/s]Extractor Estimating: 89it [01:55,  1.30it/s]Extractor Estimating: 90it [01:56,  1.30it/s]Extractor Estimating: 91it [01:57,  1.21it/s]Extractor Estimating: 92it [01:58,  1.25it/s]Extractor Estimating: 93it [01:58,  1.25it/s]Extractor Estimating: 94it [01:59,  1.31it/s]Extractor Estimating: 95it [02:00,  1.22it/s]Extractor Estimating: 96it [02:01,  1.23it/s]Extractor Estimating: 97it [02:02,  1.20it/s]Extractor Estimating: 98it [02:02,  1.24it/s]Extractor Estimating: 99it [02:03,  1.26it/s]Extractor Estimating: 100it [02:04,  1.28it/s]Extractor Estimating: 101it [02:05,  1.28it/s]Extractor Estimating: 102it [02:05,  1.30it/s]Extractor Estimating: 103it [02:06,  1.28it/s]Extractor Estimating: 104it [02:07,  1.30it/s]Extractor Estimating: 105it [02:08,  1.30it/s]Extractor Estimating: 106it [02:09,  1.34it/s]Extractor Estimating: 107it [02:09,  1.36it/s]Extractor Estimating: 108it [02:10,  1.36it/s]Extractor Estimating: 109it [02:11,  1.41it/s]Extractor Estimating: 110it [02:11,  1.34it/s]Extractor Estimating: 111it [02:12,  1.36it/s]Extractor Estimating: 112it [02:13,  1.35it/s]Extractor Estimating: 113it [02:14,  1.37it/s]Extractor Estimating: 114it [02:14,  1.32it/s]Extractor Estimating: 115it [02:15,  1.35it/s]Extractor Estimating: 116it [02:16,  1.36it/s]Extractor Estimating: 117it [02:17,  1.33it/s]Extractor Estimating: 118it [02:17,  1.30it/s]Extractor Estimating: 119it [02:18,  1.31it/s]Extractor Estimating: 120it [02:19,  1.35it/s]Extractor Estimating: 121it [02:20,  1.30it/s]Extractor Estimating: 122it [02:20,  1.30it/s]Extractor Estimating: 123it [02:21,  1.30it/s]Extractor Estimating: 124it [02:22,  1.33it/s]Extractor Estimating: 125it [02:23,  1.33it/s]Extractor Estimating: 126it [02:23,  1.33it/s]Extractor Estimating: 127it [02:24,  1.31it/s]Extractor Estimating: 128it [02:25,  1.31it/s]Extractor Estimating: 129it [02:26,  1.25it/s]Extractor Estimating: 130it [02:28,  1.07s/it]Extractor Estimating: 131it [02:28,  1.02it/s]Extractor Estimating: 132it [02:29,  1.05it/s]Extractor Estimating: 133it [02:30,  1.07it/s]Extractor Estimating: 134it [02:31,  1.12it/s]Extractor Estimating: 135it [02:32,  1.15it/s]Extractor Estimating: 136it [02:33,  1.18it/s]Extractor Estimating: 137it [02:33,  1.20it/s]Extractor Estimating: 138it [02:34,  1.24it/s]Extractor Estimating: 139it [02:35,  1.25it/s]Extractor Estimating: 140it [02:36,  1.21it/s]Extractor Estimating: 141it [02:37,  1.24it/s]Extractor Estimating: 142it [02:37,  1.24it/s]Extractor Estimating: 143it [02:38,  1.21it/s]Extractor Estimating: 144it [02:39,  1.22it/s]Extractor Estimating: 145it [02:40,  1.27it/s]Extractor Estimating: 146it [02:41,  1.26it/s]Extractor Estimating: 147it [02:41,  1.21it/s]Extractor Estimating: 148it [02:42,  1.20it/s]Extractor Estimating: 149it [02:43,  1.23it/s]Extractor Estimating: 150it [02:44,  1.23it/s]Extractor Estimating: 151it [02:45,  1.27it/s]Extractor Estimating: 152it [02:45,  1.26it/s]Extractor Estimating: 153it [02:46,  1.30it/s]Extractor Estimating: 154it [02:47,  1.27it/s]Extractor Estimating: 155it [02:48,  1.32it/s]Extractor Estimating: 156it [02:48,  1.30it/s]Extractor Estimating: 157it [02:49,  1.28it/s]Extractor Estimating: 158it [02:50,  1.34it/s]Extractor Estimating: 159it [02:51,  1.30it/s]Extractor Estimating: 160it [02:52,  1.27it/s]Extractor Estimating: 161it [02:52,  1.31it/s]Extractor Estimating: 162it [02:53,  1.38it/s]Extractor Estimating: 163it [02:54,  1.42it/s]Extractor Estimating: 164it [02:54,  1.38it/s]Extractor Estimating: 165it [02:55,  1.34it/s]Extractor Estimating: 166it [02:56,  1.34it/s]Extractor Estimating: 167it [02:57,  1.35it/s]Extractor Estimating: 168it [02:57,  1.32it/s]Extractor Estimating: 169it [02:58,  1.38it/s]Extractor Estimating: 170it [02:59,  1.31it/s]Extractor Estimating: 171it [03:00,  1.30it/s]Extractor Estimating: 172it [03:00,  1.30it/s]Extractor Estimating: 173it [03:01,  1.32it/s]Extractor Estimating: 174it [03:02,  1.32it/s]Extractor Estimating: 175it [03:03,  1.34it/s]Extractor Estimating: 176it [03:04,  1.23it/s]Extractor Estimating: 177it [03:05,  1.20it/s]Extractor Estimating: 178it [03:05,  1.18it/s]Extractor Estimating: 179it [03:06,  1.22it/s]Extractor Estimating: 180it [03:07,  1.27it/s]Extractor Estimating: 181it [03:08,  1.29it/s]Extractor Estimating: 182it [03:08,  1.31it/s]Extractor Estimating: 183it [03:09,  1.27it/s]Extractor Estimating: 184it [03:10,  1.24it/s]Extractor Estimating: 185it [03:11,  1.19it/s]Extractor Estimating: 186it [03:12,  1.18it/s]Extractor Estimating: 187it [03:13,  1.20it/s]Extractor Estimating: 188it [03:13,  1.26it/s]Extractor Estimating: 189it [03:14,  1.20it/s]Extractor Estimating: 190it [03:15,  1.22it/s]Extractor Estimating: 191it [03:16,  1.27it/s]Extractor Estimating: 192it [03:17,  1.23it/s]Extractor Estimating: 193it [03:18,  1.15it/s]Extractor Estimating: 194it [03:18,  1.22it/s]Extractor Estimating: 195it [03:19,  1.14it/s]Extractor Estimating: 196it [03:20,  1.13it/s]Extractor Estimating: 197it [03:21,  1.16it/s]Extractor Estimating: 198it [03:22,  1.20it/s]Extractor Estimating: 199it [03:23,  1.23it/s]Extractor Estimating: 200it [03:23,  1.27it/s]Extractor Estimating: 201it [03:24,  1.26it/s]Extractor Estimating: 202it [03:25,  1.25it/s]Extractor Estimating: 203it [03:26,  1.21it/s]Extractor Estimating: 204it [03:27,  1.25it/s]Extractor Estimating: 205it [03:27,  1.22it/s]Extractor Estimating: 206it [03:28,  1.25it/s]Extractor Estimating: 207it [03:29,  1.28it/s]Extractor Estimating: 208it [03:30,  1.30it/s]Extractor Estimating: 209it [03:31,  1.25it/s]Extractor Estimating: 210it [03:31,  1.21it/s]Extractor Estimating: 211it [03:32,  1.26it/s]Extractor Estimating: 212it [03:33,  1.27it/s]Extractor Estimating: 213it [03:34,  1.22it/s]Extractor Estimating: 214it [03:35,  1.19it/s]Extractor Estimating: 215it [03:35,  1.22it/s]Extractor Estimating: 216it [03:36,  1.21it/s]Extractor Estimating: 217it [03:37,  1.21it/s]Extractor Estimating: 218it [03:38,  1.21it/s]Extractor Estimating: 219it [03:39,  1.18it/s]Extractor Estimating: 220it [03:40,  1.22it/s]Extractor Estimating: 221it [03:41,  1.19it/s]Extractor Estimating: 222it [03:41,  1.19it/s]Extractor Estimating: 223it [03:42,  1.20it/s]Extractor Estimating: 224it [03:43,  1.23it/s]Extractor Estimating: 225it [03:44,  1.22it/s]Extractor Estimating: 226it [03:45,  1.21it/s]Extractor Estimating: 227it [03:45,  1.20it/s]Extractor Estimating: 228it [03:46,  1.22it/s]Extractor Estimating: 229it [03:47,  1.22it/s]Extractor Estimating: 230it [03:48,  1.20it/s]Extractor Estimating: 231it [03:49,  1.21it/s]Extractor Estimating: 232it [03:50,  1.23it/s]Extractor Estimating: 233it [03:50,  1.27it/s]Extractor Estimating: 234it [03:52,  1.22s/it]Extractor Estimating: 235it [03:53,  1.11s/it]Extractor Estimating: 236it [03:54,  1.02s/it]Extractor Estimating: 237it [03:55,  1.07it/s]Extractor Estimating: 238it [03:56,  1.12it/s]Extractor Estimating: 239it [03:56,  1.15it/s]Extractor Estimating: 240it [03:57,  1.14it/s]Extractor Estimating: 241it [03:58,  1.20it/s]Extractor Estimating: 242it [03:59,  1.25it/s]Extractor Estimating: 243it [04:00,  1.27it/s]Extractor Estimating: 244it [04:00,  1.27it/s]Extractor Estimating: 245it [04:01,  1.27it/s]Extractor Estimating: 246it [04:02,  1.26it/s]Extractor Estimating: 247it [04:03,  1.27it/s]Extractor Estimating: 248it [04:04,  1.27it/s]Extractor Estimating: 249it [04:04,  1.26it/s]Extractor Estimating: 250it [04:05,  1.25it/s]Extractor Estimating: 251it [04:06,  1.16it/s]Extractor Estimating: 252it [04:07,  1.22it/s]Extractor Estimating: 253it [04:08,  1.22it/s]Extractor Estimating: 254it [04:09,  1.22it/s]Extractor Estimating: 255it [04:09,  1.22it/s]Extractor Estimating: 256it [04:10,  1.21it/s]Extractor Estimating: 257it [04:11,  1.20it/s]Extractor Estimating: 258it [04:12,  1.20it/s]Extractor Estimating: 259it [04:13,  1.19it/s]Extractor Estimating: 260it [04:14,  1.18it/s]Extractor Estimating: 261it [04:14,  1.18it/s]Extractor Estimating: 262it [04:15,  1.20it/s]Extractor Estimating: 263it [04:16,  1.16it/s]Extractor Estimating: 264it [04:17,  1.19it/s]Extractor Estimating: 265it [04:18,  1.21it/s]Extractor Estimating: 266it [04:19,  1.24it/s]Extractor Estimating: 267it [04:19,  1.22it/s]Extractor Estimating: 268it [04:20,  1.22it/s]Extractor Estimating: 269it [04:21,  1.24it/s]Extractor Estimating: 270it [04:22,  1.20it/s]Extractor Estimating: 271it [04:23,  1.19it/s]Extractor Estimating: 272it [04:24,  1.20it/s]Extractor Estimating: 273it [04:24,  1.25it/s]Extractor Estimating: 274it [04:25,  1.23it/s]Extractor Estimating: 275it [04:26,  1.20it/s]Extractor Estimating: 276it [04:27,  1.25it/s]Extractor Estimating: 277it [04:27,  1.28it/s]Extractor Estimating: 278it [04:28,  1.29it/s]Extractor Estimating: 279it [04:29,  1.26it/s]Extractor Estimating: 280it [04:30,  1.26it/s]Extractor Estimating: 281it [04:31,  1.29it/s]Extractor Estimating: 282it [04:31,  1.30it/s]Extractor Estimating: 283it [04:32,  1.31it/s]Extractor Estimating: 284it [04:33,  1.35it/s]Extractor Estimating: 285it [04:34,  1.28it/s]Extractor Estimating: 286it [04:34,  1.30it/s]Extractor Estimating: 287it [04:35,  1.26it/s]Extractor Estimating: 288it [04:36,  1.28it/s]Extractor Estimating: 289it [04:37,  1.29it/s]Extractor Estimating: 290it [04:37,  1.32it/s]Extractor Estimating: 291it [04:38,  1.35it/s]Extractor Estimating: 292it [04:39,  1.35it/s]Extractor Estimating: 293it [04:40,  1.33it/s]Extractor Estimating: 294it [04:40,  1.31it/s]Extractor Estimating: 295it [04:41,  1.30it/s]Extractor Estimating: 296it [04:42,  1.34it/s]Extractor Estimating: 297it [04:43,  1.31it/s]Extractor Estimating: 298it [04:43,  1.34it/s]Extractor Estimating: 299it [04:44,  1.29it/s]Extractor Estimating: 300it [04:45,  1.29it/s]Extractor Estimating: 301it [04:46,  1.32it/s]Extractor Estimating: 302it [04:47,  1.26it/s]Extractor Estimating: 303it [04:48,  1.23it/s]Extractor Estimating: 304it [04:48,  1.26it/s]Extractor Estimating: 305it [04:49,  1.28it/s]Extractor Estimating: 306it [04:50,  1.26it/s]Extractor Estimating: 307it [04:51,  1.24it/s]Extractor Estimating: 308it [04:52,  1.23it/s]Extractor Estimating: 309it [04:52,  1.27it/s]Extractor Estimating: 310it [04:53,  1.28it/s]Extractor Estimating: 311it [04:54,  1.31it/s]Extractor Estimating: 312it [04:55,  1.31it/s]Extractor Estimating: 313it [04:55,  1.32it/s]Extractor Estimating: 314it [04:56,  1.30it/s]Extractor Estimating: 315it [04:57,  1.29it/s]Extractor Estimating: 316it [04:58,  1.32it/s]Extractor Estimating: 317it [04:58,  1.32it/s]Extractor Estimating: 318it [04:59,  1.29it/s]Extractor Estimating: 319it [05:00,  1.29it/s]Extractor Estimating: 320it [05:01,  1.34it/s]Extractor Estimating: 321it [05:01,  1.35it/s]Extractor Estimating: 322it [05:02,  1.35it/s]Extractor Estimating: 323it [05:03,  1.36it/s]Extractor Estimating: 324it [05:04,  1.32it/s]Extractor Estimating: 325it [05:04,  1.31it/s]Extractor Estimating: 326it [05:05,  1.20it/s]Extractor Estimating: 327it [05:06,  1.29it/s]Extractor Estimating: 328it [05:07,  1.35it/s]Extractor Estimating: 329it [05:07,  1.30it/s]Extractor Estimating: 330it [05:08,  1.30it/s]Extractor Estimating: 331it [05:09,  1.27it/s]Extractor Estimating: 332it [05:10,  1.29it/s]Extractor Estimating: 333it [05:11,  1.34it/s]Extractor Estimating: 334it [05:11,  1.34it/s]Extractor Estimating: 335it [05:12,  1.37it/s]Extractor Estimating: 336it [05:13,  1.40it/s]Extractor Estimating: 337it [05:13,  1.41it/s]Extractor Estimating: 338it [05:14,  1.40it/s]Extractor Estimating: 339it [05:15,  1.32it/s]Extractor Estimating: 340it [05:16,  1.34it/s]Extractor Estimating: 341it [05:16,  1.35it/s]Extractor Estimating: 342it [05:17,  1.42it/s]Extractor Estimating: 343it [05:18,  1.38it/s]Extractor Estimating: 344it [05:19,  1.32it/s]Extractor Estimating: 345it [05:19,  1.34it/s]Extractor Estimating: 346it [05:20,  1.31it/s]Extractor Estimating: 347it [05:21,  1.34it/s]Extractor Estimating: 348it [05:22,  1.35it/s]Extractor Estimating: 349it [05:22,  1.31it/s]Extractor Estimating: 350it [05:23,  1.35it/s]Extractor Estimating: 351it [05:24,  1.32it/s]Extractor Estimating: 352it [05:25,  1.31it/s]Extractor Estimating: 353it [05:25,  1.31it/s]Extractor Estimating: 354it [05:26,  1.31it/s]Extractor Estimating: 355it [05:27,  1.30it/s]Extractor Estimating: 356it [05:28,  1.32it/s]Extractor Estimating: 357it [05:28,  1.35it/s]Extractor Estimating: 358it [05:29,  1.32it/s]Extractor Estimating: 359it [05:30,  1.30it/s]Extractor Estimating: 360it [05:31,  1.31it/s]Extractor Estimating: 361it [05:31,  1.33it/s]Extractor Estimating: 362it [05:32,  1.39it/s]Extractor Estimating: 363it [05:33,  1.41it/s]Extractor Estimating: 364it [05:33,  1.43it/s]Extractor Estimating: 365it [05:34,  1.41it/s]Extractor Estimating: 366it [05:35,  1.36it/s]Extractor Estimating: 367it [05:36,  1.36it/s]Extractor Estimating: 368it [05:37,  1.31it/s]Extractor Estimating: 369it [05:37,  1.31it/s]Extractor Estimating: 370it [05:38,  1.34it/s]Extractor Estimating: 371it [05:39,  1.37it/s]Extractor Estimating: 372it [05:39,  1.36it/s]Extractor Estimating: 373it [05:40,  1.38it/s]Extractor Estimating: 374it [05:41,  1.37it/s]Extractor Estimating: 375it [05:42,  1.37it/s]Extractor Estimating: 376it [05:42,  1.38it/s]Extractor Estimating: 377it [05:43,  1.38it/s]Extractor Estimating: 378it [05:44,  1.41it/s]Extractor Estimating: 379it [05:44,  1.39it/s]Extractor Estimating: 380it [05:45,  1.39it/s]Extractor Estimating: 381it [05:46,  1.37it/s]Extractor Estimating: 382it [05:47,  1.38it/s]Extractor Estimating: 383it [05:47,  1.37it/s]Extractor Estimating: 384it [05:48,  1.33it/s]Extractor Estimating: 385it [05:49,  1.34it/s]Extractor Estimating: 386it [05:50,  1.33it/s]Extractor Estimating: 387it [05:50,  1.37it/s]Extractor Estimating: 388it [05:51,  1.39it/s]Extractor Estimating: 389it [05:52,  1.41it/s]Extractor Estimating: 390it [05:53,  1.35it/s]Extractor Estimating: 391it [05:53,  1.37it/s]Extractor Estimating: 392it [05:54,  1.37it/s]Extractor Estimating: 393it [05:55,  1.36it/s]Extractor Estimating: 394it [05:55,  1.38it/s]Extractor Estimating: 395it [05:56,  1.38it/s]Extractor Estimating: 396it [05:57,  1.37it/s]Extractor Estimating: 397it [05:58,  1.35it/s]Extractor Estimating: 398it [05:58,  1.33it/s]Extractor Estimating: 399it [05:59,  1.34it/s]Extractor Estimating: 400it [06:00,  1.32it/s]Extractor Estimating: 401it [06:01,  1.32it/s]Extractor Estimating: 402it [06:02,  1.26it/s]Extractor Estimating: 403it [06:02,  1.28it/s]Extractor Estimating: 404it [06:03,  1.29it/s]Extractor Estimating: 405it [06:04,  1.30it/s]Extractor Estimating: 406it [06:05,  1.28it/s]Extractor Estimating: 407it [06:05,  1.28it/s]Extractor Estimating: 408it [06:06,  1.28it/s]Extractor Estimating: 409it [06:07,  1.34it/s]Extractor Estimating: 410it [06:08,  1.30it/s]Extractor Estimating: 411it [06:08,  1.32it/s]Extractor Estimating: 412it [06:09,  1.29it/s]Extractor Estimating: 413it [06:10,  1.38it/s]Extractor Estimating: 414it [06:11,  1.38it/s]Extractor Estimating: 415it [06:12,  1.29it/s]Extractor Estimating: 416it [06:12,  1.29it/s]Extractor Estimating: 417it [06:13,  1.30it/s]Extractor Estimating: 418it [06:14,  1.35it/s]Extractor Estimating: 419it [06:15,  1.22it/s]Extractor Estimating: 420it [06:15,  1.24it/s]Extractor Estimating: 421it [06:16,  1.30it/s]Extractor Estimating: 422it [06:17,  1.31it/s]Extractor Estimating: 423it [06:18,  1.27it/s]Extractor Estimating: 424it [06:19,  1.29it/s]Extractor Estimating: 425it [06:19,  1.30it/s]Extractor Estimating: 426it [06:20,  1.32it/s]Extractor Estimating: 427it [06:21,  1.27it/s]Extractor Estimating: 428it [06:22,  1.29it/s]Extractor Estimating: 429it [06:22,  1.31it/s]Extractor Estimating: 430it [06:23,  1.29it/s]Extractor Estimating: 431it [06:24,  1.31it/s]Extractor Estimating: 432it [06:25,  1.37it/s]Extractor Estimating: 433it [06:25,  1.34it/s]Extractor Estimating: 434it [06:26,  1.32it/s]Extractor Estimating: 435it [06:27,  1.34it/s]Extractor Estimating: 436it [06:28,  1.33it/s]Extractor Estimating: 437it [06:28,  1.32it/s]Extractor Estimating: 438it [06:29,  1.32it/s]Extractor Estimating: 439it [06:30,  1.33it/s]Extractor Estimating: 440it [06:31,  1.34it/s]Extractor Estimating: 441it [06:31,  1.33it/s]Extractor Estimating: 442it [06:32,  1.32it/s]Extractor Estimating: 443it [06:33,  1.33it/s]Extractor Estimating: 444it [06:34,  1.35it/s]Extractor Estimating: 445it [06:34,  1.38it/s]Extractor Estimating: 446it [06:35,  1.37it/s]Extractor Estimating: 447it [06:36,  1.38it/s]Extractor Estimating: 448it [06:36,  1.36it/s]Extractor Estimating: 449it [06:37,  1.42it/s]Extractor Estimating: 450it [06:38,  1.36it/s]Extractor Estimating: 451it [06:39,  1.33it/s]Extractor Estimating: 452it [06:40,  1.27it/s]Extractor Estimating: 453it [06:41,  1.16it/s]Extractor Estimating: 454it [06:41,  1.19it/s]Extractor Estimating: 455it [06:42,  1.20it/s]Extractor Estimating: 456it [06:43,  1.26it/s]Extractor Estimating: 457it [06:44,  1.25it/s]Extractor Estimating: 458it [06:45,  1.25it/s]Extractor Estimating: 459it [06:45,  1.22it/s]Extractor Estimating: 460it [06:46,  1.21it/s]Extractor Estimating: 461it [06:47,  1.24it/s]Extractor Estimating: 462it [06:48,  1.22it/s]Extractor Estimating: 463it [06:49,  1.22it/s]Extractor Estimating: 464it [06:50,  1.22it/s]Extractor Estimating: 465it [06:50,  1.20it/s]Extractor Estimating: 466it [06:51,  1.20it/s]Extractor Estimating: 467it [06:52,  1.18it/s]Extractor Estimating: 468it [06:53,  1.17it/s]Extractor Estimating: 469it [06:54,  1.18it/s]Extractor Estimating: 470it [06:55,  1.23it/s]Extractor Estimating: 471it [06:55,  1.21it/s]Extractor Estimating: 472it [06:56,  1.17it/s]Extractor Estimating: 473it [06:57,  1.18it/s]Extractor Estimating: 474it [06:58,  1.20it/s]Extractor Estimating: 475it [06:59,  1.23it/s]Extractor Estimating: 476it [06:59,  1.24it/s]Extractor Estimating: 477it [07:00,  1.27it/s]Extractor Estimating: 478it [07:01,  1.27it/s]Extractor Estimating: 479it [07:02,  1.33it/s]Extractor Estimating: 480it [07:02,  1.34it/s]Extractor Estimating: 481it [07:03,  1.34it/s]Extractor Estimating: 482it [07:04,  1.34it/s]Extractor Estimating: 483it [07:05,  1.31it/s]Extractor Estimating: 484it [07:05,  1.31it/s]Extractor Estimating: 485it [07:06,  1.31it/s]Extractor Estimating: 486it [07:07,  1.26it/s]Extractor Estimating: 487it [07:08,  1.30it/s]Extractor Estimating: 488it [07:09,  1.34it/s]Extractor Estimating: 489it [07:09,  1.35it/s]Extractor Estimating: 490it [07:10,  1.32it/s]Extractor Estimating: 491it [07:11,  1.37it/s]Extractor Estimating: 492it [07:12,  1.32it/s]Extractor Estimating: 493it [07:12,  1.29it/s]Extractor Estimating: 494it [07:13,  1.26it/s]Extractor Estimating: 495it [07:14,  1.32it/s]Extractor Estimating: 496it [07:15,  1.31it/s]Extractor Estimating: 497it [07:15,  1.29it/s]Extractor Estimating: 498it [07:16,  1.31it/s]Extractor Estimating: 499it [07:17,  1.22it/s]Extractor Estimating: 500it [07:18,  1.33it/s]Extractor Estimating: 500it [07:18,  1.14it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 2022 mean pseudo reward: 0.9479595992847282
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 15241
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15341, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_1/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15341, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 15, avg_time 1.588, loss:440.2649
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 30, avg_time 1.193, loss:358.9527
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 45, avg_time 1.195, loss:319.1800
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 60, avg_time 1.201, loss:282.8512
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 75, avg_time 1.200, loss:261.6993
>> valid entity prec:0.5880, rec:0.5985, f1:0.5932
>> valid relation prec:0.2319, rec:0.1496, f1:0.1818
>> valid relation with NER prec:0.2319, rec:0.1496, f1:0.1818
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 5, avg_time 2.683, loss:237.4659
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 20, avg_time 1.204, loss:230.6357
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 35, avg_time 1.186, loss:210.9968
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 50, avg_time 1.201, loss:215.8630
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 65, avg_time 1.209, loss:196.0130
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5654, rec:0.5260, f1:0.5450
>> valid relation prec:0.2212, rec:0.1366, f1:0.1689
>> valid relation with NER prec:0.2212, rec:0.1366, f1:0.1689
g_step 1100, step 80, avg_time 2.673, loss:208.7783
g_step 1200, step 10, avg_time 1.199, loss:181.7298
g_step 1300, step 25, avg_time 1.202, loss:168.1885
g_step 1400, step 40, avg_time 1.198, loss:156.0541
g_step 1500, step 55, avg_time 1.194, loss:147.6528
>> valid entity prec:0.5623, rec:0.5804, f1:0.5712
>> valid relation prec:0.2128, rec:0.1700, f1:0.1890
>> valid relation with NER prec:0.2128, rec:0.1700, f1:0.1890
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 70, avg_time 2.679, loss:135.9307
g_step 1700, step 85, avg_time 1.204, loss:134.6346
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 00:37:25 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 00:37:25 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_00-37-24_ctolab14.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 00:37:26 - WARNING - datasets.builder -   Using custom data configuration default-cdba04f0de555eab
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-cdba04f0de555eab/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  1.51 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 00:37:29,269 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:37:29,270 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:37:29,271 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:37:29,272 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:37:29,386 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:37:29,439 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:37:29,439 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:37:29,440 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:37:29,440 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:37:29,440 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:37:29,440 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 00:37:29,827 >> loading weights file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:37:32,891 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 00:37:32,905 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_15_seed_1/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-cdba04f0de555eab/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/29/2023 00:37:32 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x145d0c16fa70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:01,  1.41ba/s] 67%|██████▋   | 2/3 [00:00<00:00,  2.44ba/s]100%|██████████| 3/3 [00:00<00:00,  3.26ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.60ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.20ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.40ba/s]100%|██████████| 4/4 [00:00<00:00,  5.60ba/s]100%|██████████| 4/4 [00:00<00:00,  4.96ba/s]
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:00,  5.38ba/s]100%|██████████| 3/3 [00:00<00:00, 10.69ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  5.71ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.89ba/s]100%|██████████| 4/4 [00:00<00:00, 10.00ba/s]
[INFO|trainer.py:414] 2023-08-29 00:37:36,383 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 00:37:36,493 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 00:37:36,493 >>   Num examples = 2022
[INFO|trainer.py:1149] 2023-08-29 00:37:36,493 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 00:37:36,493 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 00:37:36,493 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 00:37:36,493 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 00:37:36,493 >>   Total optimization steps = 160
  0%|          | 0/160 [00:00<?, ?it/s]  1%|          | 1/160 [00:01<02:54,  1.10s/it]  1%|▏         | 2/160 [00:01<01:38,  1.60it/s]  2%|▏         | 3/160 [00:01<01:14,  2.10it/s]  2%|▎         | 4/160 [00:01<01:03,  2.47it/s]  3%|▎         | 5/160 [00:02<00:56,  2.73it/s]  4%|▍         | 6/160 [00:02<00:52,  2.92it/s]  4%|▍         | 7/160 [00:02<00:50,  3.05it/s]  5%|▌         | 8/160 [00:03<00:48,  3.14it/s]  6%|▌         | 9/160 [00:03<00:49,  3.08it/s]  6%|▋         | 10/160 [00:03<00:47,  3.17it/s]  7%|▋         | 11/160 [00:04<00:45,  3.25it/s]  8%|▊         | 12/160 [00:04<00:44,  3.31it/s]  8%|▊         | 13/160 [00:04<00:43,  3.35it/s]  9%|▉         | 14/160 [00:04<00:43,  3.38it/s]  9%|▉         | 15/160 [00:05<00:42,  3.40it/s] 10%|█         | 16/160 [00:05<00:42,  3.41it/s] 11%|█         | 17/160 [00:05<00:41,  3.42it/s] 11%|█▏        | 18/160 [00:06<00:41,  3.43it/s] 12%|█▏        | 19/160 [00:06<00:41,  3.43it/s] 12%|█▎        | 20/160 [00:06<00:41,  3.35it/s] 13%|█▎        | 21/160 [00:07<00:41,  3.38it/s] 14%|█▍        | 22/160 [00:07<00:40,  3.40it/s] 14%|█▍        | 23/160 [00:07<00:40,  3.42it/s] 15%|█▌        | 24/160 [00:07<00:39,  3.43it/s] 16%|█▌        | 25/160 [00:08<00:39,  3.44it/s] 16%|█▋        | 26/160 [00:08<00:38,  3.44it/s] 17%|█▋        | 27/160 [00:08<00:38,  3.45it/s] 18%|█▊        | 28/160 [00:09<00:38,  3.45it/s] 18%|█▊        | 29/160 [00:09<00:37,  3.45it/s] 19%|█▉        | 30/160 [00:09<00:37,  3.45it/s] 19%|█▉        | 31/160 [00:10<00:40,  3.15it/s] 20%|██        | 32/160 [00:10<00:44,  2.88it/s][INFO|trainer.py:2140] 2023-08-29 00:37:46,928 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:37:46,928 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 00:37:46,928 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.75it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.17it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.37it/s][A
  5%|▌         | 22/435 [00:00<00:08, 46.49it/s][A
  6%|▌         | 27/435 [00:00<00:08, 45.94it/s][A
  7%|▋         | 32/435 [00:00<00:08, 45.60it/s][A
  9%|▊         | 37/435 [00:00<00:08, 45.28it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.66it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.20it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 42.86it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 43.44it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 43.89it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.24it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.46it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 44.58it/s][A
 19%|█▉        | 82/435 [00:01<00:07, 44.48it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.21it/s][A
 21%|██        | 92/435 [00:02<00:07, 43.95it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 43.91it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.14it/s][A
 25%|██▍       | 107/435 [00:02<00:08, 40.25it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 41.52it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 42.51it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 43.18it/s][A
 29%|██▉       | 127/435 [00:02<00:07, 43.72it/s][A
 30%|███       | 132/435 [00:02<00:06, 43.99it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 44.03it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 44.06it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 43.83it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 43.91it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.06it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 44.28it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 44.54it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.62it/s][A
 41%|████      | 177/435 [00:04<00:06, 41.77it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 42.70it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 43.11it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 43.31it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 43.57it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 43.84it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 44.10it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 44.34it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 44.24it/s][A
 51%|█████     | 222/435 [00:05<00:04, 44.36it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.41it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.34it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.25it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 42.26it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 42.97it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 43.45it/s][A
 59%|█████▉    | 257/435 [00:05<00:04, 43.82it/s][A
 60%|██████    | 262/435 [00:05<00:03, 44.11it/s][A
 61%|██████▏   | 267/435 [00:06<00:03, 44.21it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.22it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.23it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.04it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.17it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.24it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.35it/s][A
 69%|██████▉   | 302/435 [00:06<00:03, 42.86it/s][A
 71%|███████   | 307/435 [00:06<00:02, 43.49it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 43.86it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.04it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.03it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.00it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.08it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.22it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 44.14it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 44.31it/s][A
 81%|████████  | 352/435 [00:07<00:01, 44.53it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 44.61it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 44.41it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.28it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 44.19it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 39.43it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 40.85it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 42.02it/s][A
 90%|█████████ | 392/435 [00:08<00:01, 42.85it/s][A
 91%|█████████▏| 397/435 [00:09<00:00, 43.50it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 43.92it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 44.12it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 44.03it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 43.80it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 43.66it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 43.86it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.07it/s][A                                                
                                                 [A 20%|██        | 32/160 [00:20<00:44,  2.88it/s]
100%|██████████| 435/435 [00:09<00:00, 44.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:37:57,046 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-32
[INFO|configuration_utils.py:351] 2023-08-29 00:37:57,184 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-32/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:38:01,791 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-32/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:38:02,159 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-32/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:38:02,345 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-32/special_tokens_map.json
 21%|██        | 33/160 [00:35<16:20,  7.72s/it] 21%|██▏       | 34/160 [00:35<11:33,  5.51s/it] 22%|██▏       | 35/160 [00:35<08:12,  3.94s/it] 22%|██▎       | 36/160 [00:36<05:52,  2.85s/it] 23%|██▎       | 37/160 [00:36<04:15,  2.08s/it] 24%|██▍       | 38/160 [00:36<03:08,  1.54s/it] 24%|██▍       | 39/160 [00:37<02:21,  1.17s/it] 25%|██▌       | 40/160 [00:37<01:48,  1.11it/s] 26%|██▌       | 41/160 [00:37<01:25,  1.39it/s] 26%|██▋       | 42/160 [00:38<01:09,  1.69it/s] 27%|██▋       | 43/160 [00:38<00:58,  2.00it/s] 28%|██▊       | 44/160 [00:38<00:50,  2.29it/s] 28%|██▊       | 45/160 [00:38<00:46,  2.49it/s] 29%|██▉       | 46/160 [00:39<00:41,  2.72it/s] 29%|██▉       | 47/160 [00:39<00:38,  2.90it/s] 30%|███       | 48/160 [00:39<00:36,  3.04it/s] 31%|███       | 49/160 [00:40<00:35,  3.16it/s] 31%|███▏      | 50/160 [00:40<00:33,  3.24it/s] 32%|███▏      | 51/160 [00:40<00:33,  3.30it/s] 32%|███▎      | 52/160 [00:40<00:32,  3.35it/s] 33%|███▎      | 53/160 [00:41<00:31,  3.38it/s] 34%|███▍      | 54/160 [00:41<00:31,  3.40it/s] 34%|███▍      | 55/160 [00:41<00:30,  3.42it/s] 35%|███▌      | 56/160 [00:42<00:31,  3.32it/s] 36%|███▌      | 57/160 [00:42<00:30,  3.36it/s] 36%|███▋      | 58/160 [00:42<00:30,  3.39it/s] 37%|███▋      | 59/160 [00:42<00:29,  3.41it/s] 38%|███▊      | 60/160 [00:43<00:29,  3.42it/s] 38%|███▊      | 61/160 [00:43<00:28,  3.43it/s] 39%|███▉      | 62/160 [00:43<00:28,  3.43it/s] 39%|███▉      | 63/160 [00:44<00:28,  3.44it/s] 40%|████      | 64/160 [00:44<00:25,  3.82it/s][INFO|trainer.py:2140] 2023-08-29 00:38:20,841 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:38:20,841 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 00:38:20,841 >>   Batch size = 8
{'eval_loss': 0.9707925915718079, 'eval_runtime': 9.9291, 'eval_samples_per_second': 350.184, 'eval_steps_per_second': 43.811, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 55.67it/s][A
  3%|▎         | 12/435 [00:00<00:08, 48.61it/s][A
  4%|▍         | 17/435 [00:00<00:08, 46.78it/s][A
  5%|▌         | 22/435 [00:00<00:09, 45.80it/s][A
  6%|▌         | 27/435 [00:00<00:09, 43.47it/s][A
  7%|▋         | 32/435 [00:00<00:09, 43.82it/s][A
  9%|▊         | 37/435 [00:00<00:09, 43.90it/s][A
 10%|▉         | 42/435 [00:00<00:08, 43.85it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.13it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.31it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.52it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.55it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.33it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.37it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 44.38it/s][A
 19%|█▉        | 82/435 [00:01<00:07, 44.40it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.27it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.29it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.38it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.58it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.52it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.42it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 44.36it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 40.33it/s][A
 29%|██▉       | 127/435 [00:02<00:07, 41.76it/s][A
 30%|███       | 132/435 [00:02<00:07, 42.69it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 43.32it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 43.73it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 44.12it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.27it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.26it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 42.76it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 43.02it/s][A
 40%|███▉      | 172/435 [00:03<00:06, 43.60it/s][A
 41%|████      | 177/435 [00:04<00:09, 25.96it/s][A
 42%|████▏     | 182/435 [00:04<00:08, 30.14it/s][A
 43%|████▎     | 187/435 [00:04<00:07, 33.28it/s][A
 44%|████▍     | 192/435 [00:04<00:06, 36.25it/s][A
 45%|████▌     | 197/435 [00:04<00:06, 38.46it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 40.22it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 41.52it/s][A
 49%|████▊     | 212/435 [00:05<00:05, 42.57it/s][A
 50%|████▉     | 217/435 [00:05<00:05, 42.92it/s][A
 51%|█████     | 222/435 [00:05<00:04, 42.96it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 41.93it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 42.74it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 43.26it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 43.73it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.05it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 44.30it/s][A
 59%|█████▉    | 257/435 [00:06<00:04, 44.45it/s][A
 60%|██████    | 262/435 [00:06<00:03, 44.33it/s][A
 61%|██████▏   | 267/435 [00:06<00:03, 44.04it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 43.96it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.07it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.28it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.33it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.53it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.64it/s][A
 69%|██████▉   | 302/435 [00:07<00:02, 44.76it/s][A
 71%|███████   | 307/435 [00:07<00:02, 44.45it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.17it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.13it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.12it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.24it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.43it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.50it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 44.60it/s][A
 80%|███████▉  | 347/435 [00:08<00:01, 44.61it/s][A
 81%|████████  | 352/435 [00:08<00:01, 44.44it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 44.32it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 42.99it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 43.45it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 43.72it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 44.00it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 44.28it/s][A
 89%|████████▉ | 387/435 [00:09<00:01, 44.37it/s][A
 90%|█████████ | 392/435 [00:09<00:00, 44.42it/s][A
 91%|█████████▏| 397/435 [00:09<00:00, 44.31it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 44.04it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 44.16it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 44.18it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 44.29it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.41it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 44.37it/s][A
 99%|█████████▉| 432/435 [00:10<00:00, 44.53it/s][A                                                
                                                 [A 40%|████      | 64/160 [00:54<00:25,  3.82it/s]
100%|██████████| 435/435 [00:10<00:00, 44.53it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:38:31,251 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-64
[INFO|configuration_utils.py:351] 2023-08-29 00:38:31,432 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-64/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:38:34,716 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-64/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:38:34,831 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-64/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:38:34,901 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-64/special_tokens_map.json
 41%|████      | 65/160 [01:05<10:33,  6.67s/it] 41%|████▏     | 66/160 [01:06<07:28,  4.77s/it] 42%|████▏     | 67/160 [01:06<05:18,  3.43s/it] 42%|████▎     | 68/160 [01:06<03:48,  2.49s/it] 43%|████▎     | 69/160 [01:07<02:46,  1.83s/it] 44%|████▍     | 70/160 [01:07<02:03,  1.37s/it] 44%|████▍     | 71/160 [01:07<01:33,  1.05s/it] 45%|████▌     | 72/160 [01:08<01:12,  1.21it/s] 46%|████▌     | 73/160 [01:08<00:57,  1.50it/s] 46%|████▋     | 74/160 [01:08<00:47,  1.80it/s] 47%|████▋     | 75/160 [01:08<00:40,  2.09it/s] 48%|████▊     | 76/160 [01:09<00:36,  2.32it/s] 48%|████▊     | 77/160 [01:09<00:32,  2.55it/s] 49%|████▉     | 78/160 [01:09<00:29,  2.75it/s] 49%|████▉     | 79/160 [01:10<00:27,  2.91it/s] 50%|█████     | 80/160 [01:10<00:26,  3.03it/s] 51%|█████     | 81/160 [01:10<00:25,  3.12it/s] 51%|█████▏    | 82/160 [01:11<00:24,  3.19it/s] 52%|█████▏    | 83/160 [01:11<00:23,  3.24it/s] 52%|█████▎    | 84/160 [01:11<00:23,  3.28it/s] 53%|█████▎    | 85/160 [01:11<00:22,  3.30it/s] 54%|█████▍    | 86/160 [01:12<00:24,  3.07it/s] 54%|█████▍    | 87/160 [01:12<00:23,  3.15it/s] 55%|█████▌    | 88/160 [01:12<00:22,  3.21it/s] 56%|█████▌    | 89/160 [01:13<00:21,  3.26it/s] 56%|█████▋    | 90/160 [01:13<00:21,  3.29it/s] 57%|█████▋    | 91/160 [01:13<00:20,  3.31it/s] 57%|█████▊    | 92/160 [01:14<00:20,  3.33it/s] 58%|█████▊    | 93/160 [01:14<00:20,  3.34it/s] 59%|█████▉    | 94/160 [01:14<00:19,  3.34it/s] 59%|█████▉    | 95/160 [01:15<00:19,  3.35it/s] 60%|██████    | 96/160 [01:15<00:18,  3.51it/s][INFO|trainer.py:2140] 2023-08-29 00:38:51,791 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:38:51,791 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 00:38:51,791 >>   Batch size = 8
{'eval_loss': 0.9813992381095886, 'eval_runtime': 10.1142, 'eval_samples_per_second': 343.774, 'eval_steps_per_second': 43.009, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 55.86it/s][A
  3%|▎         | 12/435 [00:00<00:08, 48.50it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.26it/s][A
  5%|▌         | 22/435 [00:00<00:08, 46.41it/s][A
  6%|▌         | 27/435 [00:00<00:08, 45.88it/s][A
  7%|▋         | 32/435 [00:00<00:08, 45.39it/s][A
  9%|▊         | 37/435 [00:00<00:08, 44.66it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.20it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.20it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.41it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.60it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.68it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.77it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.81it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 44.58it/s][A
 19%|█▉        | 82/435 [00:01<00:07, 44.17it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.03it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.13it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.22it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.35it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.52it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.65it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 44.68it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 44.60it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 44.37it/s][A
 30%|███       | 132/435 [00:02<00:07, 42.20it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 42.88it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 43.40it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 43.80it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.10it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.38it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 44.48it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 44.36it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.06it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.02it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.18it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.32it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 44.43it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 44.67it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 44.65it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 44.75it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 44.53it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 44.20it/s][A
 51%|█████     | 222/435 [00:04<00:04, 44.18it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.25it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.14it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.49it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.46it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.63it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 44.62it/s][A
 59%|█████▉    | 257/435 [00:05<00:04, 44.49it/s][A
 60%|██████    | 262/435 [00:05<00:03, 44.34it/s][A
 61%|██████▏   | 267/435 [00:05<00:03, 44.27it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.26it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.43it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.44it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.61it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.63it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.54it/s][A
 69%|██████▉   | 302/435 [00:06<00:02, 44.42it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.33it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.22it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.26it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.27it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.38it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.47it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.53it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 44.56it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 44.39it/s][A
 81%|████████  | 352/435 [00:07<00:01, 44.31it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 44.26it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 40.74it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 41.94it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 42.75it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 43.39it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 43.83it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 44.06it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 44.21it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 44.02it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 43.81it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 43.85it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 44.01it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 44.34it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.48it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 44.58it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.63it/s][A                                                
                                                 [A 60%|██████    | 96/160 [01:25<00:18,  3.51it/s]
100%|██████████| 435/435 [00:09<00:00, 44.63it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:39:01,819 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-96
[INFO|configuration_utils.py:351] 2023-08-29 00:39:02,192 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-96/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:39:06,101 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-96/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:39:06,444 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-96/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:39:06,576 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-96/special_tokens_map.json
 61%|██████    | 97/160 [01:39<07:48,  7.44s/it] 61%|██████▏   | 98/160 [01:39<05:28,  5.30s/it] 62%|██████▏   | 99/160 [01:40<03:51,  3.80s/it] 62%|██████▎   | 100/160 [01:40<02:45,  2.75s/it] 63%|██████▎   | 101/160 [01:40<01:58,  2.01s/it] 64%|██████▍   | 102/160 [01:40<01:26,  1.50s/it] 64%|██████▍   | 103/160 [01:41<01:04,  1.14s/it] 65%|██████▌   | 104/160 [01:41<00:49,  1.13it/s] 66%|██████▌   | 105/160 [01:41<00:39,  1.41it/s] 66%|██████▋   | 106/160 [01:42<00:31,  1.71it/s] 67%|██████▋   | 107/160 [01:42<00:26,  2.00it/s] 68%|██████▊   | 108/160 [01:42<00:22,  2.28it/s] 68%|██████▊   | 109/160 [01:43<00:20,  2.47it/s] 69%|██████▉   | 110/160 [01:43<00:18,  2.68it/s] 69%|██████▉   | 111/160 [01:43<00:17,  2.85it/s] 70%|███████   | 112/160 [01:43<00:16,  2.99it/s] 71%|███████   | 113/160 [01:44<00:15,  3.10it/s] 71%|███████▏  | 114/160 [01:44<00:14,  3.17it/s] 72%|███████▏  | 115/160 [01:44<00:13,  3.23it/s] 72%|███████▎  | 116/160 [01:45<00:13,  3.27it/s] 73%|███████▎  | 117/160 [01:45<00:13,  3.30it/s] 74%|███████▍  | 118/160 [01:45<00:12,  3.32it/s] 74%|███████▍  | 119/160 [01:46<00:12,  3.24it/s] 75%|███████▌  | 120/160 [01:46<00:12,  3.28it/s] 76%|███████▌  | 121/160 [01:46<00:11,  3.31it/s] 76%|███████▋  | 122/160 [01:46<00:11,  3.33it/s] 77%|███████▋  | 123/160 [01:47<00:11,  3.21it/s] 78%|███████▊  | 124/160 [01:47<00:11,  3.25it/s] 78%|███████▊  | 125/160 [01:47<00:10,  3.28it/s] 79%|███████▉  | 126/160 [01:48<00:10,  3.31it/s] 79%|███████▉  | 127/160 [01:48<00:12,  2.58it/s] 80%|████████  | 128/160 [01:48<00:10,  2.93it/s][INFO|trainer.py:2140] 2023-08-29 00:39:25,483 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:39:25,483 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 00:39:25,483 >>   Batch size = 8
{'eval_loss': 0.9839627146720886, 'eval_runtime': 9.8328, 'eval_samples_per_second': 353.614, 'eval_steps_per_second': 44.24, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 55.84it/s][A
  3%|▎         | 12/435 [00:00<00:08, 48.55it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.05it/s][A
  5%|▌         | 22/435 [00:00<00:08, 46.37it/s][A
  6%|▌         | 27/435 [00:00<00:08, 45.83it/s][A
  7%|▋         | 32/435 [00:00<00:08, 45.27it/s][A
  9%|▊         | 37/435 [00:00<00:08, 44.82it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.21it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.21it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.47it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.63it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.69it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.74it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.77it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 44.66it/s][A
 19%|█▉        | 82/435 [00:01<00:07, 44.36it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.18it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.19it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.39it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.52it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.64it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.70it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 44.65it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 44.45it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 44.23it/s][A
 30%|███       | 132/435 [00:02<00:06, 44.15it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 44.09it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 44.28it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 44.55it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.67it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.72it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 44.59it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 44.44it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.18it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.10it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.13it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.31it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 44.47it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 44.59it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 44.72it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 44.69it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 44.47it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 44.25it/s][A
 51%|█████     | 222/435 [00:04<00:04, 44.11it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.21it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 43.78it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.06it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.31it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.51it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 44.62it/s][A
 59%|█████▉    | 257/435 [00:05<00:04, 44.39it/s][A
 60%|██████    | 262/435 [00:05<00:03, 44.27it/s][A
 61%|██████▏   | 267/435 [00:05<00:03, 44.15it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.20it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.30it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.21it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.47it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.57it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.59it/s][A
 69%|██████▉   | 302/435 [00:06<00:02, 44.48it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.28it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.12it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.15it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.22it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.34it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.47it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.63it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 44.70it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 44.57it/s][A
 81%|████████  | 352/435 [00:07<00:01, 44.34it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 44.25it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 44.17it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 43.17it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 43.73it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 44.05it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 44.32it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 44.44it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 44.51it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 44.24it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 44.21it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 44.06it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 44.19it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 44.40it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.55it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 44.64it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.65it/s][A                                                 
                                                 [A 80%|████████  | 128/160 [01:58<00:10,  2.93it/s]
100%|██████████| 435/435 [00:09<00:00, 44.65it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:39:35,480 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-128
[INFO|configuration_utils.py:351] 2023-08-29 00:39:35,619 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-128/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:39:38,440 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-128/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:39:38,598 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-128/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:39:38,671 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-128/special_tokens_map.json
 81%|████████  | 129/160 [02:08<03:08,  6.08s/it] 81%|████████▏ | 130/160 [02:08<02:10,  4.35s/it] 82%|████████▏ | 131/160 [02:09<01:30,  3.14s/it] 82%|████████▎ | 132/160 [02:09<01:03,  2.28s/it] 83%|████████▎ | 133/160 [02:09<00:45,  1.69s/it] 84%|████████▍ | 134/160 [02:09<00:33,  1.27s/it] 84%|████████▍ | 135/160 [02:10<00:24,  1.02it/s] 85%|████████▌ | 136/160 [02:10<00:18,  1.29it/s] 86%|████████▌ | 137/160 [02:10<00:14,  1.58it/s] 86%|████████▋ | 138/160 [02:11<00:11,  1.88it/s] 87%|████████▋ | 139/160 [02:11<00:09,  2.17it/s] 88%|████████▊ | 140/160 [02:11<00:08,  2.42it/s] 88%|████████▊ | 141/160 [02:12<00:07,  2.57it/s] 89%|████████▉ | 142/160 [02:12<00:06,  2.76it/s] 89%|████████▉ | 143/160 [02:12<00:05,  2.92it/s] 90%|█████████ | 144/160 [02:12<00:05,  3.04it/s] 91%|█████████ | 145/160 [02:13<00:04,  3.12it/s] 91%|█████████▏| 146/160 [02:13<00:04,  3.19it/s] 92%|█████████▏| 147/160 [02:13<00:04,  3.24it/s] 92%|█████████▎| 148/160 [02:14<00:03,  3.27it/s] 93%|█████████▎| 149/160 [02:14<00:03,  3.30it/s] 94%|█████████▍| 150/160 [02:14<00:03,  3.31it/s] 94%|█████████▍| 151/160 [02:15<00:02,  3.26it/s] 95%|█████████▌| 152/160 [02:15<00:02,  3.29it/s] 96%|█████████▌| 153/160 [02:15<00:02,  3.31it/s] 96%|█████████▋| 154/160 [02:15<00:01,  3.32it/s] 97%|█████████▋| 155/160 [02:16<00:01,  3.34it/s] 98%|█████████▊| 156/160 [02:16<00:01,  3.34it/s] 98%|█████████▊| 157/160 [02:16<00:00,  3.35it/s] 99%|█████████▉| 158/160 [02:17<00:00,  3.35it/s] 99%|█████████▉| 159/160 [02:17<00:00,  3.36it/s]100%|██████████| 160/160 [02:17<00:00,  3.71it/s][INFO|trainer.py:2140] 2023-08-29 00:39:54,172 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:39:54,172 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 00:39:54,172 >>   Batch size = 8
{'eval_loss': 0.9910581707954407, 'eval_runtime': 9.7959, 'eval_samples_per_second': 354.945, 'eval_steps_per_second': 44.406, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 55.80it/s][A
  3%|▎         | 12/435 [00:00<00:09, 46.22it/s][A
  4%|▍         | 17/435 [00:00<00:09, 45.50it/s][A
  5%|▌         | 22/435 [00:00<00:09, 45.18it/s][A
  6%|▌         | 27/435 [00:00<00:09, 44.87it/s][A
  7%|▋         | 32/435 [00:00<00:09, 44.60it/s][A
  9%|▊         | 37/435 [00:00<00:08, 44.44it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.29it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.41it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.52it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.63it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.65it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.66it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.47it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 44.32it/s][A
 19%|█▉        | 82/435 [00:01<00:07, 44.26it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.17it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.31it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.40it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.50it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.62it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.59it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 44.51it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 44.35it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 44.23it/s][A
 30%|███       | 132/435 [00:02<00:06, 44.19it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 44.31it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 44.43it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 41.98it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 42.80it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 43.43it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 43.83it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 43.93it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 43.94it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.09it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.19it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 41.46it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 42.43it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 43.17it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 43.65it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 43.94it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 44.07it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 44.07it/s][A
 51%|█████     | 222/435 [00:05<00:04, 44.17it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 43.95it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.04it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.22it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.42it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.62it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 44.60it/s][A
 59%|█████▉    | 257/435 [00:05<00:03, 44.63it/s][A
 60%|██████    | 262/435 [00:05<00:03, 44.44it/s][A
 61%|██████▏   | 267/435 [00:06<00:03, 44.28it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.13it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.19it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.35it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.39it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.49it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.55it/s][A
 69%|██████▉   | 302/435 [00:06<00:02, 44.53it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.54it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.31it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.23it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 37.74it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 39.67it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 41.10it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 42.22it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 43.00it/s][A
 80%|███████▉  | 347/435 [00:07<00:02, 43.51it/s][A
 81%|████████  | 352/435 [00:08<00:01, 43.91it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 44.00it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 43.78it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 43.65it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 43.75it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 44.04it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 44.27it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 44.33it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 44.50it/s][A
 91%|█████████▏| 397/435 [00:09<00:00, 44.56it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 44.61it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 44.34it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 44.16it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 44.13it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.22it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 44.43it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.51it/s][A                                                 
                                                 [A100%|██████████| 160/160 [02:27<00:00,  3.71it/s]
100%|██████████| 435/435 [00:09<00:00, 44.51it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:40:04,170 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-160
[INFO|configuration_utils.py:351] 2023-08-29 00:40:04,508 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-160/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:40:08,976 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-160/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:40:09,326 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-160/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:40:09,492 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-160/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 00:40:16,747 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 00:40:16,779 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-32 (score: 0.9707925915718079).
                                                 100%|██████████| 160/160 [02:47<00:00,  3.71it/s]100%|██████████| 160/160 [02:47<00:00,  1.05s/it]
[INFO|trainer.py:1894] 2023-08-29 00:40:24,534 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-29 00:40:24,672 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:40:27,928 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:40:28,113 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:40:28,181 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:40:29,072 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:40:29,072 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:40:29,073 >>   train_loss               =     0.5789
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:40:29,073 >>   train_runtime            = 0:02:47.94
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:40:29,073 >>   train_samples            =       2022
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:40:29,073 >>   train_samples_per_second =     60.197
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:40:29,073 >>   train_steps_per_second   =      0.953
{'eval_loss': 0.9934215545654297, 'eval_runtime': 9.905, 'eval_samples_per_second': 351.035, 'eval_steps_per_second': 43.917, 'epoch': 5.0}
{'train_runtime': 167.948, 'train_samples_per_second': 60.197, 'train_steps_per_second': 0.953, 'train_loss': 0.5788978099822998, 'epoch': 5.0}
08/29/2023 00:40:29 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 00:40:29,337 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:40:29,337 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 00:40:29,338 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|▏         | 6/435 [00:00<00:07, 55.86it/s]  3%|▎         | 12/435 [00:00<00:08, 49.44it/s]  4%|▍         | 17/435 [00:00<00:08, 47.55it/s]  5%|▌         | 22/435 [00:00<00:08, 46.81it/s]  6%|▌         | 27/435 [00:00<00:08, 46.32it/s]  7%|▋         | 32/435 [00:00<00:08, 46.03it/s]  9%|▊         | 37/435 [00:00<00:08, 45.75it/s] 10%|▉         | 42/435 [00:00<00:08, 45.41it/s] 11%|█         | 47/435 [00:01<00:08, 44.84it/s] 12%|█▏        | 52/435 [00:01<00:08, 44.51it/s] 13%|█▎        | 57/435 [00:01<00:08, 44.49it/s] 14%|█▍        | 62/435 [00:01<00:08, 44.53it/s] 15%|█▌        | 67/435 [00:01<00:08, 44.78it/s] 17%|█▋        | 72/435 [00:01<00:08, 44.82it/s] 18%|█▊        | 77/435 [00:01<00:07, 44.98it/s] 19%|█▉        | 82/435 [00:01<00:07, 44.94it/s] 20%|██        | 87/435 [00:01<00:07, 44.93it/s] 21%|██        | 92/435 [00:02<00:07, 44.69it/s] 22%|██▏       | 97/435 [00:02<00:07, 44.56it/s] 23%|██▎       | 102/435 [00:02<00:07, 44.40it/s] 25%|██▍       | 107/435 [00:02<00:07, 44.48it/s] 26%|██▌       | 112/435 [00:02<00:07, 44.60it/s] 27%|██▋       | 117/435 [00:02<00:07, 44.69it/s] 28%|██▊       | 122/435 [00:02<00:06, 44.75it/s] 29%|██▉       | 127/435 [00:02<00:07, 42.82it/s] 30%|███       | 132/435 [00:02<00:06, 43.39it/s] 31%|███▏      | 137/435 [00:03<00:06, 43.84it/s] 33%|███▎      | 142/435 [00:03<00:06, 44.03it/s] 34%|███▍      | 147/435 [00:03<00:06, 44.11it/s] 35%|███▍      | 152/435 [00:03<00:06, 44.29it/s] 36%|███▌      | 157/435 [00:03<00:06, 44.44it/s] 37%|███▋      | 162/435 [00:03<00:06, 44.58it/s] 38%|███▊      | 167/435 [00:03<00:06, 44.56it/s] 40%|███▉      | 172/435 [00:03<00:05, 44.44it/s] 41%|████      | 177/435 [00:03<00:05, 44.54it/s] 42%|████▏     | 182/435 [00:04<00:05, 44.60it/s] 43%|████▎     | 187/435 [00:04<00:05, 44.57it/s] 44%|████▍     | 192/435 [00:04<00:05, 44.51it/s] 45%|████▌     | 197/435 [00:04<00:05, 44.49it/s] 46%|████▋     | 202/435 [00:04<00:05, 44.60it/s] 48%|████▊     | 207/435 [00:04<00:05, 44.68it/s] 49%|████▊     | 212/435 [00:04<00:04, 44.64it/s] 50%|████▉     | 217/435 [00:04<00:04, 44.61it/s] 51%|█████     | 222/435 [00:04<00:04, 44.62it/s] 52%|█████▏    | 227/435 [00:05<00:04, 44.51it/s] 53%|█████▎    | 232/435 [00:05<00:04, 44.68it/s] 54%|█████▍    | 237/435 [00:05<00:04, 44.61it/s] 56%|█████▌    | 242/435 [00:05<00:04, 44.55it/s] 57%|█████▋    | 247/435 [00:05<00:04, 44.49it/s] 58%|█████▊    | 252/435 [00:05<00:04, 44.67it/s] 59%|█████▉    | 257/435 [00:05<00:03, 44.66it/s] 60%|██████    | 262/435 [00:05<00:04, 42.48it/s] 61%|██████▏   | 267/435 [00:05<00:03, 43.23it/s] 63%|██████▎   | 272/435 [00:06<00:03, 43.76it/s] 64%|██████▎   | 277/435 [00:06<00:03, 44.02it/s] 65%|██████▍   | 282/435 [00:06<00:03, 44.19it/s] 66%|██████▌   | 287/435 [00:06<00:03, 44.27it/s] 67%|██████▋   | 292/435 [00:06<00:03, 44.33it/s] 68%|██████▊   | 297/435 [00:06<00:03, 44.27it/s] 69%|██████▉   | 302/435 [00:06<00:03, 44.14it/s] 71%|███████   | 307/435 [00:06<00:02, 44.32it/s] 72%|███████▏  | 312/435 [00:06<00:02, 44.45it/s] 73%|███████▎  | 317/435 [00:07<00:02, 44.58it/s] 74%|███████▍  | 322/435 [00:07<00:02, 44.73it/s] 75%|███████▌  | 327/435 [00:07<00:02, 44.71it/s] 76%|███████▋  | 332/435 [00:07<00:02, 44.70it/s] 77%|███████▋  | 337/435 [00:07<00:02, 44.64it/s] 79%|███████▊  | 342/435 [00:07<00:02, 44.43it/s] 80%|███████▉  | 347/435 [00:07<00:01, 44.33it/s] 81%|████████  | 352/435 [00:07<00:01, 44.38it/s] 82%|████████▏ | 357/435 [00:08<00:01, 44.50it/s] 83%|████████▎ | 362/435 [00:08<00:01, 44.51it/s] 84%|████████▍ | 367/435 [00:08<00:01, 44.66it/s] 86%|████████▌ | 372/435 [00:08<00:01, 44.66it/s] 87%|████████▋ | 377/435 [00:08<00:01, 44.59it/s] 88%|████████▊ | 382/435 [00:08<00:01, 44.53it/s] 89%|████████▉ | 387/435 [00:08<00:01, 44.38it/s] 90%|█████████ | 392/435 [00:08<00:00, 44.28it/s] 91%|█████████▏| 397/435 [00:08<00:00, 43.36it/s] 92%|█████████▏| 402/435 [00:09<00:00, 43.75it/s] 94%|█████████▎| 407/435 [00:09<00:00, 44.13it/s] 95%|█████████▍| 412/435 [00:09<00:00, 44.20it/s] 96%|█████████▌| 417/435 [00:09<00:00, 44.42it/s] 97%|█████████▋| 422/435 [00:09<00:00, 44.44it/s] 98%|█████████▊| 427/435 [00:09<00:00, 44.42it/s] 99%|█████████▉| 432/435 [00:09<00:00, 44.34it/s]100%|██████████| 435/435 [00:09<00:00, 44.54it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:40:39,130 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:40:39,130 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:40:39,130 >>   eval_loss               =     0.9708
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:40:39,130 >>   eval_runtime            = 0:00:09.79
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:40:39,130 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:40:39,130 >>   eval_samples_per_second =    355.075
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:40:39,130 >>   eval_steps_per_second   =     44.423
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:40:39,130 >>   perplexity              =       2.64
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:48,770 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:48,804 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:48,805 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:48,805 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:48,805 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:40:49,390 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:40:49,391 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:40:50,206 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:40:51,275 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:40:51,275 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:53,604 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:53,639 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:53,639 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:53,639 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:53,639 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:40:54,126 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:40:54,127 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:40:54,452 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:40:54,672 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:40:54,672 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-128
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-64
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-32
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-160
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/checkpoint-96
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.41it/s]Extractor Predicting: 2it [00:01,  1.38it/s]Extractor Predicting: 3it [00:02,  1.39it/s]Extractor Predicting: 4it [00:02,  1.38it/s]Extractor Predicting: 5it [00:03,  1.37it/s]Extractor Predicting: 6it [00:04,  1.34it/s]Extractor Predicting: 7it [00:05,  1.28it/s]Extractor Predicting: 8it [00:06,  1.29it/s]Extractor Predicting: 9it [00:06,  1.31it/s]Extractor Predicting: 10it [00:07,  1.28it/s]Extractor Predicting: 11it [00:08,  1.26it/s]Extractor Predicting: 12it [00:09,  1.26it/s]Extractor Predicting: 13it [00:09,  1.29it/s]Extractor Predicting: 14it [00:10,  1.27it/s]Extractor Predicting: 15it [00:11,  1.27it/s]Extractor Predicting: 16it [00:12,  1.27it/s]Extractor Predicting: 17it [00:13,  1.31it/s]Extractor Predicting: 18it [00:13,  1.32it/s]Extractor Predicting: 19it [00:14,  1.31it/s]Extractor Predicting: 20it [00:15,  1.30it/s]Extractor Predicting: 21it [00:16,  1.31it/s]Extractor Predicting: 22it [00:16,  1.33it/s]Extractor Predicting: 23it [00:17,  1.33it/s]Extractor Predicting: 24it [00:18,  1.34it/s]Extractor Predicting: 25it [00:19,  1.31it/s]Extractor Predicting: 26it [00:19,  1.30it/s]Extractor Predicting: 27it [00:20,  1.29it/s]Extractor Predicting: 28it [00:21,  1.27it/s]Extractor Predicting: 29it [00:22,  1.26it/s]Extractor Predicting: 30it [00:23,  1.27it/s]Extractor Predicting: 31it [00:23,  1.26it/s]Extractor Predicting: 32it [00:24,  1.27it/s]Extractor Predicting: 33it [00:25,  1.25it/s]Extractor Predicting: 34it [00:26,  1.27it/s]Extractor Predicting: 35it [00:26,  1.30it/s]Extractor Predicting: 36it [00:27,  1.33it/s]Extractor Predicting: 37it [00:28,  1.32it/s]Extractor Predicting: 38it [00:29,  1.28it/s]Extractor Predicting: 39it [00:30,  1.29it/s]Extractor Predicting: 40it [00:30,  1.29it/s]Extractor Predicting: 41it [00:31,  1.32it/s]Extractor Predicting: 42it [00:32,  1.32it/s]Extractor Predicting: 43it [00:33,  1.30it/s]Extractor Predicting: 44it [00:33,  1.30it/s]Extractor Predicting: 45it [00:34,  1.28it/s]Extractor Predicting: 46it [00:35,  1.30it/s]Extractor Predicting: 47it [00:36,  1.30it/s]Extractor Predicting: 48it [00:36,  1.30it/s]Extractor Predicting: 49it [00:37,  1.32it/s]Extractor Predicting: 50it [00:38,  1.36it/s]Extractor Predicting: 51it [00:39,  1.35it/s]Extractor Predicting: 52it [00:39,  1.30it/s]Extractor Predicting: 53it [00:40,  1.30it/s]Extractor Predicting: 54it [00:41,  1.31it/s]Extractor Predicting: 55it [00:42,  1.22it/s]Extractor Predicting: 56it [00:43,  1.23it/s]Extractor Predicting: 57it [00:44,  1.24it/s]Extractor Predicting: 58it [00:44,  1.28it/s]Extractor Predicting: 59it [00:45,  1.31it/s]Extractor Predicting: 60it [00:46,  1.32it/s]Extractor Predicting: 61it [00:46,  1.33it/s]Extractor Predicting: 62it [00:47,  1.35it/s]Extractor Predicting: 63it [00:48,  1.34it/s]Extractor Predicting: 64it [00:49,  1.35it/s]Extractor Predicting: 65it [00:49,  1.33it/s]Extractor Predicting: 66it [00:50,  1.32it/s]Extractor Predicting: 67it [00:51,  1.32it/s]Extractor Predicting: 68it [00:52,  1.34it/s]Extractor Predicting: 69it [00:52,  1.33it/s]Extractor Predicting: 70it [00:53,  1.33it/s]Extractor Predicting: 71it [00:54,  1.32it/s]Extractor Predicting: 72it [00:55,  1.34it/s]Extractor Predicting: 73it [00:55,  1.34it/s]Extractor Predicting: 74it [00:56,  1.35it/s]Extractor Predicting: 75it [00:57,  1.37it/s]Extractor Predicting: 76it [00:58,  1.35it/s]Extractor Predicting: 77it [00:58,  1.32it/s]Extractor Predicting: 78it [00:59,  1.31it/s]Extractor Predicting: 79it [01:00,  1.29it/s]Extractor Predicting: 80it [01:01,  1.29it/s]Extractor Predicting: 81it [01:02,  1.28it/s]Extractor Predicting: 82it [01:02,  1.30it/s]Extractor Predicting: 83it [01:03,  1.31it/s]Extractor Predicting: 84it [01:04,  1.32it/s]Extractor Predicting: 85it [01:05,  1.31it/s]Extractor Predicting: 86it [01:05,  1.32it/s]Extractor Predicting: 87it [01:06,  1.28it/s]Extractor Predicting: 88it [01:07,  1.29it/s]Extractor Predicting: 89it [01:08,  1.32it/s]Extractor Predicting: 90it [01:08,  1.32it/s]Extractor Predicting: 91it [01:09,  1.33it/s]Extractor Predicting: 92it [01:10,  1.36it/s]Extractor Predicting: 93it [01:11,  1.38it/s]Extractor Predicting: 94it [01:11,  1.35it/s]Extractor Predicting: 95it [01:12,  1.39it/s]Extractor Predicting: 96it [01:13,  1.33it/s]Extractor Predicting: 97it [01:14,  1.34it/s]Extractor Predicting: 98it [01:14,  1.34it/s]Extractor Predicting: 99it [01:15,  1.32it/s]Extractor Predicting: 100it [01:16,  1.29it/s]Extractor Predicting: 101it [01:17,  1.29it/s]Extractor Predicting: 102it [01:17,  1.35it/s]Extractor Predicting: 103it [01:18,  1.37it/s]Extractor Predicting: 104it [01:19,  1.36it/s]Extractor Predicting: 105it [01:20,  1.34it/s]Extractor Predicting: 106it [01:20,  1.35it/s]Extractor Predicting: 107it [01:21,  1.36it/s]Extractor Predicting: 108it [01:22,  1.35it/s]Extractor Predicting: 109it [01:22,  1.35it/s]Extractor Predicting: 110it [01:23,  1.35it/s]Extractor Predicting: 111it [01:24,  1.36it/s]Extractor Predicting: 112it [01:25,  1.35it/s]Extractor Predicting: 113it [01:25,  1.39it/s]Extractor Predicting: 114it [01:26,  1.42it/s]Extractor Predicting: 115it [01:27,  1.39it/s]Extractor Predicting: 116it [01:28,  1.39it/s]Extractor Predicting: 117it [01:28,  1.40it/s]Extractor Predicting: 118it [01:29,  1.38it/s]Extractor Predicting: 119it [01:30,  1.39it/s]Extractor Predicting: 120it [01:31,  1.34it/s]Extractor Predicting: 121it [01:31,  1.32it/s]Extractor Predicting: 122it [01:32,  1.33it/s]Extractor Predicting: 123it [01:33,  1.33it/s]Extractor Predicting: 124it [01:34,  1.30it/s]Extractor Predicting: 125it [01:34,  1.31it/s]Extractor Predicting: 126it [01:35,  1.33it/s]Extractor Predicting: 127it [01:36,  1.33it/s]Extractor Predicting: 128it [01:37,  1.33it/s]Extractor Predicting: 129it [01:37,  1.33it/s]Extractor Predicting: 130it [01:38,  1.31it/s]Extractor Predicting: 131it [01:39,  1.23it/s]Extractor Predicting: 132it [01:40,  1.24it/s]Extractor Predicting: 133it [01:41,  1.27it/s]Extractor Predicting: 134it [01:41,  1.31it/s]Extractor Predicting: 135it [01:42,  1.30it/s]Extractor Predicting: 136it [01:43,  1.29it/s]Extractor Predicting: 137it [01:44,  1.32it/s]Extractor Predicting: 138it [01:44,  1.31it/s]Extractor Predicting: 139it [01:45,  1.30it/s]Extractor Predicting: 140it [01:46,  1.31it/s]Extractor Predicting: 141it [01:47,  1.32it/s]Extractor Predicting: 142it [01:47,  1.31it/s]Extractor Predicting: 143it [01:48,  1.34it/s]Extractor Predicting: 144it [01:49,  1.39it/s]Extractor Predicting: 144it [01:49,  1.32it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:42:57,044 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:42:57,085 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:42:57,085 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:42:57,085 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:42:57,085 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:42:57,718 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:42:57,719 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:42:58,088 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:42:59,204 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:42:59,204 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:43:01,579 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:43:01,595 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:43:01,595 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:43:01,595 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:43:01,596 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:43:02,049 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:43:02,050 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:43:02,769 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:43:03,024 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:43:03,024 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3285219399538106,
  "recall": 0.1636468219729652,
  "score": 0.21846803609138032,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.35it/s]Extractor Predicting: 2it [00:01,  1.31it/s]Extractor Predicting: 3it [00:02,  1.32it/s]Extractor Predicting: 4it [00:03,  1.33it/s]Extractor Predicting: 5it [00:03,  1.33it/s]Extractor Predicting: 6it [00:04,  1.35it/s]Extractor Predicting: 7it [00:05,  1.38it/s]Extractor Predicting: 8it [00:05,  1.41it/s]Extractor Predicting: 9it [00:06,  1.37it/s]Extractor Predicting: 10it [00:07,  1.36it/s]Extractor Predicting: 11it [00:08,  1.37it/s]Extractor Predicting: 12it [00:08,  1.35it/s]Extractor Predicting: 13it [00:09,  1.35it/s]Extractor Predicting: 14it [00:10,  1.36it/s]Extractor Predicting: 15it [00:11,  1.36it/s]Extractor Predicting: 16it [00:11,  1.36it/s]Extractor Predicting: 17it [00:12,  1.35it/s]Extractor Predicting: 18it [00:13,  1.29it/s]Extractor Predicting: 19it [00:14,  1.34it/s]Extractor Predicting: 20it [00:14,  1.32it/s]Extractor Predicting: 21it [00:15,  1.35it/s]Extractor Predicting: 22it [00:16,  1.37it/s]Extractor Predicting: 23it [00:16,  1.38it/s]Extractor Predicting: 24it [00:17,  1.36it/s]Extractor Predicting: 25it [00:18,  1.35it/s]Extractor Predicting: 26it [00:19,  1.36it/s]Extractor Predicting: 27it [00:19,  1.34it/s]Extractor Predicting: 28it [00:20,  1.33it/s]Extractor Predicting: 29it [00:21,  1.35it/s]Extractor Predicting: 30it [00:22,  1.38it/s]Extractor Predicting: 31it [00:22,  1.36it/s]Extractor Predicting: 32it [00:23,  1.42it/s]Extractor Predicting: 33it [00:24,  1.39it/s]Extractor Predicting: 34it [00:25,  1.37it/s]Extractor Predicting: 35it [00:25,  1.36it/s]Extractor Predicting: 36it [00:26,  1.37it/s]Extractor Predicting: 37it [00:27,  1.37it/s]Extractor Predicting: 38it [00:27,  1.39it/s]Extractor Predicting: 39it [00:28,  1.39it/s]Extractor Predicting: 40it [00:29,  1.38it/s]Extractor Predicting: 41it [00:30,  1.39it/s]Extractor Predicting: 42it [00:30,  1.39it/s]Extractor Predicting: 43it [00:31,  1.34it/s]Extractor Predicting: 44it [00:32,  1.35it/s]Extractor Predicting: 45it [00:33,  1.33it/s]Extractor Predicting: 46it [00:33,  1.31it/s]Extractor Predicting: 47it [00:34,  1.34it/s]Extractor Predicting: 48it [00:35,  1.34it/s]Extractor Predicting: 49it [00:36,  1.36it/s]Extractor Predicting: 50it [00:36,  1.36it/s]Extractor Predicting: 51it [00:37,  1.34it/s]Extractor Predicting: 52it [00:38,  1.32it/s]Extractor Predicting: 53it [00:39,  1.35it/s]Extractor Predicting: 54it [00:39,  1.32it/s]Extractor Predicting: 55it [00:40,  1.35it/s]Extractor Predicting: 56it [00:41,  1.34it/s]Extractor Predicting: 57it [00:42,  1.33it/s]Extractor Predicting: 58it [00:42,  1.31it/s]Extractor Predicting: 59it [00:43,  1.28it/s]Extractor Predicting: 60it [00:44,  1.32it/s]Extractor Predicting: 61it [00:45,  1.32it/s]Extractor Predicting: 62it [00:45,  1.32it/s]Extractor Predicting: 63it [00:46,  1.32it/s]Extractor Predicting: 64it [00:47,  1.31it/s]Extractor Predicting: 65it [00:48,  1.36it/s]Extractor Predicting: 66it [00:48,  1.33it/s]Extractor Predicting: 67it [00:49,  1.30it/s]Extractor Predicting: 68it [00:50,  1.30it/s]Extractor Predicting: 69it [00:51,  1.30it/s]Extractor Predicting: 70it [00:52,  1.28it/s]Extractor Predicting: 71it [00:52,  1.29it/s]Extractor Predicting: 72it [00:53,  1.29it/s]Extractor Predicting: 73it [00:54,  1.28it/s]Extractor Predicting: 74it [00:55,  1.30it/s]Extractor Predicting: 75it [00:55,  1.30it/s]Extractor Predicting: 76it [00:56,  1.32it/s]Extractor Predicting: 77it [00:57,  1.32it/s]Extractor Predicting: 78it [00:58,  1.31it/s]Extractor Predicting: 79it [00:58,  1.32it/s]Extractor Predicting: 80it [00:59,  1.37it/s]Extractor Predicting: 81it [01:00,  1.38it/s]Extractor Predicting: 82it [01:01,  1.37it/s]Extractor Predicting: 83it [01:01,  1.34it/s]Extractor Predicting: 84it [01:02,  1.34it/s]Extractor Predicting: 85it [01:03,  1.32it/s]Extractor Predicting: 86it [01:04,  1.28it/s]Extractor Predicting: 87it [01:05,  1.24it/s]Extractor Predicting: 88it [01:05,  1.28it/s]Extractor Predicting: 89it [01:06,  1.27it/s]Extractor Predicting: 90it [01:07,  1.26it/s]Extractor Predicting: 91it [01:08,  1.23it/s]Extractor Predicting: 92it [01:09,  1.25it/s]Extractor Predicting: 93it [01:09,  1.27it/s]Extractor Predicting: 94it [01:10,  1.30it/s]Extractor Predicting: 95it [01:11,  1.29it/s]Extractor Predicting: 96it [01:12,  1.29it/s]Extractor Predicting: 97it [01:12,  1.27it/s]Extractor Predicting: 98it [01:13,  1.25it/s]Extractor Predicting: 99it [01:14,  1.25it/s]Extractor Predicting: 100it [01:15,  1.28it/s]Extractor Predicting: 101it [01:16,  1.29it/s]Extractor Predicting: 102it [01:16,  1.29it/s]Extractor Predicting: 103it [01:17,  1.26it/s]Extractor Predicting: 104it [01:18,  1.26it/s]Extractor Predicting: 105it [01:19,  1.29it/s]Extractor Predicting: 106it [01:19,  1.28it/s]Extractor Predicting: 107it [01:20,  1.26it/s]Extractor Predicting: 108it [01:21,  1.28it/s]Extractor Predicting: 109it [01:22,  1.28it/s]Extractor Predicting: 110it [01:23,  1.27it/s]Extractor Predicting: 111it [01:23,  1.28it/s]Extractor Predicting: 112it [01:24,  1.27it/s]Extractor Predicting: 113it [01:25,  1.28it/s]Extractor Predicting: 114it [01:26,  1.31it/s]Extractor Predicting: 115it [01:27,  1.18it/s]Extractor Predicting: 116it [01:28,  1.18it/s]Extractor Predicting: 117it [01:28,  1.19it/s]Extractor Predicting: 118it [01:29,  1.23it/s]Extractor Predicting: 119it [01:30,  1.24it/s]Extractor Predicting: 120it [01:31,  1.25it/s]Extractor Predicting: 121it [01:32,  1.25it/s]Extractor Predicting: 122it [01:32,  1.29it/s]Extractor Predicting: 123it [01:33,  1.30it/s]Extractor Predicting: 124it [01:34,  1.30it/s]Extractor Predicting: 125it [01:35,  1.29it/s]Extractor Predicting: 126it [01:35,  1.35it/s]Extractor Predicting: 127it [01:36,  1.35it/s]Extractor Predicting: 128it [01:37,  1.32it/s]Extractor Predicting: 129it [01:37,  1.36it/s]Extractor Predicting: 130it [01:38,  1.35it/s]Extractor Predicting: 131it [01:39,  1.34it/s]Extractor Predicting: 132it [01:40,  1.35it/s]Extractor Predicting: 133it [01:40,  1.39it/s]Extractor Predicting: 134it [01:41,  1.40it/s]Extractor Predicting: 135it [01:42,  1.33it/s]Extractor Predicting: 136it [01:43,  1.35it/s]Extractor Predicting: 137it [01:43,  1.37it/s]Extractor Predicting: 138it [01:44,  1.40it/s]Extractor Predicting: 139it [01:45,  1.39it/s]Extractor Predicting: 140it [01:45,  1.36it/s]Extractor Predicting: 141it [01:46,  1.35it/s]Extractor Predicting: 142it [01:47,  1.37it/s]Extractor Predicting: 143it [01:48,  1.34it/s]Extractor Predicting: 144it [01:49,  1.31it/s]Extractor Predicting: 145it [01:49,  1.32it/s]Extractor Predicting: 146it [01:50,  1.35it/s]Extractor Predicting: 147it [01:51,  1.39it/s]Extractor Predicting: 148it [01:51,  1.39it/s]Extractor Predicting: 149it [01:52,  1.39it/s]Extractor Predicting: 150it [01:53,  1.43it/s]Extractor Predicting: 151it [01:53,  1.42it/s]Extractor Predicting: 152it [01:54,  1.43it/s]Extractor Predicting: 153it [01:55,  1.43it/s]Extractor Predicting: 154it [01:56,  1.44it/s]Extractor Predicting: 155it [01:56,  1.43it/s]Extractor Predicting: 156it [01:57,  1.44it/s]Extractor Predicting: 157it [01:58,  1.47it/s]Extractor Predicting: 158it [01:58,  1.46it/s]Extractor Predicting: 159it [01:59,  1.47it/s]Extractor Predicting: 160it [02:00,  1.53it/s]Extractor Predicting: 161it [02:00,  1.48it/s]Extractor Predicting: 162it [02:01,  1.44it/s]Extractor Predicting: 163it [02:02,  1.44it/s]Extractor Predicting: 164it [02:02,  1.42it/s]Extractor Predicting: 165it [02:03,  1.46it/s]Extractor Predicting: 166it [02:04,  1.47it/s]Extractor Predicting: 167it [02:04,  1.46it/s]Extractor Predicting: 168it [02:05,  1.44it/s]Extractor Predicting: 169it [02:06,  1.44it/s]Extractor Predicting: 170it [02:07,  1.44it/s]Extractor Predicting: 171it [02:07,  1.47it/s]Extractor Predicting: 172it [02:08,  1.47it/s]Extractor Predicting: 173it [02:09,  1.46it/s]Extractor Predicting: 174it [02:09,  1.37it/s]Extractor Predicting: 175it [02:10,  1.37it/s]Extractor Predicting: 176it [02:11,  1.36it/s]Extractor Predicting: 177it [02:12,  1.34it/s]Extractor Predicting: 178it [02:12,  1.29it/s]Extractor Predicting: 179it [02:13,  1.29it/s]Extractor Predicting: 180it [02:14,  1.31it/s]Extractor Predicting: 181it [02:15,  1.31it/s]Extractor Predicting: 182it [02:16,  1.31it/s]Extractor Predicting: 183it [02:16,  1.33it/s]Extractor Predicting: 184it [02:17,  1.33it/s]Extractor Predicting: 185it [02:18,  1.33it/s]Extractor Predicting: 186it [02:19,  1.33it/s]Extractor Predicting: 187it [02:19,  1.32it/s]Extractor Predicting: 188it [02:20,  1.31it/s]Extractor Predicting: 189it [02:21,  1.32it/s]Extractor Predicting: 190it [02:22,  1.29it/s]Extractor Predicting: 191it [02:22,  1.28it/s]Extractor Predicting: 192it [02:23,  1.28it/s]Extractor Predicting: 193it [02:24,  1.27it/s]Extractor Predicting: 194it [02:25,  1.26it/s]Extractor Predicting: 195it [02:26,  1.27it/s]Extractor Predicting: 196it [02:26,  1.31it/s]Extractor Predicting: 197it [02:27,  1.32it/s]Extractor Predicting: 198it [02:28,  1.31it/s]Extractor Predicting: 199it [02:29,  1.33it/s]Extractor Predicting: 200it [02:29,  1.29it/s]Extractor Predicting: 201it [02:30,  1.27it/s]Extractor Predicting: 202it [02:31,  1.25it/s]Extractor Predicting: 203it [02:32,  1.23it/s]Extractor Predicting: 204it [02:33,  1.23it/s]Extractor Predicting: 205it [02:33,  1.22it/s]Extractor Predicting: 206it [02:34,  1.23it/s]Extractor Predicting: 207it [02:35,  1.24it/s]Extractor Predicting: 208it [02:36,  1.23it/s]Extractor Predicting: 209it [02:37,  1.19it/s]Extractor Predicting: 210it [02:38,  1.20it/s]Extractor Predicting: 211it [02:38,  1.20it/s]Extractor Predicting: 212it [02:39,  1.22it/s]Extractor Predicting: 213it [02:40,  1.21it/s]Extractor Predicting: 214it [02:41,  1.24it/s]Extractor Predicting: 215it [02:42,  1.21it/s]Extractor Predicting: 216it [02:43,  1.22it/s]Extractor Predicting: 217it [02:43,  1.22it/s]Extractor Predicting: 218it [02:44,  1.22it/s]Extractor Predicting: 219it [02:45,  1.21it/s]Extractor Predicting: 220it [02:46,  1.19it/s]Extractor Predicting: 221it [02:47,  1.17it/s]Extractor Predicting: 222it [02:48,  1.17it/s]Extractor Predicting: 223it [02:48,  1.21it/s]Extractor Predicting: 224it [02:49,  1.23it/s]Extractor Predicting: 225it [02:50,  1.12it/s]Extractor Predicting: 226it [02:51,  1.14it/s]Extractor Predicting: 227it [02:52,  1.17it/s]Extractor Predicting: 228it [02:53,  1.20it/s]Extractor Predicting: 229it [02:53,  1.22it/s]Extractor Predicting: 230it [02:54,  1.25it/s]Extractor Predicting: 231it [02:55,  1.29it/s]Extractor Predicting: 232it [02:56,  1.33it/s]Extractor Predicting: 233it [02:56,  1.35it/s]Extractor Predicting: 234it [02:57,  1.29it/s]Extractor Predicting: 235it [02:58,  1.31it/s]Extractor Predicting: 236it [02:59,  1.32it/s]Extractor Predicting: 237it [02:59,  1.33it/s]Extractor Predicting: 238it [03:00,  1.32it/s]Extractor Predicting: 239it [03:01,  1.30it/s]Extractor Predicting: 240it [03:02,  1.34it/s]Extractor Predicting: 241it [03:02,  1.35it/s]Extractor Predicting: 242it [03:03,  1.38it/s]Extractor Predicting: 243it [03:04,  1.34it/s]Extractor Predicting: 244it [03:05,  1.40it/s]Extractor Predicting: 245it [03:05,  1.37it/s]Extractor Predicting: 246it [03:06,  1.35it/s]Extractor Predicting: 247it [03:07,  1.34it/s]Extractor Predicting: 248it [03:08,  1.30it/s]Extractor Predicting: 249it [03:08,  1.32it/s]Extractor Predicting: 250it [03:09,  1.33it/s]Extractor Predicting: 251it [03:10,  1.35it/s]Extractor Predicting: 252it [03:11,  1.38it/s]Extractor Predicting: 253it [03:11,  1.37it/s]Extractor Predicting: 254it [03:12,  1.34it/s]Extractor Predicting: 255it [03:13,  1.34it/s]Extractor Predicting: 256it [03:14,  1.33it/s]Extractor Predicting: 257it [03:14,  1.28it/s]Extractor Predicting: 258it [03:15,  1.29it/s]Extractor Predicting: 259it [03:16,  1.30it/s]Extractor Predicting: 260it [03:17,  1.29it/s]Extractor Predicting: 261it [03:18,  1.28it/s]Extractor Predicting: 262it [03:18,  1.30it/s]Extractor Predicting: 263it [03:19,  1.32it/s]Extractor Predicting: 264it [03:20,  1.30it/s]Extractor Predicting: 265it [03:21,  1.30it/s]Extractor Predicting: 266it [03:21,  1.28it/s]Extractor Predicting: 267it [03:22,  1.28it/s]Extractor Predicting: 268it [03:23,  1.27it/s]Extractor Predicting: 269it [03:24,  1.27it/s]Extractor Predicting: 270it [03:25,  1.28it/s]Extractor Predicting: 271it [03:25,  1.28it/s]Extractor Predicting: 272it [03:26,  1.31it/s]Extractor Predicting: 273it [03:27,  1.26it/s]Extractor Predicting: 274it [03:28,  1.24it/s]Extractor Predicting: 275it [03:28,  1.26it/s]Extractor Predicting: 276it [03:29,  1.26it/s]Extractor Predicting: 277it [03:30,  1.27it/s]Extractor Predicting: 278it [03:31,  1.28it/s]Extractor Predicting: 279it [03:32,  1.26it/s]Extractor Predicting: 280it [03:32,  1.28it/s]Extractor Predicting: 281it [03:33,  1.30it/s]Extractor Predicting: 282it [03:34,  1.29it/s]Extractor Predicting: 283it [03:35,  1.24it/s]Extractor Predicting: 284it [03:35,  1.30it/s]Extractor Predicting: 285it [03:36,  1.29it/s]Extractor Predicting: 286it [03:37,  1.32it/s]Extractor Predicting: 287it [03:38,  1.31it/s]Extractor Predicting: 288it [03:39,  1.30it/s]Extractor Predicting: 289it [03:39,  1.29it/s]Extractor Predicting: 290it [03:40,  1.28it/s]Extractor Predicting: 291it [03:41,  1.23it/s]Extractor Predicting: 292it [03:42,  1.23it/s]Extractor Predicting: 293it [03:43,  1.26it/s]Extractor Predicting: 294it [03:43,  1.23it/s]Extractor Predicting: 295it [03:44,  1.24it/s]Extractor Predicting: 296it [03:45,  1.24it/s]Extractor Predicting: 297it [03:46,  1.26it/s]Extractor Predicting: 298it [03:47,  1.25it/s]Extractor Predicting: 299it [03:47,  1.23it/s]Extractor Predicting: 300it [03:48,  1.26it/s]Extractor Predicting: 301it [03:49,  1.26it/s]Extractor Predicting: 302it [03:50,  1.28it/s]Extractor Predicting: 303it [03:51,  1.23it/s]Extractor Predicting: 304it [03:51,  1.24it/s]Extractor Predicting: 305it [03:52,  1.25it/s]Extractor Predicting: 306it [03:53,  1.28it/s]Extractor Predicting: 307it [03:54,  1.26it/s]Extractor Predicting: 308it [03:55,  1.28it/s]Extractor Predicting: 309it [03:55,  1.28it/s]Extractor Predicting: 310it [03:56,  1.30it/s]Extractor Predicting: 311it [03:57,  1.27it/s]Extractor Predicting: 312it [03:58,  1.31it/s]Extractor Predicting: 313it [03:58,  1.35it/s]Extractor Predicting: 314it [03:59,  1.37it/s]Extractor Predicting: 315it [04:00,  1.38it/s]Extractor Predicting: 316it [04:00,  1.35it/s]Extractor Predicting: 317it [04:01,  1.32it/s]Extractor Predicting: 318it [04:02,  1.29it/s]Extractor Predicting: 319it [04:03,  1.29it/s]Extractor Predicting: 320it [04:04,  1.29it/s]Extractor Predicting: 321it [04:04,  1.29it/s]Extractor Predicting: 322it [04:05,  1.28it/s]Extractor Predicting: 323it [04:06,  1.25it/s]Extractor Predicting: 324it [04:07,  1.26it/s]Extractor Predicting: 325it [04:08,  1.27it/s]Extractor Predicting: 326it [04:08,  1.27it/s]Extractor Predicting: 327it [04:09,  1.30it/s]Extractor Predicting: 328it [04:10,  1.29it/s]Extractor Predicting: 329it [04:11,  1.30it/s]Extractor Predicting: 330it [04:11,  1.30it/s]Extractor Predicting: 331it [04:12,  1.29it/s]Extractor Predicting: 332it [04:13,  1.29it/s]Extractor Predicting: 333it [04:14,  1.31it/s]Extractor Predicting: 334it [04:14,  1.30it/s]Extractor Predicting: 335it [04:15,  1.32it/s]Extractor Predicting: 336it [04:16,  1.19it/s]Extractor Predicting: 337it [04:17,  1.22it/s]Extractor Predicting: 338it [04:18,  1.25it/s]Extractor Predicting: 339it [04:18,  1.30it/s]Extractor Predicting: 340it [04:19,  1.32it/s]Extractor Predicting: 341it [04:20,  1.32it/s]Extractor Predicting: 342it [04:21,  1.31it/s]Extractor Predicting: 343it [04:22,  1.29it/s]Extractor Predicting: 344it [04:22,  1.32it/s]Extractor Predicting: 345it [04:23,  1.29it/s]Extractor Predicting: 346it [04:24,  1.30it/s]Extractor Predicting: 347it [04:25,  1.28it/s]Extractor Predicting: 348it [04:25,  1.30it/s]Extractor Predicting: 349it [04:26,  1.32it/s]Extractor Predicting: 350it [04:27,  1.33it/s]Extractor Predicting: 351it [04:28,  1.33it/s]Extractor Predicting: 352it [04:28,  1.30it/s]Extractor Predicting: 353it [04:29,  1.32it/s]Extractor Predicting: 354it [04:30,  1.35it/s]Extractor Predicting: 355it [04:31,  1.37it/s]Extractor Predicting: 356it [04:31,  1.34it/s]Extractor Predicting: 357it [04:32,  1.34it/s]Extractor Predicting: 358it [04:33,  1.34it/s]Extractor Predicting: 359it [04:34,  1.33it/s]Extractor Predicting: 360it [04:34,  1.33it/s]Extractor Predicting: 361it [04:35,  1.34it/s]Extractor Predicting: 362it [04:36,  1.32it/s]Extractor Predicting: 363it [04:37,  1.34it/s]Extractor Predicting: 364it [04:37,  1.35it/s]Extractor Predicting: 365it [04:38,  1.37it/s]Extractor Predicting: 366it [04:39,  1.35it/s]Extractor Predicting: 367it [04:40,  1.29it/s]Extractor Predicting: 368it [04:40,  1.30it/s]Extractor Predicting: 369it [04:41,  1.31it/s]Extractor Predicting: 370it [04:42,  1.35it/s]Extractor Predicting: 371it [04:43,  1.35it/s]Extractor Predicting: 372it [04:43,  1.35it/s]Extractor Predicting: 373it [04:44,  1.33it/s]Extractor Predicting: 374it [04:45,  1.34it/s]Extractor Predicting: 375it [04:46,  1.35it/s]Extractor Predicting: 376it [04:46,  1.35it/s]Extractor Predicting: 377it [04:47,  1.36it/s]Extractor Predicting: 378it [04:48,  1.39it/s]Extractor Predicting: 379it [04:48,  1.35it/s]Extractor Predicting: 380it [04:49,  1.35it/s]Extractor Predicting: 381it [04:50,  1.35it/s]Extractor Predicting: 382it [04:51,  1.36it/s]Extractor Predicting: 383it [04:51,  1.39it/s]Extractor Predicting: 384it [04:52,  1.42it/s]Extractor Predicting: 385it [04:53,  1.39it/s]Extractor Predicting: 386it [04:54,  1.36it/s]Extractor Predicting: 387it [04:54,  1.36it/s]Extractor Predicting: 388it [04:55,  1.35it/s]Extractor Predicting: 389it [04:56,  1.36it/s]Extractor Predicting: 390it [04:57,  1.36it/s]Extractor Predicting: 391it [04:57,  1.33it/s]Extractor Predicting: 392it [04:58,  1.34it/s]Extractor Predicting: 393it [04:59,  1.36it/s]Extractor Predicting: 394it [05:00,  1.31it/s]Extractor Predicting: 395it [05:00,  1.26it/s]Extractor Predicting: 396it [05:01,  1.26it/s]Extractor Predicting: 397it [05:02,  1.25it/s]Extractor Predicting: 398it [05:03,  1.26it/s]Extractor Predicting: 399it [05:04,  1.23it/s]Extractor Predicting: 400it [05:04,  1.28it/s]Extractor Predicting: 401it [05:05,  1.25it/s]Extractor Predicting: 402it [05:06,  1.24it/s]Extractor Predicting: 403it [05:07,  1.22it/s]Extractor Predicting: 404it [05:08,  1.19it/s]Extractor Predicting: 405it [05:09,  1.21it/s]Extractor Predicting: 406it [05:09,  1.21it/s]Extractor Predicting: 407it [05:10,  1.19it/s]Extractor Predicting: 408it [05:11,  1.22it/s]Extractor Predicting: 409it [05:12,  1.24it/s]Extractor Predicting: 410it [05:13,  1.22it/s]Extractor Predicting: 411it [05:14,  1.21it/s]Extractor Predicting: 412it [05:14,  1.23it/s]Extractor Predicting: 413it [05:15,  1.25it/s]Extractor Predicting: 414it [05:16,  1.28it/s]Extractor Predicting: 415it [05:17,  1.30it/s]Extractor Predicting: 416it [05:17,  1.30it/s]Extractor Predicting: 417it [05:18,  1.31it/s]Extractor Predicting: 418it [05:19,  1.29it/s]Extractor Predicting: 419it [05:20,  1.24it/s]Extractor Predicting: 420it [05:21,  1.25it/s]Extractor Predicting: 421it [05:21,  1.34it/s]Extractor Predicting: 421it [05:21,  1.31it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:40,521 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:40,562 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:40,563 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:40,563 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:40,563 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:48:41,355 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:48:41,356 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:48:41,974 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:48:43,099 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:48:43,099 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:46,025 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:46,044 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:46,044 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:46,044 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:46,044 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:48:46,815 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:48:46,816 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:48:47,436 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:48:47,685 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:48:47,685 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.18802281368821291,
  "recall": 0.0979692917285785,
  "score": 0.12881797460110714,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.24it/s]Extractor Predicting: 2it [00:01,  1.23it/s]Extractor Predicting: 3it [00:02,  1.23it/s]Extractor Predicting: 4it [00:03,  1.21it/s]Extractor Predicting: 5it [00:04,  1.26it/s]Extractor Predicting: 6it [00:04,  1.24it/s]Extractor Predicting: 7it [00:05,  1.22it/s]Extractor Predicting: 8it [00:06,  1.25it/s]Extractor Predicting: 9it [00:07,  1.16it/s]Extractor Predicting: 9it [00:07,  1.20it/s]
[INFO|configuration_utils.py:515] 2023-08-29 00:48:56,656 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:48:56,658 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:48:56,743 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:48:56,744 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 00:48:56,770 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:49:08,619 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 00:49:08,660 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 00:49:08,893 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:49:08,894 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:49:08,982 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:49:09,052 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:49:09,052 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:49:09,052 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:49:09,052 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:49:09,052 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:49:09,052 >> loading file outputs/wrapper/fewrel/unseen_15_seed_1/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.15517241379310345,
  "recall": 0.022222222222222223,
  "score": 0.038876889848812095,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 00:49:09,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:10,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:10,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:11,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:12,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:13,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:13,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:14,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:15,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:16,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:17,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:18,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:18,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:19,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:20,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:21,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:21,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:22,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:23,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:23,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:24,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:25,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:26,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:26,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:18<05:45, 18.20s/it][WARNING|generation_utils.py:914] 2023-08-29 00:49:27,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:28,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:29,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:30,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:30,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:31,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:32,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:33,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:34,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:34,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:35,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:36,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:37,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:37,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:38,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:39,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:40,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:40,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:41,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:42,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:42,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:43,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:44,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:35<05:19, 17.73s/it][WARNING|generation_utils.py:914] 2023-08-29 00:49:45,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:45,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:46,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:47,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:48,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:48,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:49,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:50,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:50,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:51,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:52,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:53,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:53,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:54,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:55,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:56,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:56,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:57,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:58,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:59,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:00,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:00,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:52<04:51, 17.16s/it][WARNING|generation_utils.py:914] 2023-08-29 00:50:01,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:02,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:02,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:03,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:04,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:05,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:05,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:06,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:07,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:08,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:09,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:09,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:10,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:11,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:12,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:12,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:13,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:14,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:15,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:15,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:16,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:17,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:08<04:31, 16.97s/it][WARNING|generation_utils.py:914] 2023-08-29 00:50:18,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:18,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:19,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:20,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:21,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:21,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:22,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:23,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:24,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:25,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:25,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:26,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:27,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:28,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:28,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:29,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:30,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:31,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:32,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:32,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:33,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:34,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:35,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:26<04:18, 17.23s/it][WARNING|generation_utils.py:914] 2023-08-29 00:50:35,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:36,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:37,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:38,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:39,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:39,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:40,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:41,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:41,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:42,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:43,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:43,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:44,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:45,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:46,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:46,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:47,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:48,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:49,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:50,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:50,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:42<03:53, 16.69s/it][WARNING|generation_utils.py:914] 2023-08-29 00:50:51,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:52,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:52,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:53,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:54,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:55,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:56,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:56,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:57,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:58,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:59,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:59,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:00,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:01,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:02,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:02,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:03,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:04,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:05,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:05,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:06,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:58<03:33, 16.44s/it][WARNING|generation_utils.py:914] 2023-08-29 00:51:07,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:08,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:09,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:09,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:11,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:11,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:12,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:13,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:14,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:15,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:15,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:16,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:17,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:18,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:19,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:19,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:20,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:21,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:22,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:23,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:24,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:25,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:25,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:26,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:27,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:18<03:34, 17.83s/it][WARNING|generation_utils.py:914] 2023-08-29 00:51:28,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:29,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:29,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:30,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:31,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:32,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:32,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:33,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:34,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:35,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:36,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:36,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:37,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:38,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:39,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:40,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:40,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:41,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:42,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:43,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:43,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:35<03:12, 17.54s/it][WARNING|generation_utils.py:914] 2023-08-29 00:51:45,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:45,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:46,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:47,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:48,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:49,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:50,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:51,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:51,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:52,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:53,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:54,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:55,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:55,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:56,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:57,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:58,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:58,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:51:59,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:00,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:01,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:02,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:53<02:55, 17.60s/it][WARNING|generation_utils.py:914] 2023-08-29 00:52:02,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:03,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:04,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:05,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:05,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:06,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:07,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:07,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:08,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:09,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:10,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:11,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:11,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:12,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:13,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:14,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:14,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:15,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:16,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:17,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:18,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:09<02:33, 17.07s/it][WARNING|generation_utils.py:914] 2023-08-29 00:52:18,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:19,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:20,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:21,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:22,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:22,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:23,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:24,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:25,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:25,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:26,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:27,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:28,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:29,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:29,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:30,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:31,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:32,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:33,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:33,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:34,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:26<02:15, 16.97s/it][WARNING|generation_utils.py:914] 2023-08-29 00:52:35,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:36,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:37,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:38,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:38,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:39,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:40,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:41,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:42,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:42,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:43,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:44,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:45,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:45,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:46,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:47,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:48,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:49,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:49,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:50,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:51,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:52,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:43<01:59, 17.08s/it][WARNING|generation_utils.py:914] 2023-08-29 00:52:52,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:53,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:54,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:54,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:55,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:56,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:57,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:57,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:58,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:52:59,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:00,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:00,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:01,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:02,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:03,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:03,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:04,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:05,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:05,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:06,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:58<01:38, 16.35s/it][WARNING|generation_utils.py:914] 2023-08-29 00:53:07,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:08,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:09,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:10,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:10,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:11,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:12,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:12,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:13,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:14,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:15,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:15,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:16,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:17,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:18,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:18,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:19,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:20,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:21,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:22,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:22,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:23,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:24,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:16<01:24, 16.84s/it][WARNING|generation_utils.py:914] 2023-08-29 00:53:25,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:26,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:26,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:27,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:28,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:29,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:29,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:30,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:32,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:32,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:33,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:34,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:35,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:35,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:36,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:37,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:38,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:39,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:40,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:40,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:41,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:42,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:34<01:08, 17.19s/it][WARNING|generation_utils.py:914] 2023-08-29 00:53:43,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:44,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:44,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:45,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:46,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:46,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:47,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:48,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:48,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:49,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:50,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:51,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:52,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:53,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:53,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:54,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:55,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:56,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:56,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:57,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:58,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:53:58,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:50<00:50, 16.89s/it][WARNING|generation_utils.py:914] 2023-08-29 00:53:59,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:00,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:01,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:01,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:02,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:03,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:03,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:04,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:05,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:06,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:07,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:07,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:08,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:09,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:09,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:10,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:11,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:12,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:13,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:13,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:14,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:15,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:16,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:16,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [05:07<00:34, 17.15s/it][WARNING|generation_utils.py:914] 2023-08-29 00:54:17,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:18,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:18,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:19,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:20,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:21,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:22,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:22,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:23,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:24,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:25,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:25,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:26,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:27,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:28,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:28,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:29,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:30,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:31,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:31,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:32,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:24<00:16, 16.82s/it][WARNING|generation_utils.py:914] 2023-08-29 00:54:33,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:34,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:35,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:36,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:37,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:38,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:39,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:39,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:40,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:41,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:42,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:43,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:43,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:44,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:45,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:45,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:46,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:48,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:48,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:49,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:54:50,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:41<00:00, 17.13s/it]Generating: 100%|██████████| 20/20 [05:41<00:00, 17.09s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:54:59,940 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:54:59,961 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:54:59,961 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:54:59,961 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:54:59,961 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:55:00,862 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:55:00,863 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:55:01,169 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:55:02,348 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:55:02,348 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:04,371 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:04,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:04,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:04,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:04,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:55:04,908 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:55:04,910 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:55:05,240 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:55:05,499 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:55:05,499 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 378, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 481, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 531, 'raw': 672}
{'target': 600, 'success': 552, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7838541666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : genre .', 'success_rate': 0.8233695652173914, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : head of government .', 'success_rate': 0.8607954545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : military branch .', 'success_rate': 0.8792613636363636, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : winner .', 'success_rate': 0.845108695652174, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.9330357142857143, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.9077380952380952, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 527, 'raw': 672}
{'target': 600, 'success': 553, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 623, 'raw': 800}
{'prompt': 'Relation : crosses .', 'success_rate': 0.77875, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.9226190476190477, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.875, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9270833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9226190476190477, 'errors': {''}}
['Relation : occupation . Context : Later in the year , the school was renamed the " Alumni School " . Head Entity : Alumni School , Tail Entity : alumni .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8607954545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9484375, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : participant .', 'success_rate': 0.8315217391304348, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8664772727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : platform .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 509, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 617, 'raw': 768}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.8033854166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8958333333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : spouse .', 'success_rate': 0.9166666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/synthetic/1_ext.jsonl'}}
estimate vocab size: 13037
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13137, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.27it/s]Extractor Estimating: 2it [00:01,  1.33it/s]Extractor Estimating: 3it [00:02,  1.37it/s]Extractor Estimating: 4it [00:02,  1.37it/s]Extractor Estimating: 5it [00:03,  1.37it/s]Extractor Estimating: 6it [00:04,  1.44it/s]Extractor Estimating: 7it [00:04,  1.46it/s]Extractor Estimating: 8it [00:05,  1.45it/s]Extractor Estimating: 9it [00:06,  1.29it/s]Extractor Estimating: 10it [00:07,  1.31it/s]Extractor Estimating: 11it [00:08,  1.33it/s]Extractor Estimating: 12it [00:08,  1.33it/s]Extractor Estimating: 13it [00:09,  1.30it/s]Extractor Estimating: 14it [00:10,  1.33it/s]Extractor Estimating: 15it [00:11,  1.34it/s]Extractor Estimating: 16it [00:11,  1.34it/s]Extractor Estimating: 17it [00:12,  1.36it/s]Extractor Estimating: 18it [00:13,  1.39it/s]Extractor Estimating: 19it [00:13,  1.40it/s]Extractor Estimating: 20it [00:14,  1.38it/s]Extractor Estimating: 21it [00:15,  1.45it/s]Extractor Estimating: 22it [00:16,  1.43it/s]Extractor Estimating: 23it [00:16,  1.41it/s]Extractor Estimating: 24it [00:17,  1.42it/s]Extractor Estimating: 25it [00:18,  1.34it/s]Extractor Estimating: 26it [00:19,  1.28it/s]Extractor Estimating: 27it [00:20,  1.22it/s]Extractor Estimating: 28it [00:20,  1.24it/s]Extractor Estimating: 29it [00:21,  1.18it/s]Extractor Estimating: 30it [00:22,  1.20it/s]Extractor Estimating: 31it [00:23,  1.15it/s]Extractor Estimating: 32it [00:24,  1.19it/s]Extractor Estimating: 33it [00:25,  1.22it/s]Extractor Estimating: 34it [00:25,  1.23it/s]Extractor Estimating: 35it [00:26,  1.25it/s]Extractor Estimating: 36it [00:27,  1.22it/s]Extractor Estimating: 37it [00:28,  1.22it/s]Extractor Estimating: 38it [00:29,  1.22it/s]Extractor Estimating: 39it [00:29,  1.24it/s]Extractor Estimating: 40it [00:30,  1.24it/s]Extractor Estimating: 41it [00:31,  1.24it/s]Extractor Estimating: 42it [00:32,  1.22it/s]Extractor Estimating: 43it [00:33,  1.24it/s]Extractor Estimating: 44it [00:33,  1.27it/s]Extractor Estimating: 45it [00:34,  1.22it/s]Extractor Estimating: 46it [00:35,  1.22it/s]Extractor Estimating: 47it [00:36,  1.17it/s]Extractor Estimating: 48it [00:37,  1.18it/s]Extractor Estimating: 49it [00:38,  1.16it/s]Extractor Estimating: 50it [00:39,  1.22it/s]Extractor Estimating: 51it [00:39,  1.24it/s]Extractor Estimating: 52it [00:40,  1.28it/s]Extractor Estimating: 53it [00:41,  1.30it/s]Extractor Estimating: 54it [00:41,  1.34it/s]Extractor Estimating: 55it [00:42,  1.37it/s]Extractor Estimating: 56it [00:43,  1.39it/s]Extractor Estimating: 57it [00:44,  1.40it/s]Extractor Estimating: 58it [00:44,  1.45it/s]Extractor Estimating: 59it [00:45,  1.44it/s]Extractor Estimating: 60it [00:46,  1.43it/s]Extractor Estimating: 61it [00:46,  1.44it/s]Extractor Estimating: 62it [00:47,  1.47it/s]Extractor Estimating: 63it [00:48,  1.48it/s]Extractor Estimating: 64it [00:48,  1.40it/s]Extractor Estimating: 65it [00:49,  1.41it/s]Extractor Estimating: 66it [00:50,  1.39it/s]Extractor Estimating: 67it [00:51,  1.41it/s]Extractor Estimating: 68it [00:51,  1.39it/s]Extractor Estimating: 69it [00:52,  1.30it/s]Extractor Estimating: 70it [00:53,  1.30it/s]Extractor Estimating: 71it [00:54,  1.38it/s]Extractor Estimating: 72it [00:54,  1.40it/s]Extractor Estimating: 73it [00:55,  1.41it/s]Extractor Estimating: 74it [00:56,  1.40it/s]Extractor Estimating: 75it [00:56,  1.42it/s]Extractor Estimating: 76it [00:57,  1.37it/s]Extractor Estimating: 77it [00:58,  1.31it/s]Extractor Estimating: 78it [00:59,  1.28it/s]Extractor Estimating: 79it [01:00,  1.30it/s]Extractor Estimating: 80it [01:00,  1.29it/s]Extractor Estimating: 81it [01:01,  1.28it/s]Extractor Estimating: 82it [01:02,  1.26it/s]Extractor Estimating: 83it [01:03,  1.24it/s]Extractor Estimating: 84it [01:04,  1.26it/s]Extractor Estimating: 85it [01:04,  1.29it/s]Extractor Estimating: 86it [01:05,  1.28it/s]Extractor Estimating: 87it [01:06,  1.17it/s]Extractor Estimating: 88it [01:07,  1.18it/s]Extractor Estimating: 89it [01:08,  1.15it/s]Extractor Estimating: 90it [01:09,  1.17it/s]Extractor Estimating: 91it [01:09,  1.21it/s]Extractor Estimating: 92it [01:10,  1.23it/s]Extractor Estimating: 93it [01:11,  1.26it/s]Extractor Estimating: 94it [01:12,  1.27it/s]Extractor Estimating: 95it [01:12,  1.27it/s]Extractor Estimating: 96it [01:13,  1.28it/s]Extractor Estimating: 97it [01:14,  1.31it/s]Extractor Estimating: 98it [01:15,  1.29it/s]Extractor Estimating: 99it [01:16,  1.26it/s]Extractor Estimating: 100it [01:16,  1.27it/s]Extractor Estimating: 101it [01:17,  1.30it/s]Extractor Estimating: 102it [01:18,  1.33it/s]Extractor Estimating: 103it [01:19,  1.31it/s]Extractor Estimating: 104it [01:19,  1.34it/s]Extractor Estimating: 105it [01:20,  1.36it/s]Extractor Estimating: 106it [01:21,  1.33it/s]Extractor Estimating: 107it [01:22,  1.32it/s]Extractor Estimating: 108it [01:22,  1.30it/s]Extractor Estimating: 109it [01:23,  1.32it/s]Extractor Estimating: 110it [01:24,  1.31it/s]Extractor Estimating: 111it [01:25,  1.30it/s]Extractor Estimating: 112it [01:25,  1.30it/s]Extractor Estimating: 113it [01:26,  1.27it/s]Extractor Estimating: 114it [01:27,  1.32it/s]Extractor Estimating: 115it [01:28,  1.31it/s]Extractor Estimating: 116it [01:29,  1.31it/s]Extractor Estimating: 117it [01:29,  1.31it/s]Extractor Estimating: 118it [01:30,  1.31it/s]Extractor Estimating: 119it [01:31,  1.30it/s]Extractor Estimating: 120it [01:32,  1.32it/s]Extractor Estimating: 121it [01:32,  1.29it/s]Extractor Estimating: 122it [01:33,  1.31it/s]Extractor Estimating: 123it [01:34,  1.34it/s]Extractor Estimating: 124it [01:35,  1.32it/s]Extractor Estimating: 125it [01:35,  1.28it/s]Extractor Estimating: 126it [01:36,  1.24it/s]Extractor Estimating: 127it [01:37,  1.21it/s]Extractor Estimating: 128it [01:38,  1.22it/s]Extractor Estimating: 129it [01:39,  1.24it/s]Extractor Estimating: 130it [01:40,  1.25it/s]Extractor Estimating: 131it [01:40,  1.23it/s]Extractor Estimating: 132it [01:41,  1.24it/s]Extractor Estimating: 133it [01:42,  1.22it/s]Extractor Estimating: 134it [01:43,  1.24it/s]Extractor Estimating: 135it [01:44,  1.26it/s]Extractor Estimating: 136it [01:44,  1.26it/s]Extractor Estimating: 137it [01:45,  1.27it/s]Extractor Estimating: 138it [01:46,  1.30it/s]Extractor Estimating: 139it [01:47,  1.19it/s]Extractor Estimating: 140it [01:48,  1.23it/s]Extractor Estimating: 141it [01:48,  1.24it/s]Extractor Estimating: 142it [01:49,  1.23it/s]Extractor Estimating: 143it [01:50,  1.25it/s]Extractor Estimating: 144it [01:51,  1.28it/s]Extractor Estimating: 145it [01:52,  1.27it/s]Extractor Estimating: 146it [01:52,  1.31it/s]Extractor Estimating: 147it [01:53,  1.23it/s]Extractor Estimating: 148it [01:54,  1.22it/s]Extractor Estimating: 149it [01:55,  1.23it/s]Extractor Estimating: 150it [01:56,  1.25it/s]Extractor Estimating: 151it [01:56,  1.28it/s]Extractor Estimating: 152it [01:57,  1.33it/s]Extractor Estimating: 153it [01:58,  1.37it/s]Extractor Estimating: 154it [01:58,  1.40it/s]Extractor Estimating: 155it [01:59,  1.37it/s]Extractor Estimating: 156it [02:00,  1.34it/s]Extractor Estimating: 157it [02:01,  1.36it/s]Extractor Estimating: 158it [02:01,  1.39it/s]Extractor Estimating: 159it [02:02,  1.36it/s]Extractor Estimating: 160it [02:03,  1.40it/s]Extractor Estimating: 161it [02:03,  1.38it/s]Extractor Estimating: 162it [02:04,  1.44it/s]Extractor Estimating: 163it [02:05,  1.46it/s]Extractor Estimating: 164it [02:05,  1.44it/s]Extractor Estimating: 165it [02:06,  1.45it/s]Extractor Estimating: 166it [02:07,  1.40it/s]Extractor Estimating: 167it [02:08,  1.36it/s]Extractor Estimating: 168it [02:08,  1.39it/s]Extractor Estimating: 169it [02:09,  1.40it/s]Extractor Estimating: 170it [02:10,  1.39it/s]Extractor Estimating: 171it [02:11,  1.36it/s]Extractor Estimating: 172it [02:11,  1.38it/s]Extractor Estimating: 173it [02:12,  1.38it/s]Extractor Estimating: 174it [02:13,  1.41it/s]Extractor Estimating: 175it [02:13,  1.38it/s]Extractor Estimating: 176it [02:14,  1.31it/s]Extractor Estimating: 177it [02:15,  1.29it/s]Extractor Estimating: 178it [02:16,  1.29it/s]Extractor Estimating: 179it [02:17,  1.22it/s]Extractor Estimating: 180it [02:18,  1.25it/s]Extractor Estimating: 181it [02:18,  1.25it/s]Extractor Estimating: 182it [02:19,  1.24it/s]Extractor Estimating: 183it [02:20,  1.24it/s]Extractor Estimating: 184it [02:21,  1.22it/s]Extractor Estimating: 185it [02:22,  1.20it/s]Extractor Estimating: 186it [02:23,  1.21it/s]Extractor Estimating: 187it [02:23,  1.21it/s]Extractor Estimating: 188it [02:24,  1.21it/s]Extractor Estimating: 189it [02:25,  1.26it/s]Extractor Estimating: 190it [02:26,  1.26it/s]Extractor Estimating: 191it [02:27,  1.21it/s]Extractor Estimating: 192it [02:27,  1.20it/s]Extractor Estimating: 193it [02:28,  1.24it/s]Extractor Estimating: 194it [02:29,  1.23it/s]Extractor Estimating: 195it [02:30,  1.20it/s]Extractor Estimating: 196it [02:31,  1.21it/s]Extractor Estimating: 197it [02:32,  1.20it/s]Extractor Estimating: 198it [02:32,  1.20it/s]Extractor Estimating: 199it [02:33,  1.20it/s]Extractor Estimating: 200it [02:34,  1.23it/s]Extractor Estimating: 201it [02:35,  1.26it/s]Extractor Estimating: 202it [02:36,  1.24it/s]Extractor Estimating: 203it [02:36,  1.19it/s]Extractor Estimating: 204it [02:37,  1.21it/s]Extractor Estimating: 205it [02:38,  1.17it/s]Extractor Estimating: 206it [02:39,  1.21it/s]Extractor Estimating: 207it [02:40,  1.23it/s]Extractor Estimating: 208it [02:41,  1.24it/s]Extractor Estimating: 209it [02:41,  1.25it/s]Extractor Estimating: 210it [02:42,  1.26it/s]Extractor Estimating: 211it [02:43,  1.21it/s]Extractor Estimating: 212it [02:44,  1.12it/s]Extractor Estimating: 213it [02:45,  1.14it/s]Extractor Estimating: 214it [02:46,  1.20it/s]Extractor Estimating: 215it [02:46,  1.19it/s]Extractor Estimating: 216it [02:47,  1.18it/s]Extractor Estimating: 217it [02:48,  1.16it/s]Extractor Estimating: 218it [02:49,  1.20it/s]Extractor Estimating: 219it [02:50,  1.25it/s]Extractor Estimating: 220it [02:51,  1.25it/s]Extractor Estimating: 221it [02:51,  1.20it/s]Extractor Estimating: 222it [02:52,  1.18it/s]Extractor Estimating: 223it [02:53,  1.22it/s]Extractor Estimating: 224it [02:54,  1.22it/s]Extractor Estimating: 225it [02:55,  1.19it/s]Extractor Estimating: 226it [02:56,  1.22it/s]Extractor Estimating: 227it [02:56,  1.26it/s]Extractor Estimating: 228it [02:57,  1.22it/s]Extractor Estimating: 229it [02:58,  1.22it/s]Extractor Estimating: 230it [02:59,  1.26it/s]Extractor Estimating: 231it [03:00,  1.23it/s]Extractor Estimating: 232it [03:00,  1.27it/s]Extractor Estimating: 233it [03:01,  1.24it/s]Extractor Estimating: 234it [03:02,  1.29it/s]Extractor Estimating: 235it [03:03,  1.28it/s]Extractor Estimating: 236it [03:04,  1.22it/s]Extractor Estimating: 237it [03:04,  1.22it/s]Extractor Estimating: 238it [03:05,  1.25it/s]Extractor Estimating: 239it [03:06,  1.27it/s]Extractor Estimating: 240it [03:07,  1.25it/s]Extractor Estimating: 241it [03:08,  1.25it/s]Extractor Estimating: 242it [03:08,  1.27it/s]Extractor Estimating: 243it [03:09,  1.32it/s]Extractor Estimating: 244it [03:10,  1.35it/s]Extractor Estimating: 245it [03:11,  1.29it/s]Extractor Estimating: 246it [03:11,  1.28it/s]Extractor Estimating: 247it [03:12,  1.29it/s]Extractor Estimating: 248it [03:13,  1.31it/s]Extractor Estimating: 249it [03:14,  1.31it/s]Extractor Estimating: 250it [03:14,  1.26it/s]Extractor Estimating: 251it [03:15,  1.22it/s]Extractor Estimating: 252it [03:16,  1.21it/s]Extractor Estimating: 253it [03:17,  1.21it/s]Extractor Estimating: 254it [03:18,  1.22it/s]Extractor Estimating: 255it [03:19,  1.21it/s]Extractor Estimating: 256it [03:19,  1.26it/s]Extractor Estimating: 257it [03:20,  1.27it/s]Extractor Estimating: 258it [03:21,  1.26it/s]Extractor Estimating: 259it [03:22,  1.24it/s]Extractor Estimating: 260it [03:23,  1.25it/s]Extractor Estimating: 261it [03:23,  1.23it/s]Extractor Estimating: 262it [03:24,  1.19it/s]Extractor Estimating: 263it [03:25,  1.20it/s]Extractor Estimating: 264it [03:26,  1.19it/s]Extractor Estimating: 265it [03:27,  1.21it/s]Extractor Estimating: 266it [03:28,  1.20it/s]Extractor Estimating: 267it [03:28,  1.23it/s]Extractor Estimating: 268it [03:29,  1.23it/s]Extractor Estimating: 269it [03:30,  1.25it/s]Extractor Estimating: 270it [03:31,  1.25it/s]Extractor Estimating: 271it [03:32,  1.18it/s]Extractor Estimating: 272it [03:33,  1.19it/s]Extractor Estimating: 273it [03:33,  1.21it/s]Extractor Estimating: 274it [03:34,  1.20it/s]Extractor Estimating: 275it [03:35,  1.19it/s]Extractor Estimating: 276it [03:36,  1.23it/s]Extractor Estimating: 277it [03:37,  1.25it/s]Extractor Estimating: 278it [03:37,  1.26it/s]Extractor Estimating: 279it [03:38,  1.27it/s]Extractor Estimating: 280it [03:39,  1.27it/s]Extractor Estimating: 281it [03:40,  1.26it/s]Extractor Estimating: 282it [03:40,  1.30it/s]Extractor Estimating: 283it [03:41,  1.29it/s]Extractor Estimating: 284it [03:42,  1.29it/s]Extractor Estimating: 285it [03:43,  1.29it/s]Extractor Estimating: 286it [03:43,  1.31it/s]Extractor Estimating: 287it [03:44,  1.29it/s]Extractor Estimating: 288it [03:45,  1.29it/s]Extractor Estimating: 289it [03:46,  1.19it/s]Extractor Estimating: 290it [03:47,  1.24it/s]Extractor Estimating: 291it [03:48,  1.26it/s]Extractor Estimating: 292it [03:48,  1.31it/s]Extractor Estimating: 293it [03:49,  1.29it/s]Extractor Estimating: 294it [03:50,  1.29it/s]Extractor Estimating: 295it [03:51,  1.30it/s]Extractor Estimating: 296it [03:51,  1.27it/s]Extractor Estimating: 297it [03:52,  1.26it/s]Extractor Estimating: 298it [03:53,  1.26it/s]Extractor Estimating: 299it [03:54,  1.28it/s]Extractor Estimating: 300it [03:55,  1.28it/s]Extractor Estimating: 301it [03:55,  1.22it/s]Extractor Estimating: 302it [03:56,  1.22it/s]Extractor Estimating: 303it [03:57,  1.23it/s]Extractor Estimating: 304it [03:58,  1.23it/s]Extractor Estimating: 305it [03:59,  1.21it/s]Extractor Estimating: 306it [04:00,  1.23it/s]Extractor Estimating: 307it [04:00,  1.24it/s]Extractor Estimating: 308it [04:01,  1.23it/s]Extractor Estimating: 309it [04:02,  1.25it/s]Extractor Estimating: 310it [04:03,  1.24it/s]Extractor Estimating: 311it [04:04,  1.21it/s]Extractor Estimating: 312it [04:04,  1.21it/s]Extractor Estimating: 313it [04:05,  1.26it/s]Extractor Estimating: 314it [04:06,  1.29it/s]Extractor Estimating: 315it [04:07,  1.26it/s]Extractor Estimating: 316it [04:08,  1.25it/s]Extractor Estimating: 317it [04:08,  1.24it/s]Extractor Estimating: 318it [04:09,  1.27it/s]Extractor Estimating: 319it [04:10,  1.27it/s]Extractor Estimating: 320it [04:11,  1.24it/s]Extractor Estimating: 321it [04:12,  1.24it/s]Extractor Estimating: 322it [04:12,  1.26it/s]Extractor Estimating: 323it [04:13,  1.28it/s]Extractor Estimating: 324it [04:14,  1.27it/s]Extractor Estimating: 325it [04:15,  1.30it/s]Extractor Estimating: 326it [04:15,  1.33it/s]Extractor Estimating: 327it [04:16,  1.33it/s]Extractor Estimating: 328it [04:17,  1.35it/s]Extractor Estimating: 329it [04:17,  1.36it/s]Extractor Estimating: 330it [04:18,  1.35it/s]Extractor Estimating: 331it [04:19,  1.40it/s]Extractor Estimating: 332it [04:20,  1.36it/s]Extractor Estimating: 333it [04:20,  1.39it/s]Extractor Estimating: 334it [04:21,  1.41it/s]Extractor Estimating: 335it [04:22,  1.41it/s]Extractor Estimating: 336it [04:23,  1.37it/s]Extractor Estimating: 337it [04:23,  1.33it/s]Extractor Estimating: 338it [04:24,  1.36it/s]Extractor Estimating: 339it [04:25,  1.37it/s]Extractor Estimating: 340it [04:26,  1.37it/s]Extractor Estimating: 341it [04:26,  1.40it/s]Extractor Estimating: 342it [04:27,  1.29it/s]Extractor Estimating: 343it [04:28,  1.36it/s]Extractor Estimating: 344it [04:28,  1.41it/s]Extractor Estimating: 345it [04:29,  1.43it/s]Extractor Estimating: 346it [04:30,  1.45it/s]Extractor Estimating: 347it [04:30,  1.40it/s]Extractor Estimating: 348it [04:31,  1.39it/s]Extractor Estimating: 349it [04:32,  1.37it/s]Extractor Estimating: 350it [04:33,  1.32it/s]Extractor Estimating: 351it [04:34,  1.29it/s]Extractor Estimating: 352it [04:34,  1.26it/s]Extractor Estimating: 353it [04:35,  1.25it/s]Extractor Estimating: 354it [04:36,  1.29it/s]Extractor Estimating: 355it [04:37,  1.25it/s]Extractor Estimating: 356it [04:38,  1.28it/s]Extractor Estimating: 357it [04:38,  1.29it/s]Extractor Estimating: 358it [04:39,  1.31it/s]Extractor Estimating: 359it [04:40,  1.36it/s]Extractor Estimating: 360it [04:41,  1.35it/s]Extractor Estimating: 361it [04:41,  1.39it/s]Extractor Estimating: 362it [04:42,  1.38it/s]Extractor Estimating: 363it [04:43,  1.37it/s]Extractor Estimating: 364it [04:44,  1.30it/s]Extractor Estimating: 365it [04:44,  1.32it/s]Extractor Estimating: 366it [04:45,  1.34it/s]Extractor Estimating: 367it [04:46,  1.40it/s]Extractor Estimating: 368it [04:46,  1.36it/s]Extractor Estimating: 369it [04:47,  1.34it/s]Extractor Estimating: 370it [04:48,  1.34it/s]Extractor Estimating: 371it [04:49,  1.28it/s]Extractor Estimating: 372it [04:49,  1.32it/s]Extractor Estimating: 373it [04:50,  1.26it/s]Extractor Estimating: 374it [04:51,  1.30it/s]Extractor Estimating: 375it [04:52,  1.34it/s]Extractor Estimating: 376it [04:52,  1.36it/s]Extractor Estimating: 377it [04:53,  1.32it/s]Extractor Estimating: 378it [04:54,  1.30it/s]Extractor Estimating: 379it [04:55,  1.33it/s]Extractor Estimating: 380it [04:56,  1.31it/s]Extractor Estimating: 381it [04:57,  1.23it/s]Extractor Estimating: 382it [04:57,  1.23it/s]Extractor Estimating: 383it [04:58,  1.25it/s]Extractor Estimating: 384it [04:59,  1.26it/s]Extractor Estimating: 385it [05:00,  1.27it/s]Extractor Estimating: 386it [05:00,  1.29it/s]Extractor Estimating: 387it [05:01,  1.28it/s]Extractor Estimating: 388it [05:02,  1.29it/s]Extractor Estimating: 389it [05:03,  1.33it/s]Extractor Estimating: 390it [05:03,  1.34it/s]Extractor Estimating: 391it [05:04,  1.35it/s]Extractor Estimating: 392it [05:05,  1.32it/s]Extractor Estimating: 393it [05:06,  1.33it/s]Extractor Estimating: 394it [05:06,  1.31it/s]Extractor Estimating: 395it [05:07,  1.25it/s]Extractor Estimating: 396it [05:08,  1.28it/s]Extractor Estimating: 397it [05:09,  1.30it/s]Extractor Estimating: 398it [05:10,  1.26it/s]Extractor Estimating: 399it [05:10,  1.25it/s]Extractor Estimating: 400it [05:11,  1.25it/s]Extractor Estimating: 401it [05:12,  1.31it/s]Extractor Estimating: 402it [05:13,  1.30it/s]Extractor Estimating: 403it [05:14,  1.24it/s]Extractor Estimating: 404it [05:14,  1.28it/s]Extractor Estimating: 405it [05:15,  1.30it/s]Extractor Estimating: 406it [05:16,  1.34it/s]Extractor Estimating: 407it [05:17,  1.29it/s]Extractor Estimating: 408it [05:17,  1.35it/s]Extractor Estimating: 409it [05:18,  1.39it/s]Extractor Estimating: 410it [05:19,  1.38it/s]Extractor Estimating: 411it [05:19,  1.37it/s]Extractor Estimating: 412it [05:20,  1.22it/s]Extractor Estimating: 413it [05:21,  1.23it/s]Extractor Estimating: 414it [05:22,  1.23it/s]Extractor Estimating: 415it [05:23,  1.25it/s]Extractor Estimating: 416it [05:24,  1.25it/s]Extractor Estimating: 417it [05:24,  1.22it/s]Extractor Estimating: 418it [05:25,  1.26it/s]Extractor Estimating: 419it [05:26,  1.27it/s]Extractor Estimating: 420it [05:27,  1.33it/s]Extractor Estimating: 421it [05:27,  1.33it/s]Extractor Estimating: 422it [05:28,  1.30it/s]Extractor Estimating: 423it [05:29,  1.28it/s]Extractor Estimating: 424it [05:30,  1.31it/s]Extractor Estimating: 425it [05:31,  1.27it/s]Extractor Estimating: 426it [05:31,  1.30it/s]Extractor Estimating: 427it [05:32,  1.32it/s]Extractor Estimating: 428it [05:33,  1.34it/s]Extractor Estimating: 429it [05:34,  1.35it/s]Extractor Estimating: 430it [05:34,  1.38it/s]Extractor Estimating: 431it [05:35,  1.40it/s]Extractor Estimating: 432it [05:36,  1.43it/s]Extractor Estimating: 433it [05:36,  1.40it/s]Extractor Estimating: 434it [05:37,  1.38it/s]Extractor Estimating: 435it [05:38,  1.36it/s]Extractor Estimating: 436it [05:39,  1.35it/s]Extractor Estimating: 437it [05:39,  1.36it/s]Extractor Estimating: 438it [05:40,  1.35it/s]Extractor Estimating: 439it [05:41,  1.35it/s]Extractor Estimating: 440it [05:41,  1.38it/s]Extractor Estimating: 441it [05:42,  1.36it/s]Extractor Estimating: 442it [05:43,  1.34it/s]Extractor Estimating: 443it [05:44,  1.29it/s]Extractor Estimating: 444it [05:45,  1.31it/s]Extractor Estimating: 445it [05:45,  1.33it/s]Extractor Estimating: 446it [05:46,  1.35it/s]Extractor Estimating: 447it [05:47,  1.37it/s]Extractor Estimating: 448it [05:47,  1.38it/s]Extractor Estimating: 449it [05:48,  1.36it/s]Extractor Estimating: 450it [05:49,  1.36it/s]Extractor Estimating: 451it [05:50,  1.35it/s]Extractor Estimating: 452it [05:50,  1.33it/s]Extractor Estimating: 453it [05:51,  1.31it/s]Extractor Estimating: 454it [05:52,  1.27it/s]Extractor Estimating: 455it [05:53,  1.25it/s]Extractor Estimating: 456it [05:54,  1.25it/s]Extractor Estimating: 457it [05:55,  1.18it/s]Extractor Estimating: 458it [05:56,  1.17it/s]Extractor Estimating: 459it [05:56,  1.19it/s]Extractor Estimating: 460it [05:57,  1.19it/s]Extractor Estimating: 461it [05:58,  1.22it/s]Extractor Estimating: 462it [05:59,  1.22it/s]Extractor Estimating: 463it [06:00,  1.22it/s]Extractor Estimating: 464it [06:01,  1.18it/s]Extractor Estimating: 465it [06:02,  1.10it/s]Extractor Estimating: 466it [06:02,  1.14it/s]Extractor Estimating: 467it [06:03,  1.19it/s]Extractor Estimating: 468it [06:04,  1.22it/s]Extractor Estimating: 469it [06:05,  1.24it/s]Extractor Estimating: 470it [06:06,  1.20it/s]Extractor Estimating: 471it [06:06,  1.22it/s]Extractor Estimating: 472it [06:07,  1.22it/s]Extractor Estimating: 473it [06:08,  1.25it/s]Extractor Estimating: 474it [06:09,  1.24it/s]Extractor Estimating: 475it [06:10,  1.26it/s]Extractor Estimating: 476it [06:10,  1.29it/s]Extractor Estimating: 477it [06:11,  1.29it/s]Extractor Estimating: 478it [06:12,  1.26it/s]Extractor Estimating: 479it [06:13,  1.29it/s]Extractor Estimating: 480it [06:13,  1.29it/s]Extractor Estimating: 481it [06:14,  1.34it/s]Extractor Estimating: 482it [06:15,  1.28it/s]Extractor Estimating: 483it [06:16,  1.28it/s]Extractor Estimating: 484it [06:16,  1.28it/s]Extractor Estimating: 485it [06:17,  1.30it/s]Extractor Estimating: 486it [06:18,  1.31it/s]Extractor Estimating: 487it [06:19,  1.28it/s]Extractor Estimating: 488it [06:20,  1.32it/s]Extractor Estimating: 489it [06:20,  1.34it/s]Extractor Estimating: 490it [06:21,  1.40it/s]Extractor Estimating: 491it [06:22,  1.40it/s]Extractor Estimating: 492it [06:22,  1.37it/s]Extractor Estimating: 493it [06:23,  1.32it/s]Extractor Estimating: 494it [06:24,  1.32it/s]Extractor Estimating: 495it [06:25,  1.33it/s]Extractor Estimating: 496it [06:25,  1.33it/s]Extractor Estimating: 497it [06:26,  1.33it/s]Extractor Estimating: 498it [06:27,  1.35it/s]Extractor Estimating: 499it [06:28,  1.33it/s]Extractor Estimating: 500it [06:29,  1.29it/s]Extractor Estimating: 500it [06:29,  1.29it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:01:54,309 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:01:54,363 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:01:54,364 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:01:54,364 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:01:54,364 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:01:55,279 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:01:55,280 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:01:55,919 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:01:57,076 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:01:57,077 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:02:00,081 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:02:00,083 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:02:00,083 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:02:00,083 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:02:00,083 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:02:00,868 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:02:00,869 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:02:01,598 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:02:01,834 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:02:01,834 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 02:26:13,553 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 02:26:13,814 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 4014 mean pseudo reward: 0.9685666672623772
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 15891
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15991, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15991, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.261, loss:390.4817
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 32, avg_time 1.283, loss:339.6747
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 132, avg_time 1.235, loss:347.8416
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 64, avg_time 1.231, loss:322.6581
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 164, avg_time 1.235, loss:321.9887
>> valid entity prec:0.5820, rec:0.5907, f1:0.5863
>> valid relation prec:0.2056, rec:0.1542, f1:0.1762
>> valid relation with NER prec:0.2056, rec:0.1542, f1:0.1762
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 96, avg_time 2.719, loss:299.3203
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 28, avg_time 1.226, loss:291.9022
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 128, avg_time 1.227, loss:282.0019
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 60, avg_time 1.245, loss:310.6588
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 160, avg_time 1.242, loss:298.6074
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6011, rec:0.6142, f1:0.6076
>> valid relation prec:0.2075, rec:0.1582, f1:0.1795
>> valid relation with NER prec:0.2075, rec:0.1582, f1:0.1795
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 92, avg_time 2.730, loss:277.1244
g_step 1200, step 24, avg_time 1.226, loss:275.7303
g_step 1300, step 124, avg_time 1.236, loss:261.4775
g_step 1400, step 56, avg_time 1.228, loss:235.2290
g_step 1500, step 156, avg_time 1.243, loss:248.2036
>> valid entity prec:0.5778, rec:0.6198, f1:0.5981
>> valid relation prec:0.2308, rec:0.1798, f1:0.2021
>> valid relation with NER prec:0.2308, rec:0.1798, f1:0.2021
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 88, avg_time 2.732, loss:221.9413
g_step 1700, step 20, avg_time 1.237, loss:234.1087
g_step 1800, step 120, avg_time 1.225, loss:212.3433
g_step 1900, step 52, avg_time 1.249, loss:203.2418
g_step 2000, step 152, avg_time 1.232, loss:204.1498
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6161, rec:0.5393, f1:0.5751
>> valid relation prec:0.2059, rec:0.1228, f1:0.1538
>> valid relation with NER prec:0.2059, rec:0.1228, f1:0.1538
g_step 2100, step 84, avg_time 2.706, loss:190.3948
g_step 2200, step 16, avg_time 1.245, loss:188.9864
g_step 2300, step 116, avg_time 1.233, loss:177.6033
g_step 2400, step 48, avg_time 1.221, loss:163.1427
g_step 2500, step 148, avg_time 1.250, loss:170.1065
>> valid entity prec:0.5532, rec:0.5980, f1:0.5747
>> valid relation prec:0.1750, rec:0.1208, f1:0.1429
>> valid relation with NER prec:0.1750, rec:0.1208, f1:0.1429
g_step 2600, step 80, avg_time 2.715, loss:163.0793
g_step 2700, step 12, avg_time 1.231, loss:171.7007
g_step 2800, step 112, avg_time 1.243, loss:148.6819
g_step 2900, step 44, avg_time 1.226, loss:154.6155
g_step 3000, step 144, avg_time 1.242, loss:155.8517
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5783, rec:0.5900, f1:0.5840
>> valid relation prec:0.1570, rec:0.1245, f1:0.1389
>> valid relation with NER prec:0.1570, rec:0.1245, f1:0.1389
g_step 3100, step 76, avg_time 2.709, loss:146.7782
g_step 3200, step 8, avg_time 1.242, loss:133.9027
g_step 3300, step 108, avg_time 1.245, loss:131.3387
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 02:26:13 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 02:26:13 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_02-26-13_ctolab14.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 02:26:14 - WARNING - datasets.builder -   Using custom data configuration default-7792d91e259633e3
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-7792d91e259633e3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 02:26:16,929 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:26:16,962 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 02:26:16,962 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:26:16,963 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 02:26:17,041 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:26:17,088 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:26:17,088 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:26:17,088 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:26:17,088 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:26:17,089 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:26:17,089 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 02:26:17,737 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 02:26:20,794 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 02:26:20,812 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-7792d91e259633e3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.91ba/s] 40%|████      | 2/5 [00:00<00:00,  3.85ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.56ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.03ba/s]100%|██████████| 5/5 [00:01<00:00,  4.73ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.35ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.99ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.30ba/s]100%|██████████| 4/4 [00:00<00:00,  5.46ba/s]100%|██████████| 4/4 [00:00<00:00,  4.80ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  6.52ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.36ba/s]100%|██████████| 5/5 [00:00<00:00, 11.66ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  6.27ba/s] 50%|█████     | 2/4 [00:00<00:00,  7.61ba/s]100%|██████████| 4/4 [00:00<00:00, 10.93ba/s]100%|██████████| 4/4 [00:00<00:00,  9.83ba/s]
[INFO|trainer.py:414] 2023-08-29 02:26:24,516 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 02:26:24,580 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 02:26:24,581 >>   Num examples = 4014
[INFO|trainer.py:1149] 2023-08-29 02:26:24,581 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 02:26:24,581 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 02:26:24,581 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 02:26:24,581 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 02:26:24,581 >>   Total optimization steps = 315
  0%|          | 0/315 [00:00<?, ?it/s]  0%|          | 1/315 [00:00<01:34,  3.31it/s]  1%|          | 2/315 [00:00<01:32,  3.40it/s]  1%|          | 3/315 [00:00<01:30,  3.43it/s]  1%|▏         | 4/315 [00:01<01:30,  3.44it/s]  2%|▏         | 5/315 [00:01<01:29,  3.45it/s]  2%|▏         | 6/315 [00:01<01:29,  3.45it/s]  2%|▏         | 7/315 [00:02<01:29,  3.46it/s]  3%|▎         | 8/315 [00:02<01:28,  3.46it/s]  3%|▎         | 9/315 [00:02<01:28,  3.46it/s]  3%|▎         | 10/315 [00:02<01:31,  3.35it/s]  3%|▎         | 11/315 [00:03<01:30,  3.35it/s]  4%|▍         | 12/315 [00:03<01:30,  3.35it/s]  4%|▍         | 13/315 [00:03<01:29,  3.36it/s]  4%|▍         | 14/315 [00:04<01:29,  3.36it/s]  5%|▍         | 15/315 [00:04<01:29,  3.36it/s]  5%|▌         | 16/315 [00:04<01:28,  3.36it/s]  5%|▌         | 17/315 [00:05<01:28,  3.36it/s]  6%|▌         | 18/315 [00:05<01:28,  3.36it/s]  6%|▌         | 19/315 [00:05<01:28,  3.36it/s]  6%|▋         | 20/315 [00:05<01:27,  3.36it/s]  7%|▋         | 21/315 [00:06<01:32,  3.19it/s]  7%|▋         | 22/315 [00:06<01:30,  3.24it/s]  7%|▋         | 23/315 [00:06<01:29,  3.28it/s]  8%|▊         | 24/315 [00:07<01:28,  3.31it/s]  8%|▊         | 25/315 [00:07<01:27,  3.33it/s]  8%|▊         | 26/315 [00:07<01:26,  3.34it/s]  9%|▊         | 27/315 [00:08<01:25,  3.35it/s]  9%|▉         | 28/315 [00:08<01:25,  3.36it/s]  9%|▉         | 29/315 [00:08<01:25,  3.36it/s] 10%|▉         | 30/315 [00:08<01:24,  3.37it/s] 10%|▉         | 31/315 [00:09<01:27,  3.24it/s] 10%|█         | 32/315 [00:09<01:26,  3.28it/s] 10%|█         | 33/315 [00:09<01:25,  3.31it/s] 11%|█         | 34/315 [00:10<01:24,  3.33it/s] 11%|█         | 35/315 [00:10<01:23,  3.34it/s] 11%|█▏        | 36/315 [00:10<01:23,  3.35it/s] 12%|█▏        | 37/315 [00:11<01:22,  3.36it/s] 12%|█▏        | 38/315 [00:11<01:22,  3.37it/s] 12%|█▏        | 39/315 [00:11<01:21,  3.37it/s] 13%|█▎        | 40/315 [00:11<01:21,  3.37it/s] 13%|█▎        | 41/315 [00:12<01:26,  3.17it/s] 13%|█▎        | 42/315 [00:12<01:24,  3.23it/s] 14%|█▎        | 43/315 [00:12<01:23,  3.27it/s] 14%|█▍        | 44/315 [00:13<01:22,  3.30it/s] 14%|█▍        | 45/315 [00:13<01:21,  3.32it/s] 15%|█▍        | 46/315 [00:13<01:20,  3.34it/s] 15%|█▍        | 47/315 [00:14<01:20,  3.35it/s] 15%|█▌        | 48/315 [00:14<01:19,  3.36it/s] 16%|█▌        | 49/315 [00:14<01:19,  3.37it/s] 16%|█▌        | 50/315 [00:14<01:18,  3.37it/s] 16%|█▌        | 51/315 [00:15<01:22,  3.18it/s] 17%|█▋        | 52/315 [00:15<01:21,  3.24it/s] 17%|█▋        | 53/315 [00:15<01:19,  3.28it/s] 17%|█▋        | 54/315 [00:16<01:18,  3.31it/s] 17%|█▋        | 55/315 [00:16<01:18,  3.33it/s] 18%|█▊        | 56/315 [00:16<01:17,  3.34it/s] 18%|█▊        | 57/315 [00:17<01:16,  3.35it/s] 18%|█▊        | 58/315 [00:17<01:16,  3.36it/s] 19%|█▊        | 59/315 [00:17<01:16,  3.37it/s] 19%|█▉        | 60/315 [00:17<01:15,  3.37it/s] 19%|█▉        | 61/315 [00:18<01:17,  3.27it/s] 20%|█▉        | 62/315 [00:18<01:16,  3.30it/s] 20%|██        | 63/315 [00:18<01:10,  3.57it/s][INFO|trainer.py:2140] 2023-08-29 02:26:43,407 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:26:43,407 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 02:26:43,407 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 55.54it/s][A
  3%|▎         | 12/435 [00:00<00:08, 48.44it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.09it/s][A
  5%|▌         | 22/435 [00:00<00:08, 46.03it/s][A
  6%|▌         | 27/435 [00:00<00:08, 45.53it/s][A
  7%|▋         | 32/435 [00:00<00:08, 45.12it/s][A
  9%|▊         | 37/435 [00:00<00:08, 44.79it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.57it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.62it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.73it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.77it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.50it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.75it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.74it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 44.50it/s][A
 19%|█▉        | 82/435 [00:01<00:07, 44.34it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.46it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.46it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.51it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.45it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.57it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.72it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 44.70it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 44.54it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 44.50it/s][A
 30%|███       | 132/435 [00:02<00:07, 41.75it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 43.90it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 44.24it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 44.35it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.45it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.54it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 44.58it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 44.41it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.41it/s][A
 41%|████      | 177/435 [00:03<00:05, 43.65it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.06it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.36it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 44.46it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 44.60it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 44.55it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 44.44it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 44.44it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 44.29it/s][A
 51%|█████     | 222/435 [00:04<00:04, 44.33it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.46it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.50it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.66it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.71it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.72it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 44.63it/s][A
 59%|█████▉    | 257/435 [00:05<00:04, 44.47it/s][A
 60%|██████    | 262/435 [00:05<00:03, 44.38it/s][A
 61%|██████▏   | 267/435 [00:05<00:03, 44.39it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.44it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.51it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.59it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.63it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.75it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.62it/s][A
 69%|██████▉   | 302/435 [00:06<00:02, 44.47it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.36it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 43.13it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 43.63it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 43.96it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.18it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.31it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.43it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 44.40it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 44.40it/s][A
 81%|████████  | 352/435 [00:07<00:01, 44.14it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 44.32it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 44.45it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.57it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 44.59it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 44.64it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 44.64it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 44.47it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 44.33it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 44.23it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 44.41it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 44.52it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 44.61it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 44.78it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.69it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 44.60it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.48it/s][A
                                                 [A                                                
100%|██████████| 435/435 [00:09<00:00, 44.48it/s][A 20%|██        | 63/315 [00:28<01:10,  3.57it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:26:53,256 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-63
[INFO|configuration_utils.py:351] 2023-08-29 02:26:53,420 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-63/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:26:56,662 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-63/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:26:56,972 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-63/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:26:57,047 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-63/special_tokens_map.json
 20%|██        | 64/315 [00:39<26:20,  6.30s/it] 21%|██        | 65/315 [00:39<18:47,  4.51s/it] 21%|██        | 66/315 [00:39<13:28,  3.25s/it] 21%|██▏       | 67/315 [00:40<09:45,  2.36s/it] 22%|██▏       | 68/315 [00:40<07:10,  1.74s/it] 22%|██▏       | 69/315 [00:40<05:21,  1.31s/it] 22%|██▏       | 70/315 [00:40<04:06,  1.01s/it] 23%|██▎       | 71/315 [00:41<03:13,  1.26it/s] 23%|██▎       | 72/315 [00:41<02:36,  1.55it/s] 23%|██▎       | 73/315 [00:41<02:10,  1.85it/s] 23%|██▎       | 74/315 [00:42<01:52,  2.14it/s] 24%|██▍       | 75/315 [00:42<01:41,  2.36it/s] 24%|██▍       | 76/315 [00:42<01:32,  2.60it/s] 24%|██▍       | 77/315 [00:43<01:25,  2.79it/s] 25%|██▍       | 78/315 [00:43<01:20,  2.94it/s] 25%|██▌       | 79/315 [00:43<01:17,  3.06it/s] 25%|██▌       | 80/315 [00:43<01:14,  3.15it/s] 26%|██▌       | 81/315 [00:44<01:12,  3.22it/s] 26%|██▌       | 82/315 [00:44<01:11,  3.26it/s] 26%|██▋       | 83/315 [00:44<01:10,  3.30it/s] 27%|██▋       | 84/315 [00:45<01:09,  3.32it/s] 27%|██▋       | 85/315 [00:45<01:08,  3.33it/s] 27%|██▋       | 86/315 [00:45<01:10,  3.26it/s] 28%|██▊       | 87/315 [00:46<01:09,  3.30it/s] 28%|██▊       | 88/315 [00:46<01:08,  3.32it/s] 28%|██▊       | 89/315 [00:46<01:07,  3.34it/s] 29%|██▊       | 90/315 [00:46<01:07,  3.35it/s] 29%|██▉       | 91/315 [00:47<01:06,  3.36it/s] 29%|██▉       | 92/315 [00:47<01:06,  3.36it/s] 30%|██▉       | 93/315 [00:47<01:07,  3.31it/s] 30%|██▉       | 94/315 [00:48<01:06,  3.33it/s] 30%|███       | 95/315 [00:48<01:05,  3.34it/s] 30%|███       | 96/315 [00:48<01:06,  3.28it/s] 31%|███       | 97/315 [00:49<01:21,  2.67it/s] 31%|███       | 98/315 [00:49<01:16,  2.85it/s] 31%|███▏      | 99/315 [00:49<01:12,  2.99it/s] 32%|███▏      | 100/315 [00:50<01:09,  3.10it/s] 32%|███▏      | 101/315 [00:50<01:07,  3.18it/s] 32%|███▏      | 102/315 [00:50<01:05,  3.23it/s] 33%|███▎      | 103/315 [00:51<01:04,  3.28it/s] 33%|███▎      | 104/315 [00:51<01:03,  3.31it/s] 33%|███▎      | 105/315 [00:51<01:03,  3.32it/s] 34%|███▎      | 106/315 [00:51<01:02,  3.34it/s] 34%|███▍      | 107/315 [00:52<01:02,  3.35it/s] 34%|███▍      | 108/315 [00:52<01:01,  3.35it/s] 35%|███▍      | 109/315 [00:52<01:01,  3.36it/s] 35%|███▍      | 110/315 [00:53<01:00,  3.37it/s] 35%|███▌      | 111/315 [00:53<01:00,  3.37it/s] 36%|███▌      | 112/315 [00:53<01:01,  3.28it/s] 36%|███▌      | 113/315 [00:54<01:01,  3.31it/s] 36%|███▌      | 114/315 [00:54<01:00,  3.33it/s] 37%|███▋      | 115/315 [00:54<00:59,  3.34it/s] 37%|███▋      | 116/315 [00:54<00:59,  3.35it/s] 37%|███▋      | 117/315 [00:55<00:59,  3.35it/s] 37%|███▋      | 118/315 [00:55<00:58,  3.36it/s] 38%|███▊      | 119/315 [00:55<00:58,  3.36it/s] 38%|███▊      | 120/315 [00:56<00:58,  3.36it/s] 38%|███▊      | 121/315 [00:56<00:57,  3.36it/s] 39%|███▊      | 122/315 [00:56<01:00,  3.19it/s] 39%|███▉      | 123/315 [00:57<00:59,  3.24it/s] 39%|███▉      | 124/315 [00:57<00:58,  3.27it/s] 40%|███▉      | 125/315 [00:57<00:57,  3.30it/s] 40%|████      | 126/315 [00:57<00:52,  3.57it/s][INFO|trainer.py:2140] 2023-08-29 02:27:22,512 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:27:22,512 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 02:27:22,512 >>   Batch size = 8
{'eval_loss': 1.0246598720550537, 'eval_runtime': 9.7909, 'eval_samples_per_second': 355.127, 'eval_steps_per_second': 44.429, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 55.59it/s][A
  3%|▎         | 12/435 [00:00<00:08, 48.54it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.25it/s][A
  5%|▌         | 22/435 [00:00<00:08, 46.23it/s][A
  6%|▌         | 27/435 [00:00<00:08, 45.54it/s][A
  7%|▋         | 32/435 [00:00<00:08, 45.11it/s][A
  9%|▊         | 37/435 [00:00<00:08, 44.61it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.47it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.48it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.56it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.75it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.82it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.88it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.81it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 43.67it/s][A
 19%|█▉        | 82/435 [00:01<00:08, 43.76it/s][A
 20%|██        | 87/435 [00:01<00:07, 43.81it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.13it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.28it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.46it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.63it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.70it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 44.55it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 44.43it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 44.26it/s][A
 30%|███       | 132/435 [00:02<00:06, 44.32it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 44.34it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 44.52it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 44.63it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.70it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.63it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 44.58it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 44.43it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.43it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.38it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.46it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.53it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 44.64it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 44.63it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 44.54it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 44.60it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 43.00it/s][A
 50%|████▉     | 217/435 [00:04<00:05, 43.49it/s][A
 51%|█████     | 222/435 [00:04<00:04, 43.76it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 43.96it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.17it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.42it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.53it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.55it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 44.36it/s][A
 59%|█████▉    | 257/435 [00:05<00:04, 44.28it/s][A
 60%|██████    | 262/435 [00:05<00:03, 44.30it/s][A
 61%|██████▏   | 267/435 [00:05<00:03, 44.43it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.36it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.56it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.59it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.63it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.74it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.57it/s][A
 69%|██████▉   | 302/435 [00:06<00:02, 44.48it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.41it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.40it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.41it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.49it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.53it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.64it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.70it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 44.64it/s][A
 80%|███████▉  | 347/435 [00:07<00:02, 43.68it/s][A
 81%|████████  | 352/435 [00:07<00:01, 43.90it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 44.07it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 44.21it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.21it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 44.41it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 44.45it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 44.61it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 44.48it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 44.42it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 44.36it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 44.34it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 44.38it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 44.49it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 44.42it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.61it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 44.60it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.53it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 44.53it/s][A 40%|████      | 126/315 [01:07<00:52,  3.57it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:27:32,622 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-126
[INFO|configuration_utils.py:351] 2023-08-29 02:27:32,915 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-126/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:27:36,728 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-126/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:27:37,065 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-126/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:27:37,239 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-126/special_tokens_map.json
 40%|████      | 127/315 [01:21<23:06,  7.38s/it] 41%|████      | 128/315 [01:22<16:22,  5.25s/it] 41%|████      | 129/315 [01:22<11:40,  3.77s/it] 41%|████▏     | 130/315 [01:22<08:24,  2.73s/it] 42%|████▏     | 131/315 [01:23<06:07,  2.00s/it] 42%|████▏     | 132/315 [01:23<04:32,  1.49s/it] 42%|████▏     | 133/315 [01:23<03:25,  1.13s/it] 43%|████▎     | 134/315 [01:23<02:39,  1.14it/s] 43%|████▎     | 135/315 [01:24<02:06,  1.42it/s] 43%|████▎     | 136/315 [01:24<01:46,  1.67it/s] 43%|████▎     | 137/315 [01:24<01:30,  1.97it/s] 44%|████▍     | 138/315 [01:25<01:18,  2.25it/s] 44%|████▍     | 139/315 [01:25<01:10,  2.50it/s] 44%|████▍     | 140/315 [01:25<01:04,  2.71it/s] 45%|████▍     | 141/315 [01:26<01:00,  2.88it/s] 45%|████▌     | 142/315 [01:26<00:57,  3.01it/s] 45%|████▌     | 143/315 [01:26<00:55,  3.11it/s] 46%|████▌     | 144/315 [01:26<00:53,  3.19it/s] 46%|████▌     | 145/315 [01:27<00:52,  3.24it/s] 46%|████▋     | 146/315 [01:27<00:52,  3.23it/s] 47%|████▋     | 147/315 [01:27<00:51,  3.27it/s] 47%|████▋     | 148/315 [01:28<00:50,  3.30it/s] 47%|████▋     | 149/315 [01:28<00:50,  3.32it/s] 48%|████▊     | 150/315 [01:28<00:49,  3.33it/s] 48%|████▊     | 151/315 [01:29<00:49,  3.34it/s] 48%|████▊     | 152/315 [01:29<00:48,  3.36it/s] 49%|████▊     | 153/315 [01:29<00:47,  3.39it/s] 49%|████▉     | 154/315 [01:29<00:47,  3.40it/s] 49%|████▉     | 155/315 [01:30<00:46,  3.42it/s] 50%|████▉     | 156/315 [01:30<00:46,  3.44it/s] 50%|████▉     | 157/315 [01:30<00:47,  3.34it/s] 50%|█████     | 158/315 [01:31<00:46,  3.38it/s] 50%|█████     | 159/315 [01:31<00:45,  3.40it/s] 51%|█████     | 160/315 [01:31<00:45,  3.42it/s] 51%|█████     | 161/315 [01:31<00:44,  3.43it/s] 51%|█████▏    | 162/315 [01:32<00:44,  3.44it/s] 52%|█████▏    | 163/315 [01:32<00:44,  3.45it/s] 52%|█████▏    | 164/315 [01:32<00:43,  3.45it/s] 52%|█████▏    | 165/315 [01:33<00:43,  3.45it/s] 53%|█████▎    | 166/315 [01:33<00:43,  3.46it/s] 53%|█████▎    | 167/315 [01:33<00:42,  3.46it/s] 53%|█████▎    | 168/315 [01:34<00:43,  3.36it/s] 54%|█████▎    | 169/315 [01:34<00:43,  3.39it/s] 54%|█████▍    | 170/315 [01:34<00:42,  3.41it/s] 54%|█████▍    | 171/315 [01:34<00:42,  3.42it/s] 55%|█████▍    | 172/315 [01:35<00:41,  3.44it/s] 55%|█████▍    | 173/315 [01:35<00:41,  3.44it/s] 55%|█████▌    | 174/315 [01:35<00:40,  3.45it/s] 56%|█████▌    | 175/315 [01:36<00:40,  3.45it/s] 56%|█████▌    | 176/315 [01:36<00:40,  3.45it/s] 56%|█████▌    | 177/315 [01:36<00:39,  3.45it/s] 57%|█████▋    | 178/315 [01:36<00:39,  3.46it/s] 57%|█████▋    | 179/315 [01:37<00:40,  3.39it/s] 57%|█████▋    | 180/315 [01:37<00:39,  3.42it/s] 57%|█████▋    | 181/315 [01:37<00:39,  3.43it/s] 58%|█████▊    | 182/315 [01:38<00:38,  3.44it/s] 58%|█████▊    | 183/315 [01:38<00:38,  3.44it/s] 58%|█████▊    | 184/315 [01:38<00:37,  3.45it/s] 59%|█████▊    | 185/315 [01:38<00:37,  3.45it/s] 59%|█████▉    | 186/315 [01:39<00:37,  3.45it/s] 59%|█████▉    | 187/315 [01:39<00:37,  3.46it/s] 60%|█████▉    | 188/315 [01:39<00:36,  3.46it/s] 60%|██████    | 189/315 [01:40<00:33,  3.72it/s][INFO|trainer.py:2140] 2023-08-29 02:28:04,657 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:28:04,657 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 02:28:04,657 >>   Batch size = 8
{'eval_loss': 1.0313701629638672, 'eval_runtime': 9.7921, 'eval_samples_per_second': 355.083, 'eval_steps_per_second': 44.424, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.14it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.44it/s][A
  4%|▍         | 18/435 [00:00<00:08, 47.29it/s][A
  5%|▌         | 23/435 [00:00<00:08, 46.27it/s][A
  6%|▋         | 28/435 [00:00<00:08, 45.74it/s][A
  8%|▊         | 33/435 [00:00<00:08, 45.26it/s][A
  9%|▊         | 38/435 [00:00<00:08, 44.99it/s][A
 10%|▉         | 43/435 [00:00<00:08, 44.59it/s][A
 11%|█         | 48/435 [00:01<00:08, 44.42it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 44.57it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 44.69it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 44.80it/s][A
 16%|█▌        | 68/435 [00:01<00:08, 44.85it/s][A
 17%|█▋        | 73/435 [00:01<00:08, 44.76it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 44.63it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 44.56it/s][A
 20%|██        | 88/435 [00:01<00:07, 44.26it/s][A
 21%|██▏       | 93/435 [00:02<00:07, 44.29it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 44.39it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 44.54it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 44.67it/s][A
 26%|██▌       | 113/435 [00:02<00:07, 44.75it/s][A
 27%|██▋       | 118/435 [00:02<00:07, 44.83it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 44.64it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 44.44it/s][A
 31%|███       | 133/435 [00:02<00:06, 44.27it/s][A
 32%|███▏      | 138/435 [00:03<00:06, 43.39it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 43.88it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 44.22it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 44.51it/s][A
 36%|███▋      | 158/435 [00:03<00:06, 44.60it/s][A
 37%|███▋      | 163/435 [00:03<00:06, 44.63it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 44.58it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 44.41it/s][A
 41%|████      | 178/435 [00:03<00:05, 44.16it/s][A
 42%|████▏     | 183/435 [00:04<00:05, 44.17it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 44.36it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 44.58it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 44.66it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 44.79it/s][A
 48%|████▊     | 208/435 [00:04<00:05, 44.73it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 44.58it/s][A
 50%|█████     | 218/435 [00:04<00:04, 44.42it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 44.19it/s][A
 52%|█████▏    | 228/435 [00:05<00:04, 44.17it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 44.37it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 44.47it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 44.72it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 44.71it/s][A
 58%|█████▊    | 253/435 [00:05<00:04, 44.74it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 44.67it/s][A
 60%|██████    | 263/435 [00:05<00:03, 44.41it/s][A
 62%|██████▏   | 268/435 [00:06<00:03, 44.28it/s][A
 63%|██████▎   | 273/435 [00:06<00:03, 42.69it/s][A
 64%|██████▍   | 278/435 [00:06<00:03, 43.41it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 43.83it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 44.19it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 44.35it/s][A
 69%|██████▊   | 298/435 [00:06<00:03, 44.46it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 44.48it/s][A
 71%|███████   | 308/435 [00:06<00:02, 44.31it/s][A
 72%|███████▏  | 313/435 [00:07<00:02, 44.03it/s][A
 73%|███████▎  | 318/435 [00:07<00:02, 44.11it/s][A
 74%|███████▍  | 323/435 [00:07<00:02, 44.39it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 44.40it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 44.62it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 44.48it/s][A
 79%|███████▉  | 343/435 [00:07<00:02, 44.63it/s][A
 80%|████████  | 348/435 [00:07<00:02, 43.46it/s][A
 81%|████████  | 353/435 [00:07<00:01, 43.67it/s][A
 82%|████████▏ | 358/435 [00:08<00:01, 43.78it/s][A
 83%|████████▎ | 363/435 [00:08<00:01, 43.94it/s][A
 85%|████████▍ | 368/435 [00:08<00:01, 44.21it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 44.43it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 44.53it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 44.57it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 44.58it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 44.44it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 44.38it/s][A
 93%|█████████▎| 403/435 [00:09<00:01, 27.06it/s][A
 94%|█████████▍| 408/435 [00:09<00:00, 30.29it/s][A
 95%|█████████▍| 413/435 [00:09<00:00, 33.54it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 36.33it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 38.61it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 40.24it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 41.60it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:10<00:00, 41.60it/s][A 60%|██████    | 189/315 [01:50<00:33,  3.72it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:28:14,773 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-189
[INFO|configuration_utils.py:351] 2023-08-29 02:28:14,884 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-189/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:28:17,686 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-189/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:28:17,795 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-189/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:28:17,849 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-189/special_tokens_map.json
 60%|██████    | 190/315 [02:01<14:06,  6.77s/it] 61%|██████    | 191/315 [02:02<09:59,  4.84s/it] 61%|██████    | 192/315 [02:02<07:07,  3.47s/it] 61%|██████▏   | 193/315 [02:02<05:07,  2.52s/it] 62%|██████▏   | 194/315 [02:03<03:44,  1.85s/it] 62%|██████▏   | 195/315 [02:03<02:46,  1.39s/it] 62%|██████▏   | 196/315 [02:03<02:06,  1.06s/it] 63%|██████▎   | 197/315 [02:04<01:38,  1.20it/s] 63%|██████▎   | 198/315 [02:04<01:18,  1.49it/s] 63%|██████▎   | 199/315 [02:04<01:04,  1.79it/s] 63%|██████▎   | 200/315 [02:04<00:55,  2.08it/s] 64%|██████▍   | 201/315 [02:05<00:49,  2.31it/s] 64%|██████▍   | 202/315 [02:05<00:44,  2.55it/s] 64%|██████▍   | 203/315 [02:05<00:40,  2.75it/s] 65%|██████▍   | 204/315 [02:06<00:38,  2.91it/s] 65%|██████▌   | 205/315 [02:06<00:36,  3.03it/s] 65%|██████▌   | 206/315 [02:06<00:34,  3.12it/s] 66%|██████▌   | 207/315 [02:07<00:33,  3.19it/s] 66%|██████▌   | 208/315 [02:07<00:33,  3.24it/s] 66%|██████▋   | 209/315 [02:07<00:32,  3.27it/s] 67%|██████▋   | 210/315 [02:07<00:31,  3.30it/s] 67%|██████▋   | 211/315 [02:08<00:31,  3.32it/s] 67%|██████▋   | 212/315 [02:08<00:31,  3.23it/s] 68%|██████▊   | 213/315 [02:08<00:31,  3.26it/s] 68%|██████▊   | 214/315 [02:09<00:30,  3.29it/s] 68%|██████▊   | 215/315 [02:09<00:30,  3.31it/s] 69%|██████▊   | 216/315 [02:09<00:29,  3.33it/s] 69%|██████▉   | 217/315 [02:10<00:29,  3.34it/s] 69%|██████▉   | 218/315 [02:10<00:28,  3.35it/s] 70%|██████▉   | 219/315 [02:10<00:28,  3.35it/s] 70%|██████▉   | 220/315 [02:10<00:28,  3.36it/s] 70%|███████   | 221/315 [02:11<00:27,  3.36it/s] 70%|███████   | 222/315 [02:11<00:28,  3.21it/s] 71%|███████   | 223/315 [02:11<00:28,  3.25it/s] 71%|███████   | 224/315 [02:12<00:27,  3.28it/s] 71%|███████▏  | 225/315 [02:12<00:27,  3.31it/s] 72%|███████▏  | 226/315 [02:12<00:26,  3.32it/s] 72%|███████▏  | 227/315 [02:13<00:26,  3.33it/s] 72%|███████▏  | 228/315 [02:13<00:26,  3.34it/s] 73%|███████▎  | 229/315 [02:13<00:25,  3.35it/s] 73%|███████▎  | 230/315 [02:14<00:25,  3.36it/s] 73%|███████▎  | 231/315 [02:14<00:24,  3.36it/s] 74%|███████▎  | 232/315 [02:14<00:25,  3.25it/s] 74%|███████▍  | 233/315 [02:14<00:24,  3.28it/s] 74%|███████▍  | 234/315 [02:15<00:24,  3.30it/s] 75%|███████▍  | 235/315 [02:15<00:24,  3.32it/s] 75%|███████▍  | 236/315 [02:15<00:23,  3.33it/s] 75%|███████▌  | 237/315 [02:16<00:23,  3.34it/s] 76%|███████▌  | 238/315 [02:16<00:23,  3.35it/s] 76%|███████▌  | 239/315 [02:16<00:22,  3.35it/s] 76%|███████▌  | 240/315 [02:17<00:22,  3.35it/s] 77%|███████▋  | 241/315 [02:17<00:22,  3.36it/s] 77%|███████▋  | 242/315 [02:17<00:22,  3.23it/s] 77%|███████▋  | 243/315 [02:17<00:22,  3.27it/s] 77%|███████▋  | 244/315 [02:18<00:21,  3.30it/s] 78%|███████▊  | 245/315 [02:18<00:21,  3.31it/s] 78%|███████▊  | 246/315 [02:18<00:20,  3.32it/s] 78%|███████▊  | 247/315 [02:19<00:20,  3.33it/s] 79%|███████▊  | 248/315 [02:19<00:20,  3.34it/s] 79%|███████▉  | 249/315 [02:19<00:19,  3.34it/s] 79%|███████▉  | 250/315 [02:20<00:19,  3.34it/s] 80%|███████▉  | 251/315 [02:20<00:19,  3.34it/s] 80%|████████  | 252/315 [02:20<00:17,  3.50it/s][INFO|trainer.py:2140] 2023-08-29 02:28:45,169 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:28:45,169 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 02:28:45,169 >>   Batch size = 8
{'eval_loss': 1.0494978427886963, 'eval_runtime': 10.0328, 'eval_samples_per_second': 346.563, 'eval_steps_per_second': 43.358, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 55.47it/s][A
  3%|▎         | 12/435 [00:00<00:08, 48.44it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.12it/s][A
  5%|▌         | 22/435 [00:00<00:08, 46.40it/s][A
  6%|▌         | 27/435 [00:00<00:08, 45.72it/s][A
  7%|▋         | 32/435 [00:00<00:08, 45.31it/s][A
  9%|▊         | 37/435 [00:00<00:08, 44.63it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.27it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.36it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.57it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 43.36it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.26it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.55it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.65it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 44.53it/s][A
 19%|█▉        | 82/435 [00:01<00:07, 44.24it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.09it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.23it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.43it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.50it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.61it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.73it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 44.80it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 44.44it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 44.21it/s][A
 30%|███       | 132/435 [00:02<00:06, 44.18it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 44.24it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 44.40it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 44.41it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.49it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.64it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 44.71it/s][A
 38%|███▊      | 167/435 [00:03<00:05, 44.68it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.59it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.39it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.36it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.38it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 44.53it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 44.61it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 44.71it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 44.65it/s][A
 49%|████▊     | 212/435 [00:04<00:04, 44.63it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 44.49it/s][A
 51%|█████     | 222/435 [00:04<00:05, 42.32it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 43.01it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 43.44it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 43.86it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.18it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.37it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 44.39it/s][A
 59%|█████▉    | 257/435 [00:05<00:04, 44.36it/s][A
 60%|██████    | 262/435 [00:05<00:03, 44.18it/s][A
 61%|██████▏   | 267/435 [00:05<00:03, 44.21it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.31it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.38it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.57it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.64it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.72it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.65it/s][A
 69%|██████▉   | 302/435 [00:06<00:02, 44.51it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.42it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.35it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.33it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.31it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.52it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.64it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.74it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 44.69it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 44.63it/s][A
 81%|████████  | 352/435 [00:07<00:01, 44.51it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 42.37it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 43.01it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 43.52it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 43.81it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 44.08it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 44.27it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 44.42it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 44.41it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 44.08it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 44.22it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 44.40it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 44.48it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 44.61it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.60it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 44.63it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.61it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 44.61it/s][A 80%|████████  | 252/315 [02:30<00:17,  3.50it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:28:55,188 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-252
[INFO|configuration_utils.py:351] 2023-08-29 02:28:55,335 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-252/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:28:59,007 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-252/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:28:59,160 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-252/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:28:59,244 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-252/special_tokens_map.json
 80%|████████  | 253/315 [02:40<06:30,  6.30s/it] 81%|████████  | 254/315 [02:41<04:34,  4.51s/it] 81%|████████  | 255/315 [02:41<03:14,  3.24s/it] 81%|████████▏ | 256/315 [02:41<02:19,  2.36s/it] 82%|████████▏ | 257/315 [02:42<01:41,  1.74s/it] 82%|████████▏ | 258/315 [02:42<01:14,  1.31s/it] 82%|████████▏ | 259/315 [02:42<00:56,  1.00s/it] 83%|████████▎ | 260/315 [02:43<00:43,  1.26it/s] 83%|████████▎ | 261/315 [02:43<00:34,  1.55it/s] 83%|████████▎ | 262/315 [02:43<00:28,  1.85it/s] 83%|████████▎ | 263/315 [02:43<00:24,  2.14it/s] 84%|████████▍ | 264/315 [02:44<00:21,  2.41it/s] 84%|████████▍ | 265/315 [02:44<00:19,  2.55it/s] 84%|████████▍ | 266/315 [02:44<00:17,  2.75it/s] 85%|████████▍ | 267/315 [02:45<00:16,  2.91it/s] 85%|████████▌ | 268/315 [02:45<00:15,  3.03it/s] 85%|████████▌ | 269/315 [02:45<00:14,  3.13it/s] 86%|████████▌ | 270/315 [02:46<00:14,  3.20it/s] 86%|████████▌ | 271/315 [02:46<00:13,  3.25it/s] 86%|████████▋ | 272/315 [02:46<00:13,  3.28it/s] 87%|████████▋ | 273/315 [02:46<00:12,  3.30it/s] 87%|████████▋ | 274/315 [02:47<00:12,  3.32it/s] 87%|████████▋ | 275/315 [02:47<00:12,  3.26it/s] 88%|████████▊ | 276/315 [02:47<00:11,  3.29it/s] 88%|████████▊ | 277/315 [02:48<00:11,  3.20it/s] 88%|████████▊ | 278/315 [02:48<00:11,  3.25it/s] 89%|████████▊ | 279/315 [02:48<00:10,  3.28it/s] 89%|████████▉ | 280/315 [02:49<00:10,  3.31it/s] 89%|████████▉ | 281/315 [02:49<00:11,  2.91it/s] 90%|████████▉ | 282/315 [02:49<00:10,  3.03it/s] 90%|████████▉ | 283/315 [02:50<00:10,  3.12it/s] 90%|█████████ | 284/315 [02:50<00:09,  3.19it/s] 90%|█████████ | 285/315 [02:50<00:09,  3.16it/s] 91%|█████████ | 286/315 [02:51<00:08,  3.22it/s] 91%|█████████ | 287/315 [02:51<00:08,  3.26it/s] 91%|█████████▏| 288/315 [02:51<00:08,  3.29it/s] 92%|█████████▏| 289/315 [02:51<00:07,  3.31it/s] 92%|█████████▏| 290/315 [02:52<00:07,  3.33it/s] 92%|█████████▏| 291/315 [02:52<00:07,  3.34it/s] 93%|█████████▎| 292/315 [02:52<00:06,  3.35it/s] 93%|█████████▎| 293/315 [02:53<00:06,  3.35it/s] 93%|█████████▎| 294/315 [02:53<00:06,  3.35it/s] 94%|█████████▎| 295/315 [02:53<00:05,  3.36it/s] 94%|█████████▍| 296/315 [02:53<00:05,  3.36it/s] 94%|█████████▍| 297/315 [02:54<00:05,  3.36it/s] 95%|█████████▍| 298/315 [02:54<00:05,  3.37it/s] 95%|█████████▍| 299/315 [02:54<00:04,  3.37it/s] 95%|█████████▌| 300/315 [02:55<00:04,  3.37it/s] 96%|█████████▌| 301/315 [02:55<00:04,  3.37it/s] 96%|█████████▌| 302/315 [02:55<00:03,  3.37it/s] 96%|█████████▌| 303/315 [02:56<00:03,  3.37it/s] 97%|█████████▋| 304/315 [02:56<00:03,  3.37it/s] 97%|█████████▋| 305/315 [02:56<00:03,  3.27it/s] 97%|█████████▋| 306/315 [02:56<00:02,  3.29it/s] 97%|█████████▋| 307/315 [02:57<00:02,  3.31it/s] 98%|█████████▊| 308/315 [02:57<00:02,  3.33it/s] 98%|█████████▊| 309/315 [02:57<00:01,  3.34it/s] 98%|█████████▊| 310/315 [02:58<00:01,  3.35it/s] 99%|█████████▊| 311/315 [02:58<00:01,  3.35it/s] 99%|█████████▉| 312/315 [02:58<00:00,  3.36it/s] 99%|█████████▉| 313/315 [02:59<00:00,  3.36it/s]100%|█████████▉| 314/315 [02:59<00:00,  3.36it/s]100%|██████████| 315/315 [02:59<00:00,  3.55it/s][INFO|trainer.py:2140] 2023-08-29 02:29:24,203 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:29:24,203 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 02:29:24,203 >>   Batch size = 8
{'eval_loss': 1.053595781326294, 'eval_runtime': 9.8086, 'eval_samples_per_second': 354.484, 'eval_steps_per_second': 44.349, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 55.95it/s][A
  3%|▎         | 12/435 [00:00<00:08, 48.74it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.07it/s][A
  5%|▌         | 22/435 [00:00<00:08, 46.06it/s][A
  6%|▌         | 27/435 [00:00<00:08, 45.53it/s][A
  7%|▋         | 32/435 [00:00<00:08, 44.95it/s][A
  9%|▊         | 37/435 [00:00<00:08, 44.40it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.25it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.38it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.62it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.68it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.85it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.96it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.80it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 44.44it/s][A
 19%|█▉        | 82/435 [00:01<00:07, 44.27it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.16it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.24it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.46it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.58it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.68it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.76it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 44.74it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 44.40it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 44.23it/s][A
 30%|███       | 132/435 [00:02<00:07, 42.50it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 43.17it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 43.71it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 44.10it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.36it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.50it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 44.38it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 44.14it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.02it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.04it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.25it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.41it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 44.55it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 44.70it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 44.87it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 44.65it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 44.36it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 44.21it/s][A
 51%|█████     | 222/435 [00:04<00:04, 44.19it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.34it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.46it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.58it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.67it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.71it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 44.55it/s][A
 59%|█████▉    | 257/435 [00:05<00:04, 44.28it/s][A
 60%|██████    | 262/435 [00:05<00:03, 44.14it/s][A
 61%|██████▏   | 267/435 [00:06<00:03, 42.34it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 43.13it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 43.62it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.07it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.28it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.30it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.21it/s][A
 69%|██████▉   | 302/435 [00:06<00:03, 44.09it/s][A
 71%|███████   | 307/435 [00:06<00:02, 43.93it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.10it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.34it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.49it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.68it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.75it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.74it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 44.52it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 44.08it/s][A
 81%|████████  | 352/435 [00:07<00:01, 44.18it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 44.11it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 44.20it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.51it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 44.65it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 44.78it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 44.69it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 44.55it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 44.30it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 44.20it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 43.36it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 43.82it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 44.11it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 44.33it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.52it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 44.52it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.43it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 44.43it/s][A100%|██████████| 315/315 [03:09<00:00,  3.55it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:29:34,116 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-315
[INFO|configuration_utils.py:351] 2023-08-29 02:29:34,274 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-315/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:29:38,272 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-315/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:29:38,502 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-315/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:29:38,655 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-315/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 02:29:46,844 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 02:29:46,880 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-63 (score: 1.0246598720550537).
                                                 100%|██████████| 315/315 [03:30<00:00,  3.55it/s]100%|██████████| 315/315 [03:30<00:00,  1.49it/s]
[INFO|trainer.py:1894] 2023-08-29 02:29:55,643 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 02:29:55,742 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:29:58,312 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:29:58,401 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:29:58,460 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:29:59,002 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:29:59,003 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:29:59,003 >>   train_loss               =     0.4815
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:29:59,003 >>   train_runtime            = 0:03:30.87
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:29:59,003 >>   train_samples            =       4014
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:29:59,003 >>   train_samples_per_second =     95.176
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:29:59,003 >>   train_steps_per_second   =      1.494
{'eval_loss': 1.0620179176330566, 'eval_runtime': 9.8147, 'eval_samples_per_second': 354.266, 'eval_steps_per_second': 44.321, 'epoch': 5.0}
{'train_runtime': 210.8733, 'train_samples_per_second': 95.176, 'train_steps_per_second': 1.494, 'train_loss': 0.4814856150793651, 'epoch': 5.0}
08/29/2023 02:29:59 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 02:29:59,205 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:29:59,205 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 02:29:59,205 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|▏         | 6/435 [00:00<00:07, 55.41it/s]  3%|▎         | 12/435 [00:00<00:08, 48.76it/s]  4%|▍         | 17/435 [00:00<00:08, 47.40it/s]  5%|▌         | 22/435 [00:00<00:08, 46.54it/s]  6%|▌         | 27/435 [00:00<00:08, 46.02it/s]  7%|▋         | 32/435 [00:00<00:08, 45.85it/s]  9%|▊         | 37/435 [00:00<00:08, 45.65it/s] 10%|▉         | 42/435 [00:00<00:08, 45.29it/s] 11%|█         | 47/435 [00:01<00:08, 44.76it/s] 12%|█▏        | 52/435 [00:01<00:08, 44.54it/s] 13%|█▎        | 57/435 [00:01<00:08, 44.47it/s] 14%|█▍        | 62/435 [00:01<00:08, 44.68it/s] 15%|█▌        | 67/435 [00:01<00:08, 44.66it/s] 17%|█▋        | 72/435 [00:01<00:08, 44.62it/s] 18%|█▊        | 77/435 [00:01<00:07, 44.79it/s] 19%|█▉        | 82/435 [00:01<00:07, 44.91it/s] 20%|██        | 87/435 [00:01<00:07, 44.76it/s] 21%|██        | 92/435 [00:02<00:07, 44.51it/s] 22%|██▏       | 97/435 [00:02<00:07, 44.42it/s] 23%|██▎       | 102/435 [00:02<00:07, 44.37it/s] 25%|██▍       | 107/435 [00:02<00:07, 43.18it/s] 26%|██▌       | 112/435 [00:02<00:07, 43.71it/s] 27%|██▋       | 117/435 [00:02<00:07, 44.15it/s] 28%|██▊       | 122/435 [00:02<00:07, 44.47it/s] 29%|██▉       | 127/435 [00:02<00:06, 44.66it/s] 30%|███       | 132/435 [00:02<00:06, 44.68it/s] 31%|███▏      | 137/435 [00:03<00:06, 44.67it/s] 33%|███▎      | 142/435 [00:03<00:06, 44.56it/s] 34%|███▍      | 147/435 [00:03<00:06, 44.31it/s] 35%|███▍      | 152/435 [00:03<00:06, 44.52it/s] 36%|███▌      | 157/435 [00:03<00:06, 44.61it/s] 37%|███▋      | 162/435 [00:03<00:06, 44.86it/s] 38%|███▊      | 167/435 [00:03<00:05, 44.92it/s] 40%|███▉      | 172/435 [00:03<00:05, 44.99it/s] 41%|████      | 177/435 [00:03<00:05, 44.91it/s] 42%|████▏     | 182/435 [00:04<00:05, 44.77it/s] 43%|████▎     | 187/435 [00:04<00:05, 44.58it/s] 44%|████▍     | 192/435 [00:04<00:05, 44.50it/s] 45%|████▌     | 197/435 [00:04<00:05, 44.45it/s] 46%|████▋     | 202/435 [00:04<00:05, 44.64it/s] 48%|████▊     | 207/435 [00:04<00:05, 44.76it/s] 49%|████▊     | 212/435 [00:04<00:04, 45.00it/s] 50%|████▉     | 217/435 [00:04<00:04, 44.99it/s] 51%|█████     | 222/435 [00:04<00:04, 45.03it/s] 52%|█████▏    | 227/435 [00:05<00:04, 44.78it/s] 53%|█████▎    | 232/435 [00:05<00:04, 44.65it/s] 54%|█████▍    | 237/435 [00:05<00:04, 44.50it/s] 56%|█████▌    | 242/435 [00:05<00:04, 42.74it/s] 57%|█████▋    | 247/435 [00:05<00:04, 43.51it/s] 58%|█████▊    | 252/435 [00:05<00:04, 43.88it/s] 59%|█████▉    | 257/435 [00:05<00:04, 44.34it/s] 60%|██████    | 262/435 [00:05<00:03, 44.61it/s] 61%|██████▏   | 267/435 [00:05<00:03, 44.66it/s] 63%|██████▎   | 272/435 [00:06<00:03, 44.59it/s] 64%|██████▎   | 277/435 [00:06<00:03, 44.46it/s] 65%|██████▍   | 282/435 [00:06<00:03, 44.31it/s] 66%|██████▌   | 287/435 [00:06<00:03, 44.42it/s] 67%|██████▋   | 292/435 [00:06<00:03, 44.59it/s] 68%|██████▊   | 297/435 [00:06<00:03, 44.82it/s] 69%|██████▉   | 302/435 [00:06<00:02, 44.94it/s] 71%|███████   | 307/435 [00:06<00:02, 44.97it/s] 72%|███████▏  | 312/435 [00:06<00:02, 45.02it/s] 73%|███████▎  | 317/435 [00:07<00:02, 44.79it/s] 74%|███████▍  | 322/435 [00:07<00:02, 44.59it/s] 75%|███████▌  | 327/435 [00:07<00:02, 44.44it/s] 76%|███████▋  | 332/435 [00:07<00:02, 44.55it/s] 77%|███████▋  | 337/435 [00:07<00:02, 44.61it/s] 79%|███████▊  | 342/435 [00:07<00:02, 44.76it/s] 80%|███████▉  | 347/435 [00:07<00:01, 44.84it/s] 81%|████████  | 352/435 [00:07<00:01, 44.99it/s] 82%|████████▏ | 357/435 [00:07<00:01, 44.92it/s] 83%|████████▎ | 362/435 [00:08<00:01, 44.63it/s] 84%|████████▍ | 367/435 [00:08<00:01, 44.56it/s] 86%|████████▌ | 372/435 [00:08<00:01, 44.54it/s] 87%|████████▋ | 377/435 [00:08<00:01, 42.98it/s] 88%|████████▊ | 382/435 [00:08<00:01, 43.63it/s] 89%|████████▉ | 387/435 [00:08<00:01, 44.15it/s] 90%|█████████ | 392/435 [00:08<00:00, 44.46it/s] 91%|█████████▏| 397/435 [00:08<00:00, 44.55it/s] 92%|█████████▏| 402/435 [00:08<00:00, 44.58it/s] 94%|█████████▎| 407/435 [00:09<00:00, 44.51it/s] 95%|█████████▍| 412/435 [00:09<00:00, 44.30it/s] 96%|█████████▌| 417/435 [00:09<00:00, 44.16it/s] 97%|█████████▋| 422/435 [00:09<00:00, 44.38it/s] 98%|█████████▊| 427/435 [00:09<00:00, 44.51it/s] 99%|█████████▉| 432/435 [00:09<00:00, 44.66it/s]100%|██████████| 435/435 [00:09<00:00, 44.66it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:30:08,969 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:30:08,969 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:30:08,969 >>   eval_loss               =     1.0247
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:30:08,969 >>   eval_runtime            = 0:00:09.76
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:30:08,969 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:30:08,969 >>   eval_samples_per_second =    356.114
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:30:08,969 >>   eval_steps_per_second   =     44.553
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:30:08,969 >>   perplexity              =     2.7861
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:30:18,699 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:30:18,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:30:18,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:30:18,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:30:18,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:30:19,536 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:30:19,537 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:30:20,127 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:30:21,291 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:30:21,292 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:30:24,565 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:30:24,611 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:30:24,611 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:30:24,611 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:30:24,611 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:30:25,524 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:30:25,525 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:30:26,207 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:30:26,456 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:30:26,456 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-252
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-189
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-315
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-126
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-63
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.23it/s]Extractor Predicting: 2it [00:01,  1.29it/s]Extractor Predicting: 3it [00:02,  1.34it/s]Extractor Predicting: 4it [00:03,  1.35it/s]Extractor Predicting: 5it [00:03,  1.34it/s]Extractor Predicting: 6it [00:04,  1.33it/s]Extractor Predicting: 7it [00:05,  1.31it/s]Extractor Predicting: 8it [00:06,  1.31it/s]Extractor Predicting: 9it [00:06,  1.24it/s]Extractor Predicting: 10it [00:07,  1.23it/s]Extractor Predicting: 11it [00:08,  1.25it/s]Extractor Predicting: 12it [00:09,  1.25it/s]Extractor Predicting: 13it [00:10,  1.27it/s]Extractor Predicting: 14it [00:10,  1.25it/s]Extractor Predicting: 15it [00:11,  1.27it/s]Extractor Predicting: 16it [00:12,  1.27it/s]Extractor Predicting: 17it [00:13,  1.29it/s]Extractor Predicting: 18it [00:14,  1.30it/s]Extractor Predicting: 19it [00:14,  1.31it/s]Extractor Predicting: 20it [00:15,  1.30it/s]Extractor Predicting: 21it [00:16,  1.30it/s]Extractor Predicting: 22it [00:17,  1.31it/s]Extractor Predicting: 23it [00:17,  1.33it/s]Extractor Predicting: 24it [00:18,  1.34it/s]Extractor Predicting: 25it [00:19,  1.31it/s]Extractor Predicting: 26it [00:20,  1.29it/s]Extractor Predicting: 27it [00:20,  1.30it/s]Extractor Predicting: 28it [00:21,  1.27it/s]Extractor Predicting: 29it [00:22,  1.26it/s]Extractor Predicting: 30it [00:23,  1.27it/s]Extractor Predicting: 31it [00:24,  1.27it/s]Extractor Predicting: 32it [00:24,  1.27it/s]Extractor Predicting: 33it [00:25,  1.24it/s]Extractor Predicting: 34it [00:26,  1.27it/s]Extractor Predicting: 35it [00:27,  1.30it/s]Extractor Predicting: 36it [00:27,  1.32it/s]Extractor Predicting: 37it [00:28,  1.31it/s]Extractor Predicting: 38it [00:29,  1.27it/s]Extractor Predicting: 39it [00:30,  1.29it/s]Extractor Predicting: 40it [00:31,  1.27it/s]Extractor Predicting: 41it [00:31,  1.30it/s]Extractor Predicting: 42it [00:32,  1.31it/s]Extractor Predicting: 43it [00:33,  1.28it/s]Extractor Predicting: 44it [00:34,  1.27it/s]Extractor Predicting: 45it [00:34,  1.26it/s]Extractor Predicting: 46it [00:35,  1.28it/s]Extractor Predicting: 47it [00:36,  1.30it/s]Extractor Predicting: 48it [00:37,  1.29it/s]Extractor Predicting: 49it [00:38,  1.32it/s]Extractor Predicting: 50it [00:38,  1.35it/s]Extractor Predicting: 51it [00:39,  1.34it/s]Extractor Predicting: 52it [00:40,  1.30it/s]Extractor Predicting: 53it [00:41,  1.29it/s]Extractor Predicting: 54it [00:41,  1.31it/s]Extractor Predicting: 55it [00:42,  1.30it/s]Extractor Predicting: 56it [00:43,  1.29it/s]Extractor Predicting: 57it [00:44,  1.28it/s]Extractor Predicting: 58it [00:44,  1.30it/s]Extractor Predicting: 59it [00:45,  1.33it/s]Extractor Predicting: 60it [00:46,  1.34it/s]Extractor Predicting: 61it [00:47,  1.35it/s]Extractor Predicting: 62it [00:47,  1.35it/s]Extractor Predicting: 63it [00:48,  1.35it/s]Extractor Predicting: 64it [00:49,  1.35it/s]Extractor Predicting: 65it [00:50,  1.35it/s]Extractor Predicting: 66it [00:50,  1.33it/s]Extractor Predicting: 67it [00:51,  1.32it/s]Extractor Predicting: 68it [00:52,  1.33it/s]Extractor Predicting: 69it [00:53,  1.34it/s]Extractor Predicting: 70it [00:53,  1.33it/s]Extractor Predicting: 71it [00:54,  1.32it/s]Extractor Predicting: 72it [00:55,  1.33it/s]Extractor Predicting: 73it [00:56,  1.33it/s]Extractor Predicting: 74it [00:56,  1.35it/s]Extractor Predicting: 75it [00:57,  1.37it/s]Extractor Predicting: 76it [00:58,  1.35it/s]Extractor Predicting: 77it [00:59,  1.31it/s]Extractor Predicting: 78it [00:59,  1.31it/s]Extractor Predicting: 79it [01:00,  1.30it/s]Extractor Predicting: 80it [01:01,  1.29it/s]Extractor Predicting: 81it [01:02,  1.26it/s]Extractor Predicting: 82it [01:03,  1.29it/s]Extractor Predicting: 83it [01:03,  1.30it/s]Extractor Predicting: 84it [01:04,  1.31it/s]Extractor Predicting: 85it [01:05,  1.30it/s]Extractor Predicting: 86it [01:06,  1.31it/s]Extractor Predicting: 87it [01:06,  1.30it/s]Extractor Predicting: 88it [01:07,  1.29it/s]Extractor Predicting: 89it [01:08,  1.31it/s]Extractor Predicting: 90it [01:09,  1.32it/s]Extractor Predicting: 91it [01:09,  1.35it/s]Extractor Predicting: 92it [01:10,  1.27it/s]Extractor Predicting: 93it [01:11,  1.29it/s]Extractor Predicting: 94it [01:12,  1.29it/s]Extractor Predicting: 95it [01:12,  1.33it/s]Extractor Predicting: 96it [01:13,  1.33it/s]Extractor Predicting: 97it [01:14,  1.34it/s]Extractor Predicting: 98it [01:15,  1.32it/s]Extractor Predicting: 99it [01:16,  1.30it/s]Extractor Predicting: 100it [01:16,  1.28it/s]Extractor Predicting: 101it [01:17,  1.28it/s]Extractor Predicting: 102it [01:18,  1.32it/s]Extractor Predicting: 103it [01:18,  1.35it/s]Extractor Predicting: 104it [01:19,  1.34it/s]Extractor Predicting: 105it [01:20,  1.34it/s]Extractor Predicting: 106it [01:21,  1.35it/s]Extractor Predicting: 107it [01:21,  1.34it/s]Extractor Predicting: 108it [01:22,  1.34it/s]Extractor Predicting: 109it [01:23,  1.34it/s]Extractor Predicting: 110it [01:24,  1.35it/s]Extractor Predicting: 111it [01:24,  1.36it/s]Extractor Predicting: 112it [01:25,  1.33it/s]Extractor Predicting: 113it [01:26,  1.37it/s]Extractor Predicting: 114it [01:27,  1.40it/s]Extractor Predicting: 115it [01:27,  1.39it/s]Extractor Predicting: 116it [01:28,  1.39it/s]Extractor Predicting: 117it [01:29,  1.37it/s]Extractor Predicting: 118it [01:30,  1.36it/s]Extractor Predicting: 119it [01:30,  1.37it/s]Extractor Predicting: 120it [01:31,  1.34it/s]Extractor Predicting: 121it [01:32,  1.31it/s]Extractor Predicting: 122it [01:33,  1.32it/s]Extractor Predicting: 123it [01:33,  1.32it/s]Extractor Predicting: 124it [01:34,  1.31it/s]Extractor Predicting: 125it [01:35,  1.31it/s]Extractor Predicting: 126it [01:36,  1.33it/s]Extractor Predicting: 127it [01:36,  1.32it/s]Extractor Predicting: 128it [01:37,  1.33it/s]Extractor Predicting: 129it [01:38,  1.34it/s]Extractor Predicting: 130it [01:39,  1.30it/s]Extractor Predicting: 131it [01:39,  1.31it/s]Extractor Predicting: 132it [01:40,  1.29it/s]Extractor Predicting: 133it [01:41,  1.32it/s]Extractor Predicting: 134it [01:42,  1.33it/s]Extractor Predicting: 135it [01:42,  1.33it/s]Extractor Predicting: 136it [01:43,  1.30it/s]Extractor Predicting: 137it [01:44,  1.33it/s]Extractor Predicting: 138it [01:45,  1.30it/s]Extractor Predicting: 139it [01:46,  1.29it/s]Extractor Predicting: 140it [01:46,  1.31it/s]Extractor Predicting: 141it [01:47,  1.33it/s]Extractor Predicting: 142it [01:48,  1.31it/s]Extractor Predicting: 143it [01:49,  1.33it/s]Extractor Predicting: 144it [01:49,  1.37it/s]Extractor Predicting: 144it [01:49,  1.31it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:29,325 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:29,361 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:29,361 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:29,361 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:29,361 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:32:30,423 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:32:30,424 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:32:30,746 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:32:31,892 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:32:31,892 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:34,912 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:34,928 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:34,928 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:34,928 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:32:34,928 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:32:35,671 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:32:35,672 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:32:36,303 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:32:36,495 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:32:36,495 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2971046770601336,
  "recall": 0.1918320391141789,
  "score": 0.23313526738902482,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.33it/s]Extractor Predicting: 2it [00:01,  1.29it/s]Extractor Predicting: 3it [00:02,  1.29it/s]Extractor Predicting: 4it [00:03,  1.31it/s]Extractor Predicting: 5it [00:03,  1.31it/s]Extractor Predicting: 6it [00:04,  1.32it/s]Extractor Predicting: 7it [00:05,  1.36it/s]Extractor Predicting: 8it [00:05,  1.40it/s]Extractor Predicting: 9it [00:06,  1.36it/s]Extractor Predicting: 10it [00:07,  1.35it/s]Extractor Predicting: 11it [00:08,  1.36it/s]Extractor Predicting: 12it [00:08,  1.35it/s]Extractor Predicting: 13it [00:09,  1.34it/s]Extractor Predicting: 14it [00:10,  1.36it/s]Extractor Predicting: 15it [00:11,  1.36it/s]Extractor Predicting: 16it [00:11,  1.36it/s]Extractor Predicting: 17it [00:12,  1.34it/s]Extractor Predicting: 18it [00:13,  1.37it/s]Extractor Predicting: 19it [00:14,  1.39it/s]Extractor Predicting: 20it [00:14,  1.35it/s]Extractor Predicting: 21it [00:15,  1.37it/s]Extractor Predicting: 22it [00:16,  1.39it/s]Extractor Predicting: 23it [00:16,  1.38it/s]Extractor Predicting: 24it [00:17,  1.36it/s]Extractor Predicting: 25it [00:18,  1.35it/s]Extractor Predicting: 26it [00:19,  1.36it/s]Extractor Predicting: 27it [00:19,  1.34it/s]Extractor Predicting: 28it [00:20,  1.34it/s]Extractor Predicting: 29it [00:21,  1.35it/s]Extractor Predicting: 30it [00:22,  1.37it/s]Extractor Predicting: 31it [00:22,  1.35it/s]Extractor Predicting: 32it [00:23,  1.40it/s]Extractor Predicting: 33it [00:24,  1.37it/s]Extractor Predicting: 34it [00:25,  1.35it/s]Extractor Predicting: 35it [00:25,  1.36it/s]Extractor Predicting: 36it [00:26,  1.38it/s]Extractor Predicting: 37it [00:27,  1.40it/s]Extractor Predicting: 38it [00:27,  1.43it/s]Extractor Predicting: 39it [00:28,  1.44it/s]Extractor Predicting: 40it [00:29,  1.43it/s]Extractor Predicting: 41it [00:29,  1.43it/s]Extractor Predicting: 42it [00:30,  1.46it/s]Extractor Predicting: 43it [00:31,  1.41it/s]Extractor Predicting: 44it [00:32,  1.42it/s]Extractor Predicting: 45it [00:32,  1.41it/s]Extractor Predicting: 46it [00:33,  1.39it/s]Extractor Predicting: 47it [00:34,  1.43it/s]Extractor Predicting: 48it [00:34,  1.41it/s]Extractor Predicting: 49it [00:35,  1.43it/s]Extractor Predicting: 50it [00:36,  1.43it/s]Extractor Predicting: 51it [00:37,  1.41it/s]Extractor Predicting: 52it [00:37,  1.39it/s]Extractor Predicting: 53it [00:38,  1.40it/s]Extractor Predicting: 54it [00:39,  1.35it/s]Extractor Predicting: 55it [00:39,  1.38it/s]Extractor Predicting: 56it [00:40,  1.36it/s]Extractor Predicting: 57it [00:41,  1.34it/s]Extractor Predicting: 58it [00:42,  1.30it/s]Extractor Predicting: 59it [00:43,  1.29it/s]Extractor Predicting: 60it [00:43,  1.32it/s]Extractor Predicting: 61it [00:44,  1.31it/s]Extractor Predicting: 62it [00:45,  1.32it/s]Extractor Predicting: 63it [00:46,  1.34it/s]Extractor Predicting: 64it [00:46,  1.33it/s]Extractor Predicting: 65it [00:47,  1.37it/s]Extractor Predicting: 66it [00:48,  1.33it/s]Extractor Predicting: 67it [00:49,  1.32it/s]Extractor Predicting: 68it [00:49,  1.32it/s]Extractor Predicting: 69it [00:50,  1.33it/s]Extractor Predicting: 70it [00:51,  1.29it/s]Extractor Predicting: 71it [00:52,  1.31it/s]Extractor Predicting: 72it [00:52,  1.31it/s]Extractor Predicting: 73it [00:53,  1.29it/s]Extractor Predicting: 74it [00:54,  1.29it/s]Extractor Predicting: 75it [00:55,  1.20it/s]Extractor Predicting: 76it [00:56,  1.24it/s]Extractor Predicting: 77it [00:56,  1.26it/s]Extractor Predicting: 78it [00:57,  1.28it/s]Extractor Predicting: 79it [00:58,  1.30it/s]Extractor Predicting: 80it [00:59,  1.35it/s]Extractor Predicting: 81it [00:59,  1.35it/s]Extractor Predicting: 82it [01:00,  1.35it/s]Extractor Predicting: 83it [01:01,  1.34it/s]Extractor Predicting: 84it [01:02,  1.34it/s]Extractor Predicting: 85it [01:02,  1.31it/s]Extractor Predicting: 86it [01:03,  1.27it/s]Extractor Predicting: 87it [01:04,  1.24it/s]Extractor Predicting: 88it [01:05,  1.27it/s]Extractor Predicting: 89it [01:06,  1.26it/s]Extractor Predicting: 90it [01:07,  1.24it/s]Extractor Predicting: 91it [01:07,  1.22it/s]Extractor Predicting: 92it [01:08,  1.24it/s]Extractor Predicting: 93it [01:09,  1.27it/s]Extractor Predicting: 94it [01:10,  1.29it/s]Extractor Predicting: 95it [01:10,  1.27it/s]Extractor Predicting: 96it [01:11,  1.28it/s]Extractor Predicting: 97it [01:12,  1.26it/s]Extractor Predicting: 98it [01:13,  1.24it/s]Extractor Predicting: 99it [01:14,  1.24it/s]Extractor Predicting: 100it [01:14,  1.27it/s]Extractor Predicting: 101it [01:15,  1.28it/s]Extractor Predicting: 102it [01:16,  1.29it/s]Extractor Predicting: 103it [01:17,  1.28it/s]Extractor Predicting: 104it [01:18,  1.26it/s]Extractor Predicting: 105it [01:18,  1.28it/s]Extractor Predicting: 106it [01:19,  1.27it/s]Extractor Predicting: 107it [01:20,  1.29it/s]Extractor Predicting: 108it [01:21,  1.30it/s]Extractor Predicting: 109it [01:21,  1.28it/s]Extractor Predicting: 110it [01:22,  1.27it/s]Extractor Predicting: 111it [01:23,  1.29it/s]Extractor Predicting: 112it [01:24,  1.28it/s]Extractor Predicting: 113it [01:25,  1.28it/s]Extractor Predicting: 114it [01:25,  1.30it/s]Extractor Predicting: 115it [01:26,  1.30it/s]Extractor Predicting: 116it [01:27,  1.26it/s]Extractor Predicting: 117it [01:28,  1.24it/s]Extractor Predicting: 118it [01:29,  1.27it/s]Extractor Predicting: 119it [01:29,  1.26it/s]Extractor Predicting: 120it [01:30,  1.26it/s]Extractor Predicting: 121it [01:31,  1.27it/s]Extractor Predicting: 122it [01:32,  1.30it/s]Extractor Predicting: 123it [01:32,  1.30it/s]Extractor Predicting: 124it [01:33,  1.30it/s]Extractor Predicting: 125it [01:34,  1.29it/s]Extractor Predicting: 126it [01:35,  1.35it/s]Extractor Predicting: 127it [01:35,  1.35it/s]Extractor Predicting: 128it [01:36,  1.32it/s]Extractor Predicting: 129it [01:37,  1.35it/s]Extractor Predicting: 130it [01:38,  1.34it/s]Extractor Predicting: 131it [01:38,  1.32it/s]Extractor Predicting: 132it [01:39,  1.33it/s]Extractor Predicting: 133it [01:40,  1.37it/s]Extractor Predicting: 134it [01:41,  1.38it/s]Extractor Predicting: 135it [01:41,  1.33it/s]Extractor Predicting: 136it [01:42,  1.34it/s]Extractor Predicting: 137it [01:43,  1.36it/s]Extractor Predicting: 138it [01:43,  1.39it/s]Extractor Predicting: 139it [01:44,  1.37it/s]Extractor Predicting: 140it [01:45,  1.36it/s]Extractor Predicting: 141it [01:46,  1.34it/s]Extractor Predicting: 142it [01:46,  1.36it/s]Extractor Predicting: 143it [01:47,  1.33it/s]Extractor Predicting: 144it [01:48,  1.32it/s]Extractor Predicting: 145it [01:49,  1.31it/s]Extractor Predicting: 146it [01:49,  1.34it/s]Extractor Predicting: 147it [01:50,  1.40it/s]Extractor Predicting: 148it [01:51,  1.40it/s]Extractor Predicting: 149it [01:52,  1.41it/s]Extractor Predicting: 150it [01:52,  1.42it/s]Extractor Predicting: 151it [01:53,  1.41it/s]Extractor Predicting: 152it [01:54,  1.43it/s]Extractor Predicting: 153it [01:54,  1.43it/s]Extractor Predicting: 154it [01:55,  1.45it/s]Extractor Predicting: 155it [01:56,  1.42it/s]Extractor Predicting: 156it [01:56,  1.44it/s]Extractor Predicting: 157it [01:57,  1.46it/s]Extractor Predicting: 158it [01:58,  1.45it/s]Extractor Predicting: 159it [01:58,  1.50it/s]Extractor Predicting: 160it [01:59,  1.53it/s]Extractor Predicting: 161it [02:00,  1.48it/s]Extractor Predicting: 162it [02:00,  1.44it/s]Extractor Predicting: 163it [02:01,  1.43it/s]Extractor Predicting: 164it [02:02,  1.44it/s]Extractor Predicting: 165it [02:03,  1.45it/s]Extractor Predicting: 166it [02:03,  1.47it/s]Extractor Predicting: 167it [02:04,  1.46it/s]Extractor Predicting: 168it [02:05,  1.44it/s]Extractor Predicting: 169it [02:05,  1.47it/s]Extractor Predicting: 170it [02:06,  1.43it/s]Extractor Predicting: 171it [02:07,  1.47it/s]Extractor Predicting: 172it [02:07,  1.47it/s]Extractor Predicting: 173it [02:08,  1.47it/s]Extractor Predicting: 174it [02:09,  1.39it/s]Extractor Predicting: 175it [02:10,  1.36it/s]Extractor Predicting: 176it [02:10,  1.35it/s]Extractor Predicting: 177it [02:11,  1.34it/s]Extractor Predicting: 178it [02:12,  1.31it/s]Extractor Predicting: 179it [02:13,  1.30it/s]Extractor Predicting: 180it [02:13,  1.32it/s]Extractor Predicting: 181it [02:14,  1.31it/s]Extractor Predicting: 182it [02:15,  1.31it/s]Extractor Predicting: 183it [02:16,  1.32it/s]Extractor Predicting: 184it [02:16,  1.32it/s]Extractor Predicting: 185it [02:17,  1.32it/s]Extractor Predicting: 186it [02:18,  1.34it/s]Extractor Predicting: 187it [02:19,  1.33it/s]Extractor Predicting: 188it [02:20,  1.29it/s]Extractor Predicting: 189it [02:20,  1.31it/s]Extractor Predicting: 190it [02:21,  1.30it/s]Extractor Predicting: 191it [02:22,  1.28it/s]Extractor Predicting: 192it [02:23,  1.27it/s]Extractor Predicting: 193it [02:23,  1.26it/s]Extractor Predicting: 194it [02:24,  1.26it/s]Extractor Predicting: 195it [02:25,  1.28it/s]Extractor Predicting: 196it [02:26,  1.30it/s]Extractor Predicting: 197it [02:27,  1.20it/s]Extractor Predicting: 198it [02:28,  1.23it/s]Extractor Predicting: 199it [02:28,  1.26it/s]Extractor Predicting: 200it [02:29,  1.23it/s]Extractor Predicting: 201it [02:30,  1.24it/s]Extractor Predicting: 202it [02:31,  1.23it/s]Extractor Predicting: 203it [02:32,  1.22it/s]Extractor Predicting: 204it [02:32,  1.21it/s]Extractor Predicting: 205it [02:33,  1.22it/s]Extractor Predicting: 206it [02:34,  1.22it/s]Extractor Predicting: 207it [02:35,  1.24it/s]Extractor Predicting: 208it [02:36,  1.22it/s]Extractor Predicting: 209it [02:37,  1.19it/s]Extractor Predicting: 210it [02:37,  1.19it/s]Extractor Predicting: 211it [02:38,  1.18it/s]Extractor Predicting: 212it [02:39,  1.20it/s]Extractor Predicting: 213it [02:40,  1.21it/s]Extractor Predicting: 214it [02:41,  1.24it/s]Extractor Predicting: 215it [02:41,  1.21it/s]Extractor Predicting: 216it [02:42,  1.21it/s]Extractor Predicting: 217it [02:43,  1.23it/s]Extractor Predicting: 218it [02:44,  1.22it/s]Extractor Predicting: 219it [02:45,  1.22it/s]Extractor Predicting: 220it [02:46,  1.18it/s]Extractor Predicting: 221it [02:47,  1.18it/s]Extractor Predicting: 222it [02:47,  1.18it/s]Extractor Predicting: 223it [02:48,  1.22it/s]Extractor Predicting: 224it [02:49,  1.23it/s]Extractor Predicting: 225it [02:50,  1.24it/s]Extractor Predicting: 226it [02:51,  1.21it/s]Extractor Predicting: 227it [02:51,  1.23it/s]Extractor Predicting: 228it [02:52,  1.24it/s]Extractor Predicting: 229it [02:53,  1.27it/s]Extractor Predicting: 230it [02:54,  1.27it/s]Extractor Predicting: 231it [02:54,  1.31it/s]Extractor Predicting: 232it [02:55,  1.35it/s]Extractor Predicting: 233it [02:56,  1.37it/s]Extractor Predicting: 234it [02:57,  1.33it/s]Extractor Predicting: 235it [02:57,  1.32it/s]Extractor Predicting: 236it [02:58,  1.33it/s]Extractor Predicting: 237it [02:59,  1.33it/s]Extractor Predicting: 238it [03:00,  1.33it/s]Extractor Predicting: 239it [03:00,  1.31it/s]Extractor Predicting: 240it [03:01,  1.33it/s]Extractor Predicting: 241it [03:02,  1.35it/s]Extractor Predicting: 242it [03:03,  1.38it/s]Extractor Predicting: 243it [03:03,  1.36it/s]Extractor Predicting: 244it [03:04,  1.41it/s]Extractor Predicting: 245it [03:05,  1.36it/s]Extractor Predicting: 246it [03:05,  1.34it/s]Extractor Predicting: 247it [03:06,  1.32it/s]Extractor Predicting: 248it [03:07,  1.31it/s]Extractor Predicting: 249it [03:08,  1.32it/s]Extractor Predicting: 250it [03:09,  1.32it/s]Extractor Predicting: 251it [03:09,  1.34it/s]Extractor Predicting: 252it [03:10,  1.37it/s]Extractor Predicting: 253it [03:11,  1.36it/s]Extractor Predicting: 254it [03:12,  1.30it/s]Extractor Predicting: 255it [03:12,  1.31it/s]Extractor Predicting: 256it [03:13,  1.31it/s]Extractor Predicting: 257it [03:14,  1.28it/s]Extractor Predicting: 258it [03:15,  1.28it/s]Extractor Predicting: 259it [03:15,  1.29it/s]Extractor Predicting: 260it [03:16,  1.28it/s]Extractor Predicting: 261it [03:17,  1.28it/s]Extractor Predicting: 262it [03:18,  1.29it/s]Extractor Predicting: 263it [03:19,  1.32it/s]Extractor Predicting: 264it [03:19,  1.30it/s]Extractor Predicting: 265it [03:20,  1.30it/s]Extractor Predicting: 266it [03:21,  1.27it/s]Extractor Predicting: 267it [03:22,  1.27it/s]Extractor Predicting: 268it [03:23,  1.26it/s]Extractor Predicting: 269it [03:23,  1.27it/s]Extractor Predicting: 270it [03:24,  1.26it/s]Extractor Predicting: 271it [03:25,  1.28it/s]Extractor Predicting: 272it [03:26,  1.30it/s]Extractor Predicting: 273it [03:26,  1.26it/s]Extractor Predicting: 274it [03:27,  1.24it/s]Extractor Predicting: 275it [03:28,  1.25it/s]Extractor Predicting: 276it [03:29,  1.26it/s]Extractor Predicting: 277it [03:30,  1.27it/s]Extractor Predicting: 278it [03:30,  1.27it/s]Extractor Predicting: 279it [03:31,  1.26it/s]Extractor Predicting: 280it [03:32,  1.27it/s]Extractor Predicting: 281it [03:33,  1.30it/s]Extractor Predicting: 282it [03:34,  1.27it/s]Extractor Predicting: 283it [03:34,  1.25it/s]Extractor Predicting: 284it [03:35,  1.31it/s]Extractor Predicting: 285it [03:36,  1.29it/s]Extractor Predicting: 286it [03:37,  1.31it/s]Extractor Predicting: 287it [03:37,  1.31it/s]Extractor Predicting: 288it [03:38,  1.30it/s]Extractor Predicting: 289it [03:39,  1.30it/s]Extractor Predicting: 290it [03:40,  1.28it/s]Extractor Predicting: 291it [03:41,  1.24it/s]Extractor Predicting: 292it [03:41,  1.24it/s]Extractor Predicting: 293it [03:42,  1.27it/s]Extractor Predicting: 294it [03:43,  1.23it/s]Extractor Predicting: 295it [03:44,  1.26it/s]Extractor Predicting: 296it [03:45,  1.25it/s]Extractor Predicting: 297it [03:45,  1.27it/s]Extractor Predicting: 298it [03:46,  1.25it/s]Extractor Predicting: 299it [03:47,  1.23it/s]Extractor Predicting: 300it [03:48,  1.26it/s]Extractor Predicting: 301it [03:49,  1.25it/s]Extractor Predicting: 302it [03:49,  1.28it/s]Extractor Predicting: 303it [03:50,  1.25it/s]Extractor Predicting: 304it [03:51,  1.14it/s]Extractor Predicting: 305it [03:52,  1.16it/s]Extractor Predicting: 306it [03:53,  1.22it/s]Extractor Predicting: 307it [03:54,  1.23it/s]Extractor Predicting: 308it [03:54,  1.26it/s]Extractor Predicting: 309it [03:55,  1.25it/s]Extractor Predicting: 310it [03:56,  1.27it/s]Extractor Predicting: 311it [03:57,  1.26it/s]Extractor Predicting: 312it [03:57,  1.30it/s]Extractor Predicting: 313it [03:58,  1.33it/s]Extractor Predicting: 314it [03:59,  1.35it/s]Extractor Predicting: 315it [04:00,  1.36it/s]Extractor Predicting: 316it [04:00,  1.34it/s]Extractor Predicting: 317it [04:01,  1.32it/s]Extractor Predicting: 318it [04:02,  1.30it/s]Extractor Predicting: 319it [04:03,  1.29it/s]Extractor Predicting: 320it [04:03,  1.28it/s]Extractor Predicting: 321it [04:04,  1.29it/s]Extractor Predicting: 322it [04:05,  1.27it/s]Extractor Predicting: 323it [04:06,  1.24it/s]Extractor Predicting: 324it [04:07,  1.25it/s]Extractor Predicting: 325it [04:07,  1.25it/s]Extractor Predicting: 326it [04:08,  1.25it/s]Extractor Predicting: 327it [04:09,  1.29it/s]Extractor Predicting: 328it [04:10,  1.28it/s]Extractor Predicting: 329it [04:11,  1.30it/s]Extractor Predicting: 330it [04:11,  1.28it/s]Extractor Predicting: 331it [04:12,  1.27it/s]Extractor Predicting: 332it [04:13,  1.27it/s]Extractor Predicting: 333it [04:14,  1.30it/s]Extractor Predicting: 334it [04:14,  1.28it/s]Extractor Predicting: 335it [04:15,  1.30it/s]Extractor Predicting: 336it [04:16,  1.28it/s]Extractor Predicting: 337it [04:17,  1.29it/s]Extractor Predicting: 338it [04:18,  1.29it/s]Extractor Predicting: 339it [04:18,  1.33it/s]Extractor Predicting: 340it [04:19,  1.34it/s]Extractor Predicting: 341it [04:20,  1.34it/s]Extractor Predicting: 342it [04:21,  1.32it/s]Extractor Predicting: 343it [04:21,  1.32it/s]Extractor Predicting: 344it [04:22,  1.34it/s]Extractor Predicting: 345it [04:23,  1.29it/s]Extractor Predicting: 346it [04:24,  1.29it/s]Extractor Predicting: 347it [04:24,  1.30it/s]Extractor Predicting: 348it [04:25,  1.30it/s]Extractor Predicting: 349it [04:26,  1.32it/s]Extractor Predicting: 350it [04:27,  1.32it/s]Extractor Predicting: 351it [04:27,  1.34it/s]Extractor Predicting: 352it [04:28,  1.31it/s]Extractor Predicting: 353it [04:29,  1.32it/s]Extractor Predicting: 354it [04:30,  1.35it/s]Extractor Predicting: 355it [04:30,  1.37it/s]Extractor Predicting: 356it [04:31,  1.35it/s]Extractor Predicting: 357it [04:32,  1.35it/s]Extractor Predicting: 358it [04:33,  1.34it/s]Extractor Predicting: 359it [04:33,  1.32it/s]Extractor Predicting: 360it [04:34,  1.33it/s]Extractor Predicting: 361it [04:35,  1.34it/s]Extractor Predicting: 362it [04:36,  1.33it/s]Extractor Predicting: 363it [04:36,  1.35it/s]Extractor Predicting: 364it [04:37,  1.37it/s]Extractor Predicting: 365it [04:38,  1.38it/s]Extractor Predicting: 366it [04:38,  1.36it/s]Extractor Predicting: 367it [04:39,  1.31it/s]Extractor Predicting: 368it [04:40,  1.31it/s]Extractor Predicting: 369it [04:41,  1.32it/s]Extractor Predicting: 370it [04:41,  1.36it/s]Extractor Predicting: 371it [04:42,  1.35it/s]Extractor Predicting: 372it [04:43,  1.36it/s]Extractor Predicting: 373it [04:44,  1.34it/s]Extractor Predicting: 374it [04:44,  1.35it/s]Extractor Predicting: 375it [04:45,  1.35it/s]Extractor Predicting: 376it [04:46,  1.35it/s]Extractor Predicting: 377it [04:47,  1.36it/s]Extractor Predicting: 378it [04:47,  1.39it/s]Extractor Predicting: 379it [04:48,  1.34it/s]Extractor Predicting: 380it [04:49,  1.34it/s]Extractor Predicting: 381it [04:50,  1.35it/s]Extractor Predicting: 382it [04:50,  1.35it/s]Extractor Predicting: 383it [04:51,  1.37it/s]Extractor Predicting: 384it [04:52,  1.40it/s]Extractor Predicting: 385it [04:53,  1.37it/s]Extractor Predicting: 386it [04:53,  1.36it/s]Extractor Predicting: 387it [04:54,  1.36it/s]Extractor Predicting: 388it [04:55,  1.35it/s]Extractor Predicting: 389it [04:55,  1.36it/s]Extractor Predicting: 390it [04:56,  1.35it/s]Extractor Predicting: 391it [04:57,  1.33it/s]Extractor Predicting: 392it [04:58,  1.33it/s]Extractor Predicting: 393it [04:58,  1.37it/s]Extractor Predicting: 394it [04:59,  1.31it/s]Extractor Predicting: 395it [05:00,  1.25it/s]Extractor Predicting: 396it [05:01,  1.24it/s]Extractor Predicting: 397it [05:02,  1.24it/s]Extractor Predicting: 398it [05:03,  1.25it/s]Extractor Predicting: 399it [05:03,  1.24it/s]Extractor Predicting: 400it [05:04,  1.28it/s]Extractor Predicting: 401it [05:05,  1.26it/s]Extractor Predicting: 402it [05:06,  1.24it/s]Extractor Predicting: 403it [05:07,  1.13it/s]Extractor Predicting: 404it [05:08,  1.12it/s]Extractor Predicting: 405it [05:09,  1.15it/s]Extractor Predicting: 406it [05:09,  1.17it/s]Extractor Predicting: 407it [05:10,  1.15it/s]Extractor Predicting: 408it [05:11,  1.20it/s]Extractor Predicting: 409it [05:12,  1.22it/s]Extractor Predicting: 410it [05:13,  1.21it/s]Extractor Predicting: 411it [05:14,  1.19it/s]Extractor Predicting: 412it [05:14,  1.21it/s]Extractor Predicting: 413it [05:15,  1.24it/s]Extractor Predicting: 414it [05:16,  1.27it/s]Extractor Predicting: 415it [05:17,  1.28it/s]Extractor Predicting: 416it [05:17,  1.29it/s]Extractor Predicting: 417it [05:18,  1.29it/s]Extractor Predicting: 418it [05:19,  1.29it/s]Extractor Predicting: 419it [05:20,  1.23it/s]Extractor Predicting: 420it [05:21,  1.24it/s]Extractor Predicting: 421it [05:21,  1.33it/s]Extractor Predicting: 421it [05:21,  1.31it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:38:10,183 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:38:10,210 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:38:10,210 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:38:10,210 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:38:10,210 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:38:11,003 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:38:11,004 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:38:11,618 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:38:12,759 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:38:12,759 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:38:15,690 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:38:15,708 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:38:15,709 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:38:15,709 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:38:15,709 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:38:16,471 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:38:16,472 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:38:17,093 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:38:17,333 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:38:17,333 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.17047207651959273,
  "recall": 0.10946012877662209,
  "score": 0.13331724678771792,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.21it/s]Extractor Predicting: 2it [00:01,  1.20it/s]Extractor Predicting: 3it [00:02,  1.20it/s]Extractor Predicting: 4it [00:03,  1.19it/s]Extractor Predicting: 5it [00:04,  1.24it/s]Extractor Predicting: 6it [00:04,  1.22it/s]Extractor Predicting: 7it [00:05,  1.21it/s]Extractor Predicting: 8it [00:06,  1.24it/s]Extractor Predicting: 9it [00:06,  1.62it/s]Extractor Predicting: 9it [00:06,  1.33it/s]
[INFO|configuration_utils.py:515] 2023-08-29 02:38:25,658 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:38:25,685 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 02:38:25,730 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:38:25,731 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 02:38:25,760 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 02:38:35,699 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 02:38:35,738 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 02:38:35,954 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:38:35,955 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 02:38:36,172 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:38:36,302 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:38:36,302 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:38:36,303 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:38:36,303 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:38:36,303 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:38:36,303 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.18518518518518517,
  "recall": 0.04938271604938271,
  "score": 0.07797270955165692,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 02:38:36,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:37,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:38,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:38,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:39,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:40,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:40,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:41,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:41,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:42,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:43,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:43,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:44,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:45,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:45,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:46,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:47,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:48,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:48,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:49,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:50,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:50,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:52,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:53,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:54,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:18<05:45, 18.17s/it][WARNING|generation_utils.py:914] 2023-08-29 02:38:54,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:55,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:56,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:56,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:57,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:58,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:59,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:38:59,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:00,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:01,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:01,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:02,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:03,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:03,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:04,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:05,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:06,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:06,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:07,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:07,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:08,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:09,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:10,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:34<05:07, 17.06s/it][WARNING|generation_utils.py:914] 2023-08-29 02:39:11,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:11,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:12,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:13,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:13,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:14,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:15,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:16,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:16,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:17,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:18,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:18,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:19,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:19,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:20,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:21,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:22,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:22,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:23,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:24,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:24,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:48<04:28, 15.80s/it][WARNING|generation_utils.py:914] 2023-08-29 02:39:25,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:26,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:27,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:27,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:28,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:29,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:30,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:31,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:31,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:32,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:33,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:34,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:34,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:35,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:36,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:37,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:37,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:38,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:39,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:40,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:40,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:04<04:13, 15.87s/it][WARNING|generation_utils.py:914] 2023-08-29 02:39:41,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:42,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:42,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:43,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:44,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:45,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:45,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:46,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:47,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:48,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:48,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:49,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:50,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:51,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:51,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:52,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:53,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:54,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:55,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:56,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:56,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:20<03:58, 15.92s/it][WARNING|generation_utils.py:914] 2023-08-29 02:39:57,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:58,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:59,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:59,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:00,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:01,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:02,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:02,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:03,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:04,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:05,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:05,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:06,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:07,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:07,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:08,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:10,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:10,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:11,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:12,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:36<03:40, 15.73s/it][WARNING|generation_utils.py:914] 2023-08-29 02:40:12,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:13,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:14,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:15,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:16,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:16,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:17,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:18,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:19,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:19,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:20,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:21,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:21,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:22,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:23,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:24,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:24,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:25,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:26,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:26,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:51<03:21, 15.47s/it][WARNING|generation_utils.py:914] 2023-08-29 02:40:27,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:28,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:29,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:30,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:31,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:32,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:33,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:34,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:34,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:35,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:36,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:37,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:38,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:39,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:40,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:40,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:41,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:42,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:43,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:44,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:45,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:45,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:09<03:18, 16.57s/it][WARNING|generation_utils.py:914] 2023-08-29 02:40:46,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:47,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:48,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:48,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:49,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:50,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:51,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:51,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:52,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:53,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:54,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:55,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:56,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:56,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:57,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:58,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:59,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:00,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:01,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:01,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:02,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:26<03:02, 16.59s/it][WARNING|generation_utils.py:914] 2023-08-29 02:41:03,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:04,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:05,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:05,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:06,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:07,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:08,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:08,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:09,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:10,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:11,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:11,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:12,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:13,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:14,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:15,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:16,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:17,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:17,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:18,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:19,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:43<02:45, 16.60s/it][WARNING|generation_utils.py:914] 2023-08-29 02:41:19,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:20,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:21,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:22,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:22,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:23,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:24,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:24,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:25,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:26,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:27,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:27,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:28,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:29,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:30,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:30,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:31,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:32,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:33,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:33,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:34,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:58<02:26, 16.28s/it][WARNING|generation_utils.py:914] 2023-08-29 02:41:35,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:36,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:37,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:37,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:38,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:39,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:39,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:40,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:41,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:42,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:42,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:43,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:44,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:45,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:45,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:46,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:47,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:47,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:48,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:49,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:50,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:14<02:08, 16.08s/it][WARNING|generation_utils.py:914] 2023-08-29 02:41:51,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:51,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:52,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:53,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:54,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:55,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:56,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:56,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:57,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:58,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:59,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:59,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:00,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:01,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:02,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:02,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:03,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:04,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:04,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:05,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:06,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:30<01:52, 16.07s/it][WARNING|generation_utils.py:914] 2023-08-29 02:42:07,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:07,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:08,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:08,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:09,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:10,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:10,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:11,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:12,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:12,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:13,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:14,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:14,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:15,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:16,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:16,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:17,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:18,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:19,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:19,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:43<01:31, 15.18s/it][WARNING|generation_utils.py:914] 2023-08-29 02:42:20,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:21,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:21,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:22,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:23,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:24,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:25,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:25,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:26,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:27,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:27,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:28,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:29,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:30,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:31,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:31,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:32,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:33,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:34,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:35,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:35,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:59<01:17, 15.50s/it][WARNING|generation_utils.py:914] 2023-08-29 02:42:36,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:37,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:37,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:38,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:39,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:40,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:41,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:41,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:42,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:43,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:43,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:44,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:45,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:46,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:46,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:47,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:48,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:48,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:49,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:50,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:51,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:15<01:01, 15.45s/it][WARNING|generation_utils.py:914] 2023-08-29 02:42:51,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:52,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:53,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:53,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:54,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:55,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:55,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:56,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:57,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:57,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:58,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:59,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:59,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:00,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:01,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:02,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:02,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:03,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:04,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:04,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:05,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:05,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:29<00:45, 15.19s/it][WARNING|generation_utils.py:914] 2023-08-29 02:43:06,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:07,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:07,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:08,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:09,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:10,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:11,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:11,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:12,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:13,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:13,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:14,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:15,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:16,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:17,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:17,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:18,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:19,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:19,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:20,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:21,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:22,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:22,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:46<00:31, 15.73s/it][WARNING|generation_utils.py:914] 2023-08-29 02:43:23,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:24,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:25,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:25,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:26,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:27,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:28,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:28,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:29,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:30,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:31,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:31,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:32,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:33,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:34,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:34,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:35,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:36,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:37,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:38,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:38,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:02<00:15, 15.89s/it][WARNING|generation_utils.py:914] 2023-08-29 02:43:39,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:40,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:41,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:42,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:42,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:43,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:44,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:45,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:46,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:46,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:47,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:48,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:49,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:50,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:50,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:51,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:52,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:53,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:54,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:54,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:19<00:00, 15.95s/it]Generating: 100%|██████████| 20/20 [05:19<00:00, 15.95s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:05,972 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:05,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:05,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:05,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:05,994 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:44:06,766 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:44:06,767 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:44:07,360 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:44:08,484 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:44:08,484 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:11,538 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:11,540 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:11,540 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:11,540 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:11,540 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:44:12,412 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:44:12,437 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:44:12,739 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:44:12,954 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:44:12,954 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 192, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 395, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 470, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 521, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 594, 'raw': 768}
{'target': 600, 'success': 617, 'raw': 800}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.77125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : genre .', 'success_rate': 0.8464673913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : head of government .', 'success_rate': 0.9241071428571429, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : military branch .', 'success_rate': 0.9092261904761905, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : winner .', 'success_rate': 0.9151785714285714, 'errors': {'', '(\'B.J. Dyson\', \'winner\', \'\', "He finished second behind eventual runner-up B.J. Dyson in the men \'s event , but failed to secure a single silver medal in the event .")'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : characters .', 'success_rate': 0.9578125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : crosses .', 'success_rate': 0.8607954545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8958333333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9345238095238095, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9136904761904762, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.90625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Linux\', \'operating system\', \'\', \'The operating system is based on " Linux " , a free . The operating system is based on " Linux " , a free .\')'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : participant .', 'success_rate': 0.9032738095238095, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : participating team .', 'success_rate': 0.9181547619047619, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : platform .', 'success_rate': 0.875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.842391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : publisher .', 'success_rate': 0.9092261904761905, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : spouse .', 'success_rate': 0.9390625, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/synthetic/2_ext.jsonl'}}
estimate vocab size: 10756
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10856, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.50it/s]Extractor Estimating: 2it [00:01,  1.51it/s]Extractor Estimating: 3it [00:01,  1.54it/s]Extractor Estimating: 4it [00:02,  1.57it/s]Extractor Estimating: 5it [00:03,  1.62it/s]Extractor Estimating: 6it [00:03,  1.58it/s]Extractor Estimating: 7it [00:04,  1.58it/s]Extractor Estimating: 8it [00:05,  1.58it/s]Extractor Estimating: 9it [00:05,  1.61it/s]Extractor Estimating: 10it [00:06,  1.63it/s]Extractor Estimating: 11it [00:06,  1.57it/s]Extractor Estimating: 12it [00:07,  1.49it/s]Extractor Estimating: 13it [00:08,  1.57it/s]Extractor Estimating: 14it [00:08,  1.56it/s]Extractor Estimating: 15it [00:09,  1.55it/s]Extractor Estimating: 16it [00:10,  1.50it/s]Extractor Estimating: 17it [00:10,  1.52it/s]Extractor Estimating: 18it [00:11,  1.60it/s]Extractor Estimating: 19it [00:12,  1.60it/s]Extractor Estimating: 20it [00:12,  1.59it/s]Extractor Estimating: 21it [00:13,  1.55it/s]Extractor Estimating: 22it [00:14,  1.58it/s]Extractor Estimating: 23it [00:14,  1.52it/s]Extractor Estimating: 24it [00:15,  1.56it/s]Extractor Estimating: 25it [00:15,  1.56it/s]Extractor Estimating: 26it [00:16,  1.46it/s]Extractor Estimating: 27it [00:17,  1.42it/s]Extractor Estimating: 28it [00:18,  1.36it/s]Extractor Estimating: 29it [00:19,  1.35it/s]Extractor Estimating: 30it [00:19,  1.34it/s]Extractor Estimating: 31it [00:20,  1.31it/s]Extractor Estimating: 32it [00:21,  1.32it/s]Extractor Estimating: 33it [00:22,  1.33it/s]Extractor Estimating: 34it [00:22,  1.31it/s]Extractor Estimating: 35it [00:23,  1.24it/s]Extractor Estimating: 36it [00:24,  1.26it/s]Extractor Estimating: 37it [00:25,  1.31it/s]Extractor Estimating: 38it [00:25,  1.35it/s]Extractor Estimating: 39it [00:26,  1.35it/s]Extractor Estimating: 40it [00:27,  1.26it/s]Extractor Estimating: 41it [00:28,  1.29it/s]Extractor Estimating: 42it [00:29,  1.34it/s]Extractor Estimating: 43it [00:29,  1.34it/s]Extractor Estimating: 44it [00:30,  1.31it/s]Extractor Estimating: 45it [00:31,  1.36it/s]Extractor Estimating: 46it [00:31,  1.39it/s]Extractor Estimating: 47it [00:32,  1.38it/s]Extractor Estimating: 48it [00:33,  1.32it/s]Extractor Estimating: 49it [00:34,  1.34it/s]Extractor Estimating: 50it [00:35,  1.33it/s]Extractor Estimating: 51it [00:35,  1.35it/s]Extractor Estimating: 52it [00:36,  1.40it/s]Extractor Estimating: 53it [00:37,  1.40it/s]Extractor Estimating: 54it [00:37,  1.41it/s]Extractor Estimating: 55it [00:38,  1.43it/s]Extractor Estimating: 56it [00:39,  1.50it/s]Extractor Estimating: 57it [00:39,  1.51it/s]Extractor Estimating: 58it [00:40,  1.50it/s]Extractor Estimating: 59it [00:41,  1.45it/s]Extractor Estimating: 60it [00:41,  1.47it/s]Extractor Estimating: 61it [00:42,  1.47it/s]Extractor Estimating: 62it [00:43,  1.47it/s]Extractor Estimating: 63it [00:43,  1.46it/s]Extractor Estimating: 64it [00:44,  1.44it/s]Extractor Estimating: 65it [00:45,  1.48it/s]Extractor Estimating: 66it [00:45,  1.49it/s]Extractor Estimating: 67it [00:46,  1.49it/s]Extractor Estimating: 68it [00:47,  1.44it/s]Extractor Estimating: 69it [00:48,  1.41it/s]Extractor Estimating: 70it [00:48,  1.41it/s]Extractor Estimating: 71it [00:49,  1.46it/s]Extractor Estimating: 72it [00:50,  1.45it/s]Extractor Estimating: 73it [00:50,  1.51it/s]Extractor Estimating: 74it [00:51,  1.43it/s]Extractor Estimating: 75it [00:52,  1.48it/s]Extractor Estimating: 76it [00:52,  1.43it/s]Extractor Estimating: 77it [00:53,  1.41it/s]Extractor Estimating: 78it [00:54,  1.33it/s]Extractor Estimating: 79it [00:55,  1.33it/s]Extractor Estimating: 80it [00:55,  1.33it/s]Extractor Estimating: 81it [00:56,  1.35it/s]Extractor Estimating: 82it [00:57,  1.35it/s]Extractor Estimating: 83it [00:58,  1.23it/s]Extractor Estimating: 84it [00:59,  1.27it/s]Extractor Estimating: 85it [00:59,  1.27it/s]Extractor Estimating: 86it [01:00,  1.33it/s]Extractor Estimating: 87it [01:01,  1.31it/s]Extractor Estimating: 88it [01:02,  1.32it/s]Extractor Estimating: 89it [01:02,  1.28it/s]Extractor Estimating: 90it [01:03,  1.29it/s]Extractor Estimating: 91it [01:04,  1.31it/s]Extractor Estimating: 92it [01:05,  1.25it/s]Extractor Estimating: 93it [01:06,  1.25it/s]Extractor Estimating: 94it [01:06,  1.27it/s]Extractor Estimating: 95it [01:07,  1.28it/s]Extractor Estimating: 96it [01:08,  1.29it/s]Extractor Estimating: 97it [01:09,  1.26it/s]Extractor Estimating: 98it [01:09,  1.29it/s]Extractor Estimating: 99it [01:10,  1.31it/s]Extractor Estimating: 100it [01:11,  1.34it/s]Extractor Estimating: 101it [01:12,  1.34it/s]Extractor Estimating: 102it [01:12,  1.33it/s]Extractor Estimating: 103it [01:13,  1.36it/s]Extractor Estimating: 104it [01:14,  1.38it/s]Extractor Estimating: 105it [01:15,  1.38it/s]Extractor Estimating: 106it [01:15,  1.37it/s]Extractor Estimating: 107it [01:16,  1.35it/s]Extractor Estimating: 108it [01:17,  1.36it/s]Extractor Estimating: 109it [01:17,  1.39it/s]Extractor Estimating: 110it [01:18,  1.43it/s]Extractor Estimating: 111it [01:19,  1.33it/s]Extractor Estimating: 112it [01:20,  1.37it/s]Extractor Estimating: 113it [01:20,  1.38it/s]Extractor Estimating: 114it [01:21,  1.40it/s]Extractor Estimating: 115it [01:22,  1.43it/s]Extractor Estimating: 116it [01:22,  1.41it/s]Extractor Estimating: 117it [01:23,  1.43it/s]Extractor Estimating: 118it [01:24,  1.41it/s]Extractor Estimating: 119it [01:25,  1.41it/s]Extractor Estimating: 120it [01:25,  1.37it/s]Extractor Estimating: 121it [01:26,  1.29it/s]Extractor Estimating: 122it [01:27,  1.33it/s]Extractor Estimating: 123it [01:28,  1.32it/s]Extractor Estimating: 124it [01:28,  1.32it/s]Extractor Estimating: 125it [01:29,  1.35it/s]Extractor Estimating: 126it [01:30,  1.34it/s]Extractor Estimating: 127it [01:31,  1.31it/s]Extractor Estimating: 128it [01:32,  1.26it/s]Extractor Estimating: 129it [01:32,  1.28it/s]Extractor Estimating: 130it [01:33,  1.21it/s]Extractor Estimating: 131it [01:34,  1.27it/s]Extractor Estimating: 132it [01:35,  1.23it/s]Extractor Estimating: 133it [01:36,  1.26it/s]Extractor Estimating: 134it [01:36,  1.23it/s]Extractor Estimating: 135it [01:37,  1.23it/s]Extractor Estimating: 136it [01:38,  1.25it/s]Extractor Estimating: 137it [01:39,  1.30it/s]Extractor Estimating: 138it [01:39,  1.32it/s]Extractor Estimating: 139it [01:40,  1.32it/s]Extractor Estimating: 140it [01:41,  1.31it/s]Extractor Estimating: 141it [01:42,  1.30it/s]Extractor Estimating: 142it [01:43,  1.30it/s]Extractor Estimating: 143it [01:43,  1.33it/s]Extractor Estimating: 144it [01:44,  1.32it/s]Extractor Estimating: 145it [01:45,  1.32it/s]Extractor Estimating: 146it [01:46,  1.30it/s]Extractor Estimating: 147it [01:46,  1.32it/s]Extractor Estimating: 148it [01:47,  1.37it/s]Extractor Estimating: 149it [01:48,  1.35it/s]Extractor Estimating: 150it [01:49,  1.32it/s]Extractor Estimating: 151it [01:49,  1.35it/s]Extractor Estimating: 152it [01:50,  1.39it/s]Extractor Estimating: 153it [01:51,  1.44it/s]Extractor Estimating: 154it [01:51,  1.44it/s]Extractor Estimating: 155it [01:52,  1.46it/s]Extractor Estimating: 156it [01:53,  1.50it/s]Extractor Estimating: 157it [01:53,  1.51it/s]Extractor Estimating: 158it [01:54,  1.53it/s]Extractor Estimating: 159it [01:54,  1.51it/s]Extractor Estimating: 160it [01:55,  1.46it/s]Extractor Estimating: 161it [01:56,  1.49it/s]Extractor Estimating: 162it [01:56,  1.51it/s]Extractor Estimating: 163it [01:57,  1.50it/s]Extractor Estimating: 164it [01:58,  1.54it/s]Extractor Estimating: 165it [01:58,  1.51it/s]Extractor Estimating: 166it [01:59,  1.49it/s]Extractor Estimating: 167it [02:00,  1.44it/s]Extractor Estimating: 168it [02:01,  1.43it/s]Extractor Estimating: 169it [02:01,  1.47it/s]Extractor Estimating: 170it [02:02,  1.50it/s]Extractor Estimating: 171it [02:03,  1.50it/s]Extractor Estimating: 172it [02:03,  1.50it/s]Extractor Estimating: 173it [02:04,  1.49it/s]Extractor Estimating: 174it [02:05,  1.49it/s]Extractor Estimating: 175it [02:05,  1.46it/s]Extractor Estimating: 176it [02:06,  1.45it/s]Extractor Estimating: 177it [02:07,  1.41it/s]Extractor Estimating: 178it [02:08,  1.37it/s]Extractor Estimating: 179it [02:08,  1.35it/s]Extractor Estimating: 180it [02:09,  1.28it/s]Extractor Estimating: 181it [02:10,  1.27it/s]Extractor Estimating: 182it [02:11,  1.28it/s]Extractor Estimating: 183it [02:12,  1.25it/s]Extractor Estimating: 184it [02:12,  1.27it/s]Extractor Estimating: 185it [02:13,  1.26it/s]Extractor Estimating: 186it [02:14,  1.27it/s]Extractor Estimating: 187it [02:15,  1.28it/s]Extractor Estimating: 188it [02:15,  1.27it/s]Extractor Estimating: 189it [02:16,  1.29it/s]Extractor Estimating: 190it [02:17,  1.31it/s]Extractor Estimating: 191it [02:18,  1.30it/s]Extractor Estimating: 192it [02:18,  1.31it/s]Extractor Estimating: 193it [02:19,  1.33it/s]Extractor Estimating: 194it [02:20,  1.30it/s]Extractor Estimating: 195it [02:21,  1.28it/s]Extractor Estimating: 196it [02:22,  1.28it/s]Extractor Estimating: 197it [02:22,  1.33it/s]Extractor Estimating: 198it [02:23,  1.29it/s]Extractor Estimating: 199it [02:24,  1.31it/s]Extractor Estimating: 200it [02:25,  1.32it/s]Extractor Estimating: 201it [02:25,  1.37it/s]Extractor Estimating: 202it [02:26,  1.38it/s]Extractor Estimating: 203it [02:27,  1.32it/s]Extractor Estimating: 204it [02:28,  1.20it/s]Extractor Estimating: 205it [02:29,  1.21it/s]Extractor Estimating: 206it [02:29,  1.25it/s]Extractor Estimating: 207it [02:30,  1.23it/s]Extractor Estimating: 208it [02:31,  1.25it/s]Extractor Estimating: 209it [02:32,  1.30it/s]Extractor Estimating: 210it [02:32,  1.29it/s]Extractor Estimating: 211it [02:33,  1.27it/s]Extractor Estimating: 212it [02:34,  1.29it/s]Extractor Estimating: 213it [02:35,  1.33it/s]Extractor Estimating: 214it [02:35,  1.34it/s]Extractor Estimating: 215it [02:36,  1.29it/s]Extractor Estimating: 216it [02:37,  1.26it/s]Extractor Estimating: 217it [02:38,  1.31it/s]Extractor Estimating: 218it [02:39,  1.35it/s]Extractor Estimating: 219it [02:39,  1.34it/s]Extractor Estimating: 220it [02:40,  1.33it/s]Extractor Estimating: 221it [02:41,  1.32it/s]Extractor Estimating: 222it [02:42,  1.30it/s]Extractor Estimating: 223it [02:42,  1.30it/s]Extractor Estimating: 224it [02:43,  1.32it/s]Extractor Estimating: 225it [02:44,  1.27it/s]Extractor Estimating: 226it [02:45,  1.27it/s]Extractor Estimating: 227it [02:46,  1.29it/s]Extractor Estimating: 228it [02:46,  1.30it/s]Extractor Estimating: 229it [02:47,  1.29it/s]Extractor Estimating: 230it [02:48,  1.30it/s]Extractor Estimating: 231it [02:49,  1.32it/s]Extractor Estimating: 232it [02:49,  1.33it/s]Extractor Estimating: 233it [02:50,  1.32it/s]Extractor Estimating: 234it [02:51,  1.37it/s]Extractor Estimating: 235it [02:51,  1.39it/s]Extractor Estimating: 236it [02:52,  1.37it/s]Extractor Estimating: 237it [02:53,  1.34it/s]Extractor Estimating: 238it [02:54,  1.35it/s]Extractor Estimating: 239it [02:54,  1.34it/s]Extractor Estimating: 240it [02:55,  1.32it/s]Extractor Estimating: 241it [02:56,  1.35it/s]Extractor Estimating: 242it [02:57,  1.34it/s]Extractor Estimating: 243it [02:58,  1.31it/s]Extractor Estimating: 244it [02:58,  1.33it/s]Extractor Estimating: 245it [02:59,  1.33it/s]Extractor Estimating: 246it [03:00,  1.30it/s]Extractor Estimating: 247it [03:01,  1.31it/s]Extractor Estimating: 248it [03:01,  1.31it/s]Extractor Estimating: 249it [03:02,  1.33it/s]Extractor Estimating: 250it [03:03,  1.37it/s]Extractor Estimating: 251it [03:03,  1.35it/s]Extractor Estimating: 252it [03:04,  1.33it/s]Extractor Estimating: 253it [03:05,  1.31it/s]Extractor Estimating: 254it [03:06,  1.36it/s]Extractor Estimating: 255it [03:06,  1.36it/s]Extractor Estimating: 256it [03:07,  1.37it/s]Extractor Estimating: 257it [03:08,  1.36it/s]Extractor Estimating: 258it [03:09,  1.36it/s]Extractor Estimating: 259it [03:09,  1.35it/s]Extractor Estimating: 260it [03:10,  1.32it/s]Extractor Estimating: 261it [03:11,  1.32it/s]Extractor Estimating: 262it [03:12,  1.27it/s]Extractor Estimating: 263it [03:13,  1.27it/s]Extractor Estimating: 264it [03:13,  1.25it/s]Extractor Estimating: 265it [03:14,  1.26it/s]Extractor Estimating: 266it [03:15,  1.26it/s]Extractor Estimating: 267it [03:16,  1.27it/s]Extractor Estimating: 268it [03:17,  1.25it/s]Extractor Estimating: 269it [03:17,  1.27it/s]Extractor Estimating: 270it [03:18,  1.25it/s]Extractor Estimating: 271it [03:19,  1.26it/s]Extractor Estimating: 272it [03:20,  1.28it/s]Extractor Estimating: 273it [03:21,  1.28it/s]Extractor Estimating: 274it [03:21,  1.21it/s]Extractor Estimating: 275it [03:22,  1.24it/s]Extractor Estimating: 276it [03:23,  1.31it/s]Extractor Estimating: 277it [03:24,  1.32it/s]Extractor Estimating: 278it [03:24,  1.34it/s]Extractor Estimating: 279it [03:25,  1.37it/s]Extractor Estimating: 280it [03:26,  1.41it/s]Extractor Estimating: 281it [03:26,  1.43it/s]Extractor Estimating: 282it [03:27,  1.45it/s]Extractor Estimating: 283it [03:28,  1.46it/s]Extractor Estimating: 284it [03:28,  1.42it/s]Extractor Estimating: 285it [03:29,  1.41it/s]Extractor Estimating: 286it [03:30,  1.41it/s]Extractor Estimating: 287it [03:31,  1.43it/s]Extractor Estimating: 288it [03:31,  1.40it/s]Extractor Estimating: 289it [03:32,  1.43it/s]Extractor Estimating: 290it [03:33,  1.42it/s]Extractor Estimating: 291it [03:33,  1.41it/s]Extractor Estimating: 292it [03:34,  1.40it/s]Extractor Estimating: 293it [03:35,  1.39it/s]Extractor Estimating: 294it [03:36,  1.41it/s]Extractor Estimating: 295it [03:36,  1.43it/s]Extractor Estimating: 296it [03:37,  1.40it/s]Extractor Estimating: 297it [03:38,  1.40it/s]Extractor Estimating: 298it [03:38,  1.35it/s]Extractor Estimating: 299it [03:39,  1.42it/s]Extractor Estimating: 300it [03:40,  1.42it/s]Extractor Estimating: 301it [03:41,  1.31it/s]Extractor Estimating: 302it [03:41,  1.33it/s]Extractor Estimating: 303it [03:42,  1.32it/s]Extractor Estimating: 304it [03:43,  1.35it/s]Extractor Estimating: 305it [03:44,  1.40it/s]Extractor Estimating: 306it [03:44,  1.33it/s]Extractor Estimating: 307it [03:45,  1.33it/s]Extractor Estimating: 308it [03:46,  1.35it/s]Extractor Estimating: 309it [03:47,  1.36it/s]Extractor Estimating: 310it [03:47,  1.38it/s]Extractor Estimating: 311it [03:48,  1.34it/s]Extractor Estimating: 312it [03:49,  1.31it/s]Extractor Estimating: 313it [03:50,  1.34it/s]Extractor Estimating: 314it [03:50,  1.36it/s]Extractor Estimating: 315it [03:51,  1.40it/s]Extractor Estimating: 316it [03:52,  1.42it/s]Extractor Estimating: 317it [03:52,  1.36it/s]Extractor Estimating: 318it [03:53,  1.36it/s]Extractor Estimating: 319it [03:54,  1.36it/s]Extractor Estimating: 320it [03:55,  1.41it/s]Extractor Estimating: 321it [03:55,  1.39it/s]Extractor Estimating: 322it [03:56,  1.38it/s]Extractor Estimating: 323it [03:57,  1.39it/s]Extractor Estimating: 324it [03:57,  1.44it/s]Extractor Estimating: 325it [03:58,  1.44it/s]Extractor Estimating: 326it [03:59,  1.47it/s]Extractor Estimating: 327it [03:59,  1.57it/s]Extractor Estimating: 328it [04:00,  1.57it/s]Extractor Estimating: 329it [04:00,  1.63it/s]Extractor Estimating: 330it [04:01,  1.62it/s]Extractor Estimating: 331it [04:02,  1.63it/s]Extractor Estimating: 332it [04:02,  1.60it/s]Extractor Estimating: 333it [04:03,  1.57it/s]Extractor Estimating: 334it [04:04,  1.59it/s]Extractor Estimating: 335it [04:04,  1.59it/s]Extractor Estimating: 336it [04:05,  1.59it/s]Extractor Estimating: 337it [04:06,  1.54it/s]Extractor Estimating: 338it [04:06,  1.54it/s]Extractor Estimating: 339it [04:07,  1.58it/s]Extractor Estimating: 340it [04:08,  1.52it/s]Extractor Estimating: 341it [04:08,  1.50it/s]Extractor Estimating: 342it [04:09,  1.46it/s]Extractor Estimating: 343it [04:10,  1.40it/s]Extractor Estimating: 344it [04:10,  1.53it/s]Extractor Estimating: 345it [04:11,  1.48it/s]Extractor Estimating: 346it [04:12,  1.48it/s]Extractor Estimating: 347it [04:12,  1.47it/s]Extractor Estimating: 348it [04:13,  1.50it/s]Extractor Estimating: 349it [04:14,  1.49it/s]Extractor Estimating: 350it [04:14,  1.48it/s]Extractor Estimating: 351it [04:15,  1.46it/s]Extractor Estimating: 352it [04:16,  1.47it/s]Extractor Estimating: 353it [04:17,  1.41it/s]Extractor Estimating: 354it [04:17,  1.46it/s]Extractor Estimating: 355it [04:18,  1.43it/s]Extractor Estimating: 356it [04:19,  1.40it/s]Extractor Estimating: 357it [04:19,  1.39it/s]Extractor Estimating: 358it [04:20,  1.38it/s]Extractor Estimating: 359it [04:21,  1.40it/s]Extractor Estimating: 360it [04:22,  1.41it/s]Extractor Estimating: 361it [04:22,  1.40it/s]Extractor Estimating: 362it [04:23,  1.40it/s]Extractor Estimating: 363it [04:24,  1.37it/s]Extractor Estimating: 364it [04:25,  1.32it/s]Extractor Estimating: 365it [04:25,  1.33it/s]Extractor Estimating: 366it [04:26,  1.29it/s]Extractor Estimating: 367it [04:27,  1.34it/s]Extractor Estimating: 368it [04:27,  1.40it/s]Extractor Estimating: 369it [04:28,  1.39it/s]Extractor Estimating: 370it [04:29,  1.42it/s]Extractor Estimating: 371it [04:29,  1.43it/s]Extractor Estimating: 372it [04:30,  1.45it/s]Extractor Estimating: 373it [04:31,  1.45it/s]Extractor Estimating: 374it [04:32,  1.44it/s]Extractor Estimating: 375it [04:32,  1.43it/s]Extractor Estimating: 376it [04:33,  1.40it/s]Extractor Estimating: 377it [04:34,  1.40it/s]Extractor Estimating: 378it [04:34,  1.43it/s]Extractor Estimating: 379it [04:35,  1.42it/s]Extractor Estimating: 380it [04:36,  1.41it/s]Extractor Estimating: 381it [04:37,  1.40it/s]Extractor Estimating: 382it [04:37,  1.39it/s]Extractor Estimating: 383it [04:38,  1.29it/s]Extractor Estimating: 384it [04:39,  1.38it/s]Extractor Estimating: 385it [04:39,  1.42it/s]Extractor Estimating: 386it [04:40,  1.37it/s]Extractor Estimating: 387it [04:41,  1.36it/s]Extractor Estimating: 388it [04:42,  1.43it/s]Extractor Estimating: 389it [04:42,  1.39it/s]Extractor Estimating: 390it [04:43,  1.41it/s]Extractor Estimating: 391it [04:44,  1.38it/s]Extractor Estimating: 392it [04:45,  1.39it/s]Extractor Estimating: 393it [04:45,  1.39it/s]Extractor Estimating: 394it [04:46,  1.39it/s]Extractor Estimating: 395it [04:47,  1.41it/s]Extractor Estimating: 396it [04:47,  1.39it/s]Extractor Estimating: 397it [04:48,  1.40it/s]Extractor Estimating: 398it [04:49,  1.41it/s]Extractor Estimating: 399it [04:50,  1.39it/s]Extractor Estimating: 400it [04:50,  1.37it/s]Extractor Estimating: 401it [04:51,  1.33it/s]Extractor Estimating: 402it [04:52,  1.40it/s]Extractor Estimating: 403it [04:52,  1.41it/s]Extractor Estimating: 404it [04:53,  1.44it/s]Extractor Estimating: 405it [04:54,  1.40it/s]Extractor Estimating: 406it [04:55,  1.37it/s]Extractor Estimating: 407it [04:55,  1.38it/s]Extractor Estimating: 408it [04:56,  1.38it/s]Extractor Estimating: 409it [04:57,  1.40it/s]Extractor Estimating: 410it [04:57,  1.39it/s]Extractor Estimating: 411it [04:58,  1.38it/s]Extractor Estimating: 412it [04:59,  1.39it/s]Extractor Estimating: 413it [05:00,  1.39it/s]Extractor Estimating: 414it [05:00,  1.34it/s]Extractor Estimating: 415it [05:01,  1.39it/s]Extractor Estimating: 416it [05:02,  1.39it/s]Extractor Estimating: 417it [05:03,  1.35it/s]Extractor Estimating: 418it [05:03,  1.32it/s]Extractor Estimating: 419it [05:04,  1.35it/s]Extractor Estimating: 420it [05:05,  1.38it/s]Extractor Estimating: 421it [05:05,  1.40it/s]Extractor Estimating: 422it [05:06,  1.42it/s]Extractor Estimating: 423it [05:07,  1.43it/s]Extractor Estimating: 424it [05:08,  1.39it/s]Extractor Estimating: 425it [05:08,  1.42it/s]Extractor Estimating: 426it [05:09,  1.44it/s]Extractor Estimating: 427it [05:10,  1.44it/s]Extractor Estimating: 428it [05:10,  1.39it/s]Extractor Estimating: 429it [05:11,  1.36it/s]Extractor Estimating: 430it [05:12,  1.39it/s]Extractor Estimating: 431it [05:13,  1.43it/s]Extractor Estimating: 432it [05:13,  1.46it/s]Extractor Estimating: 433it [05:14,  1.46it/s]Extractor Estimating: 434it [05:15,  1.42it/s]Extractor Estimating: 435it [05:15,  1.47it/s]Extractor Estimating: 436it [05:16,  1.45it/s]Extractor Estimating: 437it [05:17,  1.43it/s]Extractor Estimating: 438it [05:17,  1.45it/s]Extractor Estimating: 439it [05:18,  1.45it/s]Extractor Estimating: 440it [05:19,  1.43it/s]Extractor Estimating: 441it [05:19,  1.45it/s]Extractor Estimating: 442it [05:20,  1.43it/s]Extractor Estimating: 443it [05:21,  1.43it/s]Extractor Estimating: 444it [05:22,  1.47it/s]Extractor Estimating: 445it [05:22,  1.45it/s]Extractor Estimating: 446it [05:23,  1.41it/s]Extractor Estimating: 447it [05:24,  1.41it/s]Extractor Estimating: 448it [05:24,  1.40it/s]Extractor Estimating: 449it [05:25,  1.37it/s]Extractor Estimating: 450it [05:26,  1.38it/s]Extractor Estimating: 451it [05:27,  1.37it/s]Extractor Estimating: 452it [05:27,  1.30it/s]Extractor Estimating: 453it [05:28,  1.27it/s]Extractor Estimating: 454it [05:29,  1.30it/s]Extractor Estimating: 455it [05:30,  1.29it/s]Extractor Estimating: 456it [05:31,  1.27it/s]Extractor Estimating: 457it [05:31,  1.31it/s]Extractor Estimating: 458it [05:32,  1.27it/s]Extractor Estimating: 459it [05:33,  1.29it/s]Extractor Estimating: 460it [05:34,  1.33it/s]Extractor Estimating: 461it [05:34,  1.30it/s]Extractor Estimating: 462it [05:35,  1.27it/s]Extractor Estimating: 463it [05:36,  1.25it/s]Extractor Estimating: 464it [05:37,  1.23it/s]Extractor Estimating: 465it [05:38,  1.18it/s]Extractor Estimating: 466it [05:39,  1.22it/s]Extractor Estimating: 467it [05:39,  1.24it/s]Extractor Estimating: 468it [05:40,  1.24it/s]Extractor Estimating: 469it [05:41,  1.26it/s]Extractor Estimating: 470it [05:42,  1.26it/s]Extractor Estimating: 471it [05:43,  1.23it/s]Extractor Estimating: 472it [05:43,  1.24it/s]Extractor Estimating: 473it [05:44,  1.23it/s]Extractor Estimating: 474it [05:45,  1.24it/s]Extractor Estimating: 475it [05:46,  1.28it/s]Extractor Estimating: 476it [05:46,  1.32it/s]Extractor Estimating: 477it [05:47,  1.28it/s]Extractor Estimating: 478it [05:48,  1.32it/s]Extractor Estimating: 479it [05:49,  1.33it/s]Extractor Estimating: 480it [05:49,  1.38it/s]Extractor Estimating: 481it [05:50,  1.39it/s]Extractor Estimating: 482it [05:51,  1.33it/s]Extractor Estimating: 483it [05:52,  1.34it/s]Extractor Estimating: 484it [05:53,  1.25it/s]Extractor Estimating: 485it [05:53,  1.29it/s]Extractor Estimating: 486it [05:54,  1.31it/s]Extractor Estimating: 487it [05:55,  1.35it/s]Extractor Estimating: 488it [05:55,  1.36it/s]Extractor Estimating: 489it [05:56,  1.34it/s]Extractor Estimating: 490it [05:57,  1.41it/s]Extractor Estimating: 491it [05:58,  1.33it/s]Extractor Estimating: 492it [05:58,  1.35it/s]Extractor Estimating: 493it [05:59,  1.44it/s]Extractor Estimating: 494it [06:00,  1.38it/s]Extractor Estimating: 495it [06:01,  1.40it/s]Extractor Estimating: 496it [06:01,  1.36it/s]Extractor Estimating: 497it [06:02,  1.37it/s]Extractor Estimating: 498it [06:03,  1.41it/s]Extractor Estimating: 499it [06:03,  1.41it/s]Extractor Estimating: 500it [06:04,  1.44it/s]Extractor Estimating: 500it [06:04,  1.37it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:50:32,810 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:50:32,836 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:50:32,836 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:50:32,836 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:50:32,836 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:50:33,270 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:50:33,271 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:50:33,993 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:50:35,137 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:50:35,137 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:50:37,799 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:50:37,820 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:50:37,820 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:50:37,820 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:50:37,820 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:50:38,245 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:50:38,246 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:50:38,980 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:50:39,200 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:50:39,200 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 04:54:55,007 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 04:54:55,204 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 6000 mean pseudo reward: 0.9774506986267505
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 16164
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16264, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16264, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.192, loss:339.0617
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.193, loss:326.3826
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 50, avg_time 1.187, loss:311.2287
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 150, avg_time 1.200, loss:283.8831
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 250, avg_time 1.195, loss:311.1761
>> valid entity prec:0.5716, rec:0.6291, f1:0.5990
>> valid relation prec:0.2091, rec:0.1599, f1:0.1812
>> valid relation with NER prec:0.2091, rec:0.1599, f1:0.1812
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 100, avg_time 1.194, loss:263.9623
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 200, avg_time 1.182, loss:313.3759
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 50, avg_time 1.208, loss:281.3487
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 150, avg_time 1.181, loss:284.8060
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 250, avg_time 1.195, loss:306.3785
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5987, rec:0.5712, f1:0.5846
>> valid relation prec:0.2433, rec:0.1547, f1:0.1892
>> valid relation with NER prec:0.2433, rec:0.1547, f1:0.1892
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 100, avg_time 1.201, loss:259.1957
g_step 1200, step 200, avg_time 1.192, loss:285.7449
g_step 1300, step 50, avg_time 1.191, loss:277.5669
g_step 1400, step 150, avg_time 1.201, loss:250.7383
g_step 1500, step 250, avg_time 1.189, loss:264.3958
>> valid entity prec:0.5742, rec:0.6006, f1:0.5871
>> valid relation prec:0.2089, rec:0.1590, f1:0.1806
>> valid relation with NER prec:0.2089, rec:0.1590, f1:0.1806
g_step 1600, step 100, avg_time 1.193, loss:249.9721
g_step 1700, step 200, avg_time 1.201, loss:229.0042
g_step 1800, step 50, avg_time 1.183, loss:218.4568
g_step 1900, step 150, avg_time 1.196, loss:238.7914
g_step 2000, step 250, avg_time 1.201, loss:240.4676
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6142, rec:0.6090, f1:0.6116
>> valid relation prec:0.2349, rec:0.1752, f1:0.2007
>> valid relation with NER prec:0.2349, rec:0.1752, f1:0.2007
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 100, avg_time 1.186, loss:204.9981
g_step 2200, step 200, avg_time 1.205, loss:205.1149
g_step 2300, step 50, avg_time 1.194, loss:201.6266
g_step 2400, step 150, avg_time 1.191, loss:213.9679
g_step 2500, step 250, avg_time 1.197, loss:203.6426
>> valid entity prec:0.5736, rec:0.6393, f1:0.6047
>> valid relation prec:0.2215, rec:0.1648, f1:0.1890
>> valid relation with NER prec:0.2215, rec:0.1648, f1:0.1890
g_step 2600, step 100, avg_time 1.208, loss:176.6491
g_step 2700, step 200, avg_time 1.189, loss:192.7969
g_step 2800, step 50, avg_time 1.202, loss:183.6328
g_step 2900, step 150, avg_time 1.190, loss:186.7592
g_step 3000, step 250, avg_time 1.193, loss:186.0450
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6037, rec:0.5855, f1:0.5944
>> valid relation prec:0.2177, rec:0.1470, f1:0.1755
>> valid relation with NER prec:0.2177, rec:0.1470, f1:0.1755
g_step 3100, step 100, avg_time 1.189, loss:160.4043
g_step 3200, step 200, avg_time 1.199, loss:176.1325
g_step 3300, step 50, avg_time 1.194, loss:167.1704
g_step 3400, step 150, avg_time 1.198, loss:153.7564
g_step 3500, step 250, avg_time 1.179, loss:166.9314
>> valid entity prec:0.5819, rec:0.6021, f1:0.5918
>> valid relation prec:0.2219, rec:0.1556, f1:0.1829
>> valid relation with NER prec:0.2219, rec:0.1556, f1:0.1829
g_step 3600, step 100, avg_time 1.210, loss:155.5503
g_step 3700, step 200, avg_time 1.188, loss:163.6771
g_step 3800, step 50, avg_time 1.189, loss:150.3028
g_step 3900, step 150, avg_time 1.202, loss:153.9067
g_step 4000, step 250, avg_time 1.194, loss:164.3786
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5995, rec:0.5839, f1:0.5916
>> valid relation prec:0.2296, rec:0.1680, f1:0.1940
>> valid relation with NER prec:0.2296, rec:0.1680, f1:0.1940
g_step 4100, step 100, avg_time 1.203, loss:139.0563
g_step 4200, step 200, avg_time 1.183, loss:139.9732
g_step 4300, step 50, avg_time 1.190, loss:132.9290
g_step 4400, step 150, avg_time 1.196, loss:129.3973
g_step 4500, step 250, avg_time 1.197, loss:141.8951
>> valid entity prec:0.5959, rec:0.6129, f1:0.6043
>> valid relation prec:0.2180, rec:0.1786, f1:0.1963
>> valid relation with NER prec:0.2180, rec:0.1786, f1:0.1963
g_step 4600, step 100, avg_time 1.189, loss:130.9523
g_step 4700, step 200, avg_time 1.189, loss:138.7391
g_step 4800, step 50, avg_time 1.196, loss:131.4931
g_step 4900, step 150, avg_time 1.202, loss:125.6298
g_step 5000, step 250, avg_time 1.186, loss:124.4807
learning rate was adjusted to 0.0008
>> valid entity prec:0.5946, rec:0.5986, f1:0.5966
>> valid relation prec:0.1953, rec:0.1588, f1:0.1752
>> valid relation with NER prec:0.1953, rec:0.1588, f1:0.1752
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 04:54:55 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 04:54:55 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_04-54-55_ctolab14.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 04:54:56 - WARNING - datasets.builder -   Using custom data configuration default-5c5df5801858fef9
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-5c5df5801858fef9/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 04:54:58,978 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:54:59,015 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 04:54:59,016 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:54:59,017 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 04:54:59,152 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:54:59,224 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:54:59,225 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:54:59,225 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:54:59,225 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:54:59,225 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:54:59,225 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 04:54:59,786 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 04:55:02,896 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 04:55:02,945 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-5c5df5801858fef9/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:02,  2.49ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.54ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.13ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.50ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.72ba/s]100%|██████████| 6/6 [00:01<00:00,  4.88ba/s]100%|██████████| 6/6 [00:01<00:00,  4.37ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.36ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.33ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.86ba/s]100%|██████████| 4/4 [00:00<00:00,  5.04ba/s]100%|██████████| 4/4 [00:00<00:00,  4.20ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.89ba/s] 50%|█████     | 3/6 [00:00<00:00,  7.54ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.03ba/s]100%|██████████| 6/6 [00:00<00:00,  8.52ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.16ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  7.80ba/s]100%|██████████| 4/4 [00:00<00:00,  8.66ba/s]
[INFO|trainer.py:414] 2023-08-29 04:55:08,756 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 04:55:08,941 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 04:55:08,941 >>   Num examples = 6000
[INFO|trainer.py:1149] 2023-08-29 04:55:08,941 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 04:55:08,941 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 04:55:08,941 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 04:55:08,941 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 04:55:08,941 >>   Total optimization steps = 470
  0%|          | 0/470 [00:00<?, ?it/s]  0%|          | 1/470 [00:00<02:23,  3.27it/s]  0%|          | 2/470 [00:00<02:18,  3.38it/s]  1%|          | 3/470 [00:00<02:16,  3.42it/s]  1%|          | 4/470 [00:01<02:27,  3.17it/s]  1%|          | 5/470 [00:01<02:23,  3.24it/s]  1%|▏         | 6/470 [00:01<02:21,  3.29it/s]  1%|▏         | 7/470 [00:02<02:19,  3.32it/s]  2%|▏         | 8/470 [00:02<02:18,  3.34it/s]  2%|▏         | 9/470 [00:02<02:17,  3.35it/s]  2%|▏         | 10/470 [00:03<02:17,  3.36it/s]  2%|▏         | 11/470 [00:03<02:16,  3.36it/s]  3%|▎         | 12/470 [00:03<02:15,  3.37it/s]  3%|▎         | 13/470 [00:03<02:15,  3.37it/s]  3%|▎         | 14/470 [00:04<02:19,  3.28it/s]  3%|▎         | 15/470 [00:04<02:17,  3.31it/s]  3%|▎         | 16/470 [00:04<02:16,  3.32it/s]  4%|▎         | 17/470 [00:05<02:15,  3.34it/s]  4%|▍         | 18/470 [00:05<02:15,  3.34it/s]  4%|▍         | 19/470 [00:05<02:14,  3.35it/s]  4%|▍         | 20/470 [00:06<02:14,  3.36it/s]  4%|▍         | 21/470 [00:06<02:13,  3.36it/s]  5%|▍         | 22/470 [00:06<02:13,  3.36it/s]  5%|▍         | 23/470 [00:06<02:13,  3.36it/s]  5%|▌         | 24/470 [00:07<02:16,  3.27it/s]  5%|▌         | 25/470 [00:07<02:14,  3.30it/s]  6%|▌         | 26/470 [00:07<02:13,  3.31it/s]  6%|▌         | 27/470 [00:08<02:13,  3.33it/s]  6%|▌         | 28/470 [00:08<02:12,  3.34it/s]  6%|▌         | 29/470 [00:08<02:12,  3.33it/s]  6%|▋         | 30/470 [00:09<02:12,  3.33it/s]  7%|▋         | 31/470 [00:09<02:11,  3.33it/s]  7%|▋         | 32/470 [00:09<02:11,  3.32it/s]  7%|▋         | 33/470 [00:09<02:11,  3.32it/s]  7%|▋         | 34/470 [00:10<02:15,  3.22it/s]  7%|▋         | 35/470 [00:10<02:13,  3.25it/s]  8%|▊         | 36/470 [00:10<02:12,  3.28it/s]  8%|▊         | 37/470 [00:11<02:10,  3.31it/s]  8%|▊         | 38/470 [00:11<02:09,  3.33it/s]  8%|▊         | 39/470 [00:11<02:09,  3.34it/s]  9%|▊         | 40/470 [00:12<02:08,  3.35it/s]  9%|▊         | 41/470 [00:12<02:08,  3.35it/s]  9%|▉         | 42/470 [00:12<02:07,  3.36it/s]  9%|▉         | 43/470 [00:12<02:06,  3.36it/s]  9%|▉         | 44/470 [00:13<02:09,  3.28it/s] 10%|▉         | 45/470 [00:13<02:08,  3.31it/s] 10%|▉         | 46/470 [00:13<02:07,  3.32it/s] 10%|█         | 47/470 [00:14<02:06,  3.34it/s] 10%|█         | 48/470 [00:14<02:06,  3.35it/s] 10%|█         | 49/470 [00:14<02:05,  3.35it/s] 11%|█         | 50/470 [00:15<02:05,  3.35it/s] 11%|█         | 51/470 [00:15<02:04,  3.36it/s] 11%|█         | 52/470 [00:15<02:04,  3.36it/s] 11%|█▏        | 53/470 [00:15<02:03,  3.36it/s] 11%|█▏        | 54/470 [00:16<02:03,  3.37it/s] 12%|█▏        | 55/470 [00:16<02:05,  3.30it/s] 12%|█▏        | 56/470 [00:16<02:03,  3.34it/s] 12%|█▏        | 57/470 [00:17<02:02,  3.38it/s] 12%|█▏        | 58/470 [00:17<02:01,  3.40it/s] 13%|█▎        | 59/470 [00:17<02:00,  3.42it/s] 13%|█▎        | 60/470 [00:17<01:59,  3.43it/s] 13%|█▎        | 61/470 [00:18<01:58,  3.44it/s] 13%|█▎        | 62/470 [00:18<01:58,  3.44it/s] 13%|█▎        | 63/470 [00:18<01:57,  3.45it/s] 14%|█▎        | 64/470 [00:19<01:57,  3.46it/s] 14%|█▍        | 65/470 [00:19<01:57,  3.46it/s] 14%|█▍        | 66/470 [00:19<01:56,  3.46it/s] 14%|█▍        | 67/470 [00:20<02:00,  3.33it/s] 14%|█▍        | 68/470 [00:20<01:59,  3.37it/s] 15%|█▍        | 69/470 [00:20<01:58,  3.39it/s] 15%|█▍        | 70/470 [00:20<01:57,  3.41it/s] 15%|█▌        | 71/470 [00:21<01:56,  3.43it/s] 15%|█▌        | 72/470 [00:21<01:55,  3.44it/s] 16%|█▌        | 73/470 [00:21<01:55,  3.44it/s] 16%|█▌        | 74/470 [00:22<01:55,  3.44it/s] 16%|█▌        | 75/470 [00:22<01:54,  3.44it/s] 16%|█▌        | 76/470 [00:22<01:54,  3.45it/s] 16%|█▋        | 77/470 [00:22<01:53,  3.45it/s] 17%|█▋        | 78/470 [00:23<01:58,  3.31it/s] 17%|█▋        | 79/470 [00:23<01:56,  3.36it/s] 17%|█▋        | 80/470 [00:23<01:55,  3.38it/s] 17%|█▋        | 81/470 [00:24<01:54,  3.40it/s] 17%|█▋        | 82/470 [00:24<01:53,  3.42it/s] 18%|█▊        | 83/470 [00:24<01:52,  3.43it/s] 18%|█▊        | 84/470 [00:25<01:52,  3.44it/s] 18%|█▊        | 85/470 [00:25<01:51,  3.44it/s] 18%|█▊        | 86/470 [00:25<01:51,  3.45it/s] 19%|█▊        | 87/470 [00:25<01:51,  3.45it/s] 19%|█▊        | 88/470 [00:26<01:50,  3.45it/s] 19%|█▉        | 89/470 [00:26<01:52,  3.37it/s] 19%|█▉        | 90/470 [00:26<01:51,  3.40it/s] 19%|█▉        | 91/470 [00:27<01:50,  3.42it/s] 20%|█▉        | 92/470 [00:27<01:50,  3.43it/s] 20%|█▉        | 93/470 [00:27<01:49,  3.44it/s] 20%|██        | 94/470 [00:27<01:42,  3.68it/s][INFO|trainer.py:2140] 2023-08-29 04:55:36,802 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:55:36,802 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 04:55:36,802 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 55.93it/s][A
  3%|▎         | 12/435 [00:00<00:08, 48.49it/s][A
  4%|▍         | 17/435 [00:00<00:08, 46.78it/s][A
  5%|▌         | 22/435 [00:00<00:09, 45.69it/s][A
  6%|▌         | 27/435 [00:00<00:09, 45.26it/s][A
  7%|▋         | 32/435 [00:00<00:08, 44.96it/s][A
  9%|▊         | 37/435 [00:00<00:08, 44.78it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.49it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.62it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.76it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.72it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.60it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 43.43it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 43.88it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 43.94it/s][A
 19%|█▉        | 82/435 [00:01<00:08, 44.05it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.18it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.33it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.56it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.66it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.46it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.38it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 44.38it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 44.37it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 44.28it/s][A
 30%|███       | 132/435 [00:02<00:06, 44.38it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 44.47it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 44.69it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 44.67it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.65it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.50it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 44.45it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 44.41it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.42it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.40it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.49it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.54it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 44.63it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 44.60it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 43.59it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 43.83it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 44.00it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 44.09it/s][A
 51%|█████     | 222/435 [00:04<00:04, 44.19it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.28it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.43it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.56it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.49it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.45it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 44.52it/s][A
 59%|█████▉    | 257/435 [00:05<00:04, 44.46it/s][A
 60%|██████    | 262/435 [00:05<00:03, 44.44it/s][A
 61%|██████▏   | 267/435 [00:05<00:03, 44.46it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.37it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.47it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.56it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.52it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.49it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.51it/s][A
 69%|██████▉   | 302/435 [00:06<00:02, 44.51it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.45it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.49it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.48it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.55it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.58it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.55it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 43.47it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 43.80it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 44.09it/s][A
 81%|████████  | 352/435 [00:07<00:01, 44.21it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 44.26it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 44.29it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.41it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 44.55it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 44.46it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 44.38it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 44.47it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 44.46it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 44.41it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 44.45it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 44.27it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 44.44it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 44.43it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.61it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 44.57it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.51it/s][A                                                
                                                 [A 20%|██        | 94/470 [00:37<01:42,  3.68it/s]
100%|██████████| 435/435 [00:09<00:00, 44.51it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:55:46,752 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-29 04:55:46,946 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:55:51,357 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:55:51,630 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:55:51,785 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-94/special_tokens_map.json
 20%|██        | 95/470 [00:50<44:14,  7.08s/it] 20%|██        | 96/470 [00:51<31:30,  5.06s/it] 21%|██        | 97/470 [00:51<22:33,  3.63s/it] 21%|██        | 98/470 [00:51<16:17,  2.63s/it] 21%|██        | 99/470 [00:52<11:55,  1.93s/it] 21%|██▏       | 100/470 [00:52<08:52,  1.44s/it] 21%|██▏       | 101/470 [00:52<06:45,  1.10s/it] 22%|██▏       | 102/470 [00:52<05:15,  1.17it/s] 22%|██▏       | 103/470 [00:53<04:13,  1.45it/s] 22%|██▏       | 104/470 [00:53<03:29,  1.75it/s] 22%|██▏       | 105/470 [00:53<02:58,  2.04it/s] 23%|██▎       | 106/470 [00:54<02:44,  2.22it/s] 23%|██▎       | 107/470 [00:54<02:26,  2.47it/s] 23%|██▎       | 108/470 [00:54<02:14,  2.69it/s] 23%|██▎       | 109/470 [00:55<02:06,  2.86it/s] 23%|██▎       | 110/470 [00:55<02:00,  2.99it/s] 24%|██▎       | 111/470 [00:55<01:56,  3.09it/s] 24%|██▍       | 112/470 [00:55<01:53,  3.17it/s] 24%|██▍       | 113/470 [00:56<01:50,  3.22it/s] 24%|██▍       | 114/470 [00:56<01:49,  3.26it/s] 24%|██▍       | 115/470 [00:56<01:47,  3.29it/s] 25%|██▍       | 116/470 [00:57<01:51,  3.19it/s] 25%|██▍       | 117/470 [00:57<01:48,  3.24it/s] 25%|██▌       | 118/470 [00:57<01:47,  3.27it/s] 25%|██▌       | 119/470 [00:58<01:46,  3.30it/s] 26%|██▌       | 120/470 [00:58<01:45,  3.32it/s] 26%|██▌       | 121/470 [00:58<01:44,  3.33it/s] 26%|██▌       | 122/470 [00:58<01:44,  3.34it/s] 26%|██▌       | 123/470 [00:59<01:43,  3.35it/s] 26%|██▋       | 124/470 [00:59<01:43,  3.35it/s] 27%|██▋       | 125/470 [00:59<01:42,  3.36it/s] 27%|██▋       | 126/470 [01:00<01:48,  3.17it/s] 27%|██▋       | 127/470 [01:00<01:45,  3.25it/s] 27%|██▋       | 128/470 [01:00<01:43,  3.31it/s] 27%|██▋       | 129/470 [01:01<01:41,  3.35it/s] 28%|██▊       | 130/470 [01:01<01:40,  3.38it/s] 28%|██▊       | 131/470 [01:01<01:39,  3.41it/s] 28%|██▊       | 132/470 [01:01<01:38,  3.42it/s] 28%|██▊       | 133/470 [01:02<01:38,  3.43it/s] 29%|██▊       | 134/470 [01:02<01:37,  3.44it/s] 29%|██▊       | 135/470 [01:02<01:37,  3.44it/s] 29%|██▉       | 136/470 [01:03<01:36,  3.45it/s] 29%|██▉       | 137/470 [01:03<01:37,  3.40it/s] 29%|██▉       | 138/470 [01:03<01:37,  3.42it/s] 30%|██▉       | 139/470 [01:04<01:36,  3.43it/s] 30%|██▉       | 140/470 [01:04<01:35,  3.44it/s] 30%|███       | 141/470 [01:04<01:35,  3.44it/s] 30%|███       | 142/470 [01:04<01:35,  3.45it/s] 30%|███       | 143/470 [01:05<01:34,  3.45it/s] 31%|███       | 144/470 [01:05<01:34,  3.45it/s] 31%|███       | 145/470 [01:05<01:34,  3.45it/s] 31%|███       | 146/470 [01:06<01:33,  3.45it/s] 31%|███▏      | 147/470 [01:06<01:33,  3.45it/s] 31%|███▏      | 148/470 [01:06<01:35,  3.37it/s] 32%|███▏      | 149/470 [01:06<01:34,  3.39it/s] 32%|███▏      | 150/470 [01:07<01:33,  3.41it/s] 32%|███▏      | 151/470 [01:07<01:33,  3.42it/s] 32%|███▏      | 152/470 [01:07<01:32,  3.43it/s] 33%|███▎      | 153/470 [01:08<01:32,  3.44it/s] 33%|███▎      | 154/470 [01:08<01:31,  3.44it/s] 33%|███▎      | 155/470 [01:08<01:31,  3.44it/s] 33%|███▎      | 156/470 [01:08<01:31,  3.45it/s] 33%|███▎      | 157/470 [01:09<01:30,  3.45it/s] 34%|███▎      | 158/470 [01:09<01:30,  3.45it/s] 34%|███▍      | 159/470 [01:09<01:32,  3.37it/s] 34%|███▍      | 160/470 [01:10<01:31,  3.40it/s] 34%|███▍      | 161/470 [01:10<01:30,  3.41it/s] 34%|███▍      | 162/470 [01:10<01:29,  3.43it/s] 35%|███▍      | 163/470 [01:11<01:29,  3.43it/s] 35%|███▍      | 164/470 [01:11<01:28,  3.44it/s] 35%|███▌      | 165/470 [01:11<01:28,  3.44it/s] 35%|███▌      | 166/470 [01:11<01:28,  3.45it/s] 36%|███▌      | 167/470 [01:12<01:27,  3.45it/s] 36%|███▌      | 168/470 [01:12<01:27,  3.45it/s] 36%|███▌      | 169/470 [01:12<01:27,  3.45it/s] 36%|███▌      | 170/470 [01:13<01:30,  3.33it/s] 36%|███▋      | 171/470 [01:13<01:28,  3.37it/s] 37%|███▋      | 172/470 [01:13<01:27,  3.39it/s] 37%|███▋      | 173/470 [01:13<01:27,  3.41it/s] 37%|███▋      | 174/470 [01:14<01:26,  3.42it/s] 37%|███▋      | 175/470 [01:14<01:25,  3.43it/s] 37%|███▋      | 176/470 [01:14<01:25,  3.44it/s] 38%|███▊      | 177/470 [01:15<01:25,  3.44it/s] 38%|███▊      | 178/470 [01:15<01:24,  3.45it/s] 38%|███▊      | 179/470 [01:15<01:24,  3.45it/s] 38%|███▊      | 180/470 [01:15<01:24,  3.45it/s] 39%|███▊      | 181/470 [01:16<01:25,  3.37it/s] 39%|███▊      | 182/470 [01:16<01:24,  3.39it/s] 39%|███▉      | 183/470 [01:16<01:24,  3.41it/s] 39%|███▉      | 184/470 [01:17<01:23,  3.42it/s] 39%|███▉      | 185/470 [01:17<01:23,  3.43it/s] 40%|███▉      | 186/470 [01:17<01:22,  3.44it/s] 40%|███▉      | 187/470 [01:18<01:22,  3.44it/s] 40%|████      | 188/470 [01:18<01:19,  3.56it/s][INFO|trainer.py:2140] 2023-08-29 04:56:27,222 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:56:27,222 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 04:56:27,222 >>   Batch size = 8
{'eval_loss': 1.0685977935791016, 'eval_runtime': 9.7943, 'eval_samples_per_second': 355.002, 'eval_steps_per_second': 44.414, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 55.72it/s][A
  3%|▎         | 12/435 [00:00<00:08, 48.94it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.15it/s][A
  5%|▌         | 22/435 [00:00<00:08, 46.37it/s][A
  6%|▌         | 27/435 [00:00<00:08, 45.74it/s][A
  7%|▋         | 32/435 [00:00<00:08, 45.04it/s][A
  9%|▊         | 37/435 [00:00<00:08, 44.49it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.18it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.28it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.51it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.69it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.80it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.82it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.73it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 44.47it/s][A
 19%|█▉        | 82/435 [00:01<00:07, 44.19it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.07it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.13it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.39it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.59it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.67it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.78it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 44.67it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 44.47it/s][A
 29%|██▉       | 127/435 [00:02<00:07, 43.22it/s][A
 30%|███       | 132/435 [00:02<00:06, 43.53it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 43.79it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 44.12it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 44.36it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.58it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.55it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 44.41it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 44.17it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.04it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.19it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.31it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.46it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 44.65it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 44.73it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 44.72it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 44.60it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 44.31it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 44.22it/s][A
 51%|█████     | 222/435 [00:04<00:04, 44.27it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.31it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.45it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.56it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.74it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.73it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 44.61it/s][A
 59%|█████▉    | 257/435 [00:05<00:04, 44.38it/s][A
 60%|██████    | 262/435 [00:05<00:04, 42.97it/s][A
 61%|██████▏   | 267/435 [00:05<00:03, 43.45it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 43.72it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.02it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.35it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.44it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.50it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.52it/s][A
 69%|██████▉   | 302/435 [00:06<00:03, 44.22it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.19it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.27it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.38it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.53it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.65it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.67it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.63it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 44.54it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 44.26it/s][A
 81%|████████  | 352/435 [00:07<00:01, 44.16it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 44.27it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 44.41it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.61it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 44.75it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 44.71it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 44.72it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 44.54it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 44.32it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 43.11it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 43.60it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 43.87it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 44.21it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 44.46it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.57it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 44.45it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.33it/s][A                                                 
                                                 [A 40%|████      | 188/470 [01:28<01:19,  3.56it/s]
100%|██████████| 435/435 [00:09<00:00, 44.33it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:56:37,109 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-29 04:56:37,234 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:56:40,120 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:56:40,239 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:56:40,292 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-188/special_tokens_map.json
 40%|████      | 189/470 [01:38<28:53,  6.17s/it] 40%|████      | 190/470 [01:38<20:36,  4.42s/it] 41%|████      | 191/470 [01:38<14:47,  3.18s/it] 41%|████      | 192/470 [01:39<10:43,  2.32s/it] 41%|████      | 193/470 [01:39<07:53,  1.71s/it] 41%|████▏     | 194/470 [01:39<05:55,  1.29s/it] 41%|████▏     | 195/470 [01:40<04:32,  1.01it/s] 42%|████▏     | 196/470 [01:40<03:34,  1.28it/s] 42%|████▏     | 197/470 [01:40<02:53,  1.57it/s] 42%|████▏     | 198/470 [01:40<02:25,  1.87it/s] 42%|████▏     | 199/470 [01:41<02:05,  2.16it/s] 43%|████▎     | 200/470 [01:41<01:56,  2.32it/s] 43%|████▎     | 201/470 [01:41<01:45,  2.56it/s] 43%|████▎     | 202/470 [01:42<01:37,  2.76it/s] 43%|████▎     | 203/470 [01:42<01:31,  2.92it/s] 43%|████▎     | 204/470 [01:42<01:27,  3.04it/s] 44%|████▎     | 205/470 [01:43<01:24,  3.13it/s] 44%|████▍     | 206/470 [01:43<01:22,  3.20it/s] 44%|████▍     | 207/470 [01:43<01:20,  3.25it/s] 44%|████▍     | 208/470 [01:43<01:19,  3.29it/s] 44%|████▍     | 209/470 [01:44<01:18,  3.31it/s] 45%|████▍     | 210/470 [01:44<01:21,  3.21it/s] 45%|████▍     | 211/470 [01:44<01:19,  3.25it/s] 45%|████▌     | 212/470 [01:45<01:18,  3.29it/s] 45%|████▌     | 213/470 [01:45<01:17,  3.31it/s] 46%|████▌     | 214/470 [01:45<01:16,  3.33it/s] 46%|████▌     | 215/470 [01:46<01:16,  3.34it/s] 46%|████▌     | 216/470 [01:46<01:15,  3.35it/s] 46%|████▌     | 217/470 [01:46<01:15,  3.36it/s] 46%|████▋     | 218/470 [01:46<01:14,  3.36it/s] 47%|████▋     | 219/470 [01:47<01:14,  3.36it/s] 47%|████▋     | 220/470 [01:47<01:15,  3.30it/s] 47%|████▋     | 221/470 [01:47<01:14,  3.32it/s] 47%|████▋     | 222/470 [01:48<01:14,  3.34it/s] 47%|████▋     | 223/470 [01:48<01:13,  3.35it/s] 48%|████▊     | 224/470 [01:48<01:13,  3.35it/s] 48%|████▊     | 225/470 [01:49<01:16,  3.21it/s] 48%|████▊     | 226/470 [01:49<01:17,  3.15it/s] 48%|████▊     | 227/470 [01:49<01:15,  3.22it/s] 49%|████▊     | 228/470 [01:50<01:26,  2.79it/s] 49%|████▊     | 229/470 [01:50<01:21,  2.95it/s] 49%|████▉     | 230/470 [01:50<01:18,  3.06it/s] 49%|████▉     | 231/470 [01:51<01:15,  3.15it/s] 49%|████▉     | 232/470 [01:51<01:14,  3.20it/s] 50%|████▉     | 233/470 [01:51<01:12,  3.25it/s] 50%|████▉     | 234/470 [01:51<01:12,  3.28it/s] 50%|█████     | 235/470 [01:52<01:14,  3.16it/s] 50%|█████     | 236/470 [01:52<01:12,  3.21it/s] 50%|█████     | 237/470 [01:52<01:11,  3.25it/s] 51%|█████     | 238/470 [01:53<01:10,  3.28it/s] 51%|█████     | 239/470 [01:53<01:09,  3.30it/s] 51%|█████     | 240/470 [01:53<01:09,  3.32it/s] 51%|█████▏    | 241/470 [01:54<01:08,  3.33it/s] 51%|█████▏    | 242/470 [01:54<01:08,  3.34it/s] 52%|█████▏    | 243/470 [01:54<01:07,  3.35it/s] 52%|█████▏    | 244/470 [01:54<01:07,  3.35it/s] 52%|█████▏    | 245/470 [01:55<01:09,  3.25it/s] 52%|█████▏    | 246/470 [01:55<01:08,  3.28it/s] 53%|█████▎    | 247/470 [01:55<01:07,  3.31it/s] 53%|█████▎    | 248/470 [01:56<01:06,  3.34it/s] 53%|█████▎    | 249/470 [01:56<01:05,  3.37it/s] 53%|█████▎    | 250/470 [01:56<01:04,  3.40it/s] 53%|█████▎    | 251/470 [01:57<01:04,  3.41it/s] 54%|█████▎    | 252/470 [01:57<01:03,  3.43it/s] 54%|█████▍    | 253/470 [01:57<01:03,  3.44it/s] 54%|█████▍    | 254/470 [01:57<01:02,  3.44it/s] 54%|█████▍    | 255/470 [01:58<01:02,  3.45it/s] 54%|█████▍    | 256/470 [01:58<01:04,  3.32it/s] 55%|█████▍    | 257/470 [01:58<01:03,  3.36it/s] 55%|█████▍    | 258/470 [01:59<01:02,  3.39it/s] 55%|█████▌    | 259/470 [01:59<01:01,  3.41it/s] 55%|█████▌    | 260/470 [01:59<01:01,  3.42it/s] 56%|█████▌    | 261/470 [01:59<01:00,  3.44it/s] 56%|█████▌    | 262/470 [02:00<01:00,  3.44it/s] 56%|█████▌    | 263/470 [02:00<01:00,  3.45it/s] 56%|█████▌    | 264/470 [02:00<00:59,  3.45it/s] 56%|█████▋    | 265/470 [02:01<00:59,  3.45it/s] 57%|█████▋    | 266/470 [02:01<00:59,  3.46it/s] 57%|█████▋    | 267/470 [02:01<00:59,  3.39it/s] 57%|█████▋    | 268/470 [02:02<00:59,  3.41it/s] 57%|█████▋    | 269/470 [02:02<00:58,  3.42it/s] 57%|█████▋    | 270/470 [02:02<00:58,  3.43it/s] 58%|█████▊    | 271/470 [02:02<00:57,  3.44it/s] 58%|█████▊    | 272/470 [02:03<00:57,  3.45it/s] 58%|█████▊    | 273/470 [02:03<00:57,  3.45it/s] 58%|█████▊    | 274/470 [02:03<00:56,  3.45it/s] 59%|█████▊    | 275/470 [02:04<00:56,  3.45it/s] 59%|█████▊    | 276/470 [02:04<00:56,  3.42it/s] 59%|█████▉    | 277/470 [02:04<00:56,  3.41it/s] 59%|█████▉    | 278/470 [02:04<00:57,  3.32it/s] 59%|█████▉    | 279/470 [02:05<00:57,  3.33it/s] 60%|█████▉    | 280/470 [02:05<00:56,  3.34it/s] 60%|█████▉    | 281/470 [02:05<00:56,  3.34it/s] 60%|██████    | 282/470 [02:06<00:52,  3.58it/s][INFO|trainer.py:2140] 2023-08-29 04:57:15,038 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:57:15,038 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 04:57:15,038 >>   Batch size = 8
{'eval_loss': 1.083936095237732, 'eval_runtime': 9.8028, 'eval_samples_per_second': 354.694, 'eval_steps_per_second': 44.375, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.01it/s][A
  3%|▎         | 12/435 [00:00<00:08, 48.69it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.18it/s][A
  5%|▌         | 22/435 [00:00<00:08, 45.96it/s][A
  6%|▌         | 27/435 [00:00<00:09, 45.27it/s][A
  7%|▋         | 32/435 [00:00<00:09, 44.73it/s][A
  9%|▊         | 37/435 [00:00<00:08, 44.33it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.34it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.49it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.66it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.76it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.78it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.69it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.52it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 43.13it/s][A
 19%|█▉        | 82/435 [00:01<00:08, 43.51it/s][A
 20%|██        | 87/435 [00:01<00:07, 43.71it/s][A
 21%|██        | 92/435 [00:02<00:07, 43.98it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.32it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.50it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.64it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.56it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 44.27it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 44.17it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 44.21it/s][A
 30%|███       | 132/435 [00:02<00:06, 44.20it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 44.36it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 44.49it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 44.57it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.79it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.63it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 44.40it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 44.28it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.24it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.20it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.32it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.59it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 44.65it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 44.70it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 44.63it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 44.48it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 42.78it/s][A
 50%|████▉     | 217/435 [00:04<00:05, 43.23it/s][A
 51%|█████     | 222/435 [00:04<00:04, 43.53it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 43.91it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.15it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.39it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.56it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.53it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 44.31it/s][A
 59%|█████▉    | 257/435 [00:05<00:04, 44.28it/s][A
 60%|██████    | 262/435 [00:05<00:03, 44.34it/s][A
 61%|██████▏   | 267/435 [00:06<00:03, 44.37it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.41it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.47it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.57it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.63it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.61it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.49it/s][A
 69%|██████▉   | 302/435 [00:06<00:02, 44.50it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.39it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.39it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.44it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.51it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.62it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.63it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.56it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 44.46it/s][A
 80%|███████▉  | 347/435 [00:07<00:02, 42.82it/s][A
 81%|████████  | 352/435 [00:07<00:01, 43.40it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 43.74it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 43.95it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.22it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 44.39it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 44.53it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 44.43it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 44.20it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 44.27it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 44.32it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 44.29it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 44.46it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 44.58it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 44.67it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.64it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 44.46it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.40it/s][A                                                 
                                                 [A 60%|██████    | 282/470 [02:15<00:52,  3.58it/s]
100%|██████████| 435/435 [00:09<00:00, 44.40it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:57:25,015 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-282
[INFO|configuration_utils.py:351] 2023-08-29 04:57:25,179 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-282/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:57:27,998 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-282/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:57:28,106 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-282/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:57:28,168 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-282/special_tokens_map.json
 60%|██████    | 283/470 [02:25<18:49,  6.04s/it] 60%|██████    | 284/470 [02:25<13:24,  4.32s/it] 61%|██████    | 285/470 [02:26<09:36,  3.12s/it] 61%|██████    | 286/470 [02:26<06:57,  2.27s/it] 61%|██████    | 287/470 [02:26<05:06,  1.67s/it] 61%|██████▏   | 288/470 [02:27<03:49,  1.26s/it] 61%|██████▏   | 289/470 [02:27<02:55,  1.03it/s] 62%|██████▏   | 290/470 [02:27<02:17,  1.31it/s] 62%|██████▏   | 291/470 [02:27<01:51,  1.61it/s] 62%|██████▏   | 292/470 [02:28<01:32,  1.91it/s] 62%|██████▏   | 293/470 [02:28<01:20,  2.21it/s] 63%|██████▎   | 294/470 [02:28<01:10,  2.48it/s] 63%|██████▎   | 295/470 [02:29<01:05,  2.67it/s] 63%|██████▎   | 296/470 [02:29<01:00,  2.87it/s] 63%|██████▎   | 297/470 [02:29<00:57,  3.02it/s] 63%|██████▎   | 298/470 [02:29<00:54,  3.14it/s] 64%|██████▎   | 299/470 [02:30<00:52,  3.23it/s] 64%|██████▍   | 300/470 [02:30<00:51,  3.29it/s] 64%|██████▍   | 301/470 [02:30<00:50,  3.34it/s] 64%|██████▍   | 302/470 [02:31<00:49,  3.37it/s] 64%|██████▍   | 303/470 [02:31<00:49,  3.40it/s] 65%|██████▍   | 304/470 [02:31<00:48,  3.42it/s] 65%|██████▍   | 305/470 [02:31<00:48,  3.43it/s] 65%|██████▌   | 306/470 [02:32<00:48,  3.37it/s] 65%|██████▌   | 307/470 [02:32<00:47,  3.40it/s] 66%|██████▌   | 308/470 [02:32<00:47,  3.42it/s] 66%|██████▌   | 309/470 [02:33<00:46,  3.43it/s] 66%|██████▌   | 310/470 [02:33<00:46,  3.44it/s] 66%|██████▌   | 311/470 [02:33<00:46,  3.44it/s] 66%|██████▋   | 312/470 [02:34<00:45,  3.45it/s] 67%|██████▋   | 313/470 [02:34<00:45,  3.45it/s] 67%|██████▋   | 314/470 [02:34<00:45,  3.45it/s] 67%|██████▋   | 315/470 [02:34<00:44,  3.45it/s] 67%|██████▋   | 316/470 [02:35<00:44,  3.46it/s] 67%|██████▋   | 317/470 [02:35<00:45,  3.34it/s] 68%|██████▊   | 318/470 [02:35<00:45,  3.38it/s] 68%|██████▊   | 319/470 [02:36<00:44,  3.40it/s] 68%|██████▊   | 320/470 [02:36<00:43,  3.42it/s] 68%|██████▊   | 321/470 [02:36<00:43,  3.43it/s] 69%|██████▊   | 322/470 [02:36<00:43,  3.43it/s] 69%|██████▊   | 323/470 [02:37<00:42,  3.44it/s] 69%|██████▉   | 324/470 [02:37<00:42,  3.44it/s] 69%|██████▉   | 325/470 [02:37<00:42,  3.44it/s] 69%|██████▉   | 326/470 [02:38<00:41,  3.45it/s] 70%|██████▉   | 327/470 [02:38<00:41,  3.45it/s] 70%|██████▉   | 328/470 [02:38<00:41,  3.41it/s] 70%|███████   | 329/470 [02:39<00:41,  3.42it/s] 70%|███████   | 330/470 [02:39<00:40,  3.43it/s] 70%|███████   | 331/470 [02:39<00:40,  3.44it/s] 71%|███████   | 332/470 [02:39<00:40,  3.44it/s] 71%|███████   | 333/470 [02:40<00:39,  3.45it/s] 71%|███████   | 334/470 [02:40<00:39,  3.45it/s] 71%|███████▏  | 335/470 [02:40<00:39,  3.45it/s] 71%|███████▏  | 336/470 [02:41<00:39,  3.42it/s] 72%|███████▏  | 337/470 [02:41<00:38,  3.41it/s] 72%|███████▏  | 338/470 [02:41<00:38,  3.40it/s] 72%|███████▏  | 339/470 [02:41<00:40,  3.23it/s] 72%|███████▏  | 340/470 [02:42<00:39,  3.27it/s] 73%|███████▎  | 341/470 [02:42<00:39,  3.30it/s] 73%|███████▎  | 342/470 [02:42<00:38,  3.31it/s] 73%|███████▎  | 343/470 [02:43<00:38,  3.32it/s] 73%|███████▎  | 344/470 [02:43<00:37,  3.33it/s] 73%|███████▎  | 345/470 [02:43<00:37,  3.34it/s] 74%|███████▎  | 346/470 [02:44<00:37,  3.34it/s] 74%|███████▍  | 347/470 [02:44<00:36,  3.34it/s] 74%|███████▍  | 348/470 [02:44<00:36,  3.35it/s] 74%|███████▍  | 349/470 [02:44<00:36,  3.31it/s] 74%|███████▍  | 350/470 [02:45<00:36,  3.32it/s] 75%|███████▍  | 351/470 [02:45<00:35,  3.33it/s] 75%|███████▍  | 352/470 [02:45<00:35,  3.34it/s] 75%|███████▌  | 353/470 [02:46<00:35,  3.34it/s] 75%|███████▌  | 354/470 [02:46<00:34,  3.34it/s] 76%|███████▌  | 355/470 [02:46<00:34,  3.34it/s] 76%|███████▌  | 356/470 [02:47<00:34,  3.35it/s] 76%|███████▌  | 357/470 [02:47<00:33,  3.35it/s] 76%|███████▌  | 358/470 [02:47<00:33,  3.35it/s] 76%|███████▋  | 359/470 [02:47<00:33,  3.35it/s] 77%|███████▋  | 360/470 [02:48<00:33,  3.25it/s] 77%|███████▋  | 361/470 [02:48<00:33,  3.28it/s] 77%|███████▋  | 362/470 [02:48<00:32,  3.30it/s] 77%|███████▋  | 363/470 [02:49<00:32,  3.32it/s] 77%|███████▋  | 364/470 [02:49<00:32,  3.26it/s] 78%|███████▊  | 365/470 [02:49<00:31,  3.29it/s] 78%|███████▊  | 366/470 [02:50<00:37,  2.74it/s] 78%|███████▊  | 367/470 [02:50<00:35,  2.90it/s] 78%|███████▊  | 368/470 [02:50<00:33,  3.02it/s] 79%|███████▊  | 369/470 [02:51<00:32,  3.12it/s] 79%|███████▊  | 370/470 [02:51<00:31,  3.19it/s] 79%|███████▉  | 371/470 [02:51<00:30,  3.24it/s] 79%|███████▉  | 372/470 [02:52<00:29,  3.27it/s] 79%|███████▉  | 373/470 [02:52<00:29,  3.29it/s] 80%|███████▉  | 374/470 [02:52<00:29,  3.30it/s] 80%|███████▉  | 375/470 [02:52<00:28,  3.32it/s] 80%|████████  | 376/470 [02:53<00:26,  3.56it/s][INFO|trainer.py:2140] 2023-08-29 04:58:02,225 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:58:02,225 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 04:58:02,225 >>   Batch size = 8
{'eval_loss': 1.1008425951004028, 'eval_runtime': 9.8124, 'eval_samples_per_second': 354.346, 'eval_steps_per_second': 44.331, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.14it/s][A
  3%|▎         | 12/435 [00:00<00:08, 48.90it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.38it/s][A
  5%|▌         | 22/435 [00:00<00:08, 46.43it/s][A
  6%|▌         | 27/435 [00:00<00:08, 45.99it/s][A
  7%|▋         | 32/435 [00:00<00:08, 45.69it/s][A
  9%|▊         | 37/435 [00:00<00:08, 45.47it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.91it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.40it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.14it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.26it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.43it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.59it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.70it/s][A
 18%|█▊        | 77/435 [00:01<00:07, 44.76it/s][A
 19%|█▉        | 82/435 [00:01<00:07, 44.79it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.61it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.21it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 43.99it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.14it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.37it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.53it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 44.76it/s][A
 28%|██▊       | 122/435 [00:02<00:06, 44.73it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 44.74it/s][A
 30%|███       | 132/435 [00:02<00:06, 44.49it/s][A
 31%|███▏      | 137/435 [00:03<00:07, 42.03it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 42.72it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 43.29it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 43.77it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.14it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 44.36it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 44.53it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.48it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.13it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.03it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.10it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 44.28it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 44.48it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 44.65it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 44.78it/s][A
 49%|████▊     | 212/435 [00:04<00:04, 44.78it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 44.53it/s][A
 51%|█████     | 222/435 [00:04<00:04, 44.28it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.14it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.21it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.33it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.45it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.63it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 44.67it/s][A
 59%|█████▉    | 257/435 [00:05<00:03, 44.73it/s][A
 60%|██████    | 262/435 [00:05<00:03, 44.65it/s][A
 61%|██████▏   | 267/435 [00:06<00:03, 44.40it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 41.45it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 42.47it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 43.10it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 43.57it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.00it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.25it/s][A
 69%|██████▉   | 302/435 [00:06<00:02, 44.36it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.26it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.01it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.02it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.26it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.39it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.56it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.65it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 44.75it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 44.61it/s][A
 81%|████████  | 352/435 [00:07<00:01, 44.45it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 44.16it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 44.10it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.22it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 44.45it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 44.59it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 44.73it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 44.73it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 44.63it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 44.38it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 44.24it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 40.55it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 41.80it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 42.70it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 43.38it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 43.83it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.14it/s][A                                                 
                                                 [A 80%|████████  | 376/470 [03:03<00:26,  3.56it/s]
100%|██████████| 435/435 [00:09<00:00, 44.14it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:58:12,335 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-376
[INFO|configuration_utils.py:351] 2023-08-29 04:58:12,605 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-376/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:58:15,324 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-376/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:58:15,445 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-376/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:58:15,505 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-376/special_tokens_map.json
 80%|████████  | 377/470 [03:12<09:20,  6.03s/it] 80%|████████  | 378/470 [03:12<06:36,  4.31s/it] 81%|████████  | 379/470 [03:13<04:42,  3.11s/it] 81%|████████  | 380/470 [03:13<03:23,  2.26s/it] 81%|████████  | 381/470 [03:13<02:28,  1.67s/it] 81%|████████▏ | 382/470 [03:14<01:50,  1.26s/it] 81%|████████▏ | 383/470 [03:14<01:24,  1.03it/s] 82%|████████▏ | 384/470 [03:14<01:06,  1.30it/s] 82%|████████▏ | 385/470 [03:15<00:53,  1.59it/s] 82%|████████▏ | 386/470 [03:15<00:44,  1.89it/s] 82%|████████▏ | 387/470 [03:15<00:38,  2.18it/s] 83%|████████▎ | 388/470 [03:15<00:33,  2.44it/s] 83%|████████▎ | 389/470 [03:16<00:31,  2.59it/s] 83%|████████▎ | 390/470 [03:16<00:28,  2.79it/s] 83%|████████▎ | 391/470 [03:16<00:26,  2.94it/s] 83%|████████▎ | 392/470 [03:17<00:25,  3.06it/s] 84%|████████▎ | 393/470 [03:17<00:24,  3.14it/s] 84%|████████▍ | 394/470 [03:17<00:23,  3.21it/s] 84%|████████▍ | 395/470 [03:18<00:23,  3.26it/s] 84%|████████▍ | 396/470 [03:18<00:22,  3.29it/s] 84%|████████▍ | 397/470 [03:18<00:22,  3.31it/s] 85%|████████▍ | 398/470 [03:18<00:21,  3.33it/s] 85%|████████▍ | 399/470 [03:19<00:21,  3.26it/s] 85%|████████▌ | 400/470 [03:19<00:21,  3.29it/s] 85%|████████▌ | 401/470 [03:19<00:20,  3.31it/s] 86%|████████▌ | 402/470 [03:20<00:20,  3.33it/s] 86%|████████▌ | 403/470 [03:20<00:20,  3.34it/s] 86%|████████▌ | 404/470 [03:20<00:19,  3.35it/s] 86%|████████▌ | 405/470 [03:21<00:19,  3.35it/s] 86%|████████▋ | 406/470 [03:21<00:19,  3.35it/s] 87%|████████▋ | 407/470 [03:21<00:18,  3.36it/s] 87%|████████▋ | 408/470 [03:21<00:18,  3.35it/s] 87%|████████▋ | 409/470 [03:22<00:18,  3.35it/s] 87%|████████▋ | 410/470 [03:22<00:17,  3.36it/s] 87%|████████▋ | 411/470 [03:22<00:17,  3.36it/s] 88%|████████▊ | 412/470 [03:23<00:17,  3.36it/s] 88%|████████▊ | 413/470 [03:23<00:16,  3.36it/s] 88%|████████▊ | 414/470 [03:23<00:16,  3.36it/s] 88%|████████▊ | 415/470 [03:24<00:16,  3.36it/s] 89%|████████▊ | 416/470 [03:24<00:16,  3.30it/s] 89%|████████▊ | 417/470 [03:24<00:15,  3.32it/s] 89%|████████▉ | 418/470 [03:24<00:15,  3.33it/s] 89%|████████▉ | 419/470 [03:25<00:15,  3.34it/s] 89%|████████▉ | 420/470 [03:25<00:14,  3.35it/s] 90%|████████▉ | 421/470 [03:25<00:14,  3.35it/s] 90%|████████▉ | 422/470 [03:26<00:14,  3.35it/s] 90%|█████████ | 423/470 [03:26<00:14,  3.35it/s] 90%|█████████ | 424/470 [03:26<00:13,  3.35it/s] 90%|█████████ | 425/470 [03:27<00:13,  3.35it/s] 91%|█████████ | 426/470 [03:27<00:13,  3.36it/s] 91%|█████████ | 427/470 [03:27<00:13,  3.28it/s] 91%|█████████ | 428/470 [03:27<00:12,  3.30it/s] 91%|█████████▏| 429/470 [03:28<00:12,  3.32it/s] 91%|█████████▏| 430/470 [03:28<00:11,  3.33it/s] 92%|█████████▏| 431/470 [03:28<00:11,  3.34it/s] 92%|█████████▏| 432/470 [03:29<00:11,  3.35it/s] 92%|█████████▏| 433/470 [03:29<00:11,  3.35it/s] 92%|█████████▏| 434/470 [03:29<00:10,  3.35it/s] 93%|█████████▎| 435/470 [03:30<00:10,  3.35it/s] 93%|█████████▎| 436/470 [03:30<00:10,  3.35it/s] 93%|█████████▎| 437/470 [03:30<00:10,  3.23it/s] 93%|█████████▎| 438/470 [03:30<00:09,  3.27it/s] 93%|█████████▎| 439/470 [03:31<00:09,  3.30it/s] 94%|█████████▎| 440/470 [03:31<00:09,  3.32it/s] 94%|█████████▍| 441/470 [03:31<00:08,  3.33it/s] 94%|█████████▍| 442/470 [03:32<00:08,  3.34it/s] 94%|█████████▍| 443/470 [03:32<00:08,  3.34it/s] 94%|█████████▍| 444/470 [03:32<00:07,  3.35it/s] 95%|█████████▍| 445/470 [03:33<00:07,  3.35it/s] 95%|█████████▍| 446/470 [03:33<00:07,  3.35it/s] 95%|█████████▌| 447/470 [03:33<00:06,  3.29it/s] 95%|█████████▌| 448/470 [03:33<00:06,  3.31it/s] 96%|█████████▌| 449/470 [03:34<00:06,  3.33it/s] 96%|█████████▌| 450/470 [03:34<00:05,  3.34it/s] 96%|█████████▌| 451/470 [03:34<00:05,  3.35it/s] 96%|█████████▌| 452/470 [03:35<00:05,  3.35it/s] 96%|█████████▋| 453/470 [03:35<00:05,  3.36it/s] 97%|█████████▋| 454/470 [03:35<00:04,  3.37it/s] 97%|█████████▋| 455/470 [03:36<00:04,  3.37it/s] 97%|█████████▋| 456/470 [03:36<00:04,  3.37it/s] 97%|█████████▋| 457/470 [03:36<00:03,  3.36it/s] 97%|█████████▋| 458/470 [03:36<00:03,  3.29it/s] 98%|█████████▊| 459/470 [03:37<00:03,  3.31it/s] 98%|█████████▊| 460/470 [03:37<00:03,  3.33it/s] 98%|█████████▊| 461/470 [03:37<00:02,  3.34it/s] 98%|█████████▊| 462/470 [03:38<00:02,  3.35it/s] 99%|█████████▊| 463/470 [03:38<00:02,  3.36it/s] 99%|█████████▊| 464/470 [03:38<00:01,  3.36it/s] 99%|█████████▉| 465/470 [03:39<00:01,  3.38it/s] 99%|█████████▉| 466/470 [03:39<00:01,  3.40it/s] 99%|█████████▉| 467/470 [03:39<00:00,  3.42it/s]100%|█████████▉| 468/470 [03:39<00:00,  3.43it/s]100%|█████████▉| 469/470 [03:40<00:00,  3.34it/s]100%|██████████| 470/470 [03:40<00:00,  3.60it/s][INFO|trainer.py:2140] 2023-08-29 04:58:49,361 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:58:49,362 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 04:58:49,362 >>   Batch size = 8
{'eval_loss': 1.1186500787734985, 'eval_runtime': 9.8466, 'eval_samples_per_second': 353.116, 'eval_steps_per_second': 44.178, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.15it/s][A
  3%|▎         | 12/435 [00:00<00:08, 48.84it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.09it/s][A
  5%|▌         | 22/435 [00:00<00:09, 45.60it/s][A
  6%|▌         | 27/435 [00:00<00:09, 44.94it/s][A
  7%|▋         | 32/435 [00:00<00:09, 44.65it/s][A
  9%|▊         | 37/435 [00:00<00:08, 44.55it/s][A
 10%|▉         | 42/435 [00:00<00:09, 42.37it/s][A
 11%|█         | 47/435 [00:01<00:08, 43.17it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 43.70it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.20it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.40it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.41it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.34it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 44.33it/s][A
 19%|█▉        | 82/435 [00:01<00:08, 44.03it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.10it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.27it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.49it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.65it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.74it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.68it/s][A
 27%|██▋       | 117/435 [00:02<00:08, 39.09it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 40.74it/s][A
 29%|██▉       | 127/435 [00:02<00:07, 41.94it/s][A
 30%|███       | 132/435 [00:03<00:07, 42.77it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 43.19it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 43.69it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 44.00it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.11it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 43.92it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 43.85it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 44.00it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.31it/s][A
 41%|████      | 177/435 [00:04<00:05, 44.51it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.60it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.66it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 44.66it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 44.47it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 44.20it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 44.10it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 44.25it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 44.42it/s][A
 51%|█████     | 222/435 [00:05<00:04, 44.59it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.62it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.77it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.64it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.42it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.16it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 42.98it/s][A
 59%|█████▉    | 257/435 [00:05<00:04, 43.50it/s][A
 60%|██████    | 262/435 [00:05<00:03, 43.99it/s][A
 61%|██████▏   | 267/435 [00:06<00:03, 44.25it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.49it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.57it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.55it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.32it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.06it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.11it/s][A
 69%|██████▉   | 302/435 [00:06<00:03, 44.24it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.51it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.65it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.72it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.76it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.60it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.29it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.15it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 44.14it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 44.30it/s][A
 81%|████████  | 352/435 [00:07<00:01, 44.43it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 44.63it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 44.71it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.80it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 44.59it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 44.36it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 44.16it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 42.78it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 43.37it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 43.76it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 43.02it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 43.63it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 44.04it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 44.15it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.15it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 31.24it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 34.54it/s][A                                                 
                                                 [A100%|██████████| 470/470 [03:50<00:00,  3.60it/s]
100%|██████████| 435/435 [00:10<00:00, 34.54it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:58:59,525 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-470
[INFO|configuration_utils.py:351] 2023-08-29 04:58:59,701 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-470/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:59:02,885 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-470/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:59:03,051 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-470/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:59:03,135 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-470/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 04:59:11,816 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 04:59:11,868 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-94 (score: 1.0685977935791016).
                                                 100%|██████████| 470/470 [04:12<00:00,  3.60it/s]100%|██████████| 470/470 [04:12<00:00,  1.86it/s]
[INFO|trainer.py:1894] 2023-08-29 04:59:21,651 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 04:59:21,807 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:59:24,670 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:59:24,812 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:59:24,878 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 04:59:25,417 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:59:25,417 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:59:25,417 >>   train_loss               =     0.4181
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:59:25,417 >>   train_runtime            = 0:04:12.55
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:59:25,417 >>   train_samples            =       6000
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:59:25,417 >>   train_samples_per_second =    118.786
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:59:25,417 >>   train_steps_per_second   =      1.861
{'eval_loss': 1.121766448020935, 'eval_runtime': 10.022, 'eval_samples_per_second': 346.938, 'eval_steps_per_second': 43.405, 'epoch': 5.0}
{'train_runtime': 252.5548, 'train_samples_per_second': 118.786, 'train_steps_per_second': 1.861, 'train_loss': 0.41811831859832116, 'epoch': 5.0}
08/29/2023 04:59:25 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 04:59:25,764 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:59:25,764 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 04:59:25,764 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|▏         | 6/435 [00:00<00:07, 54.92it/s]  3%|▎         | 12/435 [00:00<00:08, 49.17it/s]  4%|▍         | 17/435 [00:00<00:08, 47.47it/s]  5%|▌         | 22/435 [00:00<00:08, 46.60it/s]  6%|▌         | 27/435 [00:00<00:08, 46.04it/s]  7%|▋         | 32/435 [00:00<00:08, 45.81it/s]  9%|▊         | 37/435 [00:00<00:08, 45.62it/s] 10%|▉         | 42/435 [00:00<00:08, 45.26it/s] 11%|█         | 47/435 [00:01<00:08, 44.68it/s] 12%|█▏        | 52/435 [00:01<00:08, 44.55it/s] 13%|█▎        | 57/435 [00:01<00:08, 44.53it/s] 14%|█▍        | 62/435 [00:01<00:08, 44.70it/s] 15%|█▌        | 67/435 [00:01<00:08, 44.75it/s] 17%|█▋        | 72/435 [00:01<00:08, 44.82it/s] 18%|█▊        | 77/435 [00:01<00:07, 44.91it/s] 19%|█▉        | 82/435 [00:01<00:07, 45.03it/s] 20%|██        | 87/435 [00:01<00:07, 44.90it/s] 21%|██        | 92/435 [00:02<00:07, 43.31it/s] 22%|██▏       | 97/435 [00:02<00:07, 43.68it/s] 23%|██▎       | 102/435 [00:02<00:07, 43.95it/s] 25%|██▍       | 107/435 [00:02<00:07, 44.19it/s] 26%|██▌       | 112/435 [00:02<00:07, 44.40it/s] 27%|██▋       | 117/435 [00:02<00:07, 44.56it/s] 28%|██▊       | 122/435 [00:02<00:06, 44.77it/s] 29%|██▉       | 127/435 [00:02<00:06, 44.83it/s] 30%|███       | 132/435 [00:02<00:06, 44.58it/s] 31%|███▏      | 137/435 [00:03<00:06, 44.51it/s] 33%|███▎      | 142/435 [00:03<00:06, 44.53it/s] 34%|███▍      | 147/435 [00:03<00:06, 44.55it/s] 35%|███▍      | 152/435 [00:03<00:06, 44.68it/s] 36%|███▌      | 157/435 [00:03<00:06, 44.72it/s] 37%|███▋      | 162/435 [00:03<00:06, 44.75it/s] 38%|███▊      | 167/435 [00:03<00:05, 44.82it/s] 40%|███▉      | 172/435 [00:03<00:05, 44.87it/s] 41%|████      | 177/435 [00:03<00:05, 44.74it/s] 42%|████▏     | 182/435 [00:04<00:05, 44.65it/s] 43%|████▎     | 187/435 [00:04<00:05, 44.56it/s] 44%|████▍     | 192/435 [00:04<00:05, 44.39it/s] 45%|████▌     | 197/435 [00:04<00:05, 44.43it/s] 46%|████▋     | 202/435 [00:04<00:05, 44.48it/s] 48%|████▊     | 207/435 [00:04<00:05, 44.58it/s] 49%|████▊     | 212/435 [00:04<00:04, 44.61it/s] 50%|████▉     | 217/435 [00:04<00:04, 44.74it/s] 51%|█████     | 222/435 [00:04<00:04, 44.57it/s] 52%|█████▏    | 227/435 [00:05<00:04, 42.90it/s] 53%|█████▎    | 232/435 [00:05<00:04, 43.52it/s] 54%|█████▍    | 237/435 [00:05<00:04, 43.95it/s] 56%|█████▌    | 242/435 [00:05<00:04, 44.21it/s] 57%|█████▋    | 247/435 [00:05<00:04, 44.30it/s] 58%|█████▊    | 252/435 [00:05<00:04, 44.38it/s] 59%|█████▉    | 257/435 [00:05<00:04, 44.48it/s] 60%|██████    | 262/435 [00:05<00:03, 44.59it/s] 61%|██████▏   | 267/435 [00:05<00:03, 44.46it/s] 63%|██████▎   | 272/435 [00:06<00:03, 44.47it/s] 64%|██████▎   | 277/435 [00:06<00:03, 44.65it/s] 65%|██████▍   | 282/435 [00:06<00:03, 44.76it/s] 66%|██████▌   | 287/435 [00:06<00:03, 44.77it/s] 67%|██████▋   | 292/435 [00:06<00:03, 44.70it/s] 68%|██████▊   | 297/435 [00:06<00:03, 44.65it/s] 69%|██████▉   | 302/435 [00:06<00:02, 44.69it/s] 71%|███████   | 307/435 [00:06<00:02, 44.47it/s] 72%|███████▏  | 312/435 [00:06<00:02, 44.34it/s] 73%|███████▎  | 317/435 [00:07<00:02, 44.55it/s] 74%|███████▍  | 322/435 [00:07<00:02, 44.60it/s] 75%|███████▌  | 327/435 [00:07<00:02, 44.72it/s] 76%|███████▋  | 332/435 [00:07<00:02, 44.83it/s] 77%|███████▋  | 337/435 [00:07<00:02, 44.86it/s] 79%|███████▊  | 342/435 [00:07<00:02, 44.88it/s] 80%|███████▉  | 347/435 [00:07<00:01, 44.75it/s] 81%|████████  | 352/435 [00:07<00:01, 44.67it/s] 82%|████████▏ | 357/435 [00:07<00:01, 44.62it/s] 83%|████████▎ | 362/435 [00:08<00:01, 44.69it/s] 84%|████████▍ | 367/435 [00:08<00:01, 44.68it/s] 86%|████████▌ | 372/435 [00:08<00:01, 44.69it/s] 87%|████████▋ | 377/435 [00:08<00:01, 44.79it/s] 88%|████████▊ | 382/435 [00:08<00:01, 44.82it/s] 89%|████████▉ | 387/435 [00:08<00:01, 43.61it/s] 90%|█████████ | 392/435 [00:08<00:00, 43.94it/s] 91%|█████████▏| 397/435 [00:08<00:00, 44.12it/s] 92%|█████████▏| 402/435 [00:08<00:00, 44.29it/s] 94%|█████████▎| 407/435 [00:09<00:00, 44.40it/s] 95%|█████████▍| 412/435 [00:09<00:00, 44.53it/s] 96%|█████████▌| 417/435 [00:09<00:00, 44.56it/s] 97%|█████████▋| 422/435 [00:09<00:00, 44.65it/s] 98%|█████████▊| 427/435 [00:09<00:00, 44.61it/s] 99%|█████████▉| 432/435 [00:09<00:00, 44.61it/s]100%|██████████| 435/435 [00:09<00:00, 44.65it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 04:59:35,529 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:59:35,529 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:59:35,529 >>   eval_loss               =     1.0686
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:59:35,529 >>   eval_runtime            = 0:00:09.76
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:59:35,529 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:59:35,529 >>   eval_samples_per_second =    356.085
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:59:35,529 >>   eval_steps_per_second   =     44.549
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:59:35,529 >>   perplexity              =     2.9113
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:59:44,806 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:59:44,829 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:59:44,829 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:59:44,829 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:59:44,829 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:59:45,659 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:59:45,661 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:59:46,374 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:59:47,423 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:59:47,423 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:59:49,262 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:59:49,273 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:59:49,273 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:59:49,273 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:59:49,273 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:59:49,690 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:59:49,692 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:59:49,972 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:59:50,179 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:59:50,180 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-376
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-188
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-282
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-470
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-94
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:02,  1.41it/s]Extractor Predicting: 4it [00:02,  1.39it/s]Extractor Predicting: 5it [00:03,  1.37it/s]Extractor Predicting: 6it [00:04,  1.34it/s]Extractor Predicting: 7it [00:05,  1.31it/s]Extractor Predicting: 8it [00:05,  1.31it/s]Extractor Predicting: 9it [00:06,  1.32it/s]Extractor Predicting: 10it [00:07,  1.28it/s]Extractor Predicting: 11it [00:08,  1.28it/s]Extractor Predicting: 12it [00:09,  1.27it/s]Extractor Predicting: 13it [00:09,  1.30it/s]Extractor Predicting: 14it [00:10,  1.27it/s]Extractor Predicting: 15it [00:11,  1.28it/s]Extractor Predicting: 16it [00:12,  1.28it/s]Extractor Predicting: 17it [00:12,  1.30it/s]Extractor Predicting: 18it [00:13,  1.31it/s]Extractor Predicting: 19it [00:14,  1.32it/s]Extractor Predicting: 20it [00:15,  1.31it/s]Extractor Predicting: 21it [00:16,  1.29it/s]Extractor Predicting: 22it [00:16,  1.31it/s]Extractor Predicting: 23it [00:17,  1.32it/s]Extractor Predicting: 24it [00:18,  1.33it/s]Extractor Predicting: 25it [00:19,  1.28it/s]Extractor Predicting: 26it [00:19,  1.27it/s]Extractor Predicting: 27it [00:20,  1.29it/s]Extractor Predicting: 28it [00:21,  1.27it/s]Extractor Predicting: 29it [00:22,  1.26it/s]Extractor Predicting: 30it [00:23,  1.27it/s]Extractor Predicting: 31it [00:23,  1.27it/s]Extractor Predicting: 32it [00:24,  1.28it/s]Extractor Predicting: 33it [00:25,  1.23it/s]Extractor Predicting: 34it [00:26,  1.25it/s]Extractor Predicting: 35it [00:26,  1.29it/s]Extractor Predicting: 36it [00:27,  1.32it/s]Extractor Predicting: 37it [00:28,  1.31it/s]Extractor Predicting: 38it [00:29,  1.26it/s]Extractor Predicting: 39it [00:30,  1.29it/s]Extractor Predicting: 40it [00:30,  1.29it/s]Extractor Predicting: 41it [00:31,  1.31it/s]Extractor Predicting: 42it [00:32,  1.24it/s]Extractor Predicting: 43it [00:33,  1.24it/s]Extractor Predicting: 44it [00:34,  1.25it/s]Extractor Predicting: 45it [00:34,  1.24it/s]Extractor Predicting: 46it [00:35,  1.27it/s]Extractor Predicting: 47it [00:36,  1.29it/s]Extractor Predicting: 48it [00:37,  1.29it/s]Extractor Predicting: 49it [00:37,  1.30it/s]Extractor Predicting: 50it [00:38,  1.34it/s]Extractor Predicting: 51it [00:39,  1.33it/s]Extractor Predicting: 52it [00:40,  1.29it/s]Extractor Predicting: 53it [00:40,  1.29it/s]Extractor Predicting: 54it [00:41,  1.31it/s]Extractor Predicting: 55it [00:42,  1.30it/s]Extractor Predicting: 56it [00:43,  1.29it/s]Extractor Predicting: 57it [00:44,  1.27it/s]Extractor Predicting: 58it [00:44,  1.30it/s]Extractor Predicting: 59it [00:45,  1.33it/s]Extractor Predicting: 60it [00:46,  1.34it/s]Extractor Predicting: 61it [00:47,  1.34it/s]Extractor Predicting: 62it [00:47,  1.35it/s]Extractor Predicting: 63it [00:48,  1.35it/s]Extractor Predicting: 64it [00:49,  1.35it/s]Extractor Predicting: 65it [00:49,  1.35it/s]Extractor Predicting: 66it [00:50,  1.33it/s]Extractor Predicting: 67it [00:51,  1.33it/s]Extractor Predicting: 68it [00:52,  1.34it/s]Extractor Predicting: 69it [00:52,  1.34it/s]Extractor Predicting: 70it [00:53,  1.33it/s]Extractor Predicting: 71it [00:54,  1.32it/s]Extractor Predicting: 72it [00:55,  1.33it/s]Extractor Predicting: 73it [00:55,  1.33it/s]Extractor Predicting: 74it [00:56,  1.35it/s]Extractor Predicting: 75it [00:57,  1.37it/s]Extractor Predicting: 76it [00:58,  1.35it/s]Extractor Predicting: 77it [00:59,  1.30it/s]Extractor Predicting: 78it [00:59,  1.29it/s]Extractor Predicting: 79it [01:00,  1.29it/s]Extractor Predicting: 80it [01:01,  1.29it/s]Extractor Predicting: 81it [01:02,  1.26it/s]Extractor Predicting: 82it [01:02,  1.29it/s]Extractor Predicting: 83it [01:03,  1.31it/s]Extractor Predicting: 84it [01:04,  1.31it/s]Extractor Predicting: 85it [01:05,  1.29it/s]Extractor Predicting: 86it [01:05,  1.31it/s]Extractor Predicting: 87it [01:06,  1.30it/s]Extractor Predicting: 88it [01:07,  1.29it/s]Extractor Predicting: 89it [01:08,  1.31it/s]Extractor Predicting: 90it [01:09,  1.31it/s]Extractor Predicting: 91it [01:09,  1.35it/s]Extractor Predicting: 92it [01:10,  1.37it/s]Extractor Predicting: 93it [01:11,  1.39it/s]Extractor Predicting: 94it [01:11,  1.35it/s]Extractor Predicting: 95it [01:12,  1.39it/s]Extractor Predicting: 96it [01:13,  1.38it/s]Extractor Predicting: 97it [01:14,  1.38it/s]Extractor Predicting: 98it [01:14,  1.34it/s]Extractor Predicting: 99it [01:15,  1.32it/s]Extractor Predicting: 100it [01:16,  1.30it/s]Extractor Predicting: 101it [01:17,  1.29it/s]Extractor Predicting: 102it [01:17,  1.34it/s]Extractor Predicting: 103it [01:18,  1.37it/s]Extractor Predicting: 104it [01:19,  1.36it/s]Extractor Predicting: 105it [01:20,  1.36it/s]Extractor Predicting: 106it [01:20,  1.36it/s]Extractor Predicting: 107it [01:21,  1.35it/s]Extractor Predicting: 108it [01:22,  1.34it/s]Extractor Predicting: 109it [01:23,  1.34it/s]Extractor Predicting: 110it [01:23,  1.35it/s]Extractor Predicting: 111it [01:24,  1.36it/s]Extractor Predicting: 112it [01:25,  1.34it/s]Extractor Predicting: 113it [01:25,  1.38it/s]Extractor Predicting: 114it [01:26,  1.41it/s]Extractor Predicting: 115it [01:27,  1.39it/s]Extractor Predicting: 116it [01:28,  1.39it/s]Extractor Predicting: 117it [01:28,  1.39it/s]Extractor Predicting: 118it [01:29,  1.37it/s]Extractor Predicting: 119it [01:30,  1.38it/s]Extractor Predicting: 120it [01:31,  1.34it/s]Extractor Predicting: 121it [01:31,  1.32it/s]Extractor Predicting: 122it [01:32,  1.23it/s]Extractor Predicting: 123it [01:33,  1.25it/s]Extractor Predicting: 124it [01:34,  1.26it/s]Extractor Predicting: 125it [01:35,  1.28it/s]Extractor Predicting: 126it [01:35,  1.29it/s]Extractor Predicting: 127it [01:36,  1.29it/s]Extractor Predicting: 128it [01:37,  1.30it/s]Extractor Predicting: 129it [01:38,  1.31it/s]Extractor Predicting: 130it [01:38,  1.29it/s]Extractor Predicting: 131it [01:39,  1.30it/s]Extractor Predicting: 132it [01:40,  1.29it/s]Extractor Predicting: 133it [01:41,  1.31it/s]Extractor Predicting: 134it [01:41,  1.34it/s]Extractor Predicting: 135it [01:42,  1.33it/s]Extractor Predicting: 136it [01:43,  1.30it/s]Extractor Predicting: 137it [01:44,  1.34it/s]Extractor Predicting: 138it [01:44,  1.33it/s]Extractor Predicting: 139it [01:45,  1.30it/s]Extractor Predicting: 140it [01:46,  1.32it/s]Extractor Predicting: 141it [01:47,  1.33it/s]Extractor Predicting: 142it [01:48,  1.32it/s]Extractor Predicting: 143it [01:48,  1.34it/s]Extractor Predicting: 144it [01:49,  1.41it/s]Extractor Predicting: 144it [01:49,  1.32it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:51,426 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:51,456 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:51,456 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:51,456 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:51,456 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:01:52,201 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:01:52,202 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:01:52,822 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:01:53,928 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:01:53,929 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:56,970 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:57,008 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:57,008 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:57,008 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:57,008 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:01:57,890 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:01:57,891 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:01:58,483 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:01:58,714 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:01:58,714 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.30499766464269035,
  "recall": 0.18780557952257693,
  "score": 0.23246707013171947,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.33it/s]Extractor Predicting: 2it [00:01,  1.29it/s]Extractor Predicting: 3it [00:02,  1.30it/s]Extractor Predicting: 4it [00:03,  1.23it/s]Extractor Predicting: 5it [00:03,  1.25it/s]Extractor Predicting: 6it [00:04,  1.28it/s]Extractor Predicting: 7it [00:05,  1.33it/s]Extractor Predicting: 8it [00:06,  1.38it/s]Extractor Predicting: 9it [00:06,  1.35it/s]Extractor Predicting: 10it [00:07,  1.34it/s]Extractor Predicting: 11it [00:08,  1.35it/s]Extractor Predicting: 12it [00:09,  1.34it/s]Extractor Predicting: 13it [00:09,  1.34it/s]Extractor Predicting: 14it [00:10,  1.35it/s]Extractor Predicting: 15it [00:11,  1.33it/s]Extractor Predicting: 16it [00:12,  1.34it/s]Extractor Predicting: 17it [00:12,  1.33it/s]Extractor Predicting: 18it [00:13,  1.36it/s]Extractor Predicting: 19it [00:14,  1.39it/s]Extractor Predicting: 20it [00:15,  1.33it/s]Extractor Predicting: 21it [00:15,  1.35it/s]Extractor Predicting: 22it [00:16,  1.37it/s]Extractor Predicting: 23it [00:17,  1.37it/s]Extractor Predicting: 24it [00:17,  1.37it/s]Extractor Predicting: 25it [00:18,  1.35it/s]Extractor Predicting: 26it [00:19,  1.36it/s]Extractor Predicting: 27it [00:20,  1.34it/s]Extractor Predicting: 28it [00:20,  1.35it/s]Extractor Predicting: 29it [00:21,  1.37it/s]Extractor Predicting: 30it [00:22,  1.38it/s]Extractor Predicting: 31it [00:23,  1.36it/s]Extractor Predicting: 32it [00:23,  1.41it/s]Extractor Predicting: 33it [00:24,  1.38it/s]Extractor Predicting: 34it [00:25,  1.36it/s]Extractor Predicting: 35it [00:26,  1.35it/s]Extractor Predicting: 36it [00:26,  1.36it/s]Extractor Predicting: 37it [00:27,  1.38it/s]Extractor Predicting: 38it [00:28,  1.39it/s]Extractor Predicting: 39it [00:28,  1.38it/s]Extractor Predicting: 40it [00:29,  1.36it/s]Extractor Predicting: 41it [00:30,  1.36it/s]Extractor Predicting: 42it [00:31,  1.39it/s]Extractor Predicting: 43it [00:31,  1.35it/s]Extractor Predicting: 44it [00:32,  1.35it/s]Extractor Predicting: 45it [00:33,  1.33it/s]Extractor Predicting: 46it [00:34,  1.32it/s]Extractor Predicting: 47it [00:34,  1.34it/s]Extractor Predicting: 48it [00:35,  1.33it/s]Extractor Predicting: 49it [00:36,  1.36it/s]Extractor Predicting: 50it [00:37,  1.36it/s]Extractor Predicting: 51it [00:37,  1.35it/s]Extractor Predicting: 52it [00:38,  1.32it/s]Extractor Predicting: 53it [00:39,  1.35it/s]Extractor Predicting: 54it [00:40,  1.32it/s]Extractor Predicting: 55it [00:40,  1.36it/s]Extractor Predicting: 56it [00:41,  1.35it/s]Extractor Predicting: 57it [00:42,  1.32it/s]Extractor Predicting: 58it [00:43,  1.31it/s]Extractor Predicting: 59it [00:43,  1.30it/s]Extractor Predicting: 60it [00:44,  1.32it/s]Extractor Predicting: 61it [00:45,  1.30it/s]Extractor Predicting: 62it [00:46,  1.30it/s]Extractor Predicting: 63it [00:46,  1.32it/s]Extractor Predicting: 64it [00:47,  1.31it/s]Extractor Predicting: 65it [00:48,  1.35it/s]Extractor Predicting: 66it [00:49,  1.32it/s]Extractor Predicting: 67it [00:49,  1.31it/s]Extractor Predicting: 68it [00:50,  1.30it/s]Extractor Predicting: 69it [00:51,  1.30it/s]Extractor Predicting: 70it [00:52,  1.28it/s]Extractor Predicting: 71it [00:53,  1.30it/s]Extractor Predicting: 72it [00:53,  1.30it/s]Extractor Predicting: 73it [00:54,  1.27it/s]Extractor Predicting: 74it [00:55,  1.30it/s]Extractor Predicting: 75it [00:56,  1.30it/s]Extractor Predicting: 76it [00:56,  1.32it/s]Extractor Predicting: 77it [00:57,  1.30it/s]Extractor Predicting: 78it [00:58,  1.33it/s]Extractor Predicting: 79it [00:59,  1.33it/s]Extractor Predicting: 80it [00:59,  1.38it/s]Extractor Predicting: 81it [01:00,  1.38it/s]Extractor Predicting: 82it [01:01,  1.37it/s]Extractor Predicting: 83it [01:02,  1.36it/s]Extractor Predicting: 84it [01:02,  1.36it/s]Extractor Predicting: 85it [01:03,  1.33it/s]Extractor Predicting: 86it [01:04,  1.29it/s]Extractor Predicting: 87it [01:05,  1.25it/s]Extractor Predicting: 88it [01:05,  1.29it/s]Extractor Predicting: 89it [01:06,  1.28it/s]Extractor Predicting: 90it [01:07,  1.26it/s]Extractor Predicting: 91it [01:08,  1.24it/s]Extractor Predicting: 92it [01:09,  1.26it/s]Extractor Predicting: 93it [01:09,  1.28it/s]Extractor Predicting: 94it [01:10,  1.29it/s]Extractor Predicting: 95it [01:11,  1.27it/s]Extractor Predicting: 96it [01:12,  1.28it/s]Extractor Predicting: 97it [01:13,  1.27it/s]Extractor Predicting: 98it [01:13,  1.24it/s]Extractor Predicting: 99it [01:14,  1.25it/s]Extractor Predicting: 100it [01:15,  1.28it/s]Extractor Predicting: 101it [01:16,  1.29it/s]Extractor Predicting: 102it [01:16,  1.30it/s]Extractor Predicting: 103it [01:17,  1.27it/s]Extractor Predicting: 104it [01:18,  1.26it/s]Extractor Predicting: 105it [01:19,  1.29it/s]Extractor Predicting: 106it [01:20,  1.28it/s]Extractor Predicting: 107it [01:20,  1.29it/s]Extractor Predicting: 108it [01:21,  1.30it/s]Extractor Predicting: 109it [01:22,  1.29it/s]Extractor Predicting: 110it [01:23,  1.28it/s]Extractor Predicting: 111it [01:23,  1.29it/s]Extractor Predicting: 112it [01:24,  1.29it/s]Extractor Predicting: 113it [01:25,  1.29it/s]Extractor Predicting: 114it [01:26,  1.20it/s]Extractor Predicting: 115it [01:27,  1.21it/s]Extractor Predicting: 116it [01:28,  1.20it/s]Extractor Predicting: 117it [01:28,  1.21it/s]Extractor Predicting: 118it [01:29,  1.24it/s]Extractor Predicting: 119it [01:30,  1.23it/s]Extractor Predicting: 120it [01:31,  1.24it/s]Extractor Predicting: 121it [01:32,  1.26it/s]Extractor Predicting: 122it [01:32,  1.29it/s]Extractor Predicting: 123it [01:33,  1.30it/s]Extractor Predicting: 124it [01:34,  1.30it/s]Extractor Predicting: 125it [01:35,  1.30it/s]Extractor Predicting: 126it [01:35,  1.34it/s]Extractor Predicting: 127it [01:36,  1.34it/s]Extractor Predicting: 128it [01:37,  1.32it/s]Extractor Predicting: 129it [01:38,  1.35it/s]Extractor Predicting: 130it [01:38,  1.34it/s]Extractor Predicting: 131it [01:39,  1.32it/s]Extractor Predicting: 132it [01:40,  1.34it/s]Extractor Predicting: 133it [01:41,  1.37it/s]Extractor Predicting: 134it [01:41,  1.39it/s]Extractor Predicting: 135it [01:42,  1.34it/s]Extractor Predicting: 136it [01:43,  1.34it/s]Extractor Predicting: 137it [01:44,  1.36it/s]Extractor Predicting: 138it [01:44,  1.40it/s]Extractor Predicting: 139it [01:45,  1.38it/s]Extractor Predicting: 140it [01:46,  1.37it/s]Extractor Predicting: 141it [01:46,  1.34it/s]Extractor Predicting: 142it [01:47,  1.36it/s]Extractor Predicting: 143it [01:48,  1.33it/s]Extractor Predicting: 144it [01:49,  1.32it/s]Extractor Predicting: 145it [01:50,  1.31it/s]Extractor Predicting: 146it [01:50,  1.34it/s]Extractor Predicting: 147it [01:51,  1.39it/s]Extractor Predicting: 148it [01:52,  1.39it/s]Extractor Predicting: 149it [01:52,  1.40it/s]Extractor Predicting: 150it [01:53,  1.42it/s]Extractor Predicting: 151it [01:54,  1.42it/s]Extractor Predicting: 152it [01:54,  1.43it/s]Extractor Predicting: 153it [01:55,  1.43it/s]Extractor Predicting: 154it [01:56,  1.45it/s]Extractor Predicting: 155it [01:56,  1.41it/s]Extractor Predicting: 156it [01:57,  1.43it/s]Extractor Predicting: 157it [01:58,  1.46it/s]Extractor Predicting: 158it [01:59,  1.45it/s]Extractor Predicting: 159it [01:59,  1.50it/s]Extractor Predicting: 160it [02:00,  1.53it/s]Extractor Predicting: 161it [02:00,  1.47it/s]Extractor Predicting: 162it [02:01,  1.44it/s]Extractor Predicting: 163it [02:02,  1.43it/s]Extractor Predicting: 164it [02:03,  1.44it/s]Extractor Predicting: 165it [02:03,  1.46it/s]Extractor Predicting: 166it [02:04,  1.47it/s]Extractor Predicting: 167it [02:05,  1.47it/s]Extractor Predicting: 168it [02:05,  1.44it/s]Extractor Predicting: 169it [02:06,  1.46it/s]Extractor Predicting: 170it [02:07,  1.45it/s]Extractor Predicting: 171it [02:07,  1.49it/s]Extractor Predicting: 172it [02:08,  1.48it/s]Extractor Predicting: 173it [02:09,  1.48it/s]Extractor Predicting: 174it [02:10,  1.37it/s]Extractor Predicting: 175it [02:10,  1.37it/s]Extractor Predicting: 176it [02:11,  1.35it/s]Extractor Predicting: 177it [02:12,  1.34it/s]Extractor Predicting: 178it [02:13,  1.29it/s]Extractor Predicting: 179it [02:13,  1.29it/s]Extractor Predicting: 180it [02:14,  1.31it/s]Extractor Predicting: 181it [02:15,  1.31it/s]Extractor Predicting: 182it [02:16,  1.29it/s]Extractor Predicting: 183it [02:16,  1.32it/s]Extractor Predicting: 184it [02:17,  1.32it/s]Extractor Predicting: 185it [02:18,  1.32it/s]Extractor Predicting: 186it [02:19,  1.33it/s]Extractor Predicting: 187it [02:19,  1.33it/s]Extractor Predicting: 188it [02:20,  1.31it/s]Extractor Predicting: 189it [02:21,  1.32it/s]Extractor Predicting: 190it [02:22,  1.30it/s]Extractor Predicting: 191it [02:23,  1.28it/s]Extractor Predicting: 192it [02:23,  1.28it/s]Extractor Predicting: 193it [02:24,  1.27it/s]Extractor Predicting: 194it [02:25,  1.26it/s]Extractor Predicting: 195it [02:26,  1.27it/s]Extractor Predicting: 196it [02:26,  1.30it/s]Extractor Predicting: 197it [02:27,  1.32it/s]Extractor Predicting: 198it [02:28,  1.31it/s]Extractor Predicting: 199it [02:29,  1.32it/s]Extractor Predicting: 200it [02:30,  1.29it/s]Extractor Predicting: 201it [02:30,  1.28it/s]Extractor Predicting: 202it [02:31,  1.26it/s]Extractor Predicting: 203it [02:32,  1.24it/s]Extractor Predicting: 204it [02:33,  1.24it/s]Extractor Predicting: 205it [02:34,  1.24it/s]Extractor Predicting: 206it [02:34,  1.24it/s]Extractor Predicting: 207it [02:35,  1.26it/s]Extractor Predicting: 208it [02:36,  1.25it/s]Extractor Predicting: 209it [02:37,  1.20it/s]Extractor Predicting: 210it [02:38,  1.20it/s]Extractor Predicting: 211it [02:39,  1.20it/s]Extractor Predicting: 212it [02:39,  1.23it/s]Extractor Predicting: 213it [02:40,  1.22it/s]Extractor Predicting: 214it [02:41,  1.24it/s]Extractor Predicting: 215it [02:42,  1.22it/s]Extractor Predicting: 216it [02:43,  1.23it/s]Extractor Predicting: 217it [02:43,  1.23it/s]Extractor Predicting: 218it [02:44,  1.23it/s]Extractor Predicting: 219it [02:45,  1.22it/s]Extractor Predicting: 220it [02:46,  1.19it/s]Extractor Predicting: 221it [02:47,  1.18it/s]Extractor Predicting: 222it [02:48,  1.18it/s]Extractor Predicting: 223it [02:49,  1.11it/s]Extractor Predicting: 224it [02:49,  1.16it/s]Extractor Predicting: 225it [02:50,  1.17it/s]Extractor Predicting: 226it [02:51,  1.17it/s]Extractor Predicting: 227it [02:52,  1.20it/s]Extractor Predicting: 228it [02:53,  1.21it/s]Extractor Predicting: 229it [02:54,  1.23it/s]Extractor Predicting: 230it [02:54,  1.25it/s]Extractor Predicting: 231it [02:55,  1.29it/s]Extractor Predicting: 232it [02:56,  1.33it/s]Extractor Predicting: 233it [02:56,  1.35it/s]Extractor Predicting: 234it [02:57,  1.29it/s]Extractor Predicting: 235it [02:58,  1.32it/s]Extractor Predicting: 236it [02:59,  1.32it/s]Extractor Predicting: 237it [03:00,  1.32it/s]Extractor Predicting: 238it [03:00,  1.32it/s]Extractor Predicting: 239it [03:01,  1.30it/s]Extractor Predicting: 240it [03:02,  1.33it/s]Extractor Predicting: 241it [03:03,  1.35it/s]Extractor Predicting: 242it [03:03,  1.37it/s]Extractor Predicting: 243it [03:04,  1.35it/s]Extractor Predicting: 244it [03:05,  1.40it/s]Extractor Predicting: 245it [03:05,  1.37it/s]Extractor Predicting: 246it [03:06,  1.34it/s]Extractor Predicting: 247it [03:07,  1.32it/s]Extractor Predicting: 248it [03:08,  1.28it/s]Extractor Predicting: 249it [03:09,  1.31it/s]Extractor Predicting: 250it [03:09,  1.32it/s]Extractor Predicting: 251it [03:10,  1.34it/s]Extractor Predicting: 252it [03:11,  1.34it/s]Extractor Predicting: 253it [03:11,  1.33it/s]Extractor Predicting: 254it [03:12,  1.30it/s]Extractor Predicting: 255it [03:13,  1.31it/s]Extractor Predicting: 256it [03:14,  1.28it/s]Extractor Predicting: 257it [03:15,  1.25it/s]Extractor Predicting: 258it [03:15,  1.27it/s]Extractor Predicting: 259it [03:16,  1.28it/s]Extractor Predicting: 260it [03:17,  1.27it/s]Extractor Predicting: 261it [03:18,  1.28it/s]Extractor Predicting: 262it [03:19,  1.30it/s]Extractor Predicting: 263it [03:19,  1.32it/s]Extractor Predicting: 264it [03:20,  1.29it/s]Extractor Predicting: 265it [03:21,  1.30it/s]Extractor Predicting: 266it [03:22,  1.28it/s]Extractor Predicting: 267it [03:22,  1.28it/s]Extractor Predicting: 268it [03:23,  1.25it/s]Extractor Predicting: 269it [03:24,  1.27it/s]Extractor Predicting: 270it [03:25,  1.27it/s]Extractor Predicting: 271it [03:26,  1.28it/s]Extractor Predicting: 272it [03:26,  1.30it/s]Extractor Predicting: 273it [03:27,  1.26it/s]Extractor Predicting: 274it [03:28,  1.25it/s]Extractor Predicting: 275it [03:29,  1.26it/s]Extractor Predicting: 276it [03:30,  1.25it/s]Extractor Predicting: 277it [03:30,  1.26it/s]Extractor Predicting: 278it [03:31,  1.28it/s]Extractor Predicting: 279it [03:32,  1.26it/s]Extractor Predicting: 280it [03:33,  1.27it/s]Extractor Predicting: 281it [03:33,  1.30it/s]Extractor Predicting: 282it [03:34,  1.29it/s]Extractor Predicting: 283it [03:35,  1.26it/s]Extractor Predicting: 284it [03:36,  1.32it/s]Extractor Predicting: 285it [03:37,  1.30it/s]Extractor Predicting: 286it [03:37,  1.33it/s]Extractor Predicting: 287it [03:38,  1.32it/s]Extractor Predicting: 288it [03:39,  1.31it/s]Extractor Predicting: 289it [03:40,  1.30it/s]Extractor Predicting: 290it [03:40,  1.29it/s]Extractor Predicting: 291it [03:41,  1.25it/s]Extractor Predicting: 292it [03:42,  1.23it/s]Extractor Predicting: 293it [03:43,  1.26it/s]Extractor Predicting: 294it [03:44,  1.24it/s]Extractor Predicting: 295it [03:44,  1.27it/s]Extractor Predicting: 296it [03:45,  1.25it/s]Extractor Predicting: 297it [03:46,  1.27it/s]Extractor Predicting: 298it [03:47,  1.26it/s]Extractor Predicting: 299it [03:48,  1.25it/s]Extractor Predicting: 300it [03:48,  1.26it/s]Extractor Predicting: 301it [03:49,  1.25it/s]Extractor Predicting: 302it [03:50,  1.28it/s]Extractor Predicting: 303it [03:51,  1.25it/s]Extractor Predicting: 304it [03:52,  1.23it/s]Extractor Predicting: 305it [03:52,  1.24it/s]Extractor Predicting: 306it [03:53,  1.27it/s]Extractor Predicting: 307it [03:54,  1.26it/s]Extractor Predicting: 308it [03:55,  1.27it/s]Extractor Predicting: 309it [03:56,  1.27it/s]Extractor Predicting: 310it [03:56,  1.29it/s]Extractor Predicting: 311it [03:57,  1.28it/s]Extractor Predicting: 312it [03:58,  1.31it/s]Extractor Predicting: 313it [03:58,  1.36it/s]Extractor Predicting: 314it [03:59,  1.38it/s]Extractor Predicting: 315it [04:00,  1.39it/s]Extractor Predicting: 316it [04:01,  1.36it/s]Extractor Predicting: 317it [04:01,  1.32it/s]Extractor Predicting: 318it [04:02,  1.32it/s]Extractor Predicting: 319it [04:03,  1.32it/s]Extractor Predicting: 320it [04:04,  1.30it/s]Extractor Predicting: 321it [04:05,  1.17it/s]Extractor Predicting: 322it [04:06,  1.21it/s]Extractor Predicting: 323it [04:06,  1.21it/s]Extractor Predicting: 324it [04:07,  1.22it/s]Extractor Predicting: 325it [04:08,  1.24it/s]Extractor Predicting: 326it [04:09,  1.26it/s]Extractor Predicting: 327it [04:10,  1.27it/s]Extractor Predicting: 328it [04:10,  1.27it/s]Extractor Predicting: 329it [04:11,  1.29it/s]Extractor Predicting: 330it [04:12,  1.30it/s]Extractor Predicting: 331it [04:13,  1.27it/s]Extractor Predicting: 332it [04:13,  1.27it/s]Extractor Predicting: 333it [04:14,  1.30it/s]Extractor Predicting: 334it [04:15,  1.31it/s]Extractor Predicting: 335it [04:16,  1.29it/s]Extractor Predicting: 336it [04:17,  1.28it/s]Extractor Predicting: 337it [04:17,  1.29it/s]Extractor Predicting: 338it [04:18,  1.31it/s]Extractor Predicting: 339it [04:19,  1.32it/s]Extractor Predicting: 340it [04:19,  1.33it/s]Extractor Predicting: 341it [04:20,  1.33it/s]Extractor Predicting: 342it [04:21,  1.32it/s]Extractor Predicting: 343it [04:22,  1.30it/s]Extractor Predicting: 344it [04:23,  1.33it/s]Extractor Predicting: 345it [04:23,  1.30it/s]Extractor Predicting: 346it [04:24,  1.30it/s]Extractor Predicting: 347it [04:25,  1.29it/s]Extractor Predicting: 348it [04:26,  1.29it/s]Extractor Predicting: 349it [04:26,  1.32it/s]Extractor Predicting: 350it [04:27,  1.33it/s]Extractor Predicting: 351it [04:28,  1.33it/s]Extractor Predicting: 352it [04:29,  1.30it/s]Extractor Predicting: 353it [04:29,  1.32it/s]Extractor Predicting: 354it [04:30,  1.34it/s]Extractor Predicting: 355it [04:31,  1.37it/s]Extractor Predicting: 356it [04:32,  1.34it/s]Extractor Predicting: 357it [04:32,  1.34it/s]Extractor Predicting: 358it [04:33,  1.35it/s]Extractor Predicting: 359it [04:34,  1.34it/s]Extractor Predicting: 360it [04:35,  1.33it/s]Extractor Predicting: 361it [04:35,  1.34it/s]Extractor Predicting: 362it [04:36,  1.34it/s]Extractor Predicting: 363it [04:37,  1.36it/s]Extractor Predicting: 364it [04:38,  1.37it/s]Extractor Predicting: 365it [04:38,  1.39it/s]Extractor Predicting: 366it [04:39,  1.36it/s]Extractor Predicting: 367it [04:40,  1.31it/s]Extractor Predicting: 368it [04:41,  1.31it/s]Extractor Predicting: 369it [04:41,  1.32it/s]Extractor Predicting: 370it [04:42,  1.36it/s]Extractor Predicting: 371it [04:43,  1.35it/s]Extractor Predicting: 372it [04:43,  1.36it/s]Extractor Predicting: 373it [04:44,  1.33it/s]Extractor Predicting: 374it [04:45,  1.35it/s]Extractor Predicting: 375it [04:46,  1.35it/s]Extractor Predicting: 376it [04:46,  1.36it/s]Extractor Predicting: 377it [04:47,  1.37it/s]Extractor Predicting: 378it [04:48,  1.39it/s]Extractor Predicting: 379it [04:49,  1.34it/s]Extractor Predicting: 380it [04:49,  1.34it/s]Extractor Predicting: 381it [04:50,  1.34it/s]Extractor Predicting: 382it [04:51,  1.35it/s]Extractor Predicting: 383it [04:52,  1.37it/s]Extractor Predicting: 384it [04:52,  1.40it/s]Extractor Predicting: 385it [04:53,  1.37it/s]Extractor Predicting: 386it [04:54,  1.36it/s]Extractor Predicting: 387it [04:55,  1.36it/s]Extractor Predicting: 388it [04:55,  1.34it/s]Extractor Predicting: 389it [04:56,  1.35it/s]Extractor Predicting: 390it [04:57,  1.35it/s]Extractor Predicting: 391it [04:58,  1.33it/s]Extractor Predicting: 392it [04:58,  1.34it/s]Extractor Predicting: 393it [04:59,  1.37it/s]Extractor Predicting: 394it [05:00,  1.32it/s]Extractor Predicting: 395it [05:01,  1.21it/s]Extractor Predicting: 396it [05:02,  1.23it/s]Extractor Predicting: 397it [05:02,  1.24it/s]Extractor Predicting: 398it [05:03,  1.25it/s]Extractor Predicting: 399it [05:04,  1.23it/s]Extractor Predicting: 400it [05:05,  1.28it/s]Extractor Predicting: 401it [05:06,  1.26it/s]Extractor Predicting: 402it [05:06,  1.25it/s]Extractor Predicting: 403it [05:07,  1.24it/s]Extractor Predicting: 404it [05:08,  1.20it/s]Extractor Predicting: 405it [05:09,  1.21it/s]Extractor Predicting: 406it [05:10,  1.22it/s]Extractor Predicting: 407it [05:11,  1.19it/s]Extractor Predicting: 408it [05:11,  1.23it/s]Extractor Predicting: 409it [05:12,  1.25it/s]Extractor Predicting: 410it [05:13,  1.22it/s]Extractor Predicting: 411it [05:14,  1.21it/s]Extractor Predicting: 412it [05:15,  1.23it/s]Extractor Predicting: 413it [05:15,  1.26it/s]Extractor Predicting: 414it [05:16,  1.28it/s]Extractor Predicting: 415it [05:17,  1.29it/s]Extractor Predicting: 416it [05:18,  1.29it/s]Extractor Predicting: 417it [05:18,  1.31it/s]Extractor Predicting: 418it [05:19,  1.30it/s]Extractor Predicting: 419it [05:20,  1.23it/s]Extractor Predicting: 420it [05:21,  1.24it/s]Extractor Predicting: 421it [05:21,  1.33it/s]Extractor Predicting: 421it [05:21,  1.31it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:35,454 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:35,471 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:35,472 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:35,472 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:35,472 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:07:36,235 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:07:36,236 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:07:36,843 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:07:37,949 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:07:37,949 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:40,873 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:40,895 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:40,895 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:40,895 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:40,895 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:07:41,666 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:07:41,667 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:07:42,257 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:07:42,466 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:07:42,466 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.19057780695994747,
  "recall": 0.1150074294205052,
  "score": 0.14344844628405512,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.20it/s]Extractor Predicting: 2it [00:01,  1.22it/s]Extractor Predicting: 3it [00:02,  1.21it/s]Extractor Predicting: 4it [00:03,  1.18it/s]Extractor Predicting: 5it [00:04,  1.23it/s]Extractor Predicting: 6it [00:04,  1.21it/s]Extractor Predicting: 7it [00:05,  1.20it/s]Extractor Predicting: 8it [00:06,  1.22it/s]Extractor Predicting: 9it [00:06,  1.60it/s]Extractor Predicting: 9it [00:06,  1.32it/s]
[INFO|configuration_utils.py:515] 2023-08-29 05:07:50,630 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:07:50,631 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 05:07:50,660 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:07:50,661 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 05:07:50,686 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 05:07:58,382 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 05:07:58,392 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 05:07:58,446 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:07:58,447 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 05:07:58,490 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:07:58,519 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:07:58,519 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:07:58,519 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:07:58,519 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:07:58,519 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:07:58,520 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3211009174311927,
  "recall": 0.08641975308641975,
  "score": 0.13618677042801555,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 05:07:58,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:07:59,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:00,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:00,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:01,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:02,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:03,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:04,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:05,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:05,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:06,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:07,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:07,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:08,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:09,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:09,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:10,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:11,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:11,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:12,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:13,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:13,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:15<04:59, 15.79s/it][WARNING|generation_utils.py:914] 2023-08-29 05:08:14,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:15,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:15,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:16,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:17,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:18,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:18,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:19,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:20,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:20,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:21,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:22,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:22,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:23,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:24,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:25,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:25,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:26,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:26,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:27,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:28,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:29,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:29,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:31<04:46, 15.93s/it][WARNING|generation_utils.py:914] 2023-08-29 05:08:30,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:31,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:32,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:32,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:33,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:33,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:34,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:35,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:35,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:36,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:37,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:37,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:38,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:38,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:39,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:40,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:40,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:41,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:42,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:42,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:44<04:05, 14.45s/it][WARNING|generation_utils.py:914] 2023-08-29 05:08:43,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:44,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:44,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:45,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:46,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:47,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:47,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:48,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:49,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:50,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:50,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:51,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:52,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:53,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:53,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:54,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:55,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:56,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:56,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:57,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:59<03:55, 14.70s/it][WARNING|generation_utils.py:914] 2023-08-29 05:08:58,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:59,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:08:59,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:00,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:01,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:01,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:02,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:03,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:04,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:04,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:05,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:06,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:07,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:07,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:08,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:09,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:10,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:10,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:11,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:12,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:14<03:40, 14.72s/it][WARNING|generation_utils.py:914] 2023-08-29 05:09:13,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:13,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:14,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:15,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:16,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:16,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:17,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:18,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:19,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:19,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:20,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:21,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:22,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:22,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:23,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:24,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:25,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:25,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:26,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:28<03:24, 14.61s/it][WARNING|generation_utils.py:914] 2023-08-29 05:09:27,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:28,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:28,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:29,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:30,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:31,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:32,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:32,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:33,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:34,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:34,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:35,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:36,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:37,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:37,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:38,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:39,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:39,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:40,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:41,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:42,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:44<03:13, 14.87s/it][WARNING|generation_utils.py:914] 2023-08-29 05:09:42,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:44,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:44,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:45,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:46,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:47,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:47,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:48,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:49,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:50,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:51,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:52,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:52,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:53,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:54,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:56,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:56,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:57,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:58,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:59,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:00,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:02<03:12, 16.08s/it][WARNING|generation_utils.py:914] 2023-08-29 05:10:01,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:02,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:03,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:03,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:04,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:05,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:05,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:06,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:07,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:08,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:08,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:09,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:10,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:11,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:11,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:12,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:13,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:13,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:14,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:15,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:17<02:51, 15.60s/it][WARNING|generation_utils.py:914] 2023-08-29 05:10:16,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:16,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:17,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:18,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:18,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:19,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:20,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:21,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:21,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:22,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:23,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:24,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:25,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:26,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:26,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:27,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:28,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:29,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:29,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:30,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:31,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:33<02:37, 15.74s/it][WARNING|generation_utils.py:914] 2023-08-29 05:10:32,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:33,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:33,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:34,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:35,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:35,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:36,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:37,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:38,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:38,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:39,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:40,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:41,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:42,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:42,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:43,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:44,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:44,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:45,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:46,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:48<02:19, 15.45s/it][WARNING|generation_utils.py:914] 2023-08-29 05:10:47,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:47,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:48,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:49,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:50,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:50,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:51,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:52,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:53,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:53,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:54,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:55,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:56,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:56,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:57,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:58,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:59,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:59,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:00,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:01,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:03<02:03, 15.39s/it][WARNING|generation_utils.py:914] 2023-08-29 05:11:02,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:03,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:03,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:04,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:05,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:05,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:06,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:07,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:08,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:08,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:09,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:10,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:11,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:11,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:12,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:13,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:13,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:14,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:15,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:16,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:16,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:18<01:47, 15.38s/it][WARNING|generation_utils.py:914] 2023-08-29 05:11:17,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:18,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:18,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:19,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:20,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:20,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:21,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:21,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:22,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:23,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:24,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:24,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:25,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:26,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:26,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:27,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:28,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:29,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:29,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:30,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:31<01:28, 14.70s/it][WARNING|generation_utils.py:914] 2023-08-29 05:11:30,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:31,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:32,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:33,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:33,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:34,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:35,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:36,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:36,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:37,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:38,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:39,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:39,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:40,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:41,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:41,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:42,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:43,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:44,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:44,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:45,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:47<01:14, 14.99s/it][WARNING|generation_utils.py:914] 2023-08-29 05:11:46,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:47,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:47,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:48,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:49,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:50,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:50,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:51,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:52,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:53,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:54,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:54,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:55,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:56,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:57,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:57,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:58,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:59,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:59,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:00,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:02<00:59, 14.96s/it][WARNING|generation_utils.py:914] 2023-08-29 05:12:01,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:01,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:02,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:03,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:03,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:04,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:05,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:05,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:06,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:07,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:07,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:08,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:09,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:09,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:10,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:11,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:11,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:12,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:13,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:13,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:14,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:16<00:43, 14.55s/it][WARNING|generation_utils.py:914] 2023-08-29 05:12:14,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:15,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:16,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:17,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:17,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:18,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:19,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:20,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:20,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:21,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:22,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:22,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:23,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:24,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:25,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:25,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:26,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:27,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:27,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:28,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:29,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:30,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:31<00:29, 14.94s/it][WARNING|generation_utils.py:914] 2023-08-29 05:12:30,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:31,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:32,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:33,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:33,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:34,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:35,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:36,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:36,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:37,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:38,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:39,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:40,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:41,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:42,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:42,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:43,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:44,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:44,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:45,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:46,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:48<00:15, 15.46s/it][WARNING|generation_utils.py:914] 2023-08-29 05:12:47,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:48,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:48,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:49,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:50,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:51,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:51,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:52,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:53,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:54,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:55,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:56,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:56,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:57,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:58,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:59,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:59,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:00,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:01,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:02,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:03<00:00, 15.44s/it]Generating: 100%|██████████| 20/20 [05:04<00:00, 15.20s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:11,474 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:11,502 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:11,502 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:11,502 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:11,502 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:13:11,962 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:13:11,963 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:13:12,286 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:13:13,433 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:13:13,433 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:15,731 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:15,766 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:15,766 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:15,766 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:13:15,766 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:13:16,283 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:13:16,284 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:13:17,034 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:13:17,294 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:13:17,294 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8664772727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : genre .', 'success_rate': 0.8328804347826086, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : head of government .', 'success_rate': 0.95, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : military branch .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : winner .', 'success_rate': 0.95, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 221, 'raw': 224}
{'target': 600, 'success': 253, 'raw': 256}
{'target': 600, 'success': 285, 'raw': 288}
{'target': 600, 'success': 317, 'raw': 320}
{'target': 600, 'success': 349, 'raw': 352}
{'target': 600, 'success': 381, 'raw': 384}
{'target': 600, 'success': 413, 'raw': 416}
{'target': 600, 'success': 445, 'raw': 448}
{'target': 600, 'success': 477, 'raw': 480}
{'target': 600, 'success': 509, 'raw': 512}
{'target': 600, 'success': 540, 'raw': 544}
{'target': 600, 'success': 572, 'raw': 576}
{'target': 600, 'success': 604, 'raw': 608}
{'prompt': 'Relation : characters .', 'success_rate': 0.993421052631579, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.9285714285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : crosses .', 'success_rate': 0.9107142857142857, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.940625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9315476190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9421875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : located on terrain feature . Context : The city is located on the border of the Pristina and Vichy provinces in the eastern part of northern France . Head Entity : Pristina , Tail Entity : western France .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.953125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.9241071428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9609375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : participant .', 'success_rate': 0.9330357142857143, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : participating team .', 'success_rate': 0.9375, 'errors': {''}}
['Relation : platform . Context : The game is a spin on " The Legend of Zelda " and is a remake of the " Zelda " series . Head Entity : The Legend of Zelda , Tail Entity : Nintendo .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.9092261904761905, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.8693181818181818, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : publisher .', 'success_rate': 0.9345238095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : spouse .', 'success_rate': 0.946875, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/synthetic/3_ext.jsonl'}}
estimate vocab size: 9449
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9549, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.51it/s]Extractor Estimating: 2it [00:01,  1.40it/s]Extractor Estimating: 3it [00:02,  1.43it/s]Extractor Estimating: 4it [00:02,  1.45it/s]Extractor Estimating: 5it [00:03,  1.42it/s]Extractor Estimating: 6it [00:04,  1.41it/s]Extractor Estimating: 7it [00:05,  1.35it/s]Extractor Estimating: 8it [00:05,  1.45it/s]Extractor Estimating: 9it [00:06,  1.45it/s]Extractor Estimating: 10it [00:06,  1.50it/s]Extractor Estimating: 11it [00:07,  1.55it/s]Extractor Estimating: 12it [00:08,  1.57it/s]Extractor Estimating: 13it [00:08,  1.60it/s]Extractor Estimating: 14it [00:09,  1.53it/s]Extractor Estimating: 15it [00:10,  1.53it/s]Extractor Estimating: 16it [00:10,  1.63it/s]Extractor Estimating: 17it [00:11,  1.55it/s]Extractor Estimating: 18it [00:11,  1.61it/s]Extractor Estimating: 19it [00:12,  1.56it/s]Extractor Estimating: 20it [00:13,  1.54it/s]Extractor Estimating: 21it [00:13,  1.59it/s]Extractor Estimating: 22it [00:14,  1.58it/s]Extractor Estimating: 23it [00:15,  1.61it/s]Extractor Estimating: 24it [00:15,  1.59it/s]Extractor Estimating: 25it [00:16,  1.60it/s]Extractor Estimating: 26it [00:17,  1.51it/s]Extractor Estimating: 27it [00:17,  1.43it/s]Extractor Estimating: 28it [00:18,  1.36it/s]Extractor Estimating: 29it [00:19,  1.36it/s]Extractor Estimating: 30it [00:20,  1.31it/s]Extractor Estimating: 31it [00:21,  1.30it/s]Extractor Estimating: 32it [00:21,  1.32it/s]Extractor Estimating: 33it [00:22,  1.38it/s]Extractor Estimating: 34it [00:23,  1.37it/s]Extractor Estimating: 35it [00:23,  1.38it/s]Extractor Estimating: 36it [00:24,  1.34it/s]Extractor Estimating: 37it [00:25,  1.33it/s]Extractor Estimating: 38it [00:26,  1.33it/s]Extractor Estimating: 39it [00:27,  1.30it/s]Extractor Estimating: 40it [00:27,  1.29it/s]Extractor Estimating: 41it [00:28,  1.32it/s]Extractor Estimating: 42it [00:29,  1.34it/s]Extractor Estimating: 43it [00:29,  1.32it/s]Extractor Estimating: 44it [00:30,  1.34it/s]Extractor Estimating: 45it [00:31,  1.34it/s]Extractor Estimating: 46it [00:32,  1.31it/s]Extractor Estimating: 47it [00:33,  1.28it/s]Extractor Estimating: 48it [00:33,  1.30it/s]Extractor Estimating: 49it [00:34,  1.30it/s]Extractor Estimating: 50it [00:35,  1.31it/s]Extractor Estimating: 51it [00:36,  1.35it/s]Extractor Estimating: 52it [00:36,  1.36it/s]Extractor Estimating: 53it [00:37,  1.37it/s]Extractor Estimating: 54it [00:38,  1.39it/s]Extractor Estimating: 55it [00:38,  1.41it/s]Extractor Estimating: 56it [00:39,  1.44it/s]Extractor Estimating: 57it [00:40,  1.46it/s]Extractor Estimating: 58it [00:40,  1.50it/s]Extractor Estimating: 59it [00:41,  1.48it/s]Extractor Estimating: 60it [00:42,  1.48it/s]Extractor Estimating: 61it [00:42,  1.52it/s]Extractor Estimating: 62it [00:43,  1.52it/s]Extractor Estimating: 63it [00:44,  1.55it/s]Extractor Estimating: 64it [00:44,  1.57it/s]Extractor Estimating: 65it [00:45,  1.62it/s]Extractor Estimating: 66it [00:45,  1.58it/s]Extractor Estimating: 67it [00:46,  1.57it/s]Extractor Estimating: 68it [00:47,  1.57it/s]Extractor Estimating: 69it [00:47,  1.53it/s]Extractor Estimating: 70it [00:48,  1.55it/s]Extractor Estimating: 71it [00:49,  1.57it/s]Extractor Estimating: 72it [00:49,  1.55it/s]Extractor Estimating: 73it [00:50,  1.55it/s]Extractor Estimating: 74it [00:51,  1.62it/s]Extractor Estimating: 75it [00:51,  1.55it/s]Extractor Estimating: 76it [00:52,  1.48it/s]Extractor Estimating: 77it [00:53,  1.44it/s]Extractor Estimating: 78it [00:53,  1.42it/s]Extractor Estimating: 79it [00:54,  1.37it/s]Extractor Estimating: 80it [00:55,  1.31it/s]Extractor Estimating: 81it [00:56,  1.33it/s]Extractor Estimating: 82it [00:56,  1.36it/s]Extractor Estimating: 83it [00:57,  1.37it/s]Extractor Estimating: 84it [00:58,  1.32it/s]Extractor Estimating: 85it [00:59,  1.29it/s]Extractor Estimating: 86it [01:00,  1.29it/s]Extractor Estimating: 87it [01:00,  1.32it/s]Extractor Estimating: 88it [01:01,  1.30it/s]Extractor Estimating: 89it [01:02,  1.26it/s]Extractor Estimating: 90it [01:03,  1.31it/s]Extractor Estimating: 91it [01:03,  1.31it/s]Extractor Estimating: 92it [01:04,  1.31it/s]Extractor Estimating: 93it [01:05,  1.35it/s]Extractor Estimating: 94it [01:06,  1.28it/s]Extractor Estimating: 95it [01:07,  1.27it/s]Extractor Estimating: 96it [01:08,  1.13it/s]Extractor Estimating: 97it [01:08,  1.19it/s]Extractor Estimating: 98it [01:09,  1.24it/s]Extractor Estimating: 99it [01:10,  1.27it/s]Extractor Estimating: 100it [01:11,  1.30it/s]Extractor Estimating: 101it [01:11,  1.36it/s]Extractor Estimating: 102it [01:12,  1.36it/s]Extractor Estimating: 103it [01:13,  1.37it/s]Extractor Estimating: 104it [01:13,  1.38it/s]Extractor Estimating: 105it [01:14,  1.36it/s]Extractor Estimating: 106it [01:15,  1.35it/s]Extractor Estimating: 107it [01:16,  1.32it/s]Extractor Estimating: 108it [01:16,  1.33it/s]Extractor Estimating: 109it [01:17,  1.34it/s]Extractor Estimating: 110it [01:18,  1.37it/s]Extractor Estimating: 111it [01:19,  1.34it/s]Extractor Estimating: 112it [01:19,  1.34it/s]Extractor Estimating: 113it [01:20,  1.36it/s]Extractor Estimating: 114it [01:21,  1.37it/s]Extractor Estimating: 115it [01:22,  1.38it/s]Extractor Estimating: 116it [01:22,  1.39it/s]Extractor Estimating: 117it [01:23,  1.41it/s]Extractor Estimating: 118it [01:24,  1.39it/s]Extractor Estimating: 119it [01:24,  1.40it/s]Extractor Estimating: 120it [01:25,  1.43it/s]Extractor Estimating: 121it [01:26,  1.37it/s]Extractor Estimating: 122it [01:27,  1.42it/s]Extractor Estimating: 123it [01:27,  1.40it/s]Extractor Estimating: 124it [01:28,  1.40it/s]Extractor Estimating: 125it [01:29,  1.39it/s]Extractor Estimating: 126it [01:29,  1.38it/s]Extractor Estimating: 127it [01:30,  1.29it/s]Extractor Estimating: 128it [01:31,  1.27it/s]Extractor Estimating: 129it [01:32,  1.28it/s]Extractor Estimating: 130it [01:33,  1.33it/s]Extractor Estimating: 131it [01:33,  1.31it/s]Extractor Estimating: 132it [01:34,  1.30it/s]Extractor Estimating: 133it [01:35,  1.31it/s]Extractor Estimating: 134it [01:36,  1.35it/s]Extractor Estimating: 135it [01:36,  1.32it/s]Extractor Estimating: 136it [01:37,  1.36it/s]Extractor Estimating: 137it [01:38,  1.29it/s]Extractor Estimating: 138it [01:39,  1.30it/s]Extractor Estimating: 139it [01:40,  1.28it/s]Extractor Estimating: 140it [01:40,  1.29it/s]Extractor Estimating: 141it [01:41,  1.32it/s]Extractor Estimating: 142it [01:42,  1.33it/s]Extractor Estimating: 143it [01:43,  1.30it/s]Extractor Estimating: 144it [01:43,  1.32it/s]Extractor Estimating: 145it [01:44,  1.33it/s]Extractor Estimating: 146it [01:45,  1.34it/s]Extractor Estimating: 147it [01:46,  1.30it/s]Extractor Estimating: 148it [01:46,  1.28it/s]Extractor Estimating: 149it [01:47,  1.31it/s]Extractor Estimating: 150it [01:48,  1.29it/s]Extractor Estimating: 151it [01:49,  1.34it/s]Extractor Estimating: 152it [01:49,  1.38it/s]Extractor Estimating: 153it [01:50,  1.44it/s]Extractor Estimating: 154it [01:51,  1.46it/s]Extractor Estimating: 155it [01:51,  1.49it/s]Extractor Estimating: 156it [01:52,  1.47it/s]Extractor Estimating: 157it [01:53,  1.42it/s]Extractor Estimating: 158it [01:53,  1.50it/s]Extractor Estimating: 159it [01:54,  1.49it/s]Extractor Estimating: 160it [01:55,  1.53it/s]Extractor Estimating: 161it [01:55,  1.54it/s]Extractor Estimating: 162it [01:56,  1.53it/s]Extractor Estimating: 163it [01:56,  1.55it/s]Extractor Estimating: 164it [01:57,  1.52it/s]Extractor Estimating: 165it [01:58,  1.50it/s]Extractor Estimating: 166it [01:59,  1.49it/s]Extractor Estimating: 167it [01:59,  1.53it/s]Extractor Estimating: 168it [02:00,  1.57it/s]Extractor Estimating: 169it [02:00,  1.59it/s]Extractor Estimating: 170it [02:01,  1.57it/s]Extractor Estimating: 171it [02:02,  1.51it/s]Extractor Estimating: 172it [02:02,  1.58it/s]Extractor Estimating: 173it [02:03,  1.57it/s]Extractor Estimating: 174it [02:04,  1.53it/s]Extractor Estimating: 175it [02:04,  1.55it/s]Extractor Estimating: 176it [02:05,  1.46it/s]Extractor Estimating: 177it [02:06,  1.43it/s]Extractor Estimating: 178it [02:06,  1.40it/s]Extractor Estimating: 179it [02:07,  1.30it/s]Extractor Estimating: 180it [02:08,  1.29it/s]Extractor Estimating: 181it [02:09,  1.30it/s]Extractor Estimating: 182it [02:10,  1.20it/s]Extractor Estimating: 183it [02:11,  1.17it/s]Extractor Estimating: 184it [02:12,  1.22it/s]Extractor Estimating: 185it [02:12,  1.27it/s]Extractor Estimating: 186it [02:13,  1.26it/s]Extractor Estimating: 187it [02:14,  1.24it/s]Extractor Estimating: 188it [02:15,  1.26it/s]Extractor Estimating: 189it [02:15,  1.25it/s]Extractor Estimating: 190it [02:16,  1.28it/s]Extractor Estimating: 191it [02:17,  1.26it/s]Extractor Estimating: 192it [02:18,  1.24it/s]Extractor Estimating: 193it [02:19,  1.20it/s]Extractor Estimating: 194it [02:20,  1.23it/s]Extractor Estimating: 195it [02:20,  1.23it/s]Extractor Estimating: 196it [02:21,  1.25it/s]Extractor Estimating: 197it [02:22,  1.31it/s]Extractor Estimating: 198it [02:23,  1.31it/s]Extractor Estimating: 199it [02:24,  1.18it/s]Extractor Estimating: 200it [02:24,  1.20it/s]Extractor Estimating: 201it [02:25,  1.22it/s]Extractor Estimating: 202it [02:26,  1.23it/s]Extractor Estimating: 203it [02:27,  1.24it/s]Extractor Estimating: 204it [02:27,  1.30it/s]Extractor Estimating: 205it [02:28,  1.32it/s]Extractor Estimating: 206it [02:29,  1.33it/s]Extractor Estimating: 207it [02:30,  1.31it/s]Extractor Estimating: 208it [02:31,  1.31it/s]Extractor Estimating: 209it [02:31,  1.35it/s]Extractor Estimating: 210it [02:32,  1.32it/s]Extractor Estimating: 211it [02:33,  1.35it/s]Extractor Estimating: 212it [02:33,  1.34it/s]Extractor Estimating: 213it [02:34,  1.34it/s]Extractor Estimating: 214it [02:35,  1.33it/s]Extractor Estimating: 215it [02:36,  1.29it/s]Extractor Estimating: 216it [02:37,  1.21it/s]Extractor Estimating: 217it [02:38,  1.23it/s]Extractor Estimating: 218it [02:38,  1.26it/s]Extractor Estimating: 219it [02:39,  1.32it/s]Extractor Estimating: 220it [02:40,  1.28it/s]Extractor Estimating: 221it [02:41,  1.27it/s]Extractor Estimating: 222it [02:41,  1.27it/s]Extractor Estimating: 223it [02:42,  1.30it/s]Extractor Estimating: 224it [02:43,  1.30it/s]Extractor Estimating: 225it [02:44,  1.27it/s]Extractor Estimating: 226it [02:44,  1.34it/s]Extractor Estimating: 227it [02:45,  1.39it/s]Extractor Estimating: 228it [02:46,  1.39it/s]Extractor Estimating: 229it [02:46,  1.42it/s]Extractor Estimating: 230it [02:47,  1.39it/s]Extractor Estimating: 231it [02:48,  1.45it/s]Extractor Estimating: 232it [02:49,  1.41it/s]Extractor Estimating: 233it [02:49,  1.42it/s]Extractor Estimating: 234it [02:50,  1.45it/s]Extractor Estimating: 235it [02:51,  1.44it/s]Extractor Estimating: 236it [02:51,  1.44it/s]Extractor Estimating: 237it [02:52,  1.45it/s]Extractor Estimating: 238it [02:53,  1.48it/s]Extractor Estimating: 239it [02:53,  1.42it/s]Extractor Estimating: 240it [02:54,  1.39it/s]Extractor Estimating: 241it [02:55,  1.40it/s]Extractor Estimating: 242it [02:56,  1.41it/s]Extractor Estimating: 243it [02:56,  1.45it/s]Extractor Estimating: 244it [02:57,  1.53it/s]Extractor Estimating: 245it [02:57,  1.46it/s]Extractor Estimating: 246it [02:58,  1.43it/s]Extractor Estimating: 247it [02:59,  1.43it/s]Extractor Estimating: 248it [03:00,  1.45it/s]Extractor Estimating: 249it [03:00,  1.40it/s]Extractor Estimating: 250it [03:01,  1.40it/s]Extractor Estimating: 251it [03:02,  1.33it/s]Extractor Estimating: 252it [03:03,  1.29it/s]Extractor Estimating: 253it [03:03,  1.31it/s]Extractor Estimating: 254it [03:04,  1.30it/s]Extractor Estimating: 255it [03:05,  1.29it/s]Extractor Estimating: 256it [03:06,  1.33it/s]Extractor Estimating: 257it [03:07,  1.25it/s]Extractor Estimating: 258it [03:07,  1.29it/s]Extractor Estimating: 259it [03:08,  1.31it/s]Extractor Estimating: 260it [03:09,  1.31it/s]Extractor Estimating: 261it [03:10,  1.33it/s]Extractor Estimating: 262it [03:10,  1.28it/s]Extractor Estimating: 263it [03:11,  1.27it/s]Extractor Estimating: 264it [03:12,  1.16it/s]Extractor Estimating: 265it [03:13,  1.19it/s]Extractor Estimating: 266it [03:14,  1.18it/s]Extractor Estimating: 267it [03:15,  1.18it/s]Extractor Estimating: 268it [03:16,  1.23it/s]Extractor Estimating: 269it [03:16,  1.28it/s]Extractor Estimating: 270it [03:17,  1.27it/s]Extractor Estimating: 271it [03:18,  1.29it/s]Extractor Estimating: 272it [03:19,  1.31it/s]Extractor Estimating: 273it [03:19,  1.30it/s]Extractor Estimating: 274it [03:20,  1.26it/s]Extractor Estimating: 275it [03:21,  1.23it/s]Extractor Estimating: 276it [03:22,  1.30it/s]Extractor Estimating: 277it [03:22,  1.36it/s]Extractor Estimating: 278it [03:23,  1.38it/s]Extractor Estimating: 279it [03:24,  1.42it/s]Extractor Estimating: 280it [03:24,  1.38it/s]Extractor Estimating: 281it [03:25,  1.45it/s]Extractor Estimating: 282it [03:26,  1.43it/s]Extractor Estimating: 283it [03:27,  1.40it/s]Extractor Estimating: 284it [03:27,  1.42it/s]Extractor Estimating: 285it [03:28,  1.42it/s]Extractor Estimating: 286it [03:29,  1.44it/s]Extractor Estimating: 287it [03:29,  1.40it/s]Extractor Estimating: 288it [03:30,  1.38it/s]Extractor Estimating: 289it [03:31,  1.38it/s]Extractor Estimating: 290it [03:32,  1.35it/s]Extractor Estimating: 291it [03:32,  1.37it/s]Extractor Estimating: 292it [03:33,  1.39it/s]Extractor Estimating: 293it [03:34,  1.39it/s]Extractor Estimating: 294it [03:34,  1.41it/s]Extractor Estimating: 295it [03:35,  1.37it/s]Extractor Estimating: 296it [03:36,  1.43it/s]Extractor Estimating: 297it [03:36,  1.44it/s]Extractor Estimating: 298it [03:37,  1.44it/s]Extractor Estimating: 299it [03:38,  1.44it/s]Extractor Estimating: 300it [03:39,  1.44it/s]Extractor Estimating: 301it [03:39,  1.40it/s]Extractor Estimating: 302it [03:40,  1.40it/s]Extractor Estimating: 303it [03:41,  1.42it/s]Extractor Estimating: 304it [03:41,  1.38it/s]Extractor Estimating: 305it [03:42,  1.39it/s]Extractor Estimating: 306it [03:43,  1.40it/s]Extractor Estimating: 307it [03:44,  1.44it/s]Extractor Estimating: 308it [03:44,  1.40it/s]Extractor Estimating: 309it [03:45,  1.39it/s]Extractor Estimating: 310it [03:46,  1.45it/s]Extractor Estimating: 311it [03:46,  1.43it/s]Extractor Estimating: 312it [03:47,  1.46it/s]Extractor Estimating: 313it [03:48,  1.44it/s]Extractor Estimating: 314it [03:48,  1.46it/s]Extractor Estimating: 315it [03:49,  1.48it/s]Extractor Estimating: 316it [03:50,  1.43it/s]Extractor Estimating: 317it [03:50,  1.47it/s]Extractor Estimating: 318it [03:51,  1.50it/s]Extractor Estimating: 319it [03:52,  1.49it/s]Extractor Estimating: 320it [03:52,  1.48it/s]Extractor Estimating: 321it [03:53,  1.46it/s]Extractor Estimating: 322it [03:54,  1.44it/s]Extractor Estimating: 323it [03:55,  1.41it/s]Extractor Estimating: 324it [03:55,  1.44it/s]Extractor Estimating: 325it [03:56,  1.47it/s]Extractor Estimating: 326it [03:57,  1.50it/s]Extractor Estimating: 327it [03:57,  1.57it/s]Extractor Estimating: 328it [03:58,  1.59it/s]Extractor Estimating: 329it [03:58,  1.68it/s]Extractor Estimating: 330it [03:59,  1.70it/s]Extractor Estimating: 331it [04:00,  1.62it/s]Extractor Estimating: 332it [04:00,  1.65it/s]Extractor Estimating: 333it [04:01,  1.69it/s]Extractor Estimating: 334it [04:01,  1.74it/s]Extractor Estimating: 335it [04:02,  1.73it/s]Extractor Estimating: 336it [04:02,  1.74it/s]Extractor Estimating: 337it [04:03,  1.46it/s]Extractor Estimating: 338it [04:04,  1.41it/s]Extractor Estimating: 339it [04:05,  1.48it/s]Extractor Estimating: 340it [04:05,  1.55it/s]Extractor Estimating: 341it [04:06,  1.58it/s]Extractor Estimating: 342it [04:06,  1.58it/s]Extractor Estimating: 343it [04:07,  1.61it/s]Extractor Estimating: 344it [04:08,  1.60it/s]Extractor Estimating: 345it [04:08,  1.63it/s]Extractor Estimating: 346it [04:09,  1.66it/s]Extractor Estimating: 347it [04:09,  1.66it/s]Extractor Estimating: 348it [04:10,  1.69it/s]Extractor Estimating: 349it [04:11,  1.68it/s]Extractor Estimating: 350it [04:11,  1.61it/s]Extractor Estimating: 351it [04:12,  1.53it/s]Extractor Estimating: 352it [04:13,  1.43it/s]Extractor Estimating: 353it [04:14,  1.41it/s]Extractor Estimating: 354it [04:14,  1.40it/s]Extractor Estimating: 355it [04:15,  1.42it/s]Extractor Estimating: 356it [04:16,  1.40it/s]Extractor Estimating: 357it [04:16,  1.36it/s]Extractor Estimating: 358it [04:17,  1.35it/s]Extractor Estimating: 359it [04:18,  1.35it/s]Extractor Estimating: 360it [04:19,  1.38it/s]Extractor Estimating: 361it [04:20,  1.24it/s]Extractor Estimating: 362it [04:20,  1.30it/s]Extractor Estimating: 363it [04:21,  1.33it/s]Extractor Estimating: 364it [04:22,  1.35it/s]Extractor Estimating: 365it [04:22,  1.38it/s]Extractor Estimating: 366it [04:23,  1.41it/s]Extractor Estimating: 367it [04:24,  1.42it/s]Extractor Estimating: 368it [04:24,  1.45it/s]Extractor Estimating: 369it [04:25,  1.38it/s]Extractor Estimating: 370it [04:26,  1.37it/s]Extractor Estimating: 371it [04:27,  1.41it/s]Extractor Estimating: 372it [04:27,  1.43it/s]Extractor Estimating: 373it [04:28,  1.39it/s]Extractor Estimating: 374it [04:29,  1.39it/s]Extractor Estimating: 375it [04:30,  1.42it/s]Extractor Estimating: 376it [04:30,  1.41it/s]Extractor Estimating: 377it [04:31,  1.41it/s]Extractor Estimating: 378it [04:32,  1.40it/s]Extractor Estimating: 379it [04:32,  1.42it/s]Extractor Estimating: 380it [04:33,  1.40it/s]Extractor Estimating: 381it [04:34,  1.45it/s]Extractor Estimating: 382it [04:34,  1.44it/s]Extractor Estimating: 383it [04:35,  1.50it/s]Extractor Estimating: 384it [04:36,  1.46it/s]Extractor Estimating: 385it [04:36,  1.44it/s]Extractor Estimating: 386it [04:37,  1.43it/s]Extractor Estimating: 387it [04:38,  1.41it/s]Extractor Estimating: 388it [04:39,  1.39it/s]Extractor Estimating: 389it [04:39,  1.36it/s]Extractor Estimating: 390it [04:40,  1.32it/s]Extractor Estimating: 391it [04:41,  1.31it/s]Extractor Estimating: 392it [04:42,  1.36it/s]Extractor Estimating: 393it [04:42,  1.39it/s]Extractor Estimating: 394it [04:43,  1.39it/s]Extractor Estimating: 395it [04:44,  1.41it/s]Extractor Estimating: 396it [04:45,  1.40it/s]Extractor Estimating: 397it [04:45,  1.43it/s]Extractor Estimating: 398it [04:46,  1.44it/s]Extractor Estimating: 399it [04:47,  1.46it/s]Extractor Estimating: 400it [04:47,  1.42it/s]Extractor Estimating: 401it [04:48,  1.39it/s]Extractor Estimating: 402it [04:49,  1.36it/s]Extractor Estimating: 403it [04:50,  1.37it/s]Extractor Estimating: 404it [04:50,  1.42it/s]Extractor Estimating: 405it [04:51,  1.42it/s]Extractor Estimating: 406it [04:52,  1.41it/s]Extractor Estimating: 407it [04:52,  1.39it/s]Extractor Estimating: 408it [04:53,  1.47it/s]Extractor Estimating: 409it [04:54,  1.45it/s]Extractor Estimating: 410it [04:54,  1.44it/s]Extractor Estimating: 411it [04:55,  1.38it/s]Extractor Estimating: 412it [04:56,  1.41it/s]Extractor Estimating: 413it [04:57,  1.41it/s]Extractor Estimating: 414it [04:57,  1.47it/s]Extractor Estimating: 415it [04:58,  1.44it/s]Extractor Estimating: 416it [04:59,  1.44it/s]Extractor Estimating: 417it [04:59,  1.50it/s]Extractor Estimating: 418it [05:00,  1.48it/s]Extractor Estimating: 419it [05:00,  1.51it/s]Extractor Estimating: 420it [05:01,  1.52it/s]Extractor Estimating: 421it [05:02,  1.50it/s]Extractor Estimating: 422it [05:03,  1.47it/s]Extractor Estimating: 423it [05:03,  1.46it/s]Extractor Estimating: 424it [05:04,  1.43it/s]Extractor Estimating: 425it [05:05,  1.43it/s]Extractor Estimating: 426it [05:05,  1.42it/s]Extractor Estimating: 427it [05:06,  1.41it/s]Extractor Estimating: 428it [05:07,  1.43it/s]Extractor Estimating: 429it [05:07,  1.43it/s]Extractor Estimating: 430it [05:08,  1.44it/s]Extractor Estimating: 431it [05:09,  1.47it/s]Extractor Estimating: 432it [05:09,  1.48it/s]Extractor Estimating: 433it [05:10,  1.46it/s]Extractor Estimating: 434it [05:11,  1.46it/s]Extractor Estimating: 435it [05:12,  1.42it/s]Extractor Estimating: 436it [05:12,  1.46it/s]Extractor Estimating: 437it [05:13,  1.45it/s]Extractor Estimating: 438it [05:14,  1.49it/s]Extractor Estimating: 439it [05:14,  1.34it/s]Extractor Estimating: 440it [05:15,  1.39it/s]Extractor Estimating: 441it [05:16,  1.38it/s]Extractor Estimating: 442it [05:17,  1.41it/s]Extractor Estimating: 443it [05:17,  1.43it/s]Extractor Estimating: 444it [05:18,  1.41it/s]Extractor Estimating: 445it [05:19,  1.39it/s]Extractor Estimating: 446it [05:19,  1.41it/s]Extractor Estimating: 447it [05:20,  1.39it/s]Extractor Estimating: 448it [05:21,  1.40it/s]Extractor Estimating: 449it [05:22,  1.41it/s]Extractor Estimating: 450it [05:22,  1.37it/s]Extractor Estimating: 451it [05:23,  1.33it/s]Extractor Estimating: 452it [05:24,  1.30it/s]Extractor Estimating: 453it [05:25,  1.33it/s]Extractor Estimating: 454it [05:26,  1.27it/s]Extractor Estimating: 455it [05:26,  1.27it/s]Extractor Estimating: 456it [05:27,  1.26it/s]Extractor Estimating: 457it [05:28,  1.22it/s]Extractor Estimating: 458it [05:29,  1.23it/s]Extractor Estimating: 459it [05:30,  1.25it/s]Extractor Estimating: 460it [05:30,  1.28it/s]Extractor Estimating: 461it [05:31,  1.27it/s]Extractor Estimating: 462it [05:32,  1.23it/s]Extractor Estimating: 463it [05:33,  1.28it/s]Extractor Estimating: 464it [05:33,  1.32it/s]Extractor Estimating: 465it [05:34,  1.29it/s]Extractor Estimating: 466it [05:35,  1.28it/s]Extractor Estimating: 467it [05:36,  1.30it/s]Extractor Estimating: 468it [05:37,  1.29it/s]Extractor Estimating: 469it [05:37,  1.33it/s]Extractor Estimating: 470it [05:38,  1.30it/s]Extractor Estimating: 471it [05:39,  1.27it/s]Extractor Estimating: 472it [05:40,  1.28it/s]Extractor Estimating: 473it [05:41,  1.22it/s]Extractor Estimating: 474it [05:41,  1.24it/s]Extractor Estimating: 475it [05:42,  1.29it/s]Extractor Estimating: 476it [05:43,  1.35it/s]Extractor Estimating: 477it [05:43,  1.34it/s]Extractor Estimating: 478it [05:44,  1.36it/s]Extractor Estimating: 479it [05:45,  1.35it/s]Extractor Estimating: 480it [05:46,  1.34it/s]Extractor Estimating: 481it [05:46,  1.42it/s]Extractor Estimating: 482it [05:47,  1.45it/s]Extractor Estimating: 483it [05:48,  1.45it/s]Extractor Estimating: 484it [05:48,  1.35it/s]Extractor Estimating: 485it [05:49,  1.42it/s]Extractor Estimating: 486it [05:50,  1.47it/s]Extractor Estimating: 487it [05:50,  1.51it/s]Extractor Estimating: 488it [05:51,  1.47it/s]Extractor Estimating: 489it [05:52,  1.52it/s]Extractor Estimating: 490it [05:52,  1.44it/s]Extractor Estimating: 491it [05:53,  1.48it/s]Extractor Estimating: 492it [05:54,  1.43it/s]Extractor Estimating: 493it [05:55,  1.38it/s]Extractor Estimating: 494it [05:55,  1.39it/s]Extractor Estimating: 495it [05:56,  1.39it/s]Extractor Estimating: 496it [05:57,  1.42it/s]Extractor Estimating: 497it [05:57,  1.47it/s]Extractor Estimating: 498it [05:58,  1.47it/s]Extractor Estimating: 499it [05:59,  1.46it/s]Extractor Estimating: 500it [05:59,  1.70it/s]Extractor Estimating: 500it [05:59,  1.39it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:19:32,225 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:19:32,227 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:19:32,227 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:19:32,227 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:19:32,227 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:19:32,942 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:19:32,989 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:19:33,341 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:19:34,499 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:19:34,499 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:19:37,356 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:19:37,409 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:19:37,410 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:19:37,410 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:19:37,410 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:19:38,362 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:19:38,363 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:19:38,694 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:19:38,977 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:19:38,977 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 08:05:31,026 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 08:05:31,461 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 7987 mean pseudo reward: 0.9797911942298673
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 16315
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16415, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16415, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.201, loss:359.5522
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.195, loss:306.7374
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.197, loss:301.2306
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 67, avg_time 1.216, loss:288.3190
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 167, avg_time 1.231, loss:295.6133
>> valid entity prec:0.5952, rec:0.6122, f1:0.6036
>> valid relation prec:0.2330, rec:0.1588, f1:0.1888
>> valid relation with NER prec:0.2330, rec:0.1588, f1:0.1888
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 267, avg_time 2.695, loss:304.5324
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 34, avg_time 1.174, loss:303.9489
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 134, avg_time 1.212, loss:278.9949
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 234, avg_time 1.181, loss:283.5427
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 1, avg_time 1.214, loss:313.1132
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6312, rec:0.5720, f1:0.6002
>> valid relation prec:0.2424, rec:0.1349, f1:0.1733
>> valid relation with NER prec:0.2424, rec:0.1349, f1:0.1733
g_step 1100, step 101, avg_time 2.676, loss:259.3324
g_step 1200, step 201, avg_time 1.213, loss:287.8016
g_step 1300, step 301, avg_time 1.195, loss:301.7298
g_step 1400, step 68, avg_time 1.200, loss:262.6679
g_step 1500, step 168, avg_time 1.179, loss:268.6102
>> valid entity prec:0.6097, rec:0.6382, f1:0.6236
>> valid relation prec:0.2376, rec:0.1605, f1:0.1916
>> valid relation with NER prec:0.2376, rec:0.1605, f1:0.1916
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 268, avg_time 2.681, loss:260.5056
g_step 1700, step 35, avg_time 1.245, loss:274.0532
g_step 1800, step 135, avg_time 1.202, loss:249.2798
g_step 1900, step 235, avg_time 1.197, loss:238.2395
g_step 2000, step 2, avg_time 1.196, loss:266.0857
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5958, rec:0.6181, f1:0.6068
>> valid relation prec:0.2194, rec:0.1668, f1:0.1895
>> valid relation with NER prec:0.2194, rec:0.1668, f1:0.1895
g_step 2100, step 102, avg_time 2.678, loss:220.6440
g_step 2200, step 202, avg_time 1.211, loss:221.5281
g_step 2300, step 302, avg_time 1.209, loss:230.0131
g_step 2400, step 69, avg_time 1.204, loss:216.5713
g_step 2500, step 169, avg_time 1.209, loss:220.8092
>> valid entity prec:0.6160, rec:0.6174, f1:0.6167
>> valid relation prec:0.2135, rec:0.1651, f1:0.1862
>> valid relation with NER prec:0.2135, rec:0.1651, f1:0.1862
g_step 2600, step 269, avg_time 2.685, loss:231.5785
g_step 2700, step 36, avg_time 1.208, loss:213.0389
g_step 2800, step 136, avg_time 1.203, loss:203.1378
g_step 2900, step 236, avg_time 1.205, loss:216.0024
g_step 3000, step 3, avg_time 1.192, loss:216.7119
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5942, rec:0.5937, f1:0.5940
>> valid relation prec:0.1813, rec:0.1409, f1:0.1586
>> valid relation with NER prec:0.1813, rec:0.1409, f1:0.1586
g_step 3100, step 103, avg_time 2.683, loss:191.0793
g_step 3200, step 203, avg_time 1.224, loss:211.1465
g_step 3300, step 303, avg_time 1.195, loss:200.9724
g_step 3400, step 70, avg_time 1.214, loss:189.5219
g_step 3500, step 170, avg_time 1.193, loss:184.1291
>> valid entity prec:0.6069, rec:0.6112, f1:0.6090
>> valid relation prec:0.2093, rec:0.1588, f1:0.1806
>> valid relation with NER prec:0.2093, rec:0.1588, f1:0.1806
g_step 3600, step 270, avg_time 2.682, loss:191.6137
g_step 3700, step 37, avg_time 1.207, loss:191.5331
g_step 3800, step 137, avg_time 1.199, loss:169.5303
g_step 3900, step 237, avg_time 1.204, loss:168.4440
g_step 4000, step 4, avg_time 1.206, loss:199.4731
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6239, rec:0.5894, f1:0.6061
>> valid relation prec:0.2220, rec:0.1565, f1:0.1836
>> valid relation with NER prec:0.2220, rec:0.1565, f1:0.1836
g_step 4100, step 104, avg_time 2.671, loss:149.2831
g_step 4200, step 204, avg_time 1.208, loss:185.2161
g_step 4300, step 304, avg_time 1.188, loss:179.3407
g_step 4400, step 71, avg_time 1.198, loss:161.8193
g_step 4500, step 171, avg_time 1.207, loss:175.2029
>> valid entity prec:0.5979, rec:0.5956, f1:0.5967
>> valid relation prec:0.1868, rec:0.1357, f1:0.1572
>> valid relation with NER prec:0.1868, rec:0.1357, f1:0.1572
g_step 4600, step 271, avg_time 2.694, loss:173.1376
g_step 4700, step 38, avg_time 1.196, loss:156.5046
g_step 4800, step 138, avg_time 1.218, loss:150.7355
g_step 4900, step 238, avg_time 1.203, loss:165.6198
g_step 5000, step 5, avg_time 1.187, loss:166.3387
learning rate was adjusted to 0.0008
>> valid entity prec:0.6248, rec:0.6081, f1:0.6164
>> valid relation prec:0.2350, rec:0.1717, f1:0.1984
>> valid relation with NER prec:0.2350, rec:0.1717, f1:0.1984
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5100, step 105, avg_time 2.685, loss:145.5041
g_step 5200, step 205, avg_time 1.215, loss:143.6617
g_step 5300, step 305, avg_time 1.196, loss:154.7016
g_step 5400, step 72, avg_time 1.204, loss:140.5034
g_step 5500, step 172, avg_time 1.209, loss:153.5757
>> valid entity prec:0.5989, rec:0.5983, f1:0.5986
>> valid relation prec:0.2027, rec:0.1547, f1:0.1755
>> valid relation with NER prec:0.2027, rec:0.1547, f1:0.1755
g_step 5600, step 272, avg_time 2.682, loss:147.6824
g_step 5700, step 39, avg_time 1.216, loss:148.5099
g_step 5800, step 139, avg_time 1.193, loss:142.0213
g_step 5900, step 239, avg_time 1.217, loss:144.1944
g_step 6000, step 6, avg_time 1.211, loss:145.7162
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.6078, rec:0.6307, f1:0.6190
>> valid relation prec:0.2193, rec:0.1783, f1:0.1967
>> valid relation with NER prec:0.2193, rec:0.1783, f1:0.1967
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 6100, step 106, avg_time 2.690, loss:130.3069
g_step 6200, step 206, avg_time 1.203, loss:131.0996
g_step 6300, step 306, avg_time 1.214, loss:142.9932
g_step 6400, step 73, avg_time 1.212, loss:123.5410
g_step 6500, step 173, avg_time 1.202, loss:134.3221
>> valid entity prec:0.6161, rec:0.5959, f1:0.6058
>> valid relation prec:0.2259, rec:0.1616, f1:0.1884
>> valid relation with NER prec:0.2259, rec:0.1616, f1:0.1884
g_step 6600, step 273, avg_time 2.690, loss:147.0309
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 08:05:31 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 08:05:31 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_08-05-31_ctolab14.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 08:05:32 - WARNING - datasets.builder -   Using custom data configuration default-4e6c06e73860cae3
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-4e6c06e73860cae3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 08:05:36,020 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:05:36,021 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 08:05:36,022 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:05:36,023 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 08:05:36,136 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:05:36,178 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:05:36,178 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:05:36,178 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:05:36,178 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:05:36,178 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:05:36,178 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 08:05:36,582 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 08:05:39,627 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 08:05:39,652 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-4e6c06e73860cae3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.85ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.86ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.41ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.95ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.31ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.59ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.78ba/s]100%|██████████| 8/8 [00:01<00:00,  4.91ba/s]100%|██████████| 8/8 [00:01<00:00,  4.35ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.42ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.07ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.33ba/s]100%|██████████| 4/4 [00:00<00:00,  5.52ba/s]100%|██████████| 4/4 [00:00<00:00,  4.86ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  5.15ba/s] 38%|███▊      | 3/8 [00:00<00:00,  8.54ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.73ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.37ba/s]100%|██████████| 8/8 [00:00<00:00,  9.80ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  5.86ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.00ba/s]100%|██████████| 4/4 [00:00<00:00, 10.12ba/s]
[INFO|trainer.py:414] 2023-08-29 08:05:44,650 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 08:05:44,727 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 08:05:44,727 >>   Num examples = 8000
[INFO|trainer.py:1149] 2023-08-29 08:05:44,727 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 08:05:44,727 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 08:05:44,727 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 08:05:44,727 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 08:05:44,727 >>   Total optimization steps = 625
  0%|          | 0/625 [00:00<?, ?it/s]  0%|          | 1/625 [00:00<03:10,  3.28it/s]  0%|          | 2/625 [00:00<03:04,  3.38it/s]  0%|          | 3/625 [00:00<03:02,  3.42it/s]  1%|          | 4/625 [00:01<03:00,  3.43it/s]  1%|          | 5/625 [00:01<03:07,  3.31it/s]  1%|          | 6/625 [00:01<03:06,  3.32it/s]  1%|          | 7/625 [00:02<03:05,  3.33it/s]  1%|▏         | 8/625 [00:02<03:04,  3.34it/s]  1%|▏         | 9/625 [00:02<03:04,  3.34it/s]  2%|▏         | 10/625 [00:02<03:03,  3.35it/s]  2%|▏         | 11/625 [00:03<03:03,  3.35it/s]  2%|▏         | 12/625 [00:03<03:02,  3.36it/s]  2%|▏         | 13/625 [00:03<03:02,  3.36it/s]  2%|▏         | 14/625 [00:04<03:02,  3.36it/s]  2%|▏         | 15/625 [00:04<03:07,  3.26it/s]  3%|▎         | 16/625 [00:04<03:05,  3.29it/s]  3%|▎         | 17/625 [00:05<03:03,  3.31it/s]  3%|▎         | 18/625 [00:05<03:02,  3.32it/s]  3%|▎         | 19/625 [00:05<03:01,  3.33it/s]  3%|▎         | 20/625 [00:05<03:01,  3.34it/s]  3%|▎         | 21/625 [00:06<03:00,  3.34it/s]  4%|▎         | 22/625 [00:06<03:00,  3.35it/s]  4%|▎         | 23/625 [00:06<02:59,  3.35it/s]  4%|▍         | 24/625 [00:07<02:59,  3.35it/s]  4%|▍         | 25/625 [00:07<03:02,  3.28it/s]  4%|▍         | 26/625 [00:07<03:01,  3.31it/s]  4%|▍         | 27/625 [00:08<02:59,  3.32it/s]  4%|▍         | 28/625 [00:08<02:58,  3.35it/s]  5%|▍         | 29/625 [00:08<02:56,  3.38it/s]  5%|▍         | 30/625 [00:08<02:55,  3.40it/s]  5%|▍         | 31/625 [00:09<02:53,  3.41it/s]  5%|▌         | 32/625 [00:09<02:53,  3.43it/s]  5%|▌         | 33/625 [00:09<02:52,  3.43it/s]  5%|▌         | 34/625 [00:10<02:51,  3.44it/s]  6%|▌         | 35/625 [00:10<02:51,  3.44it/s]  6%|▌         | 36/625 [00:10<02:55,  3.35it/s]  6%|▌         | 37/625 [00:11<02:53,  3.38it/s]  6%|▌         | 38/625 [00:11<02:52,  3.40it/s]  6%|▌         | 39/625 [00:11<02:51,  3.42it/s]  6%|▋         | 40/625 [00:11<02:50,  3.43it/s]  7%|▋         | 41/625 [00:12<02:50,  3.43it/s]  7%|▋         | 42/625 [00:12<02:49,  3.44it/s]  7%|▋         | 43/625 [00:12<02:49,  3.44it/s]  7%|▋         | 44/625 [00:13<02:48,  3.45it/s]  7%|▋         | 45/625 [00:13<02:48,  3.45it/s]  7%|▋         | 46/625 [00:13<02:47,  3.45it/s]  8%|▊         | 47/625 [00:13<02:50,  3.38it/s]  8%|▊         | 48/625 [00:14<02:49,  3.40it/s]  8%|▊         | 49/625 [00:14<02:48,  3.42it/s]  8%|▊         | 50/625 [00:14<02:47,  3.43it/s]  8%|▊         | 51/625 [00:15<02:47,  3.44it/s]  8%|▊         | 52/625 [00:15<02:46,  3.44it/s]  8%|▊         | 53/625 [00:15<02:46,  3.44it/s]  9%|▊         | 54/625 [00:15<02:45,  3.45it/s]  9%|▉         | 55/625 [00:16<02:45,  3.45it/s]  9%|▉         | 56/625 [00:16<02:45,  3.45it/s]  9%|▉         | 57/625 [00:16<02:51,  3.31it/s]  9%|▉         | 58/625 [00:17<02:49,  3.35it/s]  9%|▉         | 59/625 [00:17<02:47,  3.38it/s] 10%|▉         | 60/625 [00:17<02:46,  3.40it/s] 10%|▉         | 61/625 [00:18<02:45,  3.41it/s] 10%|▉         | 62/625 [00:18<02:44,  3.42it/s] 10%|█         | 63/625 [00:18<02:43,  3.43it/s] 10%|█         | 64/625 [00:18<02:43,  3.44it/s] 10%|█         | 65/625 [00:19<02:42,  3.44it/s] 11%|█         | 66/625 [00:19<02:42,  3.45it/s] 11%|█         | 67/625 [00:19<02:41,  3.45it/s] 11%|█         | 68/625 [00:20<02:44,  3.39it/s] 11%|█         | 69/625 [00:20<02:43,  3.41it/s] 11%|█         | 70/625 [00:20<02:42,  3.42it/s] 11%|█▏        | 71/625 [00:20<02:41,  3.43it/s] 12%|█▏        | 72/625 [00:21<02:41,  3.43it/s] 12%|█▏        | 73/625 [00:21<02:40,  3.44it/s] 12%|█▏        | 74/625 [00:21<02:40,  3.44it/s] 12%|█▏        | 75/625 [00:22<02:39,  3.44it/s] 12%|█▏        | 76/625 [00:22<02:39,  3.44it/s] 12%|█▏        | 77/625 [00:22<02:39,  3.45it/s] 12%|█▏        | 78/625 [00:22<02:38,  3.45it/s] 13%|█▎        | 79/625 [00:23<02:41,  3.38it/s] 13%|█▎        | 80/625 [00:23<02:40,  3.40it/s] 13%|█▎        | 81/625 [00:23<02:39,  3.41it/s] 13%|█▎        | 82/625 [00:24<02:38,  3.43it/s] 13%|█▎        | 83/625 [00:24<02:37,  3.43it/s] 13%|█▎        | 84/625 [00:24<02:37,  3.44it/s] 14%|█▎        | 85/625 [00:25<02:37,  3.44it/s] 14%|█▍        | 86/625 [00:25<02:36,  3.44it/s] 14%|█▍        | 87/625 [00:25<02:36,  3.44it/s] 14%|█▍        | 88/625 [00:25<02:35,  3.44it/s] 14%|█▍        | 89/625 [00:26<02:35,  3.44it/s] 14%|█▍        | 90/625 [00:26<02:38,  3.38it/s] 15%|█▍        | 91/625 [00:26<02:37,  3.40it/s] 15%|█▍        | 92/625 [00:27<02:36,  3.41it/s] 15%|█▍        | 93/625 [00:27<02:35,  3.42it/s] 15%|█▌        | 94/625 [00:27<02:34,  3.43it/s] 15%|█▌        | 95/625 [00:27<02:34,  3.43it/s] 15%|█▌        | 96/625 [00:28<02:33,  3.44it/s] 16%|█▌        | 97/625 [00:28<02:33,  3.44it/s] 16%|█▌        | 98/625 [00:28<02:33,  3.44it/s] 16%|█▌        | 99/625 [00:29<02:32,  3.44it/s] 16%|█▌        | 100/625 [00:29<02:32,  3.44it/s] 16%|█▌        | 101/625 [00:29<02:35,  3.37it/s] 16%|█▋        | 102/625 [00:30<02:34,  3.39it/s] 16%|█▋        | 103/625 [00:30<02:33,  3.40it/s] 17%|█▋        | 104/625 [00:30<02:32,  3.42it/s] 17%|█▋        | 105/625 [00:30<02:31,  3.43it/s] 17%|█▋        | 106/625 [00:31<02:31,  3.43it/s] 17%|█▋        | 107/625 [00:31<02:30,  3.43it/s] 17%|█▋        | 108/625 [00:31<02:30,  3.44it/s] 17%|█▋        | 109/625 [00:32<02:29,  3.44it/s] 18%|█▊        | 110/625 [00:32<02:29,  3.44it/s] 18%|█▊        | 111/625 [00:32<02:29,  3.45it/s] 18%|█▊        | 112/625 [00:32<02:34,  3.33it/s] 18%|█▊        | 113/625 [00:33<02:32,  3.37it/s] 18%|█▊        | 114/625 [00:33<02:30,  3.39it/s] 18%|█▊        | 115/625 [00:33<02:29,  3.41it/s] 19%|█▊        | 116/625 [00:34<02:28,  3.42it/s] 19%|█▊        | 117/625 [00:34<02:28,  3.43it/s] 19%|█▉        | 118/625 [00:34<02:27,  3.44it/s] 19%|█▉        | 119/625 [00:34<02:27,  3.44it/s] 19%|█▉        | 120/625 [00:35<02:26,  3.44it/s] 19%|█▉        | 121/625 [00:35<02:26,  3.44it/s] 20%|█▉        | 122/625 [00:35<02:25,  3.45it/s] 20%|█▉        | 123/625 [00:36<02:27,  3.40it/s] 20%|█▉        | 124/625 [00:36<02:26,  3.41it/s] 20%|██        | 125/625 [00:36<02:26,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 08:06:21,462 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:06:21,462 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 08:06:21,462 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 55.84it/s][A
  3%|▎         | 12/435 [00:00<00:08, 48.67it/s][A
  4%|▍         | 17/435 [00:00<00:08, 46.93it/s][A
  5%|▌         | 22/435 [00:00<00:09, 45.80it/s][A
  6%|▌         | 27/435 [00:00<00:09, 45.19it/s][A
  7%|▋         | 32/435 [00:00<00:09, 44.62it/s][A
  9%|▊         | 37/435 [00:00<00:08, 44.55it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.36it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.47it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.52it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.65it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.71it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.69it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.36it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 44.24it/s][A
 19%|█▉        | 82/435 [00:01<00:07, 44.18it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.30it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.36it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.45it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 43.51it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 43.96it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.13it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 44.20it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 44.10it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 44.16it/s][A
 30%|███       | 132/435 [00:02<00:06, 44.28it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 44.22it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 44.33it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 44.44it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.62it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.59it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 44.48it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 44.39it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.37it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.35it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.29it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.35it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 44.43it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 44.54it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 44.52it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 44.47it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 44.39it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 44.40it/s][A
 51%|█████     | 222/435 [00:04<00:04, 44.30it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.19it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.36it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 42.29it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 43.13it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 43.58it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 43.84it/s][A
 59%|█████▉    | 257/435 [00:05<00:04, 43.97it/s][A
 60%|██████    | 262/435 [00:05<00:03, 44.04it/s][A
 61%|██████▏   | 267/435 [00:06<00:03, 44.04it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.04it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.07it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.26it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.46it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.62it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.61it/s][A
 69%|██████▉   | 302/435 [00:06<00:02, 44.47it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.29it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.24it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.19it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.24it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 40.07it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 41.72it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 42.65it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 43.32it/s][A
 80%|███████▉  | 347/435 [00:07<00:02, 43.73it/s][A
 81%|████████  | 352/435 [00:07<00:01, 44.00it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 44.11it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 43.89it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 43.87it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 41.67it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 42.60it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 43.26it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 43.69it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 44.00it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 44.05it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 44.23it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 44.04it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 43.91it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 43.96it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.09it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 24.33it/s][A
 99%|█████████▉| 432/435 [00:10<00:00, 28.64it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:10<00:00, 28.64it/s][A 20%|██        | 125/625 [00:46<02:26,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:06:32,082 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-125
[INFO|configuration_utils.py:351] 2023-08-29 08:06:32,410 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-125/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:06:35,966 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-125/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:06:36,153 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-125/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:06:36,253 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-125/special_tokens_map.json
 20%|██        | 126/625 [00:58<55:15,  6.64s/it] 20%|██        | 127/625 [00:58<39:22,  4.74s/it] 20%|██        | 128/625 [00:58<28:14,  3.41s/it] 21%|██        | 129/625 [00:59<20:28,  2.48s/it] 21%|██        | 130/625 [00:59<15:02,  1.82s/it] 21%|██        | 131/625 [00:59<11:14,  1.36s/it] 21%|██        | 132/625 [00:59<08:34,  1.04s/it] 21%|██▏       | 133/625 [01:00<06:43,  1.22it/s] 21%|██▏       | 134/625 [01:00<05:25,  1.51it/s] 22%|██▏       | 135/625 [01:00<04:30,  1.81it/s] 22%|██▏       | 136/625 [01:01<03:52,  2.10it/s] 22%|██▏       | 137/625 [01:01<03:26,  2.37it/s] 22%|██▏       | 138/625 [01:01<03:11,  2.55it/s] 22%|██▏       | 139/625 [01:02<02:56,  2.75it/s] 22%|██▏       | 140/625 [01:02<02:46,  2.91it/s] 23%|██▎       | 141/625 [01:02<02:39,  3.03it/s] 23%|██▎       | 142/625 [01:02<02:34,  3.13it/s] 23%|██▎       | 143/625 [01:03<02:30,  3.20it/s] 23%|██▎       | 144/625 [01:03<02:28,  3.25it/s] 23%|██▎       | 145/625 [01:03<02:26,  3.28it/s] 23%|██▎       | 146/625 [01:04<02:25,  3.30it/s] 24%|██▎       | 147/625 [01:04<02:24,  3.31it/s] 24%|██▎       | 148/625 [01:04<02:23,  3.33it/s] 24%|██▍       | 149/625 [01:05<02:26,  3.24it/s] 24%|██▍       | 150/625 [01:05<02:25,  3.27it/s] 24%|██▍       | 151/625 [01:05<02:23,  3.29it/s] 24%|██▍       | 152/625 [01:05<02:22,  3.31it/s] 24%|██▍       | 153/625 [01:06<02:22,  3.32it/s] 25%|██▍       | 154/625 [01:06<02:21,  3.33it/s] 25%|██▍       | 155/625 [01:06<02:20,  3.34it/s] 25%|██▍       | 156/625 [01:07<02:20,  3.35it/s] 25%|██▌       | 157/625 [01:07<02:19,  3.35it/s] 25%|██▌       | 158/625 [01:07<02:19,  3.35it/s] 25%|██▌       | 159/625 [01:08<02:22,  3.28it/s] 26%|██▌       | 160/625 [01:08<02:20,  3.30it/s] 26%|██▌       | 161/625 [01:08<02:19,  3.32it/s] 26%|██▌       | 162/625 [01:08<02:19,  3.32it/s] 26%|██▌       | 163/625 [01:09<02:18,  3.33it/s] 26%|██▌       | 164/625 [01:09<02:18,  3.34it/s] 26%|██▋       | 165/625 [01:09<02:17,  3.34it/s] 27%|██▋       | 166/625 [01:10<02:17,  3.34it/s] 27%|██▋       | 167/625 [01:10<02:16,  3.35it/s] 27%|██▋       | 168/625 [01:10<02:16,  3.35it/s] 27%|██▋       | 169/625 [01:11<02:19,  3.28it/s] 27%|██▋       | 170/625 [01:11<02:17,  3.30it/s] 27%|██▋       | 171/625 [01:11<02:16,  3.32it/s] 28%|██▊       | 172/625 [01:12<02:16,  3.33it/s] 28%|██▊       | 173/625 [01:12<02:15,  3.34it/s] 28%|██▊       | 174/625 [01:12<02:14,  3.34it/s] 28%|██▊       | 175/625 [01:12<02:14,  3.34it/s] 28%|██▊       | 176/625 [01:13<02:14,  3.35it/s] 28%|██▊       | 177/625 [01:13<02:13,  3.35it/s] 28%|██▊       | 178/625 [01:13<02:13,  3.35it/s] 29%|██▊       | 179/625 [01:14<02:15,  3.28it/s] 29%|██▉       | 180/625 [01:14<02:14,  3.30it/s] 29%|██▉       | 181/625 [01:14<02:13,  3.32it/s] 29%|██▉       | 182/625 [01:15<02:13,  3.33it/s] 29%|██▉       | 183/625 [01:15<02:12,  3.34it/s] 29%|██▉       | 184/625 [01:15<02:11,  3.34it/s] 30%|██▉       | 185/625 [01:15<02:11,  3.35it/s] 30%|██▉       | 186/625 [01:16<02:11,  3.35it/s] 30%|██▉       | 187/625 [01:16<02:10,  3.35it/s] 30%|███       | 188/625 [01:16<02:10,  3.35it/s] 30%|███       | 189/625 [01:17<02:12,  3.28it/s] 30%|███       | 190/625 [01:17<02:11,  3.31it/s] 31%|███       | 191/625 [01:17<02:10,  3.32it/s] 31%|███       | 192/625 [01:18<02:10,  3.33it/s] 31%|███       | 193/625 [01:18<02:12,  3.26it/s] 31%|███       | 194/625 [01:18<02:10,  3.29it/s] 31%|███       | 195/625 [01:18<02:09,  3.31it/s] 31%|███▏      | 196/625 [01:19<02:09,  3.32it/s] 32%|███▏      | 197/625 [01:19<02:08,  3.34it/s] 32%|███▏      | 198/625 [01:19<02:07,  3.35it/s] 32%|███▏      | 199/625 [01:20<02:07,  3.35it/s] 32%|███▏      | 200/625 [01:20<02:06,  3.36it/s] 32%|███▏      | 201/625 [01:20<02:06,  3.36it/s] 32%|███▏      | 202/625 [01:21<02:05,  3.36it/s] 32%|███▏      | 203/625 [01:21<02:06,  3.33it/s] 33%|███▎      | 204/625 [01:21<02:06,  3.34it/s] 33%|███▎      | 205/625 [01:21<02:05,  3.35it/s] 33%|███▎      | 206/625 [01:22<02:04,  3.35it/s] 33%|███▎      | 207/625 [01:22<02:04,  3.36it/s] 33%|███▎      | 208/625 [01:22<02:04,  3.36it/s] 33%|███▎      | 209/625 [01:23<02:03,  3.36it/s] 34%|███▎      | 210/625 [01:23<02:03,  3.36it/s] 34%|███▍      | 211/625 [01:23<02:03,  3.36it/s] 34%|███▍      | 212/625 [01:23<02:02,  3.36it/s] 34%|███▍      | 213/625 [01:24<02:02,  3.36it/s] 34%|███▍      | 214/625 [01:24<02:04,  3.30it/s] 34%|███▍      | 215/625 [01:24<02:03,  3.32it/s] 35%|███▍      | 216/625 [01:25<02:02,  3.34it/s] 35%|███▍      | 217/625 [01:25<02:02,  3.34it/s] 35%|███▍      | 218/625 [01:25<02:01,  3.35it/s] 35%|███▌      | 219/625 [01:26<02:01,  3.35it/s] 35%|███▌      | 220/625 [01:26<02:00,  3.36it/s] 35%|███▌      | 221/625 [01:26<02:00,  3.36it/s] 36%|███▌      | 222/625 [01:26<01:59,  3.36it/s] 36%|███▌      | 223/625 [01:27<01:59,  3.36it/s] 36%|███▌      | 224/625 [01:27<01:59,  3.36it/s] 36%|███▌      | 225/625 [01:27<02:03,  3.25it/s] 36%|███▌      | 226/625 [01:28<02:01,  3.28it/s] 36%|███▋      | 227/625 [01:28<02:00,  3.31it/s] 36%|███▋      | 228/625 [01:28<01:59,  3.33it/s] 37%|███▋      | 229/625 [01:29<01:58,  3.34it/s] 37%|███▋      | 230/625 [01:29<01:58,  3.35it/s] 37%|███▋      | 231/625 [01:29<01:57,  3.35it/s] 37%|███▋      | 232/625 [01:29<01:57,  3.35it/s] 37%|███▋      | 233/625 [01:30<01:57,  3.35it/s] 37%|███▋      | 234/625 [01:30<01:56,  3.35it/s] 38%|███▊      | 235/625 [01:30<01:58,  3.29it/s] 38%|███▊      | 236/625 [01:31<01:57,  3.31it/s] 38%|███▊      | 237/625 [01:31<01:56,  3.32it/s] 38%|███▊      | 238/625 [01:31<01:56,  3.33it/s] 38%|███▊      | 239/625 [01:32<01:55,  3.34it/s] 38%|███▊      | 240/625 [01:32<01:55,  3.34it/s] 39%|███▊      | 241/625 [01:32<01:54,  3.34it/s] 39%|███▊      | 242/625 [01:32<01:54,  3.35it/s] 39%|███▉      | 243/625 [01:33<01:54,  3.35it/s] 39%|███▉      | 244/625 [01:33<01:53,  3.35it/s] 39%|███▉      | 245/625 [01:33<01:58,  3.21it/s] 39%|███▉      | 246/625 [01:34<01:56,  3.25it/s] 40%|███▉      | 247/625 [01:34<01:55,  3.28it/s] 40%|███▉      | 248/625 [01:34<01:54,  3.30it/s] 40%|███▉      | 249/625 [01:35<01:53,  3.31it/s] 40%|████      | 250/625 [01:35<01:52,  3.33it/s][INFO|trainer.py:2140] 2023-08-29 08:07:20,154 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:07:20,154 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 08:07:20,154 >>   Batch size = 8
{'eval_loss': 1.0988531112670898, 'eval_runtime': 10.1701, 'eval_samples_per_second': 341.886, 'eval_steps_per_second': 42.773, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 55.75it/s][A
  3%|▎         | 12/435 [00:00<00:08, 48.35it/s][A
  4%|▍         | 17/435 [00:00<00:08, 46.75it/s][A
  5%|▌         | 22/435 [00:00<00:09, 45.67it/s][A
  6%|▌         | 27/435 [00:00<00:09, 45.17it/s][A
  7%|▋         | 32/435 [00:00<00:09, 44.72it/s][A
  9%|▊         | 37/435 [00:00<00:08, 44.63it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.38it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.51it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.67it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 43.70it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.06it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.17it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.05it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 44.05it/s][A
 19%|█▉        | 82/435 [00:01<00:08, 44.05it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.15it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.23it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.26it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.40it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.51it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.60it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 44.46it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 44.37it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 44.25it/s][A
 30%|███       | 132/435 [00:02<00:06, 44.31it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 44.35it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 44.31it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 44.34it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.48it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.54it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 44.43it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 44.34it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.37it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.36it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.39it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.37it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 43.21it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 43.77it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 44.07it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 44.09it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 44.15it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 44.12it/s][A
 51%|█████     | 222/435 [00:04<00:04, 44.17it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.16it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.19it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.33it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.47it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.58it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 44.54it/s][A
 59%|█████▉    | 257/435 [00:05<00:04, 44.38it/s][A
 60%|██████    | 262/435 [00:05<00:03, 44.31it/s][A
 61%|██████▏   | 267/435 [00:06<00:03, 44.33it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.24it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.22it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.35it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.41it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.55it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.53it/s][A
 69%|██████▉   | 302/435 [00:06<00:02, 44.47it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.45it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.39it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.34it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.23it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.21it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.34it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.48it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 44.51it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 44.53it/s][A
 81%|████████  | 352/435 [00:07<00:01, 44.49it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 44.34it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 44.33it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.20it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 44.27it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 44.38it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 44.48it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 40.69it/s][A
 90%|█████████ | 392/435 [00:08<00:01, 41.99it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 42.81it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 43.44it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 43.68it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 43.99it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 44.05it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.16it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 43.93it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 43.94it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 43.94it/s][A 40%|████      | 250/625 [01:45<01:52,  3.33it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:07:30,287 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-250
[INFO|configuration_utils.py:351] 2023-08-29 08:07:30,574 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-250/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:07:33,709 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-250/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:07:33,991 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-250/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:07:34,091 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-250/special_tokens_map.json
 40%|████      | 251/625 [01:57<42:25,  6.81s/it] 40%|████      | 252/625 [01:57<30:12,  4.86s/it] 40%|████      | 253/625 [01:58<21:37,  3.49s/it] 41%|████      | 254/625 [01:58<15:38,  2.53s/it] 41%|████      | 255/625 [01:58<11:27,  1.86s/it] 41%|████      | 256/625 [01:58<08:31,  1.39s/it] 41%|████      | 257/625 [01:59<06:29,  1.06s/it] 41%|████▏     | 258/625 [01:59<05:03,  1.21it/s] 41%|████▏     | 259/625 [01:59<04:03,  1.50it/s] 42%|████▏     | 260/625 [02:00<03:21,  1.81it/s] 42%|████▏     | 261/625 [02:00<02:52,  2.11it/s] 42%|████▏     | 262/625 [02:00<02:32,  2.39it/s] 42%|████▏     | 263/625 [02:00<02:19,  2.59it/s] 42%|████▏     | 264/625 [02:01<02:08,  2.80it/s] 42%|████▏     | 265/625 [02:01<02:01,  2.97it/s] 43%|████▎     | 266/625 [02:01<01:55,  3.10it/s] 43%|████▎     | 267/625 [02:02<01:51,  3.20it/s] 43%|████▎     | 268/625 [02:02<01:49,  3.27it/s] 43%|████▎     | 269/625 [02:02<01:47,  3.32it/s] 43%|████▎     | 270/625 [02:02<01:45,  3.36it/s] 43%|████▎     | 271/625 [02:03<01:44,  3.39it/s] 44%|████▎     | 272/625 [02:03<01:43,  3.41it/s] 44%|████▎     | 273/625 [02:03<01:42,  3.42it/s] 44%|████▍     | 274/625 [02:04<01:44,  3.36it/s] 44%|████▍     | 275/625 [02:04<01:43,  3.38it/s] 44%|████▍     | 276/625 [02:04<01:42,  3.41it/s] 44%|████▍     | 277/625 [02:05<01:41,  3.42it/s] 44%|████▍     | 278/625 [02:05<01:41,  3.43it/s] 45%|████▍     | 279/625 [02:05<01:40,  3.44it/s] 45%|████▍     | 280/625 [02:05<01:40,  3.44it/s] 45%|████▍     | 281/625 [02:06<01:39,  3.44it/s] 45%|████▌     | 282/625 [02:06<01:39,  3.44it/s] 45%|████▌     | 283/625 [02:06<01:39,  3.44it/s] 45%|████▌     | 284/625 [02:07<01:38,  3.45it/s] 46%|████▌     | 285/625 [02:07<01:40,  3.37it/s] 46%|████▌     | 286/625 [02:07<01:39,  3.39it/s] 46%|████▌     | 287/625 [02:07<01:39,  3.41it/s] 46%|████▌     | 288/625 [02:08<01:38,  3.42it/s] 46%|████▌     | 289/625 [02:08<01:37,  3.43it/s] 46%|████▋     | 290/625 [02:08<01:37,  3.44it/s] 47%|████▋     | 291/625 [02:09<01:37,  3.44it/s] 47%|████▋     | 292/625 [02:09<01:36,  3.44it/s] 47%|████▋     | 293/625 [02:09<01:36,  3.44it/s] 47%|████▋     | 294/625 [02:09<01:36,  3.45it/s] 47%|████▋     | 295/625 [02:10<01:35,  3.44it/s] 47%|████▋     | 296/625 [02:10<01:37,  3.38it/s] 48%|████▊     | 297/625 [02:10<01:36,  3.39it/s] 48%|████▊     | 298/625 [02:11<01:35,  3.41it/s] 48%|████▊     | 299/625 [02:11<01:35,  3.42it/s] 48%|████▊     | 300/625 [02:11<01:34,  3.43it/s] 48%|████▊     | 301/625 [02:12<01:34,  3.43it/s] 48%|████▊     | 302/625 [02:12<01:33,  3.44it/s] 48%|████▊     | 303/625 [02:12<01:33,  3.44it/s] 49%|████▊     | 304/625 [02:12<01:33,  3.44it/s] 49%|████▉     | 305/625 [02:13<01:32,  3.44it/s] 49%|████▉     | 306/625 [02:13<01:32,  3.45it/s] 49%|████▉     | 307/625 [02:13<01:35,  3.34it/s] 49%|████▉     | 308/625 [02:14<01:34,  3.37it/s] 49%|████▉     | 309/625 [02:14<01:33,  3.40it/s] 50%|████▉     | 310/625 [02:14<01:32,  3.41it/s] 50%|████▉     | 311/625 [02:14<01:31,  3.42it/s] 50%|████▉     | 312/625 [02:15<01:31,  3.43it/s] 50%|█████     | 313/625 [02:15<01:30,  3.44it/s] 50%|█████     | 314/625 [02:15<01:30,  3.44it/s] 50%|█████     | 315/625 [02:16<01:30,  3.44it/s] 51%|█████     | 316/625 [02:16<01:29,  3.44it/s] 51%|█████     | 317/625 [02:16<01:29,  3.44it/s] 51%|█████     | 318/625 [02:16<01:30,  3.40it/s] 51%|█████     | 319/625 [02:17<01:29,  3.41it/s] 51%|█████     | 320/625 [02:17<01:29,  3.42it/s] 51%|█████▏    | 321/625 [02:17<01:28,  3.43it/s] 52%|█████▏    | 322/625 [02:18<01:28,  3.43it/s] 52%|█████▏    | 323/625 [02:18<01:27,  3.44it/s] 52%|█████▏    | 324/625 [02:18<01:27,  3.44it/s] 52%|█████▏    | 325/625 [02:19<01:27,  3.44it/s] 52%|█████▏    | 326/625 [02:19<01:26,  3.44it/s] 52%|█████▏    | 327/625 [02:19<01:26,  3.45it/s] 52%|█████▏    | 328/625 [02:19<01:26,  3.45it/s] 53%|█████▎    | 329/625 [02:20<01:25,  3.45it/s] 53%|█████▎    | 330/625 [02:20<01:25,  3.44it/s] 53%|█████▎    | 331/625 [02:20<01:25,  3.45it/s] 53%|█████▎    | 332/625 [02:21<01:25,  3.45it/s] 53%|█████▎    | 333/625 [02:21<01:24,  3.45it/s] 53%|█████▎    | 334/625 [02:21<01:24,  3.45it/s] 54%|█████▎    | 335/625 [02:21<01:24,  3.45it/s] 54%|█████▍    | 336/625 [02:22<01:24,  3.44it/s] 54%|█████▍    | 337/625 [02:22<01:23,  3.44it/s] 54%|█████▍    | 338/625 [02:22<01:26,  3.32it/s] 54%|█████▍    | 339/625 [02:23<01:25,  3.36it/s] 54%|█████▍    | 340/625 [02:23<01:24,  3.38it/s] 55%|█████▍    | 341/625 [02:23<01:23,  3.40it/s] 55%|█████▍    | 342/625 [02:23<01:22,  3.42it/s] 55%|█████▍    | 343/625 [02:24<01:22,  3.43it/s] 55%|█████▌    | 344/625 [02:24<01:21,  3.43it/s] 55%|█████▌    | 345/625 [02:24<01:21,  3.44it/s] 55%|█████▌    | 346/625 [02:25<01:21,  3.44it/s] 56%|█████▌    | 347/625 [02:25<01:20,  3.44it/s] 56%|█████▌    | 348/625 [02:25<01:20,  3.44it/s] 56%|█████▌    | 349/625 [02:26<01:21,  3.39it/s] 56%|█████▌    | 350/625 [02:26<01:20,  3.41it/s] 56%|█████▌    | 351/625 [02:26<01:20,  3.42it/s] 56%|█████▋    | 352/625 [02:26<01:19,  3.43it/s] 56%|█████▋    | 353/625 [02:27<01:19,  3.43it/s] 57%|█████▋    | 354/625 [02:27<01:18,  3.43it/s] 57%|█████▋    | 355/625 [02:27<01:18,  3.44it/s] 57%|█████▋    | 356/625 [02:28<01:18,  3.44it/s] 57%|█████▋    | 357/625 [02:28<01:17,  3.44it/s] 57%|█████▋    | 358/625 [02:28<01:17,  3.44it/s] 57%|█████▋    | 359/625 [02:28<01:17,  3.44it/s] 58%|█████▊    | 360/625 [02:29<01:18,  3.38it/s] 58%|█████▊    | 361/625 [02:29<01:17,  3.40it/s] 58%|█████▊    | 362/625 [02:29<01:17,  3.41it/s] 58%|█████▊    | 363/625 [02:30<01:16,  3.42it/s] 58%|█████▊    | 364/625 [02:30<01:16,  3.43it/s] 58%|█████▊    | 365/625 [02:30<01:15,  3.43it/s] 59%|█████▊    | 366/625 [02:30<01:15,  3.44it/s] 59%|█████▊    | 367/625 [02:31<01:15,  3.43it/s] 59%|█████▉    | 368/625 [02:31<01:14,  3.44it/s] 59%|█████▉    | 369/625 [02:31<01:14,  3.44it/s] 59%|█████▉    | 370/625 [02:32<01:14,  3.44it/s] 59%|█████▉    | 371/625 [02:32<01:15,  3.38it/s] 60%|█████▉    | 372/625 [02:32<01:14,  3.40it/s] 60%|█████▉    | 373/625 [02:33<01:13,  3.41it/s] 60%|█████▉    | 374/625 [02:33<01:13,  3.42it/s] 60%|██████    | 375/625 [02:33<01:12,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 08:08:18,356 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:08:18,357 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 08:08:18,357 >>   Batch size = 8
{'eval_loss': 1.1152031421661377, 'eval_runtime': 9.8448, 'eval_samples_per_second': 353.183, 'eval_steps_per_second': 44.186, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.03it/s][A
  3%|▎         | 12/435 [00:00<00:08, 48.68it/s][A
  4%|▍         | 17/435 [00:00<00:08, 46.71it/s][A
  5%|▌         | 22/435 [00:00<00:09, 45.67it/s][A
  6%|▌         | 27/435 [00:00<00:09, 45.26it/s][A
  7%|▋         | 32/435 [00:00<00:09, 44.78it/s][A
  9%|▊         | 37/435 [00:00<00:08, 44.63it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.45it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.51it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.65it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.66it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.68it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.47it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.34it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 43.83it/s][A
 19%|█▉        | 82/435 [00:01<00:08, 44.03it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.14it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.18it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.45it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.51it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.54it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.39it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 44.30it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 44.25it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 44.24it/s][A
 30%|███       | 132/435 [00:02<00:06, 44.34it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 44.34it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 44.55it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 44.59it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.60it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.38it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 44.22it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 44.23it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.24it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.25it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.45it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.66it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 44.61it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 44.55it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 44.27it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 44.23it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 43.56it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 43.80it/s][A
 51%|█████     | 222/435 [00:04<00:04, 43.97it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.09it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.34it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.41it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.26it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.29it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 44.21it/s][A
 59%|█████▉    | 257/435 [00:05<00:04, 44.20it/s][A
 60%|██████    | 262/435 [00:05<00:03, 44.33it/s][A
 61%|██████▏   | 267/435 [00:05<00:03, 44.36it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.47it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.52it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.56it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.50it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.39it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.26it/s][A
 69%|██████▉   | 302/435 [00:06<00:03, 44.26it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.23it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.32it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.47it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.54it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.55it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.56it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.47it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 44.39it/s][A
 80%|███████▉  | 347/435 [00:07<00:02, 42.90it/s][A
 81%|████████  | 352/435 [00:07<00:01, 43.45it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 43.71it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 43.97it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.12it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 44.28it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 44.34it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 44.30it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 44.17it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 44.25it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 44.32it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 44.48it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 44.47it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 44.47it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 44.46it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.50it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 44.30it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.23it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 44.23it/s][A 60%|██████    | 375/625 [02:43<01:12,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:08:28,213 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-375
[INFO|configuration_utils.py:351] 2023-08-29 08:08:28,316 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-375/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:08:31,133 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-375/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:08:31,605 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-375/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:08:31,744 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-375/special_tokens_map.json
 60%|██████    | 376/625 [02:55<27:38,  6.66s/it] 60%|██████    | 377/625 [02:55<19:40,  4.76s/it] 60%|██████    | 378/625 [02:55<14:05,  3.42s/it] 61%|██████    | 379/625 [02:56<10:11,  2.48s/it] 61%|██████    | 380/625 [02:56<07:28,  1.83s/it] 61%|██████    | 381/625 [02:56<05:34,  1.37s/it] 61%|██████    | 382/625 [02:56<04:14,  1.05s/it] 61%|██████▏   | 383/625 [02:57<03:19,  1.21it/s] 61%|██████▏   | 384/625 [02:57<02:40,  1.50it/s] 62%|██████▏   | 385/625 [02:57<02:13,  1.80it/s] 62%|██████▏   | 386/625 [02:58<01:54,  2.09it/s] 62%|██████▏   | 387/625 [02:58<01:43,  2.29it/s] 62%|██████▏   | 388/625 [02:58<01:33,  2.53it/s] 62%|██████▏   | 389/625 [02:59<01:26,  2.73it/s] 62%|██████▏   | 390/625 [02:59<01:21,  2.89it/s] 63%|██████▎   | 391/625 [02:59<01:17,  3.02it/s] 63%|██████▎   | 392/625 [02:59<01:14,  3.11it/s] 63%|██████▎   | 393/625 [03:00<01:12,  3.18it/s] 63%|██████▎   | 394/625 [03:00<01:11,  3.23it/s] 63%|██████▎   | 395/625 [03:00<01:10,  3.27it/s] 63%|██████▎   | 396/625 [03:01<01:09,  3.29it/s] 64%|██████▎   | 397/625 [03:01<01:09,  3.27it/s] 64%|██████▎   | 398/625 [03:01<01:08,  3.30it/s] 64%|██████▍   | 399/625 [03:02<01:08,  3.32it/s] 64%|██████▍   | 400/625 [03:02<01:07,  3.33it/s] 64%|██████▍   | 401/625 [03:02<01:07,  3.34it/s] 64%|██████▍   | 402/625 [03:02<01:06,  3.35it/s] 64%|██████▍   | 403/625 [03:03<01:06,  3.35it/s] 65%|██████▍   | 404/625 [03:03<01:05,  3.36it/s] 65%|██████▍   | 405/625 [03:03<01:05,  3.36it/s] 65%|██████▍   | 406/625 [03:04<01:05,  3.36it/s] 65%|██████▌   | 407/625 [03:04<01:04,  3.36it/s] 65%|██████▌   | 408/625 [03:04<01:05,  3.30it/s] 65%|██████▌   | 409/625 [03:05<01:05,  3.32it/s] 66%|██████▌   | 410/625 [03:05<01:04,  3.33it/s] 66%|██████▌   | 411/625 [03:05<01:04,  3.34it/s] 66%|██████▌   | 412/625 [03:05<01:03,  3.35it/s] 66%|██████▌   | 413/625 [03:06<01:03,  3.36it/s] 66%|██████▌   | 414/625 [03:06<01:02,  3.36it/s] 66%|██████▋   | 415/625 [03:06<01:02,  3.36it/s] 67%|██████▋   | 416/625 [03:07<01:02,  3.36it/s] 67%|██████▋   | 417/625 [03:07<01:01,  3.36it/s] 67%|██████▋   | 418/625 [03:07<01:01,  3.36it/s] 67%|██████▋   | 419/625 [03:08<01:03,  3.26it/s] 67%|██████▋   | 420/625 [03:08<01:02,  3.28it/s] 67%|██████▋   | 421/625 [03:08<01:01,  3.30it/s] 68%|██████▊   | 422/625 [03:08<01:01,  3.31it/s] 68%|██████▊   | 423/625 [03:09<01:00,  3.32it/s] 68%|██████▊   | 424/625 [03:09<01:00,  3.33it/s] 68%|██████▊   | 425/625 [03:09<00:59,  3.33it/s] 68%|██████▊   | 426/625 [03:10<00:59,  3.34it/s] 68%|██████▊   | 427/625 [03:10<00:59,  3.32it/s] 68%|██████▊   | 428/625 [03:10<00:59,  3.34it/s] 69%|██████▊   | 429/625 [03:11<01:00,  3.26it/s] 69%|██████▉   | 430/625 [03:11<00:59,  3.28it/s] 69%|██████▉   | 431/625 [03:11<00:58,  3.30it/s] 69%|██████▉   | 432/625 [03:11<00:58,  3.31it/s] 69%|██████▉   | 433/625 [03:12<00:57,  3.32it/s] 69%|██████▉   | 434/625 [03:12<00:57,  3.33it/s] 70%|██████▉   | 435/625 [03:12<00:56,  3.33it/s] 70%|██████▉   | 436/625 [03:13<00:56,  3.34it/s] 70%|██████▉   | 437/625 [03:13<00:56,  3.34it/s] 70%|███████   | 438/625 [03:13<00:55,  3.35it/s] 70%|███████   | 439/625 [03:14<00:57,  3.26it/s] 70%|███████   | 440/625 [03:14<00:56,  3.29it/s] 71%|███████   | 441/625 [03:14<00:55,  3.31it/s] 71%|███████   | 442/625 [03:15<00:55,  3.33it/s] 71%|███████   | 443/625 [03:15<00:54,  3.34it/s] 71%|███████   | 444/625 [03:15<00:54,  3.34it/s] 71%|███████   | 445/625 [03:15<00:53,  3.35it/s] 71%|███████▏  | 446/625 [03:16<00:53,  3.34it/s] 72%|███████▏  | 447/625 [03:16<00:53,  3.34it/s] 72%|███████▏  | 448/625 [03:16<00:52,  3.35it/s] 72%|███████▏  | 449/625 [03:17<00:53,  3.27it/s] 72%|███████▏  | 450/625 [03:17<00:53,  3.30it/s] 72%|███████▏  | 451/625 [03:17<00:52,  3.31it/s] 72%|███████▏  | 452/625 [03:18<00:51,  3.33it/s] 72%|███████▏  | 453/625 [03:18<00:51,  3.34it/s] 73%|███████▎  | 454/625 [03:18<00:51,  3.35it/s] 73%|███████▎  | 455/625 [03:18<00:50,  3.35it/s] 73%|███████▎  | 456/625 [03:19<00:50,  3.35it/s] 73%|███████▎  | 457/625 [03:19<00:50,  3.35it/s] 73%|███████▎  | 458/625 [03:19<00:49,  3.35it/s] 73%|███████▎  | 459/625 [03:20<00:50,  3.29it/s] 74%|███████▎  | 460/625 [03:20<00:49,  3.31it/s] 74%|███████▍  | 461/625 [03:20<00:49,  3.32it/s] 74%|███████▍  | 462/625 [03:21<00:48,  3.33it/s] 74%|███████▍  | 463/625 [03:21<00:48,  3.34it/s] 74%|███████▍  | 464/625 [03:21<00:48,  3.34it/s] 74%|███████▍  | 465/625 [03:21<00:47,  3.35it/s] 75%|███████▍  | 466/625 [03:22<00:47,  3.35it/s] 75%|███████▍  | 467/625 [03:22<00:47,  3.35it/s] 75%|███████▍  | 468/625 [03:22<00:46,  3.35it/s] 75%|███████▌  | 469/625 [03:23<00:46,  3.35it/s] 75%|███████▌  | 470/625 [03:23<00:46,  3.35it/s] 75%|███████▌  | 471/625 [03:23<00:46,  3.35it/s] 76%|███████▌  | 472/625 [03:23<00:45,  3.35it/s] 76%|███████▌  | 473/625 [03:24<00:46,  3.30it/s] 76%|███████▌  | 474/625 [03:24<00:45,  3.31it/s] 76%|███████▌  | 475/625 [03:24<00:45,  3.33it/s] 76%|███████▌  | 476/625 [03:25<00:44,  3.34it/s] 76%|███████▋  | 477/625 [03:25<00:44,  3.35it/s] 76%|███████▋  | 478/625 [03:25<00:43,  3.35it/s] 77%|███████▋  | 479/625 [03:26<00:43,  3.35it/s] 77%|███████▋  | 480/625 [03:26<00:43,  3.35it/s] 77%|███████▋  | 481/625 [03:26<00:42,  3.35it/s] 77%|███████▋  | 482/625 [03:26<00:42,  3.36it/s] 77%|███████▋  | 483/625 [03:27<00:42,  3.36it/s] 77%|███████▋  | 484/625 [03:27<00:42,  3.31it/s] 78%|███████▊  | 485/625 [03:27<00:42,  3.33it/s] 78%|███████▊  | 486/625 [03:28<00:41,  3.34it/s] 78%|███████▊  | 487/625 [03:28<00:41,  3.34it/s] 78%|███████▊  | 488/625 [03:28<00:40,  3.35it/s] 78%|███████▊  | 489/625 [03:29<00:40,  3.35it/s] 78%|███████▊  | 490/625 [03:29<00:40,  3.35it/s] 79%|███████▊  | 491/625 [03:29<00:39,  3.36it/s] 79%|███████▊  | 492/625 [03:29<00:39,  3.36it/s] 79%|███████▉  | 493/625 [03:30<00:39,  3.36it/s] 79%|███████▉  | 494/625 [03:30<00:39,  3.35it/s] 79%|███████▉  | 495/625 [03:30<00:40,  3.19it/s] 79%|███████▉  | 496/625 [03:31<00:39,  3.23it/s] 80%|███████▉  | 497/625 [03:31<00:39,  3.26it/s] 80%|███████▉  | 498/625 [03:31<00:38,  3.28it/s] 80%|███████▉  | 499/625 [03:32<00:38,  3.29it/s] 80%|████████  | 500/625 [03:32<00:37,  3.30it/s]                                                  80%|████████  | 500/625 [03:32<00:37,  3.30it/s][INFO|trainer.py:2140] 2023-08-29 08:09:17,161 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:09:17,161 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 08:09:17,161 >>   Batch size = 8
{'eval_loss': 1.135076880455017, 'eval_runtime': 9.812, 'eval_samples_per_second': 354.362, 'eval_steps_per_second': 44.334, 'epoch': 3.0}
{'loss': 0.3898, 'learning_rate': 7.5e-06, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 55.50it/s][A
  3%|▎         | 12/435 [00:00<00:08, 48.61it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.02it/s][A
  5%|▌         | 22/435 [00:00<00:08, 46.02it/s][A
  6%|▌         | 27/435 [00:00<00:08, 45.51it/s][A
  7%|▋         | 32/435 [00:00<00:09, 44.74it/s][A
  9%|▊         | 37/435 [00:00<00:08, 44.54it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.31it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.34it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.62it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 42.91it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 43.53it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 43.96it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 43.96it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 43.98it/s][A
 19%|█▉        | 82/435 [00:01<00:08, 43.90it/s][A
 20%|██        | 87/435 [00:01<00:07, 43.81it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.12it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.22it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.43it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.56it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.70it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 44.59it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 44.39it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 44.16it/s][A
 30%|███       | 132/435 [00:02<00:06, 44.12it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 44.25it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 44.44it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 44.44it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.58it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.79it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 44.55it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 44.45it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.23it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.16it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.33it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.44it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 40.66it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 41.92it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 42.77it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 43.46it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 43.81it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 43.88it/s][A
 51%|█████     | 222/435 [00:05<00:04, 44.12it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.16it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 43.94it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 43.98it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.33it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.47it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 44.63it/s][A
 59%|█████▉    | 257/435 [00:05<00:03, 44.62it/s][A
 60%|██████    | 262/435 [00:05<00:03, 44.46it/s][A
 61%|██████▏   | 267/435 [00:06<00:03, 44.39it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.34it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.10it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.16it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.33it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.46it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.63it/s][A
 69%|██████▉   | 302/435 [00:06<00:02, 44.61it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.48it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.40it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.26it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.22it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 43.35it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 43.84it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.14it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 44.25it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 44.38it/s][A
 81%|████████  | 352/435 [00:07<00:01, 44.33it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 44.25it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 44.23it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.09it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 44.24it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 44.31it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 44.43it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 44.63it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 44.61it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 44.46it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 44.41it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 44.29it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 44.13it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 44.26it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.37it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 44.44it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.48it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 44.48it/s][A 80%|████████  | 500/625 [03:42<00:37,  3.30it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:09:27,063 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-500
[INFO|configuration_utils.py:351] 2023-08-29 08:09:27,214 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-500/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:09:29,893 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:09:29,996 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:09:30,032 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-500/special_tokens_map.json
 80%|████████  | 501/625 [03:53<13:44,  6.65s/it] 80%|████████  | 502/625 [03:54<09:43,  4.75s/it] 80%|████████  | 503/625 [03:54<07:09,  3.52s/it] 81%|████████  | 504/625 [03:55<05:10,  2.57s/it] 81%|████████  | 505/625 [03:55<03:46,  1.89s/it] 81%|████████  | 506/625 [03:55<02:47,  1.41s/it] 81%|████████  | 507/625 [03:56<02:06,  1.08s/it] 81%|████████▏ | 508/625 [03:56<01:38,  1.19it/s] 81%|████████▏ | 509/625 [03:56<01:18,  1.47it/s] 82%|████████▏ | 510/625 [03:56<01:05,  1.77it/s] 82%|████████▏ | 511/625 [03:57<00:55,  2.06it/s] 82%|████████▏ | 512/625 [03:57<00:48,  2.33it/s] 82%|████████▏ | 513/625 [03:57<00:43,  2.56it/s] 82%|████████▏ | 514/625 [03:58<00:42,  2.63it/s] 82%|████████▏ | 515/625 [03:58<00:39,  2.81it/s] 83%|████████▎ | 516/625 [03:58<00:36,  2.96it/s] 83%|████████▎ | 517/625 [03:59<00:35,  3.07it/s] 83%|████████▎ | 518/625 [03:59<00:33,  3.15it/s] 83%|████████▎ | 519/625 [03:59<00:33,  3.21it/s] 83%|████████▎ | 520/625 [04:00<00:32,  3.25it/s] 83%|████████▎ | 521/625 [04:00<00:31,  3.28it/s] 84%|████████▎ | 522/625 [04:00<00:31,  3.30it/s] 84%|████████▎ | 523/625 [04:00<00:30,  3.31it/s] 84%|████████▍ | 524/625 [04:01<00:31,  3.23it/s] 84%|████████▍ | 525/625 [04:01<00:30,  3.27it/s] 84%|████████▍ | 526/625 [04:01<00:30,  3.29it/s] 84%|████████▍ | 527/625 [04:02<00:29,  3.31it/s] 84%|████████▍ | 528/625 [04:02<00:29,  3.33it/s] 85%|████████▍ | 529/625 [04:02<00:28,  3.34it/s] 85%|████████▍ | 530/625 [04:03<00:28,  3.35it/s] 85%|████████▍ | 531/625 [04:03<00:28,  3.36it/s] 85%|████████▌ | 532/625 [04:03<00:27,  3.36it/s] 85%|████████▌ | 533/625 [04:03<00:27,  3.36it/s] 85%|████████▌ | 534/625 [04:04<00:28,  3.24it/s] 86%|████████▌ | 535/625 [04:04<00:27,  3.28it/s] 86%|████████▌ | 536/625 [04:04<00:26,  3.31it/s] 86%|████████▌ | 537/625 [04:05<00:26,  3.33it/s] 86%|████████▌ | 538/625 [04:05<00:26,  3.34it/s] 86%|████████▌ | 539/625 [04:05<00:25,  3.35it/s] 86%|████████▋ | 540/625 [04:06<00:25,  3.35it/s] 87%|████████▋ | 541/625 [04:06<00:25,  3.35it/s] 87%|████████▋ | 542/625 [04:06<00:24,  3.36it/s] 87%|████████▋ | 543/625 [04:06<00:24,  3.36it/s] 87%|████████▋ | 544/625 [04:07<00:24,  3.30it/s] 87%|████████▋ | 545/625 [04:07<00:24,  3.32it/s] 87%|████████▋ | 546/625 [04:07<00:23,  3.33it/s] 88%|████████▊ | 547/625 [04:08<00:23,  3.34it/s] 88%|████████▊ | 548/625 [04:08<00:23,  3.34it/s] 88%|████████▊ | 549/625 [04:08<00:22,  3.34it/s] 88%|████████▊ | 550/625 [04:09<00:22,  3.35it/s] 88%|████████▊ | 551/625 [04:09<00:22,  3.35it/s] 88%|████████▊ | 552/625 [04:09<00:21,  3.35it/s] 88%|████████▊ | 553/625 [04:09<00:21,  3.36it/s] 89%|████████▊ | 554/625 [04:10<00:21,  3.36it/s] 89%|████████▉ | 555/625 [04:10<00:21,  3.30it/s] 89%|████████▉ | 556/625 [04:10<00:20,  3.32it/s] 89%|████████▉ | 557/625 [04:11<00:20,  3.33it/s] 89%|████████▉ | 558/625 [04:11<00:20,  3.34it/s] 89%|████████▉ | 559/625 [04:11<00:19,  3.34it/s] 90%|████████▉ | 560/625 [04:12<00:19,  3.35it/s] 90%|████████▉ | 561/625 [04:12<00:19,  3.35it/s] 90%|████████▉ | 562/625 [04:12<00:18,  3.36it/s] 90%|█████████ | 563/625 [04:12<00:18,  3.35it/s] 90%|█████████ | 564/625 [04:13<00:18,  3.35it/s] 90%|█████████ | 565/625 [04:13<00:17,  3.35it/s] 91%|█████████ | 566/625 [04:13<00:17,  3.29it/s] 91%|█████████ | 567/625 [04:14<00:17,  3.31it/s] 91%|█████████ | 568/625 [04:14<00:17,  3.32it/s] 91%|█████████ | 569/625 [04:14<00:16,  3.33it/s] 91%|█████████ | 570/625 [04:15<00:16,  3.34it/s] 91%|█████████▏| 571/625 [04:15<00:16,  3.34it/s] 92%|█████████▏| 572/625 [04:15<00:15,  3.35it/s] 92%|█████████▏| 573/625 [04:15<00:15,  3.35it/s] 92%|█████████▏| 574/625 [04:16<00:15,  3.35it/s] 92%|█████████▏| 575/625 [04:16<00:14,  3.34it/s] 92%|█████████▏| 576/625 [04:16<00:14,  3.27it/s] 92%|█████████▏| 577/625 [04:17<00:14,  3.29it/s] 92%|█████████▏| 578/625 [04:17<00:14,  3.31it/s] 93%|█████████▎| 579/625 [04:17<00:13,  3.32it/s] 93%|█████████▎| 580/625 [04:18<00:13,  3.33it/s] 93%|█████████▎| 581/625 [04:18<00:13,  3.33it/s] 93%|█████████▎| 582/625 [04:18<00:12,  3.34it/s] 93%|█████████▎| 583/625 [04:18<00:12,  3.34it/s] 93%|█████████▎| 584/625 [04:19<00:12,  3.34it/s] 94%|█████████▎| 585/625 [04:19<00:11,  3.34it/s] 94%|█████████▍| 586/625 [04:19<00:11,  3.25it/s] 94%|█████████▍| 587/625 [04:20<00:11,  3.28it/s] 94%|█████████▍| 588/625 [04:20<00:11,  3.30it/s] 94%|█████████▍| 589/625 [04:20<00:10,  3.32it/s] 94%|█████████▍| 590/625 [04:21<00:10,  3.33it/s] 95%|█████████▍| 591/625 [04:21<00:10,  3.34it/s] 95%|█████████▍| 592/625 [04:21<00:09,  3.35it/s] 95%|█████████▍| 593/625 [04:21<00:09,  3.36it/s] 95%|█████████▌| 594/625 [04:22<00:09,  3.36it/s] 95%|█████████▌| 595/625 [04:22<00:08,  3.36it/s] 95%|█████████▌| 596/625 [04:22<00:08,  3.36it/s] 96%|█████████▌| 597/625 [04:23<00:08,  3.36it/s] 96%|█████████▌| 598/625 [04:23<00:08,  3.35it/s] 96%|█████████▌| 599/625 [04:23<00:07,  3.36it/s] 96%|█████████▌| 600/625 [04:24<00:07,  3.36it/s] 96%|█████████▌| 601/625 [04:24<00:07,  3.36it/s] 96%|█████████▋| 602/625 [04:24<00:06,  3.36it/s] 96%|█████████▋| 603/625 [04:24<00:06,  3.37it/s] 97%|█████████▋| 604/625 [04:25<00:06,  3.36it/s] 97%|█████████▋| 605/625 [04:25<00:05,  3.36it/s] 97%|█████████▋| 606/625 [04:25<00:05,  3.26it/s] 97%|█████████▋| 607/625 [04:26<00:05,  3.29it/s] 97%|█████████▋| 608/625 [04:26<00:05,  3.31it/s] 97%|█████████▋| 609/625 [04:26<00:04,  3.32it/s] 98%|█████████▊| 610/625 [04:27<00:04,  3.33it/s] 98%|█████████▊| 611/625 [04:27<00:04,  3.33it/s] 98%|█████████▊| 612/625 [04:27<00:03,  3.34it/s] 98%|█████████▊| 613/625 [04:27<00:03,  3.33it/s] 98%|█████████▊| 614/625 [04:28<00:03,  3.33it/s] 98%|█████████▊| 615/625 [04:28<00:02,  3.34it/s] 99%|█████████▊| 616/625 [04:28<00:02,  3.21it/s] 99%|█████████▊| 617/625 [04:29<00:02,  3.25it/s] 99%|█████████▉| 618/625 [04:29<00:02,  3.28it/s] 99%|█████████▉| 619/625 [04:29<00:01,  3.29it/s] 99%|█████████▉| 620/625 [04:30<00:01,  3.31it/s] 99%|█████████▉| 621/625 [04:30<00:01,  3.32it/s]100%|█████████▉| 622/625 [04:30<00:00,  3.33it/s]100%|█████████▉| 623/625 [04:30<00:00,  3.34it/s]100%|█████████▉| 624/625 [04:31<00:00,  3.34it/s]100%|██████████| 625/625 [04:31<00:00,  3.34it/s][INFO|trainer.py:2140] 2023-08-29 08:10:16,314 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:10:16,314 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 08:10:16,314 >>   Batch size = 8
{'eval_loss': 1.1503639221191406, 'eval_runtime': 9.842, 'eval_samples_per_second': 353.282, 'eval_steps_per_second': 44.198, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.44it/s][A
  3%|▎         | 12/435 [00:00<00:08, 47.82it/s][A
  4%|▍         | 17/435 [00:00<00:08, 46.73it/s][A
  5%|▌         | 22/435 [00:00<00:08, 45.98it/s][A
  6%|▌         | 27/435 [00:00<00:08, 45.54it/s][A
  7%|▋         | 32/435 [00:00<00:08, 45.02it/s][A
  9%|▊         | 37/435 [00:00<00:08, 44.83it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.48it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.34it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.46it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.65it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.63it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.66it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.55it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 44.50it/s][A
 19%|█▉        | 82/435 [00:01<00:07, 44.34it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.28it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.31it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.31it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.50it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.53it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.48it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 44.48it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 44.38it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 44.31it/s][A
 30%|███       | 132/435 [00:02<00:06, 44.03it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 44.23it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 43.56it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 43.96it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.01it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.16it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 44.28it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 43.66it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.10it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.16it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 43.99it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.14it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 44.32it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 44.41it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 44.53it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 44.43it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 44.38it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 44.35it/s][A
 51%|█████     | 222/435 [00:04<00:04, 44.41it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.34it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.44it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.49it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.57it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.39it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 44.40it/s][A
 59%|█████▉    | 257/435 [00:05<00:04, 44.40it/s][A
 60%|██████    | 262/435 [00:05<00:03, 44.39it/s][A
 61%|██████▏   | 267/435 [00:05<00:03, 44.31it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.35it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 41.37it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 42.42it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 43.18it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 43.60it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 43.90it/s][A
 69%|██████▉   | 302/435 [00:06<00:03, 44.09it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.17it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.14it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 43.87it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.07it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.17it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.37it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.46it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 44.64it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 44.52it/s][A
 81%|████████  | 352/435 [00:07<00:01, 44.44it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 44.29it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 44.07it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.13it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 44.23it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 44.43it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 44.56it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 44.75it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 44.62it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 44.46it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 44.34it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 44.17it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 42.93it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 43.47it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 43.90it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 44.05it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.26it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 44.26it/s][A100%|██████████| 625/625 [04:41<00:00,  3.34it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:10:26,408 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-625
[INFO|configuration_utils.py:351] 2023-08-29 08:10:26,668 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-625/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:10:29,075 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-625/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:10:29,187 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-625/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:10:29,231 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-625/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 08:10:36,769 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 08:10:36,806 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-125 (score: 1.0988531112670898).
                                                 100%|██████████| 625/625 [05:04<00:00,  3.34it/s]100%|██████████| 625/625 [05:04<00:00,  2.05it/s]
[INFO|trainer.py:1894] 2023-08-29 08:10:49,132 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 08:10:49,214 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:10:51,418 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:10:51,518 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:10:51,572 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:10:51,944 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:10:51,969 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:10:51,969 >>   train_loss               =     0.3859
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:10:51,969 >>   train_runtime            = 0:05:04.18
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:10:51,969 >>   train_samples            =       8000
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:10:51,970 >>   train_samples_per_second =    131.501
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:10:51,970 >>   train_steps_per_second   =      2.055
{'eval_loss': 1.1580151319503784, 'eval_runtime': 9.8386, 'eval_samples_per_second': 353.402, 'eval_steps_per_second': 44.213, 'epoch': 5.0}
{'train_runtime': 304.1809, 'train_samples_per_second': 131.501, 'train_steps_per_second': 2.055, 'train_loss': 0.38587396240234373, 'epoch': 5.0}
08/29/2023 08:10:52 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 08:10:52,119 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:10:52,120 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 08:10:52,120 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|▏         | 6/435 [00:00<00:07, 55.56it/s]  3%|▎         | 12/435 [00:00<00:08, 48.56it/s]  4%|▍         | 17/435 [00:00<00:08, 47.30it/s]  5%|▌         | 22/435 [00:00<00:08, 46.59it/s]  6%|▌         | 27/435 [00:00<00:08, 46.02it/s]  7%|▋         | 32/435 [00:00<00:08, 45.66it/s]  9%|▊         | 37/435 [00:00<00:08, 45.43it/s] 10%|▉         | 42/435 [00:00<00:08, 45.03it/s] 11%|█         | 47/435 [00:01<00:08, 44.55it/s] 12%|█▏        | 52/435 [00:01<00:08, 44.29it/s] 13%|█▎        | 57/435 [00:01<00:08, 44.35it/s] 14%|█▍        | 62/435 [00:01<00:08, 44.49it/s] 15%|█▌        | 67/435 [00:01<00:08, 44.63it/s] 17%|█▋        | 72/435 [00:01<00:08, 44.81it/s] 18%|█▊        | 77/435 [00:01<00:07, 44.86it/s] 19%|█▉        | 82/435 [00:01<00:07, 44.87it/s] 20%|██        | 87/435 [00:01<00:07, 44.69it/s] 21%|██        | 92/435 [00:02<00:07, 44.39it/s] 22%|██▏       | 97/435 [00:02<00:07, 43.16it/s] 23%|██▎       | 102/435 [00:02<00:07, 43.99it/s] 25%|██▍       | 107/435 [00:02<00:07, 44.26it/s] 26%|██▌       | 112/435 [00:02<00:07, 44.52it/s] 27%|██▋       | 117/435 [00:02<00:07, 44.66it/s] 28%|██▊       | 122/435 [00:02<00:07, 44.68it/s] 29%|██▉       | 127/435 [00:02<00:06, 44.72it/s] 30%|███       | 132/435 [00:02<00:06, 43.68it/s] 31%|███▏      | 137/435 [00:03<00:06, 43.72it/s] 33%|███▎      | 142/435 [00:03<00:06, 43.82it/s] 34%|███▍      | 147/435 [00:03<00:06, 43.98it/s] 35%|███▍      | 152/435 [00:03<00:06, 44.22it/s] 36%|███▌      | 157/435 [00:03<00:06, 44.49it/s] 37%|███▋      | 162/435 [00:03<00:06, 44.55it/s] 38%|███▊      | 167/435 [00:03<00:05, 44.74it/s] 40%|███▉      | 172/435 [00:03<00:05, 44.58it/s] 41%|████      | 177/435 [00:03<00:05, 44.52it/s] 42%|████▏     | 182/435 [00:04<00:05, 44.39it/s] 43%|████▎     | 187/435 [00:04<00:05, 44.19it/s] 44%|████▍     | 192/435 [00:04<00:05, 44.18it/s] 45%|████▌     | 197/435 [00:04<00:05, 44.48it/s] 46%|████▋     | 202/435 [00:04<00:05, 44.64it/s] 48%|████▊     | 207/435 [00:04<00:05, 44.81it/s] 49%|████▊     | 212/435 [00:04<00:04, 44.78it/s] 50%|████▉     | 217/435 [00:04<00:04, 44.64it/s] 51%|█████     | 222/435 [00:04<00:04, 44.56it/s] 52%|█████▏    | 227/435 [00:05<00:04, 44.42it/s] 53%|█████▎    | 232/435 [00:05<00:04, 44.26it/s] 54%|█████▍    | 237/435 [00:05<00:04, 44.25it/s] 56%|█████▌    | 242/435 [00:05<00:04, 44.38it/s] 57%|█████▋    | 247/435 [00:05<00:04, 44.57it/s] 58%|█████▊    | 252/435 [00:05<00:04, 44.65it/s] 59%|█████▉    | 257/435 [00:05<00:03, 44.66it/s] 60%|██████    | 262/435 [00:05<00:03, 44.73it/s] 61%|██████▏   | 267/435 [00:05<00:03, 43.39it/s] 63%|██████▎   | 272/435 [00:06<00:03, 43.67it/s] 64%|██████▎   | 277/435 [00:06<00:03, 43.80it/s] 65%|██████▍   | 282/435 [00:06<00:03, 44.03it/s] 66%|██████▌   | 287/435 [00:06<00:03, 44.20it/s] 67%|██████▋   | 292/435 [00:06<00:03, 44.42it/s] 68%|██████▊   | 297/435 [00:06<00:03, 44.52it/s] 69%|██████▉   | 302/435 [00:06<00:02, 44.55it/s] 71%|███████   | 307/435 [00:06<00:02, 44.50it/s] 72%|███████▏  | 312/435 [00:06<00:02, 44.33it/s] 73%|███████▎  | 317/435 [00:07<00:02, 44.31it/s] 74%|███████▍  | 322/435 [00:07<00:02, 44.32it/s] 75%|███████▌  | 327/435 [00:07<00:02, 44.38it/s] 76%|███████▋  | 332/435 [00:07<00:02, 44.49it/s] 77%|███████▋  | 337/435 [00:07<00:02, 44.49it/s] 79%|███████▊  | 342/435 [00:07<00:02, 44.67it/s] 80%|███████▉  | 347/435 [00:07<00:01, 44.66it/s] 81%|████████  | 352/435 [00:07<00:01, 44.57it/s] 82%|████████▏ | 357/435 [00:08<00:01, 44.31it/s] 83%|████████▎ | 362/435 [00:08<00:01, 44.31it/s] 84%|████████▍ | 367/435 [00:08<00:01, 44.33it/s] 86%|████████▌ | 372/435 [00:08<00:01, 44.45it/s] 87%|████████▋ | 377/435 [00:08<00:01, 44.47it/s] 88%|████████▊ | 382/435 [00:08<00:01, 44.40it/s] 89%|████████▉ | 387/435 [00:08<00:01, 44.57it/s] 90%|█████████ | 392/435 [00:08<00:00, 44.62it/s] 91%|█████████▏| 397/435 [00:08<00:00, 44.47it/s] 92%|█████████▏| 402/435 [00:09<00:00, 43.06it/s] 94%|█████████▎| 407/435 [00:09<00:00, 43.48it/s] 95%|█████████▍| 412/435 [00:09<00:00, 43.82it/s] 96%|█████████▌| 417/435 [00:09<00:00, 44.07it/s] 97%|█████████▋| 422/435 [00:09<00:00, 44.35it/s] 98%|█████████▊| 427/435 [00:09<00:00, 44.41it/s] 99%|█████████▉| 432/435 [00:09<00:00, 44.55it/s]100%|██████████| 435/435 [00:09<00:00, 44.49it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:11:01,920 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:11:01,920 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:11:01,920 >>   eval_loss               =     1.0989
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:11:01,920 >>   eval_runtime            = 0:00:09.80
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:11:01,920 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:11:01,920 >>   eval_samples_per_second =    354.787
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:11:01,920 >>   eval_steps_per_second   =     44.387
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:11:01,920 >>   perplexity              =     3.0007
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:11:11,278 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:11:11,279 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:11:11,279 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:11:11,279 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:11:11,279 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:11:11,978 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:11:11,995 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:11:12,590 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:11:13,644 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:11:13,644 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:11:16,580 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:11:16,600 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:11:16,600 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:11:16,600 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:11:16,600 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:11:17,351 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:11:17,353 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:11:17,952 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:11:18,178 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:11:18,178 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-500
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-375
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-625
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-125
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-250
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:02,  1.41it/s]Extractor Predicting: 4it [00:02,  1.38it/s]Extractor Predicting: 5it [00:03,  1.37it/s]Extractor Predicting: 6it [00:04,  1.33it/s]Extractor Predicting: 7it [00:05,  1.31it/s]Extractor Predicting: 8it [00:05,  1.31it/s]Extractor Predicting: 9it [00:06,  1.32it/s]Extractor Predicting: 10it [00:07,  1.28it/s]Extractor Predicting: 11it [00:08,  1.28it/s]Extractor Predicting: 12it [00:09,  1.27it/s]Extractor Predicting: 13it [00:09,  1.29it/s]Extractor Predicting: 14it [00:10,  1.25it/s]Extractor Predicting: 15it [00:11,  1.27it/s]Extractor Predicting: 16it [00:12,  1.27it/s]Extractor Predicting: 17it [00:12,  1.30it/s]Extractor Predicting: 18it [00:13,  1.29it/s]Extractor Predicting: 19it [00:14,  1.31it/s]Extractor Predicting: 20it [00:15,  1.30it/s]Extractor Predicting: 21it [00:16,  1.31it/s]Extractor Predicting: 22it [00:16,  1.30it/s]Extractor Predicting: 23it [00:17,  1.32it/s]Extractor Predicting: 24it [00:18,  1.34it/s]Extractor Predicting: 25it [00:19,  1.30it/s]Extractor Predicting: 26it [00:19,  1.29it/s]Extractor Predicting: 27it [00:20,  1.30it/s]Extractor Predicting: 28it [00:21,  1.27it/s]Extractor Predicting: 29it [00:22,  1.24it/s]Extractor Predicting: 30it [00:23,  1.25it/s]Extractor Predicting: 31it [00:23,  1.25it/s]Extractor Predicting: 32it [00:24,  1.26it/s]Extractor Predicting: 33it [00:25,  1.23it/s]Extractor Predicting: 34it [00:26,  1.25it/s]Extractor Predicting: 35it [00:27,  1.28it/s]Extractor Predicting: 36it [00:27,  1.31it/s]Extractor Predicting: 37it [00:28,  1.30it/s]Extractor Predicting: 38it [00:29,  1.26it/s]Extractor Predicting: 39it [00:30,  1.27it/s]Extractor Predicting: 40it [00:30,  1.28it/s]Extractor Predicting: 41it [00:31,  1.30it/s]Extractor Predicting: 42it [00:32,  1.31it/s]Extractor Predicting: 43it [00:33,  1.29it/s]Extractor Predicting: 44it [00:34,  1.29it/s]Extractor Predicting: 45it [00:34,  1.26it/s]Extractor Predicting: 46it [00:35,  1.29it/s]Extractor Predicting: 47it [00:36,  1.30it/s]Extractor Predicting: 48it [00:37,  1.30it/s]Extractor Predicting: 49it [00:37,  1.31it/s]Extractor Predicting: 50it [00:38,  1.35it/s]Extractor Predicting: 51it [00:39,  1.34it/s]Extractor Predicting: 52it [00:40,  1.31it/s]Extractor Predicting: 53it [00:40,  1.30it/s]Extractor Predicting: 54it [00:41,  1.31it/s]Extractor Predicting: 55it [00:42,  1.30it/s]Extractor Predicting: 56it [00:43,  1.29it/s]Extractor Predicting: 57it [00:44,  1.28it/s]Extractor Predicting: 58it [00:44,  1.29it/s]Extractor Predicting: 59it [00:45,  1.33it/s]Extractor Predicting: 60it [00:46,  1.34it/s]Extractor Predicting: 61it [00:46,  1.34it/s]Extractor Predicting: 62it [00:47,  1.36it/s]Extractor Predicting: 63it [00:48,  1.34it/s]Extractor Predicting: 64it [00:49,  1.34it/s]Extractor Predicting: 65it [00:49,  1.34it/s]Extractor Predicting: 66it [00:50,  1.32it/s]Extractor Predicting: 67it [00:51,  1.33it/s]Extractor Predicting: 68it [00:52,  1.34it/s]Extractor Predicting: 69it [00:52,  1.33it/s]Extractor Predicting: 70it [00:53,  1.24it/s]Extractor Predicting: 71it [00:54,  1.25it/s]Extractor Predicting: 72it [00:55,  1.28it/s]Extractor Predicting: 73it [00:56,  1.28it/s]Extractor Predicting: 74it [00:56,  1.31it/s]Extractor Predicting: 75it [00:57,  1.34it/s]Extractor Predicting: 76it [00:58,  1.32it/s]Extractor Predicting: 77it [00:59,  1.30it/s]Extractor Predicting: 78it [00:59,  1.29it/s]Extractor Predicting: 79it [01:00,  1.28it/s]Extractor Predicting: 80it [01:01,  1.28it/s]Extractor Predicting: 81it [01:02,  1.26it/s]Extractor Predicting: 82it [01:03,  1.28it/s]Extractor Predicting: 83it [01:03,  1.29it/s]Extractor Predicting: 84it [01:04,  1.30it/s]Extractor Predicting: 85it [01:05,  1.30it/s]Extractor Predicting: 86it [01:06,  1.29it/s]Extractor Predicting: 87it [01:06,  1.29it/s]Extractor Predicting: 88it [01:07,  1.28it/s]Extractor Predicting: 89it [01:08,  1.31it/s]Extractor Predicting: 90it [01:09,  1.30it/s]Extractor Predicting: 91it [01:09,  1.33it/s]Extractor Predicting: 92it [01:10,  1.36it/s]Extractor Predicting: 93it [01:11,  1.37it/s]Extractor Predicting: 94it [01:12,  1.34it/s]Extractor Predicting: 95it [01:12,  1.36it/s]Extractor Predicting: 96it [01:13,  1.35it/s]Extractor Predicting: 97it [01:14,  1.35it/s]Extractor Predicting: 98it [01:15,  1.34it/s]Extractor Predicting: 99it [01:15,  1.31it/s]Extractor Predicting: 100it [01:16,  1.27it/s]Extractor Predicting: 101it [01:17,  1.28it/s]Extractor Predicting: 102it [01:18,  1.34it/s]Extractor Predicting: 103it [01:18,  1.36it/s]Extractor Predicting: 104it [01:19,  1.33it/s]Extractor Predicting: 105it [01:20,  1.33it/s]Extractor Predicting: 106it [01:21,  1.33it/s]Extractor Predicting: 107it [01:21,  1.35it/s]Extractor Predicting: 108it [01:22,  1.34it/s]Extractor Predicting: 109it [01:23,  1.33it/s]Extractor Predicting: 110it [01:24,  1.34it/s]Extractor Predicting: 111it [01:24,  1.35it/s]Extractor Predicting: 112it [01:25,  1.35it/s]Extractor Predicting: 113it [01:26,  1.38it/s]Extractor Predicting: 114it [01:27,  1.40it/s]Extractor Predicting: 115it [01:27,  1.38it/s]Extractor Predicting: 116it [01:28,  1.38it/s]Extractor Predicting: 117it [01:29,  1.38it/s]Extractor Predicting: 118it [01:30,  1.36it/s]Extractor Predicting: 119it [01:30,  1.36it/s]Extractor Predicting: 120it [01:31,  1.33it/s]Extractor Predicting: 121it [01:32,  1.30it/s]Extractor Predicting: 122it [01:33,  1.31it/s]Extractor Predicting: 123it [01:33,  1.31it/s]Extractor Predicting: 124it [01:34,  1.30it/s]Extractor Predicting: 125it [01:35,  1.30it/s]Extractor Predicting: 126it [01:36,  1.32it/s]Extractor Predicting: 127it [01:36,  1.30it/s]Extractor Predicting: 128it [01:37,  1.31it/s]Extractor Predicting: 129it [01:38,  1.31it/s]Extractor Predicting: 130it [01:39,  1.30it/s]Extractor Predicting: 131it [01:40,  1.30it/s]Extractor Predicting: 132it [01:40,  1.28it/s]Extractor Predicting: 133it [01:41,  1.31it/s]Extractor Predicting: 134it [01:42,  1.34it/s]Extractor Predicting: 135it [01:43,  1.31it/s]Extractor Predicting: 136it [01:43,  1.29it/s]Extractor Predicting: 137it [01:44,  1.32it/s]Extractor Predicting: 138it [01:45,  1.31it/s]Extractor Predicting: 139it [01:46,  1.29it/s]Extractor Predicting: 140it [01:46,  1.30it/s]Extractor Predicting: 141it [01:47,  1.32it/s]Extractor Predicting: 142it [01:48,  1.32it/s]Extractor Predicting: 143it [01:49,  1.34it/s]Extractor Predicting: 144it [01:49,  1.37it/s]Extractor Predicting: 144it [01:49,  1.31it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:13:20,583 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:13:20,604 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:13:20,604 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:13:20,604 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:13:20,604 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:13:21,350 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:13:21,351 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:13:21,951 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:13:23,038 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:13:23,038 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:13:25,973 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:13:25,999 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:13:25,999 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:13:25,999 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:13:25,999 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:13:26,840 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:13:26,841 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:13:27,492 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:13:27,722 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:13:27,722 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2777537796976242,
  "recall": 0.18492953695714698,
  "score": 0.22203038674033151,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.33it/s]Extractor Predicting: 2it [00:01,  1.29it/s]Extractor Predicting: 3it [00:02,  1.30it/s]Extractor Predicting: 4it [00:03,  1.31it/s]Extractor Predicting: 5it [00:03,  1.33it/s]Extractor Predicting: 6it [00:04,  1.34it/s]Extractor Predicting: 7it [00:05,  1.37it/s]Extractor Predicting: 8it [00:05,  1.41it/s]Extractor Predicting: 9it [00:06,  1.37it/s]Extractor Predicting: 10it [00:07,  1.35it/s]Extractor Predicting: 11it [00:08,  1.36it/s]Extractor Predicting: 12it [00:08,  1.35it/s]Extractor Predicting: 13it [00:09,  1.34it/s]Extractor Predicting: 14it [00:10,  1.35it/s]Extractor Predicting: 15it [00:11,  1.36it/s]Extractor Predicting: 16it [00:11,  1.35it/s]Extractor Predicting: 17it [00:12,  1.33it/s]Extractor Predicting: 18it [00:13,  1.36it/s]Extractor Predicting: 19it [00:14,  1.36it/s]Extractor Predicting: 20it [00:14,  1.33it/s]Extractor Predicting: 21it [00:15,  1.35it/s]Extractor Predicting: 22it [00:16,  1.37it/s]Extractor Predicting: 23it [00:17,  1.37it/s]Extractor Predicting: 24it [00:17,  1.35it/s]Extractor Predicting: 25it [00:18,  1.34it/s]Extractor Predicting: 26it [00:19,  1.36it/s]Extractor Predicting: 27it [00:20,  1.34it/s]Extractor Predicting: 28it [00:20,  1.33it/s]Extractor Predicting: 29it [00:21,  1.35it/s]Extractor Predicting: 30it [00:22,  1.37it/s]Extractor Predicting: 31it [00:22,  1.35it/s]Extractor Predicting: 32it [00:23,  1.40it/s]Extractor Predicting: 33it [00:24,  1.36it/s]Extractor Predicting: 34it [00:25,  1.34it/s]Extractor Predicting: 35it [00:25,  1.34it/s]Extractor Predicting: 36it [00:26,  1.35it/s]Extractor Predicting: 37it [00:27,  1.36it/s]Extractor Predicting: 38it [00:28,  1.37it/s]Extractor Predicting: 39it [00:28,  1.37it/s]Extractor Predicting: 40it [00:29,  1.27it/s]Extractor Predicting: 41it [00:30,  1.30it/s]Extractor Predicting: 42it [00:31,  1.34it/s]Extractor Predicting: 43it [00:31,  1.32it/s]Extractor Predicting: 44it [00:32,  1.33it/s]Extractor Predicting: 45it [00:33,  1.32it/s]Extractor Predicting: 46it [00:34,  1.29it/s]Extractor Predicting: 47it [00:34,  1.33it/s]Extractor Predicting: 48it [00:35,  1.33it/s]Extractor Predicting: 49it [00:36,  1.35it/s]Extractor Predicting: 50it [00:37,  1.35it/s]Extractor Predicting: 51it [00:37,  1.33it/s]Extractor Predicting: 52it [00:38,  1.31it/s]Extractor Predicting: 53it [00:39,  1.34it/s]Extractor Predicting: 54it [00:40,  1.31it/s]Extractor Predicting: 55it [00:40,  1.35it/s]Extractor Predicting: 56it [00:41,  1.34it/s]Extractor Predicting: 57it [00:42,  1.32it/s]Extractor Predicting: 58it [00:43,  1.30it/s]Extractor Predicting: 59it [00:44,  1.29it/s]Extractor Predicting: 60it [00:44,  1.30it/s]Extractor Predicting: 61it [00:45,  1.30it/s]Extractor Predicting: 62it [00:46,  1.31it/s]Extractor Predicting: 63it [00:47,  1.32it/s]Extractor Predicting: 64it [00:47,  1.30it/s]Extractor Predicting: 65it [00:48,  1.35it/s]Extractor Predicting: 66it [00:49,  1.32it/s]Extractor Predicting: 67it [00:50,  1.31it/s]Extractor Predicting: 68it [00:50,  1.28it/s]Extractor Predicting: 69it [00:51,  1.29it/s]Extractor Predicting: 70it [00:52,  1.27it/s]Extractor Predicting: 71it [00:53,  1.29it/s]Extractor Predicting: 72it [00:54,  1.28it/s]Extractor Predicting: 73it [00:54,  1.27it/s]Extractor Predicting: 74it [00:55,  1.30it/s]Extractor Predicting: 75it [00:56,  1.30it/s]Extractor Predicting: 76it [00:57,  1.30it/s]Extractor Predicting: 77it [00:57,  1.31it/s]Extractor Predicting: 78it [00:58,  1.33it/s]Extractor Predicting: 79it [00:59,  1.33it/s]Extractor Predicting: 80it [01:00,  1.38it/s]Extractor Predicting: 81it [01:00,  1.37it/s]Extractor Predicting: 82it [01:01,  1.37it/s]Extractor Predicting: 83it [01:02,  1.37it/s]Extractor Predicting: 84it [01:02,  1.36it/s]Extractor Predicting: 85it [01:03,  1.34it/s]Extractor Predicting: 86it [01:04,  1.28it/s]Extractor Predicting: 87it [01:05,  1.26it/s]Extractor Predicting: 88it [01:06,  1.29it/s]Extractor Predicting: 89it [01:06,  1.28it/s]Extractor Predicting: 90it [01:07,  1.24it/s]Extractor Predicting: 91it [01:08,  1.24it/s]Extractor Predicting: 92it [01:09,  1.25it/s]Extractor Predicting: 93it [01:10,  1.27it/s]Extractor Predicting: 94it [01:10,  1.27it/s]Extractor Predicting: 95it [01:11,  1.27it/s]Extractor Predicting: 96it [01:12,  1.27it/s]Extractor Predicting: 97it [01:13,  1.25it/s]Extractor Predicting: 98it [01:14,  1.23it/s]Extractor Predicting: 99it [01:15,  1.24it/s]Extractor Predicting: 100it [01:15,  1.25it/s]Extractor Predicting: 101it [01:16,  1.26it/s]Extractor Predicting: 102it [01:17,  1.28it/s]Extractor Predicting: 103it [01:18,  1.26it/s]Extractor Predicting: 104it [01:19,  1.23it/s]Extractor Predicting: 105it [01:19,  1.27it/s]Extractor Predicting: 106it [01:20,  1.26it/s]Extractor Predicting: 107it [01:21,  1.28it/s]Extractor Predicting: 108it [01:22,  1.29it/s]Extractor Predicting: 109it [01:22,  1.28it/s]Extractor Predicting: 110it [01:23,  1.27it/s]Extractor Predicting: 111it [01:24,  1.28it/s]Extractor Predicting: 112it [01:25,  1.27it/s]Extractor Predicting: 113it [01:25,  1.28it/s]Extractor Predicting: 114it [01:26,  1.30it/s]Extractor Predicting: 115it [01:27,  1.29it/s]Extractor Predicting: 116it [01:28,  1.25it/s]Extractor Predicting: 117it [01:29,  1.25it/s]Extractor Predicting: 118it [01:29,  1.27it/s]Extractor Predicting: 119it [01:30,  1.26it/s]Extractor Predicting: 120it [01:31,  1.25it/s]Extractor Predicting: 121it [01:32,  1.26it/s]Extractor Predicting: 122it [01:33,  1.29it/s]Extractor Predicting: 123it [01:33,  1.30it/s]Extractor Predicting: 124it [01:34,  1.30it/s]Extractor Predicting: 125it [01:35,  1.29it/s]Extractor Predicting: 126it [01:36,  1.35it/s]Extractor Predicting: 127it [01:36,  1.36it/s]Extractor Predicting: 128it [01:37,  1.33it/s]Extractor Predicting: 129it [01:38,  1.35it/s]Extractor Predicting: 130it [01:39,  1.34it/s]Extractor Predicting: 131it [01:39,  1.33it/s]Extractor Predicting: 132it [01:40,  1.34it/s]Extractor Predicting: 133it [01:41,  1.38it/s]Extractor Predicting: 134it [01:41,  1.38it/s]Extractor Predicting: 135it [01:42,  1.34it/s]Extractor Predicting: 136it [01:43,  1.35it/s]Extractor Predicting: 137it [01:44,  1.36it/s]Extractor Predicting: 138it [01:44,  1.40it/s]Extractor Predicting: 139it [01:45,  1.38it/s]Extractor Predicting: 140it [01:46,  1.36it/s]Extractor Predicting: 141it [01:47,  1.35it/s]Extractor Predicting: 142it [01:47,  1.36it/s]Extractor Predicting: 143it [01:48,  1.33it/s]Extractor Predicting: 144it [01:49,  1.31it/s]Extractor Predicting: 145it [01:50,  1.31it/s]Extractor Predicting: 146it [01:50,  1.34it/s]Extractor Predicting: 147it [01:51,  1.25it/s]Extractor Predicting: 148it [01:52,  1.29it/s]Extractor Predicting: 149it [01:53,  1.32it/s]Extractor Predicting: 150it [01:53,  1.37it/s]Extractor Predicting: 151it [01:54,  1.38it/s]Extractor Predicting: 152it [01:55,  1.40it/s]Extractor Predicting: 153it [01:56,  1.39it/s]Extractor Predicting: 154it [01:56,  1.42it/s]Extractor Predicting: 155it [01:57,  1.41it/s]Extractor Predicting: 156it [01:58,  1.43it/s]Extractor Predicting: 157it [01:58,  1.45it/s]Extractor Predicting: 158it [01:59,  1.42it/s]Extractor Predicting: 159it [02:00,  1.47it/s]Extractor Predicting: 160it [02:00,  1.52it/s]Extractor Predicting: 161it [02:01,  1.46it/s]Extractor Predicting: 162it [02:02,  1.42it/s]Extractor Predicting: 163it [02:02,  1.41it/s]Extractor Predicting: 164it [02:03,  1.42it/s]Extractor Predicting: 165it [02:04,  1.44it/s]Extractor Predicting: 166it [02:04,  1.46it/s]Extractor Predicting: 167it [02:05,  1.46it/s]Extractor Predicting: 168it [02:06,  1.41it/s]Extractor Predicting: 169it [02:07,  1.44it/s]Extractor Predicting: 170it [02:07,  1.43it/s]Extractor Predicting: 171it [02:08,  1.46it/s]Extractor Predicting: 172it [02:09,  1.47it/s]Extractor Predicting: 173it [02:09,  1.43it/s]Extractor Predicting: 174it [02:10,  1.37it/s]Extractor Predicting: 175it [02:11,  1.37it/s]Extractor Predicting: 176it [02:12,  1.35it/s]Extractor Predicting: 177it [02:12,  1.31it/s]Extractor Predicting: 178it [02:13,  1.28it/s]Extractor Predicting: 179it [02:14,  1.28it/s]Extractor Predicting: 180it [02:15,  1.31it/s]Extractor Predicting: 181it [02:16,  1.31it/s]Extractor Predicting: 182it [02:16,  1.28it/s]Extractor Predicting: 183it [02:17,  1.31it/s]Extractor Predicting: 184it [02:18,  1.31it/s]Extractor Predicting: 185it [02:19,  1.31it/s]Extractor Predicting: 186it [02:19,  1.31it/s]Extractor Predicting: 187it [02:20,  1.31it/s]Extractor Predicting: 188it [02:21,  1.29it/s]Extractor Predicting: 189it [02:22,  1.31it/s]Extractor Predicting: 190it [02:22,  1.28it/s]Extractor Predicting: 191it [02:23,  1.27it/s]Extractor Predicting: 192it [02:24,  1.27it/s]Extractor Predicting: 193it [02:25,  1.26it/s]Extractor Predicting: 194it [02:26,  1.25it/s]Extractor Predicting: 195it [02:26,  1.27it/s]Extractor Predicting: 196it [02:27,  1.30it/s]Extractor Predicting: 197it [02:28,  1.32it/s]Extractor Predicting: 198it [02:29,  1.29it/s]Extractor Predicting: 199it [02:29,  1.31it/s]Extractor Predicting: 200it [02:30,  1.28it/s]Extractor Predicting: 201it [02:31,  1.27it/s]Extractor Predicting: 202it [02:32,  1.23it/s]Extractor Predicting: 203it [02:33,  1.22it/s]Extractor Predicting: 204it [02:34,  1.21it/s]Extractor Predicting: 205it [02:34,  1.22it/s]Extractor Predicting: 206it [02:35,  1.22it/s]Extractor Predicting: 207it [02:36,  1.24it/s]Extractor Predicting: 208it [02:37,  1.23it/s]Extractor Predicting: 209it [02:38,  1.20it/s]Extractor Predicting: 210it [02:39,  1.19it/s]Extractor Predicting: 211it [02:39,  1.19it/s]Extractor Predicting: 212it [02:40,  1.22it/s]Extractor Predicting: 213it [02:41,  1.22it/s]Extractor Predicting: 214it [02:42,  1.23it/s]Extractor Predicting: 215it [02:43,  1.21it/s]Extractor Predicting: 216it [02:44,  1.21it/s]Extractor Predicting: 217it [02:44,  1.23it/s]Extractor Predicting: 218it [02:45,  1.22it/s]Extractor Predicting: 219it [02:46,  1.22it/s]Extractor Predicting: 220it [02:47,  1.18it/s]Extractor Predicting: 221it [02:48,  1.17it/s]Extractor Predicting: 222it [02:49,  1.17it/s]Extractor Predicting: 223it [02:49,  1.21it/s]Extractor Predicting: 224it [02:50,  1.22it/s]Extractor Predicting: 225it [02:51,  1.22it/s]Extractor Predicting: 226it [02:52,  1.21it/s]Extractor Predicting: 227it [02:53,  1.22it/s]Extractor Predicting: 228it [02:53,  1.23it/s]Extractor Predicting: 229it [02:54,  1.24it/s]Extractor Predicting: 230it [02:55,  1.26it/s]Extractor Predicting: 231it [02:56,  1.30it/s]Extractor Predicting: 232it [02:56,  1.34it/s]Extractor Predicting: 233it [02:57,  1.35it/s]Extractor Predicting: 234it [02:58,  1.30it/s]Extractor Predicting: 235it [02:59,  1.32it/s]Extractor Predicting: 236it [02:59,  1.32it/s]Extractor Predicting: 237it [03:00,  1.33it/s]Extractor Predicting: 238it [03:01,  1.33it/s]Extractor Predicting: 239it [03:02,  1.31it/s]Extractor Predicting: 240it [03:02,  1.34it/s]Extractor Predicting: 241it [03:03,  1.35it/s]Extractor Predicting: 242it [03:04,  1.38it/s]Extractor Predicting: 243it [03:05,  1.36it/s]Extractor Predicting: 244it [03:05,  1.41it/s]Extractor Predicting: 245it [03:06,  1.37it/s]Extractor Predicting: 246it [03:07,  1.35it/s]Extractor Predicting: 247it [03:08,  1.33it/s]Extractor Predicting: 248it [03:08,  1.30it/s]Extractor Predicting: 249it [03:09,  1.32it/s]Extractor Predicting: 250it [03:10,  1.32it/s]Extractor Predicting: 251it [03:11,  1.35it/s]Extractor Predicting: 252it [03:11,  1.36it/s]Extractor Predicting: 253it [03:12,  1.35it/s]Extractor Predicting: 254it [03:13,  1.32it/s]Extractor Predicting: 255it [03:14,  1.32it/s]Extractor Predicting: 256it [03:14,  1.30it/s]Extractor Predicting: 257it [03:15,  1.28it/s]Extractor Predicting: 258it [03:16,  1.17it/s]Extractor Predicting: 259it [03:17,  1.21it/s]Extractor Predicting: 260it [03:18,  1.20it/s]Extractor Predicting: 261it [03:19,  1.22it/s]Extractor Predicting: 262it [03:19,  1.25it/s]Extractor Predicting: 263it [03:20,  1.28it/s]Extractor Predicting: 264it [03:21,  1.26it/s]Extractor Predicting: 265it [03:22,  1.27it/s]Extractor Predicting: 266it [03:23,  1.26it/s]Extractor Predicting: 267it [03:23,  1.26it/s]Extractor Predicting: 268it [03:24,  1.24it/s]Extractor Predicting: 269it [03:25,  1.25it/s]Extractor Predicting: 270it [03:26,  1.26it/s]Extractor Predicting: 271it [03:26,  1.26it/s]Extractor Predicting: 272it [03:27,  1.29it/s]Extractor Predicting: 273it [03:28,  1.25it/s]Extractor Predicting: 274it [03:29,  1.24it/s]Extractor Predicting: 275it [03:30,  1.25it/s]Extractor Predicting: 276it [03:31,  1.24it/s]Extractor Predicting: 277it [03:31,  1.25it/s]Extractor Predicting: 278it [03:32,  1.26it/s]Extractor Predicting: 279it [03:33,  1.25it/s]Extractor Predicting: 280it [03:34,  1.25it/s]Extractor Predicting: 281it [03:34,  1.28it/s]Extractor Predicting: 282it [03:35,  1.27it/s]Extractor Predicting: 283it [03:36,  1.24it/s]Extractor Predicting: 284it [03:37,  1.28it/s]Extractor Predicting: 285it [03:38,  1.28it/s]Extractor Predicting: 286it [03:38,  1.30it/s]Extractor Predicting: 287it [03:39,  1.31it/s]Extractor Predicting: 288it [03:40,  1.28it/s]Extractor Predicting: 289it [03:41,  1.28it/s]Extractor Predicting: 290it [03:41,  1.28it/s]Extractor Predicting: 291it [03:42,  1.23it/s]Extractor Predicting: 292it [03:43,  1.21it/s]Extractor Predicting: 293it [03:44,  1.24it/s]Extractor Predicting: 294it [03:45,  1.22it/s]Extractor Predicting: 295it [03:46,  1.24it/s]Extractor Predicting: 296it [03:46,  1.24it/s]Extractor Predicting: 297it [03:47,  1.26it/s]Extractor Predicting: 298it [03:48,  1.25it/s]Extractor Predicting: 299it [03:49,  1.22it/s]Extractor Predicting: 300it [03:50,  1.25it/s]Extractor Predicting: 301it [03:50,  1.24it/s]Extractor Predicting: 302it [03:51,  1.27it/s]Extractor Predicting: 303it [03:52,  1.23it/s]Extractor Predicting: 304it [03:53,  1.23it/s]Extractor Predicting: 305it [03:54,  1.24it/s]Extractor Predicting: 306it [03:54,  1.27it/s]Extractor Predicting: 307it [03:55,  1.24it/s]Extractor Predicting: 308it [03:56,  1.27it/s]Extractor Predicting: 309it [03:57,  1.27it/s]Extractor Predicting: 310it [03:58,  1.29it/s]Extractor Predicting: 311it [03:58,  1.26it/s]Extractor Predicting: 312it [03:59,  1.31it/s]Extractor Predicting: 313it [04:00,  1.35it/s]Extractor Predicting: 314it [04:00,  1.37it/s]Extractor Predicting: 315it [04:01,  1.39it/s]Extractor Predicting: 316it [04:02,  1.34it/s]Extractor Predicting: 317it [04:03,  1.32it/s]Extractor Predicting: 318it [04:03,  1.31it/s]Extractor Predicting: 319it [04:04,  1.30it/s]Extractor Predicting: 320it [04:05,  1.28it/s]Extractor Predicting: 321it [04:06,  1.29it/s]Extractor Predicting: 322it [04:07,  1.30it/s]Extractor Predicting: 323it [04:07,  1.27it/s]Extractor Predicting: 324it [04:08,  1.25it/s]Extractor Predicting: 325it [04:09,  1.26it/s]Extractor Predicting: 326it [04:10,  1.28it/s]Extractor Predicting: 327it [04:11,  1.30it/s]Extractor Predicting: 328it [04:11,  1.28it/s]Extractor Predicting: 329it [04:12,  1.30it/s]Extractor Predicting: 330it [04:13,  1.30it/s]Extractor Predicting: 331it [04:14,  1.29it/s]Extractor Predicting: 332it [04:14,  1.27it/s]Extractor Predicting: 333it [04:15,  1.30it/s]Extractor Predicting: 334it [04:16,  1.30it/s]Extractor Predicting: 335it [04:17,  1.32it/s]Extractor Predicting: 336it [04:17,  1.29it/s]Extractor Predicting: 337it [04:18,  1.30it/s]Extractor Predicting: 338it [04:19,  1.29it/s]Extractor Predicting: 339it [04:20,  1.33it/s]Extractor Predicting: 340it [04:20,  1.34it/s]Extractor Predicting: 341it [04:21,  1.33it/s]Extractor Predicting: 342it [04:22,  1.32it/s]Extractor Predicting: 343it [04:23,  1.30it/s]Extractor Predicting: 344it [04:24,  1.32it/s]Extractor Predicting: 345it [04:24,  1.30it/s]Extractor Predicting: 346it [04:25,  1.30it/s]Extractor Predicting: 347it [04:26,  1.28it/s]Extractor Predicting: 348it [04:27,  1.28it/s]Extractor Predicting: 349it [04:27,  1.31it/s]Extractor Predicting: 350it [04:28,  1.32it/s]Extractor Predicting: 351it [04:29,  1.33it/s]Extractor Predicting: 352it [04:30,  1.30it/s]Extractor Predicting: 353it [04:30,  1.32it/s]Extractor Predicting: 354it [04:31,  1.35it/s]Extractor Predicting: 355it [04:32,  1.38it/s]Extractor Predicting: 356it [04:33,  1.33it/s]Extractor Predicting: 357it [04:33,  1.33it/s]Extractor Predicting: 358it [04:34,  1.35it/s]Extractor Predicting: 359it [04:35,  1.33it/s]Extractor Predicting: 360it [04:36,  1.33it/s]Extractor Predicting: 361it [04:36,  1.34it/s]Extractor Predicting: 362it [04:37,  1.21it/s]Extractor Predicting: 363it [04:38,  1.26it/s]Extractor Predicting: 364it [04:39,  1.28it/s]Extractor Predicting: 365it [04:40,  1.32it/s]Extractor Predicting: 366it [04:40,  1.31it/s]Extractor Predicting: 367it [04:41,  1.28it/s]Extractor Predicting: 368it [04:42,  1.27it/s]Extractor Predicting: 369it [04:43,  1.29it/s]Extractor Predicting: 370it [04:43,  1.33it/s]Extractor Predicting: 371it [04:44,  1.34it/s]Extractor Predicting: 372it [04:45,  1.34it/s]Extractor Predicting: 373it [04:46,  1.30it/s]Extractor Predicting: 374it [04:46,  1.32it/s]Extractor Predicting: 375it [04:47,  1.33it/s]Extractor Predicting: 376it [04:48,  1.34it/s]Extractor Predicting: 377it [04:49,  1.35it/s]Extractor Predicting: 378it [04:49,  1.38it/s]Extractor Predicting: 379it [04:50,  1.32it/s]Extractor Predicting: 380it [04:51,  1.33it/s]Extractor Predicting: 381it [04:52,  1.34it/s]Extractor Predicting: 382it [04:52,  1.34it/s]Extractor Predicting: 383it [04:53,  1.36it/s]Extractor Predicting: 384it [04:54,  1.38it/s]Extractor Predicting: 385it [04:55,  1.35it/s]Extractor Predicting: 386it [04:55,  1.36it/s]Extractor Predicting: 387it [04:56,  1.35it/s]Extractor Predicting: 388it [04:57,  1.33it/s]Extractor Predicting: 389it [04:58,  1.34it/s]Extractor Predicting: 390it [04:58,  1.33it/s]Extractor Predicting: 391it [04:59,  1.31it/s]Extractor Predicting: 392it [05:00,  1.32it/s]Extractor Predicting: 393it [05:01,  1.35it/s]Extractor Predicting: 394it [05:01,  1.29it/s]Extractor Predicting: 395it [05:02,  1.25it/s]Extractor Predicting: 396it [05:03,  1.23it/s]Extractor Predicting: 397it [05:04,  1.23it/s]Extractor Predicting: 398it [05:05,  1.24it/s]Extractor Predicting: 399it [05:06,  1.23it/s]Extractor Predicting: 400it [05:06,  1.26it/s]Extractor Predicting: 401it [05:07,  1.23it/s]Extractor Predicting: 402it [05:08,  1.22it/s]Extractor Predicting: 403it [05:09,  1.23it/s]Extractor Predicting: 404it [05:10,  1.17it/s]Extractor Predicting: 405it [05:11,  1.18it/s]Extractor Predicting: 406it [05:11,  1.19it/s]Extractor Predicting: 407it [05:12,  1.18it/s]Extractor Predicting: 408it [05:13,  1.20it/s]Extractor Predicting: 409it [05:14,  1.22it/s]Extractor Predicting: 410it [05:15,  1.20it/s]Extractor Predicting: 411it [05:15,  1.21it/s]Extractor Predicting: 412it [05:16,  1.22it/s]Extractor Predicting: 413it [05:17,  1.24it/s]Extractor Predicting: 414it [05:18,  1.27it/s]Extractor Predicting: 415it [05:19,  1.30it/s]Extractor Predicting: 416it [05:19,  1.29it/s]Extractor Predicting: 417it [05:20,  1.30it/s]Extractor Predicting: 418it [05:21,  1.27it/s]Extractor Predicting: 419it [05:22,  1.22it/s]Extractor Predicting: 420it [05:23,  1.24it/s]Extractor Predicting: 421it [05:23,  1.32it/s]Extractor Predicting: 421it [05:23,  1.30it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:19:03,988 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:19:04,009 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:19:04,009 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:19:04,009 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:19:04,009 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:19:04,813 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:19:04,815 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:19:05,441 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:19:06,542 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:19:06,542 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:19:09,518 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:19:09,532 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:19:09,532 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:19:09,532 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:19:09,533 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:19:10,262 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:19:10,263 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:19:10,882 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:19:11,102 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:19:11,102 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.17121341290139244,
  "recall": 0.11936602278355622,
  "score": 0.14066421525710618,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.21it/s]Extractor Predicting: 2it [00:01,  1.21it/s]Extractor Predicting: 3it [00:02,  1.21it/s]Extractor Predicting: 4it [00:03,  1.20it/s]Extractor Predicting: 5it [00:04,  1.24it/s]Extractor Predicting: 6it [00:04,  1.22it/s]Extractor Predicting: 7it [00:05,  1.21it/s]Extractor Predicting: 8it [00:06,  1.23it/s]Extractor Predicting: 9it [00:06,  1.61it/s]Extractor Predicting: 9it [00:06,  1.33it/s]
[INFO|configuration_utils.py:515] 2023-08-29 08:19:18,971 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:19:18,984 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 08:19:19,005 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:19:19,006 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 08:19:19,024 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 08:19:26,299 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 08:19:26,318 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 08:19:26,415 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:19:26,416 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 08:19:26,473 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:19:26,509 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:19:26,509 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:19:26,509 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:19:26,509 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:19:26,510 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:19:26,510 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.30708661417322836,
  "recall": 0.0962962962962963,
  "score": 0.14661654135338345,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 08:19:26,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:27,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:28,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:28,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:29,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:30,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:31,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:31,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:32,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:33,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:33,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:34,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:35,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:35,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:36,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:37,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:37,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:38,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:39,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:39,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:40,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:41,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:15<04:57, 15.65s/it][WARNING|generation_utils.py:914] 2023-08-29 08:19:42,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:43,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:44,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:44,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:45,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:46,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:46,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:47,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:48,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:48,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:49,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:50,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:51,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:51,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:52,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:52,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:53,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:54,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:54,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:55,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:56,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:57,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:58,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:32<04:51, 16.19s/it][WARNING|generation_utils.py:914] 2023-08-29 08:19:59,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:19:59,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:00,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:00,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:01,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:02,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:03,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:03,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:04,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:04,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:05,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:06,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:06,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:07,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:08,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:09,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:09,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:10,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:11,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:11,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:45<04:15, 15.06s/it][WARNING|generation_utils.py:914] 2023-08-29 08:20:12,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:13,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:14,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:15,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:15,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:16,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:17,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:17,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:18,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:19,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:20,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:20,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:21,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:22,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:23,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:23,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:24,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:25,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:26,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:26,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:27,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:01<04:02, 15.16s/it][WARNING|generation_utils.py:914] 2023-08-29 08:20:28,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:28,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:29,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:30,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:31,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:32,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:32,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:33,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:34,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:35,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:35,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:36,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:37,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:38,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:39,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:40,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:40,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:41,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:42,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:43,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:17<03:51, 15.43s/it][WARNING|generation_utils.py:914] 2023-08-29 08:20:44,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:44,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:45,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:46,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:47,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:47,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:48,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:49,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:49,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:50,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:51,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:52,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:52,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:53,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:53,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:54,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:55,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:55,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:56,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:57,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:30<03:27, 14.85s/it][WARNING|generation_utils.py:914] 2023-08-29 08:20:57,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:58,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:59,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:20:59,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:00,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:01,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:02,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:02,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:03,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:04,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:04,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:05,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:06,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:07,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:07,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:08,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:09,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:10,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:10,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:11,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:12,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:45<03:13, 14.88s/it][WARNING|generation_utils.py:914] 2023-08-29 08:21:12,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:13,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:14,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:14,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:15,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:16,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:17,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:18,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:18,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:19,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:20,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:21,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:22,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:23,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:24,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:24,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:25,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:26,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:27,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:28,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:29,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:02<03:07, 15.61s/it][WARNING|generation_utils.py:914] 2023-08-29 08:21:29,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:30,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:31,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:31,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:32,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:33,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:33,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:34,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:35,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:36,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:36,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:37,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:38,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:38,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:39,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:40,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:40,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:41,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:42,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:43,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:16<02:46, 15.10s/it][WARNING|generation_utils.py:914] 2023-08-29 08:21:43,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:44,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:45,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:45,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:46,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:47,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:48,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:48,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:49,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:50,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:50,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:51,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:52,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:53,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:53,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:54,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:55,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:55,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:56,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:57,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:31<02:28, 14.89s/it][WARNING|generation_utils.py:914] 2023-08-29 08:21:58,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:58,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:21:59,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:00,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:01,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:02,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:02,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:03,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:04,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:05,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:05,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:06,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:07,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:08,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:08,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:09,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:10,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:10,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:11,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:12,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:46<02:14, 14.97s/it][WARNING|generation_utils.py:914] 2023-08-29 08:22:13,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:14,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:14,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:15,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:16,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:16,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:17,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:18,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:19,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:19,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:20,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:21,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:21,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:22,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:23,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:24,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:24,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:25,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:26,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:26,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:00<01:58, 14.79s/it][WARNING|generation_utils.py:914] 2023-08-29 08:22:27,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:28,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:29,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:30,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:30,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:31,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:32,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:33,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:33,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:34,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:35,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:36,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:37,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:37,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:38,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:39,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:40,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:40,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:41,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:42,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:15<01:43, 14.83s/it][WARNING|generation_utils.py:914] 2023-08-29 08:22:42,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:43,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:44,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:44,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:45,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:45,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:46,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:47,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:47,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:48,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:48,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:49,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:49,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:50,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:51,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:51,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:52,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:52,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:53,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:54,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:28<01:24, 14.03s/it][WARNING|generation_utils.py:914] 2023-08-29 08:22:54,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:55,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:56,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:57,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:57,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:58,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:22:59,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:00,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:00,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:01,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:02,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:03,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:04,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:04,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:05,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:06,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:06,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:07,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:08,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:09,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:43<01:11, 14.33s/it][WARNING|generation_utils.py:914] 2023-08-29 08:23:09,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:10,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:11,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:12,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:12,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:13,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:14,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:15,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:16,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:16,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:17,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:18,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:18,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:19,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:20,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:21,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:21,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:22,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:23,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:24,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:57<00:57, 14.49s/it][WARNING|generation_utils.py:914] 2023-08-29 08:23:24,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:25,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:26,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:26,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:27,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:27,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:28,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:29,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:30,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:30,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:31,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:32,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:32,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:33,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:33,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:34,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:35,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:36,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:36,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:37,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:38,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:11<00:43, 14.33s/it][WARNING|generation_utils.py:914] 2023-08-29 08:23:38,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:39,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:40,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:40,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:41,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:42,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:42,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:43,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:44,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:45,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:45,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:46,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:47,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:47,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:48,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:49,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:50,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:50,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:51,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:52,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:52,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:26<00:29, 14.52s/it][WARNING|generation_utils.py:914] 2023-08-29 08:23:53,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:54,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:55,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:56,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:57,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:58,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:58,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:23:59,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:00,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:01,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:02,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:02,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:03,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:04,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:04,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:05,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:06,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:07,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:08,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:09,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:43<00:15, 15.22s/it][WARNING|generation_utils.py:914] 2023-08-29 08:24:10,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:11,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:12,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:12,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:13,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:14,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:14,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:15,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:16,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:17,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:17,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:18,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:19,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:20,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:21,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:21,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:22,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:23,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:23,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:24:24,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:59<00:00, 15.27s/it]Generating: 100%|██████████| 20/20 [04:59<00:00, 14.95s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:33,735 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:33,758 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:33,758 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:33,758 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:33,758 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:24:34,513 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:24:34,514 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:24:35,137 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:24:36,256 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:24:36,256 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:39,271 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:39,286 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:39,286 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:39,287 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:39,287 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:24:40,017 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:24:40,019 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:24:40,638 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:24:40,870 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:24:40,870 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 628, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8920454545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 628, 'raw': 736}
{'prompt': 'Relation : genre .', 'success_rate': 0.8532608695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 497, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 560, 'raw': 576}
{'target': 600, 'success': 592, 'raw': 608}
{'target': 600, 'success': 623, 'raw': 640}
{'prompt': 'Relation : head of government .', 'success_rate': 0.9734375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : military branch .', 'success_rate': 0.9360119047619048, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 312, 'raw': 320}
{'target': 600, 'success': 344, 'raw': 352}
{'target': 600, 'success': 376, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 616, 'raw': 640}
{'prompt': 'Relation : winner .', 'success_rate': 0.9625, 'errors': {''}}
['Relation : characters . Context : Later in the year , the character of the same name , played by actor John Cusack , had a character in " The Adventures of Krusty the Clown Prince " , a 1999 American animated film directed by Mark Wahlberg . Head Entity : The Adventures of Krusty the Clown prince , Tail Entity : Mark Wahlberg .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 221, 'raw': 224}
{'target': 600, 'success': 253, 'raw': 256}
{'target': 600, 'success': 285, 'raw': 288}
{'target': 600, 'success': 317, 'raw': 320}
{'target': 600, 'success': 348, 'raw': 352}
{'target': 600, 'success': 379, 'raw': 384}
{'target': 600, 'success': 409, 'raw': 416}
{'target': 600, 'success': 440, 'raw': 448}
{'target': 600, 'success': 472, 'raw': 480}
{'target': 600, 'success': 503, 'raw': 512}
{'target': 600, 'success': 534, 'raw': 544}
{'target': 600, 'success': 566, 'raw': 576}
{'target': 600, 'success': 598, 'raw': 608}
{'target': 600, 'success': 630, 'raw': 640}
{'prompt': 'Relation : characters .', 'success_rate': 0.984375, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.9285714285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : crosses .', 'success_rate': 0.9166666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 466, 'raw': 480}
{'target': 600, 'success': 497, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 586, 'raw': 608}
{'target': 600, 'success': 617, 'raw': 640}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.9640625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 554, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9609375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 282, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 468, 'raw': 480}
{'target': 600, 'success': 500, 'raw': 512}
{'target': 600, 'success': 532, 'raw': 544}
{'target': 600, 'success': 563, 'raw': 576}
{'target': 600, 'success': 594, 'raw': 608}
{'target': 600, 'success': 625, 'raw': 640}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9765625, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 313, 'raw': 320}
{'target': 600, 'success': 345, 'raw': 352}
{'target': 600, 'success': 377, 'raw': 384}
{'target': 600, 'success': 409, 'raw': 416}
{'target': 600, 'success': 441, 'raw': 448}
{'target': 600, 'success': 469, 'raw': 480}
{'target': 600, 'success': 501, 'raw': 512}
{'target': 600, 'success': 530, 'raw': 544}
{'target': 600, 'success': 561, 'raw': 576}
{'target': 600, 'success': 593, 'raw': 608}
{'target': 600, 'success': 623, 'raw': 640}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9734375, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : occupation .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.953125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : participant .', 'success_rate': 0.9578125, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 616, 'raw': 640}
{'prompt': 'Relation : participating team .', 'success_rate': 0.9625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.9196428571428571, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.9092261904761905, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : publisher .', 'success_rate': 0.9453125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : spouse . Context : Later in life , he married actress and actor , actress and musician , actress and novelist , actress and poet , novelist and novelist , actor , and actor , and actress , actress , and poet . Head Entity : actress and actress , Tail Entity : actress .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 313, 'raw': 320}
{'target': 600, 'success': 345, 'raw': 352}
{'target': 600, 'success': 376, 'raw': 384}
{'target': 600, 'success': 406, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 468, 'raw': 480}
{'target': 600, 'success': 498, 'raw': 512}
{'target': 600, 'success': 530, 'raw': 544}
{'target': 600, 'success': 560, 'raw': 576}
{'target': 600, 'success': 592, 'raw': 608}
{'target': 600, 'success': 621, 'raw': 640}
{'prompt': 'Relation : spouse .', 'success_rate': 0.9703125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/synthetic/4_ext.jsonl'}}
estimate vocab size: 8421
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8521, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.56it/s]Extractor Estimating: 2it [00:01,  1.39it/s]Extractor Estimating: 3it [00:01,  1.55it/s]Extractor Estimating: 4it [00:02,  1.63it/s]Extractor Estimating: 5it [00:03,  1.67it/s]Extractor Estimating: 6it [00:03,  1.55it/s]Extractor Estimating: 7it [00:04,  1.37it/s]Extractor Estimating: 8it [00:05,  1.39it/s]Extractor Estimating: 9it [00:06,  1.47it/s]Extractor Estimating: 10it [00:06,  1.53it/s]Extractor Estimating: 11it [00:07,  1.59it/s]Extractor Estimating: 12it [00:07,  1.64it/s]Extractor Estimating: 13it [00:08,  1.62it/s]Extractor Estimating: 14it [00:08,  1.66it/s]Extractor Estimating: 15it [00:09,  1.69it/s]Extractor Estimating: 16it [00:10,  1.63it/s]Extractor Estimating: 17it [00:10,  1.56it/s]Extractor Estimating: 18it [00:11,  1.57it/s]Extractor Estimating: 19it [00:12,  1.58it/s]Extractor Estimating: 20it [00:12,  1.56it/s]Extractor Estimating: 21it [00:13,  1.55it/s]Extractor Estimating: 22it [00:14,  1.51it/s]Extractor Estimating: 23it [00:14,  1.55it/s]Extractor Estimating: 24it [00:15,  1.59it/s]Extractor Estimating: 25it [00:15,  1.67it/s]Extractor Estimating: 26it [00:16,  1.50it/s]Extractor Estimating: 27it [00:17,  1.41it/s]Extractor Estimating: 28it [00:18,  1.38it/s]Extractor Estimating: 29it [00:19,  1.36it/s]Extractor Estimating: 30it [00:19,  1.36it/s]Extractor Estimating: 31it [00:20,  1.36it/s]Extractor Estimating: 32it [00:21,  1.33it/s]Extractor Estimating: 33it [00:22,  1.35it/s]Extractor Estimating: 34it [00:22,  1.35it/s]Extractor Estimating: 35it [00:23,  1.38it/s]Extractor Estimating: 36it [00:24,  1.36it/s]Extractor Estimating: 37it [00:24,  1.37it/s]Extractor Estimating: 38it [00:25,  1.36it/s]Extractor Estimating: 39it [00:26,  1.41it/s]Extractor Estimating: 40it [00:27,  1.40it/s]Extractor Estimating: 41it [00:27,  1.43it/s]Extractor Estimating: 42it [00:28,  1.46it/s]Extractor Estimating: 43it [00:29,  1.44it/s]Extractor Estimating: 44it [00:29,  1.44it/s]Extractor Estimating: 45it [00:30,  1.44it/s]Extractor Estimating: 46it [00:31,  1.37it/s]Extractor Estimating: 47it [00:32,  1.37it/s]Extractor Estimating: 48it [00:32,  1.34it/s]Extractor Estimating: 49it [00:33,  1.30it/s]Extractor Estimating: 50it [00:34,  1.31it/s]Extractor Estimating: 51it [00:35,  1.34it/s]Extractor Estimating: 52it [00:35,  1.40it/s]Extractor Estimating: 53it [00:36,  1.41it/s]Extractor Estimating: 54it [00:37,  1.46it/s]Extractor Estimating: 55it [00:37,  1.46it/s]Extractor Estimating: 56it [00:38,  1.44it/s]Extractor Estimating: 57it [00:39,  1.42it/s]Extractor Estimating: 58it [00:39,  1.46it/s]Extractor Estimating: 59it [00:40,  1.54it/s]Extractor Estimating: 60it [00:41,  1.52it/s]Extractor Estimating: 61it [00:41,  1.49it/s]Extractor Estimating: 62it [00:42,  1.49it/s]Extractor Estimating: 63it [00:43,  1.49it/s]Extractor Estimating: 64it [00:43,  1.49it/s]Extractor Estimating: 65it [00:44,  1.54it/s]Extractor Estimating: 66it [00:45,  1.55it/s]Extractor Estimating: 67it [00:45,  1.48it/s]Extractor Estimating: 68it [00:46,  1.49it/s]Extractor Estimating: 69it [00:47,  1.45it/s]Extractor Estimating: 70it [00:47,  1.48it/s]Extractor Estimating: 71it [00:48,  1.43it/s]Extractor Estimating: 72it [00:49,  1.46it/s]Extractor Estimating: 73it [00:49,  1.49it/s]Extractor Estimating: 74it [00:50,  1.48it/s]Extractor Estimating: 75it [00:51,  1.45it/s]Extractor Estimating: 76it [00:51,  1.41it/s]Extractor Estimating: 77it [00:52,  1.36it/s]Extractor Estimating: 78it [00:53,  1.36it/s]Extractor Estimating: 79it [00:54,  1.39it/s]Extractor Estimating: 80it [00:54,  1.38it/s]Extractor Estimating: 81it [00:55,  1.37it/s]Extractor Estimating: 82it [00:56,  1.35it/s]Extractor Estimating: 83it [00:57,  1.36it/s]Extractor Estimating: 84it [00:57,  1.35it/s]Extractor Estimating: 85it [00:58,  1.34it/s]Extractor Estimating: 86it [00:59,  1.32it/s]Extractor Estimating: 87it [01:00,  1.29it/s]Extractor Estimating: 88it [01:01,  1.30it/s]Extractor Estimating: 89it [01:01,  1.28it/s]Extractor Estimating: 90it [01:02,  1.32it/s]Extractor Estimating: 91it [01:03,  1.32it/s]Extractor Estimating: 92it [01:04,  1.31it/s]Extractor Estimating: 93it [01:04,  1.33it/s]Extractor Estimating: 94it [01:05,  1.36it/s]Extractor Estimating: 95it [01:06,  1.35it/s]Extractor Estimating: 96it [01:07,  1.34it/s]Extractor Estimating: 97it [01:07,  1.37it/s]Extractor Estimating: 98it [01:08,  1.35it/s]Extractor Estimating: 99it [01:09,  1.36it/s]Extractor Estimating: 100it [01:09,  1.34it/s]Extractor Estimating: 101it [01:10,  1.34it/s]Extractor Estimating: 102it [01:11,  1.25it/s]Extractor Estimating: 103it [01:12,  1.31it/s]Extractor Estimating: 104it [01:13,  1.34it/s]Extractor Estimating: 105it [01:13,  1.32it/s]Extractor Estimating: 106it [01:14,  1.35it/s]Extractor Estimating: 107it [01:15,  1.35it/s]Extractor Estimating: 108it [01:15,  1.38it/s]Extractor Estimating: 109it [01:16,  1.39it/s]Extractor Estimating: 110it [01:17,  1.39it/s]Extractor Estimating: 111it [01:18,  1.40it/s]Extractor Estimating: 112it [01:18,  1.42it/s]Extractor Estimating: 113it [01:19,  1.35it/s]Extractor Estimating: 114it [01:20,  1.30it/s]Extractor Estimating: 115it [01:21,  1.32it/s]Extractor Estimating: 116it [01:21,  1.34it/s]Extractor Estimating: 117it [01:22,  1.38it/s]Extractor Estimating: 118it [01:23,  1.34it/s]Extractor Estimating: 119it [01:24,  1.36it/s]Extractor Estimating: 120it [01:24,  1.37it/s]Extractor Estimating: 121it [01:25,  1.33it/s]Extractor Estimating: 122it [01:26,  1.31it/s]Extractor Estimating: 123it [01:27,  1.33it/s]Extractor Estimating: 124it [01:27,  1.33it/s]Extractor Estimating: 125it [01:28,  1.35it/s]Extractor Estimating: 126it [01:29,  1.34it/s]Extractor Estimating: 127it [01:30,  1.34it/s]Extractor Estimating: 128it [01:30,  1.35it/s]Extractor Estimating: 129it [01:31,  1.32it/s]Extractor Estimating: 130it [01:32,  1.32it/s]Extractor Estimating: 131it [01:33,  1.29it/s]Extractor Estimating: 132it [01:33,  1.33it/s]Extractor Estimating: 133it [01:34,  1.40it/s]Extractor Estimating: 134it [01:35,  1.41it/s]Extractor Estimating: 135it [01:35,  1.42it/s]Extractor Estimating: 136it [01:36,  1.45it/s]Extractor Estimating: 137it [01:37,  1.43it/s]Extractor Estimating: 138it [01:37,  1.43it/s]Extractor Estimating: 139it [01:38,  1.39it/s]Extractor Estimating: 140it [01:39,  1.43it/s]Extractor Estimating: 141it [01:40,  1.35it/s]Extractor Estimating: 142it [01:40,  1.37it/s]Extractor Estimating: 143it [01:41,  1.38it/s]Extractor Estimating: 144it [01:42,  1.42it/s]Extractor Estimating: 145it [01:43,  1.41it/s]Extractor Estimating: 146it [01:43,  1.44it/s]Extractor Estimating: 147it [01:44,  1.45it/s]Extractor Estimating: 148it [01:45,  1.42it/s]Extractor Estimating: 149it [01:45,  1.40it/s]Extractor Estimating: 150it [01:46,  1.37it/s]Extractor Estimating: 151it [01:47,  1.42it/s]Extractor Estimating: 152it [01:47,  1.52it/s]Extractor Estimating: 153it [01:48,  1.56it/s]Extractor Estimating: 154it [01:49,  1.55it/s]Extractor Estimating: 155it [01:49,  1.51it/s]Extractor Estimating: 156it [01:50,  1.53it/s]Extractor Estimating: 157it [01:51,  1.52it/s]Extractor Estimating: 158it [01:51,  1.59it/s]Extractor Estimating: 159it [01:52,  1.62it/s]Extractor Estimating: 160it [01:52,  1.63it/s]Extractor Estimating: 161it [01:53,  1.60it/s]Extractor Estimating: 162it [01:54,  1.53it/s]Extractor Estimating: 163it [01:54,  1.52it/s]Extractor Estimating: 164it [01:55,  1.52it/s]Extractor Estimating: 165it [01:56,  1.51it/s]Extractor Estimating: 166it [01:56,  1.51it/s]Extractor Estimating: 167it [01:57,  1.55it/s]Extractor Estimating: 168it [01:57,  1.62it/s]Extractor Estimating: 169it [01:58,  1.58it/s]Extractor Estimating: 170it [01:59,  1.63it/s]Extractor Estimating: 171it [01:59,  1.56it/s]Extractor Estimating: 172it [02:00,  1.58it/s]Extractor Estimating: 173it [02:01,  1.64it/s]Extractor Estimating: 174it [02:01,  1.65it/s]Extractor Estimating: 175it [02:02,  1.59it/s]Extractor Estimating: 176it [02:03,  1.48it/s]Extractor Estimating: 177it [02:03,  1.44it/s]Extractor Estimating: 178it [02:04,  1.43it/s]Extractor Estimating: 179it [02:05,  1.41it/s]Extractor Estimating: 180it [02:06,  1.36it/s]Extractor Estimating: 181it [02:06,  1.33it/s]Extractor Estimating: 182it [02:07,  1.36it/s]Extractor Estimating: 183it [02:08,  1.38it/s]Extractor Estimating: 184it [02:09,  1.40it/s]Extractor Estimating: 185it [02:09,  1.39it/s]Extractor Estimating: 186it [02:10,  1.37it/s]Extractor Estimating: 187it [02:11,  1.25it/s]Extractor Estimating: 188it [02:12,  1.33it/s]Extractor Estimating: 189it [02:12,  1.35it/s]Extractor Estimating: 190it [02:13,  1.34it/s]Extractor Estimating: 191it [02:14,  1.38it/s]Extractor Estimating: 192it [02:14,  1.37it/s]Extractor Estimating: 193it [02:15,  1.38it/s]Extractor Estimating: 194it [02:16,  1.30it/s]Extractor Estimating: 195it [02:17,  1.28it/s]Extractor Estimating: 196it [02:18,  1.31it/s]Extractor Estimating: 197it [02:18,  1.36it/s]Extractor Estimating: 198it [02:19,  1.33it/s]Extractor Estimating: 199it [02:20,  1.36it/s]Extractor Estimating: 200it [02:21,  1.34it/s]Extractor Estimating: 201it [02:21,  1.34it/s]Extractor Estimating: 202it [02:22,  1.31it/s]Extractor Estimating: 203it [02:23,  1.33it/s]Extractor Estimating: 204it [02:24,  1.32it/s]Extractor Estimating: 205it [02:24,  1.36it/s]Extractor Estimating: 206it [02:25,  1.40it/s]Extractor Estimating: 207it [02:26,  1.34it/s]Extractor Estimating: 208it [02:27,  1.33it/s]Extractor Estimating: 209it [02:27,  1.34it/s]Extractor Estimating: 210it [02:28,  1.38it/s]Extractor Estimating: 211it [02:29,  1.36it/s]Extractor Estimating: 212it [02:29,  1.39it/s]Extractor Estimating: 213it [02:30,  1.38it/s]Extractor Estimating: 214it [02:31,  1.39it/s]Extractor Estimating: 215it [02:32,  1.36it/s]Extractor Estimating: 216it [02:32,  1.33it/s]Extractor Estimating: 217it [02:33,  1.34it/s]Extractor Estimating: 218it [02:34,  1.32it/s]Extractor Estimating: 219it [02:35,  1.34it/s]Extractor Estimating: 220it [02:35,  1.34it/s]Extractor Estimating: 221it [02:36,  1.36it/s]Extractor Estimating: 222it [02:37,  1.35it/s]Extractor Estimating: 223it [02:38,  1.37it/s]Extractor Estimating: 224it [02:38,  1.36it/s]Extractor Estimating: 225it [02:39,  1.35it/s]Extractor Estimating: 226it [02:40,  1.39it/s]Extractor Estimating: 227it [02:40,  1.38it/s]Extractor Estimating: 228it [02:41,  1.35it/s]Extractor Estimating: 229it [02:42,  1.42it/s]Extractor Estimating: 230it [02:43,  1.44it/s]Extractor Estimating: 231it [02:43,  1.43it/s]Extractor Estimating: 232it [02:44,  1.49it/s]Extractor Estimating: 233it [02:45,  1.48it/s]Extractor Estimating: 234it [02:45,  1.55it/s]Extractor Estimating: 235it [02:46,  1.53it/s]Extractor Estimating: 236it [02:46,  1.50it/s]Extractor Estimating: 237it [02:47,  1.52it/s]Extractor Estimating: 238it [02:48,  1.52it/s]Extractor Estimating: 239it [02:48,  1.49it/s]Extractor Estimating: 240it [02:49,  1.51it/s]Extractor Estimating: 241it [02:50,  1.36it/s]Extractor Estimating: 242it [02:51,  1.43it/s]Extractor Estimating: 243it [02:51,  1.43it/s]Extractor Estimating: 244it [02:52,  1.46it/s]Extractor Estimating: 245it [02:53,  1.47it/s]Extractor Estimating: 246it [02:53,  1.45it/s]Extractor Estimating: 247it [02:54,  1.49it/s]Extractor Estimating: 248it [02:55,  1.48it/s]Extractor Estimating: 249it [02:55,  1.47it/s]Extractor Estimating: 250it [02:56,  1.47it/s]Extractor Estimating: 251it [02:57,  1.42it/s]Extractor Estimating: 252it [02:58,  1.39it/s]Extractor Estimating: 253it [02:58,  1.33it/s]Extractor Estimating: 254it [02:59,  1.28it/s]Extractor Estimating: 255it [03:00,  1.27it/s]Extractor Estimating: 256it [03:01,  1.31it/s]Extractor Estimating: 257it [03:02,  1.27it/s]Extractor Estimating: 258it [03:02,  1.26it/s]Extractor Estimating: 259it [03:03,  1.29it/s]Extractor Estimating: 260it [03:04,  1.30it/s]Extractor Estimating: 261it [03:05,  1.30it/s]Extractor Estimating: 262it [03:05,  1.29it/s]Extractor Estimating: 263it [03:06,  1.34it/s]Extractor Estimating: 264it [03:07,  1.32it/s]Extractor Estimating: 265it [03:08,  1.29it/s]Extractor Estimating: 266it [03:08,  1.32it/s]Extractor Estimating: 267it [03:09,  1.30it/s]Extractor Estimating: 268it [03:10,  1.31it/s]Extractor Estimating: 269it [03:11,  1.28it/s]Extractor Estimating: 270it [03:12,  1.27it/s]Extractor Estimating: 271it [03:12,  1.28it/s]Extractor Estimating: 272it [03:13,  1.30it/s]Extractor Estimating: 273it [03:14,  1.24it/s]Extractor Estimating: 274it [03:15,  1.26it/s]Extractor Estimating: 275it [03:16,  1.29it/s]Extractor Estimating: 276it [03:16,  1.34it/s]Extractor Estimating: 277it [03:17,  1.38it/s]Extractor Estimating: 278it [03:17,  1.44it/s]Extractor Estimating: 279it [03:18,  1.46it/s]Extractor Estimating: 280it [03:19,  1.46it/s]Extractor Estimating: 281it [03:20,  1.46it/s]Extractor Estimating: 282it [03:20,  1.47it/s]Extractor Estimating: 283it [03:21,  1.47it/s]Extractor Estimating: 284it [03:22,  1.44it/s]Extractor Estimating: 285it [03:22,  1.47it/s]Extractor Estimating: 286it [03:23,  1.48it/s]Extractor Estimating: 287it [03:24,  1.47it/s]Extractor Estimating: 288it [03:24,  1.47it/s]Extractor Estimating: 289it [03:25,  1.47it/s]Extractor Estimating: 290it [03:26,  1.48it/s]Extractor Estimating: 291it [03:27,  1.36it/s]Extractor Estimating: 292it [03:27,  1.41it/s]Extractor Estimating: 293it [03:28,  1.42it/s]Extractor Estimating: 294it [03:29,  1.42it/s]Extractor Estimating: 295it [03:29,  1.43it/s]Extractor Estimating: 296it [03:30,  1.45it/s]Extractor Estimating: 297it [03:31,  1.50it/s]Extractor Estimating: 298it [03:31,  1.51it/s]Extractor Estimating: 299it [03:32,  1.51it/s]Extractor Estimating: 300it [03:32,  1.51it/s]Extractor Estimating: 301it [03:33,  1.50it/s]Extractor Estimating: 302it [03:34,  1.48it/s]Extractor Estimating: 303it [03:35,  1.43it/s]Extractor Estimating: 304it [03:35,  1.42it/s]Extractor Estimating: 305it [03:36,  1.44it/s]Extractor Estimating: 306it [03:37,  1.46it/s]Extractor Estimating: 307it [03:37,  1.49it/s]Extractor Estimating: 308it [03:38,  1.45it/s]Extractor Estimating: 309it [03:39,  1.49it/s]Extractor Estimating: 310it [03:39,  1.49it/s]Extractor Estimating: 311it [03:40,  1.53it/s]Extractor Estimating: 312it [03:41,  1.50it/s]Extractor Estimating: 313it [03:41,  1.47it/s]Extractor Estimating: 314it [03:42,  1.45it/s]Extractor Estimating: 315it [03:43,  1.49it/s]Extractor Estimating: 316it [03:43,  1.47it/s]Extractor Estimating: 317it [03:44,  1.41it/s]Extractor Estimating: 318it [03:45,  1.42it/s]Extractor Estimating: 319it [03:46,  1.41it/s]Extractor Estimating: 320it [03:46,  1.45it/s]Extractor Estimating: 321it [03:47,  1.47it/s]Extractor Estimating: 322it [03:48,  1.48it/s]Extractor Estimating: 323it [03:48,  1.48it/s]Extractor Estimating: 324it [03:49,  1.53it/s]Extractor Estimating: 325it [03:50,  1.50it/s]Extractor Estimating: 326it [03:50,  1.60it/s]Extractor Estimating: 327it [03:51,  1.69it/s]Extractor Estimating: 328it [03:51,  1.64it/s]Extractor Estimating: 329it [03:52,  1.69it/s]Extractor Estimating: 330it [03:52,  1.66it/s]Extractor Estimating: 331it [03:53,  1.65it/s]Extractor Estimating: 332it [03:54,  1.68it/s]Extractor Estimating: 333it [03:54,  1.74it/s]Extractor Estimating: 334it [03:55,  1.76it/s]Extractor Estimating: 335it [03:55,  1.82it/s]Extractor Estimating: 336it [03:56,  1.78it/s]Extractor Estimating: 337it [03:56,  1.79it/s]Extractor Estimating: 338it [03:57,  1.81it/s]Extractor Estimating: 339it [03:57,  1.85it/s]Extractor Estimating: 340it [03:58,  1.74it/s]Extractor Estimating: 341it [03:59,  1.76it/s]Extractor Estimating: 342it [03:59,  1.77it/s]Extractor Estimating: 343it [04:00,  1.72it/s]Extractor Estimating: 344it [04:00,  1.71it/s]Extractor Estimating: 345it [04:01,  1.74it/s]Extractor Estimating: 346it [04:01,  1.75it/s]Extractor Estimating: 347it [04:02,  1.75it/s]Extractor Estimating: 348it [04:03,  1.75it/s]Extractor Estimating: 349it [04:03,  1.60it/s]Extractor Estimating: 350it [04:04,  1.56it/s]Extractor Estimating: 351it [04:05,  1.48it/s]Extractor Estimating: 352it [04:05,  1.48it/s]Extractor Estimating: 353it [04:06,  1.47it/s]Extractor Estimating: 354it [04:07,  1.49it/s]Extractor Estimating: 355it [04:08,  1.48it/s]Extractor Estimating: 356it [04:08,  1.45it/s]Extractor Estimating: 357it [04:09,  1.47it/s]Extractor Estimating: 358it [04:10,  1.45it/s]Extractor Estimating: 359it [04:10,  1.47it/s]Extractor Estimating: 360it [04:11,  1.40it/s]Extractor Estimating: 361it [04:12,  1.39it/s]Extractor Estimating: 362it [04:12,  1.42it/s]Extractor Estimating: 363it [04:13,  1.38it/s]Extractor Estimating: 364it [04:14,  1.36it/s]Extractor Estimating: 365it [04:15,  1.36it/s]Extractor Estimating: 366it [04:15,  1.40it/s]Extractor Estimating: 367it [04:16,  1.44it/s]Extractor Estimating: 368it [04:17,  1.39it/s]Extractor Estimating: 369it [04:18,  1.40it/s]Extractor Estimating: 370it [04:18,  1.43it/s]Extractor Estimating: 371it [04:19,  1.43it/s]Extractor Estimating: 372it [04:20,  1.43it/s]Extractor Estimating: 373it [04:20,  1.41it/s]Extractor Estimating: 374it [04:21,  1.41it/s]Extractor Estimating: 375it [04:22,  1.41it/s]Extractor Estimating: 376it [04:22,  1.43it/s]Extractor Estimating: 377it [04:23,  1.43it/s]Extractor Estimating: 378it [04:24,  1.40it/s]Extractor Estimating: 379it [04:25,  1.42it/s]Extractor Estimating: 380it [04:25,  1.45it/s]Extractor Estimating: 381it [04:26,  1.42it/s]Extractor Estimating: 382it [04:27,  1.39it/s]Extractor Estimating: 383it [04:27,  1.38it/s]Extractor Estimating: 384it [04:28,  1.40it/s]Extractor Estimating: 385it [04:29,  1.41it/s]Extractor Estimating: 386it [04:30,  1.41it/s]Extractor Estimating: 387it [04:30,  1.43it/s]Extractor Estimating: 388it [04:31,  1.42it/s]Extractor Estimating: 389it [04:32,  1.45it/s]Extractor Estimating: 390it [04:32,  1.44it/s]Extractor Estimating: 391it [04:33,  1.47it/s]Extractor Estimating: 392it [04:34,  1.45it/s]Extractor Estimating: 393it [04:34,  1.43it/s]Extractor Estimating: 394it [04:35,  1.43it/s]Extractor Estimating: 395it [04:36,  1.46it/s]Extractor Estimating: 396it [04:36,  1.45it/s]Extractor Estimating: 397it [04:37,  1.45it/s]Extractor Estimating: 398it [04:38,  1.29it/s]Extractor Estimating: 399it [04:39,  1.32it/s]Extractor Estimating: 400it [04:40,  1.32it/s]Extractor Estimating: 401it [04:40,  1.33it/s]Extractor Estimating: 402it [04:41,  1.39it/s]Extractor Estimating: 403it [04:42,  1.39it/s]Extractor Estimating: 404it [04:42,  1.44it/s]Extractor Estimating: 405it [04:43,  1.51it/s]Extractor Estimating: 406it [04:44,  1.50it/s]Extractor Estimating: 407it [04:44,  1.48it/s]Extractor Estimating: 408it [04:45,  1.42it/s]Extractor Estimating: 409it [04:46,  1.40it/s]Extractor Estimating: 410it [04:46,  1.44it/s]Extractor Estimating: 411it [04:47,  1.43it/s]Extractor Estimating: 412it [04:48,  1.40it/s]Extractor Estimating: 413it [04:49,  1.42it/s]Extractor Estimating: 414it [04:49,  1.49it/s]Extractor Estimating: 415it [04:50,  1.47it/s]Extractor Estimating: 416it [04:50,  1.49it/s]Extractor Estimating: 417it [04:51,  1.41it/s]Extractor Estimating: 418it [04:52,  1.40it/s]Extractor Estimating: 419it [04:53,  1.40it/s]Extractor Estimating: 420it [04:53,  1.43it/s]Extractor Estimating: 421it [04:54,  1.43it/s]Extractor Estimating: 422it [04:55,  1.45it/s]Extractor Estimating: 423it [04:55,  1.45it/s]Extractor Estimating: 424it [04:56,  1.44it/s]Extractor Estimating: 425it [04:57,  1.45it/s]Extractor Estimating: 426it [04:58,  1.45it/s]Extractor Estimating: 427it [04:58,  1.48it/s]Extractor Estimating: 428it [04:59,  1.50it/s]Extractor Estimating: 429it [05:00,  1.48it/s]Extractor Estimating: 430it [05:00,  1.49it/s]Extractor Estimating: 431it [05:01,  1.52it/s]Extractor Estimating: 432it [05:01,  1.51it/s]Extractor Estimating: 433it [05:02,  1.49it/s]Extractor Estimating: 434it [05:03,  1.47it/s]Extractor Estimating: 435it [05:03,  1.50it/s]Extractor Estimating: 436it [05:04,  1.44it/s]Extractor Estimating: 437it [05:05,  1.49it/s]Extractor Estimating: 438it [05:06,  1.49it/s]Extractor Estimating: 439it [05:06,  1.51it/s]Extractor Estimating: 440it [05:07,  1.57it/s]Extractor Estimating: 441it [05:07,  1.52it/s]Extractor Estimating: 442it [05:08,  1.48it/s]Extractor Estimating: 443it [05:09,  1.55it/s]Extractor Estimating: 444it [05:09,  1.56it/s]Extractor Estimating: 445it [05:10,  1.57it/s]Extractor Estimating: 446it [05:11,  1.53it/s]Extractor Estimating: 447it [05:11,  1.49it/s]Extractor Estimating: 448it [05:12,  1.50it/s]Extractor Estimating: 449it [05:13,  1.41it/s]Extractor Estimating: 450it [05:14,  1.36it/s]Extractor Estimating: 451it [05:14,  1.32it/s]Extractor Estimating: 452it [05:15,  1.33it/s]Extractor Estimating: 453it [05:16,  1.33it/s]Extractor Estimating: 454it [05:17,  1.29it/s]Extractor Estimating: 455it [05:18,  1.25it/s]Extractor Estimating: 456it [05:18,  1.26it/s]Extractor Estimating: 457it [05:19,  1.26it/s]Extractor Estimating: 458it [05:20,  1.28it/s]Extractor Estimating: 459it [05:21,  1.24it/s]Extractor Estimating: 460it [05:22,  1.28it/s]Extractor Estimating: 461it [05:22,  1.27it/s]Extractor Estimating: 462it [05:23,  1.27it/s]Extractor Estimating: 463it [05:24,  1.30it/s]Extractor Estimating: 464it [05:25,  1.34it/s]Extractor Estimating: 465it [05:25,  1.33it/s]Extractor Estimating: 466it [05:26,  1.28it/s]Extractor Estimating: 467it [05:27,  1.25it/s]Extractor Estimating: 468it [05:28,  1.30it/s]Extractor Estimating: 469it [05:29,  1.27it/s]Extractor Estimating: 470it [05:29,  1.27it/s]Extractor Estimating: 471it [05:30,  1.17it/s]Extractor Estimating: 472it [05:31,  1.21it/s]Extractor Estimating: 473it [05:32,  1.22it/s]Extractor Estimating: 474it [05:33,  1.29it/s]Extractor Estimating: 475it [05:33,  1.35it/s]Extractor Estimating: 476it [05:34,  1.39it/s]Extractor Estimating: 477it [05:35,  1.43it/s]Extractor Estimating: 478it [05:35,  1.42it/s]Extractor Estimating: 479it [05:36,  1.52it/s]Extractor Estimating: 480it [05:37,  1.52it/s]Extractor Estimating: 481it [05:37,  1.59it/s]Extractor Estimating: 482it [05:38,  1.58it/s]Extractor Estimating: 483it [05:38,  1.56it/s]Extractor Estimating: 484it [05:39,  1.54it/s]Extractor Estimating: 485it [05:40,  1.58it/s]Extractor Estimating: 486it [05:40,  1.54it/s]Extractor Estimating: 487it [05:41,  1.54it/s]Extractor Estimating: 488it [05:42,  1.59it/s]Extractor Estimating: 489it [05:42,  1.54it/s]Extractor Estimating: 490it [05:43,  1.54it/s]Extractor Estimating: 491it [05:44,  1.56it/s]Extractor Estimating: 492it [05:44,  1.61it/s]Extractor Estimating: 493it [05:45,  1.64it/s]Extractor Estimating: 494it [05:45,  1.62it/s]Extractor Estimating: 495it [05:46,  1.57it/s]Extractor Estimating: 496it [05:47,  1.54it/s]Extractor Estimating: 497it [05:47,  1.57it/s]Extractor Estimating: 498it [05:48,  1.56it/s]Extractor Estimating: 499it [05:48,  1.76it/s]Extractor Estimating: 499it [05:48,  1.43it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:45,605 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:45,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:45,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:45,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:45,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:30:46,337 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:30:46,338 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:30:47,043 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:30:48,159 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:30:48,159 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:51,182 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:51,218 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:51,218 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:51,218 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:51,218 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:30:52,025 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:30:52,027 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:30:52,655 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:30:52,878 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:30:52,878 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 11:52:00,970 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 11:52:01,232 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 9967 mean pseudo reward: 0.9704673957320334
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 16410
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16510, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16510, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.164, loss:410.1860
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.178, loss:346.9517
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.164, loss:343.5072
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.180, loss:321.4980
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.175, loss:290.3072
>> valid entity prec:0.5812, rec:0.6214, f1:0.6007
>> valid relation prec:0.2125, rec:0.1421, f1:0.1703
>> valid relation with NER prec:0.2125, rec:0.1421, f1:0.1703
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.678, loss:302.8494
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 1.180, loss:304.5664
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 1.175, loss:310.1176
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.152, loss:287.0583
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.191, loss:289.4345
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6074, rec:0.5980, f1:0.6027
>> valid relation prec:0.2251, rec:0.1369, f1:0.1702
>> valid relation with NER prec:0.2251, rec:0.1369, f1:0.1702
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 268, avg_time 2.661, loss:321.3411
g_step 1200, step 368, avg_time 1.177, loss:314.5952
g_step 1300, step 52, avg_time 1.173, loss:298.4350
g_step 1400, step 152, avg_time 1.171, loss:281.0565
g_step 1500, step 252, avg_time 1.164, loss:266.1515
>> valid entity prec:0.6199, rec:0.5741, f1:0.5961
>> valid relation prec:0.2002, rec:0.1343, f1:0.1608
>> valid relation with NER prec:0.2002, rec:0.1343, f1:0.1608
g_step 1600, step 352, avg_time 2.646, loss:291.4175
g_step 1700, step 36, avg_time 1.166, loss:275.9465
g_step 1800, step 136, avg_time 1.157, loss:242.8144
g_step 1900, step 236, avg_time 1.163, loss:279.1410
g_step 2000, step 336, avg_time 1.188, loss:272.9120
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6077, rec:0.5996, f1:0.6036
>> valid relation prec:0.2062, rec:0.1438, f1:0.1694
>> valid relation with NER prec:0.2062, rec:0.1438, f1:0.1694
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 20, avg_time 2.664, loss:266.6287
g_step 2200, step 120, avg_time 1.171, loss:237.8602
g_step 2300, step 220, avg_time 1.177, loss:252.6699
g_step 2400, step 320, avg_time 1.161, loss:253.2593
g_step 2500, step 4, avg_time 1.170, loss:248.5099
>> valid entity prec:0.5946, rec:0.6025, f1:0.5985
>> valid relation prec:0.1975, rec:0.1450, f1:0.1672
>> valid relation with NER prec:0.1975, rec:0.1450, f1:0.1672
g_step 2600, step 104, avg_time 2.631, loss:226.9764
g_step 2700, step 204, avg_time 1.193, loss:231.0167
g_step 2800, step 304, avg_time 1.167, loss:264.6150
g_step 2900, step 404, avg_time 1.168, loss:240.6822
g_step 3000, step 88, avg_time 1.176, loss:226.3793
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6201, rec:0.5801, f1:0.5994
>> valid relation prec:0.1808, rec:0.1211, f1:0.1450
>> valid relation with NER prec:0.1808, rec:0.1211, f1:0.1450
g_step 3100, step 188, avg_time 2.650, loss:219.8633
g_step 3200, step 288, avg_time 1.173, loss:241.0430
g_step 3300, step 388, avg_time 1.169, loss:252.2252
g_step 3400, step 72, avg_time 1.177, loss:210.2697
g_step 3500, step 172, avg_time 1.179, loss:221.9431
>> valid entity prec:0.5759, rec:0.6120, f1:0.5934
>> valid relation prec:0.2078, rec:0.1372, f1:0.1653
>> valid relation with NER prec:0.2078, rec:0.1372, f1:0.1653
g_step 3600, step 272, avg_time 2.660, loss:230.5507
g_step 3700, step 372, avg_time 1.139, loss:230.4971
g_step 3800, step 56, avg_time 1.163, loss:215.8032
g_step 3900, step 156, avg_time 1.170, loss:209.3355
g_step 4000, step 256, avg_time 1.178, loss:218.9007
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6003, rec:0.5928, f1:0.5965
>> valid relation prec:0.1975, rec:0.1349, f1:0.1603
>> valid relation with NER prec:0.1975, rec:0.1349, f1:0.1603
g_step 4100, step 356, avg_time 2.646, loss:217.8860
g_step 4200, step 40, avg_time 1.162, loss:216.3106
g_step 4300, step 140, avg_time 1.180, loss:197.3190
g_step 4400, step 240, avg_time 1.166, loss:205.2215
g_step 4500, step 340, avg_time 1.180, loss:199.4378
>> valid entity prec:0.5955, rec:0.6207, f1:0.6078
>> valid relation prec:0.2090, rec:0.1450, f1:0.1712
>> valid relation with NER prec:0.2090, rec:0.1450, f1:0.1712
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4600, step 24, avg_time 2.636, loss:188.2933
g_step 4700, step 124, avg_time 1.158, loss:188.3955
g_step 4800, step 224, avg_time 1.154, loss:197.1781
g_step 4900, step 324, avg_time 1.170, loss:202.1121
g_step 5000, step 8, avg_time 1.151, loss:202.1806
learning rate was adjusted to 0.0008
>> valid entity prec:0.5949, rec:0.5879, f1:0.5914
>> valid relation prec:0.1700, rec:0.1242, f1:0.1436
>> valid relation with NER prec:0.1700, rec:0.1242, f1:0.1436
g_step 5100, step 108, avg_time 2.638, loss:178.5804
g_step 5200, step 208, avg_time 1.146, loss:192.1651
g_step 5300, step 308, avg_time 1.155, loss:197.3829
g_step 5400, step 408, avg_time 1.165, loss:191.3464
g_step 5500, step 92, avg_time 1.147, loss:180.7904
>> valid entity prec:0.5965, rec:0.6044, f1:0.6004
>> valid relation prec:0.1971, rec:0.1444, f1:0.1667
>> valid relation with NER prec:0.1971, rec:0.1444, f1:0.1667
g_step 5600, step 192, avg_time 2.627, loss:183.1527
g_step 5700, step 292, avg_time 1.162, loss:181.6442
g_step 5800, step 392, avg_time 1.162, loss:203.9545
g_step 5900, step 76, avg_time 1.180, loss:181.3169
g_step 6000, step 176, avg_time 1.156, loss:163.2181
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.6168, rec:0.5859, f1:0.6010
>> valid relation prec:0.2003, rec:0.1464, f1:0.1692
>> valid relation with NER prec:0.2003, rec:0.1464, f1:0.1692
g_step 6100, step 276, avg_time 2.602, loss:192.4844
g_step 6200, step 376, avg_time 1.147, loss:166.4861
g_step 6300, step 60, avg_time 1.174, loss:160.2521
g_step 6400, step 160, avg_time 1.156, loss:169.2056
g_step 6500, step 260, avg_time 1.154, loss:168.6133
>> valid entity prec:0.5565, rec:0.5947, f1:0.5750
>> valid relation prec:0.1635, rec:0.1231, f1:0.1405
>> valid relation with NER prec:0.1635, rec:0.1231, f1:0.1405
g_step 6600, step 360, avg_time 2.626, loss:167.7567
g_step 6700, step 44, avg_time 1.154, loss:160.0098
g_step 6800, step 144, avg_time 1.165, loss:163.7087
g_step 6900, step 244, avg_time 1.170, loss:154.2455
g_step 7000, step 344, avg_time 1.158, loss:165.9857
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5897, rec:0.5970, f1:0.5933
>> valid relation prec:0.2000, rec:0.1487, f1:0.1706
>> valid relation with NER prec:0.2000, rec:0.1487, f1:0.1706
g_step 7100, step 28, avg_time 2.612, loss:159.4955
g_step 7200, step 128, avg_time 1.146, loss:146.0774
g_step 7300, step 228, avg_time 1.173, loss:163.7588
g_step 7400, step 328, avg_time 1.169, loss:166.3963
g_step 7500, step 12, avg_time 1.155, loss:155.4635
>> valid entity prec:0.5835, rec:0.5986, f1:0.5909
>> valid relation prec:0.1784, rec:0.1337, f1:0.1529
>> valid relation with NER prec:0.1784, rec:0.1337, f1:0.1529
g_step 7600, step 112, avg_time 2.631, loss:142.4331
g_step 7700, step 212, avg_time 1.169, loss:153.6897
g_step 7800, step 312, avg_time 1.174, loss:158.9611
g_step 7900, step 412, avg_time 1.149, loss:162.0595
g_step 8000, step 96, avg_time 1.171, loss:152.3584
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.6286, rec:0.5833, f1:0.6051
>> valid relation prec:0.1848, rec:0.1291, f1:0.1520
>> valid relation with NER prec:0.1848, rec:0.1291, f1:0.1520
g_step 8100, step 196, avg_time 2.612, loss:141.3974
g_step 8200, step 296, avg_time 1.178, loss:166.4423
g_step 8300, step 396, avg_time 1.170, loss:143.5780
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 11:52:01 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 11:52:01 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_11-52-00_ctolab14.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 11:52:02 - WARNING - datasets.builder -   Using custom data configuration default-8650455800eb5dcb
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-8650455800eb5dcb/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 11:52:04,482 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:52:04,515 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 11:52:04,515 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:52:04,516 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 11:52:04,625 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:52:04,683 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:52:04,684 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:52:04,684 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:52:04,684 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:52:04,684 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:52:04,684 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 11:52:05,060 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 11:52:08,074 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 11:52:08,074 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-8650455800eb5dcb/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  3.02ba/s] 20%|██        | 2/10 [00:00<00:01,  4.03ba/s] 30%|███       | 3/10 [00:00<00:01,  4.51ba/s] 40%|████      | 4/10 [00:01<00:01,  3.86ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.26ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.57ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.77ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.93ba/s] 90%|█████████ | 9/10 [00:01<00:00,  5.04ba/s]100%|██████████| 10/10 [00:02<00:00,  5.12ba/s]100%|██████████| 10/10 [00:02<00:00,  4.62ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.03ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.83ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.20ba/s]100%|██████████| 4/4 [00:00<00:00,  5.40ba/s]100%|██████████| 4/4 [00:00<00:00,  4.66ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:01,  5.30ba/s] 30%|███       | 3/10 [00:00<00:00,  8.62ba/s] 50%|█████     | 5/10 [00:00<00:00,  9.84ba/s] 70%|███████   | 7/10 [00:00<00:00, 10.41ba/s] 90%|█████████ | 9/10 [00:00<00:00, 10.69ba/s]100%|██████████| 10/10 [00:00<00:00, 10.11ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  5.37ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.71ba/s]100%|██████████| 4/4 [00:00<00:00,  9.76ba/s]
[INFO|trainer.py:414] 2023-08-29 11:52:13,708 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 11:52:13,785 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 11:52:13,785 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 11:52:13,785 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 11:52:13,785 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 11:52:13,785 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 11:52:13,785 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 11:52:13,786 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:58,  3.27it/s]  0%|          | 2/780 [00:00<03:49,  3.38it/s]  0%|          | 3/780 [00:00<03:47,  3.42it/s]  1%|          | 4/780 [00:01<03:45,  3.44it/s]  1%|          | 5/780 [00:01<03:44,  3.45it/s]  1%|          | 6/780 [00:01<03:44,  3.45it/s]  1%|          | 7/780 [00:02<03:43,  3.46it/s]  1%|          | 8/780 [00:02<03:43,  3.46it/s]  1%|          | 9/780 [00:02<03:42,  3.46it/s]  1%|▏         | 10/780 [00:02<03:42,  3.46it/s]  1%|▏         | 11/780 [00:03<03:42,  3.46it/s]  2%|▏         | 12/780 [00:03<03:41,  3.46it/s]  2%|▏         | 13/780 [00:03<03:41,  3.46it/s]  2%|▏         | 14/780 [00:04<03:41,  3.46it/s]  2%|▏         | 15/780 [00:04<03:40,  3.46it/s]  2%|▏         | 16/780 [00:04<03:40,  3.46it/s]  2%|▏         | 17/780 [00:04<03:40,  3.46it/s]  2%|▏         | 18/780 [00:05<03:39,  3.46it/s]  2%|▏         | 19/780 [00:05<03:39,  3.46it/s]  3%|▎         | 20/780 [00:05<03:39,  3.46it/s]  3%|▎         | 21/780 [00:06<03:39,  3.47it/s]  3%|▎         | 22/780 [00:06<03:38,  3.46it/s]  3%|▎         | 23/780 [00:06<03:38,  3.46it/s]  3%|▎         | 24/780 [00:06<03:38,  3.46it/s]  3%|▎         | 25/780 [00:07<03:37,  3.46it/s]  3%|▎         | 26/780 [00:07<03:37,  3.47it/s]  3%|▎         | 27/780 [00:07<03:37,  3.46it/s]  4%|▎         | 28/780 [00:08<03:37,  3.46it/s]  4%|▎         | 29/780 [00:08<03:36,  3.46it/s]  4%|▍         | 30/780 [00:08<03:36,  3.46it/s]  4%|▍         | 31/780 [00:08<03:36,  3.46it/s]  4%|▍         | 32/780 [00:09<03:42,  3.37it/s]  4%|▍         | 33/780 [00:09<03:39,  3.40it/s]  4%|▍         | 34/780 [00:09<03:38,  3.42it/s]  4%|▍         | 35/780 [00:10<03:37,  3.43it/s]  5%|▍         | 36/780 [00:10<03:36,  3.44it/s]  5%|▍         | 37/780 [00:10<03:35,  3.45it/s]  5%|▍         | 38/780 [00:11<03:35,  3.45it/s]  5%|▌         | 39/780 [00:11<03:34,  3.45it/s]  5%|▌         | 40/780 [00:11<03:34,  3.46it/s]  5%|▌         | 41/780 [00:11<03:33,  3.46it/s]  5%|▌         | 42/780 [00:12<03:33,  3.46it/s]  6%|▌         | 43/780 [00:12<03:33,  3.46it/s]  6%|▌         | 44/780 [00:12<03:32,  3.46it/s]  6%|▌         | 45/780 [00:13<03:32,  3.46it/s]  6%|▌         | 46/780 [00:13<03:31,  3.46it/s]  6%|▌         | 47/780 [00:13<03:31,  3.46it/s]  6%|▌         | 48/780 [00:13<03:31,  3.46it/s]  6%|▋         | 49/780 [00:14<03:30,  3.47it/s]  6%|▋         | 50/780 [00:14<03:30,  3.46it/s]  7%|▋         | 51/780 [00:14<03:30,  3.46it/s]  7%|▋         | 52/780 [00:15<03:30,  3.46it/s]  7%|▋         | 53/780 [00:15<03:30,  3.46it/s]  7%|▋         | 54/780 [00:15<03:29,  3.46it/s]  7%|▋         | 55/780 [00:15<03:29,  3.46it/s]  7%|▋         | 56/780 [00:16<03:29,  3.46it/s]  7%|▋         | 57/780 [00:16<03:29,  3.46it/s]  7%|▋         | 58/780 [00:16<03:28,  3.46it/s]  8%|▊         | 59/780 [00:17<03:28,  3.45it/s]  8%|▊         | 60/780 [00:17<03:28,  3.45it/s]  8%|▊         | 61/780 [00:17<03:28,  3.45it/s]  8%|▊         | 62/780 [00:17<03:28,  3.45it/s]  8%|▊         | 63/780 [00:18<03:27,  3.45it/s]  8%|▊         | 64/780 [00:18<03:27,  3.46it/s]  8%|▊         | 65/780 [00:18<03:26,  3.46it/s]  8%|▊         | 66/780 [00:19<03:26,  3.46it/s]  9%|▊         | 67/780 [00:19<03:26,  3.45it/s]  9%|▊         | 68/780 [00:19<03:25,  3.46it/s]  9%|▉         | 69/780 [00:19<03:25,  3.46it/s]  9%|▉         | 70/780 [00:20<03:25,  3.46it/s]  9%|▉         | 71/780 [00:20<03:24,  3.46it/s]  9%|▉         | 72/780 [00:20<03:24,  3.46it/s]  9%|▉         | 73/780 [00:21<03:24,  3.46it/s]  9%|▉         | 74/780 [00:21<03:24,  3.46it/s] 10%|▉         | 75/780 [00:21<03:23,  3.46it/s] 10%|▉         | 76/780 [00:22<03:23,  3.46it/s] 10%|▉         | 77/780 [00:22<03:23,  3.46it/s] 10%|█         | 78/780 [00:22<03:22,  3.46it/s] 10%|█         | 79/780 [00:22<03:22,  3.46it/s] 10%|█         | 80/780 [00:23<03:22,  3.46it/s] 10%|█         | 81/780 [00:23<03:22,  3.46it/s] 11%|█         | 82/780 [00:23<03:21,  3.46it/s] 11%|█         | 83/780 [00:24<03:21,  3.46it/s] 11%|█         | 84/780 [00:24<03:21,  3.46it/s] 11%|█         | 85/780 [00:24<03:21,  3.45it/s] 11%|█         | 86/780 [00:24<03:20,  3.45it/s] 11%|█         | 87/780 [00:25<03:20,  3.46it/s] 11%|█▏        | 88/780 [00:25<03:20,  3.45it/s] 11%|█▏        | 89/780 [00:25<03:20,  3.45it/s] 12%|█▏        | 90/780 [00:26<03:19,  3.46it/s] 12%|█▏        | 91/780 [00:26<03:19,  3.45it/s] 12%|█▏        | 92/780 [00:26<03:19,  3.46it/s] 12%|█▏        | 93/780 [00:26<03:18,  3.45it/s] 12%|█▏        | 94/780 [00:27<03:18,  3.46it/s] 12%|█▏        | 95/780 [00:27<03:18,  3.46it/s] 12%|█▏        | 96/780 [00:27<03:17,  3.46it/s] 12%|█▏        | 97/780 [00:28<03:17,  3.46it/s] 13%|█▎        | 98/780 [00:28<03:17,  3.46it/s] 13%|█▎        | 99/780 [00:28<03:17,  3.46it/s] 13%|█▎        | 100/780 [00:28<03:16,  3.46it/s] 13%|█▎        | 101/780 [00:29<03:16,  3.45it/s] 13%|█▎        | 102/780 [00:29<03:16,  3.45it/s] 13%|█▎        | 103/780 [00:29<03:16,  3.45it/s] 13%|█▎        | 104/780 [00:30<03:15,  3.45it/s] 13%|█▎        | 105/780 [00:30<03:15,  3.45it/s] 14%|█▎        | 106/780 [00:30<03:15,  3.45it/s] 14%|█▎        | 107/780 [00:30<03:14,  3.45it/s] 14%|█▍        | 108/780 [00:31<03:14,  3.45it/s] 14%|█▍        | 109/780 [00:31<03:14,  3.45it/s] 14%|█▍        | 110/780 [00:31<03:14,  3.45it/s] 14%|█▍        | 111/780 [00:32<03:13,  3.45it/s] 14%|█▍        | 112/780 [00:32<03:13,  3.45it/s] 14%|█▍        | 113/780 [00:32<03:13,  3.45it/s] 15%|█▍        | 114/780 [00:33<03:13,  3.45it/s] 15%|█▍        | 115/780 [00:33<03:12,  3.45it/s] 15%|█▍        | 116/780 [00:33<03:12,  3.45it/s] 15%|█▌        | 117/780 [00:33<03:12,  3.45it/s] 15%|█▌        | 118/780 [00:34<03:11,  3.45it/s] 15%|█▌        | 119/780 [00:34<03:11,  3.45it/s] 15%|█▌        | 120/780 [00:34<03:11,  3.45it/s] 16%|█▌        | 121/780 [00:35<03:11,  3.45it/s] 16%|█▌        | 122/780 [00:35<03:10,  3.45it/s] 16%|█▌        | 123/780 [00:35<03:10,  3.45it/s] 16%|█▌        | 124/780 [00:35<03:10,  3.45it/s] 16%|█▌        | 125/780 [00:36<03:09,  3.45it/s] 16%|█▌        | 126/780 [00:36<03:09,  3.45it/s] 16%|█▋        | 127/780 [00:36<03:08,  3.46it/s] 16%|█▋        | 128/780 [00:37<03:08,  3.46it/s] 17%|█▋        | 129/780 [00:37<03:08,  3.46it/s] 17%|█▋        | 130/780 [00:37<03:08,  3.45it/s] 17%|█▋        | 131/780 [00:37<03:07,  3.46it/s] 17%|█▋        | 132/780 [00:38<03:07,  3.45it/s] 17%|█▋        | 133/780 [00:38<03:07,  3.45it/s] 17%|█▋        | 134/780 [00:38<03:07,  3.45it/s] 17%|█▋        | 135/780 [00:39<03:06,  3.45it/s] 17%|█▋        | 136/780 [00:39<03:06,  3.45it/s] 18%|█▊        | 137/780 [00:39<03:06,  3.45it/s] 18%|█▊        | 138/780 [00:39<03:05,  3.45it/s] 18%|█▊        | 139/780 [00:40<03:05,  3.45it/s] 18%|█▊        | 140/780 [00:40<03:05,  3.45it/s] 18%|█▊        | 141/780 [00:40<03:05,  3.45it/s] 18%|█▊        | 142/780 [00:41<03:04,  3.45it/s] 18%|█▊        | 143/780 [00:41<03:04,  3.45it/s] 18%|█▊        | 144/780 [00:41<03:10,  3.33it/s] 19%|█▊        | 145/780 [00:42<03:08,  3.37it/s] 19%|█▊        | 146/780 [00:42<03:06,  3.39it/s] 19%|█▉        | 147/780 [00:42<03:05,  3.41it/s] 19%|█▉        | 148/780 [00:42<03:04,  3.43it/s] 19%|█▉        | 149/780 [00:43<03:03,  3.43it/s] 19%|█▉        | 150/780 [00:43<03:03,  3.44it/s] 19%|█▉        | 151/780 [00:43<03:02,  3.44it/s] 19%|█▉        | 152/780 [00:44<03:02,  3.45it/s] 20%|█▉        | 153/780 [00:44<03:01,  3.45it/s] 20%|█▉        | 154/780 [00:44<03:01,  3.45it/s] 20%|█▉        | 155/780 [00:44<03:07,  3.33it/s] 20%|██        | 156/780 [00:45<03:12,  3.24it/s][INFO|trainer.py:2140] 2023-08-29 11:52:59,107 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:52:59,108 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 11:52:59,108 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 55.69it/s][A
  3%|▎         | 12/435 [00:00<00:08, 48.52it/s][A
  4%|▍         | 17/435 [00:00<00:08, 46.78it/s][A
  5%|▌         | 22/435 [00:00<00:08, 45.99it/s][A
  6%|▌         | 27/435 [00:00<00:09, 45.31it/s][A
  7%|▋         | 32/435 [00:00<00:08, 45.05it/s][A
  9%|▊         | 37/435 [00:00<00:08, 44.69it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.19it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.27it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.47it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.59it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.69it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.76it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.51it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 44.36it/s][A
 19%|█▉        | 82/435 [00:01<00:07, 44.31it/s][A
 20%|██        | 87/435 [00:02<00:14, 24.16it/s][A
 21%|██        | 92/435 [00:02<00:12, 28.13it/s][A
 22%|██▏       | 96/435 [00:02<00:11, 30.34it/s][A
 23%|██▎       | 101/435 [00:02<00:09, 33.79it/s][A
 24%|██▍       | 106/435 [00:02<00:08, 36.57it/s][A
 26%|██▌       | 111/435 [00:02<00:08, 38.76it/s][A
 27%|██▋       | 116/435 [00:02<00:07, 40.48it/s][A
 28%|██▊       | 121/435 [00:03<00:07, 41.76it/s][A
 29%|██▉       | 126/435 [00:03<00:07, 42.23it/s][A
 30%|███       | 131/435 [00:03<00:07, 42.54it/s][A
 31%|███▏      | 136/435 [00:03<00:06, 42.95it/s][A
 32%|███▏      | 141/435 [00:03<00:06, 43.33it/s][A
 34%|███▎      | 146/435 [00:03<00:06, 43.76it/s][A
 35%|███▍      | 151/435 [00:03<00:06, 44.09it/s][A
 36%|███▌      | 156/435 [00:03<00:06, 44.32it/s][A
 37%|███▋      | 161/435 [00:03<00:06, 44.56it/s][A
 38%|███▊      | 166/435 [00:04<00:06, 44.57it/s][A
 39%|███▉      | 171/435 [00:04<00:05, 44.27it/s][A
 40%|████      | 176/435 [00:04<00:05, 44.11it/s][A
 42%|████▏     | 181/435 [00:04<00:05, 44.05it/s][A
 43%|████▎     | 186/435 [00:04<00:05, 44.18it/s][A
 44%|████▍     | 191/435 [00:04<00:05, 44.32it/s][A
 45%|████▌     | 196/435 [00:04<00:05, 44.52it/s][A
 46%|████▌     | 201/435 [00:04<00:05, 44.65it/s][A
 47%|████▋     | 206/435 [00:04<00:05, 44.68it/s][A
 49%|████▊     | 211/435 [00:05<00:05, 44.60it/s][A
 50%|████▉     | 216/435 [00:05<00:04, 44.38it/s][A
 51%|█████     | 221/435 [00:05<00:04, 44.15it/s][A
 52%|█████▏    | 226/435 [00:05<00:04, 44.04it/s][A
 53%|█████▎    | 231/435 [00:05<00:04, 44.17it/s][A
 54%|█████▍    | 236/435 [00:05<00:04, 44.39it/s][A
 55%|█████▌    | 241/435 [00:05<00:04, 44.57it/s][A
 57%|█████▋    | 246/435 [00:05<00:04, 43.07it/s][A
 58%|█████▊    | 251/435 [00:05<00:04, 43.61it/s][A
 59%|█████▉    | 256/435 [00:06<00:04, 43.87it/s][A
 60%|██████    | 261/435 [00:06<00:03, 43.87it/s][A
 61%|██████    | 266/435 [00:06<00:03, 43.86it/s][A
 62%|██████▏   | 271/435 [00:06<00:03, 43.76it/s][A
 63%|██████▎   | 276/435 [00:06<00:03, 43.91it/s][A
 65%|██████▍   | 281/435 [00:06<00:03, 44.25it/s][A
 66%|██████▌   | 286/435 [00:06<00:03, 44.33it/s][A
 67%|██████▋   | 291/435 [00:06<00:03, 44.46it/s][A
 68%|██████▊   | 296/435 [00:06<00:03, 44.53it/s][A
 69%|██████▉   | 301/435 [00:07<00:03, 44.56it/s][A
 70%|███████   | 306/435 [00:07<00:02, 44.48it/s][A
 71%|███████▏  | 311/435 [00:07<00:02, 44.23it/s][A
 73%|███████▎  | 316/435 [00:07<00:02, 44.07it/s][A
 74%|███████▍  | 321/435 [00:07<00:02, 44.09it/s][A
 75%|███████▍  | 326/435 [00:07<00:02, 44.21it/s][A
 76%|███████▌  | 331/435 [00:07<00:02, 44.35it/s][A
 77%|███████▋  | 336/435 [00:07<00:02, 44.56it/s][A
 78%|███████▊  | 341/435 [00:07<00:02, 44.60it/s][A
 80%|███████▉  | 346/435 [00:08<00:01, 44.61it/s][A
 81%|████████  | 351/435 [00:08<00:01, 44.35it/s][A
 82%|████████▏ | 356/435 [00:08<00:01, 44.24it/s][A
 83%|████████▎ | 361/435 [00:08<00:01, 44.13it/s][A
 84%|████████▍ | 366/435 [00:08<00:01, 44.14it/s][A
 85%|████████▌ | 371/435 [00:08<00:01, 44.35it/s][A
 86%|████████▋ | 376/435 [00:08<00:01, 44.42it/s][A
 88%|████████▊ | 381/435 [00:08<00:01, 43.24it/s][A
 89%|████████▊ | 386/435 [00:09<00:01, 43.77it/s][A
 90%|████████▉ | 391/435 [00:09<00:00, 44.05it/s][A
 91%|█████████ | 396/435 [00:09<00:00, 44.13it/s][A
 92%|█████████▏| 401/435 [00:09<00:00, 44.11it/s][A
 93%|█████████▎| 406/435 [00:09<00:00, 43.99it/s][A
 94%|█████████▍| 411/435 [00:09<00:00, 44.16it/s][A
 96%|█████████▌| 416/435 [00:09<00:00, 44.23it/s][A
 97%|█████████▋| 421/435 [00:09<00:00, 44.20it/s][A
 98%|█████████▊| 426/435 [00:09<00:00, 44.35it/s][A
 99%|█████████▉| 431/435 [00:10<00:00, 44.48it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:10<00:00, 44.48it/s][A 20%|██        | 156/780 [00:55<03:12,  3.24it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:53:09,499 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 11:53:09,666 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:53:13,011 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:53:13,166 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:53:13,240 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:06<1:08:30,  6.60s/it] 20%|██        | 158/780 [01:06<48:53,  4.72s/it]   20%|██        | 159/780 [01:07<35:05,  3.39s/it] 21%|██        | 160/780 [01:07<25:26,  2.46s/it] 21%|██        | 161/780 [01:07<18:42,  1.81s/it] 21%|██        | 162/780 [01:08<13:59,  1.36s/it] 21%|██        | 163/780 [01:08<10:41,  1.04s/it] 21%|██        | 164/780 [01:08<08:23,  1.22it/s] 21%|██        | 165/780 [01:08<06:46,  1.51it/s] 21%|██▏       | 166/780 [01:09<05:39,  1.81it/s] 21%|██▏       | 167/780 [01:09<04:52,  2.10it/s] 22%|██▏       | 168/780 [01:09<04:24,  2.32it/s] 22%|██▏       | 169/780 [01:10<03:59,  2.55it/s] 22%|██▏       | 170/780 [01:10<03:41,  2.75it/s] 22%|██▏       | 171/780 [01:10<03:29,  2.91it/s] 22%|██▏       | 172/780 [01:11<03:20,  3.03it/s] 22%|██▏       | 173/780 [01:11<03:14,  3.12it/s] 22%|██▏       | 174/780 [01:11<03:09,  3.19it/s] 22%|██▏       | 175/780 [01:11<03:06,  3.25it/s] 23%|██▎       | 176/780 [01:12<03:03,  3.28it/s] 23%|██▎       | 177/780 [01:12<03:02,  3.31it/s] 23%|██▎       | 178/780 [01:12<03:06,  3.23it/s] 23%|██▎       | 179/780 [01:13<03:04,  3.26it/s] 23%|██▎       | 180/780 [01:13<03:02,  3.29it/s] 23%|██▎       | 181/780 [01:13<03:00,  3.31it/s] 23%|██▎       | 182/780 [01:14<02:59,  3.32it/s] 23%|██▎       | 183/780 [01:14<02:59,  3.33it/s] 24%|██▎       | 184/780 [01:14<02:58,  3.34it/s] 24%|██▎       | 185/780 [01:14<02:57,  3.34it/s] 24%|██▍       | 186/780 [01:15<02:57,  3.35it/s] 24%|██▍       | 187/780 [01:15<02:56,  3.35it/s] 24%|██▍       | 188/780 [01:15<03:02,  3.24it/s] 24%|██▍       | 189/780 [01:16<03:00,  3.28it/s] 24%|██▍       | 190/780 [01:16<02:58,  3.30it/s] 24%|██▍       | 191/780 [01:16<02:57,  3.32it/s] 25%|██▍       | 192/780 [01:17<02:56,  3.33it/s] 25%|██▍       | 193/780 [01:17<02:55,  3.34it/s] 25%|██▍       | 194/780 [01:17<02:55,  3.35it/s] 25%|██▌       | 195/780 [01:17<02:54,  3.35it/s] 25%|██▌       | 196/780 [01:18<02:53,  3.36it/s] 25%|██▌       | 197/780 [01:18<02:53,  3.36it/s] 25%|██▌       | 198/780 [01:18<02:58,  3.27it/s] 26%|██▌       | 199/780 [01:19<02:56,  3.30it/s] 26%|██▌       | 200/780 [01:19<02:54,  3.32it/s] 26%|██▌       | 201/780 [01:19<02:53,  3.33it/s] 26%|██▌       | 202/780 [01:20<02:52,  3.34it/s] 26%|██▌       | 203/780 [01:20<02:52,  3.34it/s] 26%|██▌       | 204/780 [01:20<02:51,  3.35it/s] 26%|██▋       | 205/780 [01:20<02:51,  3.36it/s] 26%|██▋       | 206/780 [01:21<02:50,  3.36it/s] 27%|██▋       | 207/780 [01:21<02:50,  3.36it/s] 27%|██▋       | 208/780 [01:21<02:50,  3.36it/s] 27%|██▋       | 209/780 [01:22<02:54,  3.27it/s] 27%|██▋       | 210/780 [01:22<02:52,  3.30it/s] 27%|██▋       | 211/780 [01:22<02:51,  3.31it/s] 27%|██▋       | 212/780 [01:23<02:50,  3.32it/s] 27%|██▋       | 213/780 [01:23<02:50,  3.33it/s] 27%|██▋       | 214/780 [01:23<02:49,  3.34it/s] 28%|██▊       | 215/780 [01:23<02:49,  3.34it/s] 28%|██▊       | 216/780 [01:24<02:48,  3.34it/s] 28%|██▊       | 217/780 [01:24<02:48,  3.35it/s] 28%|██▊       | 218/780 [01:24<02:47,  3.35it/s] 28%|██▊       | 219/780 [01:25<02:52,  3.24it/s] 28%|██▊       | 220/780 [01:25<02:50,  3.28it/s] 28%|██▊       | 221/780 [01:25<02:49,  3.30it/s] 28%|██▊       | 222/780 [01:26<02:47,  3.32it/s] 29%|██▊       | 223/780 [01:26<02:46,  3.34it/s] 29%|██▊       | 224/780 [01:26<02:46,  3.35it/s] 29%|██▉       | 225/780 [01:26<02:45,  3.36it/s] 29%|██▉       | 226/780 [01:27<02:44,  3.36it/s] 29%|██▉       | 227/780 [01:27<02:44,  3.36it/s] 29%|██▉       | 228/780 [01:27<02:44,  3.36it/s] 29%|██▉       | 229/780 [01:28<02:47,  3.28it/s] 29%|██▉       | 230/780 [01:28<02:46,  3.31it/s] 30%|██▉       | 231/780 [01:28<02:45,  3.32it/s] 30%|██▉       | 232/780 [01:29<02:44,  3.34it/s] 30%|██▉       | 233/780 [01:29<02:43,  3.35it/s] 30%|███       | 234/780 [01:29<02:42,  3.35it/s] 30%|███       | 235/780 [01:29<02:42,  3.36it/s] 30%|███       | 236/780 [01:30<02:41,  3.36it/s] 30%|███       | 237/780 [01:30<02:41,  3.36it/s] 31%|███       | 238/780 [01:30<02:41,  3.36it/s] 31%|███       | 239/780 [01:31<02:40,  3.36it/s] 31%|███       | 240/780 [01:31<02:51,  3.15it/s] 31%|███       | 241/780 [01:31<02:47,  3.21it/s] 31%|███       | 242/780 [01:32<02:45,  3.25it/s] 31%|███       | 243/780 [01:32<02:43,  3.28it/s] 31%|███▏      | 244/780 [01:32<02:41,  3.32it/s] 31%|███▏      | 245/780 [01:33<02:39,  3.36it/s] 32%|███▏      | 246/780 [01:33<02:37,  3.39it/s] 32%|███▏      | 247/780 [01:33<02:36,  3.41it/s] 32%|███▏      | 248/780 [01:33<02:35,  3.42it/s] 32%|███▏      | 249/780 [01:34<02:34,  3.43it/s] 32%|███▏      | 250/780 [01:34<02:33,  3.44it/s] 32%|███▏      | 251/780 [01:34<02:43,  3.24it/s] 32%|███▏      | 252/780 [01:35<02:39,  3.30it/s] 32%|███▏      | 253/780 [01:35<02:37,  3.35it/s] 33%|███▎      | 254/780 [01:35<02:35,  3.37it/s] 33%|███▎      | 255/780 [01:35<02:34,  3.40it/s] 33%|███▎      | 256/780 [01:36<02:33,  3.41it/s] 33%|███▎      | 257/780 [01:36<02:32,  3.43it/s] 33%|███▎      | 258/780 [01:36<02:32,  3.43it/s] 33%|███▎      | 259/780 [01:37<02:31,  3.44it/s] 33%|███▎      | 260/780 [01:37<02:31,  3.44it/s] 33%|███▎      | 261/780 [01:37<02:30,  3.45it/s] 34%|███▎      | 262/780 [01:38<02:39,  3.24it/s] 34%|███▎      | 263/780 [01:38<02:36,  3.30it/s] 34%|███▍      | 264/780 [01:38<02:34,  3.34it/s] 34%|███▍      | 265/780 [01:38<02:32,  3.37it/s] 34%|███▍      | 266/780 [01:39<02:31,  3.40it/s] 34%|███▍      | 267/780 [01:39<02:30,  3.41it/s] 34%|███▍      | 268/780 [01:39<02:29,  3.42it/s] 34%|███▍      | 269/780 [01:40<02:29,  3.43it/s] 35%|███▍      | 270/780 [01:40<02:28,  3.43it/s] 35%|███▍      | 271/780 [01:40<02:27,  3.44it/s] 35%|███▍      | 272/780 [01:40<02:27,  3.44it/s] 35%|███▌      | 273/780 [01:41<02:35,  3.26it/s] 35%|███▌      | 274/780 [01:41<02:32,  3.32it/s] 35%|███▌      | 275/780 [01:41<02:30,  3.36it/s] 35%|███▌      | 276/780 [01:42<02:29,  3.38it/s] 36%|███▌      | 277/780 [01:42<02:27,  3.40it/s] 36%|███▌      | 278/780 [01:42<02:26,  3.42it/s] 36%|███▌      | 279/780 [01:43<02:26,  3.43it/s] 36%|███▌      | 280/780 [01:43<02:25,  3.43it/s] 36%|███▌      | 281/780 [01:43<02:25,  3.44it/s] 36%|███▌      | 282/780 [01:43<02:24,  3.44it/s] 36%|███▋      | 283/780 [01:44<02:24,  3.44it/s] 36%|███▋      | 284/780 [01:44<02:29,  3.33it/s] 37%|███▋      | 285/780 [01:44<02:27,  3.36it/s] 37%|███▋      | 286/780 [01:45<02:25,  3.39it/s] 37%|███▋      | 287/780 [01:45<02:30,  3.28it/s] 37%|███▋      | 288/780 [01:45<02:27,  3.33it/s] 37%|███▋      | 289/780 [01:46<02:25,  3.37it/s] 37%|███▋      | 290/780 [01:46<02:24,  3.40it/s] 37%|███▋      | 291/780 [01:46<02:23,  3.41it/s] 37%|███▋      | 292/780 [01:46<02:22,  3.42it/s] 38%|███▊      | 293/780 [01:47<02:22,  3.43it/s] 38%|███▊      | 294/780 [01:47<02:48,  2.89it/s] 38%|███▊      | 295/780 [01:47<02:46,  2.92it/s] 38%|███▊      | 296/780 [01:48<02:38,  3.06it/s] 38%|███▊      | 297/780 [01:48<02:32,  3.17it/s] 38%|███▊      | 298/780 [01:48<02:28,  3.25it/s] 38%|███▊      | 299/780 [01:49<02:25,  3.31it/s] 38%|███▊      | 300/780 [01:49<02:23,  3.35it/s] 39%|███▊      | 301/780 [01:49<02:25,  3.30it/s] 39%|███▊      | 302/780 [01:50<02:22,  3.34it/s] 39%|███▉      | 303/780 [01:50<02:21,  3.38it/s] 39%|███▉      | 304/780 [01:50<02:20,  3.40it/s] 39%|███▉      | 305/780 [01:50<02:19,  3.41it/s] 39%|███▉      | 306/780 [01:51<02:18,  3.42it/s] 39%|███▉      | 307/780 [01:51<02:17,  3.43it/s] 39%|███▉      | 308/780 [01:51<02:17,  3.44it/s] 40%|███▉      | 309/780 [01:52<02:16,  3.44it/s] 40%|███▉      | 310/780 [01:52<02:16,  3.44it/s] 40%|███▉      | 311/780 [01:52<02:16,  3.45it/s] 40%|████      | 312/780 [01:52<02:22,  3.29it/s][INFO|trainer.py:2140] 2023-08-29 11:54:06,808 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:54:06,808 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 11:54:06,808 >>   Batch size = 8
{'eval_loss': 1.1338765621185303, 'eval_runtime': 10.1607, 'eval_samples_per_second': 342.202, 'eval_steps_per_second': 42.812, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 55.59it/s][A
  3%|▎         | 12/435 [00:00<00:08, 48.74it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.05it/s][A
  5%|▌         | 22/435 [00:00<00:08, 46.26it/s][A
  6%|▌         | 27/435 [00:00<00:08, 45.34it/s][A
  7%|▋         | 32/435 [00:00<00:09, 44.55it/s][A
  9%|▊         | 37/435 [00:00<00:09, 44.18it/s][A
 10%|▉         | 42/435 [00:00<00:08, 43.94it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.14it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.29it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.53it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.66it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.79it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.77it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 44.49it/s][A
 19%|█▉        | 82/435 [00:01<00:08, 44.12it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.01it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.15it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.26it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.48it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.63it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.66it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 44.69it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 42.19it/s][A
 29%|██▉       | 127/435 [00:02<00:07, 42.73it/s][A
 30%|███       | 132/435 [00:02<00:07, 43.13it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 43.51it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 43.85it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 44.09it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.38it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.44it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 44.20it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 44.16it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.12it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.12it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.22it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.37it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 44.49it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 44.58it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 44.49it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 44.32it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 44.24it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 44.15it/s][A
 51%|█████     | 222/435 [00:04<00:04, 44.15it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.31it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.42it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.48it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.45it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.45it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 44.36it/s][A
 59%|█████▉    | 257/435 [00:05<00:04, 42.79it/s][A
 60%|██████    | 262/435 [00:05<00:04, 43.19it/s][A
 61%|██████▏   | 267/435 [00:06<00:03, 43.55it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 42.06it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 43.02it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 43.61it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 43.88it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.00it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 43.85it/s][A
 69%|██████▉   | 302/435 [00:06<00:03, 43.97it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.11it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.04it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.13it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.28it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.44it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.56it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.51it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 44.35it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 44.25it/s][A
 81%|████████  | 352/435 [00:07<00:01, 44.34it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 44.28it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 44.29it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.31it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 44.49it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 44.55it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 44.51it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 44.26it/s][A
 90%|█████████ | 392/435 [00:08<00:01, 40.73it/s][A
 91%|█████████▏| 397/435 [00:09<00:00, 41.93it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 42.61it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 43.23it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 43.71it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 44.07it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.25it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 44.17it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 43.77it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 43.77it/s][A 40%|████      | 312/780 [02:02<02:22,  3.29it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:54:17,012 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 11:54:17,281 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:54:20,606 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:54:20,732 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:54:20,826 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:14<52:29,  6.74s/it] 40%|████      | 314/780 [02:15<37:26,  4.82s/it] 40%|████      | 315/780 [02:15<26:50,  3.46s/it] 41%|████      | 316/780 [02:15<19:26,  2.51s/it] 41%|████      | 317/780 [02:15<14:15,  1.85s/it] 41%|████      | 318/780 [02:16<10:38,  1.38s/it] 41%|████      | 319/780 [02:16<08:07,  1.06s/it] 41%|████      | 320/780 [02:16<06:21,  1.21it/s] 41%|████      | 321/780 [02:17<05:07,  1.49it/s] 41%|████▏     | 322/780 [02:17<04:15,  1.79it/s] 41%|████▏     | 323/780 [02:17<03:39,  2.08it/s] 42%|████▏     | 324/780 [02:18<03:17,  2.31it/s] 42%|████▏     | 325/780 [02:18<02:58,  2.55it/s] 42%|████▏     | 326/780 [02:18<02:45,  2.75it/s] 42%|████▏     | 327/780 [02:18<02:35,  2.91it/s] 42%|████▏     | 328/780 [02:19<02:29,  3.03it/s] 42%|████▏     | 329/780 [02:19<02:24,  3.13it/s] 42%|████▏     | 330/780 [02:19<02:20,  3.20it/s] 42%|████▏     | 331/780 [02:20<02:18,  3.25it/s] 43%|████▎     | 332/780 [02:20<02:18,  3.23it/s] 43%|████▎     | 333/780 [02:20<02:16,  3.27it/s] 43%|████▎     | 334/780 [02:21<02:15,  3.29it/s] 43%|████▎     | 335/780 [02:21<02:14,  3.31it/s] 43%|████▎     | 336/780 [02:21<02:13,  3.33it/s] 43%|████▎     | 337/780 [02:21<02:12,  3.34it/s] 43%|████▎     | 338/780 [02:22<02:12,  3.35it/s] 43%|████▎     | 339/780 [02:22<02:11,  3.35it/s] 44%|████▎     | 340/780 [02:22<02:11,  3.35it/s] 44%|████▎     | 341/780 [02:23<02:11,  3.35it/s] 44%|████▍     | 342/780 [02:23<02:15,  3.23it/s] 44%|████▍     | 343/780 [02:23<02:13,  3.26it/s] 44%|████▍     | 344/780 [02:24<02:12,  3.29it/s] 44%|████▍     | 345/780 [02:24<02:11,  3.31it/s] 44%|████▍     | 346/780 [02:24<02:10,  3.32it/s] 44%|████▍     | 347/780 [02:24<02:09,  3.34it/s] 45%|████▍     | 348/780 [02:25<02:09,  3.34it/s] 45%|████▍     | 349/780 [02:25<02:08,  3.35it/s] 45%|████▍     | 350/780 [02:25<02:08,  3.35it/s] 45%|████▌     | 351/780 [02:26<02:08,  3.35it/s] 45%|████▌     | 352/780 [02:26<02:15,  3.16it/s] 45%|████▌     | 353/780 [02:26<02:12,  3.22it/s] 45%|████▌     | 354/780 [02:27<02:10,  3.26it/s] 46%|████▌     | 355/780 [02:27<02:09,  3.29it/s] 46%|████▌     | 356/780 [02:27<02:08,  3.31it/s] 46%|████▌     | 357/780 [02:28<02:07,  3.32it/s] 46%|████▌     | 358/780 [02:28<02:06,  3.33it/s] 46%|████▌     | 359/780 [02:28<02:06,  3.34it/s] 46%|████▌     | 360/780 [02:28<02:05,  3.34it/s] 46%|████▋     | 361/780 [02:29<02:05,  3.35it/s] 46%|████▋     | 362/780 [02:29<02:09,  3.22it/s] 47%|████▋     | 363/780 [02:29<02:07,  3.26it/s] 47%|████▋     | 364/780 [02:30<02:06,  3.29it/s] 47%|████▋     | 365/780 [02:30<02:05,  3.31it/s] 47%|████▋     | 366/780 [02:30<02:04,  3.32it/s] 47%|████▋     | 367/780 [02:31<02:03,  3.33it/s] 47%|████▋     | 368/780 [02:31<02:03,  3.34it/s] 47%|████▋     | 369/780 [02:31<02:02,  3.35it/s] 47%|████▋     | 370/780 [02:31<02:02,  3.35it/s] 48%|████▊     | 371/780 [02:32<02:02,  3.34it/s] 48%|████▊     | 372/780 [02:32<02:14,  3.04it/s] 48%|████▊     | 373/780 [02:32<02:10,  3.13it/s] 48%|████▊     | 374/780 [02:33<02:07,  3.19it/s] 48%|████▊     | 375/780 [02:33<02:05,  3.24it/s] 48%|████▊     | 376/780 [02:33<02:03,  3.27it/s] 48%|████▊     | 377/780 [02:34<02:02,  3.29it/s] 48%|████▊     | 378/780 [02:34<02:01,  3.31it/s] 49%|████▊     | 379/780 [02:34<02:00,  3.32it/s] 49%|████▊     | 380/780 [02:35<02:00,  3.33it/s] 49%|████▉     | 381/780 [02:35<01:59,  3.34it/s] 49%|████▉     | 382/780 [02:35<02:08,  3.10it/s] 49%|████▉     | 383/780 [02:35<02:05,  3.18it/s] 49%|████▉     | 384/780 [02:36<02:02,  3.23it/s] 49%|████▉     | 385/780 [02:36<02:00,  3.27it/s] 49%|████▉     | 386/780 [02:36<01:59,  3.29it/s] 50%|████▉     | 387/780 [02:37<01:58,  3.31it/s] 50%|████▉     | 388/780 [02:37<01:57,  3.33it/s] 50%|████▉     | 389/780 [02:37<01:57,  3.34it/s] 50%|█████     | 390/780 [02:38<01:56,  3.34it/s] 50%|█████     | 391/780 [02:38<01:56,  3.35it/s] 50%|█████     | 392/780 [02:38<02:05,  3.10it/s] 50%|█████     | 393/780 [02:39<02:01,  3.17it/s] 51%|█████     | 394/780 [02:39<01:59,  3.22it/s] 51%|█████     | 395/780 [02:39<01:58,  3.26it/s] 51%|█████     | 396/780 [02:39<01:56,  3.29it/s] 51%|█████     | 397/780 [02:40<01:55,  3.31it/s] 51%|█████     | 398/780 [02:40<01:54,  3.33it/s] 51%|█████     | 399/780 [02:40<01:54,  3.34it/s] 51%|█████▏    | 400/780 [02:41<01:53,  3.34it/s] 51%|█████▏    | 401/780 [02:41<01:53,  3.35it/s] 52%|█████▏    | 402/780 [02:41<01:58,  3.19it/s] 52%|█████▏    | 403/780 [02:42<01:56,  3.24it/s] 52%|█████▏    | 404/780 [02:42<01:54,  3.28it/s] 52%|█████▏    | 405/780 [02:42<01:53,  3.30it/s] 52%|█████▏    | 406/780 [02:42<01:52,  3.32it/s] 52%|█████▏    | 407/780 [02:43<01:52,  3.33it/s] 52%|█████▏    | 408/780 [02:43<01:51,  3.34it/s] 52%|█████▏    | 409/780 [02:43<01:50,  3.34it/s] 53%|█████▎    | 410/780 [02:44<01:50,  3.35it/s] 53%|█████▎    | 411/780 [02:44<01:50,  3.35it/s] 53%|█████▎    | 412/780 [02:44<01:53,  3.24it/s] 53%|█████▎    | 413/780 [02:45<01:52,  3.27it/s] 53%|█████▎    | 414/780 [02:45<01:50,  3.30it/s] 53%|█████▎    | 415/780 [02:45<01:54,  3.19it/s] 53%|█████▎    | 416/780 [02:46<01:52,  3.24it/s] 53%|█████▎    | 417/780 [02:46<01:50,  3.27it/s] 54%|█████▎    | 418/780 [02:46<01:49,  3.30it/s] 54%|█████▎    | 419/780 [02:46<01:48,  3.32it/s] 54%|█████▍    | 420/780 [02:47<01:48,  3.33it/s] 54%|█████▍    | 421/780 [02:47<02:18,  2.59it/s] 54%|█████▍    | 422/780 [02:48<02:11,  2.71it/s] 54%|█████▍    | 423/780 [02:48<02:04,  2.88it/s] 54%|█████▍    | 424/780 [02:48<01:58,  3.00it/s] 54%|█████▍    | 425/780 [02:49<01:54,  3.10it/s] 55%|█████▍    | 426/780 [02:49<01:51,  3.17it/s] 55%|█████▍    | 427/780 [02:49<01:49,  3.22it/s] 55%|█████▍    | 428/780 [02:49<01:47,  3.26it/s] 55%|█████▌    | 429/780 [02:50<01:46,  3.29it/s] 55%|█████▌    | 430/780 [02:50<01:45,  3.31it/s] 55%|█████▌    | 431/780 [02:50<01:44,  3.32it/s] 55%|█████▌    | 432/780 [02:51<01:48,  3.21it/s] 56%|█████▌    | 433/780 [02:51<01:46,  3.25it/s] 56%|█████▌    | 434/780 [02:51<01:45,  3.28it/s] 56%|█████▌    | 435/780 [02:52<01:44,  3.30it/s] 56%|█████▌    | 436/780 [02:52<01:43,  3.32it/s] 56%|█████▌    | 437/780 [02:52<01:43,  3.33it/s] 56%|█████▌    | 438/780 [02:52<01:42,  3.34it/s] 56%|█████▋    | 439/780 [02:53<01:42,  3.34it/s] 56%|█████▋    | 440/780 [02:53<01:41,  3.35it/s] 57%|█████▋    | 441/780 [02:53<01:41,  3.35it/s] 57%|█████▋    | 442/780 [02:54<01:40,  3.35it/s] 57%|█████▋    | 443/780 [02:54<01:44,  3.22it/s] 57%|█████▋    | 444/780 [02:54<01:43,  3.26it/s] 57%|█████▋    | 445/780 [02:55<01:41,  3.29it/s] 57%|█████▋    | 446/780 [02:55<01:41,  3.30it/s] 57%|█████▋    | 447/780 [02:55<01:40,  3.32it/s] 57%|█████▋    | 448/780 [02:55<01:39,  3.33it/s] 58%|█████▊    | 449/780 [02:56<01:38,  3.34it/s] 58%|█████▊    | 450/780 [02:56<01:38,  3.35it/s] 58%|█████▊    | 451/780 [02:56<01:38,  3.36it/s] 58%|█████▊    | 452/780 [02:57<01:37,  3.36it/s] 58%|█████▊    | 453/780 [02:57<01:40,  3.26it/s] 58%|█████▊    | 454/780 [02:57<01:39,  3.29it/s] 58%|█████▊    | 455/780 [02:58<01:38,  3.31it/s] 58%|█████▊    | 456/780 [02:58<01:37,  3.33it/s] 59%|█████▊    | 457/780 [02:58<01:36,  3.34it/s] 59%|█████▊    | 458/780 [02:58<01:36,  3.35it/s] 59%|█████▉    | 459/780 [02:59<01:35,  3.37it/s] 59%|█████▉    | 460/780 [02:59<01:34,  3.39it/s] 59%|█████▉    | 461/780 [02:59<01:33,  3.41it/s] 59%|█████▉    | 462/780 [03:00<01:32,  3.42it/s] 59%|█████▉    | 463/780 [03:00<01:32,  3.43it/s] 59%|█████▉    | 464/780 [03:00<01:35,  3.32it/s] 60%|█████▉    | 465/780 [03:01<01:33,  3.36it/s] 60%|█████▉    | 466/780 [03:01<01:32,  3.38it/s] 60%|█████▉    | 467/780 [03:01<01:31,  3.40it/s] 60%|██████    | 468/780 [03:01<01:31,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 11:55:15,728 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:55:15,728 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 11:55:15,728 >>   Batch size = 8
{'eval_loss': 1.1478512287139893, 'eval_runtime': 9.9048, 'eval_samples_per_second': 351.044, 'eval_steps_per_second': 43.918, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 55.65it/s][A
  3%|▎         | 12/435 [00:00<00:08, 48.71it/s][A
  4%|▍         | 17/435 [00:00<00:08, 46.63it/s][A
  5%|▌         | 22/435 [00:00<00:09, 45.76it/s][A
  6%|▌         | 27/435 [00:00<00:09, 45.11it/s][A
  7%|▋         | 32/435 [00:00<00:09, 44.76it/s][A
  9%|▊         | 37/435 [00:00<00:08, 44.55it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.33it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.49it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.67it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.85it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.80it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.65it/s][A
 17%|█▋        | 72/435 [00:01<00:11, 31.33it/s][A
 18%|█▊        | 77/435 [00:01<00:10, 34.50it/s][A
 19%|█▉        | 82/435 [00:01<00:09, 37.09it/s][A
 20%|██        | 87/435 [00:02<00:08, 39.22it/s][A
 21%|██        | 92/435 [00:02<00:08, 40.78it/s][A
 22%|██▏       | 97/435 [00:02<00:08, 41.93it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 42.82it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 43.33it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 43.29it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 43.26it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 43.39it/s][A
 29%|██▉       | 127/435 [00:02<00:07, 43.75it/s][A
 30%|███       | 132/435 [00:03<00:06, 44.12it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 44.37it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 44.56it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 44.66it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.54it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.26it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 43.98it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 43.97it/s][A
 40%|███▉      | 172/435 [00:04<00:05, 44.16it/s][A
 41%|████      | 177/435 [00:04<00:05, 44.27it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.49it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.66it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 44.73it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 44.52it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 41.79it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 42.43it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 42.95it/s][A
 50%|████▉     | 217/435 [00:05<00:05, 43.44it/s][A
 51%|█████     | 222/435 [00:05<00:04, 43.87it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.22it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.49it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.48it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.18it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.09it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 44.18it/s][A
 59%|█████▉    | 257/435 [00:05<00:04, 44.10it/s][A
 60%|██████    | 262/435 [00:06<00:03, 44.22it/s][A
 61%|██████▏   | 267/435 [00:06<00:03, 44.40it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.50it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.60it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.62it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.49it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.29it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.20it/s][A
 69%|██████▉   | 302/435 [00:06<00:03, 44.28it/s][A
 71%|███████   | 307/435 [00:07<00:02, 44.29it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.35it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.50it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.65it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.64it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.44it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 40.15it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 41.45it/s][A
 80%|███████▉  | 347/435 [00:07<00:02, 42.40it/s][A
 81%|████████  | 352/435 [00:08<00:01, 43.04it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 43.44it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 43.79it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.07it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 44.18it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 43.91it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 43.92it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 44.11it/s][A
 90%|█████████ | 392/435 [00:09<00:00, 44.26it/s][A
 91%|█████████▏| 397/435 [00:09<00:00, 44.38it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 44.52it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 44.44it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 44.50it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 44.38it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.20it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 43.99it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.17it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 44.17it/s][A 60%|██████    | 468/780 [03:11<01:31,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:55:25,954 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 11:55:26,101 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:55:29,630 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:55:29,783 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:55:29,857 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:24<36:23,  7.02s/it] 60%|██████    | 470/780 [03:24<25:55,  5.02s/it] 60%|██████    | 471/780 [03:25<18:33,  3.60s/it] 61%|██████    | 472/780 [03:25<13:24,  2.61s/it] 61%|██████    | 473/780 [03:25<09:48,  1.92s/it] 61%|██████    | 474/780 [03:26<07:18,  1.43s/it] 61%|██████    | 475/780 [03:26<05:33,  1.09s/it] 61%|██████    | 476/780 [03:26<04:19,  1.17it/s] 61%|██████    | 477/780 [03:27<03:28,  1.46it/s] 61%|██████▏   | 478/780 [03:27<02:52,  1.75it/s] 61%|██████▏   | 479/780 [03:27<02:26,  2.05it/s] 62%|██████▏   | 480/780 [03:28<02:15,  2.22it/s] 62%|██████▏   | 481/780 [03:28<02:00,  2.47it/s] 62%|██████▏   | 482/780 [03:28<01:51,  2.68it/s] 62%|██████▏   | 483/780 [03:28<01:44,  2.86it/s] 62%|██████▏   | 484/780 [03:29<01:39,  2.99it/s] 62%|██████▏   | 485/780 [03:29<01:35,  3.09it/s] 62%|██████▏   | 486/780 [03:29<01:32,  3.16it/s] 62%|██████▏   | 487/780 [03:30<01:31,  3.21it/s] 63%|██████▎   | 488/780 [03:30<01:29,  3.26it/s] 63%|██████▎   | 489/780 [03:30<01:28,  3.28it/s] 63%|██████▎   | 490/780 [03:31<01:31,  3.17it/s] 63%|██████▎   | 491/780 [03:31<01:29,  3.22it/s] 63%|██████▎   | 492/780 [03:31<01:28,  3.26it/s] 63%|██████▎   | 493/780 [03:31<01:27,  3.29it/s] 63%|██████▎   | 494/780 [03:32<01:26,  3.31it/s] 63%|██████▎   | 495/780 [03:32<01:25,  3.32it/s] 64%|██████▎   | 496/780 [03:32<01:25,  3.33it/s] 64%|██████▎   | 497/780 [03:33<01:24,  3.34it/s] 64%|██████▍   | 498/780 [03:33<01:24,  3.34it/s] 64%|██████▍   | 499/780 [03:33<01:24,  3.34it/s] 64%|██████▍   | 500/780 [03:34<01:30,  3.10it/s]                                                  64%|██████▍   | 500/780 [03:34<01:30,  3.10it/s] 64%|██████▍   | 501/780 [03:34<01:27,  3.18it/s] 64%|██████▍   | 502/780 [03:34<01:26,  3.23it/s] 64%|██████▍   | 503/780 [03:34<01:24,  3.27it/s] 65%|██████▍   | 504/780 [03:35<01:23,  3.30it/s] 65%|██████▍   | 505/780 [03:35<01:22,  3.32it/s] 65%|██████▍   | 506/780 [03:35<01:22,  3.33it/s] 65%|██████▌   | 507/780 [03:36<01:21,  3.34it/s] 65%|██████▌   | 508/780 [03:36<01:21,  3.35it/s] 65%|██████▌   | 509/780 [03:36<01:20,  3.35it/s] 65%|██████▌   | 510/780 [03:37<01:25,  3.15it/s] 66%|██████▌   | 511/780 [03:37<01:23,  3.21it/s] 66%|██████▌   | 512/780 [03:37<01:22,  3.25it/s] 66%|██████▌   | 513/780 [03:38<01:21,  3.29it/s] 66%|██████▌   | 514/780 [03:38<01:20,  3.31it/s] 66%|██████▌   | 515/780 [03:38<01:19,  3.33it/s] 66%|██████▌   | 516/780 [03:38<01:19,  3.34it/s] 66%|██████▋   | 517/780 [03:39<01:18,  3.34it/s] 66%|██████▋   | 518/780 [03:39<01:18,  3.34it/s] 67%|██████▋   | 519/780 [03:39<01:17,  3.35it/s] 67%|██████▋   | 520/780 [03:40<01:27,  2.96it/s] 67%|██████▋   | 521/780 [03:40<01:24,  3.07it/s] 67%|██████▋   | 522/780 [03:40<01:21,  3.15it/s] 67%|██████▋   | 523/780 [03:41<01:20,  3.20it/s] 67%|██████▋   | 524/780 [03:41<01:18,  3.25it/s] 67%|██████▋   | 525/780 [03:41<01:17,  3.28it/s] 67%|██████▋   | 526/780 [03:42<01:17,  3.30it/s] 68%|██████▊   | 527/780 [03:42<01:16,  3.31it/s] 68%|██████▊   | 528/780 [03:42<01:15,  3.32it/s] 68%|██████▊   | 529/780 [03:42<01:15,  3.33it/s] 68%|██████▊   | 530/780 [03:43<01:19,  3.13it/s] 68%|██████▊   | 531/780 [03:43<01:17,  3.19it/s] 68%|██████▊   | 532/780 [03:43<01:16,  3.24it/s] 68%|██████▊   | 533/780 [03:44<01:15,  3.27it/s] 68%|██████▊   | 534/780 [03:44<01:14,  3.29it/s] 69%|██████▊   | 535/780 [03:44<01:14,  3.31it/s] 69%|██████▊   | 536/780 [03:45<01:13,  3.32it/s] 69%|██████▉   | 537/780 [03:45<01:12,  3.33it/s] 69%|██████▉   | 538/780 [03:45<01:12,  3.34it/s] 69%|██████▉   | 539/780 [03:46<01:14,  3.22it/s] 69%|██████▉   | 540/780 [03:46<01:16,  3.14it/s] 69%|██████▉   | 541/780 [03:46<01:14,  3.20it/s] 69%|██████▉   | 542/780 [03:46<01:13,  3.25it/s] 70%|██████▉   | 543/780 [03:47<01:12,  3.28it/s] 70%|██████▉   | 544/780 [03:47<01:32,  2.54it/s] 70%|██████▉   | 545/780 [03:48<01:25,  2.74it/s] 70%|███████   | 546/780 [03:48<01:20,  2.90it/s] 70%|███████   | 547/780 [03:48<01:16,  3.03it/s] 70%|███████   | 548/780 [03:49<01:14,  3.13it/s] 70%|███████   | 549/780 [03:49<01:13,  3.13it/s] 71%|███████   | 550/780 [03:49<01:11,  3.20it/s] 71%|███████   | 551/780 [03:49<01:10,  3.25it/s] 71%|███████   | 552/780 [03:50<01:09,  3.28it/s] 71%|███████   | 553/780 [03:50<01:08,  3.30it/s] 71%|███████   | 554/780 [03:50<01:08,  3.32it/s] 71%|███████   | 555/780 [03:51<01:07,  3.33it/s] 71%|███████▏  | 556/780 [03:51<01:07,  3.34it/s] 71%|███████▏  | 557/780 [03:51<01:06,  3.35it/s] 72%|███████▏  | 558/780 [03:52<01:06,  3.35it/s] 72%|███████▏  | 559/780 [03:52<01:07,  3.28it/s] 72%|███████▏  | 560/780 [03:52<01:06,  3.30it/s] 72%|███████▏  | 561/780 [03:52<01:06,  3.32it/s] 72%|███████▏  | 562/780 [03:53<01:05,  3.33it/s] 72%|███████▏  | 563/780 [03:53<01:05,  3.33it/s] 72%|███████▏  | 564/780 [03:53<01:04,  3.34it/s] 72%|███████▏  | 565/780 [03:54<01:04,  3.35it/s] 73%|███████▎  | 566/780 [03:54<01:03,  3.35it/s] 73%|███████▎  | 567/780 [03:54<01:03,  3.36it/s] 73%|███████▎  | 568/780 [03:55<01:03,  3.36it/s] 73%|███████▎  | 569/780 [03:55<01:02,  3.35it/s] 73%|███████▎  | 570/780 [03:55<01:04,  3.26it/s] 73%|███████▎  | 571/780 [03:55<01:03,  3.29it/s] 73%|███████▎  | 572/780 [03:56<01:02,  3.32it/s] 73%|███████▎  | 573/780 [03:56<01:02,  3.33it/s] 74%|███████▎  | 574/780 [03:56<01:01,  3.34it/s] 74%|███████▎  | 575/780 [03:57<01:01,  3.35it/s] 74%|███████▍  | 576/780 [03:57<01:00,  3.36it/s] 74%|███████▍  | 577/780 [03:57<01:00,  3.36it/s] 74%|███████▍  | 578/780 [03:58<01:00,  3.37it/s] 74%|███████▍  | 579/780 [03:58<00:59,  3.37it/s] 74%|███████▍  | 580/780 [03:58<01:01,  3.23it/s] 74%|███████▍  | 581/780 [03:58<01:00,  3.27it/s] 75%|███████▍  | 582/780 [03:59<01:00,  3.30it/s] 75%|███████▍  | 583/780 [03:59<00:59,  3.32it/s] 75%|███████▍  | 584/780 [03:59<00:58,  3.33it/s] 75%|███████▌  | 585/780 [04:00<00:58,  3.34it/s] 75%|███████▌  | 586/780 [04:00<00:57,  3.35it/s] 75%|███████▌  | 587/780 [04:00<00:57,  3.35it/s] 75%|███████▌  | 588/780 [04:01<00:57,  3.36it/s] 76%|███████▌  | 589/780 [04:01<00:56,  3.36it/s] 76%|███████▌  | 590/780 [04:01<00:58,  3.23it/s] 76%|███████▌  | 591/780 [04:01<00:57,  3.27it/s] 76%|███████▌  | 592/780 [04:02<00:57,  3.30it/s] 76%|███████▌  | 593/780 [04:02<00:56,  3.32it/s] 76%|███████▌  | 594/780 [04:02<00:55,  3.33it/s] 76%|███████▋  | 595/780 [04:03<00:55,  3.33it/s] 76%|███████▋  | 596/780 [04:03<00:55,  3.34it/s] 77%|███████▋  | 597/780 [04:03<00:54,  3.34it/s] 77%|███████▋  | 598/780 [04:04<00:54,  3.34it/s] 77%|███████▋  | 599/780 [04:04<00:54,  3.35it/s] 77%|███████▋  | 600/780 [04:04<00:55,  3.24it/s] 77%|███████▋  | 601/780 [04:04<00:54,  3.27it/s] 77%|███████▋  | 602/780 [04:05<00:53,  3.30it/s] 77%|███████▋  | 603/780 [04:05<00:53,  3.32it/s] 77%|███████▋  | 604/780 [04:05<00:52,  3.33it/s] 78%|███████▊  | 605/780 [04:06<00:52,  3.33it/s] 78%|███████▊  | 606/780 [04:06<00:52,  3.34it/s] 78%|███████▊  | 607/780 [04:06<00:51,  3.34it/s] 78%|███████▊  | 608/780 [04:07<00:51,  3.34it/s] 78%|███████▊  | 609/780 [04:07<00:51,  3.34it/s] 78%|███████▊  | 610/780 [04:07<00:53,  3.20it/s] 78%|███████▊  | 611/780 [04:08<00:52,  3.24it/s] 78%|███████▊  | 612/780 [04:08<00:51,  3.27it/s] 79%|███████▊  | 613/780 [04:08<00:50,  3.30it/s] 79%|███████▊  | 614/780 [04:08<00:50,  3.31it/s] 79%|███████▉  | 615/780 [04:09<00:49,  3.32it/s] 79%|███████▉  | 616/780 [04:09<00:49,  3.33it/s] 79%|███████▉  | 617/780 [04:09<00:48,  3.34it/s] 79%|███████▉  | 618/780 [04:10<00:48,  3.35it/s] 79%|███████▉  | 619/780 [04:10<00:48,  3.35it/s] 79%|███████▉  | 620/780 [04:10<00:49,  3.24it/s] 80%|███████▉  | 621/780 [04:11<00:48,  3.27it/s] 80%|███████▉  | 622/780 [04:11<00:47,  3.29it/s] 80%|███████▉  | 623/780 [04:11<00:47,  3.31it/s] 80%|████████  | 624/780 [04:11<00:46,  3.33it/s][INFO|trainer.py:2140] 2023-08-29 11:56:25,770 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:56:25,771 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 11:56:25,771 >>   Batch size = 8
{'eval_loss': 1.161638855934143, 'eval_runtime': 10.0269, 'eval_samples_per_second': 346.767, 'eval_steps_per_second': 43.383, 'epoch': 3.0}
{'loss': 0.3655, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 54.64it/s][A
  3%|▎         | 12/435 [00:00<00:08, 47.49it/s][A
  4%|▍         | 17/435 [00:00<00:09, 46.28it/s][A
  5%|▌         | 22/435 [00:00<00:09, 45.48it/s][A
  6%|▌         | 27/435 [00:00<00:09, 44.88it/s][A
  7%|▋         | 32/435 [00:00<00:09, 44.70it/s][A
  9%|▊         | 37/435 [00:00<00:08, 44.55it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.41it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.58it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.72it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.62it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.46it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.31it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 40.91it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 42.05it/s][A
 19%|█▉        | 82/435 [00:01<00:08, 42.81it/s][A
 20%|██        | 87/435 [00:01<00:08, 43.38it/s][A
 21%|██        | 92/435 [00:02<00:07, 43.87it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.20it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 44.19it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.14it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 43.92it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 43.90it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 44.13it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 44.32it/s][A
 30%|███       | 132/435 [00:02<00:06, 44.43it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 44.55it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 44.63it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 44.54it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.25it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.08it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 43.98it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 44.23it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.35it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.51it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.59it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.54it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 44.46it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 44.24it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 44.11it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 40.37it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 41.64it/s][A
 50%|████▉     | 217/435 [00:04<00:05, 42.48it/s][A
 51%|█████     | 222/435 [00:05<00:04, 43.23it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 43.51it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.05it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.15it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.26it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 43.89it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 43.83it/s][A
 59%|█████▉    | 257/435 [00:05<00:04, 44.04it/s][A
 60%|██████    | 262/435 [00:05<00:03, 44.24it/s][A
 61%|██████▏   | 267/435 [00:06<00:03, 44.48it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.63it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.69it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.63it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.26it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 43.97it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 43.93it/s][A
 69%|██████▉   | 302/435 [00:06<00:03, 44.04it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.30it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.48it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.57it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.72it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.66it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.40it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.11it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 41.94it/s][A
 80%|███████▉  | 347/435 [00:07<00:02, 42.79it/s][A
 81%|████████  | 352/435 [00:07<00:01, 43.36it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 43.80it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 44.08it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.25it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 44.28it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 44.03it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 43.76it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 43.89it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 44.22it/s][A
 91%|█████████▏| 397/435 [00:09<00:00, 44.35it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 44.54it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 44.63it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 44.65it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 44.43it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.12it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 44.06it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.11it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 44.11it/s][A 80%|████████  | 624/780 [04:21<00:46,  3.33it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:56:35,975 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 11:56:36,201 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:56:44,654 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:56:44,826 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:56:44,917 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:41<23:11,  8.98s/it] 80%|████████  | 626/780 [04:41<16:25,  6.40s/it] 80%|████████  | 627/780 [04:41<11:39,  4.57s/it] 81%|████████  | 628/780 [04:42<08:20,  3.29s/it] 81%|████████  | 629/780 [04:42<06:01,  2.39s/it] 81%|████████  | 630/780 [04:42<04:24,  1.76s/it] 81%|████████  | 631/780 [04:43<03:17,  1.32s/it] 81%|████████  | 632/780 [04:43<02:30,  1.02s/it] 81%|████████  | 633/780 [04:43<01:57,  1.25it/s] 81%|████████▏ | 634/780 [04:43<01:34,  1.54it/s] 81%|████████▏ | 635/780 [04:44<01:18,  1.84it/s] 82%|████████▏ | 636/780 [04:44<01:09,  2.06it/s] 82%|████████▏ | 637/780 [04:44<01:01,  2.33it/s] 82%|████████▏ | 638/780 [04:45<00:55,  2.57it/s] 82%|████████▏ | 639/780 [04:45<00:51,  2.76it/s] 82%|████████▏ | 640/780 [04:45<00:48,  2.92it/s] 82%|████████▏ | 641/780 [04:46<00:47,  2.95it/s] 82%|████████▏ | 642/780 [04:46<00:44,  3.07it/s] 82%|████████▏ | 643/780 [04:46<00:43,  3.15it/s] 83%|████████▎ | 644/780 [04:46<00:42,  3.21it/s] 83%|████████▎ | 645/780 [04:47<00:41,  3.25it/s] 83%|████████▎ | 646/780 [04:47<00:49,  2.72it/s] 83%|████████▎ | 647/780 [04:48<00:48,  2.71it/s] 83%|████████▎ | 648/780 [04:48<00:45,  2.88it/s] 83%|████████▎ | 649/780 [04:48<00:43,  3.02it/s] 83%|████████▎ | 650/780 [04:49<00:41,  3.12it/s] 83%|████████▎ | 651/780 [04:49<00:40,  3.19it/s] 84%|████████▎ | 652/780 [04:49<00:39,  3.24it/s] 84%|████████▎ | 653/780 [04:49<00:38,  3.28it/s] 84%|████████▍ | 654/780 [04:50<00:38,  3.30it/s] 84%|████████▍ | 655/780 [04:50<00:37,  3.33it/s] 84%|████████▍ | 656/780 [04:50<00:38,  3.25it/s] 84%|████████▍ | 657/780 [04:51<00:37,  3.29it/s] 84%|████████▍ | 658/780 [04:51<00:36,  3.32it/s] 84%|████████▍ | 659/780 [04:51<00:36,  3.33it/s] 85%|████████▍ | 660/780 [04:52<00:35,  3.34it/s] 85%|████████▍ | 661/780 [04:52<00:35,  3.35it/s] 85%|████████▍ | 662/780 [04:52<00:35,  3.36it/s] 85%|████████▌ | 663/780 [04:52<00:34,  3.37it/s] 85%|████████▌ | 664/780 [04:53<00:34,  3.37it/s] 85%|████████▌ | 665/780 [04:53<00:34,  3.37it/s] 85%|████████▌ | 666/780 [04:53<00:33,  3.37it/s] 86%|████████▌ | 667/780 [04:54<00:33,  3.37it/s] 86%|████████▌ | 668/780 [04:54<00:33,  3.37it/s] 86%|████████▌ | 669/780 [04:54<00:32,  3.38it/s] 86%|████████▌ | 670/780 [04:55<00:32,  3.37it/s] 86%|████████▌ | 671/780 [04:55<00:32,  3.37it/s] 86%|████████▌ | 672/780 [04:55<00:32,  3.37it/s] 86%|████████▋ | 673/780 [04:55<00:31,  3.37it/s] 86%|████████▋ | 674/780 [04:56<00:31,  3.37it/s] 87%|████████▋ | 675/780 [04:56<00:31,  3.37it/s] 87%|████████▋ | 676/780 [04:56<00:30,  3.37it/s] 87%|████████▋ | 677/780 [04:57<00:31,  3.26it/s] 87%|████████▋ | 678/780 [04:57<00:30,  3.29it/s] 87%|████████▋ | 679/780 [04:57<00:30,  3.31it/s] 87%|████████▋ | 680/780 [04:58<00:30,  3.33it/s] 87%|████████▋ | 681/780 [04:58<00:29,  3.34it/s] 87%|████████▋ | 682/780 [04:58<00:29,  3.34it/s] 88%|████████▊ | 683/780 [04:58<00:28,  3.34it/s] 88%|████████▊ | 684/780 [04:59<00:28,  3.35it/s] 88%|████████▊ | 685/780 [04:59<00:28,  3.35it/s] 88%|████████▊ | 686/780 [04:59<00:28,  3.35it/s] 88%|████████▊ | 687/780 [05:00<00:28,  3.23it/s] 88%|████████▊ | 688/780 [05:00<00:28,  3.27it/s] 88%|████████▊ | 689/780 [05:00<00:27,  3.29it/s] 88%|████████▊ | 690/780 [05:01<00:27,  3.31it/s] 89%|████████▊ | 691/780 [05:01<00:26,  3.32it/s] 89%|████████▊ | 692/780 [05:01<00:26,  3.33it/s] 89%|████████▉ | 693/780 [05:01<00:26,  3.34it/s] 89%|████████▉ | 694/780 [05:02<00:25,  3.34it/s] 89%|████████▉ | 695/780 [05:02<00:25,  3.34it/s] 89%|████████▉ | 696/780 [05:02<00:25,  3.35it/s] 89%|████████▉ | 697/780 [05:03<00:25,  3.23it/s] 89%|████████▉ | 698/780 [05:03<00:25,  3.27it/s] 90%|████████▉ | 699/780 [05:03<00:24,  3.30it/s] 90%|████████▉ | 700/780 [05:04<00:24,  3.33it/s] 90%|████████▉ | 701/780 [05:04<00:23,  3.37it/s] 90%|█████████ | 702/780 [05:04<00:22,  3.39it/s] 90%|█████████ | 703/780 [05:04<00:22,  3.41it/s] 90%|█████████ | 704/780 [05:05<00:22,  3.42it/s] 90%|█████████ | 705/780 [05:05<00:21,  3.43it/s] 91%|█████████ | 706/780 [05:05<00:21,  3.44it/s] 91%|█████████ | 707/780 [05:06<00:21,  3.45it/s] 91%|█████████ | 708/780 [05:06<00:21,  3.35it/s] 91%|█████████ | 709/780 [05:06<00:20,  3.38it/s] 91%|█████████ | 710/780 [05:06<00:20,  3.41it/s] 91%|█████████ | 711/780 [05:07<00:20,  3.42it/s] 91%|█████████▏| 712/780 [05:07<00:19,  3.43it/s] 91%|█████████▏| 713/780 [05:07<00:19,  3.43it/s] 92%|█████████▏| 714/780 [05:08<00:19,  3.44it/s] 92%|█████████▏| 715/780 [05:08<00:18,  3.44it/s] 92%|█████████▏| 716/780 [05:08<00:18,  3.45it/s] 92%|█████████▏| 717/780 [05:08<00:18,  3.45it/s] 92%|█████████▏| 718/780 [05:09<00:17,  3.45it/s] 92%|█████████▏| 719/780 [05:09<00:18,  3.37it/s] 92%|█████████▏| 720/780 [05:09<00:17,  3.40it/s] 92%|█████████▏| 721/780 [05:10<00:17,  3.42it/s] 93%|█████████▎| 722/780 [05:10<00:16,  3.43it/s] 93%|█████████▎| 723/780 [05:10<00:16,  3.43it/s] 93%|█████████▎| 724/780 [05:11<00:16,  3.44it/s] 93%|█████████▎| 725/780 [05:11<00:15,  3.44it/s] 93%|█████████▎| 726/780 [05:11<00:15,  3.44it/s] 93%|█████████▎| 727/780 [05:11<00:15,  3.45it/s] 93%|█████████▎| 728/780 [05:12<00:15,  3.45it/s] 93%|█████████▎| 729/780 [05:12<00:14,  3.45it/s] 94%|█████████▎| 730/780 [05:12<00:15,  3.29it/s] 94%|█████████▎| 731/780 [05:13<00:14,  3.34it/s] 94%|█████████▍| 732/780 [05:13<00:14,  3.38it/s] 94%|█████████▍| 733/780 [05:13<00:13,  3.40it/s] 94%|█████████▍| 734/780 [05:13<00:13,  3.41it/s] 94%|█████████▍| 735/780 [05:14<00:13,  3.43it/s] 94%|█████████▍| 736/780 [05:14<00:12,  3.44it/s] 94%|█████████▍| 737/780 [05:14<00:12,  3.44it/s] 95%|█████████▍| 738/780 [05:15<00:12,  3.45it/s] 95%|█████████▍| 739/780 [05:15<00:11,  3.45it/s] 95%|█████████▍| 740/780 [05:15<00:11,  3.45it/s] 95%|█████████▌| 741/780 [05:16<00:11,  3.36it/s] 95%|█████████▌| 742/780 [05:16<00:11,  3.39it/s] 95%|█████████▌| 743/780 [05:16<00:10,  3.41it/s] 95%|█████████▌| 744/780 [05:16<00:10,  3.42it/s] 96%|█████████▌| 745/780 [05:17<00:10,  3.43it/s] 96%|█████████▌| 746/780 [05:17<00:09,  3.44it/s] 96%|█████████▌| 747/780 [05:17<00:09,  3.44it/s] 96%|█████████▌| 748/780 [05:18<00:09,  3.44it/s] 96%|█████████▌| 749/780 [05:18<00:08,  3.45it/s] 96%|█████████▌| 750/780 [05:18<00:08,  3.45it/s] 96%|█████████▋| 751/780 [05:18<00:08,  3.45it/s] 96%|█████████▋| 752/780 [05:19<00:08,  3.37it/s] 97%|█████████▋| 753/780 [05:19<00:07,  3.39it/s] 97%|█████████▋| 754/780 [05:19<00:07,  3.41it/s] 97%|█████████▋| 755/780 [05:20<00:07,  3.42it/s] 97%|█████████▋| 756/780 [05:20<00:06,  3.44it/s] 97%|█████████▋| 757/780 [05:20<00:06,  3.44it/s] 97%|█████████▋| 758/780 [05:20<00:06,  3.44it/s] 97%|█████████▋| 759/780 [05:21<00:06,  3.45it/s] 97%|█████████▋| 760/780 [05:21<00:05,  3.45it/s] 98%|█████████▊| 761/780 [05:21<00:05,  3.45it/s] 98%|█████████▊| 762/780 [05:22<00:05,  3.45it/s] 98%|█████████▊| 763/780 [05:22<00:05,  3.29it/s] 98%|█████████▊| 764/780 [05:22<00:04,  3.34it/s] 98%|█████████▊| 765/780 [05:23<00:04,  3.37it/s] 98%|█████████▊| 766/780 [05:23<00:04,  3.40it/s] 98%|█████████▊| 767/780 [05:23<00:03,  3.41it/s] 98%|█████████▊| 768/780 [05:23<00:03,  3.43it/s] 99%|█████████▊| 769/780 [05:24<00:03,  3.43it/s] 99%|█████████▊| 770/780 [05:24<00:02,  3.44it/s] 99%|█████████▉| 771/780 [05:24<00:02,  3.28it/s] 99%|█████████▉| 772/780 [05:25<00:02,  3.33it/s] 99%|█████████▉| 773/780 [05:25<00:02,  3.37it/s] 99%|█████████▉| 774/780 [05:25<00:01,  3.39it/s] 99%|█████████▉| 775/780 [05:25<00:01,  3.41it/s] 99%|█████████▉| 776/780 [05:26<00:01,  3.42it/s]100%|█████████▉| 777/780 [05:26<00:00,  3.43it/s]100%|█████████▉| 778/780 [05:26<00:00,  3.44it/s]100%|█████████▉| 779/780 [05:27<00:00,  3.44it/s]100%|██████████| 780/780 [05:27<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 11:57:41,232 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:57:41,232 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 11:57:41,232 >>   Batch size = 8
{'eval_loss': 1.1728451251983643, 'eval_runtime': 9.9054, 'eval_samples_per_second': 351.022, 'eval_steps_per_second': 43.916, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 59.41it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.00it/s][A
  4%|▍         | 18/435 [00:00<00:08, 47.60it/s][A
  5%|▌         | 23/435 [00:00<00:08, 46.50it/s][A
  6%|▋         | 28/435 [00:00<00:08, 45.70it/s][A
  8%|▊         | 33/435 [00:00<00:08, 45.26it/s][A
  9%|▊         | 38/435 [00:00<00:08, 44.97it/s][A
 10%|▉         | 43/435 [00:00<00:08, 44.47it/s][A
 11%|█         | 48/435 [00:01<00:08, 44.34it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 44.37it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 44.66it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 44.68it/s][A
 16%|█▌        | 68/435 [00:01<00:08, 44.76it/s][A
 17%|█▋        | 73/435 [00:01<00:08, 44.62it/s][A
 18%|█▊        | 78/435 [00:01<00:08, 44.47it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 44.38it/s][A
 20%|██        | 88/435 [00:01<00:07, 44.10it/s][A
 21%|██▏       | 93/435 [00:02<00:07, 44.04it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 44.27it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 44.51it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 44.58it/s][A
 26%|██▌       | 113/435 [00:02<00:07, 44.59it/s][A
 27%|██▋       | 118/435 [00:02<00:07, 44.53it/s][A
 28%|██▊       | 123/435 [00:02<00:07, 44.38it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 44.32it/s][A
 31%|███       | 133/435 [00:02<00:06, 44.10it/s][A
 32%|███▏      | 138/435 [00:03<00:07, 41.63it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 42.60it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 43.31it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 43.78it/s][A
 36%|███▋      | 158/435 [00:03<00:06, 44.16it/s][A
 37%|███▋      | 163/435 [00:03<00:06, 44.20it/s][A
 39%|███▊      | 168/435 [00:03<00:06, 44.15it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 44.09it/s][A
 41%|████      | 178/435 [00:03<00:05, 43.82it/s][A
 42%|████▏     | 183/435 [00:04<00:05, 43.92it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 44.18it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 44.38it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 44.48it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 44.64it/s][A
 48%|████▊     | 208/435 [00:04<00:05, 44.51it/s][A
 49%|████▉     | 213/435 [00:04<00:05, 44.38it/s][A
 50%|█████     | 218/435 [00:04<00:04, 44.21it/s][A
 51%|█████▏    | 223/435 [00:05<00:04, 44.07it/s][A
 52%|█████▏    | 228/435 [00:05<00:04, 44.08it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 44.24it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 44.34it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 44.48it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 44.66it/s][A
 58%|█████▊    | 253/435 [00:05<00:04, 44.68it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 44.51it/s][A
 60%|██████    | 263/435 [00:05<00:03, 44.32it/s][A
 62%|██████▏   | 268/435 [00:06<00:03, 44.09it/s][A
 63%|██████▎   | 273/435 [00:06<00:04, 40.29it/s][A
 64%|██████▍   | 278/435 [00:06<00:03, 41.62it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 42.50it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 43.29it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 43.77it/s][A
 69%|██████▊   | 298/435 [00:06<00:03, 44.12it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 44.13it/s][A
 71%|███████   | 308/435 [00:06<00:02, 44.06it/s][A
 72%|███████▏  | 313/435 [00:07<00:02, 43.77it/s][A
 73%|███████▎  | 318/435 [00:07<00:02, 43.71it/s][A
 74%|███████▍  | 323/435 [00:07<00:02, 43.93it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 44.20it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 44.40it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 44.58it/s][A
 79%|███████▉  | 343/435 [00:07<00:02, 44.71it/s][A
 80%|████████  | 348/435 [00:07<00:01, 44.68it/s][A
 81%|████████  | 353/435 [00:07<00:01, 44.30it/s][A
 82%|████████▏ | 358/435 [00:08<00:01, 44.08it/s][A
 83%|████████▎ | 363/435 [00:08<00:01, 43.98it/s][A
 85%|████████▍ | 368/435 [00:08<00:01, 44.12it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 44.28it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 44.41it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 44.52it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 44.61it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 44.65it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 44.40it/s][A
 93%|█████████▎| 403/435 [00:09<00:00, 44.18it/s][A
 94%|█████████▍| 408/435 [00:09<00:00, 38.78it/s][A
 95%|█████████▍| 413/435 [00:09<00:00, 40.46it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 41.72it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 42.63it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 43.30it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 43.79it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 43.79it/s][A100%|██████████| 780/780 [05:37<00:00,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:57:51,977 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 11:57:52,509 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:57:57,391 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:57:57,784 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:57:57,995 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 11:58:08,807 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 11:58:08,844 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156 (score: 1.1338765621185303).
                                                 100%|██████████| 780/780 [06:07<00:00,  3.45it/s]100%|██████████| 780/780 [06:07<00:00,  2.12it/s]
[INFO|trainer.py:1894] 2023-08-29 11:58:21,274 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 11:58:21,531 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:58:25,436 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:58:25,645 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:58:25,779 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 11:58:26,411 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:58:26,411 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:58:26,411 >>   train_loss               =     0.3588
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:58:26,411 >>   train_runtime            = 0:06:07.39
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:58:26,412 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:58:26,412 >>   train_samples_per_second =    136.092
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:58:26,412 >>   train_steps_per_second   =      2.123
{'eval_loss': 1.1818490028381348, 'eval_runtime': 9.9324, 'eval_samples_per_second': 350.066, 'eval_steps_per_second': 43.796, 'epoch': 5.0}
{'train_runtime': 367.3981, 'train_samples_per_second': 136.092, 'train_steps_per_second': 2.123, 'train_loss': 0.35884984334309894, 'epoch': 5.0}
08/29/2023 11:58:26 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 11:58:26,764 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:58:26,764 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 11:58:26,764 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|▏         | 6/435 [00:00<00:07, 55.47it/s]  3%|▎         | 12/435 [00:00<00:08, 49.18it/s]  4%|▍         | 17/435 [00:00<00:08, 47.46it/s]  5%|▌         | 22/435 [00:00<00:08, 46.70it/s]  6%|▌         | 27/435 [00:00<00:08, 46.22it/s]  7%|▋         | 32/435 [00:00<00:08, 45.93it/s]  9%|▊         | 37/435 [00:00<00:08, 45.76it/s] 10%|▉         | 42/435 [00:00<00:08, 45.34it/s] 11%|█         | 47/435 [00:01<00:08, 44.77it/s] 12%|█▏        | 52/435 [00:01<00:08, 44.33it/s] 13%|█▎        | 57/435 [00:01<00:08, 44.35it/s] 14%|█▍        | 62/435 [00:01<00:08, 44.54it/s] 15%|█▌        | 67/435 [00:01<00:08, 44.70it/s] 17%|█▋        | 72/435 [00:01<00:08, 44.83it/s] 18%|█▊        | 77/435 [00:01<00:07, 44.91it/s] 19%|█▉        | 82/435 [00:01<00:08, 43.37it/s] 20%|██        | 87/435 [00:01<00:07, 43.77it/s] 21%|██        | 92/435 [00:02<00:07, 43.78it/s] 22%|██▏       | 97/435 [00:02<00:07, 43.80it/s] 23%|██▎       | 102/435 [00:02<00:07, 43.89it/s] 25%|██▍       | 107/435 [00:02<00:07, 44.23it/s] 26%|██▌       | 112/435 [00:02<00:07, 44.50it/s] 27%|██▋       | 117/435 [00:02<00:07, 44.67it/s] 28%|██▊       | 122/435 [00:02<00:07, 44.66it/s] 29%|██▉       | 127/435 [00:02<00:06, 44.68it/s] 30%|███       | 132/435 [00:02<00:06, 44.62it/s] 31%|███▏      | 137/435 [00:03<00:06, 44.45it/s] 33%|███▎      | 142/435 [00:03<00:06, 44.42it/s] 34%|███▍      | 147/435 [00:03<00:06, 44.37it/s] 35%|███▍      | 152/435 [00:03<00:06, 44.55it/s] 36%|███▌      | 157/435 [00:03<00:06, 44.65it/s] 37%|███▋      | 162/435 [00:03<00:06, 44.72it/s] 38%|███▊      | 167/435 [00:03<00:05, 44.79it/s] 40%|███▉      | 172/435 [00:03<00:05, 44.76it/s] 41%|████      | 177/435 [00:03<00:05, 44.49it/s] 42%|████▏     | 182/435 [00:04<00:05, 44.60it/s] 43%|████▎     | 187/435 [00:04<00:05, 44.16it/s] 44%|████▍     | 192/435 [00:04<00:05, 44.37it/s] 45%|████▌     | 197/435 [00:04<00:05, 44.49it/s] 46%|████▋     | 202/435 [00:04<00:05, 44.71it/s] 48%|████▊     | 207/435 [00:04<00:05, 44.83it/s] 49%|████▊     | 212/435 [00:04<00:04, 44.89it/s] 50%|████▉     | 217/435 [00:04<00:06, 32.99it/s] 51%|█████     | 222/435 [00:05<00:05, 35.87it/s] 52%|█████▏    | 227/435 [00:05<00:05, 38.18it/s] 53%|█████▎    | 232/435 [00:05<00:05, 40.01it/s] 54%|█████▍    | 237/435 [00:05<00:04, 41.36it/s] 56%|█████▌    | 242/435 [00:05<00:04, 42.39it/s] 57%|█████▋    | 247/435 [00:05<00:04, 43.17it/s] 58%|█████▊    | 252/435 [00:05<00:04, 43.51it/s] 59%|█████▉    | 257/435 [00:05<00:04, 43.42it/s] 60%|██████    | 262/435 [00:05<00:03, 43.42it/s] 61%|██████▏   | 267/435 [00:06<00:03, 43.74it/s] 63%|██████▎   | 272/435 [00:06<00:03, 43.82it/s] 64%|██████▎   | 277/435 [00:06<00:03, 44.20it/s] 65%|██████▍   | 282/435 [00:06<00:03, 44.38it/s] 66%|██████▌   | 287/435 [00:06<00:03, 44.57it/s] 67%|██████▋   | 292/435 [00:06<00:03, 44.75it/s] 68%|██████▊   | 297/435 [00:06<00:03, 44.66it/s] 69%|██████▉   | 302/435 [00:06<00:03, 44.33it/s] 71%|███████   | 307/435 [00:06<00:02, 44.14it/s] 72%|███████▏  | 312/435 [00:07<00:02, 44.17it/s] 73%|███████▎  | 317/435 [00:07<00:02, 44.41it/s] 74%|███████▍  | 322/435 [00:07<00:02, 44.43it/s] 75%|███████▌  | 327/435 [00:07<00:02, 44.56it/s] 76%|███████▋  | 332/435 [00:07<00:02, 44.70it/s] 77%|███████▋  | 337/435 [00:07<00:02, 44.75it/s] 79%|███████▊  | 342/435 [00:07<00:02, 44.60it/s] 80%|███████▉  | 347/435 [00:07<00:02, 42.06it/s] 81%|████████  | 352/435 [00:08<00:01, 42.62it/s] 82%|████████▏ | 357/435 [00:08<00:01, 43.18it/s] 83%|████████▎ | 362/435 [00:08<00:01, 43.60it/s] 84%|████████▍ | 367/435 [00:08<00:01, 44.01it/s] 86%|████████▌ | 372/435 [00:08<00:01, 44.30it/s] 87%|████████▋ | 377/435 [00:08<00:01, 44.44it/s] 88%|████████▊ | 382/435 [00:08<00:01, 44.39it/s] 89%|████████▉ | 387/435 [00:08<00:01, 44.12it/s] 90%|█████████ | 392/435 [00:08<00:00, 44.03it/s] 91%|█████████▏| 397/435 [00:09<00:00, 44.17it/s] 92%|█████████▏| 402/435 [00:09<00:00, 44.26it/s] 94%|█████████▎| 407/435 [00:09<00:00, 44.40it/s] 95%|█████████▍| 412/435 [00:09<00:00, 44.52it/s] 96%|█████████▌| 417/435 [00:09<00:00, 44.58it/s] 97%|█████████▋| 422/435 [00:09<00:00, 44.66it/s] 98%|█████████▊| 427/435 [00:09<00:00, 44.44it/s] 99%|█████████▉| 432/435 [00:09<00:00, 44.29it/s]100%|██████████| 435/435 [00:09<00:00, 43.94it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 11:58:36,687 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:58:36,688 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:58:36,688 >>   eval_loss               =     1.1339
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:58:36,688 >>   eval_runtime            = 0:00:09.92
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:58:36,688 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:58:36,688 >>   eval_samples_per_second =    350.377
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:58:36,688 >>   eval_steps_per_second   =     43.835
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:58:36,688 >>   perplexity              =     3.1077
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:58:48,888 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:58:48,927 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:58:48,927 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:58:48,927 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:58:48,927 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:58:50,051 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:58:50,052 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:58:50,807 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:58:52,001 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:58:52,069 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:58:55,285 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:58:55,344 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:58:55,344 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:58:55,344 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:58:55,344 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:58:56,360 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:58:56,361 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:58:57,016 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:58:57,290 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:58:57,290 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.37it/s]Extractor Predicting: 2it [00:01,  1.35it/s]Extractor Predicting: 3it [00:02,  1.37it/s]Extractor Predicting: 4it [00:02,  1.36it/s]Extractor Predicting: 5it [00:03,  1.35it/s]Extractor Predicting: 6it [00:04,  1.32it/s]Extractor Predicting: 7it [00:05,  1.30it/s]Extractor Predicting: 8it [00:06,  1.31it/s]Extractor Predicting: 9it [00:06,  1.32it/s]Extractor Predicting: 10it [00:07,  1.28it/s]Extractor Predicting: 11it [00:08,  1.28it/s]Extractor Predicting: 12it [00:09,  1.28it/s]Extractor Predicting: 13it [00:09,  1.31it/s]Extractor Predicting: 14it [00:10,  1.28it/s]Extractor Predicting: 15it [00:11,  1.29it/s]Extractor Predicting: 16it [00:12,  1.27it/s]Extractor Predicting: 17it [00:12,  1.31it/s]Extractor Predicting: 18it [00:13,  1.32it/s]Extractor Predicting: 19it [00:14,  1.33it/s]Extractor Predicting: 20it [00:15,  1.30it/s]Extractor Predicting: 21it [00:16,  1.32it/s]Extractor Predicting: 22it [00:16,  1.33it/s]Extractor Predicting: 23it [00:17,  1.35it/s]Extractor Predicting: 24it [00:18,  1.36it/s]Extractor Predicting: 25it [00:19,  1.30it/s]Extractor Predicting: 26it [00:19,  1.28it/s]Extractor Predicting: 27it [00:20,  1.30it/s]Extractor Predicting: 28it [00:21,  1.28it/s]Extractor Predicting: 29it [00:22,  1.25it/s]Extractor Predicting: 30it [00:23,  1.26it/s]Extractor Predicting: 31it [00:23,  1.27it/s]Extractor Predicting: 32it [00:24,  1.28it/s]Extractor Predicting: 33it [00:25,  1.24it/s]Extractor Predicting: 34it [00:26,  1.26it/s]Extractor Predicting: 35it [00:26,  1.30it/s]Extractor Predicting: 36it [00:27,  1.33it/s]Extractor Predicting: 37it [00:28,  1.30it/s]Extractor Predicting: 38it [00:29,  1.26it/s]Extractor Predicting: 39it [00:30,  1.28it/s]Extractor Predicting: 40it [00:30,  1.27it/s]Extractor Predicting: 41it [00:31,  1.29it/s]Extractor Predicting: 42it [00:32,  1.30it/s]Extractor Predicting: 43it [00:33,  1.28it/s]Extractor Predicting: 44it [00:33,  1.28it/s]Extractor Predicting: 45it [00:34,  1.25it/s]Extractor Predicting: 46it [00:35,  1.29it/s]Extractor Predicting: 47it [00:36,  1.30it/s]Extractor Predicting: 48it [00:37,  1.31it/s]Extractor Predicting: 49it [00:37,  1.32it/s]Extractor Predicting: 50it [00:38,  1.35it/s]Extractor Predicting: 51it [00:39,  1.35it/s]Extractor Predicting: 52it [00:40,  1.31it/s]Extractor Predicting: 53it [00:40,  1.30it/s]Extractor Predicting: 54it [00:41,  1.32it/s]Extractor Predicting: 55it [00:42,  1.31it/s]Extractor Predicting: 56it [00:43,  1.28it/s]Extractor Predicting: 57it [00:43,  1.27it/s]Extractor Predicting: 58it [00:44,  1.29it/s]Extractor Predicting: 59it [00:45,  1.33it/s]Extractor Predicting: 60it [00:46,  1.33it/s]Extractor Predicting: 61it [00:46,  1.34it/s]Extractor Predicting: 62it [00:47,  1.36it/s]Extractor Predicting: 63it [00:48,  1.36it/s]Extractor Predicting: 64it [00:49,  1.36it/s]Extractor Predicting: 65it [00:49,  1.33it/s]Extractor Predicting: 66it [00:50,  1.32it/s]Extractor Predicting: 67it [00:51,  1.24it/s]Extractor Predicting: 68it [00:52,  1.28it/s]Extractor Predicting: 69it [00:53,  1.27it/s]Extractor Predicting: 70it [00:53,  1.28it/s]Extractor Predicting: 71it [00:54,  1.29it/s]Extractor Predicting: 72it [00:55,  1.32it/s]Extractor Predicting: 73it [00:56,  1.29it/s]Extractor Predicting: 74it [00:56,  1.32it/s]Extractor Predicting: 75it [00:57,  1.34it/s]Extractor Predicting: 76it [00:58,  1.33it/s]Extractor Predicting: 77it [00:59,  1.29it/s]Extractor Predicting: 78it [00:59,  1.29it/s]Extractor Predicting: 79it [01:00,  1.28it/s]Extractor Predicting: 80it [01:01,  1.28it/s]Extractor Predicting: 81it [01:02,  1.24it/s]Extractor Predicting: 82it [01:03,  1.28it/s]Extractor Predicting: 83it [01:03,  1.29it/s]Extractor Predicting: 84it [01:04,  1.30it/s]Extractor Predicting: 85it [01:05,  1.28it/s]Extractor Predicting: 86it [01:06,  1.29it/s]Extractor Predicting: 87it [01:06,  1.29it/s]Extractor Predicting: 88it [01:07,  1.29it/s]Extractor Predicting: 89it [01:08,  1.31it/s]Extractor Predicting: 90it [01:09,  1.32it/s]Extractor Predicting: 91it [01:09,  1.36it/s]Extractor Predicting: 92it [01:10,  1.39it/s]Extractor Predicting: 93it [01:11,  1.40it/s]Extractor Predicting: 94it [01:12,  1.36it/s]Extractor Predicting: 95it [01:12,  1.39it/s]Extractor Predicting: 96it [01:13,  1.36it/s]Extractor Predicting: 97it [01:14,  1.36it/s]Extractor Predicting: 98it [01:14,  1.36it/s]Extractor Predicting: 99it [01:15,  1.32it/s]Extractor Predicting: 100it [01:16,  1.28it/s]Extractor Predicting: 101it [01:17,  1.28it/s]Extractor Predicting: 102it [01:18,  1.35it/s]Extractor Predicting: 103it [01:18,  1.37it/s]Extractor Predicting: 104it [01:19,  1.36it/s]Extractor Predicting: 105it [01:20,  1.33it/s]Extractor Predicting: 106it [01:21,  1.34it/s]Extractor Predicting: 107it [01:21,  1.34it/s]Extractor Predicting: 108it [01:22,  1.33it/s]Extractor Predicting: 109it [01:23,  1.33it/s]Extractor Predicting: 110it [01:24,  1.35it/s]Extractor Predicting: 111it [01:24,  1.35it/s]Extractor Predicting: 112it [01:25,  1.35it/s]Extractor Predicting: 113it [01:26,  1.39it/s]Extractor Predicting: 114it [01:26,  1.40it/s]Extractor Predicting: 115it [01:27,  1.39it/s]Extractor Predicting: 116it [01:28,  1.39it/s]Extractor Predicting: 117it [01:29,  1.39it/s]Extractor Predicting: 118it [01:29,  1.38it/s]Extractor Predicting: 119it [01:30,  1.37it/s]Extractor Predicting: 120it [01:31,  1.34it/s]Extractor Predicting: 121it [01:32,  1.33it/s]Extractor Predicting: 122it [01:32,  1.33it/s]Extractor Predicting: 123it [01:33,  1.31it/s]Extractor Predicting: 124it [01:34,  1.31it/s]Extractor Predicting: 125it [01:35,  1.32it/s]Extractor Predicting: 126it [01:35,  1.35it/s]Extractor Predicting: 127it [01:36,  1.32it/s]Extractor Predicting: 128it [01:37,  1.33it/s]Extractor Predicting: 129it [01:38,  1.34it/s]Extractor Predicting: 130it [01:38,  1.32it/s]Extractor Predicting: 131it [01:39,  1.32it/s]Extractor Predicting: 132it [01:40,  1.30it/s]Extractor Predicting: 133it [01:41,  1.32it/s]Extractor Predicting: 134it [01:41,  1.35it/s]Extractor Predicting: 135it [01:42,  1.34it/s]Extractor Predicting: 136it [01:43,  1.31it/s]Extractor Predicting: 137it [01:44,  1.34it/s]Extractor Predicting: 138it [01:44,  1.32it/s]Extractor Predicting: 139it [01:45,  1.31it/s]Extractor Predicting: 140it [01:46,  1.33it/s]Extractor Predicting: 141it [01:47,  1.24it/s]Extractor Predicting: 142it [01:48,  1.24it/s]Extractor Predicting: 143it [01:48,  1.29it/s]Extractor Predicting: 144it [01:49,  1.34it/s]Extractor Predicting: 144it [01:49,  1.31it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:01:05,973 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:01:06,006 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:01:06,006 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:01:06,006 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:01:06,006 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:01:06,858 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:01:06,860 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:01:07,496 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:01:08,591 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:01:08,591 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:01:11,633 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:01:11,654 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:01:11,655 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:01:11,655 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:01:11,655 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:01:12,447 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:01:12,448 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:01:13,171 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:01:13,407 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:01:13,407 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3064340239912759,
  "recall": 0.16163359217716422,
  "score": 0.21163622669930332,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.33it/s]Extractor Predicting: 2it [00:01,  1.28it/s]Extractor Predicting: 3it [00:02,  1.30it/s]Extractor Predicting: 4it [00:03,  1.32it/s]Extractor Predicting: 5it [00:03,  1.33it/s]Extractor Predicting: 6it [00:04,  1.34it/s]Extractor Predicting: 7it [00:05,  1.37it/s]Extractor Predicting: 8it [00:05,  1.41it/s]Extractor Predicting: 9it [00:06,  1.36it/s]Extractor Predicting: 10it [00:07,  1.37it/s]Extractor Predicting: 11it [00:08,  1.37it/s]Extractor Predicting: 12it [00:08,  1.35it/s]Extractor Predicting: 13it [00:09,  1.34it/s]Extractor Predicting: 14it [00:10,  1.35it/s]Extractor Predicting: 15it [00:11,  1.35it/s]Extractor Predicting: 16it [00:11,  1.35it/s]Extractor Predicting: 17it [00:12,  1.34it/s]Extractor Predicting: 18it [00:13,  1.36it/s]Extractor Predicting: 19it [00:14,  1.39it/s]Extractor Predicting: 20it [00:14,  1.34it/s]Extractor Predicting: 21it [00:15,  1.36it/s]Extractor Predicting: 22it [00:16,  1.37it/s]Extractor Predicting: 23it [00:16,  1.38it/s]Extractor Predicting: 24it [00:17,  1.37it/s]Extractor Predicting: 25it [00:18,  1.35it/s]Extractor Predicting: 26it [00:19,  1.28it/s]Extractor Predicting: 27it [00:20,  1.29it/s]Extractor Predicting: 28it [00:20,  1.31it/s]Extractor Predicting: 29it [00:21,  1.31it/s]Extractor Predicting: 30it [00:22,  1.35it/s]Extractor Predicting: 31it [00:23,  1.34it/s]Extractor Predicting: 32it [00:23,  1.40it/s]Extractor Predicting: 33it [00:24,  1.37it/s]Extractor Predicting: 34it [00:25,  1.34it/s]Extractor Predicting: 35it [00:26,  1.34it/s]Extractor Predicting: 36it [00:26,  1.35it/s]Extractor Predicting: 37it [00:27,  1.37it/s]Extractor Predicting: 38it [00:28,  1.39it/s]Extractor Predicting: 39it [00:28,  1.39it/s]Extractor Predicting: 40it [00:29,  1.38it/s]Extractor Predicting: 41it [00:30,  1.36it/s]Extractor Predicting: 42it [00:31,  1.39it/s]Extractor Predicting: 43it [00:31,  1.34it/s]Extractor Predicting: 44it [00:32,  1.35it/s]Extractor Predicting: 45it [00:33,  1.33it/s]Extractor Predicting: 46it [00:34,  1.30it/s]Extractor Predicting: 47it [00:34,  1.34it/s]Extractor Predicting: 48it [00:35,  1.33it/s]Extractor Predicting: 49it [00:36,  1.35it/s]Extractor Predicting: 50it [00:37,  1.35it/s]Extractor Predicting: 51it [00:37,  1.34it/s]Extractor Predicting: 52it [00:38,  1.32it/s]Extractor Predicting: 53it [00:39,  1.35it/s]Extractor Predicting: 54it [00:40,  1.32it/s]Extractor Predicting: 55it [00:40,  1.33it/s]Extractor Predicting: 56it [00:41,  1.33it/s]Extractor Predicting: 57it [00:42,  1.32it/s]Extractor Predicting: 58it [00:43,  1.30it/s]Extractor Predicting: 59it [00:43,  1.29it/s]Extractor Predicting: 60it [00:44,  1.31it/s]Extractor Predicting: 61it [00:45,  1.30it/s]Extractor Predicting: 62it [00:46,  1.31it/s]Extractor Predicting: 63it [00:46,  1.32it/s]Extractor Predicting: 64it [00:47,  1.32it/s]Extractor Predicting: 65it [00:48,  1.36it/s]Extractor Predicting: 66it [00:49,  1.33it/s]Extractor Predicting: 67it [00:49,  1.32it/s]Extractor Predicting: 68it [00:50,  1.31it/s]Extractor Predicting: 69it [00:51,  1.31it/s]Extractor Predicting: 70it [00:52,  1.28it/s]Extractor Predicting: 71it [00:53,  1.29it/s]Extractor Predicting: 72it [00:53,  1.29it/s]Extractor Predicting: 73it [00:54,  1.28it/s]Extractor Predicting: 74it [00:55,  1.30it/s]Extractor Predicting: 75it [00:56,  1.30it/s]Extractor Predicting: 76it [00:56,  1.32it/s]Extractor Predicting: 77it [00:57,  1.31it/s]Extractor Predicting: 78it [00:58,  1.33it/s]Extractor Predicting: 79it [00:59,  1.33it/s]Extractor Predicting: 80it [00:59,  1.38it/s]Extractor Predicting: 81it [01:00,  1.38it/s]Extractor Predicting: 82it [01:01,  1.37it/s]Extractor Predicting: 83it [01:02,  1.36it/s]Extractor Predicting: 84it [01:02,  1.35it/s]Extractor Predicting: 85it [01:03,  1.33it/s]Extractor Predicting: 86it [01:04,  1.28it/s]Extractor Predicting: 87it [01:05,  1.26it/s]Extractor Predicting: 88it [01:05,  1.29it/s]Extractor Predicting: 89it [01:06,  1.28it/s]Extractor Predicting: 90it [01:07,  1.26it/s]Extractor Predicting: 91it [01:08,  1.25it/s]Extractor Predicting: 92it [01:09,  1.27it/s]Extractor Predicting: 93it [01:09,  1.28it/s]Extractor Predicting: 94it [01:10,  1.30it/s]Extractor Predicting: 95it [01:11,  1.30it/s]Extractor Predicting: 96it [01:12,  1.30it/s]Extractor Predicting: 97it [01:13,  1.27it/s]Extractor Predicting: 98it [01:13,  1.25it/s]Extractor Predicting: 99it [01:14,  1.26it/s]Extractor Predicting: 100it [01:15,  1.29it/s]Extractor Predicting: 101it [01:16,  1.30it/s]Extractor Predicting: 102it [01:16,  1.31it/s]Extractor Predicting: 103it [01:17,  1.29it/s]Extractor Predicting: 104it [01:18,  1.27it/s]Extractor Predicting: 105it [01:19,  1.30it/s]Extractor Predicting: 106it [01:20,  1.29it/s]Extractor Predicting: 107it [01:20,  1.30it/s]Extractor Predicting: 108it [01:21,  1.30it/s]Extractor Predicting: 109it [01:22,  1.29it/s]Extractor Predicting: 110it [01:23,  1.28it/s]Extractor Predicting: 111it [01:23,  1.29it/s]Extractor Predicting: 112it [01:24,  1.28it/s]Extractor Predicting: 113it [01:25,  1.28it/s]Extractor Predicting: 114it [01:26,  1.31it/s]Extractor Predicting: 115it [01:26,  1.30it/s]Extractor Predicting: 116it [01:27,  1.25it/s]Extractor Predicting: 117it [01:28,  1.25it/s]Extractor Predicting: 118it [01:29,  1.27it/s]Extractor Predicting: 119it [01:30,  1.27it/s]Extractor Predicting: 120it [01:30,  1.27it/s]Extractor Predicting: 121it [01:31,  1.27it/s]Extractor Predicting: 122it [01:32,  1.31it/s]Extractor Predicting: 123it [01:33,  1.31it/s]Extractor Predicting: 124it [01:34,  1.32it/s]Extractor Predicting: 125it [01:34,  1.30it/s]Extractor Predicting: 126it [01:35,  1.36it/s]Extractor Predicting: 127it [01:36,  1.36it/s]Extractor Predicting: 128it [01:36,  1.33it/s]Extractor Predicting: 129it [01:37,  1.37it/s]Extractor Predicting: 130it [01:38,  1.33it/s]Extractor Predicting: 131it [01:39,  1.32it/s]Extractor Predicting: 132it [01:39,  1.34it/s]Extractor Predicting: 133it [01:40,  1.38it/s]Extractor Predicting: 134it [01:41,  1.40it/s]Extractor Predicting: 135it [01:42,  1.32it/s]Extractor Predicting: 136it [01:42,  1.34it/s]Extractor Predicting: 137it [01:43,  1.36it/s]Extractor Predicting: 138it [01:44,  1.40it/s]Extractor Predicting: 139it [01:45,  1.38it/s]Extractor Predicting: 140it [01:45,  1.35it/s]Extractor Predicting: 141it [01:46,  1.22it/s]Extractor Predicting: 142it [01:47,  1.27it/s]Extractor Predicting: 143it [01:48,  1.27it/s]Extractor Predicting: 144it [01:49,  1.25it/s]Extractor Predicting: 145it [01:49,  1.27it/s]Extractor Predicting: 146it [01:50,  1.31it/s]Extractor Predicting: 147it [01:51,  1.36it/s]Extractor Predicting: 148it [01:51,  1.37it/s]Extractor Predicting: 149it [01:52,  1.37it/s]Extractor Predicting: 150it [01:53,  1.41it/s]Extractor Predicting: 151it [01:54,  1.41it/s]Extractor Predicting: 152it [01:54,  1.42it/s]Extractor Predicting: 153it [01:55,  1.41it/s]Extractor Predicting: 154it [01:56,  1.42it/s]Extractor Predicting: 155it [01:56,  1.41it/s]Extractor Predicting: 156it [01:57,  1.42it/s]Extractor Predicting: 157it [01:58,  1.45it/s]Extractor Predicting: 158it [01:58,  1.44it/s]Extractor Predicting: 159it [01:59,  1.50it/s]Extractor Predicting: 160it [02:00,  1.53it/s]Extractor Predicting: 161it [02:00,  1.48it/s]Extractor Predicting: 162it [02:01,  1.44it/s]Extractor Predicting: 163it [02:02,  1.42it/s]Extractor Predicting: 164it [02:03,  1.43it/s]Extractor Predicting: 165it [02:03,  1.46it/s]Extractor Predicting: 166it [02:04,  1.48it/s]Extractor Predicting: 167it [02:05,  1.46it/s]Extractor Predicting: 168it [02:05,  1.42it/s]Extractor Predicting: 169it [02:06,  1.45it/s]Extractor Predicting: 170it [02:07,  1.45it/s]Extractor Predicting: 171it [02:07,  1.48it/s]Extractor Predicting: 172it [02:08,  1.48it/s]Extractor Predicting: 173it [02:09,  1.46it/s]Extractor Predicting: 174it [02:09,  1.38it/s]Extractor Predicting: 175it [02:10,  1.38it/s]Extractor Predicting: 176it [02:11,  1.35it/s]Extractor Predicting: 177it [02:12,  1.32it/s]Extractor Predicting: 178it [02:13,  1.29it/s]Extractor Predicting: 179it [02:13,  1.29it/s]Extractor Predicting: 180it [02:14,  1.31it/s]Extractor Predicting: 181it [02:15,  1.30it/s]Extractor Predicting: 182it [02:16,  1.30it/s]Extractor Predicting: 183it [02:16,  1.32it/s]Extractor Predicting: 184it [02:17,  1.32it/s]Extractor Predicting: 185it [02:18,  1.31it/s]Extractor Predicting: 186it [02:19,  1.32it/s]Extractor Predicting: 187it [02:19,  1.32it/s]Extractor Predicting: 188it [02:20,  1.31it/s]Extractor Predicting: 189it [02:21,  1.30it/s]Extractor Predicting: 190it [02:22,  1.29it/s]Extractor Predicting: 191it [02:23,  1.27it/s]Extractor Predicting: 192it [02:23,  1.27it/s]Extractor Predicting: 193it [02:24,  1.25it/s]Extractor Predicting: 194it [02:25,  1.25it/s]Extractor Predicting: 195it [02:26,  1.27it/s]Extractor Predicting: 196it [02:26,  1.30it/s]Extractor Predicting: 197it [02:27,  1.31it/s]Extractor Predicting: 198it [02:28,  1.30it/s]Extractor Predicting: 199it [02:29,  1.31it/s]Extractor Predicting: 200it [02:30,  1.29it/s]Extractor Predicting: 201it [02:30,  1.28it/s]Extractor Predicting: 202it [02:31,  1.26it/s]Extractor Predicting: 203it [02:32,  1.24it/s]Extractor Predicting: 204it [02:33,  1.22it/s]Extractor Predicting: 205it [02:34,  1.23it/s]Extractor Predicting: 206it [02:34,  1.23it/s]Extractor Predicting: 207it [02:35,  1.24it/s]Extractor Predicting: 208it [02:36,  1.22it/s]Extractor Predicting: 209it [02:37,  1.19it/s]Extractor Predicting: 210it [02:38,  1.20it/s]Extractor Predicting: 211it [02:39,  1.20it/s]Extractor Predicting: 212it [02:39,  1.21it/s]Extractor Predicting: 213it [02:40,  1.21it/s]Extractor Predicting: 214it [02:41,  1.24it/s]Extractor Predicting: 215it [02:42,  1.22it/s]Extractor Predicting: 216it [02:43,  1.19it/s]Extractor Predicting: 217it [02:44,  1.22it/s]Extractor Predicting: 218it [02:44,  1.22it/s]Extractor Predicting: 219it [02:45,  1.21it/s]Extractor Predicting: 220it [02:46,  1.17it/s]Extractor Predicting: 221it [02:47,  1.18it/s]Extractor Predicting: 222it [02:48,  1.17it/s]Extractor Predicting: 223it [02:49,  1.22it/s]Extractor Predicting: 224it [02:49,  1.20it/s]Extractor Predicting: 225it [02:50,  1.21it/s]Extractor Predicting: 226it [02:51,  1.20it/s]Extractor Predicting: 227it [02:52,  1.22it/s]Extractor Predicting: 228it [02:53,  1.22it/s]Extractor Predicting: 229it [02:54,  1.24it/s]Extractor Predicting: 230it [02:54,  1.27it/s]Extractor Predicting: 231it [02:55,  1.31it/s]Extractor Predicting: 232it [02:56,  1.33it/s]Extractor Predicting: 233it [02:56,  1.35it/s]Extractor Predicting: 234it [02:57,  1.30it/s]Extractor Predicting: 235it [02:58,  1.32it/s]Extractor Predicting: 236it [02:59,  1.33it/s]Extractor Predicting: 237it [02:59,  1.33it/s]Extractor Predicting: 238it [03:00,  1.34it/s]Extractor Predicting: 239it [03:01,  1.32it/s]Extractor Predicting: 240it [03:02,  1.35it/s]Extractor Predicting: 241it [03:02,  1.36it/s]Extractor Predicting: 242it [03:03,  1.38it/s]Extractor Predicting: 243it [03:04,  1.24it/s]Extractor Predicting: 244it [03:05,  1.31it/s]Extractor Predicting: 245it [03:06,  1.30it/s]Extractor Predicting: 246it [03:06,  1.29it/s]Extractor Predicting: 247it [03:07,  1.29it/s]Extractor Predicting: 248it [03:08,  1.29it/s]Extractor Predicting: 249it [03:09,  1.32it/s]Extractor Predicting: 250it [03:09,  1.31it/s]Extractor Predicting: 251it [03:10,  1.33it/s]Extractor Predicting: 252it [03:11,  1.37it/s]Extractor Predicting: 253it [03:12,  1.35it/s]Extractor Predicting: 254it [03:12,  1.32it/s]Extractor Predicting: 255it [03:13,  1.31it/s]Extractor Predicting: 256it [03:14,  1.30it/s]Extractor Predicting: 257it [03:15,  1.27it/s]Extractor Predicting: 258it [03:16,  1.28it/s]Extractor Predicting: 259it [03:16,  1.28it/s]Extractor Predicting: 260it [03:17,  1.27it/s]Extractor Predicting: 261it [03:18,  1.28it/s]Extractor Predicting: 262it [03:19,  1.29it/s]Extractor Predicting: 263it [03:19,  1.30it/s]Extractor Predicting: 264it [03:20,  1.29it/s]Extractor Predicting: 265it [03:21,  1.29it/s]Extractor Predicting: 266it [03:22,  1.27it/s]Extractor Predicting: 267it [03:23,  1.25it/s]Extractor Predicting: 268it [03:23,  1.24it/s]Extractor Predicting: 269it [03:24,  1.26it/s]Extractor Predicting: 270it [03:25,  1.27it/s]Extractor Predicting: 271it [03:26,  1.26it/s]Extractor Predicting: 272it [03:26,  1.29it/s]Extractor Predicting: 273it [03:27,  1.26it/s]Extractor Predicting: 274it [03:28,  1.24it/s]Extractor Predicting: 275it [03:29,  1.24it/s]Extractor Predicting: 276it [03:30,  1.24it/s]Extractor Predicting: 277it [03:31,  1.23it/s]Extractor Predicting: 278it [03:31,  1.25it/s]Extractor Predicting: 279it [03:32,  1.25it/s]Extractor Predicting: 280it [03:33,  1.26it/s]Extractor Predicting: 281it [03:34,  1.27it/s]Extractor Predicting: 282it [03:35,  1.26it/s]Extractor Predicting: 283it [03:35,  1.24it/s]Extractor Predicting: 284it [03:36,  1.30it/s]Extractor Predicting: 285it [03:37,  1.27it/s]Extractor Predicting: 286it [03:38,  1.30it/s]Extractor Predicting: 287it [03:38,  1.30it/s]Extractor Predicting: 288it [03:39,  1.29it/s]Extractor Predicting: 289it [03:40,  1.25it/s]Extractor Predicting: 290it [03:41,  1.26it/s]Extractor Predicting: 291it [03:42,  1.23it/s]Extractor Predicting: 292it [03:42,  1.22it/s]Extractor Predicting: 293it [03:43,  1.23it/s]Extractor Predicting: 294it [03:44,  1.21it/s]Extractor Predicting: 295it [03:45,  1.24it/s]Extractor Predicting: 296it [03:46,  1.24it/s]Extractor Predicting: 297it [03:47,  1.24it/s]Extractor Predicting: 298it [03:47,  1.24it/s]Extractor Predicting: 299it [03:48,  1.23it/s]Extractor Predicting: 300it [03:49,  1.26it/s]Extractor Predicting: 301it [03:50,  1.23it/s]Extractor Predicting: 302it [03:51,  1.26it/s]Extractor Predicting: 303it [03:51,  1.24it/s]Extractor Predicting: 304it [03:52,  1.24it/s]Extractor Predicting: 305it [03:53,  1.23it/s]Extractor Predicting: 306it [03:54,  1.27it/s]Extractor Predicting: 307it [03:55,  1.27it/s]Extractor Predicting: 308it [03:55,  1.29it/s]Extractor Predicting: 309it [03:56,  1.27it/s]Extractor Predicting: 310it [03:57,  1.29it/s]Extractor Predicting: 311it [03:58,  1.28it/s]Extractor Predicting: 312it [03:58,  1.33it/s]Extractor Predicting: 313it [03:59,  1.35it/s]Extractor Predicting: 314it [04:00,  1.37it/s]Extractor Predicting: 315it [04:00,  1.38it/s]Extractor Predicting: 316it [04:01,  1.36it/s]Extractor Predicting: 317it [04:02,  1.34it/s]Extractor Predicting: 318it [04:03,  1.33it/s]Extractor Predicting: 319it [04:03,  1.33it/s]Extractor Predicting: 320it [04:04,  1.29it/s]Extractor Predicting: 321it [04:05,  1.30it/s]Extractor Predicting: 322it [04:06,  1.31it/s]Extractor Predicting: 323it [04:07,  1.27it/s]Extractor Predicting: 324it [04:07,  1.25it/s]Extractor Predicting: 325it [04:08,  1.26it/s]Extractor Predicting: 326it [04:09,  1.28it/s]Extractor Predicting: 327it [04:10,  1.31it/s]Extractor Predicting: 328it [04:11,  1.28it/s]Extractor Predicting: 329it [04:11,  1.30it/s]Extractor Predicting: 330it [04:12,  1.30it/s]Extractor Predicting: 331it [04:13,  1.29it/s]Extractor Predicting: 332it [04:14,  1.28it/s]Extractor Predicting: 333it [04:14,  1.30it/s]Extractor Predicting: 334it [04:15,  1.31it/s]Extractor Predicting: 335it [04:16,  1.33it/s]Extractor Predicting: 336it [04:17,  1.28it/s]Extractor Predicting: 337it [04:17,  1.29it/s]Extractor Predicting: 338it [04:18,  1.31it/s]Extractor Predicting: 339it [04:19,  1.21it/s]Extractor Predicting: 340it [04:20,  1.24it/s]Extractor Predicting: 341it [04:21,  1.27it/s]Extractor Predicting: 342it [04:21,  1.27it/s]Extractor Predicting: 343it [04:22,  1.28it/s]Extractor Predicting: 344it [04:23,  1.30it/s]Extractor Predicting: 345it [04:24,  1.27it/s]Extractor Predicting: 346it [04:25,  1.28it/s]Extractor Predicting: 347it [04:25,  1.28it/s]Extractor Predicting: 348it [04:26,  1.27it/s]Extractor Predicting: 349it [04:27,  1.30it/s]Extractor Predicting: 350it [04:28,  1.31it/s]Extractor Predicting: 351it [04:28,  1.33it/s]Extractor Predicting: 352it [04:29,  1.28it/s]Extractor Predicting: 353it [04:30,  1.30it/s]Extractor Predicting: 354it [04:31,  1.33it/s]Extractor Predicting: 355it [04:31,  1.36it/s]Extractor Predicting: 356it [04:32,  1.34it/s]Extractor Predicting: 357it [04:33,  1.34it/s]Extractor Predicting: 358it [04:34,  1.35it/s]Extractor Predicting: 359it [04:34,  1.33it/s]Extractor Predicting: 360it [04:35,  1.32it/s]Extractor Predicting: 361it [04:36,  1.34it/s]Extractor Predicting: 362it [04:37,  1.35it/s]Extractor Predicting: 363it [04:37,  1.36it/s]Extractor Predicting: 364it [04:38,  1.37it/s]Extractor Predicting: 365it [04:39,  1.37it/s]Extractor Predicting: 366it [04:40,  1.35it/s]Extractor Predicting: 367it [04:40,  1.31it/s]Extractor Predicting: 368it [04:41,  1.31it/s]Extractor Predicting: 369it [04:42,  1.29it/s]Extractor Predicting: 370it [04:43,  1.33it/s]Extractor Predicting: 371it [04:43,  1.35it/s]Extractor Predicting: 372it [04:44,  1.36it/s]Extractor Predicting: 373it [04:45,  1.33it/s]Extractor Predicting: 374it [04:46,  1.31it/s]Extractor Predicting: 375it [04:46,  1.32it/s]Extractor Predicting: 376it [04:47,  1.34it/s]Extractor Predicting: 377it [04:48,  1.35it/s]Extractor Predicting: 378it [04:49,  1.38it/s]Extractor Predicting: 379it [04:49,  1.31it/s]Extractor Predicting: 380it [04:50,  1.32it/s]Extractor Predicting: 381it [04:51,  1.33it/s]Extractor Predicting: 382it [04:52,  1.34it/s]Extractor Predicting: 383it [04:52,  1.35it/s]Extractor Predicting: 384it [04:53,  1.38it/s]Extractor Predicting: 385it [04:54,  1.35it/s]Extractor Predicting: 386it [04:54,  1.36it/s]Extractor Predicting: 387it [04:55,  1.35it/s]Extractor Predicting: 388it [04:56,  1.33it/s]Extractor Predicting: 389it [04:57,  1.34it/s]Extractor Predicting: 390it [04:58,  1.33it/s]Extractor Predicting: 391it [04:58,  1.31it/s]Extractor Predicting: 392it [04:59,  1.32it/s]Extractor Predicting: 393it [05:00,  1.35it/s]Extractor Predicting: 394it [05:01,  1.29it/s]Extractor Predicting: 395it [05:01,  1.25it/s]Extractor Predicting: 396it [05:02,  1.26it/s]Extractor Predicting: 397it [05:03,  1.24it/s]Extractor Predicting: 398it [05:04,  1.25it/s]Extractor Predicting: 399it [05:05,  1.25it/s]Extractor Predicting: 400it [05:05,  1.29it/s]Extractor Predicting: 401it [05:06,  1.26it/s]Extractor Predicting: 402it [05:07,  1.25it/s]Extractor Predicting: 403it [05:08,  1.25it/s]Extractor Predicting: 404it [05:09,  1.21it/s]Extractor Predicting: 405it [05:10,  1.20it/s]Extractor Predicting: 406it [05:10,  1.21it/s]Extractor Predicting: 407it [05:11,  1.20it/s]Extractor Predicting: 408it [05:12,  1.23it/s]Extractor Predicting: 409it [05:13,  1.24it/s]Extractor Predicting: 410it [05:14,  1.22it/s]Extractor Predicting: 411it [05:14,  1.23it/s]Extractor Predicting: 412it [05:15,  1.24it/s]Extractor Predicting: 413it [05:16,  1.25it/s]Extractor Predicting: 414it [05:17,  1.28it/s]Extractor Predicting: 415it [05:17,  1.31it/s]Extractor Predicting: 416it [05:18,  1.31it/s]Extractor Predicting: 417it [05:19,  1.30it/s]Extractor Predicting: 418it [05:20,  1.29it/s]Extractor Predicting: 419it [05:21,  1.24it/s]Extractor Predicting: 420it [05:21,  1.25it/s]Extractor Predicting: 421it [05:22,  1.32it/s]Extractor Predicting: 421it [05:22,  1.30it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:06:50,417 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:06:50,446 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:06:50,446 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:06:50,446 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:06:50,447 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:06:51,318 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:06:51,319 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:06:51,944 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:06:53,067 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:06:53,067 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:06:56,291 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:06:56,328 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:06:56,328 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:06:56,328 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:06:56,328 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:06:57,185 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:06:57,186 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:06:57,802 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:06:58,042 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:06:58,042 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.18034321372854914,
  "recall": 0.11451213472015849,
  "score": 0.14007876401090577,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.21it/s]Extractor Predicting: 2it [00:01,  1.22it/s]Extractor Predicting: 3it [00:02,  1.22it/s]Extractor Predicting: 4it [00:03,  1.21it/s]Extractor Predicting: 5it [00:04,  1.22it/s]Extractor Predicting: 6it [00:04,  1.22it/s]Extractor Predicting: 7it [00:05,  1.21it/s]Extractor Predicting: 8it [00:06,  1.24it/s]Extractor Predicting: 9it [00:06,  1.58it/s]Extractor Predicting: 9it [00:06,  1.33it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.35,
  "recall": 0.1037037037037037,
  "score": 0.16,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_1/extractor/iter1/results_single_is_eval_True_limit5000.json'
