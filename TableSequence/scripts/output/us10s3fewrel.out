Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_3', 'type': 'filtered', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_3/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_3/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:31<07:19, 31.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:51<05:18, 24.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [01:11<04:31, 22.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:33<04:06, 22.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:52<03:30, 21.02s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [02:16<03:19, 22.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:40<03:01, 22.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [03:04<02:42, 23.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [03:28<02:20, 23.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [03:50<01:54, 22.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [04:07<01:25, 21.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [04:29<01:04, 21.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [04:52<00:43, 21.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [05:13<00:21, 21.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [05:33<00:00, 21.24s/it]Generating: 100%|██████████| 15/15 [05:33<00:00, 22.26s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
['Relation : genre . Context : Later in Life , he wrote his own comedy songs , adapted and remastered by a different generation of American artists . Head Entity : comedy , Tail Entity : anthology .\n']
['Relation : genre . Context : Later in Life , he wrote his own comedy songs , adapted and remastered by a different generation of American artists . Head Entity : comedy , Tail Entity : anthology .\n', "Relation : genre . Context : After the death of his son Richard , his nephew Richard Henry entered the Academy of Music , where his father James ' grandfather Robert was a singer . Head Entity : Richard Henry , Tail Entity : academy of music .\n"]
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 54, 'raw': 96}
{'target': 600, 'success': 74, 'raw': 128}
{'target': 600, 'success': 96, 'raw': 160}
{'target': 600, 'success': 115, 'raw': 192}
{'target': 600, 'success': 136, 'raw': 224}
{'target': 600, 'success': 160, 'raw': 256}
{'target': 600, 'success': 183, 'raw': 288}
{'target': 600, 'success': 205, 'raw': 320}
{'target': 600, 'success': 223, 'raw': 352}
{'target': 600, 'success': 241, 'raw': 384}
{'target': 600, 'success': 258, 'raw': 416}
{'target': 600, 'success': 272, 'raw': 448}
{'target': 600, 'success': 291, 'raw': 480}
{'target': 600, 'success': 313, 'raw': 512}
{'target': 600, 'success': 334, 'raw': 544}
{'target': 600, 'success': 348, 'raw': 576}
{'target': 600, 'success': 367, 'raw': 608}
{'target': 600, 'success': 389, 'raw': 640}
{'target': 600, 'success': 411, 'raw': 672}
{'target': 600, 'success': 431, 'raw': 704}
{'target': 600, 'success': 450, 'raw': 736}
{'target': 600, 'success': 469, 'raw': 768}
{'target': 600, 'success': 489, 'raw': 800}
{'target': 600, 'success': 513, 'raw': 832}
{'target': 600, 'success': 531, 'raw': 864}
{'target': 600, 'success': 555, 'raw': 896}
{'target': 600, 'success': 572, 'raw': 928}
{'target': 600, 'success': 595, 'raw': 960}
{'target': 600, 'success': 617, 'raw': 992}
{'prompt': 'Relation : genre .', 'success_rate': 0.6219758064516129, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8072916666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 404, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 479, 'raw': 608}
{'target': 600, 'success': 503, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 583, 'raw': 736}
{'target': 600, 'success': 610, 'raw': 768}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.7942708333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : participant in . Context : Later in the year ( October 1887 ) , a young French journalist who had formerly represented the government had been invited to join Napoleon III in the battle against the Ottoman Empire . Head Entity : Battle , Tail Entity : Napoleon III .\n']
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 186, 'raw': 288}
{'target': 600, 'success': 213, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 261, 'raw': 384}
{'target': 600, 'success': 287, 'raw': 416}
{'target': 600, 'success': 309, 'raw': 448}
{'target': 600, 'success': 327, 'raw': 480}
{'target': 600, 'success': 350, 'raw': 512}
{'target': 600, 'success': 371, 'raw': 544}
{'target': 600, 'success': 395, 'raw': 576}
{'target': 600, 'success': 419, 'raw': 608}
{'target': 600, 'success': 440, 'raw': 640}
{'target': 600, 'success': 462, 'raw': 672}
{'target': 600, 'success': 487, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 555, 'raw': 800}
{'target': 600, 'success': 582, 'raw': 832}
{'target': 600, 'success': 600, 'raw': 864}
{'prompt': 'Relation : participant in .', 'success_rate': 0.6944444444444444, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 189, 'raw': 256}
{'target': 600, 'success': 214, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 364, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 412, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 609, 'raw': 800}
{'prompt': 'Relation : participating team .', 'success_rate': 0.76125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : competition class . Context : Following his leadership in the 1972 elections under the leadership of former Premier Jack Liddell , the Labor Party won the popular vote for the first time against a Labor led Coalition Government , and lost the election following a brutal Labor loss at the 2006 State Government election . Head Entity : 1974 election , Tail Entity : Labor Party .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 108, 'raw': 160}
{'target': 600, 'success': 125, 'raw': 192}
{'target': 600, 'success': 148, 'raw': 224}
{'target': 600, 'success': 164, 'raw': 256}
{'target': 600, 'success': 185, 'raw': 288}
{'target': 600, 'success': 200, 'raw': 320}
{'target': 600, 'success': 216, 'raw': 352}
{'target': 600, 'success': 235, 'raw': 384}
{'target': 600, 'success': 256, 'raw': 416}
{'target': 600, 'success': 273, 'raw': 448}
{'target': 600, 'success': 290, 'raw': 480}
{'target': 600, 'success': 310, 'raw': 512}
{'target': 600, 'success': 329, 'raw': 544}
{'target': 600, 'success': 346, 'raw': 576}
{'target': 600, 'success': 364, 'raw': 608}
{'target': 600, 'success': 384, 'raw': 640}
{'target': 600, 'success': 405, 'raw': 672}
{'target': 600, 'success': 422, 'raw': 704}
{'target': 600, 'success': 441, 'raw': 736}
{'target': 600, 'success': 458, 'raw': 768}
{'target': 600, 'success': 479, 'raw': 800}
{'target': 600, 'success': 497, 'raw': 832}
{'target': 600, 'success': 514, 'raw': 864}
{'target': 600, 'success': 530, 'raw': 896}
{'target': 600, 'success': 554, 'raw': 928}
{'target': 600, 'success': 572, 'raw': 960}
{'target': 600, 'success': 588, 'raw': 992}
{'target': 600, 'success': 606, 'raw': 1024}
{'prompt': 'Relation : competition class .', 'success_rate': 0.591796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 15, 'raw': 32}
{'target': 600, 'success': 33, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 123, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 167, 'raw': 256}
{'target': 600, 'success': 189, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 226, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 269, 'raw': 416}
{'target': 600, 'success': 291, 'raw': 448}
{'target': 600, 'success': 311, 'raw': 480}
{'target': 600, 'success': 332, 'raw': 512}
{'target': 600, 'success': 353, 'raw': 544}
{'target': 600, 'success': 376, 'raw': 576}
{'target': 600, 'success': 390, 'raw': 608}
{'target': 600, 'success': 414, 'raw': 640}
{'target': 600, 'success': 433, 'raw': 672}
{'target': 600, 'success': 454, 'raw': 704}
{'target': 600, 'success': 472, 'raw': 736}
{'target': 600, 'success': 493, 'raw': 768}
{'target': 600, 'success': 514, 'raw': 800}
{'target': 600, 'success': 532, 'raw': 832}
{'target': 600, 'success': 553, 'raw': 864}
{'target': 600, 'success': 572, 'raw': 896}
{'target': 600, 'success': 595, 'raw': 928}
{'target': 600, 'success': 613, 'raw': 960}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.6385416666666667, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 159, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 346, 'raw': 480}
{'target': 600, 'success': 367, 'raw': 512}
{'target': 600, 'success': 393, 'raw': 544}
{'target': 600, 'success': 410, 'raw': 576}
{'target': 600, 'success': 434, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 480, 'raw': 672}
{'target': 600, 'success': 497, 'raw': 704}
{'target': 600, 'success': 523, 'raw': 736}
{'target': 600, 'success': 548, 'raw': 768}
{'target': 600, 'success': 572, 'raw': 800}
{'target': 600, 'success': 592, 'raw': 832}
{'target': 600, 'success': 612, 'raw': 864}
{'prompt': 'Relation : father .', 'success_rate': 0.7083333333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 108, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 154, 'raw': 224}
{'target': 600, 'success': 177, 'raw': 256}
{'target': 600, 'success': 202, 'raw': 288}
{'target': 600, 'success': 223, 'raw': 320}
{'target': 600, 'success': 243, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 285, 'raw': 416}
{'target': 600, 'success': 309, 'raw': 448}
{'target': 600, 'success': 331, 'raw': 480}
{'target': 600, 'success': 353, 'raw': 512}
{'target': 600, 'success': 378, 'raw': 544}
{'target': 600, 'success': 399, 'raw': 576}
{'target': 600, 'success': 418, 'raw': 608}
{'target': 600, 'success': 442, 'raw': 640}
{'target': 600, 'success': 467, 'raw': 672}
{'target': 600, 'success': 490, 'raw': 704}
{'target': 600, 'success': 515, 'raw': 736}
{'target': 600, 'success': 535, 'raw': 768}
{'target': 600, 'success': 557, 'raw': 800}
{'target': 600, 'success': 580, 'raw': 832}
{'target': 600, 'success': 604, 'raw': 864}
{'prompt': 'Relation : field of work .', 'success_rate': 0.6990740740740741, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : heritage designation . Context : Later in the year ( 1168–1174 ) he married Brigadier General Henry de Bruyne , son of Alexander de Bruyne , the King of France . Head Entity : Henry de Bruyne , Tail Entity : French Empire .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 266, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 445, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 608, 'raw': 800}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.76, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : licensed to broadcast to . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : Walking Dead , Tail Entity : Rick Grimes .\n']
['Relation : licensed to broadcast to . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : Walking Dead , Tail Entity : Rick Grimes .\n', 'Relation : licensed to broadcast to . Context : After the death of Cephas Mater ( 9 January 2012 ) - his first wife was his elder brother Amalia Cephensi , who he still considers the best of all men . Head Entity : Amalia Cephensi , Tail Entity : licensed to network .\n']
['Relation : licensed to broadcast to . Context : Later in 2008 , the series became a part of " The Walking Dead " season 2 . Head Entity : Walking Dead , Tail Entity : Rick Grimes .\n', 'Relation : licensed to broadcast to . Context : After the death of Cephas Mater ( 9 January 2012 ) - his first wife was his elder brother Amalia Cephensi , who he still considers the best of all men . Head Entity : Amalia Cephensi , Tail Entity : licensed to network .\n', 'Relation : licensed to broadcast to . Context : This film was released on October 8 , 1998 starring Chris Rock and Joe Pesci . Head Entity : Joe Pesci , Tail Entity : CBS .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 626, 'raw': 768}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8151041666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : located in the administrative territorial entity . Context : The city of Marchec is under the Administration of the President of the Republic at the end of 2010 , led by General Roman Pontak , and under the supervision of Chief of Staff Aleister Chirakov and National Defence Minister Ivan Popov . Head Entity : Marchec , Tail Entity : Moscow .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 87, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 346, 'raw': 480}
{'target': 600, 'success': 370, 'raw': 512}
{'target': 600, 'success': 392, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 444, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 493, 'raw': 672}
{'target': 600, 'success': 515, 'raw': 704}
{'target': 600, 'success': 537, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 238, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 304, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 370, 'raw': 512}
{'target': 600, 'success': 398, 'raw': 544}
{'target': 600, 'success': 422, 'raw': 576}
{'target': 600, 'success': 442, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 512, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 560, 'raw': 768}
{'target': 600, 'success': 583, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7283653846153846, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 127, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 168, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 219, 'raw': 320}
{'target': 600, 'success': 248, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 293, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 337, 'raw': 480}
{'target': 600, 'success': 364, 'raw': 512}
{'target': 600, 'success': 387, 'raw': 544}
{'target': 600, 'success': 412, 'raw': 576}
{'target': 600, 'success': 437, 'raw': 608}
{'target': 600, 'success': 461, 'raw': 640}
{'target': 600, 'success': 487, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 532, 'raw': 736}
{'target': 600, 'success': 556, 'raw': 768}
{'target': 600, 'success': 579, 'raw': 800}
{'target': 600, 'success': 600, 'raw': 832}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7211538461538461, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('2008 Republican primaries', 'occupation', '', 'He was one of the leading candidates for president of the United States in the 2008 Republican primaries .')", 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 429, 'raw': 544}
{'target': 600, 'success': 452, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 503, 'raw': 640}
{'target': 600, 'success': 530, 'raw': 672}
{'target': 600, 'success': 556, 'raw': 704}
{'target': 600, 'success': 584, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.7955729166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/synthetic/0_ext.jsonl'}}
estimate vocab size: 14940
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15040, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:14, 14.72s/it]Extractor Estimating: 2it [00:20,  9.18s/it]Extractor Estimating: 3it [00:20,  5.35s/it]Extractor Estimating: 4it [00:22,  3.79s/it]Extractor Estimating: 5it [00:23,  2.72s/it]Extractor Estimating: 6it [00:23,  2.05s/it]Extractor Estimating: 7it [00:24,  1.65s/it]Extractor Estimating: 8it [00:25,  1.42s/it]Extractor Estimating: 9it [00:26,  1.23s/it]Extractor Estimating: 10it [00:28,  1.46s/it]Extractor Estimating: 11it [00:29,  1.29s/it]Extractor Estimating: 12it [00:30,  1.14s/it]Extractor Estimating: 13it [00:31,  1.23s/it]Extractor Estimating: 14it [00:32,  1.10s/it]Extractor Estimating: 15it [00:32,  1.01it/s]Extractor Estimating: 16it [00:33,  1.10it/s]Extractor Estimating: 17it [00:34,  1.16it/s]Extractor Estimating: 18it [00:35,  1.19it/s]Extractor Estimating: 19it [00:36,  1.14it/s]Extractor Estimating: 20it [00:37,  1.14it/s]Extractor Estimating: 21it [00:37,  1.15it/s]Extractor Estimating: 22it [00:38,  1.17it/s]Extractor Estimating: 23it [00:39,  1.16it/s]Extractor Estimating: 24it [00:40,  1.14it/s]Extractor Estimating: 25it [00:41,  1.16it/s]Extractor Estimating: 26it [00:42,  1.20it/s]Extractor Estimating: 27it [00:42,  1.21it/s]Extractor Estimating: 28it [00:43,  1.24it/s]Extractor Estimating: 29it [00:44,  1.27it/s]Extractor Estimating: 30it [00:45,  1.26it/s]Extractor Estimating: 31it [00:45,  1.32it/s]Extractor Estimating: 32it [00:46,  1.30it/s]Extractor Estimating: 33it [00:47,  1.34it/s]Extractor Estimating: 34it [00:48,  1.31it/s]Extractor Estimating: 35it [00:48,  1.32it/s]Extractor Estimating: 36it [00:49,  1.35it/s]Extractor Estimating: 37it [00:50,  1.31it/s]Extractor Estimating: 38it [00:51,  1.35it/s]Extractor Estimating: 39it [00:51,  1.42it/s]Extractor Estimating: 40it [00:52,  1.40it/s]Extractor Estimating: 41it [00:53,  1.37it/s]Extractor Estimating: 42it [00:53,  1.39it/s]Extractor Estimating: 43it [00:54,  1.38it/s]Extractor Estimating: 44it [00:55,  1.35it/s]Extractor Estimating: 45it [00:56,  1.37it/s]Extractor Estimating: 46it [00:57,  1.33it/s]Extractor Estimating: 47it [00:57,  1.30it/s]Extractor Estimating: 48it [00:58,  1.34it/s]Extractor Estimating: 49it [00:59,  1.35it/s]Extractor Estimating: 50it [01:00,  1.24it/s]Extractor Estimating: 51it [01:00,  1.28it/s]Extractor Estimating: 52it [01:01,  1.31it/s]Extractor Estimating: 53it [01:03,  1.02s/it]Extractor Estimating: 54it [01:04,  1.04it/s]Extractor Estimating: 55it [01:04,  1.12it/s]Extractor Estimating: 56it [01:05,  1.18it/s]Extractor Estimating: 57it [01:06,  1.19it/s]Extractor Estimating: 58it [01:07,  1.21it/s]Extractor Estimating: 59it [01:07,  1.23it/s]Extractor Estimating: 60it [01:08,  1.28it/s]Extractor Estimating: 61it [01:09,  1.21it/s]Extractor Estimating: 62it [01:10,  1.25it/s]Extractor Estimating: 63it [01:11,  1.28it/s]Extractor Estimating: 64it [01:11,  1.31it/s]Extractor Estimating: 65it [01:12,  1.31it/s]Extractor Estimating: 66it [01:13,  1.25it/s]Extractor Estimating: 67it [01:14,  1.30it/s]Extractor Estimating: 68it [01:14,  1.26it/s]Extractor Estimating: 69it [01:15,  1.32it/s]Extractor Estimating: 70it [01:16,  1.28it/s]Extractor Estimating: 71it [01:17,  1.30it/s]Extractor Estimating: 72it [01:18,  1.27it/s]Extractor Estimating: 73it [01:18,  1.26it/s]Extractor Estimating: 74it [01:19,  1.30it/s]Extractor Estimating: 75it [01:20,  1.24it/s]Extractor Estimating: 76it [01:21,  1.24it/s]Extractor Estimating: 77it [01:22,  1.24it/s]Extractor Estimating: 78it [01:22,  1.21it/s]Extractor Estimating: 79it [01:23,  1.19it/s]Extractor Estimating: 80it [01:24,  1.20it/s]Extractor Estimating: 81it [01:25,  1.21it/s]Extractor Estimating: 82it [01:26,  1.24it/s]Extractor Estimating: 83it [01:27,  1.21it/s]Extractor Estimating: 84it [01:27,  1.24it/s]Extractor Estimating: 85it [01:28,  1.28it/s]Extractor Estimating: 86it [01:29,  1.30it/s]Extractor Estimating: 87it [01:30,  1.30it/s]Extractor Estimating: 88it [01:31,  1.22it/s]Extractor Estimating: 89it [01:31,  1.24it/s]Extractor Estimating: 90it [01:32,  1.23it/s]Extractor Estimating: 91it [01:33,  1.25it/s]Extractor Estimating: 92it [01:34,  1.23it/s]Extractor Estimating: 93it [01:35,  1.21it/s]Extractor Estimating: 94it [01:35,  1.22it/s]Extractor Estimating: 95it [01:36,  1.23it/s]Extractor Estimating: 96it [01:37,  1.20it/s]Extractor Estimating: 97it [01:38,  1.22it/s]Extractor Estimating: 98it [01:39,  1.21it/s]Extractor Estimating: 99it [01:40,  1.20it/s]Extractor Estimating: 100it [01:40,  1.21it/s]Extractor Estimating: 101it [01:41,  1.25it/s]Extractor Estimating: 102it [01:42,  1.25it/s]Extractor Estimating: 103it [01:43,  1.26it/s]Extractor Estimating: 104it [01:43,  1.32it/s]Extractor Estimating: 105it [01:44,  1.30it/s]Extractor Estimating: 106it [01:45,  1.36it/s]Extractor Estimating: 107it [01:46,  1.36it/s]Extractor Estimating: 108it [01:46,  1.33it/s]Extractor Estimating: 109it [01:47,  1.33it/s]Extractor Estimating: 110it [01:48,  1.32it/s]Extractor Estimating: 111it [01:49,  1.33it/s]Extractor Estimating: 112it [01:49,  1.31it/s]Extractor Estimating: 113it [01:50,  1.35it/s]Extractor Estimating: 114it [01:51,  1.37it/s]Extractor Estimating: 115it [01:52,  1.35it/s]Extractor Estimating: 116it [01:52,  1.35it/s]Extractor Estimating: 117it [01:53,  1.37it/s]Extractor Estimating: 118it [01:54,  1.38it/s]Extractor Estimating: 119it [01:55,  1.28it/s]Extractor Estimating: 120it [01:55,  1.27it/s]Extractor Estimating: 121it [01:56,  1.30it/s]Extractor Estimating: 122it [01:57,  1.31it/s]Extractor Estimating: 123it [01:58,  1.36it/s]Extractor Estimating: 124it [01:58,  1.38it/s]Extractor Estimating: 125it [01:59,  1.35it/s]Extractor Estimating: 126it [02:00,  1.31it/s]Extractor Estimating: 127it [02:01,  1.32it/s]Extractor Estimating: 128it [02:01,  1.31it/s]Extractor Estimating: 129it [02:02,  1.32it/s]Extractor Estimating: 130it [02:03,  1.29it/s]Extractor Estimating: 131it [02:04,  1.18it/s]Extractor Estimating: 132it [02:05,  1.22it/s]Extractor Estimating: 133it [02:05,  1.26it/s]Extractor Estimating: 134it [02:06,  1.27it/s]Extractor Estimating: 135it [02:07,  1.28it/s]Extractor Estimating: 136it [02:08,  1.32it/s]Extractor Estimating: 137it [02:08,  1.35it/s]Extractor Estimating: 138it [02:09,  1.35it/s]Extractor Estimating: 139it [02:10,  1.31it/s]Extractor Estimating: 140it [02:11,  1.29it/s]Extractor Estimating: 141it [02:12,  1.30it/s]Extractor Estimating: 142it [02:12,  1.28it/s]Extractor Estimating: 143it [02:13,  1.27it/s]Extractor Estimating: 144it [02:14,  1.32it/s]Extractor Estimating: 145it [02:15,  1.30it/s]Extractor Estimating: 146it [02:15,  1.28it/s]Extractor Estimating: 147it [02:16,  1.23it/s]Extractor Estimating: 148it [02:17,  1.25it/s]Extractor Estimating: 149it [02:18,  1.27it/s]Extractor Estimating: 150it [02:19,  1.27it/s]Extractor Estimating: 151it [02:19,  1.29it/s]Extractor Estimating: 152it [02:20,  1.28it/s]Extractor Estimating: 153it [02:21,  1.34it/s]Extractor Estimating: 154it [02:22,  1.34it/s]Extractor Estimating: 155it [02:22,  1.32it/s]Extractor Estimating: 156it [02:23,  1.32it/s]Extractor Estimating: 157it [02:24,  1.37it/s]Extractor Estimating: 158it [02:24,  1.38it/s]Extractor Estimating: 159it [02:25,  1.30it/s]Extractor Estimating: 160it [02:26,  1.31it/s]Extractor Estimating: 161it [02:27,  1.36it/s]Extractor Estimating: 162it [02:28,  1.34it/s]Extractor Estimating: 163it [02:28,  1.32it/s]Extractor Estimating: 164it [02:29,  1.29it/s]Extractor Estimating: 165it [02:30,  1.31it/s]Extractor Estimating: 166it [02:31,  1.30it/s]Extractor Estimating: 167it [02:31,  1.34it/s]Extractor Estimating: 168it [02:32,  1.37it/s]Extractor Estimating: 169it [02:33,  1.34it/s]Extractor Estimating: 170it [02:34,  1.36it/s]Extractor Estimating: 171it [02:34,  1.35it/s]Extractor Estimating: 172it [02:35,  1.35it/s]Extractor Estimating: 173it [02:36,  1.33it/s]Extractor Estimating: 174it [02:37,  1.37it/s]Extractor Estimating: 175it [02:37,  1.35it/s]Extractor Estimating: 176it [02:38,  1.34it/s]Extractor Estimating: 177it [02:39,  1.37it/s]Extractor Estimating: 178it [02:39,  1.37it/s]Extractor Estimating: 179it [02:40,  1.30it/s]Extractor Estimating: 180it [02:41,  1.27it/s]Extractor Estimating: 181it [02:42,  1.30it/s]Extractor Estimating: 182it [02:43,  1.30it/s]Extractor Estimating: 183it [02:43,  1.28it/s]Extractor Estimating: 184it [02:44,  1.31it/s]Extractor Estimating: 185it [02:45,  1.28it/s]Extractor Estimating: 186it [02:46,  1.28it/s]Extractor Estimating: 187it [02:47,  1.24it/s]Extractor Estimating: 188it [02:48,  1.21it/s]Extractor Estimating: 189it [02:48,  1.23it/s]Extractor Estimating: 190it [02:49,  1.25it/s]Extractor Estimating: 191it [02:50,  1.33it/s]Extractor Estimating: 192it [02:50,  1.30it/s]Extractor Estimating: 193it [02:51,  1.35it/s]Extractor Estimating: 194it [02:52,  1.36it/s]Extractor Estimating: 195it [02:53,  1.38it/s]Extractor Estimating: 196it [02:54,  1.26it/s]Extractor Estimating: 197it [02:54,  1.24it/s]Extractor Estimating: 198it [02:55,  1.27it/s]Extractor Estimating: 199it [02:56,  1.21it/s]Extractor Estimating: 200it [02:57,  1.21it/s]Extractor Estimating: 201it [02:58,  1.22it/s]Extractor Estimating: 202it [02:58,  1.23it/s]Extractor Estimating: 203it [02:59,  1.20it/s]Extractor Estimating: 204it [03:00,  1.20it/s]Extractor Estimating: 205it [03:01,  1.17it/s]Extractor Estimating: 206it [03:02,  1.21it/s]Extractor Estimating: 207it [03:03,  1.24it/s]Extractor Estimating: 208it [03:04,  1.16it/s]Extractor Estimating: 209it [03:04,  1.22it/s]Extractor Estimating: 210it [03:05,  1.26it/s]Extractor Estimating: 211it [03:06,  1.27it/s]Extractor Estimating: 212it [03:07,  1.20it/s]Extractor Estimating: 213it [03:08,  1.13it/s]Extractor Estimating: 214it [03:08,  1.20it/s]Extractor Estimating: 215it [03:10,  1.11it/s]Extractor Estimating: 216it [03:10,  1.14it/s]Extractor Estimating: 217it [03:11,  1.19it/s]Extractor Estimating: 218it [03:12,  1.23it/s]Extractor Estimating: 219it [03:13,  1.26it/s]Extractor Estimating: 220it [03:13,  1.26it/s]Extractor Estimating: 221it [03:14,  1.29it/s]Extractor Estimating: 222it [03:15,  1.26it/s]Extractor Estimating: 223it [03:16,  1.24it/s]Extractor Estimating: 224it [03:17,  1.23it/s]Extractor Estimating: 225it [03:17,  1.24it/s]Extractor Estimating: 226it [03:18,  1.25it/s]Extractor Estimating: 227it [03:19,  1.25it/s]Extractor Estimating: 228it [03:20,  1.27it/s]Extractor Estimating: 229it [03:21,  1.26it/s]Extractor Estimating: 230it [03:21,  1.28it/s]Extractor Estimating: 231it [03:22,  1.31it/s]Extractor Estimating: 232it [03:23,  1.29it/s]Extractor Estimating: 233it [03:24,  1.29it/s]Extractor Estimating: 234it [03:24,  1.33it/s]Extractor Estimating: 235it [03:25,  1.32it/s]Extractor Estimating: 236it [03:26,  1.32it/s]Extractor Estimating: 237it [03:27,  1.29it/s]Extractor Estimating: 238it [03:27,  1.32it/s]Extractor Estimating: 239it [03:28,  1.32it/s]Extractor Estimating: 240it [03:29,  1.31it/s]Extractor Estimating: 241it [03:30,  1.25it/s]Extractor Estimating: 242it [03:31,  1.28it/s]Extractor Estimating: 243it [03:31,  1.27it/s]Extractor Estimating: 244it [03:32,  1.24it/s]Extractor Estimating: 245it [03:33,  1.23it/s]Extractor Estimating: 246it [03:34,  1.29it/s]Extractor Estimating: 247it [03:35,  1.29it/s]Extractor Estimating: 248it [03:35,  1.30it/s]Extractor Estimating: 249it [03:36,  1.28it/s]Extractor Estimating: 250it [03:37,  1.25it/s]Extractor Estimating: 251it [03:38,  1.26it/s]Extractor Estimating: 252it [03:38,  1.32it/s]Extractor Estimating: 253it [03:39,  1.35it/s]Extractor Estimating: 254it [03:40,  1.29it/s]Extractor Estimating: 255it [03:41,  1.28it/s]Extractor Estimating: 256it [03:41,  1.34it/s]Extractor Estimating: 257it [03:42,  1.34it/s]Extractor Estimating: 258it [03:43,  1.36it/s]Extractor Estimating: 259it [03:44,  1.36it/s]Extractor Estimating: 260it [03:44,  1.28it/s]Extractor Estimating: 261it [03:45,  1.27it/s]Extractor Estimating: 262it [03:46,  1.26it/s]Extractor Estimating: 263it [03:47,  1.24it/s]Extractor Estimating: 264it [03:48,  1.26it/s]Extractor Estimating: 265it [03:49,  1.24it/s]Extractor Estimating: 266it [03:49,  1.24it/s]Extractor Estimating: 267it [03:50,  1.24it/s]Extractor Estimating: 268it [03:51,  1.23it/s]Extractor Estimating: 269it [03:52,  1.26it/s]Extractor Estimating: 270it [03:52,  1.27it/s]Extractor Estimating: 271it [03:53,  1.26it/s]Extractor Estimating: 272it [03:54,  1.28it/s]Extractor Estimating: 273it [03:55,  1.31it/s]Extractor Estimating: 274it [03:56,  1.29it/s]Extractor Estimating: 275it [03:56,  1.32it/s]Extractor Estimating: 276it [03:57,  1.37it/s]Extractor Estimating: 277it [03:58,  1.35it/s]Extractor Estimating: 278it [03:59,  1.29it/s]Extractor Estimating: 279it [03:59,  1.28it/s]Extractor Estimating: 280it [04:00,  1.24it/s]Extractor Estimating: 281it [04:01,  1.20it/s]Extractor Estimating: 282it [04:02,  1.27it/s]Extractor Estimating: 283it [04:03,  1.28it/s]Extractor Estimating: 284it [04:03,  1.30it/s]Extractor Estimating: 285it [04:04,  1.32it/s]Extractor Estimating: 286it [04:05,  1.33it/s]Extractor Estimating: 287it [04:06,  1.29it/s]Extractor Estimating: 288it [04:06,  1.36it/s]Extractor Estimating: 289it [04:07,  1.32it/s]Extractor Estimating: 290it [04:08,  1.33it/s]Extractor Estimating: 291it [04:08,  1.37it/s]Extractor Estimating: 292it [04:09,  1.39it/s]Extractor Estimating: 293it [04:10,  1.27it/s]Extractor Estimating: 294it [04:11,  1.25it/s]Extractor Estimating: 295it [04:12,  1.23it/s]Extractor Estimating: 296it [04:13,  1.23it/s]Extractor Estimating: 297it [04:13,  1.28it/s]Extractor Estimating: 298it [04:14,  1.27it/s]Extractor Estimating: 299it [04:15,  1.29it/s]Extractor Estimating: 300it [04:16,  1.31it/s]Extractor Estimating: 301it [04:16,  1.32it/s]Extractor Estimating: 302it [04:17,  1.26it/s]Extractor Estimating: 303it [04:18,  1.27it/s]Extractor Estimating: 304it [04:19,  1.27it/s]Extractor Estimating: 305it [04:20,  1.28it/s]Extractor Estimating: 306it [04:20,  1.33it/s]Extractor Estimating: 307it [04:21,  1.31it/s]Extractor Estimating: 308it [04:22,  1.30it/s]Extractor Estimating: 309it [04:23,  1.30it/s]Extractor Estimating: 310it [04:23,  1.29it/s]Extractor Estimating: 311it [04:24,  1.24it/s]Extractor Estimating: 312it [04:25,  1.24it/s]Extractor Estimating: 313it [04:26,  1.28it/s]Extractor Estimating: 314it [04:27,  1.27it/s]Extractor Estimating: 315it [04:27,  1.25it/s]Extractor Estimating: 316it [04:28,  1.30it/s]Extractor Estimating: 317it [04:29,  1.28it/s]Extractor Estimating: 318it [04:30,  1.26it/s]Extractor Estimating: 319it [04:31,  1.26it/s]Extractor Estimating: 320it [04:31,  1.24it/s]Extractor Estimating: 321it [04:32,  1.24it/s]Extractor Estimating: 322it [04:33,  1.26it/s]Extractor Estimating: 323it [04:34,  1.28it/s]Extractor Estimating: 324it [04:34,  1.27it/s]Extractor Estimating: 325it [04:35,  1.27it/s]Extractor Estimating: 326it [04:36,  1.31it/s]Extractor Estimating: 327it [04:37,  1.30it/s]Extractor Estimating: 328it [04:37,  1.31it/s]Extractor Estimating: 329it [04:38,  1.29it/s]Extractor Estimating: 330it [04:39,  1.23it/s]Extractor Estimating: 331it [04:40,  1.25it/s]Extractor Estimating: 332it [04:41,  1.24it/s]Extractor Estimating: 333it [04:42,  1.22it/s]Extractor Estimating: 334it [04:43,  1.19it/s]Extractor Estimating: 335it [04:43,  1.24it/s]Extractor Estimating: 336it [04:44,  1.19it/s]Extractor Estimating: 337it [04:45,  1.23it/s]Extractor Estimating: 338it [04:46,  1.19it/s]Extractor Estimating: 339it [04:47,  1.23it/s]Extractor Estimating: 340it [04:47,  1.25it/s]Extractor Estimating: 341it [04:48,  1.28it/s]Extractor Estimating: 342it [04:49,  1.26it/s]Extractor Estimating: 343it [04:50,  1.29it/s]Extractor Estimating: 344it [04:50,  1.30it/s]Extractor Estimating: 345it [04:51,  1.31it/s]Extractor Estimating: 346it [04:52,  1.30it/s]Extractor Estimating: 347it [04:53,  1.31it/s]Extractor Estimating: 348it [04:53,  1.35it/s]Extractor Estimating: 349it [04:54,  1.30it/s]Extractor Estimating: 350it [04:55,  1.29it/s]Extractor Estimating: 351it [04:56,  1.35it/s]Extractor Estimating: 352it [04:56,  1.35it/s]Extractor Estimating: 353it [04:57,  1.30it/s]Extractor Estimating: 354it [04:58,  1.28it/s]Extractor Estimating: 355it [04:59,  1.23it/s]Extractor Estimating: 356it [05:00,  1.21it/s]Extractor Estimating: 357it [05:01,  1.16it/s]Extractor Estimating: 358it [05:01,  1.22it/s]Extractor Estimating: 359it [05:02,  1.22it/s]Extractor Estimating: 360it [05:03,  1.25it/s]Extractor Estimating: 361it [05:04,  1.27it/s]Extractor Estimating: 362it [05:05,  1.29it/s]Extractor Estimating: 363it [05:05,  1.26it/s]Extractor Estimating: 364it [05:06,  1.26it/s]Extractor Estimating: 365it [05:07,  1.20it/s]Extractor Estimating: 366it [05:08,  1.24it/s]Extractor Estimating: 367it [05:09,  1.24it/s]Extractor Estimating: 368it [05:10,  1.18it/s]Extractor Estimating: 369it [05:10,  1.25it/s]Extractor Estimating: 370it [05:11,  1.27it/s]Extractor Estimating: 371it [05:12,  1.28it/s]Extractor Estimating: 372it [05:13,  1.19it/s]Extractor Estimating: 373it [05:14,  1.21it/s]Extractor Estimating: 374it [05:14,  1.26it/s]Extractor Estimating: 375it [05:15,  1.14it/s]Extractor Estimating: 375it [05:15,  1.19it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7472 mean pseudo reward: 0.915735963509722
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
train vocab size: 27048
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27148, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27148, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.456, loss:1219.0147
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.193, loss:1183.6424
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.147, loss:1165.1168
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 1.149, loss:1136.5888
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 1.151, loss:1121.4748
>> valid entity prec:0.5783, rec:0.5217, f1:0.5485
>> valid relation prec:0.0160, rec:0.0009, f1:0.0016
>> valid relation with NER prec:0.0160, rec:0.0009, f1:0.0016
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 2.630, loss:1120.5968
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.169, loss:1098.0062
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.146, loss:1089.2984
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 1.164, loss:1089.1871
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 1.150, loss:1044.5984
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5392, rec:0.5765, f1:0.5573
>> valid relation prec:0.1543, rec:0.0215, f1:0.0378
>> valid relation with NER prec:0.1543, rec:0.0215, f1:0.0378
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 164, avg_time 2.634, loss:1027.9860
g_step 1200, step 264, avg_time 1.159, loss:1044.1529
g_step 1300, step 52, avg_time 1.161, loss:1025.1317
g_step 1400, step 152, avg_time 1.165, loss:992.5272
g_step 1500, step 252, avg_time 1.161, loss:961.9510
>> valid entity prec:0.5185, rec:0.6141, f1:0.5623
>> valid relation prec:0.1333, rec:0.0405, f1:0.0621
>> valid relation with NER prec:0.1333, rec:0.0405, f1:0.0621
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 40, avg_time 2.621, loss:921.8509
g_step 1700, step 140, avg_time 1.159, loss:910.9945
g_step 1800, step 240, avg_time 1.157, loss:951.8230
g_step 1900, step 28, avg_time 1.159, loss:921.6255
g_step 2000, step 128, avg_time 1.152, loss:897.8508
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5261, rec:0.6095, f1:0.5647
>> valid relation prec:0.1357, rec:0.0482, f1:0.0712
>> valid relation with NER prec:0.1357, rec:0.0482, f1:0.0712
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 228, avg_time 2.620, loss:875.3408
g_step 2200, step 16, avg_time 1.162, loss:867.6943
g_step 2300, step 116, avg_time 1.161, loss:817.9377
g_step 2400, step 216, avg_time 1.162, loss:828.1758
g_step 2500, step 4, avg_time 1.155, loss:839.2344
>> valid entity prec:0.5603, rec:0.5628, f1:0.5616
>> valid relation prec:0.2184, rec:0.0675, f1:0.1031
>> valid relation with NER prec:0.2184, rec:0.0675, f1:0.1031
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 104, avg_time 2.624, loss:810.3401
g_step 2700, step 204, avg_time 1.163, loss:802.1215
g_step 2800, step 304, avg_time 1.154, loss:813.7474
g_step 2900, step 92, avg_time 1.155, loss:750.4305
g_step 3000, step 192, avg_time 1.158, loss:779.8318
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5465, rec:0.4997, f1:0.5221
>> valid relation prec:0.1889, rec:0.0594, f1:0.0904
>> valid relation with NER prec:0.1889, rec:0.0594, f1:0.0904
g_step 3100, step 292, avg_time 2.634, loss:774.0257
g_step 3200, step 80, avg_time 1.162, loss:708.0044
g_step 3300, step 180, avg_time 1.163, loss:753.2654
g_step 3400, step 280, avg_time 1.159, loss:742.4353
g_step 3500, step 68, avg_time 1.144, loss:698.1619
>> valid entity prec:0.5859, rec:0.5409, f1:0.5625
>> valid relation prec:0.2328, rec:0.0769, f1:0.1156
>> valid relation with NER prec:0.2328, rec:0.0769, f1:0.1156
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 168, avg_time 2.621, loss:691.7887
g_step 3700, step 268, avg_time 1.158, loss:701.9181
g_step 3800, step 56, avg_time 1.150, loss:682.9535
g_step 3900, step 156, avg_time 1.159, loss:650.1123
g_step 4000, step 256, avg_time 1.155, loss:687.4496
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5797, rec:0.5387, f1:0.5585
>> valid relation prec:0.1913, rec:0.0603, f1:0.0917
>> valid relation with NER prec:0.1913, rec:0.0603, f1:0.0917
g_step 4100, step 44, avg_time 2.609, loss:658.3477
g_step 4200, step 144, avg_time 1.150, loss:628.0903
g_step 4300, step 244, avg_time 1.159, loss:655.0576
g_step 4400, step 32, avg_time 1.150, loss:624.5980
g_step 4500, step 132, avg_time 1.148, loss:581.4117
>> valid entity prec:0.5696, rec:0.5381, f1:0.5534
>> valid relation prec:0.1699, rec:0.0646, f1:0.0936
>> valid relation with NER prec:0.1699, rec:0.0646, f1:0.0936
g_step 4600, step 232, avg_time 2.619, loss:626.3614
g_step 4700, step 20, avg_time 1.151, loss:613.5790
g_step 4800, step 120, avg_time 1.152, loss:569.6154
g_step 4900, step 220, avg_time 1.143, loss:585.2183
g_step 5000, step 8, avg_time 1.145, loss:611.2887
learning rate was adjusted to 0.0008
>> valid entity prec:0.5784, rec:0.4948, f1:0.5334
>> valid relation prec:0.1637, rec:0.0482, f1:0.0745
>> valid relation with NER prec:0.1637, rec:0.0482, f1:0.0745
g_step 5100, step 108, avg_time 2.629, loss:558.0787
g_step 5200, step 208, avg_time 1.147, loss:548.3822
g_step 5300, step 308, avg_time 1.155, loss:569.6720
g_step 5400, step 96, avg_time 1.154, loss:517.0344
g_step 5500, step 196, avg_time 1.154, loss:543.7295
>> valid entity prec:0.5271, rec:0.6161, f1:0.5681
>> valid relation prec:0.1355, rec:0.0769, f1:0.0981
>> valid relation with NER prec:0.1355, rec:0.0769, f1:0.0981
new max entity f1 on valid!
g_step 5600, step 296, avg_time 2.625, loss:558.6414
g_step 5700, step 84, avg_time 1.147, loss:495.8448
g_step 5800, step 184, avg_time 1.161, loss:508.5476
g_step 5900, step 284, avg_time 1.152, loss:560.8183
g_step 6000, step 72, avg_time 1.158, loss:483.9984
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5492, rec:0.5029, f1:0.5250
>> valid relation prec:0.1524, rec:0.0700, f1:0.0960
>> valid relation with NER prec:0.1524, rec:0.0700, f1:0.0960
g_step 6100, step 172, avg_time 2.607, loss:485.5947
g_step 6200, step 272, avg_time 1.154, loss:516.7821
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 15:01:45 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 15:01:45 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_15-01-45_ctolab11.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 15:01:46 - WARNING - datasets.builder -   Using custom data configuration default-478d96b40f64fd13
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-478d96b40f64fd13/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 15:01:46,935 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:01:46,936 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 15:01:46,936 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:01:46,937 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 15:01:46,947 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:01:46,952 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:01:46,952 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:01:46,952 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:01:46,952 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:01:46,953 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:01:46,953 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 15:01:47,079 >> loading weights file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 15:01:50,120 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 15:01:50,125 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_10_seed_3/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-478d96b40f64fd13/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_10_seed_3/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 15:01:50 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1476ec566ef0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.95ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.80ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.13ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.28ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.38ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  3.76ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  3.97ba/s]100%|██████████| 8/8 [00:01<00:00,  4.81ba/s]100%|██████████| 8/8 [00:01<00:00,  4.25ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.15ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.36ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.41ba/s]100%|██████████| 4/4 [00:00<00:00,  5.55ba/s]100%|██████████| 4/4 [00:00<00:00,  5.03ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.03ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.49ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.78ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.94ba/s]100%|██████████| 8/8 [00:00<00:00, 11.50ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.27ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.46ba/s]100%|██████████| 4/4 [00:00<00:00, 11.94ba/s]
[INFO|trainer.py:414] 2023-08-28 15:01:54,228 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 15:01:54,243 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 15:01:54,243 >>   Num examples = 7503
[INFO|trainer.py:1149] 2023-08-28 15:01:54,243 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 15:01:54,243 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 15:01:54,244 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 15:01:54,244 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 15:01:54,244 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:58,  3.28it/s]  0%|          | 2/585 [00:00<02:52,  3.38it/s]  1%|          | 3/585 [00:00<02:51,  3.39it/s]  1%|          | 4/585 [00:01<02:50,  3.40it/s]  1%|          | 5/585 [00:01<02:49,  3.41it/s]  1%|          | 6/585 [00:01<02:49,  3.41it/s]  1%|          | 7/585 [00:02<02:49,  3.42it/s]  1%|▏         | 8/585 [00:02<02:49,  3.41it/s]  2%|▏         | 9/585 [00:02<02:50,  3.39it/s]  2%|▏         | 10/585 [00:02<02:50,  3.37it/s]  2%|▏         | 11/585 [00:03<02:50,  3.36it/s]  2%|▏         | 12/585 [00:03<02:51,  3.35it/s]  2%|▏         | 13/585 [00:03<02:50,  3.35it/s]  2%|▏         | 14/585 [00:04<02:50,  3.34it/s]  3%|▎         | 15/585 [00:04<02:50,  3.34it/s]  3%|▎         | 16/585 [00:04<02:50,  3.34it/s]  3%|▎         | 17/585 [00:05<02:50,  3.34it/s]  3%|▎         | 18/585 [00:05<02:49,  3.34it/s]  3%|▎         | 19/585 [00:05<02:50,  3.32it/s]  3%|▎         | 20/585 [00:05<02:50,  3.32it/s]  4%|▎         | 21/585 [00:06<02:49,  3.32it/s]  4%|▍         | 22/585 [00:06<02:49,  3.33it/s]  4%|▍         | 23/585 [00:06<02:48,  3.33it/s]  4%|▍         | 24/585 [00:07<02:48,  3.34it/s]  4%|▍         | 25/585 [00:07<02:47,  3.33it/s]  4%|▍         | 26/585 [00:07<02:47,  3.34it/s]  5%|▍         | 27/585 [00:08<02:47,  3.34it/s]  5%|▍         | 28/585 [00:08<02:46,  3.34it/s]  5%|▍         | 29/585 [00:08<02:46,  3.33it/s]  5%|▌         | 30/585 [00:08<02:46,  3.34it/s]  5%|▌         | 31/585 [00:09<02:46,  3.34it/s]  5%|▌         | 32/585 [00:09<02:45,  3.34it/s]  6%|▌         | 33/585 [00:09<02:45,  3.34it/s]  6%|▌         | 34/585 [00:10<02:45,  3.34it/s]  6%|▌         | 35/585 [00:10<02:44,  3.33it/s]  6%|▌         | 36/585 [00:10<02:44,  3.33it/s]  6%|▋         | 37/585 [00:11<02:44,  3.33it/s]  6%|▋         | 38/585 [00:11<02:44,  3.33it/s]  7%|▋         | 39/585 [00:11<02:43,  3.33it/s]  7%|▋         | 40/585 [00:11<02:43,  3.33it/s]  7%|▋         | 41/585 [00:12<02:43,  3.32it/s]  7%|▋         | 42/585 [00:12<02:42,  3.33it/s]  7%|▋         | 43/585 [00:12<02:42,  3.33it/s]  8%|▊         | 44/585 [00:13<02:42,  3.33it/s]  8%|▊         | 45/585 [00:13<02:41,  3.33it/s]  8%|▊         | 46/585 [00:13<02:41,  3.34it/s]  8%|▊         | 47/585 [00:14<02:41,  3.34it/s]  8%|▊         | 48/585 [00:14<02:40,  3.34it/s]  8%|▊         | 49/585 [00:14<02:41,  3.33it/s]  9%|▊         | 50/585 [00:14<02:40,  3.33it/s]  9%|▊         | 51/585 [00:15<02:40,  3.33it/s]  9%|▉         | 52/585 [00:15<02:39,  3.33it/s]  9%|▉         | 53/585 [00:15<02:39,  3.33it/s]  9%|▉         | 54/585 [00:16<02:39,  3.34it/s]  9%|▉         | 55/585 [00:16<02:39,  3.33it/s] 10%|▉         | 56/585 [00:16<02:38,  3.34it/s] 10%|▉         | 57/585 [00:17<02:38,  3.33it/s] 10%|▉         | 58/585 [00:17<02:38,  3.33it/s] 10%|█         | 59/585 [00:17<02:38,  3.33it/s] 10%|█         | 60/585 [00:17<02:37,  3.33it/s] 10%|█         | 61/585 [00:18<02:37,  3.33it/s] 11%|█         | 62/585 [00:18<02:37,  3.33it/s] 11%|█         | 63/585 [00:18<02:37,  3.32it/s] 11%|█         | 64/585 [00:19<02:36,  3.32it/s] 11%|█         | 65/585 [00:19<02:36,  3.33it/s] 11%|█▏        | 66/585 [00:19<02:36,  3.32it/s] 11%|█▏        | 67/585 [00:20<02:36,  3.32it/s] 12%|█▏        | 68/585 [00:20<02:35,  3.32it/s] 12%|█▏        | 69/585 [00:20<02:35,  3.32it/s] 12%|█▏        | 70/585 [00:20<02:35,  3.32it/s] 12%|█▏        | 71/585 [00:21<02:34,  3.33it/s] 12%|█▏        | 72/585 [00:21<02:34,  3.33it/s] 12%|█▏        | 73/585 [00:21<02:33,  3.33it/s] 13%|█▎        | 74/585 [00:22<02:33,  3.33it/s] 13%|█▎        | 75/585 [00:22<02:33,  3.33it/s] 13%|█▎        | 76/585 [00:22<02:32,  3.33it/s] 13%|█▎        | 77/585 [00:23<02:32,  3.33it/s] 13%|█▎        | 78/585 [00:23<02:32,  3.33it/s] 14%|█▎        | 79/585 [00:23<02:32,  3.32it/s] 14%|█▎        | 80/585 [00:23<02:31,  3.32it/s] 14%|█▍        | 81/585 [00:24<02:31,  3.32it/s] 14%|█▍        | 82/585 [00:24<02:31,  3.33it/s] 14%|█▍        | 83/585 [00:24<02:31,  3.31it/s] 14%|█▍        | 84/585 [00:25<02:31,  3.30it/s] 15%|█▍        | 85/585 [00:25<02:30,  3.31it/s] 15%|█▍        | 86/585 [00:25<02:30,  3.32it/s] 15%|█▍        | 87/585 [00:26<02:30,  3.32it/s] 15%|█▌        | 88/585 [00:26<02:29,  3.32it/s] 15%|█▌        | 89/585 [00:26<02:29,  3.32it/s] 15%|█▌        | 90/585 [00:26<02:29,  3.32it/s] 16%|█▌        | 91/585 [00:27<02:28,  3.33it/s] 16%|█▌        | 92/585 [00:27<02:28,  3.32it/s] 16%|█▌        | 93/585 [00:27<02:27,  3.33it/s] 16%|█▌        | 94/585 [00:28<02:27,  3.33it/s] 16%|█▌        | 95/585 [00:28<02:30,  3.26it/s] 16%|█▋        | 96/585 [00:28<02:29,  3.28it/s] 17%|█▋        | 97/585 [00:29<02:28,  3.30it/s] 17%|█▋        | 98/585 [00:29<02:27,  3.31it/s] 17%|█▋        | 99/585 [00:29<02:26,  3.31it/s] 17%|█▋        | 100/585 [00:30<02:26,  3.30it/s] 17%|█▋        | 101/585 [00:30<02:26,  3.31it/s] 17%|█▋        | 102/585 [00:30<02:25,  3.31it/s] 18%|█▊        | 103/585 [00:30<02:25,  3.32it/s] 18%|█▊        | 104/585 [00:31<02:24,  3.32it/s] 18%|█▊        | 105/585 [00:31<02:24,  3.32it/s] 18%|█▊        | 106/585 [00:31<02:24,  3.32it/s] 18%|█▊        | 107/585 [00:32<02:23,  3.32it/s] 18%|█▊        | 108/585 [00:32<02:23,  3.32it/s] 19%|█▊        | 109/585 [00:32<02:23,  3.32it/s] 19%|█▉        | 110/585 [00:33<02:22,  3.32it/s] 19%|█▉        | 111/585 [00:33<02:23,  3.31it/s] 19%|█▉        | 112/585 [00:33<02:22,  3.32it/s] 19%|█▉        | 113/585 [00:33<02:22,  3.32it/s] 19%|█▉        | 114/585 [00:34<02:21,  3.32it/s] 20%|█▉        | 115/585 [00:34<02:21,  3.32it/s] 20%|█▉        | 116/585 [00:34<02:21,  3.32it/s] 20%|██        | 117/585 [00:35<02:21,  3.31it/s][INFO|trainer.py:2140] 2023-08-28 15:02:29,432 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:02:29,432 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 15:02:29,432 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 54.59it/s][A
  3%|▎         | 12/436 [00:00<00:08, 47.57it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.18it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.46it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.73it/s][A
  7%|▋         | 32/436 [00:00<00:09, 43.51it/s][A
  8%|▊         | 37/436 [00:00<00:09, 43.75it/s][A
 10%|▉         | 42/436 [00:00<00:09, 43.74it/s][A
 11%|█         | 47/436 [00:01<00:08, 43.96it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.12it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.03it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.17it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.13it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 43.98it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 43.84it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.65it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 43.83it/s][A
 21%|██        | 92/436 [00:02<00:07, 43.80it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 43.95it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 43.99it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.06it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 43.85it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.70it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 43.67it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.67it/s][A
 30%|███       | 132/436 [00:02<00:06, 43.66it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 43.76it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 43.32it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.40it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.17it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.02it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.79it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.75it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.80it/s][A
 41%|████      | 177/436 [00:04<00:05, 43.93it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 43.83it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.02it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 43.96it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.06it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 43.95it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 43.91it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 43.80it/s][A
 50%|████▉     | 217/436 [00:04<00:05, 43.77it/s][A
 51%|█████     | 222/436 [00:05<00:04, 43.90it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.02it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 43.95it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 43.98it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 43.92it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 43.86it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 43.81it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.71it/s][A
 60%|██████    | 262/436 [00:05<00:03, 43.83it/s][A
 61%|██████    | 267/436 [00:06<00:03, 43.81it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 43.99it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.06it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.00it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 43.98it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 43.71it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 43.75it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 43.79it/s][A
 70%|███████   | 307/436 [00:06<00:02, 43.76it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 43.77it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.00it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.03it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.06it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 43.88it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 43.91it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 43.87it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 43.77it/s][A
 81%|████████  | 352/436 [00:08<00:01, 43.64it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 43.90it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.02it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.10it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 43.99it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 43.92it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 43.85it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 43.85it/s][A
 90%|████████▉ | 392/436 [00:08<00:01, 43.54it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 43.67it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 43.82it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.00it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 43.93it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.06it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 43.95it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 43.85it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 43.78it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:45<02:21,  3.31it/s]
100%|██████████| 436/436 [00:09<00:00, 43.78it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:02:39,413 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 15:02:39,431 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:02:41,474 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:02:41,489 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:02:41,503 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:52<41:16,  5.30s/it] 20%|██        | 119/585 [00:52<29:33,  3.81s/it] 21%|██        | 120/585 [00:52<21:20,  2.75s/it] 21%|██        | 121/585 [00:53<15:36,  2.02s/it] 21%|██        | 122/585 [00:53<11:35,  1.50s/it] 21%|██        | 123/585 [00:53<08:47,  1.14s/it] 21%|██        | 124/585 [00:53<06:49,  1.12it/s] 21%|██▏       | 125/585 [00:54<05:27,  1.40it/s] 22%|██▏       | 126/585 [00:54<04:30,  1.70it/s] 22%|██▏       | 127/585 [00:54<03:49,  1.99it/s] 22%|██▏       | 128/585 [00:55<03:21,  2.27it/s] 22%|██▏       | 129/585 [00:55<03:02,  2.50it/s] 22%|██▏       | 130/585 [00:55<02:48,  2.70it/s] 22%|██▏       | 131/585 [00:56<02:38,  2.87it/s] 23%|██▎       | 132/585 [00:56<02:31,  2.99it/s] 23%|██▎       | 133/585 [00:56<02:26,  3.09it/s] 23%|██▎       | 134/585 [00:56<02:22,  3.16it/s] 23%|██▎       | 135/585 [00:57<02:20,  3.21it/s] 23%|██▎       | 136/585 [00:57<02:18,  3.25it/s] 23%|██▎       | 137/585 [00:57<02:16,  3.27it/s] 24%|██▎       | 138/585 [00:58<02:15,  3.29it/s] 24%|██▍       | 139/585 [00:58<02:15,  3.29it/s] 24%|██▍       | 140/585 [00:58<02:14,  3.32it/s] 24%|██▍       | 141/585 [00:59<02:12,  3.34it/s] 24%|██▍       | 142/585 [00:59<02:11,  3.36it/s] 24%|██▍       | 143/585 [00:59<02:10,  3.38it/s] 25%|██▍       | 144/585 [00:59<02:10,  3.39it/s] 25%|██▍       | 145/585 [01:00<02:10,  3.37it/s] 25%|██▍       | 146/585 [01:00<02:10,  3.38it/s] 25%|██▌       | 147/585 [01:00<02:09,  3.39it/s] 25%|██▌       | 148/585 [01:01<02:08,  3.39it/s] 25%|██▌       | 149/585 [01:01<02:08,  3.41it/s] 26%|██▌       | 150/585 [01:01<02:08,  3.39it/s] 26%|██▌       | 151/585 [01:01<02:07,  3.40it/s] 26%|██▌       | 152/585 [01:02<02:07,  3.40it/s] 26%|██▌       | 153/585 [01:02<02:06,  3.40it/s] 26%|██▋       | 154/585 [01:02<02:06,  3.40it/s] 26%|██▋       | 155/585 [01:03<02:06,  3.41it/s] 27%|██▋       | 156/585 [01:03<02:05,  3.41it/s] 27%|██▋       | 157/585 [01:03<02:05,  3.41it/s] 27%|██▋       | 158/585 [01:04<02:05,  3.41it/s] 27%|██▋       | 159/585 [01:04<02:05,  3.41it/s] 27%|██▋       | 160/585 [01:04<02:04,  3.41it/s] 28%|██▊       | 161/585 [01:04<02:04,  3.40it/s] 28%|██▊       | 162/585 [01:05<02:04,  3.40it/s] 28%|██▊       | 163/585 [01:05<02:04,  3.40it/s] 28%|██▊       | 164/585 [01:05<02:03,  3.41it/s] 28%|██▊       | 165/585 [01:06<02:03,  3.41it/s] 28%|██▊       | 166/585 [01:06<02:03,  3.40it/s] 29%|██▊       | 167/585 [01:06<02:02,  3.41it/s] 29%|██▊       | 168/585 [01:06<02:02,  3.41it/s] 29%|██▉       | 169/585 [01:07<02:02,  3.40it/s] 29%|██▉       | 170/585 [01:07<02:01,  3.40it/s] 29%|██▉       | 171/585 [01:07<02:01,  3.41it/s] 29%|██▉       | 172/585 [01:08<02:01,  3.39it/s] 30%|██▉       | 173/585 [01:08<02:01,  3.39it/s] 30%|██▉       | 174/585 [01:08<02:00,  3.40it/s] 30%|██▉       | 175/585 [01:09<02:00,  3.40it/s] 30%|███       | 176/585 [01:09<02:00,  3.40it/s] 30%|███       | 177/585 [01:09<01:59,  3.40it/s] 30%|███       | 178/585 [01:09<01:59,  3.40it/s] 31%|███       | 179/585 [01:10<01:59,  3.40it/s] 31%|███       | 180/585 [01:10<01:58,  3.41it/s] 31%|███       | 181/585 [01:10<01:58,  3.40it/s] 31%|███       | 182/585 [01:11<01:58,  3.40it/s] 31%|███▏      | 183/585 [01:11<01:59,  3.38it/s] 31%|███▏      | 184/585 [01:11<01:58,  3.38it/s] 32%|███▏      | 185/585 [01:11<01:58,  3.39it/s] 32%|███▏      | 186/585 [01:12<01:57,  3.39it/s] 32%|███▏      | 187/585 [01:12<01:57,  3.40it/s] 32%|███▏      | 188/585 [01:12<01:56,  3.40it/s] 32%|███▏      | 189/585 [01:13<01:56,  3.40it/s] 32%|███▏      | 190/585 [01:13<01:56,  3.40it/s] 33%|███▎      | 191/585 [01:13<01:56,  3.39it/s] 33%|███▎      | 192/585 [01:14<01:55,  3.39it/s] 33%|███▎      | 193/585 [01:14<01:55,  3.40it/s] 33%|███▎      | 194/585 [01:14<01:55,  3.40it/s] 33%|███▎      | 195/585 [01:14<01:54,  3.40it/s] 34%|███▎      | 196/585 [01:15<01:54,  3.40it/s] 34%|███▎      | 197/585 [01:15<01:54,  3.39it/s] 34%|███▍      | 198/585 [01:15<01:53,  3.40it/s] 34%|███▍      | 199/585 [01:16<01:54,  3.36it/s] 34%|███▍      | 200/585 [01:16<01:54,  3.37it/s] 34%|███▍      | 201/585 [01:16<01:53,  3.38it/s] 35%|███▍      | 202/585 [01:16<01:53,  3.39it/s] 35%|███▍      | 203/585 [01:17<01:52,  3.39it/s] 35%|███▍      | 204/585 [01:17<01:52,  3.39it/s] 35%|███▌      | 205/585 [01:17<01:51,  3.39it/s] 35%|███▌      | 206/585 [01:18<01:51,  3.39it/s] 35%|███▌      | 207/585 [01:18<01:51,  3.39it/s] 36%|███▌      | 208/585 [01:18<01:50,  3.40it/s] 36%|███▌      | 209/585 [01:19<01:50,  3.40it/s] 36%|███▌      | 210/585 [01:19<01:51,  3.38it/s] 36%|███▌      | 211/585 [01:19<01:50,  3.38it/s] 36%|███▌      | 212/585 [01:19<01:50,  3.38it/s] 36%|███▋      | 213/585 [01:20<01:49,  3.39it/s] 37%|███▋      | 214/585 [01:20<01:49,  3.39it/s] 37%|███▋      | 215/585 [01:20<01:48,  3.40it/s] 37%|███▋      | 216/585 [01:21<01:48,  3.40it/s] 37%|███▋      | 217/585 [01:21<01:48,  3.40it/s] 37%|███▋      | 218/585 [01:21<01:47,  3.41it/s] 37%|███▋      | 219/585 [01:21<01:47,  3.41it/s] 38%|███▊      | 220/585 [01:22<01:46,  3.41it/s] 38%|███▊      | 221/585 [01:22<01:47,  3.40it/s] 38%|███▊      | 222/585 [01:22<01:46,  3.40it/s] 38%|███▊      | 223/585 [01:23<01:46,  3.40it/s] 38%|███▊      | 224/585 [01:23<01:45,  3.41it/s] 38%|███▊      | 225/585 [01:23<01:45,  3.41it/s] 39%|███▊      | 226/585 [01:24<01:45,  3.41it/s] 39%|███▉      | 227/585 [01:24<01:45,  3.41it/s] 39%|███▉      | 228/585 [01:24<01:44,  3.41it/s] 39%|███▉      | 229/585 [01:24<01:44,  3.41it/s] 39%|███▉      | 230/585 [01:25<01:44,  3.39it/s] 39%|███▉      | 231/585 [01:25<01:44,  3.40it/s] 40%|███▉      | 232/585 [01:25<01:43,  3.40it/s] 40%|███▉      | 233/585 [01:26<01:43,  3.40it/s] 40%|████      | 234/585 [01:26<01:43,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 15:03:20,675 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:03:20,675 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 15:03:20,675 >>   Batch size = 8
{'eval_loss': 0.9592005014419556, 'eval_runtime': 9.96, 'eval_samples_per_second': 350.2, 'eval_steps_per_second': 43.775, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 54.84it/s][A
  3%|▎         | 12/436 [00:00<00:08, 47.41it/s][A
  4%|▍         | 17/436 [00:00<00:09, 45.64it/s][A
  5%|▌         | 22/436 [00:00<00:09, 44.36it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.62it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.39it/s][A
  8%|▊         | 37/436 [00:00<00:09, 44.22it/s][A
 10%|▉         | 42/436 [00:00<00:09, 43.61it/s][A
 11%|█         | 47/436 [00:01<00:08, 43.89it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.06it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.01it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 43.89it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 43.82it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 43.72it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 43.67it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.72it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 43.72it/s][A
 21%|██        | 92/436 [00:02<00:07, 43.14it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 43.63it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 43.71it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 43.77it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 43.83it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.84it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 43.79it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.71it/s][A
 30%|███       | 132/436 [00:02<00:06, 43.77it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 43.82it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.01it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.13it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 43.86it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 43.91it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.86it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.93it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.85it/s][A
 41%|████      | 177/436 [00:04<00:05, 43.79it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 43.83it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.00it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 43.98it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 43.95it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 43.98it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 43.74it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 43.88it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 43.91it/s][A
 51%|█████     | 222/436 [00:05<00:04, 43.82it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 43.91it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.04it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.00it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 43.98it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 43.97it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 43.95it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.94it/s][A
 60%|██████    | 262/436 [00:05<00:03, 43.80it/s][A
 61%|██████    | 267/436 [00:06<00:03, 43.87it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 43.94it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.03it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.00it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 43.96it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 43.99it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.03it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 43.94it/s][A
 70%|███████   | 307/436 [00:06<00:02, 43.88it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 43.98it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 43.93it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 43.99it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.00it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 43.99it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 43.93it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 43.84it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 43.76it/s][A
 81%|████████  | 352/436 [00:08<00:01, 43.86it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 43.96it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 43.91it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 43.87it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 43.88it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 43.93it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 43.94it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 43.85it/s][A
 90%|████████▉ | 392/436 [00:08<00:01, 43.81it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 43.91it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 43.91it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 43.99it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.01it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 43.84it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 43.74it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 43.78it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 43.76it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:36<01:43,  3.40it/s]
100%|██████████| 436/436 [00:09<00:00, 43.76it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:03:30,666 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 15:03:30,704 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:03:33,400 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:03:33,419 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:03:33,429 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:44<33:35,  5.76s/it] 40%|████      | 236/585 [01:45<23:59,  4.12s/it] 41%|████      | 237/585 [01:45<17:16,  2.98s/it] 41%|████      | 238/585 [01:45<12:34,  2.17s/it] 41%|████      | 239/585 [01:46<09:17,  1.61s/it] 41%|████      | 240/585 [01:46<07:00,  1.22s/it] 41%|████      | 241/585 [01:46<05:24,  1.06it/s] 41%|████▏     | 242/585 [01:47<04:17,  1.33it/s] 42%|████▏     | 243/585 [01:47<03:30,  1.63it/s] 42%|████▏     | 244/585 [01:47<02:57,  1.92it/s] 42%|████▏     | 245/585 [01:47<02:34,  2.20it/s] 42%|████▏     | 246/585 [01:48<02:18,  2.45it/s] 42%|████▏     | 247/585 [01:48<02:06,  2.66it/s] 42%|████▏     | 248/585 [01:48<01:58,  2.83it/s] 43%|████▎     | 249/585 [01:49<01:53,  2.97it/s] 43%|████▎     | 250/585 [01:49<01:49,  3.07it/s] 43%|████▎     | 251/585 [01:49<01:46,  3.14it/s] 43%|████▎     | 252/585 [01:50<01:44,  3.20it/s] 43%|████▎     | 253/585 [01:50<01:42,  3.24it/s] 43%|████▎     | 254/585 [01:50<01:41,  3.26it/s] 44%|████▎     | 255/585 [01:50<01:40,  3.29it/s] 44%|████▍     | 256/585 [01:51<01:40,  3.26it/s] 44%|████▍     | 257/585 [01:51<01:40,  3.28it/s] 44%|████▍     | 258/585 [01:51<01:39,  3.29it/s] 44%|████▍     | 259/585 [01:52<01:38,  3.30it/s] 44%|████▍     | 260/585 [01:52<01:38,  3.31it/s] 45%|████▍     | 261/585 [01:52<01:37,  3.32it/s] 45%|████▍     | 262/585 [01:53<01:37,  3.32it/s] 45%|████▍     | 263/585 [01:53<01:36,  3.32it/s] 45%|████▌     | 264/585 [01:53<01:36,  3.33it/s] 45%|████▌     | 265/585 [01:53<01:36,  3.33it/s] 45%|████▌     | 266/585 [01:54<01:35,  3.33it/s] 46%|████▌     | 267/585 [01:54<01:35,  3.33it/s] 46%|████▌     | 268/585 [01:54<01:35,  3.33it/s] 46%|████▌     | 269/585 [01:55<01:34,  3.33it/s] 46%|████▌     | 270/585 [01:55<01:34,  3.33it/s] 46%|████▋     | 271/585 [01:55<01:34,  3.33it/s] 46%|████▋     | 272/585 [01:56<01:33,  3.33it/s] 47%|████▋     | 273/585 [01:56<01:33,  3.33it/s] 47%|████▋     | 274/585 [01:56<01:33,  3.33it/s] 47%|████▋     | 275/585 [01:56<01:33,  3.32it/s] 47%|████▋     | 276/585 [01:57<01:33,  3.32it/s] 47%|████▋     | 277/585 [01:57<01:32,  3.32it/s] 48%|████▊     | 278/585 [01:57<01:32,  3.33it/s] 48%|████▊     | 279/585 [01:58<01:31,  3.33it/s] 48%|████▊     | 280/585 [01:58<01:31,  3.33it/s] 48%|████▊     | 281/585 [01:58<01:31,  3.33it/s] 48%|████▊     | 282/585 [01:59<01:31,  3.33it/s] 48%|████▊     | 283/585 [01:59<01:30,  3.33it/s] 49%|████▊     | 284/585 [01:59<01:30,  3.33it/s] 49%|████▊     | 285/585 [01:59<01:30,  3.32it/s] 49%|████▉     | 286/585 [02:00<01:30,  3.31it/s] 49%|████▉     | 287/585 [02:00<01:29,  3.32it/s] 49%|████▉     | 288/585 [02:00<01:29,  3.32it/s] 49%|████▉     | 289/585 [02:01<01:29,  3.32it/s] 50%|████▉     | 290/585 [02:01<01:28,  3.32it/s] 50%|████▉     | 291/585 [02:01<01:28,  3.32it/s] 50%|████▉     | 292/585 [02:02<01:28,  3.32it/s] 50%|█████     | 293/585 [02:02<01:27,  3.32it/s] 50%|█████     | 294/585 [02:02<01:27,  3.33it/s] 50%|█████     | 295/585 [02:02<01:27,  3.31it/s] 51%|█████     | 296/585 [02:03<01:27,  3.32it/s] 51%|█████     | 297/585 [02:03<01:26,  3.32it/s] 51%|█████     | 298/585 [02:03<01:26,  3.33it/s] 51%|█████     | 299/585 [02:04<01:26,  3.33it/s] 51%|█████▏    | 300/585 [02:04<01:25,  3.33it/s] 51%|█████▏    | 301/585 [02:04<01:25,  3.33it/s] 52%|█████▏    | 302/585 [02:05<01:25,  3.32it/s] 52%|█████▏    | 303/585 [02:05<01:24,  3.33it/s] 52%|█████▏    | 304/585 [02:05<01:24,  3.33it/s] 52%|█████▏    | 305/585 [02:05<01:24,  3.31it/s] 52%|█████▏    | 306/585 [02:06<01:24,  3.32it/s] 52%|█████▏    | 307/585 [02:06<01:23,  3.32it/s] 53%|█████▎    | 308/585 [02:06<01:23,  3.32it/s] 53%|█████▎    | 309/585 [02:07<01:23,  3.32it/s] 53%|█████▎    | 310/585 [02:07<01:22,  3.33it/s] 53%|█████▎    | 311/585 [02:07<01:22,  3.33it/s] 53%|█████▎    | 312/585 [02:08<01:22,  3.33it/s] 54%|█████▎    | 313/585 [02:08<01:21,  3.33it/s] 54%|█████▎    | 314/585 [02:08<01:21,  3.33it/s] 54%|█████▍    | 315/585 [02:08<01:21,  3.32it/s] 54%|█████▍    | 316/585 [02:09<01:20,  3.33it/s] 54%|█████▍    | 317/585 [02:09<01:20,  3.32it/s] 54%|█████▍    | 318/585 [02:09<01:20,  3.33it/s] 55%|█████▍    | 319/585 [02:10<01:19,  3.33it/s] 55%|█████▍    | 320/585 [02:10<01:19,  3.33it/s] 55%|█████▍    | 321/585 [02:10<01:19,  3.32it/s] 55%|█████▌    | 322/585 [02:11<01:19,  3.32it/s] 55%|█████▌    | 323/585 [02:11<01:18,  3.32it/s] 55%|█████▌    | 324/585 [02:11<01:18,  3.32it/s] 56%|█████▌    | 325/585 [02:11<01:18,  3.31it/s] 56%|█████▌    | 326/585 [02:12<01:18,  3.31it/s] 56%|█████▌    | 327/585 [02:12<01:17,  3.32it/s] 56%|█████▌    | 328/585 [02:12<01:17,  3.32it/s] 56%|█████▌    | 329/585 [02:13<01:17,  3.32it/s] 56%|█████▋    | 330/585 [02:13<01:16,  3.32it/s] 57%|█████▋    | 331/585 [02:13<01:16,  3.32it/s] 57%|█████▋    | 332/585 [02:14<01:16,  3.33it/s] 57%|█████▋    | 333/585 [02:14<01:15,  3.33it/s] 57%|█████▋    | 334/585 [02:14<01:15,  3.33it/s] 57%|█████▋    | 335/585 [02:14<01:15,  3.32it/s] 57%|█████▋    | 336/585 [02:15<01:14,  3.32it/s] 58%|█████▊    | 337/585 [02:15<01:14,  3.32it/s] 58%|█████▊    | 338/585 [02:15<01:14,  3.33it/s] 58%|█████▊    | 339/585 [02:16<01:14,  3.32it/s] 58%|█████▊    | 340/585 [02:16<01:13,  3.32it/s] 58%|█████▊    | 341/585 [02:16<01:13,  3.32it/s] 58%|█████▊    | 342/585 [02:17<01:13,  3.32it/s] 59%|█████▊    | 343/585 [02:17<01:12,  3.32it/s] 59%|█████▉    | 344/585 [02:17<01:12,  3.32it/s] 59%|█████▉    | 345/585 [02:17<01:12,  3.31it/s] 59%|█████▉    | 346/585 [02:18<01:12,  3.32it/s] 59%|█████▉    | 347/585 [02:18<01:11,  3.32it/s] 59%|█████▉    | 348/585 [02:18<01:11,  3.33it/s] 60%|█████▉    | 349/585 [02:19<01:10,  3.33it/s] 60%|█████▉    | 350/585 [02:19<01:10,  3.33it/s] 60%|██████    | 351/585 [02:19<01:10,  3.33it/s][INFO|trainer.py:2140] 2023-08-28 15:04:14,091 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:04:14,091 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 15:04:14,091 >>   Batch size = 8
{'eval_loss': 0.9472498297691345, 'eval_runtime': 9.9672, 'eval_samples_per_second': 349.947, 'eval_steps_per_second': 43.743, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 54.65it/s][A
  3%|▎         | 12/436 [00:00<00:08, 47.46it/s][A
  4%|▍         | 17/436 [00:00<00:09, 45.87it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.10it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.48it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.26it/s][A
  8%|▊         | 37/436 [00:00<00:09, 44.13it/s][A
 10%|▉         | 42/436 [00:00<00:08, 43.94it/s][A
 11%|█         | 47/436 [00:01<00:08, 43.96it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.07it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.07it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 43.99it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 43.92it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 43.73it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 43.86it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.87it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 43.89it/s][A
 21%|██        | 92/436 [00:02<00:07, 43.95it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 43.98it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.03it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 43.80it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 43.78it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.87it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 43.87it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.71it/s][A
 30%|███       | 132/436 [00:02<00:06, 43.80it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 43.99it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.04it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 43.97it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 43.83it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 43.81it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.81it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.71it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.67it/s][A
 41%|████      | 177/436 [00:04<00:05, 43.81it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.00it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 43.96it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 43.93it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.00it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.00it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 43.79it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 43.68it/s][A
 50%|████▉     | 217/436 [00:04<00:05, 43.78it/s][A
 51%|█████     | 222/436 [00:05<00:04, 43.95it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.01it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 43.95it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 43.84it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 43.84it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 43.90it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 43.79it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.77it/s][A
 60%|██████    | 262/436 [00:05<00:03, 43.81it/s][A
 61%|██████    | 267/436 [00:06<00:03, 43.98it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 43.97it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 43.97it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 43.95it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 43.82it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 43.67it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 43.82it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 43.84it/s][A
 70%|███████   | 307/436 [00:06<00:02, 43.84it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 43.83it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 43.96it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 43.96it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 43.88it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 43.83it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 43.86it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 43.80it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 43.80it/s][A
 81%|████████  | 352/436 [00:08<00:01, 43.81it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 43.88it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 43.92it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 43.81it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 43.82it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 43.81it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 42.93it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 43.40it/s][A
 90%|████████▉ | 392/436 [00:08<00:01, 43.50it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 43.67it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 43.79it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 43.76it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 43.72it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 43.67it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 43.66it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 43.76it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 43.87it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [02:29<01:10,  3.33it/s]
100%|██████████| 436/436 [00:09<00:00, 43.87it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:04:24,075 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 15:04:24,096 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:04:26,290 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:04:26,328 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:04:26,338 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:36<19:46,  5.09s/it] 60%|██████    | 353/585 [02:36<14:08,  3.66s/it] 61%|██████    | 354/585 [02:36<10:12,  2.65s/it] 61%|██████    | 355/585 [02:36<07:27,  1.95s/it] 61%|██████    | 356/585 [02:37<05:32,  1.45s/it] 61%|██████    | 357/585 [02:37<04:12,  1.11s/it] 61%|██████    | 358/585 [02:37<03:16,  1.16it/s] 61%|██████▏   | 359/585 [02:38<02:37,  1.44it/s] 62%|██████▏   | 360/585 [02:38<02:09,  1.73it/s] 62%|██████▏   | 361/585 [02:38<01:50,  2.02it/s] 62%|██████▏   | 362/585 [02:39<01:37,  2.29it/s] 62%|██████▏   | 363/585 [02:39<01:27,  2.52it/s] 62%|██████▏   | 364/585 [02:39<01:21,  2.72it/s] 62%|██████▏   | 365/585 [02:39<01:16,  2.88it/s] 63%|██████▎   | 366/585 [02:40<01:12,  3.00it/s] 63%|██████▎   | 367/585 [02:40<01:10,  3.09it/s] 63%|██████▎   | 368/585 [02:40<01:08,  3.16it/s] 63%|██████▎   | 369/585 [02:41<01:07,  3.21it/s] 63%|██████▎   | 370/585 [02:41<01:06,  3.25it/s] 63%|██████▎   | 371/585 [02:41<01:05,  3.27it/s] 64%|██████▎   | 372/585 [02:42<01:04,  3.29it/s] 64%|██████▍   | 373/585 [02:42<01:04,  3.29it/s] 64%|██████▍   | 374/585 [02:42<01:03,  3.30it/s] 64%|██████▍   | 375/585 [02:42<01:03,  3.31it/s] 64%|██████▍   | 376/585 [02:43<01:02,  3.32it/s] 64%|██████▍   | 377/585 [02:43<01:02,  3.32it/s] 65%|██████▍   | 378/585 [02:43<01:02,  3.32it/s] 65%|██████▍   | 379/585 [02:44<01:01,  3.33it/s] 65%|██████▍   | 380/585 [02:44<01:01,  3.32it/s] 65%|██████▌   | 381/585 [02:44<01:01,  3.32it/s] 65%|██████▌   | 382/585 [02:45<01:01,  3.32it/s] 65%|██████▌   | 383/585 [02:45<01:00,  3.31it/s] 66%|██████▌   | 384/585 [02:45<01:00,  3.32it/s] 66%|██████▌   | 385/585 [02:46<01:00,  3.33it/s] 66%|██████▌   | 386/585 [02:46<00:59,  3.33it/s] 66%|██████▌   | 387/585 [02:46<00:59,  3.33it/s] 66%|██████▋   | 388/585 [02:46<00:59,  3.33it/s] 66%|██████▋   | 389/585 [02:47<00:58,  3.33it/s] 67%|██████▋   | 390/585 [02:47<00:58,  3.33it/s] 67%|██████▋   | 391/585 [02:47<00:57,  3.35it/s] 67%|██████▋   | 392/585 [02:48<00:57,  3.36it/s] 67%|██████▋   | 393/585 [02:48<00:56,  3.38it/s] 67%|██████▋   | 394/585 [02:48<00:56,  3.35it/s] 68%|██████▊   | 395/585 [02:48<00:56,  3.37it/s] 68%|██████▊   | 396/585 [02:49<00:55,  3.38it/s] 68%|██████▊   | 397/585 [02:49<00:55,  3.39it/s] 68%|██████▊   | 398/585 [02:49<00:55,  3.40it/s] 68%|██████▊   | 399/585 [02:50<00:54,  3.40it/s] 68%|██████▊   | 400/585 [02:50<00:54,  3.40it/s] 69%|██████▊   | 401/585 [02:50<00:54,  3.40it/s] 69%|██████▊   | 402/585 [02:51<00:53,  3.40it/s] 69%|██████▉   | 403/585 [02:51<00:53,  3.40it/s] 69%|██████▉   | 404/585 [02:51<00:53,  3.41it/s] 69%|██████▉   | 405/585 [02:51<00:52,  3.40it/s] 69%|██████▉   | 406/585 [02:52<00:52,  3.40it/s] 70%|██████▉   | 407/585 [02:52<00:52,  3.40it/s] 70%|██████▉   | 408/585 [02:52<00:52,  3.40it/s] 70%|██████▉   | 409/585 [02:53<00:51,  3.40it/s] 70%|███████   | 410/585 [02:53<00:51,  3.41it/s] 70%|███████   | 411/585 [02:53<00:51,  3.41it/s] 70%|███████   | 412/585 [02:53<00:50,  3.41it/s] 71%|███████   | 413/585 [02:54<00:50,  3.41it/s] 71%|███████   | 414/585 [02:54<00:50,  3.41it/s] 71%|███████   | 415/585 [02:54<00:49,  3.41it/s] 71%|███████   | 416/585 [02:55<00:49,  3.41it/s] 71%|███████▏  | 417/585 [02:55<00:49,  3.40it/s] 71%|███████▏  | 418/585 [02:55<00:49,  3.40it/s] 72%|███████▏  | 419/585 [02:56<00:48,  3.40it/s] 72%|███████▏  | 420/585 [02:56<00:48,  3.40it/s] 72%|███████▏  | 421/585 [02:56<00:48,  3.40it/s] 72%|███████▏  | 422/585 [02:56<00:47,  3.41it/s] 72%|███████▏  | 423/585 [02:57<00:47,  3.41it/s] 72%|███████▏  | 424/585 [02:57<00:47,  3.41it/s] 73%|███████▎  | 425/585 [02:57<00:46,  3.41it/s] 73%|███████▎  | 426/585 [02:58<00:46,  3.41it/s] 73%|███████▎  | 427/585 [02:58<00:46,  3.40it/s] 73%|███████▎  | 428/585 [02:58<00:46,  3.40it/s] 73%|███████▎  | 429/585 [02:58<00:45,  3.40it/s] 74%|███████▎  | 430/585 [02:59<00:45,  3.40it/s] 74%|███████▎  | 431/585 [02:59<00:45,  3.40it/s] 74%|███████▍  | 432/585 [02:59<00:44,  3.41it/s] 74%|███████▍  | 433/585 [03:00<00:44,  3.40it/s] 74%|███████▍  | 434/585 [03:00<00:44,  3.40it/s] 74%|███████▍  | 435/585 [03:00<00:44,  3.40it/s] 75%|███████▍  | 436/585 [03:01<00:43,  3.40it/s] 75%|███████▍  | 437/585 [03:01<00:43,  3.40it/s] 75%|███████▍  | 438/585 [03:01<00:43,  3.40it/s] 75%|███████▌  | 439/585 [03:01<00:42,  3.40it/s] 75%|███████▌  | 440/585 [03:02<00:42,  3.40it/s] 75%|███████▌  | 441/585 [03:02<00:42,  3.40it/s] 76%|███████▌  | 442/585 [03:02<00:42,  3.40it/s] 76%|███████▌  | 443/585 [03:03<00:41,  3.40it/s] 76%|███████▌  | 444/585 [03:03<00:41,  3.41it/s] 76%|███████▌  | 445/585 [03:03<00:41,  3.41it/s] 76%|███████▌  | 446/585 [03:03<00:40,  3.41it/s] 76%|███████▋  | 447/585 [03:04<00:40,  3.41it/s] 77%|███████▋  | 448/585 [03:04<00:40,  3.41it/s] 77%|███████▋  | 449/585 [03:04<00:40,  3.40it/s] 77%|███████▋  | 450/585 [03:05<00:39,  3.40it/s] 77%|███████▋  | 451/585 [03:05<00:39,  3.39it/s] 77%|███████▋  | 452/585 [03:05<00:39,  3.39it/s] 77%|███████▋  | 453/585 [03:06<00:38,  3.40it/s] 78%|███████▊  | 454/585 [03:06<00:38,  3.40it/s] 78%|███████▊  | 455/585 [03:06<00:38,  3.41it/s] 78%|███████▊  | 456/585 [03:06<00:37,  3.41it/s] 78%|███████▊  | 457/585 [03:07<00:37,  3.41it/s] 78%|███████▊  | 458/585 [03:07<00:37,  3.41it/s] 78%|███████▊  | 459/585 [03:07<00:36,  3.41it/s] 79%|███████▊  | 460/585 [03:08<00:36,  3.40it/s] 79%|███████▉  | 461/585 [03:08<00:36,  3.40it/s] 79%|███████▉  | 462/585 [03:08<00:36,  3.40it/s] 79%|███████▉  | 463/585 [03:08<00:35,  3.40it/s] 79%|███████▉  | 464/585 [03:09<00:35,  3.40it/s] 79%|███████▉  | 465/585 [03:09<00:35,  3.40it/s] 80%|███████▉  | 466/585 [03:09<00:34,  3.40it/s] 80%|███████▉  | 467/585 [03:10<00:34,  3.41it/s] 80%|████████  | 468/585 [03:10<00:34,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 15:05:04,725 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:05:04,725 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 15:05:04,725 >>   Batch size = 8
{'eval_loss': 0.9538477659225464, 'eval_runtime': 9.968, 'eval_samples_per_second': 349.918, 'eval_steps_per_second': 43.74, 'epoch': 3.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.04it/s][A
  3%|▎         | 12/436 [00:00<00:08, 47.80it/s][A
  4%|▍         | 17/436 [00:00<00:09, 45.97it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.22it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.65it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.18it/s][A
  8%|▊         | 37/436 [00:00<00:09, 43.96it/s][A
 10%|▉         | 42/436 [00:00<00:09, 43.77it/s][A
 11%|█         | 47/436 [00:01<00:08, 43.93it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.12it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.06it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 43.99it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 43.88it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 43.95it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 43.74it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.58it/s][A
 20%|█▉        | 87/436 [00:01<00:08, 43.55it/s][A
 21%|██        | 92/436 [00:02<00:07, 43.81it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 43.96it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.08it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.07it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.07it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.77it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 43.75it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.67it/s][A
 30%|███       | 132/436 [00:02<00:06, 43.62it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 43.81it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 43.97it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.01it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.01it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 43.74it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.78it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.68it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.47it/s][A
 41%|████      | 177/436 [00:04<00:05, 43.56it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 43.69it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 43.90it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.03it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.02it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 43.98it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 43.88it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 43.72it/s][A
 50%|████▉     | 217/436 [00:04<00:05, 43.62it/s][A
 51%|█████     | 222/436 [00:05<00:04, 43.62it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 43.73it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 43.92it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 43.89it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 43.91it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 43.98it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 43.86it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.69it/s][A
 60%|██████    | 262/436 [00:05<00:04, 43.41it/s][A
 61%|██████    | 267/436 [00:06<00:03, 43.67it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 43.79it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 43.96it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 43.99it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.05it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.01it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 43.95it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 43.67it/s][A
 70%|███████   | 307/436 [00:06<00:02, 43.51it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 43.67it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 43.80it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 43.88it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 43.95it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 43.99it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 43.90it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 43.78it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 43.62it/s][A
 81%|████████  | 352/436 [00:08<00:01, 43.65it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 43.77it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 43.84it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 43.94it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 43.88it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 43.98it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 43.92it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 43.71it/s][A
 90%|████████▉ | 392/436 [00:08<00:01, 42.75it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 42.23it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 42.53it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 43.09it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 43.35it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 43.58it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 43.65it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 43.65it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 43.48it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:20<00:34,  3.40it/s]
100%|██████████| 436/436 [00:09<00:00, 43.48it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:05:14,737 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 15:05:14,764 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:05:16,763 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:05:16,785 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:05:16,793 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:26<09:54,  5.13s/it] 80%|████████  | 470/585 [03:27<07:02,  3.68s/it] 81%|████████  | 471/585 [03:27<05:03,  2.67s/it] 81%|████████  | 472/585 [03:27<03:41,  1.96s/it] 81%|████████  | 473/585 [03:28<02:43,  1.46s/it] 81%|████████  | 474/585 [03:28<02:03,  1.11s/it] 81%|████████  | 475/585 [03:28<01:36,  1.14it/s] 81%|████████▏ | 476/585 [03:28<01:16,  1.42it/s] 82%|████████▏ | 477/585 [03:29<01:02,  1.72it/s] 82%|████████▏ | 478/585 [03:29<00:53,  2.01it/s] 82%|████████▏ | 479/585 [03:29<00:46,  2.28it/s] 82%|████████▏ | 480/585 [03:30<00:41,  2.52it/s] 82%|████████▏ | 481/585 [03:30<00:38,  2.72it/s] 82%|████████▏ | 482/585 [03:30<00:35,  2.88it/s] 83%|████████▎ | 483/585 [03:31<00:33,  3.00it/s] 83%|████████▎ | 484/585 [03:31<00:32,  3.10it/s] 83%|████████▎ | 485/585 [03:31<00:31,  3.17it/s] 83%|████████▎ | 486/585 [03:31<00:30,  3.21it/s] 83%|████████▎ | 487/585 [03:32<00:30,  3.24it/s] 83%|████████▎ | 488/585 [03:32<00:29,  3.27it/s] 84%|████████▎ | 489/585 [03:32<00:29,  3.29it/s] 84%|████████▍ | 490/585 [03:33<00:28,  3.29it/s] 84%|████████▍ | 491/585 [03:33<00:28,  3.29it/s] 84%|████████▍ | 492/585 [03:33<00:28,  3.30it/s] 84%|████████▍ | 493/585 [03:34<00:27,  3.30it/s] 84%|████████▍ | 494/585 [03:34<00:27,  3.31it/s] 85%|████████▍ | 495/585 [03:34<00:27,  3.31it/s] 85%|████████▍ | 496/585 [03:34<00:26,  3.31it/s] 85%|████████▍ | 497/585 [03:35<00:26,  3.32it/s] 85%|████████▌ | 498/585 [03:35<00:26,  3.32it/s] 85%|████████▌ | 499/585 [03:35<00:25,  3.33it/s] 85%|████████▌ | 500/585 [03:36<00:25,  3.33it/s]                                                  85%|████████▌ | 500/585 [03:36<00:25,  3.33it/s] 86%|████████▌ | 501/585 [03:36<00:25,  3.32it/s] 86%|████████▌ | 502/585 [03:36<00:24,  3.33it/s] 86%|████████▌ | 503/585 [03:37<00:24,  3.33it/s] 86%|████████▌ | 504/585 [03:37<00:24,  3.32it/s] 86%|████████▋ | 505/585 [03:37<00:24,  3.32it/s] 86%|████████▋ | 506/585 [03:37<00:23,  3.30it/s] 87%|████████▋ | 507/585 [03:38<00:23,  3.33it/s] 87%|████████▋ | 508/585 [03:38<00:22,  3.35it/s] 87%|████████▋ | 509/585 [03:38<00:22,  3.37it/s] 87%|████████▋ | 510/585 [03:39<00:22,  3.38it/s] 87%|████████▋ | 511/585 [03:39<00:21,  3.39it/s] 88%|████████▊ | 512/585 [03:39<00:21,  3.37it/s] 88%|████████▊ | 513/585 [03:40<00:21,  3.39it/s] 88%|████████▊ | 514/585 [03:40<00:20,  3.39it/s] 88%|████████▊ | 515/585 [03:40<00:20,  3.40it/s] 88%|████████▊ | 516/585 [03:40<00:20,  3.40it/s] 88%|████████▊ | 517/585 [03:41<00:20,  3.39it/s] 89%|████████▊ | 518/585 [03:41<00:19,  3.39it/s] 89%|████████▊ | 519/585 [03:41<00:19,  3.39it/s] 89%|████████▉ | 520/585 [03:42<00:19,  3.40it/s] 89%|████████▉ | 521/585 [03:42<00:18,  3.40it/s] 89%|████████▉ | 522/585 [03:42<00:18,  3.40it/s] 89%|████████▉ | 523/585 [03:42<00:18,  3.40it/s] 90%|████████▉ | 524/585 [03:43<00:17,  3.40it/s] 90%|████████▉ | 525/585 [03:43<00:17,  3.40it/s] 90%|████████▉ | 526/585 [03:43<00:17,  3.40it/s] 90%|█████████ | 527/585 [03:44<00:17,  3.41it/s] 90%|█████████ | 528/585 [03:44<00:16,  3.37it/s] 90%|█████████ | 529/585 [03:44<00:16,  3.38it/s] 91%|█████████ | 530/585 [03:45<00:16,  3.39it/s] 91%|█████████ | 531/585 [03:45<00:15,  3.39it/s] 91%|█████████ | 532/585 [03:45<00:15,  3.40it/s] 91%|█████████ | 533/585 [03:45<00:15,  3.40it/s] 91%|█████████▏| 534/585 [03:46<00:14,  3.40it/s] 91%|█████████▏| 535/585 [03:46<00:14,  3.40it/s] 92%|█████████▏| 536/585 [03:46<00:14,  3.40it/s] 92%|█████████▏| 537/585 [03:47<00:14,  3.41it/s] 92%|█████████▏| 538/585 [03:47<00:13,  3.41it/s] 92%|█████████▏| 539/585 [03:47<00:13,  3.39it/s] 92%|█████████▏| 540/585 [03:47<00:13,  3.40it/s] 92%|█████████▏| 541/585 [03:48<00:12,  3.40it/s] 93%|█████████▎| 542/585 [03:48<00:12,  3.40it/s] 93%|█████████▎| 543/585 [03:48<00:12,  3.40it/s] 93%|█████████▎| 544/585 [03:49<00:12,  3.40it/s] 93%|█████████▎| 545/585 [03:49<00:11,  3.41it/s] 93%|█████████▎| 546/585 [03:49<00:11,  3.40it/s] 94%|█████████▎| 547/585 [03:50<00:11,  3.41it/s] 94%|█████████▎| 548/585 [03:50<00:10,  3.41it/s] 94%|█████████▍| 549/585 [03:50<00:10,  3.40it/s] 94%|█████████▍| 550/585 [03:50<00:10,  3.39it/s] 94%|█████████▍| 551/585 [03:51<00:10,  3.40it/s] 94%|█████████▍| 552/585 [03:51<00:09,  3.40it/s] 95%|█████████▍| 553/585 [03:51<00:09,  3.40it/s] 95%|█████████▍| 554/585 [03:52<00:09,  3.41it/s] 95%|█████████▍| 555/585 [03:52<00:08,  3.41it/s] 95%|█████████▌| 556/585 [03:52<00:08,  3.41it/s] 95%|█████████▌| 557/585 [03:52<00:08,  3.41it/s] 95%|█████████▌| 558/585 [03:53<00:07,  3.41it/s] 96%|█████████▌| 559/585 [03:53<00:07,  3.41it/s] 96%|█████████▌| 560/585 [03:53<00:07,  3.41it/s] 96%|█████████▌| 561/585 [03:54<00:07,  3.39it/s] 96%|█████████▌| 562/585 [03:54<00:06,  3.40it/s] 96%|█████████▌| 563/585 [03:54<00:06,  3.40it/s] 96%|█████████▋| 564/585 [03:55<00:06,  3.40it/s] 97%|█████████▋| 565/585 [03:55<00:05,  3.40it/s] 97%|█████████▋| 566/585 [03:55<00:05,  3.40it/s] 97%|█████████▋| 567/585 [03:55<00:05,  3.40it/s] 97%|█████████▋| 568/585 [03:56<00:04,  3.41it/s] 97%|█████████▋| 569/585 [03:56<00:04,  3.40it/s] 97%|█████████▋| 570/585 [03:56<00:04,  3.40it/s] 98%|█████████▊| 571/585 [03:57<00:04,  3.41it/s] 98%|█████████▊| 572/585 [03:57<00:03,  3.40it/s] 98%|█████████▊| 573/585 [03:57<00:03,  3.40it/s] 98%|█████████▊| 574/585 [03:57<00:03,  3.41it/s] 98%|█████████▊| 575/585 [03:58<00:02,  3.40it/s] 98%|█████████▊| 576/585 [03:58<00:02,  3.41it/s] 99%|█████████▊| 577/585 [03:58<00:02,  3.41it/s] 99%|█████████▉| 578/585 [03:59<00:02,  3.41it/s] 99%|█████████▉| 579/585 [03:59<00:01,  3.41it/s] 99%|█████████▉| 580/585 [03:59<00:01,  3.39it/s] 99%|█████████▉| 581/585 [04:00<00:01,  3.39it/s] 99%|█████████▉| 582/585 [04:00<00:00,  3.40it/s]100%|█████████▉| 583/585 [04:00<00:00,  3.39it/s]100%|█████████▉| 584/585 [04:00<00:00,  3.39it/s]100%|██████████| 585/585 [04:01<00:00,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 15:05:55,467 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:05:55,468 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 15:05:55,468 >>   Batch size = 8
{'eval_loss': 0.9573178887367249, 'eval_runtime': 9.9897, 'eval_samples_per_second': 349.159, 'eval_steps_per_second': 43.645, 'epoch': 4.0}
{'loss': 0.8755, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.25it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.06it/s][A
  4%|▍         | 17/436 [00:00<00:09, 45.99it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.14it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.49it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.18it/s][A
  8%|▊         | 37/436 [00:00<00:09, 44.14it/s][A
 10%|▉         | 42/436 [00:00<00:08, 43.95it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.04it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.04it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.06it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.07it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 43.90it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 43.86it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 43.88it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.77it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 43.83it/s][A
 21%|██        | 92/436 [00:02<00:07, 43.95it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.06it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.11it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 43.89it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 43.86it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.82it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 43.78it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.85it/s][A
 30%|███       | 132/436 [00:02<00:06, 43.74it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 43.92it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.05it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 43.89it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 43.97it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 43.81it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.86it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.91it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.89it/s][A
 41%|████      | 177/436 [00:04<00:05, 44.00it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 43.89it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.03it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 43.96it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 43.90it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 43.88it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 43.83it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 43.65it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 43.82it/s][A
 51%|█████     | 222/436 [00:05<00:04, 43.92it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.03it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 43.99it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 43.98it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 43.99it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 43.98it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 43.82it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.76it/s][A
 60%|██████    | 262/436 [00:05<00:03, 43.75it/s][A
 61%|██████    | 267/436 [00:06<00:03, 43.74it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 43.84it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 43.92it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.06it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 43.84it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 43.87it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 43.76it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 43.83it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.00it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 43.89it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 43.98it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.00it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 43.96it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 43.93it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 43.83it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 43.65it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 43.64it/s][A
 81%|████████  | 352/436 [00:07<00:01, 43.73it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 43.92it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 43.97it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 43.89it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.02it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.01it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 43.94it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 43.83it/s][A
 90%|████████▉ | 392/436 [00:08<00:01, 43.59it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 43.67it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 43.87it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 43.88it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 43.97it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.08it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.08it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 43.91it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 43.68it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:11<00:00,  3.40it/s]
100%|██████████| 436/436 [00:09<00:00, 43.68it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:06:05,419 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 15:06:05,448 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:06:09,802 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:06:09,818 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:06:09,828 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 15:06:14,060 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 15:06:14,066 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-234 (score: 0.9472498297691345).
                                                 100%|██████████| 585/585 [04:22<00:00,  3.40it/s]100%|██████████| 585/585 [04:22<00:00,  2.23it/s]
[INFO|trainer.py:1894] 2023-08-28 15:06:16,584 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 15:06:16,603 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:06:18,707 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:06:18,731 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:06:18,742 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 15:06:18,955 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:18,955 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:18,955 >>   train_loss               =     0.8695
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:18,955 >>   train_runtime            = 0:04:22.32
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:18,955 >>   train_samples            =       7503
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:18,955 >>   train_samples_per_second =     143.01
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:18,955 >>   train_steps_per_second   =       2.23
{'eval_loss': 0.9587648510932922, 'eval_runtime': 9.938, 'eval_samples_per_second': 350.976, 'eval_steps_per_second': 43.872, 'epoch': 5.0}
{'train_runtime': 262.3242, 'train_samples_per_second': 143.01, 'train_steps_per_second': 2.23, 'train_loss': 0.8695316151676015, 'epoch': 5.0}
08/28/2023 15:06:18 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 15:06:18,997 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:06:18,997 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 15:06:18,997 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 54.40it/s]  3%|▎         | 12/436 [00:00<00:08, 48.07it/s]  4%|▍         | 17/436 [00:00<00:09, 46.43it/s]  5%|▌         | 22/436 [00:00<00:09, 45.75it/s]  6%|▌         | 27/436 [00:00<00:09, 45.41it/s]  7%|▋         | 32/436 [00:00<00:08, 44.93it/s]  8%|▊         | 37/436 [00:00<00:08, 45.06it/s] 10%|▉         | 42/436 [00:00<00:08, 44.52it/s] 11%|█         | 47/436 [00:01<00:08, 44.03it/s] 12%|█▏        | 52/436 [00:01<00:08, 44.04it/s] 13%|█▎        | 57/436 [00:01<00:08, 44.11it/s] 14%|█▍        | 62/436 [00:01<00:08, 44.21it/s] 15%|█▌        | 67/436 [00:01<00:08, 44.22it/s] 17%|█▋        | 72/436 [00:01<00:08, 44.30it/s] 18%|█▊        | 77/436 [00:01<00:08, 44.34it/s] 19%|█▉        | 82/436 [00:01<00:07, 44.26it/s] 20%|█▉        | 87/436 [00:01<00:07, 44.07it/s] 21%|██        | 92/436 [00:02<00:07, 43.87it/s] 22%|██▏       | 97/436 [00:02<00:07, 43.92it/s] 23%|██▎       | 102/436 [00:02<00:07, 43.88it/s] 25%|██▍       | 107/436 [00:02<00:07, 44.07it/s] 26%|██▌       | 112/436 [00:02<00:07, 44.27it/s] 27%|██▋       | 117/436 [00:02<00:07, 44.34it/s] 28%|██▊       | 122/436 [00:02<00:07, 44.23it/s] 29%|██▉       | 127/436 [00:02<00:06, 44.32it/s] 30%|███       | 132/436 [00:02<00:06, 44.02it/s] 31%|███▏      | 137/436 [00:03<00:06, 43.95it/s] 33%|███▎      | 142/436 [00:03<00:06, 44.00it/s] 34%|███▎      | 147/436 [00:03<00:06, 44.00it/s] 35%|███▍      | 152/436 [00:03<00:06, 44.11it/s] 36%|███▌      | 157/436 [00:03<00:06, 44.08it/s] 37%|███▋      | 162/436 [00:03<00:06, 44.20it/s] 38%|███▊      | 167/436 [00:03<00:06, 44.32it/s] 39%|███▉      | 172/436 [00:03<00:06, 42.92it/s] 41%|████      | 177/436 [00:03<00:05, 43.75it/s] 42%|████▏     | 182/436 [00:04<00:05, 43.57it/s] 43%|████▎     | 187/436 [00:04<00:05, 43.64it/s] 44%|████▍     | 192/436 [00:04<00:05, 43.92it/s] 45%|████▌     | 197/436 [00:04<00:05, 43.95it/s] 46%|████▋     | 202/436 [00:04<00:05, 44.02it/s] 47%|████▋     | 207/436 [00:04<00:05, 44.15it/s] 49%|████▊     | 212/436 [00:04<00:05, 44.04it/s] 50%|████▉     | 217/436 [00:04<00:04, 44.22it/s] 51%|█████     | 222/436 [00:05<00:04, 44.17it/s] 52%|█████▏    | 227/436 [00:05<00:04, 44.05it/s] 53%|█████▎    | 232/436 [00:05<00:04, 43.98it/s] 54%|█████▍    | 237/436 [00:05<00:04, 43.88it/s] 56%|█████▌    | 242/436 [00:05<00:04, 44.00it/s] 57%|█████▋    | 247/436 [00:05<00:04, 44.08it/s] 58%|█████▊    | 252/436 [00:05<00:04, 44.23it/s] 59%|█████▉    | 257/436 [00:05<00:04, 44.25it/s] 60%|██████    | 262/436 [00:05<00:03, 44.06it/s] 61%|██████    | 267/436 [00:06<00:03, 44.10it/s] 62%|██████▏   | 272/436 [00:06<00:03, 44.00it/s] 64%|██████▎   | 277/436 [00:06<00:03, 44.01it/s] 65%|██████▍   | 282/436 [00:06<00:03, 43.95it/s] 66%|██████▌   | 287/436 [00:06<00:03, 43.98it/s] 67%|██████▋   | 292/436 [00:06<00:03, 44.06it/s] 68%|██████▊   | 297/436 [00:06<00:03, 44.19it/s] 69%|██████▉   | 302/436 [00:06<00:03, 44.19it/s] 70%|███████   | 307/436 [00:06<00:02, 44.14it/s] 72%|███████▏  | 312/436 [00:07<00:02, 44.19it/s] 73%|███████▎  | 317/436 [00:07<00:02, 44.12it/s] 74%|███████▍  | 322/436 [00:07<00:02, 43.97it/s] 75%|███████▌  | 327/436 [00:07<00:02, 44.01it/s] 76%|███████▌  | 332/436 [00:07<00:02, 43.97it/s] 77%|███████▋  | 337/436 [00:07<00:02, 44.02it/s] 78%|███████▊  | 342/436 [00:07<00:02, 44.09it/s] 80%|███████▉  | 347/436 [00:07<00:02, 44.16it/s] 81%|████████  | 352/436 [00:07<00:01, 44.16it/s] 82%|████████▏ | 357/436 [00:08<00:01, 44.15it/s] 83%|████████▎ | 362/436 [00:08<00:01, 44.04it/s] 84%|████████▍ | 367/436 [00:08<00:01, 43.98it/s] 85%|████████▌ | 372/436 [00:08<00:01, 44.01it/s] 86%|████████▋ | 377/436 [00:08<00:01, 44.03it/s] 88%|████████▊ | 382/436 [00:08<00:01, 44.07it/s] 89%|████████▉ | 387/436 [00:08<00:01, 44.12it/s] 90%|████████▉ | 392/436 [00:08<00:00, 44.21it/s] 91%|█████████ | 397/436 [00:08<00:00, 44.04it/s] 92%|█████████▏| 402/436 [00:09<00:00, 44.14it/s] 93%|█████████▎| 407/436 [00:09<00:00, 44.13it/s] 94%|█████████▍| 412/436 [00:09<00:00, 44.05it/s] 96%|█████████▌| 417/436 [00:09<00:00, 44.08it/s] 97%|█████████▋| 422/436 [00:09<00:00, 43.89it/s] 98%|█████████▊| 427/436 [00:09<00:00, 44.03it/s] 99%|█████████▉| 432/436 [00:09<00:00, 44.04it/s]100%|██████████| 436/436 [00:09<00:00, 44.14it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 15:06:28,895 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:28,896 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:28,896 >>   eval_loss               =     0.9472
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:28,896 >>   eval_runtime            = 0:00:09.89
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:28,896 >>   eval_samples            =       3488
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:28,896 >>   eval_samples_per_second =    352.371
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:28,896 >>   eval_steps_per_second   =     44.046
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:28,896 >>   perplexity              =     2.5786
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:35,440 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:35,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:35,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:35,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:35,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:06:36,053 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:06:36,054 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:06:36,624 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:06:37,675 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:06:37,675 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:40,808 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:40,815 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:40,815 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:40,815 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:40,815 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:06:41,424 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:06:41,425 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:06:42,002 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:06:42,175 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:06:42,175 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11910
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12010, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.34it/s]Extractor Predicting: 2it [00:01,  1.31it/s]Extractor Predicting: 3it [00:02,  1.33it/s]Extractor Predicting: 4it [00:02,  1.37it/s]Extractor Predicting: 5it [00:03,  1.40it/s]Extractor Predicting: 6it [00:04,  1.40it/s]Extractor Predicting: 7it [00:05,  1.42it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:06,  1.42it/s]Extractor Predicting: 10it [00:07,  1.42it/s]Extractor Predicting: 11it [00:07,  1.42it/s]Extractor Predicting: 12it [00:08,  1.40it/s]Extractor Predicting: 13it [00:09,  1.40it/s]Extractor Predicting: 14it [00:10,  1.39it/s]Extractor Predicting: 15it [00:10,  1.34it/s]Extractor Predicting: 16it [00:11,  1.34it/s]Extractor Predicting: 17it [00:12,  1.35it/s]Extractor Predicting: 18it [00:13,  1.37it/s]Extractor Predicting: 19it [00:13,  1.41it/s]Extractor Predicting: 20it [00:14,  1.41it/s]Extractor Predicting: 21it [00:15,  1.39it/s]Extractor Predicting: 22it [00:15,  1.41it/s]Extractor Predicting: 23it [00:16,  1.41it/s]Extractor Predicting: 24it [00:17,  1.42it/s]Extractor Predicting: 25it [00:17,  1.39it/s]Extractor Predicting: 26it [00:18,  1.39it/s]Extractor Predicting: 27it [00:19,  1.39it/s]Extractor Predicting: 28it [00:20,  1.39it/s]Extractor Predicting: 29it [00:20,  1.38it/s]Extractor Predicting: 30it [00:21,  1.37it/s]Extractor Predicting: 31it [00:22,  1.35it/s]Extractor Predicting: 32it [00:23,  1.37it/s]Extractor Predicting: 33it [00:23,  1.35it/s]Extractor Predicting: 34it [00:24,  1.34it/s]Extractor Predicting: 35it [00:25,  1.32it/s]Extractor Predicting: 36it [00:26,  1.31it/s]Extractor Predicting: 37it [00:26,  1.32it/s]Extractor Predicting: 38it [00:27,  1.31it/s]Extractor Predicting: 39it [00:28,  1.32it/s]Extractor Predicting: 40it [00:29,  1.31it/s]Extractor Predicting: 41it [00:29,  1.31it/s]Extractor Predicting: 42it [00:30,  1.30it/s]Extractor Predicting: 43it [00:31,  1.28it/s]Extractor Predicting: 44it [00:32,  1.28it/s]Extractor Predicting: 45it [00:33,  1.27it/s]Extractor Predicting: 46it [00:33,  1.26it/s]Extractor Predicting: 47it [00:34,  1.30it/s]Extractor Predicting: 48it [00:35,  1.30it/s]Extractor Predicting: 49it [00:36,  1.33it/s]Extractor Predicting: 50it [00:36,  1.29it/s]Extractor Predicting: 51it [00:37,  1.32it/s]Extractor Predicting: 52it [00:38,  1.33it/s]Extractor Predicting: 53it [00:39,  1.32it/s]Extractor Predicting: 54it [00:39,  1.32it/s]Extractor Predicting: 55it [00:40,  1.32it/s]Extractor Predicting: 56it [00:41,  1.31it/s]Extractor Predicting: 57it [00:42,  1.34it/s]Extractor Predicting: 58it [00:42,  1.34it/s]Extractor Predicting: 59it [00:43,  1.31it/s]Extractor Predicting: 60it [00:44,  1.29it/s]Extractor Predicting: 61it [00:45,  1.28it/s]Extractor Predicting: 62it [00:46,  1.29it/s]Extractor Predicting: 63it [00:46,  1.30it/s]Extractor Predicting: 64it [00:47,  1.28it/s]Extractor Predicting: 65it [00:48,  1.31it/s]Extractor Predicting: 66it [00:49,  1.32it/s]Extractor Predicting: 67it [00:49,  1.32it/s]Extractor Predicting: 68it [00:50,  1.33it/s]Extractor Predicting: 69it [00:51,  1.32it/s]Extractor Predicting: 70it [00:52,  1.32it/s]Extractor Predicting: 71it [00:52,  1.30it/s]Extractor Predicting: 72it [00:53,  1.31it/s]Extractor Predicting: 73it [00:54,  1.31it/s]Extractor Predicting: 74it [00:55,  1.32it/s]Extractor Predicting: 75it [00:55,  1.33it/s]Extractor Predicting: 76it [00:56,  1.33it/s]Extractor Predicting: 77it [00:57,  1.35it/s]Extractor Predicting: 78it [00:58,  1.32it/s]Extractor Predicting: 79it [00:58,  1.32it/s]Extractor Predicting: 80it [00:59,  1.30it/s]Extractor Predicting: 81it [01:00,  1.30it/s]Extractor Predicting: 82it [01:01,  1.21it/s]Extractor Predicting: 83it [01:02,  1.27it/s]Extractor Predicting: 84it [01:02,  1.27it/s]Extractor Predicting: 85it [01:03,  1.27it/s]Extractor Predicting: 86it [01:04,  1.29it/s]Extractor Predicting: 87it [01:05,  1.31it/s]Extractor Predicting: 88it [01:05,  1.33it/s]Extractor Predicting: 89it [01:06,  1.33it/s]Extractor Predicting: 90it [01:07,  1.36it/s]Extractor Predicting: 91it [01:08,  1.40it/s]Extractor Predicting: 92it [01:08,  1.40it/s]Extractor Predicting: 93it [01:09,  1.38it/s]Extractor Predicting: 94it [01:10,  1.40it/s]Extractor Predicting: 95it [01:10,  1.40it/s]Extractor Predicting: 96it [01:11,  1.40it/s]Extractor Predicting: 97it [01:12,  1.38it/s]Extractor Predicting: 98it [01:13,  1.36it/s]Extractor Predicting: 99it [01:13,  1.33it/s]Extractor Predicting: 100it [01:14,  1.32it/s]Extractor Predicting: 101it [01:15,  1.36it/s]Extractor Predicting: 102it [01:16,  1.40it/s]Extractor Predicting: 103it [01:16,  1.39it/s]Extractor Predicting: 104it [01:17,  1.40it/s]Extractor Predicting: 105it [01:18,  1.38it/s]Extractor Predicting: 106it [01:18,  1.40it/s]Extractor Predicting: 107it [01:19,  1.38it/s]Extractor Predicting: 108it [01:20,  1.38it/s]Extractor Predicting: 109it [01:21,  1.38it/s]Extractor Predicting: 110it [01:21,  1.36it/s]Extractor Predicting: 111it [01:22,  1.39it/s]Extractor Predicting: 112it [01:23,  1.40it/s]Extractor Predicting: 113it [01:23,  1.44it/s]Extractor Predicting: 114it [01:24,  1.43it/s]Extractor Predicting: 115it [01:25,  1.44it/s]Extractor Predicting: 116it [01:26,  1.42it/s]Extractor Predicting: 117it [01:26,  1.40it/s]Extractor Predicting: 118it [01:27,  1.37it/s]Extractor Predicting: 119it [01:28,  1.35it/s]Extractor Predicting: 120it [01:29,  1.33it/s]Extractor Predicting: 121it [01:29,  1.36it/s]Extractor Predicting: 122it [01:30,  1.36it/s]Extractor Predicting: 123it [01:31,  1.35it/s]Extractor Predicting: 124it [01:32,  1.33it/s]Extractor Predicting: 125it [01:32,  1.32it/s]Extractor Predicting: 126it [01:33,  1.29it/s]Extractor Predicting: 127it [01:34,  1.30it/s]Extractor Predicting: 128it [01:35,  1.36it/s]Extractor Predicting: 129it [01:35,  1.33it/s]Extractor Predicting: 130it [01:36,  1.35it/s]Extractor Predicting: 131it [01:37,  1.34it/s]Extractor Predicting: 132it [01:38,  1.34it/s]Extractor Predicting: 133it [01:38,  1.33it/s]Extractor Predicting: 134it [01:39,  1.31it/s]Extractor Predicting: 135it [01:40,  1.31it/s]Extractor Predicting: 136it [01:41,  1.33it/s]Extractor Predicting: 137it [01:41,  1.33it/s]Extractor Predicting: 138it [01:42,  1.34it/s]Extractor Predicting: 139it [01:43,  1.32it/s]Extractor Predicting: 140it [01:44,  1.35it/s]Extractor Predicting: 141it [01:44,  1.33it/s]Extractor Predicting: 142it [01:45,  1.30it/s]Extractor Predicting: 143it [01:46,  1.32it/s]Extractor Predicting: 144it [01:46,  1.59it/s]Extractor Predicting: 144it [01:46,  1.35it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:36,142 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:36,148 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:36,148 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:36,148 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:36,148 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:08:36,568 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:08:36,569 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:08:36,828 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:08:37,889 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:08:37,889 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:39,217 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:39,222 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:39,223 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:39,223 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:39,223 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:08:39,563 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:08:39,569 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:08:39,842 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:08:40,015 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:08:40,016 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4779969650986343,
  "recall": 0.09030963302752294,
  "score": 0.15191704846877263,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19834
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19934, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.45it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:02,  1.36it/s]Extractor Predicting: 4it [00:02,  1.34it/s]Extractor Predicting: 5it [00:03,  1.32it/s]Extractor Predicting: 6it [00:04,  1.32it/s]Extractor Predicting: 7it [00:05,  1.34it/s]Extractor Predicting: 8it [00:05,  1.34it/s]Extractor Predicting: 9it [00:06,  1.36it/s]Extractor Predicting: 10it [00:07,  1.32it/s]Extractor Predicting: 11it [00:08,  1.31it/s]Extractor Predicting: 12it [00:08,  1.32it/s]Extractor Predicting: 13it [00:09,  1.33it/s]Extractor Predicting: 14it [00:10,  1.33it/s]Extractor Predicting: 15it [00:11,  1.32it/s]Extractor Predicting: 16it [00:12,  1.32it/s]Extractor Predicting: 17it [00:12,  1.30it/s]Extractor Predicting: 18it [00:13,  1.32it/s]Extractor Predicting: 19it [00:14,  1.34it/s]Extractor Predicting: 20it [00:15,  1.34it/s]Extractor Predicting: 21it [00:15,  1.34it/s]Extractor Predicting: 22it [00:16,  1.37it/s]Extractor Predicting: 23it [00:17,  1.38it/s]Extractor Predicting: 24it [00:17,  1.35it/s]Extractor Predicting: 25it [00:18,  1.33it/s]Extractor Predicting: 26it [00:19,  1.35it/s]Extractor Predicting: 27it [00:20,  1.33it/s]Extractor Predicting: 28it [00:20,  1.32it/s]Extractor Predicting: 29it [00:21,  1.33it/s]Extractor Predicting: 30it [00:22,  1.29it/s]Extractor Predicting: 31it [00:23,  1.35it/s]Extractor Predicting: 32it [00:23,  1.40it/s]Extractor Predicting: 33it [00:24,  1.43it/s]Extractor Predicting: 34it [00:25,  1.45it/s]Extractor Predicting: 35it [00:25,  1.47it/s]Extractor Predicting: 36it [00:26,  1.47it/s]Extractor Predicting: 37it [00:27,  1.48it/s]Extractor Predicting: 38it [00:27,  1.50it/s]Extractor Predicting: 39it [00:28,  1.53it/s]Extractor Predicting: 40it [00:29,  1.54it/s]Extractor Predicting: 41it [00:29,  1.53it/s]Extractor Predicting: 42it [00:30,  1.53it/s]Extractor Predicting: 43it [00:31,  1.52it/s]Extractor Predicting: 44it [00:31,  1.55it/s]Extractor Predicting: 45it [00:32,  1.53it/s]Extractor Predicting: 46it [00:33,  1.48it/s]Extractor Predicting: 47it [00:33,  1.52it/s]Extractor Predicting: 48it [00:34,  1.55it/s]Extractor Predicting: 49it [00:34,  1.56it/s]Extractor Predicting: 50it [00:35,  1.54it/s]Extractor Predicting: 51it [00:36,  1.56it/s]Extractor Predicting: 52it [00:36,  1.54it/s]Extractor Predicting: 53it [00:37,  1.53it/s]Extractor Predicting: 54it [00:38,  1.51it/s]Extractor Predicting: 55it [00:38,  1.49it/s]Extractor Predicting: 56it [00:39,  1.49it/s]Extractor Predicting: 57it [00:40,  1.48it/s]Extractor Predicting: 58it [00:41,  1.45it/s]Extractor Predicting: 59it [00:41,  1.41it/s]Extractor Predicting: 60it [00:42,  1.37it/s]Extractor Predicting: 61it [00:43,  1.31it/s]Extractor Predicting: 62it [00:44,  1.27it/s]Extractor Predicting: 63it [00:45,  1.28it/s]Extractor Predicting: 64it [00:45,  1.27it/s]Extractor Predicting: 65it [00:46,  1.30it/s]Extractor Predicting: 66it [00:47,  1.30it/s]Extractor Predicting: 67it [00:48,  1.30it/s]Extractor Predicting: 68it [00:48,  1.28it/s]Extractor Predicting: 69it [00:49,  1.26it/s]Extractor Predicting: 70it [00:50,  1.25it/s]Extractor Predicting: 71it [00:51,  1.23it/s]Extractor Predicting: 72it [00:52,  1.26it/s]Extractor Predicting: 73it [00:53,  1.17it/s]Extractor Predicting: 74it [00:53,  1.21it/s]Extractor Predicting: 75it [00:54,  1.24it/s]Extractor Predicting: 76it [00:55,  1.26it/s]Extractor Predicting: 77it [00:56,  1.26it/s]Extractor Predicting: 78it [00:57,  1.25it/s]Extractor Predicting: 79it [00:57,  1.24it/s]Extractor Predicting: 80it [00:58,  1.25it/s]Extractor Predicting: 81it [00:59,  1.24it/s]Extractor Predicting: 82it [01:00,  1.23it/s]Extractor Predicting: 83it [01:01,  1.24it/s]Extractor Predicting: 84it [01:01,  1.27it/s]Extractor Predicting: 85it [01:02,  1.26it/s]Extractor Predicting: 86it [01:03,  1.24it/s]Extractor Predicting: 87it [01:04,  1.24it/s]Extractor Predicting: 88it [01:05,  1.26it/s]Extractor Predicting: 89it [01:05,  1.27it/s]Extractor Predicting: 90it [01:06,  1.30it/s]Extractor Predicting: 91it [01:07,  1.27it/s]Extractor Predicting: 92it [01:08,  1.25it/s]Extractor Predicting: 93it [01:08,  1.29it/s]Extractor Predicting: 94it [01:09,  1.29it/s]Extractor Predicting: 95it [01:10,  1.31it/s]Extractor Predicting: 96it [01:11,  1.35it/s]Extractor Predicting: 97it [01:11,  1.34it/s]Extractor Predicting: 98it [01:12,  1.37it/s]Extractor Predicting: 99it [01:13,  1.36it/s]Extractor Predicting: 100it [01:14,  1.35it/s]Extractor Predicting: 101it [01:14,  1.41it/s]Extractor Predicting: 102it [01:15,  1.41it/s]Extractor Predicting: 103it [01:16,  1.37it/s]Extractor Predicting: 104it [01:16,  1.32it/s]Extractor Predicting: 105it [01:17,  1.33it/s]Extractor Predicting: 106it [01:18,  1.32it/s]Extractor Predicting: 107it [01:19,  1.34it/s]Extractor Predicting: 108it [01:20,  1.32it/s]Extractor Predicting: 109it [01:20,  1.31it/s]Extractor Predicting: 110it [01:21,  1.29it/s]Extractor Predicting: 111it [01:22,  1.32it/s]Extractor Predicting: 112it [01:23,  1.29it/s]Extractor Predicting: 113it [01:23,  1.33it/s]Extractor Predicting: 114it [01:24,  1.29it/s]Extractor Predicting: 115it [01:25,  1.30it/s]Extractor Predicting: 116it [01:26,  1.36it/s]Extractor Predicting: 117it [01:26,  1.35it/s]Extractor Predicting: 118it [01:27,  1.40it/s]Extractor Predicting: 119it [01:28,  1.42it/s]Extractor Predicting: 120it [01:28,  1.43it/s]Extractor Predicting: 121it [01:29,  1.46it/s]Extractor Predicting: 122it [01:30,  1.45it/s]Extractor Predicting: 123it [01:30,  1.46it/s]Extractor Predicting: 124it [01:31,  1.47it/s]Extractor Predicting: 125it [01:32,  1.48it/s]Extractor Predicting: 126it [01:32,  1.45it/s]Extractor Predicting: 127it [01:33,  1.44it/s]Extractor Predicting: 128it [01:34,  1.48it/s]Extractor Predicting: 129it [01:34,  1.48it/s]Extractor Predicting: 130it [01:35,  1.54it/s]Extractor Predicting: 131it [01:36,  1.56it/s]Extractor Predicting: 132it [01:36,  1.53it/s]Extractor Predicting: 133it [01:37,  1.48it/s]Extractor Predicting: 134it [01:38,  1.46it/s]Extractor Predicting: 135it [01:38,  1.49it/s]Extractor Predicting: 136it [01:39,  1.51it/s]Extractor Predicting: 137it [01:40,  1.51it/s]Extractor Predicting: 138it [01:40,  1.51it/s]Extractor Predicting: 139it [01:41,  1.48it/s]Extractor Predicting: 140it [01:42,  1.51it/s]Extractor Predicting: 141it [01:42,  1.49it/s]Extractor Predicting: 142it [01:43,  1.54it/s]Extractor Predicting: 143it [01:44,  1.52it/s]Extractor Predicting: 144it [01:44,  1.50it/s]Extractor Predicting: 145it [01:45,  1.48it/s]Extractor Predicting: 146it [01:46,  1.44it/s]Extractor Predicting: 147it [01:47,  1.41it/s]Extractor Predicting: 148it [01:47,  1.41it/s]Extractor Predicting: 149it [01:48,  1.38it/s]Extractor Predicting: 150it [01:49,  1.36it/s]Extractor Predicting: 151it [01:49,  1.37it/s]Extractor Predicting: 152it [01:50,  1.37it/s]Extractor Predicting: 153it [01:51,  1.34it/s]Extractor Predicting: 154it [01:52,  1.36it/s]Extractor Predicting: 155it [01:52,  1.38it/s]Extractor Predicting: 156it [01:53,  1.36it/s]Extractor Predicting: 157it [01:54,  1.37it/s]Extractor Predicting: 158it [01:55,  1.35it/s]Extractor Predicting: 159it [01:55,  1.34it/s]Extractor Predicting: 160it [01:56,  1.33it/s]Extractor Predicting: 161it [01:57,  1.33it/s]Extractor Predicting: 162it [01:58,  1.32it/s]Extractor Predicting: 163it [01:58,  1.34it/s]Extractor Predicting: 164it [01:59,  1.34it/s]Extractor Predicting: 165it [02:00,  1.32it/s]Extractor Predicting: 166it [02:01,  1.29it/s]Extractor Predicting: 167it [02:01,  1.32it/s]Extractor Predicting: 168it [02:02,  1.31it/s]Extractor Predicting: 169it [02:03,  1.34it/s]Extractor Predicting: 170it [02:04,  1.33it/s]Extractor Predicting: 171it [02:04,  1.33it/s]Extractor Predicting: 172it [02:05,  1.34it/s]Extractor Predicting: 173it [02:06,  1.32it/s]Extractor Predicting: 174it [02:07,  1.33it/s]Extractor Predicting: 175it [02:07,  1.34it/s]Extractor Predicting: 176it [02:08,  1.23it/s]Extractor Predicting: 177it [02:09,  1.25it/s]Extractor Predicting: 178it [02:10,  1.28it/s]Extractor Predicting: 179it [02:11,  1.29it/s]Extractor Predicting: 180it [02:11,  1.30it/s]Extractor Predicting: 181it [02:12,  1.30it/s]Extractor Predicting: 182it [02:13,  1.31it/s]Extractor Predicting: 183it [02:14,  1.31it/s]Extractor Predicting: 184it [02:14,  1.32it/s]Extractor Predicting: 185it [02:15,  1.33it/s]Extractor Predicting: 186it [02:16,  1.31it/s]Extractor Predicting: 187it [02:17,  1.33it/s]Extractor Predicting: 188it [02:17,  1.33it/s]Extractor Predicting: 189it [02:18,  1.35it/s]Extractor Predicting: 190it [02:19,  1.34it/s]Extractor Predicting: 191it [02:20,  1.36it/s]Extractor Predicting: 192it [02:20,  1.36it/s]Extractor Predicting: 193it [02:21,  1.39it/s]Extractor Predicting: 194it [02:22,  1.36it/s]Extractor Predicting: 195it [02:23,  1.36it/s]Extractor Predicting: 196it [02:23,  1.36it/s]Extractor Predicting: 197it [02:24,  1.36it/s]Extractor Predicting: 198it [02:25,  1.32it/s]Extractor Predicting: 199it [02:26,  1.33it/s]Extractor Predicting: 200it [02:26,  1.33it/s]Extractor Predicting: 201it [02:27,  1.36it/s]Extractor Predicting: 202it [02:28,  1.34it/s]Extractor Predicting: 203it [02:29,  1.37it/s]Extractor Predicting: 204it [02:29,  1.35it/s]Extractor Predicting: 205it [02:30,  1.34it/s]Extractor Predicting: 206it [02:31,  1.39it/s]Extractor Predicting: 207it [02:31,  1.38it/s]Extractor Predicting: 208it [02:32,  1.40it/s]Extractor Predicting: 209it [02:33,  1.38it/s]Extractor Predicting: 210it [02:34,  1.38it/s]Extractor Predicting: 211it [02:34,  1.36it/s]Extractor Predicting: 212it [02:35,  1.32it/s]Extractor Predicting: 213it [02:36,  1.33it/s]Extractor Predicting: 214it [02:37,  1.33it/s]Extractor Predicting: 215it [02:37,  1.31it/s]Extractor Predicting: 216it [02:38,  1.31it/s]Extractor Predicting: 217it [02:39,  1.27it/s]Extractor Predicting: 218it [02:40,  1.29it/s]Extractor Predicting: 219it [02:41,  1.30it/s]Extractor Predicting: 220it [02:41,  1.34it/s]Extractor Predicting: 221it [02:42,  1.33it/s]Extractor Predicting: 222it [02:43,  1.33it/s]Extractor Predicting: 223it [02:44,  1.34it/s]Extractor Predicting: 224it [02:44,  1.33it/s]Extractor Predicting: 225it [02:45,  1.32it/s]Extractor Predicting: 226it [02:46,  1.33it/s]Extractor Predicting: 227it [02:47,  1.32it/s]Extractor Predicting: 228it [02:47,  1.35it/s]Extractor Predicting: 229it [02:48,  1.32it/s]Extractor Predicting: 230it [02:49,  1.33it/s]Extractor Predicting: 231it [02:50,  1.30it/s]Extractor Predicting: 232it [02:50,  1.30it/s]Extractor Predicting: 233it [02:51,  1.30it/s]Extractor Predicting: 234it [02:52,  1.32it/s]Extractor Predicting: 235it [02:53,  1.33it/s]Extractor Predicting: 236it [02:53,  1.34it/s]Extractor Predicting: 237it [02:54,  1.28it/s]Extractor Predicting: 238it [02:55,  1.27it/s]Extractor Predicting: 239it [02:56,  1.27it/s]Extractor Predicting: 240it [02:57,  1.29it/s]Extractor Predicting: 241it [02:57,  1.28it/s]Extractor Predicting: 242it [02:58,  1.30it/s]Extractor Predicting: 243it [02:59,  1.30it/s]Extractor Predicting: 244it [03:00,  1.30it/s]Extractor Predicting: 245it [03:00,  1.30it/s]Extractor Predicting: 246it [03:01,  1.28it/s]Extractor Predicting: 247it [03:02,  1.24it/s]Extractor Predicting: 248it [03:03,  1.26it/s]Extractor Predicting: 249it [03:04,  1.31it/s]Extractor Predicting: 250it [03:04,  1.33it/s]Extractor Predicting: 251it [03:05,  1.31it/s]Extractor Predicting: 252it [03:06,  1.34it/s]Extractor Predicting: 253it [03:07,  1.34it/s]Extractor Predicting: 254it [03:07,  1.31it/s]Extractor Predicting: 255it [03:08,  1.31it/s]Extractor Predicting: 256it [03:09,  1.29it/s]Extractor Predicting: 257it [03:10,  1.30it/s]Extractor Predicting: 258it [03:10,  1.29it/s]Extractor Predicting: 259it [03:11,  1.32it/s]Extractor Predicting: 260it [03:12,  1.33it/s]Extractor Predicting: 261it [03:13,  1.35it/s]Extractor Predicting: 262it [03:13,  1.35it/s]Extractor Predicting: 263it [03:14,  1.34it/s]Extractor Predicting: 264it [03:15,  1.24it/s]Extractor Predicting: 265it [03:16,  1.27it/s]Extractor Predicting: 266it [03:17,  1.29it/s]Extractor Predicting: 267it [03:17,  1.32it/s]Extractor Predicting: 268it [03:18,  1.33it/s]Extractor Predicting: 269it [03:19,  1.33it/s]Extractor Predicting: 270it [03:20,  1.31it/s]Extractor Predicting: 271it [03:20,  1.34it/s]Extractor Predicting: 272it [03:21,  1.35it/s]Extractor Predicting: 273it [03:22,  1.37it/s]Extractor Predicting: 274it [03:22,  1.38it/s]Extractor Predicting: 275it [03:23,  1.39it/s]Extractor Predicting: 276it [03:24,  1.36it/s]Extractor Predicting: 277it [03:25,  1.36it/s]Extractor Predicting: 278it [03:25,  1.37it/s]Extractor Predicting: 279it [03:26,  1.37it/s]Extractor Predicting: 280it [03:27,  1.35it/s]Extractor Predicting: 281it [03:27,  1.38it/s]Extractor Predicting: 282it [03:28,  1.37it/s]Extractor Predicting: 283it [03:29,  1.33it/s]Extractor Predicting: 284it [03:29,  1.52it/s]Extractor Predicting: 284it [03:29,  1.35it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:12:17,441 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:12:17,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:12:17,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:12:17,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:12:17,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:12:17,760 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:12:17,765 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:12:18,024 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:12:19,109 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:12:19,109 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:12:20,388 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:12:20,390 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:12:20,390 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:12:20,390 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:12:20,390 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:12:21,116 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:12:21,121 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:12:21,380 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:12:21,554 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:12:21,554 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5665914221218962,
  "recall": 0.07380182299323729,
  "score": 0.13059313215400623,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1045
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1145, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.31it/s]Extractor Predicting: 2it [00:01,  1.19it/s]Extractor Predicting: 3it [00:02,  1.22it/s]Extractor Predicting: 4it [00:03,  1.25it/s]Extractor Predicting: 5it [00:03,  1.71it/s]Extractor Predicting: 5it [00:03,  1.46it/s]
[INFO|configuration_utils.py:515] 2023-08-28 15:12:25,384 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:12:25,385 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 15:12:25,389 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:12:25,389 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 15:12:25,393 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 15:12:29,484 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 15:12:29,490 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 15:12:29,515 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:12:29,516 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 15:12:29,526 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:12:29,532 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:12:29,532 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:12:29,532 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:12:29,532 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:12:29,532 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:12:29,532 >> loading file outputs/wrapper/fewrel/unseen_10_seed_3/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 1.0,
  "recall": 0.005050505050505051,
  "score": 0.010050251256281409,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 15:12:29,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:30,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:31,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:32,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:32,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:33,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:34,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:35,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:36,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:36,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:37,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:38,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:39,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:40,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:40,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:41,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:42,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:43,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:44,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:45,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:46,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:46,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:47,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:48,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:49,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:50,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:51,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:51,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:52,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:23<05:30, 23.58s/it][WARNING|generation_utils.py:914] 2023-08-28 15:12:53,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:54,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:54,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:55,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:56,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:56,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:57,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:58,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:12:59,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:00,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:01,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:01,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:02,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:03,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:04,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:04,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:05,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:06,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:07,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:07,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:08,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:09,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:40<04:15, 19.63s/it][WARNING|generation_utils.py:914] 2023-08-28 15:13:10,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:11,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:12,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:12,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:13,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:14,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:15,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:16,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:17,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:18,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:19,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:19,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:20,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:21,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:22,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:23,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:24,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:25,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:26,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:26,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:27,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:28,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:29,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [01:00<03:55, 19.66s/it][WARNING|generation_utils.py:914] 2023-08-28 15:13:29,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:30,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:31,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:32,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:33,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:34,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:35,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:35,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:36,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:37,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:38,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:38,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:39,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:40,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:41,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:42,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:42,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:43,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:44,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:45,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:46,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:47,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:47,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:48,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:49,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:50,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:21<03:44, 20.37s/it][WARNING|generation_utils.py:914] 2023-08-28 15:13:51,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:52,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:52,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:53,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:54,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:55,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:56,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:56,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:57,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:58,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:59,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:59,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:00,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:01,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:02,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:02,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:03,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:04,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:04,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:05,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:06,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:06,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:07,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:08,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:39<03:15, 19.51s/it][WARNING|generation_utils.py:914] 2023-08-28 15:14:09,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:10,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:10,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:11,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:12,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:13,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:13,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:14,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:15,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:15,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:16,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:17,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:18,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:18,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:19,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:20,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:21,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:21,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:22,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:23,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:23,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:24,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:25,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:26,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:26,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:28,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:29,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:29,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [02:00<02:59, 19.98s/it][WARNING|generation_utils.py:914] 2023-08-28 15:14:30,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:31,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:31,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:32,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:33,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:33,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:34,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:35,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:35,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:36,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:37,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:37,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:38,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:39,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:39,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:40,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:41,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:42,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:42,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:43,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:44,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:44,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:45,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:46,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:47,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:47,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:48,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:19<02:36, 19.62s/it][WARNING|generation_utils.py:914] 2023-08-28 15:14:49,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:50,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:50,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:51,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:52,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:53,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:54,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:55,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:55,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:56,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:57,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:58,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:59,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:00,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:01,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:02,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:03,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:03,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:04,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:05,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:06,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:07,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:08,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:09,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:10,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:41<02:22, 20.35s/it][WARNING|generation_utils.py:914] 2023-08-28 15:15:11,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:11,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:12,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:13,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:14,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:15,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:15,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:16,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:17,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:18,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:19,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:20,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:21,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:22,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:22,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:23,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:24,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:25,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:25,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:26,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:27,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:28,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:29,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:30,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:30,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [03:02<02:03, 20.57s/it][WARNING|generation_utils.py:914] 2023-08-28 15:15:32,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:32,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:33,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:34,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:35,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:36,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:37,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:38,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:38,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:39,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:40,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:41,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:41,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:42,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:43,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:44,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:45,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:45,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:46,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:47,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:48,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:49,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:49,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:50,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:51,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [03:22<01:42, 20.49s/it][WARNING|generation_utils.py:914] 2023-08-28 15:15:52,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:53,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:53,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:54,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:55,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:55,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:56,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:57,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:57,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:58,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:59,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:00,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:00,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:01,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:02,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:02,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:03,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:04,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:05,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:05,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:06,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:07,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:07,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:38<01:16, 19.21s/it][WARNING|generation_utils.py:914] 2023-08-28 15:16:08,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:09,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:10,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:10,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:11,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:12,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:13,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:14,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:15,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:15,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:16,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:17,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:18,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:19,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:20,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:21,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:22,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:23,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:24,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:24,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:25,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:26,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:27,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:28,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:59<00:58, 19.55s/it][WARNING|generation_utils.py:914] 2023-08-28 15:16:29,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:29,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:30,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:31,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:32,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:32,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:33,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:34,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:35,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:36,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:37,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:38,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:38,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:39,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:40,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:41,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:42,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:43,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:43,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:44,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:45,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:46,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:46,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:47,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:48,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [04:19<00:39, 19.81s/it][WARNING|generation_utils.py:914] 2023-08-28 15:16:49,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:50,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:50,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:51,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:52,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:53,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:54,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:55,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:55,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:56,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:58,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:58,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:59,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:00,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:01,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:02,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:03,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:04,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:04,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:05,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:06,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:07,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:08,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:09,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [04:40<00:20, 20.06s/it][WARNING|generation_utils.py:914] 2023-08-28 15:17:10,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:10,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:11,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:12,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:12,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:13,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:14,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:15,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:16,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:16,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:17,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:18,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:19,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:19,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:20,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:21,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:21,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:22,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:23,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:24,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:24,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:25,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:17:26,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:57<00:00, 19.08s/it]Generating: 100%|██████████| 15/15 [04:57<00:00, 19.81s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:17:33,413 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:17:33,419 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:17:33,419 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:17:33,419 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:17:33,419 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:17:34,030 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:17:34,031 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:17:34,603 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:17:35,675 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:17:35,675 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:17:38,553 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:17:38,558 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:17:38,559 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:17:38,559 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:17:38,559 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:17:39,205 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:17:39,207 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:17:39,795 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:17:39,971 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:17:39,971 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 137, 'raw': 192}
{'target': 600, 'success': 154, 'raw': 224}
{'target': 600, 'success': 172, 'raw': 256}
{'target': 600, 'success': 196, 'raw': 288}
{'target': 600, 'success': 215, 'raw': 320}
{'target': 600, 'success': 237, 'raw': 352}
{'target': 600, 'success': 259, 'raw': 384}
{'target': 600, 'success': 280, 'raw': 416}
{'target': 600, 'success': 301, 'raw': 448}
{'target': 600, 'success': 321, 'raw': 480}
{'target': 600, 'success': 340, 'raw': 512}
{'target': 600, 'success': 361, 'raw': 544}
{'target': 600, 'success': 385, 'raw': 576}
{'target': 600, 'success': 404, 'raw': 608}
{'target': 600, 'success': 426, 'raw': 640}
{'target': 600, 'success': 448, 'raw': 672}
{'target': 600, 'success': 466, 'raw': 704}
{'target': 600, 'success': 490, 'raw': 736}
{'target': 600, 'success': 515, 'raw': 768}
{'target': 600, 'success': 531, 'raw': 800}
{'target': 600, 'success': 550, 'raw': 832}
{'target': 600, 'success': 569, 'raw': 864}
{'target': 600, 'success': 590, 'raw': 896}
{'target': 600, 'success': 616, 'raw': 928}
{'prompt': 'Relation : genre .', 'success_rate': 0.6637931034482759, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'The Big Lebowski\', \'genre\', \'\', \'The album was featured on the soundtrack for " The Big Lebowski " .\')'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 628, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8920454545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.84375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 227, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 324, 'raw': 448}
{'target': 600, 'success': 353, 'raw': 480}
{'target': 600, 'success': 378, 'raw': 512}
{'target': 600, 'success': 400, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 446, 'raw': 608}
{'target': 600, 'success': 465, 'raw': 640}
{'target': 600, 'success': 493, 'raw': 672}
{'target': 600, 'success': 522, 'raw': 704}
{'target': 600, 'success': 547, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 594, 'raw': 800}
{'target': 600, 'success': 617, 'raw': 832}
{'prompt': 'Relation : participant in .', 'success_rate': 0.7415865384615384, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 324, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 478, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 584, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : participating team .', 'success_rate': 0.7916666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 111, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 149, 'raw': 224}
{'target': 600, 'success': 173, 'raw': 256}
{'target': 600, 'success': 197, 'raw': 288}
{'target': 600, 'success': 214, 'raw': 320}
{'target': 600, 'success': 238, 'raw': 352}
{'target': 600, 'success': 263, 'raw': 384}
{'target': 600, 'success': 288, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 333, 'raw': 480}
{'target': 600, 'success': 354, 'raw': 512}
{'target': 600, 'success': 375, 'raw': 544}
{'target': 600, 'success': 396, 'raw': 576}
{'target': 600, 'success': 417, 'raw': 608}
{'target': 600, 'success': 437, 'raw': 640}
{'target': 600, 'success': 457, 'raw': 672}
{'target': 600, 'success': 486, 'raw': 704}
{'target': 600, 'success': 509, 'raw': 736}
{'target': 600, 'success': 530, 'raw': 768}
{'target': 600, 'success': 553, 'raw': 800}
{'target': 600, 'success': 578, 'raw': 832}
{'target': 600, 'success': 599, 'raw': 864}
{'target': 600, 'success': 620, 'raw': 896}
{'prompt': 'Relation : competition class .', 'success_rate': 0.6919642857142857, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 192, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 284, 'raw': 384}
{'target': 600, 'success': 308, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 352, 'raw': 480}
{'target': 600, 'success': 372, 'raw': 512}
{'target': 600, 'success': 392, 'raw': 544}
{'target': 600, 'success': 415, 'raw': 576}
{'target': 600, 'success': 442, 'raw': 608}
{'target': 600, 'success': 462, 'raw': 640}
{'target': 600, 'success': 486, 'raw': 672}
{'target': 600, 'success': 507, 'raw': 704}
{'target': 600, 'success': 528, 'raw': 736}
{'target': 600, 'success': 549, 'raw': 768}
{'target': 600, 'success': 574, 'raw': 800}
{'target': 600, 'success': 597, 'raw': 832}
{'target': 600, 'success': 616, 'raw': 864}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7129629629629629, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : father . Context : Later in Life he married the daughter of the Countess of Leopold of Bordeaux and had two sons : Henry VIII ( in May 1520 , 12th century , married to Marie Antoinette of France , 1st century , married to Louis XV , 2nd century , married to Charles I , and married his grandson : Thomas Baule of Lorraine . Head Entity : Charles I , Tail Entity : Anne de Gaulle .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 390, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 482, 'raw': 640}
{'target': 600, 'success': 508, 'raw': 672}
{'target': 600, 'success': 528, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : father .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 220, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 342, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 467, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 513, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 564, 'raw': 736}
{'target': 600, 'success': 586, 'raw': 768}
{'target': 600, 'success': 607, 'raw': 800}
{'prompt': 'Relation : field of work .', 'success_rate': 0.75875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : heritage designation . Context : Later in the year ( 11 July 1871 , 28 June 1864 , 31 June 1872 , 31 February 1874 ) , the first museum to be commissioned by the British Government was opened . Head Entity : Museum , Tail Entity : British Government .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 214, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 266, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 390, 'raw': 512}
{'target': 600, 'success': 415, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 464, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 565, 'raw': 736}
{'target': 600, 'success': 590, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.7725, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8491847826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 590, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.7981770833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 291, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 499, 'raw': 640}
{'target': 600, 'success': 525, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 620, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.775, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 273, 'raw': 352}
{'target': 600, 'success': 300, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 377, 'raw': 480}
{'target': 600, 'success': 404, 'raw': 512}
{'target': 600, 'success': 431, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 487, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 589, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : occupation .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : record label .', 'success_rate': 0.84375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/synthetic/1_ext.jsonl'}}
estimate vocab size: 13296
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13396, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.06it/s]Extractor Estimating: 2it [00:01,  1.12it/s]Extractor Estimating: 3it [00:02,  1.14it/s]Extractor Estimating: 4it [00:03,  1.22it/s]Extractor Estimating: 5it [00:04,  1.26it/s]Extractor Estimating: 6it [00:04,  1.25it/s]Extractor Estimating: 7it [00:05,  1.23it/s]Extractor Estimating: 8it [00:06,  1.23it/s]Extractor Estimating: 9it [00:07,  1.26it/s]Extractor Estimating: 10it [00:08,  1.25it/s]Extractor Estimating: 11it [00:08,  1.24it/s]Extractor Estimating: 12it [00:09,  1.26it/s]Extractor Estimating: 13it [00:10,  1.31it/s]Extractor Estimating: 14it [00:11,  1.30it/s]Extractor Estimating: 15it [00:12,  1.24it/s]Extractor Estimating: 16it [00:13,  1.18it/s]Extractor Estimating: 17it [00:13,  1.24it/s]Extractor Estimating: 18it [00:14,  1.21it/s]Extractor Estimating: 19it [00:15,  1.18it/s]Extractor Estimating: 20it [00:16,  1.17it/s]Extractor Estimating: 21it [00:17,  1.15it/s]Extractor Estimating: 22it [00:18,  1.20it/s]Extractor Estimating: 23it [00:18,  1.20it/s]Extractor Estimating: 24it [00:19,  1.25it/s]Extractor Estimating: 25it [00:20,  1.24it/s]Extractor Estimating: 26it [00:21,  1.35it/s]Extractor Estimating: 27it [00:21,  1.39it/s]Extractor Estimating: 28it [00:22,  1.34it/s]Extractor Estimating: 29it [00:23,  1.36it/s]Extractor Estimating: 30it [00:23,  1.43it/s]Extractor Estimating: 31it [00:24,  1.47it/s]Extractor Estimating: 32it [00:25,  1.46it/s]Extractor Estimating: 33it [00:25,  1.44it/s]Extractor Estimating: 34it [00:26,  1.45it/s]Extractor Estimating: 35it [00:27,  1.47it/s]Extractor Estimating: 36it [00:27,  1.48it/s]Extractor Estimating: 37it [00:28,  1.32it/s]Extractor Estimating: 38it [00:29,  1.38it/s]Extractor Estimating: 39it [00:30,  1.44it/s]Extractor Estimating: 40it [00:30,  1.47it/s]Extractor Estimating: 41it [00:31,  1.43it/s]Extractor Estimating: 42it [00:32,  1.48it/s]Extractor Estimating: 43it [00:32,  1.45it/s]Extractor Estimating: 44it [00:33,  1.44it/s]Extractor Estimating: 45it [00:34,  1.39it/s]Extractor Estimating: 46it [00:34,  1.41it/s]Extractor Estimating: 47it [00:35,  1.44it/s]Extractor Estimating: 48it [00:36,  1.47it/s]Extractor Estimating: 49it [00:37,  1.45it/s]Extractor Estimating: 50it [00:37,  1.42it/s]Extractor Estimating: 51it [00:38,  1.44it/s]Extractor Estimating: 52it [00:39,  1.36it/s]Extractor Estimating: 53it [00:40,  1.32it/s]Extractor Estimating: 54it [00:40,  1.34it/s]Extractor Estimating: 55it [00:41,  1.30it/s]Extractor Estimating: 56it [00:42,  1.26it/s]Extractor Estimating: 57it [00:43,  1.23it/s]Extractor Estimating: 58it [00:44,  1.24it/s]Extractor Estimating: 59it [00:44,  1.22it/s]Extractor Estimating: 60it [00:45,  1.18it/s]Extractor Estimating: 61it [00:46,  1.22it/s]Extractor Estimating: 62it [00:47,  1.21it/s]Extractor Estimating: 63it [00:48,  1.28it/s]Extractor Estimating: 64it [00:48,  1.30it/s]Extractor Estimating: 65it [00:49,  1.30it/s]Extractor Estimating: 66it [00:50,  1.29it/s]Extractor Estimating: 67it [00:51,  1.29it/s]Extractor Estimating: 68it [00:51,  1.31it/s]Extractor Estimating: 69it [00:52,  1.22it/s]Extractor Estimating: 70it [00:53,  1.22it/s]Extractor Estimating: 71it [00:54,  1.30it/s]Extractor Estimating: 72it [00:55,  1.35it/s]Extractor Estimating: 73it [00:55,  1.35it/s]Extractor Estimating: 74it [00:56,  1.34it/s]Extractor Estimating: 75it [00:57,  1.26it/s]Extractor Estimating: 76it [00:58,  1.19it/s]Extractor Estimating: 77it [00:59,  1.22it/s]Extractor Estimating: 78it [00:59,  1.25it/s]Extractor Estimating: 79it [01:00,  1.27it/s]Extractor Estimating: 80it [01:01,  1.18it/s]Extractor Estimating: 81it [01:02,  1.20it/s]Extractor Estimating: 82it [01:03,  1.22it/s]Extractor Estimating: 83it [01:04,  1.25it/s]Extractor Estimating: 84it [01:04,  1.31it/s]Extractor Estimating: 85it [01:05,  1.30it/s]Extractor Estimating: 86it [01:06,  1.29it/s]Extractor Estimating: 87it [01:07,  1.27it/s]Extractor Estimating: 88it [01:07,  1.30it/s]Extractor Estimating: 89it [01:08,  1.32it/s]Extractor Estimating: 90it [01:09,  1.30it/s]Extractor Estimating: 91it [01:10,  1.34it/s]Extractor Estimating: 92it [01:10,  1.30it/s]Extractor Estimating: 93it [01:11,  1.29it/s]Extractor Estimating: 94it [01:12,  1.28it/s]Extractor Estimating: 95it [01:13,  1.35it/s]Extractor Estimating: 96it [01:13,  1.35it/s]Extractor Estimating: 97it [01:14,  1.36it/s]Extractor Estimating: 98it [01:15,  1.30it/s]Extractor Estimating: 99it [01:16,  1.26it/s]Extractor Estimating: 100it [01:17,  1.27it/s]Extractor Estimating: 101it [01:17,  1.35it/s]Extractor Estimating: 102it [01:18,  1.38it/s]Extractor Estimating: 103it [01:19,  1.38it/s]Extractor Estimating: 104it [01:19,  1.40it/s]Extractor Estimating: 105it [01:20,  1.33it/s]Extractor Estimating: 106it [01:21,  1.27it/s]Extractor Estimating: 107it [01:22,  1.31it/s]Extractor Estimating: 108it [01:22,  1.35it/s]Extractor Estimating: 109it [01:23,  1.36it/s]Extractor Estimating: 110it [01:24,  1.39it/s]Extractor Estimating: 111it [01:24,  1.44it/s]Extractor Estimating: 112it [01:25,  1.43it/s]Extractor Estimating: 113it [01:26,  1.38it/s]Extractor Estimating: 114it [01:27,  1.41it/s]Extractor Estimating: 115it [01:27,  1.41it/s]Extractor Estimating: 116it [01:28,  1.44it/s]Extractor Estimating: 117it [01:29,  1.45it/s]Extractor Estimating: 118it [01:29,  1.42it/s]Extractor Estimating: 119it [01:30,  1.40it/s]Extractor Estimating: 120it [01:31,  1.43it/s]Extractor Estimating: 121it [01:31,  1.45it/s]Extractor Estimating: 122it [01:32,  1.45it/s]Extractor Estimating: 123it [01:33,  1.47it/s]Extractor Estimating: 124it [01:34,  1.45it/s]Extractor Estimating: 125it [01:34,  1.44it/s]Extractor Estimating: 126it [01:35,  1.43it/s]Extractor Estimating: 127it [01:36,  1.43it/s]Extractor Estimating: 128it [01:36,  1.42it/s]Extractor Estimating: 129it [01:37,  1.37it/s]Extractor Estimating: 130it [01:38,  1.35it/s]Extractor Estimating: 131it [01:39,  1.28it/s]Extractor Estimating: 132it [01:39,  1.33it/s]Extractor Estimating: 133it [01:40,  1.44it/s]Extractor Estimating: 134it [01:41,  1.45it/s]Extractor Estimating: 135it [01:41,  1.45it/s]Extractor Estimating: 136it [01:42,  1.47it/s]Extractor Estimating: 137it [01:43,  1.45it/s]Extractor Estimating: 138it [01:43,  1.42it/s]Extractor Estimating: 139it [01:44,  1.47it/s]Extractor Estimating: 140it [01:45,  1.46it/s]Extractor Estimating: 141it [01:46,  1.43it/s]Extractor Estimating: 142it [01:46,  1.41it/s]Extractor Estimating: 143it [01:47,  1.43it/s]Extractor Estimating: 144it [01:48,  1.41it/s]Extractor Estimating: 145it [01:48,  1.42it/s]Extractor Estimating: 146it [01:49,  1.42it/s]Extractor Estimating: 147it [01:50,  1.42it/s]Extractor Estimating: 148it [01:51,  1.40it/s]Extractor Estimating: 149it [01:51,  1.39it/s]Extractor Estimating: 150it [01:52,  1.46it/s]Extractor Estimating: 151it [01:53,  1.39it/s]Extractor Estimating: 152it [01:53,  1.43it/s]Extractor Estimating: 153it [01:54,  1.41it/s]Extractor Estimating: 154it [01:55,  1.43it/s]Extractor Estimating: 155it [01:55,  1.44it/s]Extractor Estimating: 156it [01:56,  1.42it/s]Extractor Estimating: 157it [01:57,  1.43it/s]Extractor Estimating: 158it [01:58,  1.44it/s]Extractor Estimating: 159it [01:58,  1.46it/s]Extractor Estimating: 160it [01:59,  1.44it/s]Extractor Estimating: 161it [02:00,  1.48it/s]Extractor Estimating: 162it [02:00,  1.47it/s]Extractor Estimating: 163it [02:01,  1.46it/s]Extractor Estimating: 164it [02:02,  1.45it/s]Extractor Estimating: 165it [02:02,  1.46it/s]Extractor Estimating: 166it [02:03,  1.50it/s]Extractor Estimating: 167it [02:04,  1.49it/s]Extractor Estimating: 168it [02:04,  1.52it/s]Extractor Estimating: 169it [02:05,  1.48it/s]Extractor Estimating: 170it [02:06,  1.44it/s]Extractor Estimating: 171it [02:06,  1.43it/s]Extractor Estimating: 172it [02:07,  1.41it/s]Extractor Estimating: 173it [02:08,  1.41it/s]Extractor Estimating: 174it [02:09,  1.40it/s]Extractor Estimating: 175it [02:09,  1.41it/s]Extractor Estimating: 176it [02:10,  1.38it/s]Extractor Estimating: 177it [02:11,  1.37it/s]Extractor Estimating: 178it [02:11,  1.37it/s]Extractor Estimating: 179it [02:12,  1.29it/s]Extractor Estimating: 180it [02:13,  1.32it/s]Extractor Estimating: 181it [02:14,  1.30it/s]Extractor Estimating: 182it [02:15,  1.35it/s]Extractor Estimating: 183it [02:15,  1.30it/s]Extractor Estimating: 184it [02:16,  1.33it/s]Extractor Estimating: 185it [02:17,  1.33it/s]Extractor Estimating: 186it [02:18,  1.31it/s]Extractor Estimating: 187it [02:18,  1.33it/s]Extractor Estimating: 188it [02:19,  1.35it/s]Extractor Estimating: 189it [02:20,  1.32it/s]Extractor Estimating: 190it [02:21,  1.35it/s]Extractor Estimating: 191it [02:21,  1.34it/s]Extractor Estimating: 192it [02:22,  1.36it/s]Extractor Estimating: 193it [02:23,  1.30it/s]Extractor Estimating: 194it [02:24,  1.32it/s]Extractor Estimating: 195it [02:24,  1.33it/s]Extractor Estimating: 196it [02:25,  1.38it/s]Extractor Estimating: 197it [02:26,  1.38it/s]Extractor Estimating: 198it [02:26,  1.41it/s]Extractor Estimating: 199it [02:27,  1.37it/s]Extractor Estimating: 200it [02:28,  1.38it/s]Extractor Estimating: 201it [02:29,  1.36it/s]Extractor Estimating: 202it [02:30,  1.28it/s]Extractor Estimating: 203it [02:30,  1.29it/s]Extractor Estimating: 204it [02:31,  1.32it/s]Extractor Estimating: 205it [02:32,  1.29it/s]Extractor Estimating: 206it [02:33,  1.29it/s]Extractor Estimating: 207it [02:34,  1.21it/s]Extractor Estimating: 208it [02:34,  1.23it/s]Extractor Estimating: 209it [02:35,  1.16it/s]Extractor Estimating: 210it [02:36,  1.19it/s]Extractor Estimating: 211it [02:37,  1.22it/s]Extractor Estimating: 212it [02:38,  1.25it/s]Extractor Estimating: 213it [02:38,  1.28it/s]Extractor Estimating: 214it [02:39,  1.30it/s]Extractor Estimating: 215it [02:40,  1.24it/s]Extractor Estimating: 216it [02:41,  1.29it/s]Extractor Estimating: 217it [02:42,  1.28it/s]Extractor Estimating: 218it [02:42,  1.29it/s]Extractor Estimating: 219it [02:43,  1.30it/s]Extractor Estimating: 220it [02:44,  1.30it/s]Extractor Estimating: 221it [02:45,  1.32it/s]Extractor Estimating: 222it [02:45,  1.23it/s]Extractor Estimating: 223it [02:46,  1.24it/s]Extractor Estimating: 224it [02:47,  1.25it/s]Extractor Estimating: 225it [02:48,  1.17it/s]Extractor Estimating: 226it [02:49,  1.20it/s]Extractor Estimating: 227it [02:50,  1.22it/s]Extractor Estimating: 228it [02:50,  1.27it/s]Extractor Estimating: 229it [02:51,  1.29it/s]Extractor Estimating: 230it [02:52,  1.31it/s]Extractor Estimating: 231it [02:53,  1.29it/s]Extractor Estimating: 232it [02:53,  1.34it/s]Extractor Estimating: 233it [02:54,  1.33it/s]Extractor Estimating: 234it [02:55,  1.27it/s]Extractor Estimating: 235it [02:56,  1.34it/s]Extractor Estimating: 236it [02:56,  1.33it/s]Extractor Estimating: 237it [02:57,  1.29it/s]Extractor Estimating: 238it [02:58,  1.32it/s]Extractor Estimating: 239it [02:59,  1.37it/s]Extractor Estimating: 240it [02:59,  1.33it/s]Extractor Estimating: 241it [03:00,  1.36it/s]Extractor Estimating: 242it [03:01,  1.34it/s]Extractor Estimating: 243it [03:02,  1.33it/s]Extractor Estimating: 244it [03:02,  1.33it/s]Extractor Estimating: 245it [03:03,  1.37it/s]Extractor Estimating: 246it [03:04,  1.36it/s]Extractor Estimating: 247it [03:05,  1.33it/s]Extractor Estimating: 248it [03:05,  1.35it/s]Extractor Estimating: 249it [03:06,  1.32it/s]Extractor Estimating: 250it [03:07,  1.25it/s]Extractor Estimating: 251it [03:08,  1.30it/s]Extractor Estimating: 252it [03:08,  1.31it/s]Extractor Estimating: 253it [03:09,  1.32it/s]Extractor Estimating: 254it [03:10,  1.34it/s]Extractor Estimating: 255it [03:11,  1.35it/s]Extractor Estimating: 256it [03:11,  1.37it/s]Extractor Estimating: 257it [03:12,  1.36it/s]Extractor Estimating: 258it [03:13,  1.36it/s]Extractor Estimating: 259it [03:14,  1.37it/s]Extractor Estimating: 260it [03:14,  1.37it/s]Extractor Estimating: 261it [03:15,  1.36it/s]Extractor Estimating: 262it [03:16,  1.32it/s]Extractor Estimating: 263it [03:17,  1.27it/s]Extractor Estimating: 264it [03:17,  1.31it/s]Extractor Estimating: 265it [03:18,  1.34it/s]Extractor Estimating: 266it [03:19,  1.38it/s]Extractor Estimating: 267it [03:20,  1.35it/s]Extractor Estimating: 268it [03:20,  1.32it/s]Extractor Estimating: 269it [03:21,  1.34it/s]Extractor Estimating: 270it [03:22,  1.37it/s]Extractor Estimating: 271it [03:23,  1.33it/s]Extractor Estimating: 272it [03:23,  1.36it/s]Extractor Estimating: 273it [03:24,  1.34it/s]Extractor Estimating: 274it [03:25,  1.36it/s]Extractor Estimating: 275it [03:25,  1.37it/s]Extractor Estimating: 276it [03:26,  1.40it/s]Extractor Estimating: 277it [03:27,  1.39it/s]Extractor Estimating: 278it [03:28,  1.41it/s]Extractor Estimating: 279it [03:28,  1.48it/s]Extractor Estimating: 280it [03:29,  1.46it/s]Extractor Estimating: 281it [03:30,  1.43it/s]Extractor Estimating: 282it [03:30,  1.37it/s]Extractor Estimating: 283it [03:31,  1.43it/s]Extractor Estimating: 284it [03:32,  1.39it/s]Extractor Estimating: 285it [03:32,  1.40it/s]Extractor Estimating: 286it [03:33,  1.35it/s]Extractor Estimating: 287it [03:34,  1.36it/s]Extractor Estimating: 288it [03:35,  1.39it/s]Extractor Estimating: 289it [03:35,  1.37it/s]Extractor Estimating: 290it [03:36,  1.32it/s]Extractor Estimating: 291it [03:37,  1.31it/s]Extractor Estimating: 292it [03:38,  1.28it/s]Extractor Estimating: 293it [03:39,  1.30it/s]Extractor Estimating: 294it [03:39,  1.30it/s]Extractor Estimating: 295it [03:40,  1.33it/s]Extractor Estimating: 296it [03:41,  1.27it/s]Extractor Estimating: 297it [03:42,  1.33it/s]Extractor Estimating: 298it [03:42,  1.35it/s]Extractor Estimating: 299it [03:43,  1.38it/s]Extractor Estimating: 300it [03:44,  1.31it/s]Extractor Estimating: 301it [03:45,  1.35it/s]Extractor Estimating: 302it [03:45,  1.30it/s]Extractor Estimating: 303it [03:46,  1.27it/s]Extractor Estimating: 304it [03:47,  1.28it/s]Extractor Estimating: 305it [03:48,  1.34it/s]Extractor Estimating: 306it [03:48,  1.37it/s]Extractor Estimating: 307it [03:49,  1.44it/s]Extractor Estimating: 308it [03:50,  1.40it/s]Extractor Estimating: 309it [03:50,  1.40it/s]Extractor Estimating: 310it [03:51,  1.34it/s]Extractor Estimating: 311it [03:52,  1.33it/s]Extractor Estimating: 312it [03:53,  1.30it/s]Extractor Estimating: 313it [03:54,  1.30it/s]Extractor Estimating: 314it [03:54,  1.30it/s]Extractor Estimating: 315it [03:55,  1.28it/s]Extractor Estimating: 316it [03:56,  1.32it/s]Extractor Estimating: 317it [03:57,  1.25it/s]Extractor Estimating: 318it [03:57,  1.28it/s]Extractor Estimating: 319it [03:58,  1.29it/s]Extractor Estimating: 320it [03:59,  1.29it/s]Extractor Estimating: 321it [04:00,  1.32it/s]Extractor Estimating: 322it [04:00,  1.37it/s]Extractor Estimating: 323it [04:01,  1.33it/s]Extractor Estimating: 324it [04:02,  1.34it/s]Extractor Estimating: 325it [04:03,  1.30it/s]Extractor Estimating: 326it [04:04,  1.28it/s]Extractor Estimating: 327it [04:04,  1.30it/s]Extractor Estimating: 328it [04:05,  1.32it/s]Extractor Estimating: 329it [04:06,  1.36it/s]Extractor Estimating: 330it [04:07,  1.31it/s]Extractor Estimating: 331it [04:07,  1.27it/s]Extractor Estimating: 332it [04:08,  1.33it/s]Extractor Estimating: 333it [04:09,  1.34it/s]Extractor Estimating: 334it [04:10,  1.35it/s]Extractor Estimating: 335it [04:10,  1.28it/s]Extractor Estimating: 336it [04:11,  1.30it/s]Extractor Estimating: 337it [04:12,  1.32it/s]Extractor Estimating: 338it [04:13,  1.36it/s]Extractor Estimating: 339it [04:13,  1.39it/s]Extractor Estimating: 340it [04:14,  1.37it/s]Extractor Estimating: 341it [04:15,  1.38it/s]Extractor Estimating: 342it [04:16,  1.33it/s]Extractor Estimating: 343it [04:16,  1.26it/s]Extractor Estimating: 344it [04:17,  1.34it/s]Extractor Estimating: 345it [04:18,  1.28it/s]Extractor Estimating: 346it [04:19,  1.34it/s]Extractor Estimating: 347it [04:19,  1.38it/s]Extractor Estimating: 348it [04:20,  1.36it/s]Extractor Estimating: 349it [04:21,  1.36it/s]Extractor Estimating: 350it [04:22,  1.31it/s]Extractor Estimating: 351it [04:22,  1.32it/s]Extractor Estimating: 352it [04:23,  1.33it/s]Extractor Estimating: 353it [04:24,  1.33it/s]Extractor Estimating: 354it [04:25,  1.33it/s]Extractor Estimating: 355it [04:25,  1.34it/s]Extractor Estimating: 356it [04:26,  1.35it/s]Extractor Estimating: 357it [04:27,  1.33it/s]Extractor Estimating: 358it [04:28,  1.31it/s]Extractor Estimating: 359it [04:28,  1.34it/s]Extractor Estimating: 360it [04:29,  1.31it/s]Extractor Estimating: 361it [04:30,  1.30it/s]Extractor Estimating: 362it [04:31,  1.26it/s]Extractor Estimating: 363it [04:32,  1.26it/s]Extractor Estimating: 364it [04:32,  1.30it/s]Extractor Estimating: 365it [04:33,  1.29it/s]Extractor Estimating: 366it [04:34,  1.32it/s]Extractor Estimating: 367it [04:34,  1.37it/s]Extractor Estimating: 368it [04:35,  1.34it/s]Extractor Estimating: 369it [04:36,  1.33it/s]Extractor Estimating: 370it [04:37,  1.31it/s]Extractor Estimating: 371it [04:37,  1.36it/s]Extractor Estimating: 372it [04:38,  1.32it/s]Extractor Estimating: 373it [04:39,  1.34it/s]Extractor Estimating: 374it [04:40,  1.35it/s]Extractor Estimating: 375it [04:40,  1.33it/s]Extractor Estimating: 375it [04:40,  1.33it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:22:33,899 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:22:33,903 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:22:33,903 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:22:33,903 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:22:33,903 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:22:34,513 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:22:34,514 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:22:35,104 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:22:36,178 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:22:36,178 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:22:39,229 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:22:39,236 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:22:39,236 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:22:39,236 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:22:39,236 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:22:40,034 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:22:40,035 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:22:40,609 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:22:40,789 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:22:40,789 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 17:54:12,970 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 17:54:13,039 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7482 mean pseudo reward: 0.9174177817608674
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
train vocab size: 25044
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25144, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25144, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.173, loss:758.4899
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.200, loss:728.1439
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.172, loss:724.2582
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 1.172, loss:689.9850
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 1.180, loss:713.3224
>> valid entity prec:0.5526, rec:0.6080, f1:0.5790
>> valid relation prec:0.2434, rec:0.0933, f1:0.1349
>> valid relation with NER prec:0.2434, rec:0.0933, f1:0.1349
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 2.645, loss:715.0377
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.171, loss:651.2914
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.181, loss:682.9117
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 1.180, loss:746.5513
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 1.168, loss:700.7227
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5634, rec:0.5794, f1:0.5713
>> valid relation prec:0.1839, rec:0.0654, f1:0.0965
>> valid relation with NER prec:0.1839, rec:0.0654, f1:0.0965
g_step 1100, step 164, avg_time 2.661, loss:694.6392
g_step 1200, step 264, avg_time 1.159, loss:759.3674
g_step 1300, step 52, avg_time 1.165, loss:670.3636
g_step 1400, step 152, avg_time 1.161, loss:670.5402
g_step 1500, step 252, avg_time 1.181, loss:694.8193
>> valid entity prec:0.5631, rec:0.5130, f1:0.5369
>> valid relation prec:0.2594, rec:0.0755, f1:0.1169
>> valid relation with NER prec:0.2594, rec:0.0755, f1:0.1169
g_step 1600, step 40, avg_time 2.661, loss:680.1678
g_step 1700, step 140, avg_time 1.169, loss:634.4280
g_step 1800, step 240, avg_time 1.175, loss:672.8338
g_step 1900, step 28, avg_time 1.168, loss:638.7452
g_step 2000, step 128, avg_time 1.184, loss:615.2443
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5648, rec:0.5114, f1:0.5368
>> valid relation prec:0.2220, rec:0.0723, f1:0.1091
>> valid relation with NER prec:0.2220, rec:0.0723, f1:0.1091
g_step 2100, step 228, avg_time 2.627, loss:624.1261
g_step 2200, step 16, avg_time 1.179, loss:627.3411
g_step 2300, step 116, avg_time 1.175, loss:598.9668
g_step 2400, step 216, avg_time 1.164, loss:576.8384
g_step 2500, step 4, avg_time 1.169, loss:583.5159
>> valid entity prec:0.5436, rec:0.5773, f1:0.5599
>> valid relation prec:0.2156, rec:0.0921, f1:0.1291
>> valid relation with NER prec:0.2156, rec:0.0921, f1:0.1291
g_step 2600, step 104, avg_time 2.639, loss:557.4744
g_step 2700, step 204, avg_time 1.169, loss:573.2463
g_step 2800, step 304, avg_time 1.162, loss:603.6908
g_step 2900, step 92, avg_time 1.167, loss:519.0341
g_step 3000, step 192, avg_time 1.190, loss:564.3898
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5828, rec:0.5091, f1:0.5435
>> valid relation prec:0.2323, rec:0.0677, f1:0.1049
>> valid relation with NER prec:0.2323, rec:0.0677, f1:0.1049
g_step 3100, step 292, avg_time 2.610, loss:563.1231
g_step 3200, step 80, avg_time 1.153, loss:519.5870
g_step 3300, step 180, avg_time 1.192, loss:529.3842
g_step 3400, step 280, avg_time 1.155, loss:519.1834
g_step 3500, step 68, avg_time 1.172, loss:475.7185
>> valid entity prec:0.5858, rec:0.4915, f1:0.5345
>> valid relation prec:0.2396, rec:0.0729, f1:0.1118
>> valid relation with NER prec:0.2396, rec:0.0729, f1:0.1118
g_step 3600, step 168, avg_time 2.672, loss:506.3605
g_step 3700, step 268, avg_time 1.197, loss:522.5906
g_step 3800, step 56, avg_time 1.196, loss:489.7042
g_step 3900, step 156, avg_time 1.191, loss:468.3329
g_step 4000, step 256, avg_time 1.174, loss:500.4238
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5579, rec:0.4858, f1:0.5194
>> valid relation prec:0.1757, rec:0.0611, f1:0.0907
>> valid relation with NER prec:0.1757, rec:0.0611, f1:0.0907
g_step 4100, step 44, avg_time 2.607, loss:491.3520
g_step 4200, step 144, avg_time 1.175, loss:451.0178
g_step 4300, step 244, avg_time 1.185, loss:473.6330
g_step 4400, step 32, avg_time 1.156, loss:463.6803
g_step 4500, step 132, avg_time 1.180, loss:437.1166
>> valid entity prec:0.5575, rec:0.5442, f1:0.5508
>> valid relation prec:0.1756, rec:0.0763, f1:0.1064
>> valid relation with NER prec:0.1756, rec:0.0763, f1:0.1064
g_step 4600, step 232, avg_time 2.636, loss:455.8510
g_step 4700, step 20, avg_time 1.189, loss:470.2162
g_step 4800, step 120, avg_time 1.172, loss:415.0085
g_step 4900, step 220, avg_time 1.167, loss:443.4185
g_step 5000, step 8, avg_time 1.177, loss:455.3465
learning rate was adjusted to 0.0008
>> valid entity prec:0.5550, rec:0.5090, f1:0.5310
>> valid relation prec:0.1883, rec:0.0821, f1:0.1143
>> valid relation with NER prec:0.1883, rec:0.0821, f1:0.1143
g_step 5100, step 108, avg_time 2.643, loss:407.8438
g_step 5200, step 208, avg_time 1.184, loss:403.1432
g_step 5300, step 308, avg_time 1.176, loss:433.7850
g_step 5400, step 96, avg_time 1.168, loss:378.1318
g_step 5500, step 196, avg_time 1.177, loss:396.7687
>> valid entity prec:0.5379, rec:0.5139, f1:0.5256
>> valid relation prec:0.1300, rec:0.0534, f1:0.0757
>> valid relation with NER prec:0.1300, rec:0.0534, f1:0.0757
g_step 5600, step 296, avg_time 2.639, loss:430.9244
g_step 5700, step 84, avg_time 1.167, loss:389.4931
g_step 5800, step 184, avg_time 1.171, loss:381.3505
g_step 5900, step 284, avg_time 1.174, loss:412.7934
g_step 6000, step 72, avg_time 1.172, loss:369.5859
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5548, rec:0.5094, f1:0.5311
>> valid relation prec:0.1661, rec:0.0775, f1:0.1057
>> valid relation with NER prec:0.1661, rec:0.0775, f1:0.1057
g_step 6100, step 172, avg_time 2.642, loss:381.6511
g_step 6200, step 272, avg_time 1.167, loss:379.8589
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 17:54:13 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 17:54:13 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_17-54-12_ctolab11.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 17:54:14 - WARNING - datasets.builder -   Using custom data configuration default-ed5bd92686c58931
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-ed5bd92686c58931/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 17:54:15,465 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:54:15,466 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:54:15,467 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:54:15,468 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:54:15,479 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:54:15,486 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:54:15,486 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:54:15,486 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:54:15,487 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:54:15,487 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:54:15,487 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 17:54:15,712 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:54:18,834 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 17:54:18,843 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-ed5bd92686c58931/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.92ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.84ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.17ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.35ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.45ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.33ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.43ba/s]100%|██████████| 8/8 [00:01<00:00,  5.29ba/s]100%|██████████| 8/8 [00:01<00:00,  4.55ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.04ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.27ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.74ba/s]100%|██████████| 4/4 [00:00<00:00,  4.86ba/s]100%|██████████| 4/4 [00:00<00:00,  4.31ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  7.84ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.92ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.60ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.93ba/s]100%|██████████| 8/8 [00:00<00:00, 11.27ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.50ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.42ba/s]100%|██████████| 4/4 [00:00<00:00,  8.88ba/s]
[INFO|trainer.py:414] 2023-08-28 17:54:23,626 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 17:54:23,695 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 17:54:23,695 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-28 17:54:23,695 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 17:54:23,695 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 17:54:23,695 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 17:54:23,695 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 17:54:23,695 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:58,  3.28it/s]  0%|          | 2/585 [00:00<02:53,  3.37it/s]  1%|          | 3/585 [00:00<02:51,  3.40it/s]  1%|          | 4/585 [00:01<02:50,  3.41it/s]  1%|          | 5/585 [00:01<02:49,  3.41it/s]  1%|          | 6/585 [00:01<03:23,  2.84it/s]  1%|          | 7/585 [00:02<03:13,  2.99it/s]  1%|▏         | 8/585 [00:02<03:06,  3.09it/s]  2%|▏         | 9/585 [00:02<03:01,  3.16it/s]  2%|▏         | 10/585 [00:03<02:58,  3.22it/s]  2%|▏         | 11/585 [00:03<02:56,  3.25it/s]  2%|▏         | 12/585 [00:03<02:54,  3.28it/s]  2%|▏         | 13/585 [00:04<02:53,  3.30it/s]  2%|▏         | 14/585 [00:04<02:52,  3.30it/s]  3%|▎         | 15/585 [00:04<02:51,  3.32it/s]  3%|▎         | 16/585 [00:05<03:10,  2.98it/s]  3%|▎         | 17/585 [00:05<03:04,  3.08it/s]  3%|▎         | 18/585 [00:05<03:00,  3.15it/s]  3%|▎         | 19/585 [00:05<02:56,  3.20it/s]  3%|▎         | 20/585 [00:06<02:54,  3.24it/s]  4%|▎         | 21/585 [00:06<02:52,  3.27it/s]  4%|▍         | 22/585 [00:06<02:51,  3.29it/s]  4%|▍         | 23/585 [00:07<02:50,  3.30it/s]  4%|▍         | 24/585 [00:07<02:49,  3.31it/s]  4%|▍         | 25/585 [00:07<02:49,  3.31it/s]  4%|▍         | 26/585 [00:08<02:48,  3.32it/s]  5%|▍         | 27/585 [00:08<02:47,  3.32it/s]  5%|▍         | 28/585 [00:08<02:47,  3.33it/s]  5%|▍         | 29/585 [00:08<02:47,  3.33it/s]  5%|▌         | 30/585 [00:09<02:46,  3.33it/s]  5%|▌         | 31/585 [00:09<02:46,  3.33it/s]  5%|▌         | 32/585 [00:09<02:45,  3.33it/s]  6%|▌         | 33/585 [00:10<02:46,  3.32it/s]  6%|▌         | 34/585 [00:10<02:45,  3.33it/s]  6%|▌         | 35/585 [00:10<02:45,  3.33it/s]  6%|▌         | 36/585 [00:11<02:44,  3.33it/s]  6%|▋         | 37/585 [00:11<02:44,  3.33it/s]  6%|▋         | 38/585 [00:11<02:44,  3.34it/s]  7%|▋         | 39/585 [00:11<02:43,  3.33it/s]  7%|▋         | 40/585 [00:12<02:43,  3.34it/s]  7%|▋         | 41/585 [00:12<02:42,  3.34it/s]  7%|▋         | 42/585 [00:12<02:42,  3.34it/s]  7%|▋         | 43/585 [00:13<02:42,  3.34it/s]  8%|▊         | 44/585 [00:13<02:42,  3.34it/s]  8%|▊         | 45/585 [00:13<02:42,  3.33it/s]  8%|▊         | 46/585 [00:14<02:41,  3.34it/s]  8%|▊         | 47/585 [00:14<02:41,  3.33it/s]  8%|▊         | 48/585 [00:14<02:40,  3.34it/s]  8%|▊         | 49/585 [00:14<02:40,  3.34it/s]  9%|▊         | 50/585 [00:15<03:24,  2.61it/s]  9%|▊         | 51/585 [00:15<03:11,  2.79it/s]  9%|▉         | 52/585 [00:16<03:01,  2.94it/s]  9%|▉         | 53/585 [00:16<02:54,  3.04it/s]  9%|▉         | 54/585 [00:16<02:49,  3.13it/s]  9%|▉         | 55/585 [00:17<02:46,  3.19it/s] 10%|▉         | 56/585 [00:17<02:43,  3.23it/s] 10%|▉         | 57/585 [00:17<02:42,  3.26it/s] 10%|▉         | 58/585 [00:17<02:40,  3.28it/s] 10%|█         | 59/585 [00:18<02:39,  3.30it/s] 10%|█         | 60/585 [00:18<02:38,  3.30it/s] 10%|█         | 61/585 [00:18<02:37,  3.32it/s] 11%|█         | 62/585 [00:19<02:37,  3.32it/s] 11%|█         | 63/585 [00:19<02:37,  3.32it/s] 11%|█         | 64/585 [00:19<02:36,  3.33it/s] 11%|█         | 65/585 [00:20<02:36,  3.33it/s] 11%|█▏        | 66/585 [00:20<02:36,  3.32it/s] 11%|█▏        | 67/585 [00:20<02:35,  3.33it/s] 12%|█▏        | 68/585 [00:20<02:35,  3.33it/s] 12%|█▏        | 69/585 [00:21<02:34,  3.33it/s] 12%|█▏        | 70/585 [00:21<02:34,  3.34it/s] 12%|█▏        | 71/585 [00:21<02:34,  3.34it/s] 12%|█▏        | 72/585 [00:22<02:33,  3.34it/s] 12%|█▏        | 73/585 [00:22<02:33,  3.34it/s] 13%|█▎        | 74/585 [00:22<02:32,  3.34it/s] 13%|█▎        | 75/585 [00:23<02:32,  3.34it/s] 13%|█▎        | 76/585 [00:23<02:32,  3.34it/s] 13%|█▎        | 77/585 [00:23<02:32,  3.33it/s] 13%|█▎        | 78/585 [00:23<02:32,  3.33it/s] 14%|█▎        | 79/585 [00:24<02:31,  3.33it/s] 14%|█▎        | 80/585 [00:24<02:31,  3.34it/s] 14%|█▍        | 81/585 [00:24<02:31,  3.33it/s] 14%|█▍        | 82/585 [00:25<02:30,  3.33it/s] 14%|█▍        | 83/585 [00:25<02:33,  3.27it/s] 14%|█▍        | 84/585 [00:25<02:32,  3.29it/s] 15%|█▍        | 85/585 [00:26<02:31,  3.30it/s] 15%|█▍        | 86/585 [00:26<02:30,  3.31it/s] 15%|█▍        | 87/585 [00:26<02:30,  3.32it/s] 15%|█▌        | 88/585 [00:26<02:29,  3.32it/s] 15%|█▌        | 89/585 [00:27<02:29,  3.33it/s] 15%|█▌        | 90/585 [00:27<02:28,  3.33it/s] 16%|█▌        | 91/585 [00:27<02:28,  3.33it/s] 16%|█▌        | 92/585 [00:28<02:27,  3.34it/s] 16%|█▌        | 93/585 [00:28<02:27,  3.33it/s] 16%|█▌        | 94/585 [00:28<02:27,  3.33it/s] 16%|█▌        | 95/585 [00:29<02:27,  3.33it/s] 16%|█▋        | 96/585 [00:29<02:26,  3.33it/s] 17%|█▋        | 97/585 [00:29<02:26,  3.33it/s] 17%|█▋        | 98/585 [00:29<02:25,  3.34it/s] 17%|█▋        | 99/585 [00:30<02:25,  3.33it/s] 17%|█▋        | 100/585 [00:30<02:28,  3.26it/s] 17%|█▋        | 101/585 [00:30<02:27,  3.28it/s] 17%|█▋        | 102/585 [00:31<02:26,  3.30it/s] 18%|█▊        | 103/585 [00:31<02:25,  3.31it/s] 18%|█▊        | 104/585 [00:31<02:25,  3.32it/s] 18%|█▊        | 105/585 [00:32<02:24,  3.32it/s] 18%|█▊        | 106/585 [00:32<02:23,  3.33it/s] 18%|█▊        | 107/585 [00:32<02:23,  3.33it/s] 18%|█▊        | 108/585 [00:32<02:23,  3.33it/s] 19%|█▊        | 109/585 [00:33<02:22,  3.33it/s] 19%|█▉        | 110/585 [00:33<02:22,  3.33it/s] 19%|█▉        | 111/585 [00:33<02:22,  3.33it/s] 19%|█▉        | 112/585 [00:34<02:21,  3.33it/s] 19%|█▉        | 113/585 [00:34<02:21,  3.34it/s] 19%|█▉        | 114/585 [00:34<02:21,  3.33it/s] 20%|█▉        | 115/585 [00:35<02:20,  3.34it/s] 20%|█▉        | 116/585 [00:35<02:20,  3.33it/s] 20%|██        | 117/585 [00:35<02:21,  3.32it/s][INFO|trainer.py:2140] 2023-08-28 17:54:59,416 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:54:59,416 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 17:54:59,416 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.26it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.20it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.74it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.95it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.04it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.70it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.45it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.22it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.28it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.37it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.48it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.61it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.56it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.30it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.29it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.98it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.02it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.01it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.23it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.48it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.30it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.39it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.35it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.11it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 44.05it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.01it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.11it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.22it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.38it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.54it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.51it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.36it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.16it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.09it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.00it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.15it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.21it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.44it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.47it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.53it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.34it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.19it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.03it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.01it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.03it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.18it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.39it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.38it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.40it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.37it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.30it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.18it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.13it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.02it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.21it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.38it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.41it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.35it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.15it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 40.40it/s][A
 70%|███████   | 307/436 [00:06<00:03, 41.67it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 42.44it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 43.04it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 43.43it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 43.83it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.13it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.07it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 43.78it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 43.79it/s][A
 81%|████████  | 352/436 [00:07<00:01, 43.86it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.12it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.28it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.42it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.55it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.52it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.21it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 43.91it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.02it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.08it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.27it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.28it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.44it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.50it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.44it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.21it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.02it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:45<02:21,  3.32it/s]
100%|██████████| 436/436 [00:09<00:00, 44.02it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:55:09,931 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 17:55:10,119 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:55:15,888 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:55:16,647 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:55:16,699 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:05<1:11:34,  9.19s/it] 20%|██        | 119/585 [01:05<50:43,  6.53s/it]   21%|██        | 120/585 [01:06<36:07,  4.66s/it] 21%|██        | 121/585 [01:06<25:55,  3.35s/it] 21%|██        | 122/585 [01:06<18:48,  2.44s/it] 21%|██        | 123/585 [01:07<13:49,  1.80s/it] 21%|██        | 124/585 [01:07<10:20,  1.35s/it] 21%|██▏       | 125/585 [01:07<07:54,  1.03s/it] 22%|██▏       | 126/585 [01:08<06:12,  1.23it/s] 22%|██▏       | 127/585 [01:08<05:01,  1.52it/s] 22%|██▏       | 128/585 [01:08<04:11,  1.82it/s] 22%|██▏       | 129/585 [01:08<03:37,  2.10it/s] 22%|██▏       | 130/585 [01:09<03:12,  2.36it/s] 22%|██▏       | 131/585 [01:09<02:55,  2.59it/s] 23%|██▎       | 132/585 [01:09<02:43,  2.78it/s] 23%|██▎       | 133/585 [01:10<02:34,  2.92it/s] 23%|██▎       | 134/585 [01:10<02:28,  3.03it/s] 23%|██▎       | 135/585 [01:10<02:24,  3.12it/s] 23%|██▎       | 136/585 [01:11<02:21,  3.18it/s] 23%|██▎       | 137/585 [01:11<02:18,  3.23it/s] 24%|██▎       | 138/585 [01:11<02:17,  3.26it/s] 24%|██▍       | 139/585 [01:11<02:18,  3.23it/s] 24%|██▍       | 140/585 [01:12<02:16,  3.26it/s] 24%|██▍       | 141/585 [01:12<02:15,  3.29it/s] 24%|██▍       | 142/585 [01:12<02:14,  3.30it/s] 24%|██▍       | 143/585 [01:13<02:13,  3.32it/s] 25%|██▍       | 144/585 [01:13<02:13,  3.31it/s] 25%|██▍       | 145/585 [01:13<02:12,  3.32it/s] 25%|██▍       | 146/585 [01:14<02:11,  3.33it/s] 25%|██▌       | 147/585 [01:14<02:11,  3.33it/s] 25%|██▌       | 148/585 [01:14<02:10,  3.34it/s] 25%|██▌       | 149/585 [01:14<02:10,  3.34it/s] 26%|██▌       | 150/585 [01:15<02:10,  3.34it/s] 26%|██▌       | 151/585 [01:15<02:09,  3.34it/s] 26%|██▌       | 152/585 [01:15<02:09,  3.34it/s] 26%|██▌       | 153/585 [01:16<02:09,  3.34it/s] 26%|██▋       | 154/585 [01:16<02:08,  3.35it/s] 26%|██▋       | 155/585 [01:16<02:08,  3.33it/s] 27%|██▋       | 156/585 [01:17<02:08,  3.34it/s] 27%|██▋       | 157/585 [01:17<02:08,  3.34it/s] 27%|██▋       | 158/585 [01:17<02:07,  3.34it/s] 27%|██▋       | 159/585 [01:17<02:07,  3.34it/s] 27%|██▋       | 160/585 [01:18<02:07,  3.34it/s] 28%|██▊       | 161/585 [01:18<02:06,  3.34it/s] 28%|██▊       | 162/585 [01:18<02:06,  3.34it/s] 28%|██▊       | 163/585 [01:19<02:06,  3.34it/s] 28%|██▊       | 164/585 [01:19<02:06,  3.34it/s] 28%|██▊       | 165/585 [01:19<02:05,  3.34it/s] 28%|██▊       | 166/585 [01:20<02:05,  3.33it/s] 29%|██▊       | 167/585 [01:20<02:05,  3.33it/s] 29%|██▊       | 168/585 [01:20<02:05,  3.34it/s] 29%|██▉       | 169/585 [01:20<02:04,  3.34it/s] 29%|██▉       | 170/585 [01:21<02:04,  3.34it/s] 29%|██▉       | 171/585 [01:21<02:04,  3.33it/s] 29%|██▉       | 172/585 [01:21<02:03,  3.34it/s] 30%|██▉       | 173/585 [01:22<02:03,  3.34it/s] 30%|██▉       | 174/585 [01:22<02:03,  3.34it/s] 30%|██▉       | 175/585 [01:22<02:02,  3.34it/s] 30%|███       | 176/585 [01:23<02:02,  3.34it/s] 30%|███       | 177/585 [01:23<02:03,  3.29it/s] 30%|███       | 178/585 [01:23<02:02,  3.31it/s] 31%|███       | 179/585 [01:23<02:02,  3.32it/s] 31%|███       | 180/585 [01:24<02:01,  3.34it/s] 31%|███       | 181/585 [01:24<02:00,  3.36it/s] 31%|███       | 182/585 [01:24<01:59,  3.38it/s] 31%|███▏      | 183/585 [01:25<01:58,  3.39it/s] 31%|███▏      | 184/585 [01:25<01:57,  3.40it/s] 32%|███▏      | 185/585 [01:25<01:57,  3.41it/s] 32%|███▏      | 186/585 [01:25<01:56,  3.41it/s] 32%|███▏      | 187/585 [01:26<01:56,  3.41it/s] 32%|███▏      | 188/585 [01:26<02:00,  3.30it/s] 32%|███▏      | 189/585 [01:26<01:58,  3.33it/s] 32%|███▏      | 190/585 [01:27<01:57,  3.36it/s] 33%|███▎      | 191/585 [01:27<01:56,  3.38it/s] 33%|███▎      | 192/585 [01:27<01:55,  3.39it/s] 33%|███▎      | 193/585 [01:28<01:55,  3.40it/s] 33%|███▎      | 194/585 [01:28<01:54,  3.40it/s] 33%|███▎      | 195/585 [01:28<01:54,  3.41it/s] 34%|███▎      | 196/585 [01:28<01:53,  3.41it/s] 34%|███▎      | 197/585 [01:29<01:53,  3.41it/s] 34%|███▍      | 198/585 [01:29<01:53,  3.42it/s] 34%|███▍      | 199/585 [01:29<01:54,  3.37it/s] 34%|███▍      | 200/585 [01:30<01:53,  3.39it/s] 34%|███▍      | 201/585 [01:30<01:53,  3.39it/s] 35%|███▍      | 202/585 [01:30<01:52,  3.40it/s] 35%|███▍      | 203/585 [01:31<01:52,  3.41it/s] 35%|███▍      | 204/585 [01:31<01:51,  3.41it/s] 35%|███▌      | 205/585 [01:31<01:51,  3.41it/s] 35%|███▌      | 206/585 [01:31<01:50,  3.42it/s] 35%|███▌      | 207/585 [01:32<01:50,  3.42it/s] 36%|███▌      | 208/585 [01:32<01:50,  3.42it/s] 36%|███▌      | 209/585 [01:32<01:50,  3.41it/s] 36%|███▌      | 210/585 [01:33<01:53,  3.30it/s] 36%|███▌      | 211/585 [01:33<01:52,  3.33it/s] 36%|███▌      | 212/585 [01:33<01:51,  3.36it/s] 36%|███▋      | 213/585 [01:33<01:50,  3.38it/s] 37%|███▋      | 214/585 [01:34<01:49,  3.39it/s] 37%|███▋      | 215/585 [01:34<01:49,  3.39it/s] 37%|███▋      | 216/585 [01:34<01:48,  3.40it/s] 37%|███▋      | 217/585 [01:35<01:48,  3.41it/s] 37%|███▋      | 218/585 [01:35<01:47,  3.41it/s] 37%|███▋      | 219/585 [01:35<01:47,  3.41it/s] 38%|███▊      | 220/585 [01:36<01:47,  3.41it/s] 38%|███▊      | 221/585 [01:36<01:46,  3.41it/s] 38%|███▊      | 222/585 [01:36<01:46,  3.41it/s] 38%|███▊      | 223/585 [01:36<01:46,  3.41it/s] 38%|███▊      | 224/585 [01:37<01:46,  3.39it/s] 38%|███▊      | 225/585 [01:37<01:46,  3.39it/s] 39%|███▊      | 226/585 [01:37<01:45,  3.40it/s] 39%|███▉      | 227/585 [01:38<01:45,  3.40it/s] 39%|███▉      | 228/585 [01:38<01:44,  3.40it/s] 39%|███▉      | 229/585 [01:38<01:44,  3.41it/s] 39%|███▉      | 230/585 [01:38<01:44,  3.41it/s] 39%|███▉      | 231/585 [01:39<01:56,  3.04it/s] 40%|███▉      | 232/585 [01:39<01:52,  3.14it/s] 40%|███▉      | 233/585 [01:39<01:49,  3.22it/s] 40%|████      | 234/585 [01:40<01:47,  3.27it/s][INFO|trainer.py:2140] 2023-08-28 17:56:03,985 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:56:03,985 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 17:56:03,986 >>   Batch size = 8
{'eval_loss': 0.9548506140708923, 'eval_runtime': 9.9615, 'eval_samples_per_second': 350.15, 'eval_steps_per_second': 43.769, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.47it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.39it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.78it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.66it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.07it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.64it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.46it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.28it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.30it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.44it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.57it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.54it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.45it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.23it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.15it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.85it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 43.98it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.17it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.31it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.50it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.50it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.46it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.36it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.09it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 44.01it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.06it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.09it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.40it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.53it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.55it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.55it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.34it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.88it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.87it/s][A
 41%|████      | 177/436 [00:03<00:05, 43.95it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.10it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.33it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.53it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.50it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.46it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.29it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.21it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.07it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.01it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.08it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.21it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.49it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.50it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.37it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.22it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.14it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.12it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.04it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.23it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.33it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.49it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.40it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.26it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.07it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.07it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.01it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.05it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.16it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.28it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.55it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.53it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.33it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.08it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.08it/s][A
 81%|████████  | 352/436 [00:07<00:01, 43.88it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.23it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.24it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.33it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.40it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.44it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.17it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.12it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.13it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.18it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.25it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.20it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.20it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.39it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.34it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.36it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.32it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:50<01:47,  3.27it/s]
100%|██████████| 436/436 [00:09<00:00, 44.32it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:56:13,866 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 17:56:13,894 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:56:18,491 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:56:18,521 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:56:18,536 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:04<43:31,  7.46s/it] 40%|████      | 236/585 [02:04<30:55,  5.32s/it] 41%|████      | 237/585 [02:05<22:06,  3.81s/it] 41%|████      | 238/585 [02:05<15:57,  2.76s/it] 41%|████      | 239/585 [02:05<11:39,  2.02s/it] 41%|████      | 240/585 [02:05<08:38,  1.50s/it] 41%|████      | 241/585 [02:06<06:32,  1.14s/it] 41%|████▏     | 242/585 [02:06<05:05,  1.12it/s] 42%|████▏     | 243/585 [02:06<04:03,  1.40it/s] 42%|████▏     | 244/585 [02:07<03:20,  1.70it/s] 42%|████▏     | 245/585 [02:07<02:50,  1.99it/s] 42%|████▏     | 246/585 [02:07<02:31,  2.24it/s] 42%|████▏     | 247/585 [02:08<02:15,  2.49it/s] 42%|████▏     | 248/585 [02:08<02:05,  2.69it/s] 43%|████▎     | 249/585 [02:08<01:57,  2.86it/s] 43%|████▎     | 250/585 [02:08<01:52,  2.99it/s] 43%|████▎     | 251/585 [02:09<01:48,  3.09it/s] 43%|████▎     | 252/585 [02:09<01:45,  3.16it/s] 43%|████▎     | 253/585 [02:09<01:43,  3.21it/s] 43%|████▎     | 254/585 [02:10<01:41,  3.25it/s] 44%|████▎     | 255/585 [02:10<01:40,  3.27it/s] 44%|████▍     | 256/585 [02:10<01:40,  3.27it/s] 44%|████▍     | 257/585 [02:11<01:39,  3.29it/s] 44%|████▍     | 258/585 [02:11<01:38,  3.30it/s] 44%|████▍     | 259/585 [02:11<01:38,  3.31it/s] 44%|████▍     | 260/585 [02:11<01:37,  3.32it/s] 45%|████▍     | 261/585 [02:12<01:37,  3.33it/s] 45%|████▍     | 262/585 [02:12<01:37,  3.33it/s] 45%|████▍     | 263/585 [02:12<01:36,  3.33it/s] 45%|████▌     | 264/585 [02:13<01:36,  3.33it/s] 45%|████▌     | 265/585 [02:13<01:36,  3.33it/s] 45%|████▌     | 266/585 [02:13<01:36,  3.32it/s] 46%|████▌     | 267/585 [02:14<01:35,  3.32it/s] 46%|████▌     | 268/585 [02:14<01:35,  3.33it/s] 46%|████▌     | 269/585 [02:14<01:35,  3.33it/s] 46%|████▌     | 270/585 [02:14<01:34,  3.33it/s] 46%|████▋     | 271/585 [02:15<01:34,  3.33it/s] 46%|████▋     | 272/585 [02:15<01:34,  3.32it/s] 47%|████▋     | 273/585 [02:15<01:33,  3.33it/s] 47%|████▋     | 274/585 [02:16<01:33,  3.34it/s] 47%|████▋     | 275/585 [02:16<01:32,  3.34it/s] 47%|████▋     | 276/585 [02:16<01:33,  3.32it/s] 47%|████▋     | 277/585 [02:17<01:32,  3.33it/s] 48%|████▊     | 278/585 [02:17<01:32,  3.33it/s] 48%|████▊     | 279/585 [02:17<01:31,  3.34it/s] 48%|████▊     | 280/585 [02:17<01:31,  3.34it/s] 48%|████▊     | 281/585 [02:18<01:31,  3.34it/s] 48%|████▊     | 282/585 [02:18<01:30,  3.34it/s] 48%|████▊     | 283/585 [02:18<01:30,  3.34it/s] 49%|████▊     | 284/585 [02:19<01:30,  3.33it/s] 49%|████▊     | 285/585 [02:19<01:29,  3.34it/s] 49%|████▉     | 286/585 [02:19<01:29,  3.33it/s] 49%|████▉     | 287/585 [02:20<01:29,  3.33it/s] 49%|████▉     | 288/585 [02:20<01:29,  3.34it/s] 49%|████▉     | 289/585 [02:20<01:28,  3.34it/s] 50%|████▉     | 290/585 [02:20<01:28,  3.34it/s] 50%|████▉     | 291/585 [02:21<01:28,  3.34it/s] 50%|████▉     | 292/585 [02:21<01:27,  3.34it/s] 50%|█████     | 293/585 [02:21<01:27,  3.34it/s] 50%|█████     | 294/585 [02:22<01:27,  3.34it/s] 50%|█████     | 295/585 [02:22<01:26,  3.34it/s] 51%|█████     | 296/585 [02:22<01:26,  3.33it/s] 51%|█████     | 297/585 [02:23<01:26,  3.33it/s] 51%|█████     | 298/585 [02:23<01:26,  3.33it/s] 51%|█████     | 299/585 [02:23<01:25,  3.33it/s] 51%|█████▏    | 300/585 [02:23<01:25,  3.33it/s] 51%|█████▏    | 301/585 [02:24<01:25,  3.33it/s] 52%|█████▏    | 302/585 [02:24<01:24,  3.33it/s] 52%|█████▏    | 303/585 [02:24<01:24,  3.34it/s] 52%|█████▏    | 304/585 [02:25<01:24,  3.34it/s] 52%|█████▏    | 305/585 [02:25<01:23,  3.34it/s] 52%|█████▏    | 306/585 [02:25<01:23,  3.34it/s] 52%|█████▏    | 307/585 [02:26<01:23,  3.32it/s] 53%|█████▎    | 308/585 [02:26<01:23,  3.32it/s] 53%|█████▎    | 309/585 [02:26<01:22,  3.33it/s] 53%|█████▎    | 310/585 [02:26<01:22,  3.33it/s] 53%|█████▎    | 311/585 [02:27<01:22,  3.34it/s] 53%|█████▎    | 312/585 [02:27<01:21,  3.34it/s] 54%|█████▎    | 313/585 [02:27<01:21,  3.34it/s] 54%|█████▎    | 314/585 [02:28<01:20,  3.36it/s] 54%|█████▍    | 315/585 [02:28<01:19,  3.38it/s] 54%|█████▍    | 316/585 [02:28<01:19,  3.39it/s] 54%|█████▍    | 317/585 [02:28<01:18,  3.40it/s] 54%|█████▍    | 318/585 [02:29<01:18,  3.39it/s] 55%|█████▍    | 319/585 [02:29<01:18,  3.39it/s] 55%|█████▍    | 320/585 [02:29<01:17,  3.40it/s] 55%|█████▍    | 321/585 [02:30<01:17,  3.40it/s] 55%|█████▌    | 322/585 [02:30<01:17,  3.41it/s] 55%|█████▌    | 323/585 [02:30<01:16,  3.41it/s] 55%|█████▌    | 324/585 [02:31<01:16,  3.41it/s] 56%|█████▌    | 325/585 [02:31<01:16,  3.42it/s] 56%|█████▌    | 326/585 [02:31<01:15,  3.42it/s] 56%|█████▌    | 327/585 [02:31<01:15,  3.42it/s] 56%|█████▌    | 328/585 [02:32<01:15,  3.42it/s] 56%|█████▌    | 329/585 [02:32<01:14,  3.42it/s] 56%|█████▋    | 330/585 [02:32<01:14,  3.42it/s] 57%|█████▋    | 331/585 [02:33<01:14,  3.42it/s] 57%|█████▋    | 332/585 [02:33<01:14,  3.40it/s] 57%|█████▋    | 333/585 [02:33<01:14,  3.41it/s] 57%|█████▋    | 334/585 [02:33<01:13,  3.41it/s] 57%|█████▋    | 335/585 [02:34<01:13,  3.41it/s] 57%|█████▋    | 336/585 [02:34<01:13,  3.41it/s] 58%|█████▊    | 337/585 [02:34<01:12,  3.41it/s] 58%|█████▊    | 338/585 [02:35<01:12,  3.41it/s] 58%|█████▊    | 339/585 [02:35<01:21,  3.03it/s] 58%|█████▊    | 340/585 [02:35<01:18,  3.13it/s] 58%|█████▊    | 341/585 [02:36<01:15,  3.21it/s] 58%|█████▊    | 342/585 [02:36<01:14,  3.27it/s] 59%|█████▊    | 343/585 [02:36<01:13,  3.31it/s] 59%|█████▉    | 344/585 [02:37<01:12,  3.34it/s] 59%|█████▉    | 345/585 [02:37<01:11,  3.36it/s] 59%|█████▉    | 346/585 [02:37<01:10,  3.38it/s] 59%|█████▉    | 347/585 [02:37<01:10,  3.38it/s] 59%|█████▉    | 348/585 [02:38<01:09,  3.39it/s] 60%|█████▉    | 349/585 [02:38<01:11,  3.32it/s] 60%|█████▉    | 350/585 [02:38<01:10,  3.35it/s] 60%|██████    | 351/585 [02:39<01:09,  3.37it/s][INFO|trainer.py:2140] 2023-08-28 17:57:02,851 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:57:02,852 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 17:57:02,852 >>   Batch size = 8
{'eval_loss': 0.9599837064743042, 'eval_runtime': 9.8652, 'eval_samples_per_second': 353.565, 'eval_steps_per_second': 44.196, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.10it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.27it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.56it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.75it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.08it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.49it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.44it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.11it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.26it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.39it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.49it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.56it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.44it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.24it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.17it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 44.08it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.01it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.17it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.27it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.38it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.49it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.33it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.17it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 43.90it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.93it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.03it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.01it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.12it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.35it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.42it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.40it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.16it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.10it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.08it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.06it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.12it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.16it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.29it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.40it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.29it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.20it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.06it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.02it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.09it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.11it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.20it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.33it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.36it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.31it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.29it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.13it/s][A
 60%|██████    | 262/436 [00:05<00:03, 43.90it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.13it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.10it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.15it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.32it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.43it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.44it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.24it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.15it/s][A
 70%|███████   | 307/436 [00:06<00:02, 43.99it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.21it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.15it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.18it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.30it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.25it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.28it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.14it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.10it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.06it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.14it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.18it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.32it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.41it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.31it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.27it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.15it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.13it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.21it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.16it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.21it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 39.74it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 41.18it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 42.23it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 42.97it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 43.42it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [02:49<01:09,  3.37it/s]
100%|██████████| 436/436 [00:09<00:00, 43.42it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:57:12,807 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 17:57:12,883 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:57:16,859 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:57:16,900 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:57:16,921 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:01<26:59,  6.95s/it] 60%|██████    | 353/585 [03:01<19:10,  4.96s/it] 61%|██████    | 354/585 [03:02<13:42,  3.56s/it] 61%|██████    | 355/585 [03:02<09:53,  2.58s/it] 61%|██████    | 356/585 [03:02<07:14,  1.90s/it] 61%|██████    | 357/585 [03:03<05:23,  1.42s/it] 61%|██████    | 358/585 [03:03<04:05,  1.08s/it] 61%|██████▏   | 359/585 [03:03<03:11,  1.18it/s] 62%|██████▏   | 360/585 [03:03<02:33,  1.46it/s] 62%|██████▏   | 361/585 [03:04<02:07,  1.76it/s] 62%|██████▏   | 362/585 [03:04<01:48,  2.05it/s] 62%|██████▏   | 363/585 [03:04<01:36,  2.31it/s] 62%|██████▏   | 364/585 [03:05<01:26,  2.54it/s] 62%|██████▏   | 365/585 [03:05<01:20,  2.74it/s] 63%|██████▎   | 366/585 [03:05<01:15,  2.90it/s] 63%|██████▎   | 367/585 [03:06<01:12,  3.02it/s] 63%|██████▎   | 368/585 [03:06<01:09,  3.11it/s] 63%|██████▎   | 369/585 [03:06<01:08,  3.17it/s] 63%|██████▎   | 370/585 [03:06<01:06,  3.21it/s] 63%|██████▎   | 371/585 [03:07<01:05,  3.25it/s] 64%|██████▎   | 372/585 [03:07<01:04,  3.28it/s] 64%|██████▍   | 373/585 [03:07<01:05,  3.24it/s] 64%|██████▍   | 374/585 [03:08<01:04,  3.27it/s] 64%|██████▍   | 375/585 [03:08<01:03,  3.29it/s] 64%|██████▍   | 376/585 [03:08<01:03,  3.30it/s] 64%|██████▍   | 377/585 [03:09<01:02,  3.31it/s] 65%|██████▍   | 378/585 [03:09<01:02,  3.32it/s] 65%|██████▍   | 379/585 [03:09<01:01,  3.33it/s] 65%|██████▍   | 380/585 [03:10<01:01,  3.33it/s] 65%|██████▌   | 381/585 [03:10<01:01,  3.33it/s] 65%|██████▌   | 382/585 [03:10<01:00,  3.33it/s] 65%|██████▌   | 383/585 [03:11<01:06,  3.02it/s] 66%|██████▌   | 384/585 [03:11<01:04,  3.11it/s] 66%|██████▌   | 385/585 [03:11<01:02,  3.18it/s] 66%|██████▌   | 386/585 [03:11<01:01,  3.22it/s] 66%|██████▌   | 387/585 [03:12<01:00,  3.25it/s] 66%|██████▋   | 388/585 [03:12<01:00,  3.27it/s] 66%|██████▋   | 389/585 [03:12<00:59,  3.29it/s] 67%|██████▋   | 390/585 [03:13<00:59,  3.30it/s] 67%|██████▋   | 391/585 [03:13<00:58,  3.31it/s] 67%|██████▋   | 392/585 [03:13<00:58,  3.32it/s] 67%|██████▋   | 393/585 [03:14<00:57,  3.32it/s] 67%|██████▋   | 394/585 [03:14<00:57,  3.32it/s] 68%|██████▊   | 395/585 [03:14<00:57,  3.33it/s] 68%|██████▊   | 396/585 [03:14<00:56,  3.33it/s] 68%|██████▊   | 397/585 [03:15<00:56,  3.33it/s] 68%|██████▊   | 398/585 [03:15<00:56,  3.33it/s] 68%|██████▊   | 399/585 [03:15<00:55,  3.33it/s] 68%|██████▊   | 400/585 [03:16<00:55,  3.33it/s] 69%|██████▊   | 401/585 [03:16<00:55,  3.34it/s] 69%|██████▊   | 402/585 [03:16<00:54,  3.34it/s] 69%|██████▉   | 403/585 [03:17<00:54,  3.33it/s] 69%|██████▉   | 404/585 [03:17<00:54,  3.34it/s] 69%|██████▉   | 405/585 [03:17<00:53,  3.34it/s] 69%|██████▉   | 406/585 [03:17<00:53,  3.33it/s] 70%|██████▉   | 407/585 [03:18<00:53,  3.33it/s] 70%|██████▉   | 408/585 [03:18<00:52,  3.34it/s] 70%|██████▉   | 409/585 [03:18<00:52,  3.34it/s] 70%|███████   | 410/585 [03:19<00:52,  3.34it/s] 70%|███████   | 411/585 [03:19<00:53,  3.25it/s] 70%|███████   | 412/585 [03:19<00:52,  3.28it/s] 71%|███████   | 413/585 [03:20<00:52,  3.29it/s] 71%|███████   | 414/585 [03:20<00:51,  3.31it/s] 71%|███████   | 415/585 [03:20<00:51,  3.32it/s] 71%|███████   | 416/585 [03:20<00:50,  3.33it/s] 71%|███████▏  | 417/585 [03:21<00:50,  3.33it/s] 71%|███████▏  | 418/585 [03:21<00:50,  3.33it/s] 72%|███████▏  | 419/585 [03:21<00:49,  3.33it/s] 72%|███████▏  | 420/585 [03:22<00:49,  3.34it/s] 72%|███████▏  | 421/585 [03:22<00:52,  3.15it/s] 72%|███████▏  | 422/585 [03:22<00:50,  3.20it/s] 72%|███████▏  | 423/585 [03:23<00:49,  3.24it/s] 72%|███████▏  | 424/585 [03:23<00:49,  3.27it/s] 73%|███████▎  | 425/585 [03:23<00:48,  3.28it/s] 73%|███████▎  | 426/585 [03:23<00:48,  3.30it/s] 73%|███████▎  | 427/585 [03:24<00:47,  3.31it/s] 73%|███████▎  | 428/585 [03:24<00:47,  3.32it/s] 73%|███████▎  | 429/585 [03:24<00:46,  3.32it/s] 74%|███████▎  | 430/585 [03:25<00:46,  3.32it/s] 74%|███████▎  | 431/585 [03:25<00:46,  3.31it/s] 74%|███████▍  | 432/585 [03:25<00:46,  3.32it/s] 74%|███████▍  | 433/585 [03:26<00:45,  3.32it/s] 74%|███████▍  | 434/585 [03:26<00:45,  3.33it/s] 74%|███████▍  | 435/585 [03:26<00:45,  3.33it/s] 75%|███████▍  | 436/585 [03:26<00:44,  3.33it/s] 75%|███████▍  | 437/585 [03:27<00:44,  3.33it/s] 75%|███████▍  | 438/585 [03:27<00:44,  3.33it/s] 75%|███████▌  | 439/585 [03:27<00:43,  3.33it/s] 75%|███████▌  | 440/585 [03:28<00:43,  3.34it/s] 75%|███████▌  | 441/585 [03:28<00:45,  3.16it/s] 76%|███████▌  | 442/585 [03:28<00:44,  3.21it/s] 76%|███████▌  | 443/585 [03:29<00:43,  3.24it/s] 76%|███████▌  | 444/585 [03:29<00:43,  3.27it/s] 76%|███████▌  | 445/585 [03:29<00:42,  3.29it/s] 76%|███████▌  | 446/585 [03:30<00:42,  3.30it/s] 76%|███████▋  | 447/585 [03:30<00:41,  3.31it/s] 77%|███████▋  | 448/585 [03:30<00:41,  3.32it/s] 77%|███████▋  | 449/585 [03:30<00:40,  3.32it/s] 77%|███████▋  | 450/585 [03:31<00:40,  3.33it/s] 77%|███████▋  | 451/585 [03:31<00:45,  2.97it/s] 77%|███████▋  | 452/585 [03:31<00:43,  3.07it/s] 77%|███████▋  | 453/585 [03:32<00:41,  3.14it/s] 78%|███████▊  | 454/585 [03:32<00:40,  3.20it/s] 78%|███████▊  | 455/585 [03:32<00:40,  3.24it/s] 78%|███████▊  | 456/585 [03:33<00:39,  3.27it/s] 78%|███████▊  | 457/585 [03:33<00:38,  3.29it/s] 78%|███████▊  | 458/585 [03:33<00:38,  3.30it/s] 78%|███████▊  | 459/585 [03:34<00:38,  3.31it/s] 79%|███████▊  | 460/585 [03:34<00:37,  3.32it/s] 79%|███████▉  | 461/585 [03:34<00:37,  3.32it/s] 79%|███████▉  | 462/585 [03:35<00:38,  3.19it/s] 79%|███████▉  | 463/585 [03:35<00:37,  3.22it/s] 79%|███████▉  | 464/585 [03:35<00:37,  3.26it/s] 79%|███████▉  | 465/585 [03:35<00:36,  3.28it/s] 80%|███████▉  | 466/585 [03:36<00:36,  3.30it/s] 80%|███████▉  | 467/585 [03:36<00:35,  3.31it/s] 80%|████████  | 468/585 [03:36<00:40,  2.90it/s][INFO|trainer.py:2140] 2023-08-28 17:58:00,705 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:58:00,705 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 17:58:00,705 >>   Batch size = 8
{'eval_loss': 0.971801221370697, 'eval_runtime': 9.9134, 'eval_samples_per_second': 351.848, 'eval_steps_per_second': 43.981, 'epoch': 3.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.43it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.43it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.03it/s][A
  5%|▌         | 22/436 [00:00<00:08, 46.13it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.26it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.61it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.36it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.10it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.25it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.47it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.49it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.51it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.51it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.37it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.26it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.26it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.11it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.14it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.29it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.40it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.47it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.32it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.16it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.21it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 44.13it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.09it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.11it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.23it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.39it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.41it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.29it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.25it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.09it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.07it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.01it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.19it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.30it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.38it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.34it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.28it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.18it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.11it/s][A
 50%|████▉     | 217/436 [00:05<00:04, 44.10it/s][A
 51%|█████     | 222/436 [00:05<00:08, 26.27it/s][A
 52%|█████▏    | 227/436 [00:05<00:06, 30.01it/s][A
 53%|█████▎    | 232/436 [00:05<00:06, 33.20it/s][A
 54%|█████▍    | 237/436 [00:05<00:05, 35.98it/s][A
 56%|█████▌    | 242/436 [00:05<00:05, 38.15it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 39.87it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 41.20it/s][A
 59%|█████▉    | 257/436 [00:06<00:04, 42.08it/s][A
 60%|██████    | 262/436 [00:06<00:04, 42.39it/s][A
 61%|██████    | 267/436 [00:06<00:03, 42.95it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 43.38it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 43.70it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 43.94it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.08it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.17it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.19it/s][A
 69%|██████▉   | 302/436 [00:07<00:03, 44.07it/s][A
 70%|███████   | 307/436 [00:07<00:02, 44.00it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.05it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.06it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.33it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.38it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.27it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.21it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 43.19it/s][A
 80%|███████▉  | 347/436 [00:08<00:02, 43.30it/s][A
 81%|████████  | 352/436 [00:08<00:01, 43.50it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 43.71it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.00it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.18it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.23it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.34it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.15it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.00it/s][A
 90%|████████▉ | 392/436 [00:09<00:01, 43.89it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 43.86it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.03it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.22it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.36it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.50it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.46it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.33it/s][A
 99%|█████████▉| 432/436 [00:10<00:00, 44.06it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:47<00:40,  2.90it/s]
100%|██████████| 436/436 [00:10<00:00, 44.06it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:58:11,178 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 17:58:11,410 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:58:16,730 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:58:16,747 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:58:16,756 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [04:01<14:27,  7.48s/it] 80%|████████  | 470/585 [04:01<10:23,  5.42s/it] 81%|████████  | 471/585 [04:01<07:22,  3.88s/it] 81%|████████  | 472/585 [04:02<05:17,  2.81s/it] 81%|████████  | 473/585 [04:02<03:50,  2.06s/it] 81%|████████  | 474/585 [04:02<02:49,  1.53s/it] 81%|████████  | 475/585 [04:03<02:07,  1.16s/it] 81%|████████▏ | 476/585 [04:03<01:38,  1.11it/s] 82%|████████▏ | 477/585 [04:03<01:17,  1.39it/s] 82%|████████▏ | 478/585 [04:04<01:03,  1.68it/s] 82%|████████▏ | 479/585 [04:04<00:54,  1.96it/s] 82%|████████▏ | 480/585 [04:04<00:46,  2.24it/s] 82%|████████▏ | 481/585 [04:05<00:41,  2.48it/s] 82%|████████▏ | 482/585 [04:05<00:38,  2.69it/s] 83%|████████▎ | 483/585 [04:05<00:35,  2.85it/s] 83%|████████▎ | 484/585 [04:05<00:33,  2.98it/s] 83%|████████▎ | 485/585 [04:06<00:32,  3.08it/s] 83%|████████▎ | 486/585 [04:06<00:31,  3.15it/s] 83%|████████▎ | 487/585 [04:06<00:30,  3.20it/s] 83%|████████▎ | 488/585 [04:07<00:29,  3.24it/s] 84%|████████▎ | 489/585 [04:07<00:29,  3.22it/s] 84%|████████▍ | 490/585 [04:07<00:29,  3.25it/s] 84%|████████▍ | 491/585 [04:08<00:28,  3.27it/s] 84%|████████▍ | 492/585 [04:08<00:28,  3.29it/s] 84%|████████▍ | 493/585 [04:08<00:27,  3.30it/s] 84%|████████▍ | 494/585 [04:08<00:27,  3.31it/s] 85%|████████▍ | 495/585 [04:09<00:27,  3.31it/s] 85%|████████▍ | 496/585 [04:09<00:26,  3.32it/s] 85%|████████▍ | 497/585 [04:09<00:26,  3.32it/s] 85%|████████▌ | 498/585 [04:10<00:26,  3.32it/s] 85%|████████▌ | 499/585 [04:10<00:25,  3.31it/s] 85%|████████▌ | 500/585 [04:10<00:25,  3.32it/s]                                                  85%|████████▌ | 500/585 [04:10<00:25,  3.32it/s] 86%|████████▌ | 501/585 [04:11<00:25,  3.32it/s] 86%|████████▌ | 502/585 [04:11<00:24,  3.32it/s] 86%|████████▌ | 503/585 [04:11<00:24,  3.33it/s] 86%|████████▌ | 504/585 [04:11<00:24,  3.33it/s] 86%|████████▋ | 505/585 [04:12<00:24,  3.33it/s] 86%|████████▋ | 506/585 [04:12<00:23,  3.33it/s] 87%|████████▋ | 507/585 [04:12<00:23,  3.33it/s] 87%|████████▋ | 508/585 [04:13<00:23,  3.34it/s] 87%|████████▋ | 509/585 [04:13<00:22,  3.32it/s] 87%|████████▋ | 510/585 [04:13<00:22,  3.33it/s] 87%|████████▋ | 511/585 [04:14<00:22,  3.33it/s] 88%|████████▊ | 512/585 [04:14<00:21,  3.33it/s] 88%|████████▊ | 513/585 [04:14<00:21,  3.33it/s] 88%|████████▊ | 514/585 [04:14<00:21,  3.33it/s] 88%|████████▊ | 515/585 [04:15<00:21,  3.33it/s] 88%|████████▊ | 516/585 [04:15<00:20,  3.33it/s] 88%|████████▊ | 517/585 [04:15<00:20,  3.33it/s] 89%|████████▊ | 518/585 [04:16<00:20,  3.34it/s] 89%|████████▊ | 519/585 [04:16<00:20,  3.27it/s] 89%|████████▉ | 520/585 [04:16<00:19,  3.29it/s] 89%|████████▉ | 521/585 [04:17<00:19,  3.30it/s] 89%|████████▉ | 522/585 [04:17<00:19,  3.31it/s] 89%|████████▉ | 523/585 [04:17<00:18,  3.31it/s] 90%|████████▉ | 524/585 [04:17<00:18,  3.33it/s] 90%|████████▉ | 525/585 [04:18<00:18,  3.33it/s] 90%|████████▉ | 526/585 [04:18<00:17,  3.33it/s] 90%|█████████ | 527/585 [04:18<00:17,  3.33it/s] 90%|█████████ | 528/585 [04:19<00:17,  3.33it/s] 90%|█████████ | 529/585 [04:19<00:16,  3.33it/s] 91%|█████████ | 530/585 [04:19<00:16,  3.34it/s] 91%|█████████ | 531/585 [04:20<00:16,  3.34it/s] 91%|█████████ | 532/585 [04:20<00:15,  3.34it/s] 91%|█████████ | 533/585 [04:20<00:15,  3.34it/s] 91%|█████████▏| 534/585 [04:20<00:15,  3.34it/s] 91%|█████████▏| 535/585 [04:21<00:15,  3.33it/s] 92%|█████████▏| 536/585 [04:21<00:14,  3.34it/s] 92%|█████████▏| 537/585 [04:21<00:14,  3.34it/s] 92%|█████████▏| 538/585 [04:22<00:14,  3.34it/s] 92%|█████████▏| 539/585 [04:22<00:13,  3.34it/s] 92%|█████████▏| 540/585 [04:22<00:13,  3.34it/s] 92%|█████████▏| 541/585 [04:23<00:13,  3.34it/s] 93%|█████████▎| 542/585 [04:23<00:12,  3.34it/s] 93%|█████████▎| 543/585 [04:23<00:12,  3.34it/s] 93%|█████████▎| 544/585 [04:23<00:12,  3.34it/s] 93%|█████████▎| 545/585 [04:24<00:11,  3.34it/s] 93%|█████████▎| 546/585 [04:24<00:11,  3.32it/s] 94%|█████████▎| 547/585 [04:24<00:11,  3.33it/s] 94%|█████████▎| 548/585 [04:25<00:11,  3.34it/s] 94%|█████████▍| 549/585 [04:25<00:10,  3.33it/s] 94%|█████████▍| 550/585 [04:25<00:10,  3.34it/s] 94%|█████████▍| 551/585 [04:26<00:10,  3.34it/s] 94%|█████████▍| 552/585 [04:26<00:09,  3.34it/s] 95%|█████████▍| 553/585 [04:26<00:09,  3.33it/s] 95%|█████████▍| 554/585 [04:26<00:09,  3.34it/s] 95%|█████████▍| 555/585 [04:27<00:08,  3.34it/s] 95%|█████████▌| 556/585 [04:27<00:08,  3.27it/s] 95%|█████████▌| 557/585 [04:27<00:08,  3.29it/s] 95%|█████████▌| 558/585 [04:28<00:08,  3.30it/s] 96%|█████████▌| 559/585 [04:28<00:07,  3.31it/s] 96%|█████████▌| 560/585 [04:28<00:07,  3.32it/s] 96%|█████████▌| 561/585 [04:29<00:07,  3.32it/s] 96%|█████████▌| 562/585 [04:29<00:06,  3.33it/s] 96%|█████████▌| 563/585 [04:29<00:06,  3.33it/s] 96%|█████████▋| 564/585 [04:29<00:06,  3.33it/s] 97%|█████████▋| 565/585 [04:30<00:06,  3.33it/s] 97%|█████████▋| 566/585 [04:30<00:05,  3.33it/s] 97%|█████████▋| 567/585 [04:30<00:05,  3.33it/s] 97%|█████████▋| 568/585 [04:31<00:05,  3.33it/s] 97%|█████████▋| 569/585 [04:31<00:04,  3.32it/s] 97%|█████████▋| 570/585 [04:31<00:04,  3.33it/s] 98%|█████████▊| 571/585 [04:32<00:04,  3.33it/s] 98%|█████████▊| 572/585 [04:32<00:03,  3.33it/s] 98%|█████████▊| 573/585 [04:32<00:03,  3.33it/s] 98%|█████████▊| 574/585 [04:32<00:03,  3.33it/s] 98%|█████████▊| 575/585 [04:33<00:03,  3.33it/s] 98%|█████████▊| 576/585 [04:33<00:02,  3.34it/s] 99%|█████████▊| 577/585 [04:33<00:02,  3.34it/s] 99%|█████████▉| 578/585 [04:34<00:02,  3.34it/s] 99%|█████████▉| 579/585 [04:34<00:01,  3.34it/s] 99%|█████████▉| 580/585 [04:34<00:01,  3.34it/s] 99%|█████████▉| 581/585 [04:35<00:01,  3.34it/s] 99%|█████████▉| 582/585 [04:35<00:00,  3.34it/s]100%|█████████▉| 583/585 [04:35<00:00,  3.33it/s]100%|█████████▉| 584/585 [04:35<00:00,  3.34it/s]100%|██████████| 585/585 [04:36<00:00,  3.33it/s][INFO|trainer.py:2140] 2023-08-28 17:58:59,991 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:58:59,992 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 17:58:59,992 >>   Batch size = 8
{'eval_loss': 0.9770625233650208, 'eval_runtime': 10.1307, 'eval_samples_per_second': 344.299, 'eval_steps_per_second': 43.037, 'epoch': 4.0}
{'loss': 0.7691, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.92it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.67it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.09it/s][A
  5%|▌         | 22/436 [00:00<00:08, 46.24it/s][A
  6%|▌         | 27/436 [00:00<00:08, 45.57it/s][A
  7%|▋         | 32/436 [00:00<00:08, 44.98it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.59it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.17it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.02it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.21it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.33it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.54it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.50it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.47it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.30it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 44.07it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 43.92it/s][A
 21%|██        | 92/436 [00:02<00:07, 43.93it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.08it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.03it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.45it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.61it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.48it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.29it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.98it/s][A
 30%|███       | 132/436 [00:02<00:06, 43.94it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 43.93it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.00it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.17it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.24it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.40it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.37it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.23it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.15it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.02it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.01it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.07it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.15it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.43it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.42it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.29it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.29it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.16it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.00it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.01it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.04it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.24it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.36it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.48it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.27it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.30it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.17it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.05it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.03it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 43.98it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.27it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.38it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.40it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.27it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.27it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.21it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.08it/s][A
 73%|███████▎  | 317/436 [00:07<00:03, 35.05it/s][A
 74%|███████▍  | 322/436 [00:07<00:03, 37.46it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 39.42it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 40.88it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 41.93it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 42.80it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 43.26it/s][A
 81%|████████  | 352/436 [00:08<00:01, 43.56it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 43.36it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 43.43it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 43.73it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 43.88it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 43.93it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.15it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.24it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.32it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 44.13it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 43.96it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 43.79it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 43.97it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.11it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.23it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.36it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.33it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:46<00:00,  3.33it/s]
100%|██████████| 436/436 [00:09<00:00, 44.33it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:59:10,006 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 17:59:10,028 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:59:19,427 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:59:19,455 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:59:19,465 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 17:59:35,458 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 17:59:35,467 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-117 (score: 0.9548506140708923).
                                                 100%|██████████| 585/585 [05:17<00:00,  3.33it/s]100%|██████████| 585/585 [05:17<00:00,  1.84it/s]
[INFO|trainer.py:1894] 2023-08-28 17:59:41,181 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 17:59:41,200 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:59:48,040 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:59:48,281 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:59:48,369 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:59:49,308 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:59:49,309 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:59:49,309 >>   train_loss               =     0.7636
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:59:49,309 >>   train_runtime            = 0:05:17.46
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:59:49,309 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:59:49,309 >>   train_samples_per_second =    118.122
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:59:49,309 >>   train_steps_per_second   =      1.843
{'eval_loss': 0.9806485176086426, 'eval_runtime': 9.9585, 'eval_samples_per_second': 350.252, 'eval_steps_per_second': 43.782, 'epoch': 5.0}
{'train_runtime': 317.4673, 'train_samples_per_second': 118.122, 'train_steps_per_second': 1.843, 'train_loss': 0.7635561755579761, 'epoch': 5.0}
08/28/2023 17:59:49 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 17:59:49,487 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:59:49,487 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 17:59:49,487 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 55.12it/s]  3%|▎         | 12/436 [00:00<00:08, 48.84it/s]  4%|▍         | 17/436 [00:00<00:08, 47.16it/s]  5%|▌         | 22/436 [00:00<00:08, 46.34it/s]  6%|▌         | 27/436 [00:00<00:08, 45.89it/s]  7%|▋         | 32/436 [00:00<00:08, 45.62it/s]  8%|▊         | 37/436 [00:00<00:08, 45.40it/s] 10%|▉         | 42/436 [00:00<00:08, 44.95it/s] 11%|█         | 47/436 [00:01<00:08, 44.37it/s] 12%|█▏        | 52/436 [00:01<00:08, 44.19it/s] 13%|█▎        | 57/436 [00:01<00:08, 44.29it/s] 14%|█▍        | 62/436 [00:01<00:08, 44.45it/s] 15%|█▌        | 67/436 [00:01<00:08, 44.57it/s] 17%|█▋        | 72/436 [00:01<00:08, 44.74it/s] 18%|█▊        | 77/436 [00:01<00:08, 44.40it/s] 19%|█▉        | 82/436 [00:01<00:07, 44.48it/s] 20%|█▉        | 87/436 [00:01<00:07, 44.36it/s] 21%|██        | 92/436 [00:02<00:07, 44.08it/s] 22%|██▏       | 97/436 [00:02<00:07, 44.09it/s] 23%|██▎       | 102/436 [00:02<00:07, 44.21it/s] 25%|██▍       | 107/436 [00:02<00:07, 44.35it/s] 26%|██▌       | 112/436 [00:02<00:07, 44.50it/s] 27%|██▋       | 117/436 [00:02<00:07, 44.61it/s] 28%|██▊       | 122/436 [00:02<00:07, 44.69it/s] 29%|██▉       | 127/436 [00:02<00:06, 44.62it/s] 30%|███       | 132/436 [00:02<00:06, 44.29it/s] 31%|███▏      | 137/436 [00:03<00:06, 44.19it/s] 33%|███▎      | 142/436 [00:03<00:06, 44.17it/s] 34%|███▎      | 147/436 [00:03<00:06, 44.26it/s] 35%|███▍      | 152/436 [00:03<00:06, 44.43it/s] 36%|███▌      | 157/436 [00:03<00:06, 44.47it/s] 37%|███▋      | 162/436 [00:03<00:06, 44.66it/s] 38%|███▊      | 167/436 [00:03<00:06, 44.76it/s] 39%|███▉      | 172/436 [00:03<00:05, 44.46it/s] 41%|████      | 177/436 [00:03<00:05, 44.33it/s] 42%|████▏     | 182/436 [00:04<00:05, 44.23it/s] 43%|████▎     | 187/436 [00:04<00:05, 44.19it/s] 44%|████▍     | 192/436 [00:04<00:05, 44.30it/s] 45%|████▌     | 197/436 [00:04<00:05, 44.34it/s] 46%|████▋     | 202/436 [00:04<00:05, 44.48it/s] 47%|████▋     | 207/436 [00:04<00:05, 44.58it/s] 49%|████▊     | 212/436 [00:04<00:05, 44.58it/s] 50%|████▉     | 217/436 [00:04<00:04, 44.49it/s] 51%|█████     | 222/436 [00:04<00:04, 44.22it/s] 52%|█████▏    | 227/436 [00:05<00:04, 44.11it/s] 53%|█████▎    | 232/436 [00:05<00:04, 44.00it/s] 54%|█████▍    | 237/436 [00:05<00:04, 44.17it/s] 56%|█████▌    | 242/436 [00:05<00:04, 44.28it/s] 57%|█████▋    | 247/436 [00:05<00:04, 44.49it/s] 58%|█████▊    | 252/436 [00:05<00:04, 44.58it/s] 59%|█████▉    | 257/436 [00:05<00:04, 44.45it/s] 60%|██████    | 262/436 [00:05<00:03, 44.43it/s] 61%|██████    | 267/436 [00:05<00:03, 44.23it/s] 62%|██████▏   | 272/436 [00:06<00:03, 44.01it/s] 64%|██████▎   | 277/436 [00:06<00:03, 44.04it/s] 65%|██████▍   | 282/436 [00:06<00:03, 44.20it/s] 66%|██████▌   | 287/436 [00:06<00:03, 44.33it/s] 67%|██████▋   | 292/436 [00:06<00:03, 44.48it/s] 68%|██████▊   | 297/436 [00:06<00:03, 44.58it/s] 69%|██████▉   | 302/436 [00:06<00:03, 44.64it/s] 70%|███████   | 307/436 [00:06<00:02, 44.42it/s] 72%|███████▏  | 312/436 [00:06<00:02, 44.23it/s] 73%|███████▎  | 317/436 [00:07<00:02, 44.23it/s] 74%|███████▍  | 322/436 [00:07<00:02, 44.11it/s] 75%|███████▌  | 327/436 [00:07<00:02, 44.14it/s] 76%|███████▌  | 332/436 [00:07<00:02, 44.25it/s] 77%|███████▋  | 337/436 [00:07<00:02, 44.45it/s] 78%|███████▊  | 342/436 [00:07<00:02, 44.54it/s] 80%|███████▉  | 347/436 [00:07<00:02, 42.57it/s] 81%|████████  | 352/436 [00:07<00:01, 43.15it/s] 82%|████████▏ | 357/436 [00:08<00:01, 43.43it/s] 83%|████████▎ | 362/436 [00:08<00:01, 43.57it/s] 84%|████████▍ | 367/436 [00:08<00:01, 43.77it/s] 85%|████████▌ | 372/436 [00:08<00:01, 43.99it/s] 86%|████████▋ | 377/436 [00:08<00:01, 44.15it/s] 88%|████████▊ | 382/436 [00:08<00:01, 44.20it/s] 89%|████████▉ | 387/436 [00:08<00:01, 44.13it/s] 90%|████████▉ | 392/436 [00:08<00:00, 44.19it/s] 91%|█████████ | 397/436 [00:08<00:00, 44.25it/s] 92%|█████████▏| 402/436 [00:09<00:00, 44.25it/s] 93%|█████████▎| 407/436 [00:09<00:00, 44.18it/s] 94%|█████████▍| 412/436 [00:09<00:00, 44.16it/s] 96%|█████████▌| 417/436 [00:09<00:00, 44.33it/s] 97%|█████████▋| 422/436 [00:09<00:00, 44.30it/s] 98%|█████████▊| 427/436 [00:09<00:00, 44.33it/s] 99%|█████████▉| 432/436 [00:09<00:00, 44.29it/s]100%|██████████| 436/436 [00:09<00:00, 44.37it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:59:59,335 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:59:59,335 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:59:59,335 >>   eval_loss               =     0.9549
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:59:59,335 >>   eval_runtime            = 0:00:09.84
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:59:59,335 >>   eval_samples            =       3488
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:59:59,335 >>   eval_samples_per_second =    354.189
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:59:59,335 >>   eval_steps_per_second   =     44.274
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:59:59,335 >>   perplexity              =     2.5983
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:06,966 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:06,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:06,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:06,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:06,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:00:07,602 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:00:07,603 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:00:08,277 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:00:09,313 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:00:09,313 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:12,458 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:12,510 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:12,510 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:12,510 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:12,510 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:00:13,263 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:00:13,265 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:00:14,032 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:00:14,203 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:00:14,203 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/checkpoint-117
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11910
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12010, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.32it/s]Extractor Predicting: 3it [00:02,  1.33it/s]Extractor Predicting: 4it [00:02,  1.36it/s]Extractor Predicting: 5it [00:03,  1.38it/s]Extractor Predicting: 6it [00:04,  1.39it/s]Extractor Predicting: 7it [00:05,  1.41it/s]Extractor Predicting: 8it [00:05,  1.45it/s]Extractor Predicting: 9it [00:06,  1.39it/s]Extractor Predicting: 10it [00:07,  1.40it/s]Extractor Predicting: 11it [00:07,  1.40it/s]Extractor Predicting: 12it [00:08,  1.39it/s]Extractor Predicting: 13it [00:09,  1.39it/s]Extractor Predicting: 14it [00:10,  1.37it/s]Extractor Predicting: 15it [00:10,  1.39it/s]Extractor Predicting: 16it [00:11,  1.36it/s]Extractor Predicting: 17it [00:12,  1.36it/s]Extractor Predicting: 18it [00:13,  1.38it/s]Extractor Predicting: 19it [00:13,  1.41it/s]Extractor Predicting: 20it [00:14,  1.41it/s]Extractor Predicting: 21it [00:15,  1.38it/s]Extractor Predicting: 22it [00:15,  1.40it/s]Extractor Predicting: 23it [00:16,  1.40it/s]Extractor Predicting: 24it [00:17,  1.34it/s]Extractor Predicting: 25it [00:18,  1.33it/s]Extractor Predicting: 26it [00:18,  1.35it/s]Extractor Predicting: 27it [00:19,  1.36it/s]Extractor Predicting: 28it [00:20,  1.37it/s]Extractor Predicting: 29it [00:21,  1.36it/s]Extractor Predicting: 30it [00:21,  1.34it/s]Extractor Predicting: 31it [00:22,  1.34it/s]Extractor Predicting: 32it [00:23,  1.35it/s]Extractor Predicting: 33it [00:24,  1.34it/s]Extractor Predicting: 34it [00:24,  1.33it/s]Extractor Predicting: 35it [00:25,  1.31it/s]Extractor Predicting: 36it [00:26,  1.30it/s]Extractor Predicting: 37it [00:27,  1.30it/s]Extractor Predicting: 38it [00:27,  1.30it/s]Extractor Predicting: 39it [00:28,  1.31it/s]Extractor Predicting: 40it [00:29,  1.29it/s]Extractor Predicting: 41it [00:30,  1.29it/s]Extractor Predicting: 42it [00:31,  1.29it/s]Extractor Predicting: 43it [00:31,  1.28it/s]Extractor Predicting: 44it [00:32,  1.26it/s]Extractor Predicting: 45it [00:33,  1.26it/s]Extractor Predicting: 46it [00:34,  1.26it/s]Extractor Predicting: 47it [00:34,  1.31it/s]Extractor Predicting: 48it [00:35,  1.31it/s]Extractor Predicting: 49it [00:36,  1.33it/s]Extractor Predicting: 50it [00:37,  1.29it/s]Extractor Predicting: 51it [00:37,  1.32it/s]Extractor Predicting: 52it [00:38,  1.33it/s]Extractor Predicting: 53it [00:39,  1.31it/s]Extractor Predicting: 54it [00:40,  1.31it/s]Extractor Predicting: 55it [00:41,  1.31it/s]Extractor Predicting: 56it [00:41,  1.31it/s]Extractor Predicting: 57it [00:42,  1.33it/s]Extractor Predicting: 58it [00:43,  1.34it/s]Extractor Predicting: 59it [00:44,  1.31it/s]Extractor Predicting: 60it [00:44,  1.30it/s]Extractor Predicting: 61it [00:45,  1.28it/s]Extractor Predicting: 62it [00:46,  1.28it/s]Extractor Predicting: 63it [00:47,  1.30it/s]Extractor Predicting: 64it [00:47,  1.28it/s]Extractor Predicting: 65it [00:48,  1.31it/s]Extractor Predicting: 66it [00:49,  1.32it/s]Extractor Predicting: 67it [00:50,  1.32it/s]Extractor Predicting: 68it [00:50,  1.33it/s]Extractor Predicting: 69it [00:51,  1.32it/s]Extractor Predicting: 70it [00:52,  1.32it/s]Extractor Predicting: 71it [00:53,  1.29it/s]Extractor Predicting: 72it [00:54,  1.30it/s]Extractor Predicting: 73it [00:54,  1.30it/s]Extractor Predicting: 74it [00:55,  1.31it/s]Extractor Predicting: 75it [00:56,  1.32it/s]Extractor Predicting: 76it [00:57,  1.33it/s]Extractor Predicting: 77it [00:57,  1.33it/s]Extractor Predicting: 78it [00:58,  1.31it/s]Extractor Predicting: 79it [00:59,  1.31it/s]Extractor Predicting: 80it [01:00,  1.30it/s]Extractor Predicting: 81it [01:00,  1.29it/s]Extractor Predicting: 82it [01:01,  1.30it/s]Extractor Predicting: 83it [01:02,  1.33it/s]Extractor Predicting: 84it [01:03,  1.31it/s]Extractor Predicting: 85it [01:03,  1.30it/s]Extractor Predicting: 86it [01:04,  1.31it/s]Extractor Predicting: 87it [01:05,  1.32it/s]Extractor Predicting: 88it [01:06,  1.34it/s]Extractor Predicting: 89it [01:06,  1.33it/s]Extractor Predicting: 90it [01:07,  1.36it/s]Extractor Predicting: 91it [01:08,  1.39it/s]Extractor Predicting: 92it [01:09,  1.40it/s]Extractor Predicting: 93it [01:09,  1.38it/s]Extractor Predicting: 94it [01:10,  1.40it/s]Extractor Predicting: 95it [01:11,  1.40it/s]Extractor Predicting: 96it [01:11,  1.39it/s]Extractor Predicting: 97it [01:12,  1.38it/s]Extractor Predicting: 98it [01:13,  1.35it/s]Extractor Predicting: 99it [01:14,  1.33it/s]Extractor Predicting: 100it [01:14,  1.32it/s]Extractor Predicting: 101it [01:15,  1.36it/s]Extractor Predicting: 102it [01:16,  1.30it/s]Extractor Predicting: 103it [01:17,  1.31it/s]Extractor Predicting: 104it [01:17,  1.35it/s]Extractor Predicting: 105it [01:18,  1.33it/s]Extractor Predicting: 106it [01:19,  1.36it/s]Extractor Predicting: 107it [01:20,  1.36it/s]Extractor Predicting: 108it [01:20,  1.36it/s]Extractor Predicting: 109it [01:21,  1.36it/s]Extractor Predicting: 110it [01:22,  1.35it/s]Extractor Predicting: 111it [01:23,  1.39it/s]Extractor Predicting: 112it [01:23,  1.40it/s]Extractor Predicting: 113it [01:24,  1.44it/s]Extractor Predicting: 114it [01:25,  1.43it/s]Extractor Predicting: 115it [01:25,  1.44it/s]Extractor Predicting: 116it [01:26,  1.43it/s]Extractor Predicting: 117it [01:27,  1.40it/s]Extractor Predicting: 118it [01:28,  1.38it/s]Extractor Predicting: 119it [01:28,  1.32it/s]Extractor Predicting: 120it [01:29,  1.31it/s]Extractor Predicting: 121it [01:30,  1.35it/s]Extractor Predicting: 122it [01:31,  1.35it/s]Extractor Predicting: 123it [01:31,  1.34it/s]Extractor Predicting: 124it [01:32,  1.32it/s]Extractor Predicting: 125it [01:33,  1.31it/s]Extractor Predicting: 126it [01:34,  1.28it/s]Extractor Predicting: 127it [01:34,  1.29it/s]Extractor Predicting: 128it [01:35,  1.35it/s]Extractor Predicting: 129it [01:36,  1.32it/s]Extractor Predicting: 130it [01:37,  1.36it/s]Extractor Predicting: 131it [01:37,  1.35it/s]Extractor Predicting: 132it [01:38,  1.34it/s]Extractor Predicting: 133it [01:39,  1.33it/s]Extractor Predicting: 134it [01:40,  1.31it/s]Extractor Predicting: 135it [01:40,  1.31it/s]Extractor Predicting: 136it [01:41,  1.33it/s]Extractor Predicting: 137it [01:42,  1.34it/s]Extractor Predicting: 138it [01:43,  1.33it/s]Extractor Predicting: 139it [01:43,  1.31it/s]Extractor Predicting: 140it [01:44,  1.35it/s]Extractor Predicting: 141it [01:45,  1.33it/s]Extractor Predicting: 142it [01:46,  1.30it/s]Extractor Predicting: 143it [01:46,  1.32it/s]Extractor Predicting: 144it [01:47,  1.60it/s]Extractor Predicting: 144it [01:47,  1.34it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:11,997 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:12,014 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:12,014 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:12,014 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:12,014 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:02:12,495 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:02:12,496 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:02:12,751 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:02:13,873 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:02:13,873 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:15,742 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:15,748 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:15,748 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:15,749 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:15,749 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:02:16,072 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:02:16,073 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:02:16,332 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:02:16,553 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:02:16,553 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.40716612377850164,
  "recall": 0.10751146788990826,
  "score": 0.1701066001360853,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19834
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19934, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:02,  1.37it/s]Extractor Predicting: 4it [00:02,  1.34it/s]Extractor Predicting: 5it [00:03,  1.32it/s]Extractor Predicting: 6it [00:04,  1.32it/s]Extractor Predicting: 7it [00:05,  1.34it/s]Extractor Predicting: 8it [00:06,  1.30it/s]Extractor Predicting: 9it [00:06,  1.34it/s]Extractor Predicting: 10it [00:07,  1.30it/s]Extractor Predicting: 11it [00:08,  1.30it/s]Extractor Predicting: 12it [00:09,  1.32it/s]Extractor Predicting: 13it [00:09,  1.33it/s]Extractor Predicting: 14it [00:10,  1.33it/s]Extractor Predicting: 15it [00:11,  1.32it/s]Extractor Predicting: 16it [00:12,  1.32it/s]Extractor Predicting: 17it [00:12,  1.30it/s]Extractor Predicting: 18it [00:13,  1.32it/s]Extractor Predicting: 19it [00:14,  1.34it/s]Extractor Predicting: 20it [00:15,  1.34it/s]Extractor Predicting: 21it [00:15,  1.34it/s]Extractor Predicting: 22it [00:16,  1.37it/s]Extractor Predicting: 23it [00:17,  1.37it/s]Extractor Predicting: 24it [00:18,  1.34it/s]Extractor Predicting: 25it [00:18,  1.31it/s]Extractor Predicting: 26it [00:19,  1.34it/s]Extractor Predicting: 27it [00:20,  1.33it/s]Extractor Predicting: 28it [00:21,  1.30it/s]Extractor Predicting: 29it [00:21,  1.31it/s]Extractor Predicting: 30it [00:22,  1.28it/s]Extractor Predicting: 31it [00:23,  1.35it/s]Extractor Predicting: 32it [00:23,  1.41it/s]Extractor Predicting: 33it [00:24,  1.44it/s]Extractor Predicting: 34it [00:25,  1.45it/s]Extractor Predicting: 35it [00:25,  1.47it/s]Extractor Predicting: 36it [00:26,  1.47it/s]Extractor Predicting: 37it [00:27,  1.48it/s]Extractor Predicting: 38it [00:27,  1.50it/s]Extractor Predicting: 39it [00:28,  1.53it/s]Extractor Predicting: 40it [00:29,  1.55it/s]Extractor Predicting: 41it [00:29,  1.53it/s]Extractor Predicting: 42it [00:30,  1.54it/s]Extractor Predicting: 43it [00:31,  1.50it/s]Extractor Predicting: 44it [00:31,  1.54it/s]Extractor Predicting: 45it [00:32,  1.52it/s]Extractor Predicting: 46it [00:33,  1.48it/s]Extractor Predicting: 47it [00:33,  1.51it/s]Extractor Predicting: 48it [00:34,  1.55it/s]Extractor Predicting: 49it [00:35,  1.56it/s]Extractor Predicting: 50it [00:35,  1.54it/s]Extractor Predicting: 51it [00:36,  1.56it/s]Extractor Predicting: 52it [00:37,  1.54it/s]Extractor Predicting: 53it [00:37,  1.53it/s]Extractor Predicting: 54it [00:38,  1.51it/s]Extractor Predicting: 55it [00:39,  1.49it/s]Extractor Predicting: 56it [00:39,  1.50it/s]Extractor Predicting: 57it [00:40,  1.48it/s]Extractor Predicting: 58it [00:41,  1.46it/s]Extractor Predicting: 59it [00:41,  1.42it/s]Extractor Predicting: 60it [00:42,  1.37it/s]Extractor Predicting: 61it [00:43,  1.32it/s]Extractor Predicting: 62it [00:44,  1.28it/s]Extractor Predicting: 63it [00:45,  1.29it/s]Extractor Predicting: 64it [00:45,  1.27it/s]Extractor Predicting: 65it [00:46,  1.30it/s]Extractor Predicting: 66it [00:47,  1.30it/s]Extractor Predicting: 67it [00:48,  1.30it/s]Extractor Predicting: 68it [00:48,  1.28it/s]Extractor Predicting: 69it [00:49,  1.26it/s]Extractor Predicting: 70it [00:50,  1.25it/s]Extractor Predicting: 71it [00:51,  1.23it/s]Extractor Predicting: 72it [00:52,  1.24it/s]Extractor Predicting: 73it [00:53,  1.23it/s]Extractor Predicting: 74it [00:53,  1.26it/s]Extractor Predicting: 75it [00:54,  1.27it/s]Extractor Predicting: 76it [00:55,  1.28it/s]Extractor Predicting: 77it [00:56,  1.27it/s]Extractor Predicting: 78it [00:56,  1.25it/s]Extractor Predicting: 79it [00:57,  1.23it/s]Extractor Predicting: 80it [00:58,  1.25it/s]Extractor Predicting: 81it [00:59,  1.12it/s]Extractor Predicting: 82it [01:00,  1.15it/s]Extractor Predicting: 83it [01:01,  1.18it/s]Extractor Predicting: 84it [01:02,  1.23it/s]Extractor Predicting: 85it [01:02,  1.21it/s]Extractor Predicting: 86it [01:03,  1.21it/s]Extractor Predicting: 87it [01:04,  1.13it/s]Extractor Predicting: 88it [01:05,  1.17it/s]Extractor Predicting: 89it [01:06,  1.20it/s]Extractor Predicting: 90it [01:07,  1.26it/s]Extractor Predicting: 91it [01:07,  1.24it/s]Extractor Predicting: 92it [01:08,  1.23it/s]Extractor Predicting: 93it [01:09,  1.27it/s]Extractor Predicting: 94it [01:10,  1.28it/s]Extractor Predicting: 95it [01:10,  1.30it/s]Extractor Predicting: 96it [01:11,  1.34it/s]Extractor Predicting: 97it [01:12,  1.33it/s]Extractor Predicting: 98it [01:13,  1.34it/s]Extractor Predicting: 99it [01:13,  1.34it/s]Extractor Predicting: 100it [01:14,  1.34it/s]Extractor Predicting: 101it [01:15,  1.39it/s]Extractor Predicting: 102it [01:15,  1.39it/s]Extractor Predicting: 103it [01:16,  1.36it/s]Extractor Predicting: 104it [01:17,  1.32it/s]Extractor Predicting: 105it [01:18,  1.32it/s]Extractor Predicting: 106it [01:19,  1.32it/s]Extractor Predicting: 107it [01:19,  1.33it/s]Extractor Predicting: 108it [01:20,  1.32it/s]Extractor Predicting: 109it [01:21,  1.31it/s]Extractor Predicting: 110it [01:22,  1.29it/s]Extractor Predicting: 111it [01:22,  1.31it/s]Extractor Predicting: 112it [01:23,  1.28it/s]Extractor Predicting: 113it [01:24,  1.32it/s]Extractor Predicting: 114it [01:25,  1.28it/s]Extractor Predicting: 115it [01:26,  1.29it/s]Extractor Predicting: 116it [01:26,  1.35it/s]Extractor Predicting: 117it [01:27,  1.34it/s]Extractor Predicting: 118it [01:28,  1.39it/s]Extractor Predicting: 119it [01:28,  1.41it/s]Extractor Predicting: 120it [01:29,  1.43it/s]Extractor Predicting: 121it [01:30,  1.44it/s]Extractor Predicting: 122it [01:30,  1.44it/s]Extractor Predicting: 123it [01:31,  1.46it/s]Extractor Predicting: 124it [01:32,  1.46it/s]Extractor Predicting: 125it [01:32,  1.48it/s]Extractor Predicting: 126it [01:33,  1.45it/s]Extractor Predicting: 127it [01:34,  1.45it/s]Extractor Predicting: 128it [01:34,  1.50it/s]Extractor Predicting: 129it [01:35,  1.48it/s]Extractor Predicting: 130it [01:36,  1.55it/s]Extractor Predicting: 131it [01:36,  1.57it/s]Extractor Predicting: 132it [01:37,  1.52it/s]Extractor Predicting: 133it [01:38,  1.47it/s]Extractor Predicting: 134it [01:38,  1.46it/s]Extractor Predicting: 135it [01:39,  1.48it/s]Extractor Predicting: 136it [01:40,  1.51it/s]Extractor Predicting: 137it [01:40,  1.50it/s]Extractor Predicting: 138it [01:41,  1.50it/s]Extractor Predicting: 139it [01:42,  1.46it/s]Extractor Predicting: 140it [01:42,  1.49it/s]Extractor Predicting: 141it [01:43,  1.48it/s]Extractor Predicting: 142it [01:44,  1.53it/s]Extractor Predicting: 143it [01:44,  1.51it/s]Extractor Predicting: 144it [01:45,  1.49it/s]Extractor Predicting: 145it [01:46,  1.47it/s]Extractor Predicting: 146it [01:46,  1.43it/s]Extractor Predicting: 147it [01:47,  1.41it/s]Extractor Predicting: 148it [01:48,  1.41it/s]Extractor Predicting: 149it [01:49,  1.35it/s]Extractor Predicting: 150it [01:50,  1.34it/s]Extractor Predicting: 151it [01:50,  1.33it/s]Extractor Predicting: 152it [01:51,  1.35it/s]Extractor Predicting: 153it [01:52,  1.32it/s]Extractor Predicting: 154it [01:52,  1.35it/s]Extractor Predicting: 155it [01:53,  1.37it/s]Extractor Predicting: 156it [01:54,  1.35it/s]Extractor Predicting: 157it [01:55,  1.36it/s]Extractor Predicting: 158it [01:55,  1.34it/s]Extractor Predicting: 159it [01:56,  1.33it/s]Extractor Predicting: 160it [01:57,  1.32it/s]Extractor Predicting: 161it [01:58,  1.32it/s]Extractor Predicting: 162it [01:59,  1.31it/s]Extractor Predicting: 163it [01:59,  1.34it/s]Extractor Predicting: 164it [02:00,  1.34it/s]Extractor Predicting: 165it [02:01,  1.32it/s]Extractor Predicting: 166it [02:02,  1.30it/s]Extractor Predicting: 167it [02:02,  1.32it/s]Extractor Predicting: 168it [02:03,  1.31it/s]Extractor Predicting: 169it [02:04,  1.25it/s]Extractor Predicting: 170it [02:05,  1.27it/s]Extractor Predicting: 171it [02:05,  1.29it/s]Extractor Predicting: 172it [02:06,  1.31it/s]Extractor Predicting: 173it [02:07,  1.30it/s]Extractor Predicting: 174it [02:08,  1.31it/s]Extractor Predicting: 175it [02:09,  1.22it/s]Extractor Predicting: 176it [02:09,  1.23it/s]Extractor Predicting: 177it [02:10,  1.24it/s]Extractor Predicting: 178it [02:11,  1.27it/s]Extractor Predicting: 179it [02:12,  1.28it/s]Extractor Predicting: 180it [02:13,  1.29it/s]Extractor Predicting: 181it [02:13,  1.29it/s]Extractor Predicting: 182it [02:14,  1.30it/s]Extractor Predicting: 183it [02:15,  1.30it/s]Extractor Predicting: 184it [02:16,  1.31it/s]Extractor Predicting: 185it [02:16,  1.32it/s]Extractor Predicting: 186it [02:17,  1.31it/s]Extractor Predicting: 187it [02:18,  1.33it/s]Extractor Predicting: 188it [02:19,  1.32it/s]Extractor Predicting: 189it [02:19,  1.28it/s]Extractor Predicting: 190it [02:20,  1.29it/s]Extractor Predicting: 191it [02:21,  1.32it/s]Extractor Predicting: 192it [02:22,  1.33it/s]Extractor Predicting: 193it [02:22,  1.36it/s]Extractor Predicting: 194it [02:23,  1.34it/s]Extractor Predicting: 195it [02:24,  1.35it/s]Extractor Predicting: 196it [02:25,  1.34it/s]Extractor Predicting: 197it [02:25,  1.34it/s]Extractor Predicting: 198it [02:26,  1.30it/s]Extractor Predicting: 199it [02:27,  1.31it/s]Extractor Predicting: 200it [02:28,  1.32it/s]Extractor Predicting: 201it [02:28,  1.35it/s]Extractor Predicting: 202it [02:29,  1.33it/s]Extractor Predicting: 203it [02:30,  1.36it/s]Extractor Predicting: 204it [02:31,  1.34it/s]Extractor Predicting: 205it [02:31,  1.34it/s]Extractor Predicting: 206it [02:32,  1.39it/s]Extractor Predicting: 207it [02:33,  1.38it/s]Extractor Predicting: 208it [02:33,  1.40it/s]Extractor Predicting: 209it [02:34,  1.38it/s]Extractor Predicting: 210it [02:35,  1.34it/s]Extractor Predicting: 211it [02:36,  1.33it/s]Extractor Predicting: 212it [02:37,  1.30it/s]Extractor Predicting: 213it [02:37,  1.32it/s]Extractor Predicting: 214it [02:38,  1.32it/s]Extractor Predicting: 215it [02:39,  1.30it/s]Extractor Predicting: 216it [02:40,  1.26it/s]Extractor Predicting: 217it [02:41,  1.24it/s]Extractor Predicting: 218it [02:41,  1.27it/s]Extractor Predicting: 219it [02:42,  1.28it/s]Extractor Predicting: 220it [02:43,  1.32it/s]Extractor Predicting: 221it [02:44,  1.32it/s]Extractor Predicting: 222it [02:44,  1.31it/s]Extractor Predicting: 223it [02:45,  1.31it/s]Extractor Predicting: 224it [02:46,  1.30it/s]Extractor Predicting: 225it [02:47,  1.30it/s]Extractor Predicting: 226it [02:47,  1.31it/s]Extractor Predicting: 227it [02:48,  1.30it/s]Extractor Predicting: 228it [02:49,  1.34it/s]Extractor Predicting: 229it [02:50,  1.31it/s]Extractor Predicting: 230it [02:50,  1.32it/s]Extractor Predicting: 231it [02:51,  1.29it/s]Extractor Predicting: 232it [02:52,  1.28it/s]Extractor Predicting: 233it [02:53,  1.29it/s]Extractor Predicting: 234it [02:53,  1.31it/s]Extractor Predicting: 235it [02:54,  1.31it/s]Extractor Predicting: 236it [02:55,  1.31it/s]Extractor Predicting: 237it [02:56,  1.25it/s]Extractor Predicting: 238it [02:57,  1.24it/s]Extractor Predicting: 239it [02:57,  1.24it/s]Extractor Predicting: 240it [02:58,  1.27it/s]Extractor Predicting: 241it [02:59,  1.27it/s]Extractor Predicting: 242it [03:00,  1.29it/s]Extractor Predicting: 243it [03:01,  1.29it/s]Extractor Predicting: 244it [03:01,  1.29it/s]Extractor Predicting: 245it [03:02,  1.29it/s]Extractor Predicting: 246it [03:03,  1.27it/s]Extractor Predicting: 247it [03:04,  1.23it/s]Extractor Predicting: 248it [03:05,  1.25it/s]Extractor Predicting: 249it [03:05,  1.30it/s]Extractor Predicting: 250it [03:06,  1.32it/s]Extractor Predicting: 251it [03:07,  1.30it/s]Extractor Predicting: 252it [03:07,  1.33it/s]Extractor Predicting: 253it [03:08,  1.33it/s]Extractor Predicting: 254it [03:09,  1.30it/s]Extractor Predicting: 255it [03:10,  1.30it/s]Extractor Predicting: 256it [03:11,  1.27it/s]Extractor Predicting: 257it [03:12,  1.18it/s]Extractor Predicting: 258it [03:12,  1.20it/s]Extractor Predicting: 259it [03:13,  1.25it/s]Extractor Predicting: 260it [03:14,  1.27it/s]Extractor Predicting: 261it [03:15,  1.31it/s]Extractor Predicting: 262it [03:15,  1.31it/s]Extractor Predicting: 263it [03:16,  1.31it/s]Extractor Predicting: 264it [03:17,  1.32it/s]Extractor Predicting: 265it [03:18,  1.33it/s]Extractor Predicting: 266it [03:18,  1.33it/s]Extractor Predicting: 267it [03:19,  1.35it/s]Extractor Predicting: 268it [03:20,  1.35it/s]Extractor Predicting: 269it [03:21,  1.34it/s]Extractor Predicting: 270it [03:21,  1.31it/s]Extractor Predicting: 271it [03:22,  1.34it/s]Extractor Predicting: 272it [03:23,  1.34it/s]Extractor Predicting: 273it [03:24,  1.37it/s]Extractor Predicting: 274it [03:24,  1.37it/s]Extractor Predicting: 275it [03:25,  1.38it/s]Extractor Predicting: 276it [03:26,  1.33it/s]Extractor Predicting: 277it [03:27,  1.33it/s]Extractor Predicting: 278it [03:27,  1.34it/s]Extractor Predicting: 279it [03:28,  1.34it/s]Extractor Predicting: 280it [03:29,  1.33it/s]Extractor Predicting: 281it [03:29,  1.36it/s]Extractor Predicting: 282it [03:30,  1.35it/s]Extractor Predicting: 283it [03:31,  1.31it/s]Extractor Predicting: 284it [03:31,  1.52it/s]Extractor Predicting: 284it [03:31,  1.34it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:59,166 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:59,171 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:59,171 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:59,171 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:59,171 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:05:59,805 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:05:59,806 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:06:00,378 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:06:01,437 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:06:01,437 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:02,819 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:02,821 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:02,821 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:02,821 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:02,821 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:06:03,170 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:06:03,171 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:06:03,472 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:06:03,661 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:06:03,661 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5521472392638037,
  "recall": 0.09261981770067627,
  "score": 0.15863024046330101,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1045
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1145, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.35it/s]Extractor Predicting: 2it [00:01,  1.20it/s]Extractor Predicting: 3it [00:02,  1.22it/s]Extractor Predicting: 4it [00:03,  1.24it/s]Extractor Predicting: 5it [00:03,  1.70it/s]Extractor Predicting: 5it [00:03,  1.46it/s]
[INFO|configuration_utils.py:515] 2023-08-28 18:06:08,080 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:06:08,081 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:06:08,090 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:06:08,091 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 18:06:08,104 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:06:14,875 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 18:06:14,875 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 18:06:14,894 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:06:14,894 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:06:14,901 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:06:14,906 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:06:14,907 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:06:14,907 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:06:14,907 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:06:14,907 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:06:14,907 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6,
  "recall": 0.015151515151515152,
  "score": 0.029556650246305417,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 18:06:15,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:15,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:16,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:17,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:17,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:18,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:19,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:20,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:21,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:21,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:22,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:23,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:23,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:24,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:25,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:26,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:27,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:27,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:28,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:29,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:29,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:30,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:31,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:32,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:32,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:33,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:34,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:35,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:20<04:50, 20.75s/it][WARNING|generation_utils.py:914] 2023-08-28 18:06:35,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:36,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:37,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:37,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:38,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:39,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:40,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:40,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:41,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:42,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:43,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:43,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:44,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:45,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:46,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:47,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:47,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:48,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:49,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:49,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:50,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:36<03:49, 17.67s/it][WARNING|generation_utils.py:914] 2023-08-28 18:06:51,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:52,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:53,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:53,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:54,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:55,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:56,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:56,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:57,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:58,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:59,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:00,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:00,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:01,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:02,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:02,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:03,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:04,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:05,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:06,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:06,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:07,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:53<03:29, 17.46s/it][WARNING|generation_utils.py:914] 2023-08-28 18:07:08,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:09,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:10,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:11,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:11,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:12,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:13,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:14,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:15,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:15,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:16,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:17,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:18,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:19,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:20,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:20,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:21,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:22,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:22,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:23,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:24,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:25,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:25,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:26,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:27,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:12<03:20, 18.20s/it][WARNING|generation_utils.py:914] 2023-08-28 18:07:28,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:28,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:29,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:30,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:30,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:31,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:32,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:32,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:33,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:34,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:34,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:35,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:36,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:36,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:37,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:38,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:38,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:39,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:40,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:41,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:41,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:42,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:27<02:50, 17.04s/it][WARNING|generation_utils.py:914] 2023-08-28 18:07:43,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:43,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:44,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:45,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:45,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:46,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:47,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:47,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:48,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:49,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:49,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:50,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:51,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:51,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:52,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:53,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:54,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:54,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:55,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:56,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:56,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:57,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:58,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:58,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:59,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:45<02:34, 17.12s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:00,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:01,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:01,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:02,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:03,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:04,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:04,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:05,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:06,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:06,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:07,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:08,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:08,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:09,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:10,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:10,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:11,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:12,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:12,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:13,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:14,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:15,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:15,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:16,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:17,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:03<02:19, 17.46s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:18,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:19,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:20,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:21,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:22,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:23,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:24,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:25,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:25,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:26,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:27,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:28,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:29,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:29,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:30,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:32,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:32,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:33,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:34,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:35,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:36,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:37,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:37,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:38,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:39,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:40,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:26<02:14, 19.23s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:41,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:42,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:43,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:43,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:44,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:45,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:46,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:46,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:47,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:48,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:49,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:49,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:50,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:51,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:52,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:53,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:54,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:54,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:55,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:56,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:57,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:58,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:59,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:44<01:53, 19.00s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:59,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:00,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:01,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:02,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:03,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:03,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:04,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:05,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:06,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:07,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:07,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:08,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:09,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:10,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:10,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:11,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:12,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:13,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:14,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:14,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:15,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:16,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:17,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [03:02<01:33, 18.66s/it][WARNING|generation_utils.py:914] 2023-08-28 18:09:17,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:18,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:19,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:19,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:20,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:21,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:22,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:22,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:23,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:24,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:24,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:25,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:26,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:26,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:27,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:28,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:28,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:29,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:30,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:31,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:31,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:32,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:17<01:10, 17.64s/it][WARNING|generation_utils.py:914] 2023-08-28 18:09:33,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:34,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:34,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:35,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:36,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:37,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:37,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:38,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:39,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:40,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:41,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:42,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:42,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:43,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:44,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:45,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:46,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:47,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:48,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:48,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:49,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:50,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:51,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:52,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:37<00:54, 18.23s/it][WARNING|generation_utils.py:914] 2023-08-28 18:09:52,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:53,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:54,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:55,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:55,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:56,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:57,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:58,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:58,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:59,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:00,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:01,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:01,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:02,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:03,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:04,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:04,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:05,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:06,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:06,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:07,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:08,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:09,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:09,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:55<00:36, 18.06s/it][WARNING|generation_utils.py:914] 2023-08-28 18:10:10,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:11,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:12,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:12,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:13,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:14,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:14,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:15,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:16,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:17,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:17,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:18,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:19,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:20,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:21,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:21,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:22,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:23,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:23,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:25,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:25,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:26,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:27,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:28,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [04:13<00:18, 18.22s/it][WARNING|generation_utils.py:914] 2023-08-28 18:10:29,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:29,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:30,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:31,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:31,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:32,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:33,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:33,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:34,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:35,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:36,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:36,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:37,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:38,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:38,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:39,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:40,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:41,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:41,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:42,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:43,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:28<00:00, 17.19s/it]Generating: 100%|██████████| 15/15 [04:28<00:00, 17.91s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:50,844 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:50,850 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:50,850 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:50,850 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:50,850 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:10:51,464 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:10:51,465 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:10:52,036 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:10:53,155 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:10:53,155 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:56,268 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:56,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:56,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:56,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:56,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:10:57,168 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:10:57,169 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:10:57,866 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:10:58,134 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:10:58,134 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : genre . Context : Later in the year , the band formed the single " The Walking Dead " . Head Entity : The Walking Dead , Tail Entity : horror .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 165, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 229, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 270, 'raw': 384}
{'target': 600, 'success': 291, 'raw': 416}
{'target': 600, 'success': 312, 'raw': 448}
{'target': 600, 'success': 333, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 375, 'raw': 544}
{'target': 600, 'success': 395, 'raw': 576}
{'target': 600, 'success': 418, 'raw': 608}
{'target': 600, 'success': 440, 'raw': 640}
{'target': 600, 'success': 460, 'raw': 672}
{'target': 600, 'success': 486, 'raw': 704}
{'target': 600, 'success': 508, 'raw': 736}
{'target': 600, 'success': 528, 'raw': 768}
{'target': 600, 'success': 549, 'raw': 800}
{'target': 600, 'success': 570, 'raw': 832}
{'target': 600, 'success': 595, 'raw': 864}
{'target': 600, 'success': 619, 'raw': 896}
{'prompt': 'Relation : genre .', 'success_rate': 0.6908482142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8988095238095238, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8849431818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : participant in . Context : Later in the year ( October 1887 ) , a campaign to elect a new governor was undertaken by the New York Republican Party . Head Entity : election , Tail Entity : New York Republican Party .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 220, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 321, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 419, 'raw': 544}
{'target': 600, 'success': 443, 'raw': 576}
{'target': 600, 'success': 473, 'raw': 608}
{'target': 600, 'success': 497, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 544, 'raw': 704}
{'target': 600, 'success': 570, 'raw': 736}
{'target': 600, 'success': 594, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : participant in .', 'success_rate': 0.7725, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8579545454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 369, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 445, 'raw': 576}
{'target': 600, 'success': 470, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : competition class .', 'success_rate': 0.76375, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 300, 'raw': 384}
{'target': 600, 'success': 330, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 481, 'raw': 608}
{'target': 600, 'success': 502, 'raw': 640}
{'target': 600, 'success': 527, 'raw': 672}
{'target': 600, 'success': 552, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 597, 'raw': 768}
{'target': 600, 'success': 616, 'raw': 800}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.77, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('2012 Winter Olympics', 'country of citizenship', '', 'He has competed for the 2012 Winter Olympics in the snowboarding division .')", 'too many values to unpack (expected 2)'}}
['Relation : father . Context : Later in life , he married his second wife , a young princess of the family at the age of eleven , Cecilia De Rota , whom he had succeeded to the throne , and remarried shortly afterwards . Head Entity : Cecilia De Rota , Tail Entity : Cecilia de Rota .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 288, 'raw': 384}
{'target': 600, 'success': 310, 'raw': 416}
{'target': 600, 'success': 334, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 378, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 420, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 513, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 561, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 612, 'raw': 832}
{'prompt': 'Relation : father .', 'success_rate': 0.7355769230769231, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 628, 'raw': 704}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8920454545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 573, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.8138020833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8098958333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupation . Context : Later in the year ( 1143–1231 ) he married Brigadier John Baskerville , daughter of the Marquis of Warwick , the King of England . Head Entity : Robert Baskerville , Tail Entity : Warwick .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 300, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 536, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 615, 'raw': 768}
{'prompt': 'Relation : occupation .', 'success_rate': 0.80078125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '("Don\'t Look", \'record label\', \'\', \'The original album was composed from his own lyrics " Don\\\'t Look ( Let Me Kiss You ) " .\')', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/synthetic/2_ext.jsonl'}}
estimate vocab size: 11689
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11789, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.29it/s]Extractor Estimating: 2it [00:01,  1.37it/s]Extractor Estimating: 3it [00:02,  1.41it/s]Extractor Estimating: 4it [00:02,  1.39it/s]Extractor Estimating: 5it [00:03,  1.36it/s]Extractor Estimating: 6it [00:04,  1.37it/s]Extractor Estimating: 7it [00:05,  1.34it/s]Extractor Estimating: 8it [00:05,  1.36it/s]Extractor Estimating: 9it [00:06,  1.30it/s]Extractor Estimating: 10it [00:07,  1.35it/s]Extractor Estimating: 11it [00:08,  1.35it/s]Extractor Estimating: 12it [00:08,  1.37it/s]Extractor Estimating: 13it [00:09,  1.35it/s]Extractor Estimating: 14it [00:10,  1.34it/s]Extractor Estimating: 15it [00:11,  1.26it/s]Extractor Estimating: 16it [00:11,  1.32it/s]Extractor Estimating: 17it [00:12,  1.33it/s]Extractor Estimating: 18it [00:13,  1.33it/s]Extractor Estimating: 19it [00:14,  1.31it/s]Extractor Estimating: 20it [00:15,  1.30it/s]Extractor Estimating: 21it [00:15,  1.29it/s]Extractor Estimating: 22it [00:16,  1.31it/s]Extractor Estimating: 23it [00:17,  1.32it/s]Extractor Estimating: 24it [00:18,  1.28it/s]Extractor Estimating: 25it [00:19,  1.19it/s]Extractor Estimating: 26it [00:19,  1.29it/s]Extractor Estimating: 27it [00:20,  1.38it/s]Extractor Estimating: 28it [00:20,  1.45it/s]Extractor Estimating: 29it [00:21,  1.50it/s]Extractor Estimating: 30it [00:22,  1.43it/s]Extractor Estimating: 31it [00:22,  1.48it/s]Extractor Estimating: 32it [00:23,  1.46it/s]Extractor Estimating: 33it [00:24,  1.45it/s]Extractor Estimating: 34it [00:24,  1.49it/s]Extractor Estimating: 35it [00:25,  1.49it/s]Extractor Estimating: 36it [00:26,  1.42it/s]Extractor Estimating: 37it [00:27,  1.44it/s]Extractor Estimating: 38it [00:27,  1.52it/s]Extractor Estimating: 39it [00:28,  1.49it/s]Extractor Estimating: 40it [00:29,  1.47it/s]Extractor Estimating: 41it [00:29,  1.52it/s]Extractor Estimating: 42it [00:30,  1.51it/s]Extractor Estimating: 43it [00:31,  1.51it/s]Extractor Estimating: 44it [00:31,  1.56it/s]Extractor Estimating: 45it [00:32,  1.52it/s]Extractor Estimating: 46it [00:32,  1.53it/s]Extractor Estimating: 47it [00:33,  1.56it/s]Extractor Estimating: 48it [00:34,  1.53it/s]Extractor Estimating: 49it [00:34,  1.54it/s]Extractor Estimating: 50it [00:35,  1.51it/s]Extractor Estimating: 51it [00:36,  1.40it/s]Extractor Estimating: 52it [00:37,  1.41it/s]Extractor Estimating: 53it [00:37,  1.40it/s]Extractor Estimating: 54it [00:38,  1.30it/s]Extractor Estimating: 55it [00:39,  1.33it/s]Extractor Estimating: 56it [00:40,  1.38it/s]Extractor Estimating: 57it [00:40,  1.40it/s]Extractor Estimating: 58it [00:41,  1.40it/s]Extractor Estimating: 59it [00:42,  1.40it/s]Extractor Estimating: 60it [00:42,  1.37it/s]Extractor Estimating: 61it [00:43,  1.43it/s]Extractor Estimating: 62it [00:44,  1.44it/s]Extractor Estimating: 63it [00:45,  1.37it/s]Extractor Estimating: 64it [00:45,  1.38it/s]Extractor Estimating: 65it [00:46,  1.36it/s]Extractor Estimating: 66it [00:47,  1.35it/s]Extractor Estimating: 67it [00:48,  1.39it/s]Extractor Estimating: 68it [00:48,  1.38it/s]Extractor Estimating: 69it [00:49,  1.39it/s]Extractor Estimating: 70it [00:50,  1.38it/s]Extractor Estimating: 71it [00:50,  1.45it/s]Extractor Estimating: 72it [00:51,  1.44it/s]Extractor Estimating: 73it [00:52,  1.43it/s]Extractor Estimating: 74it [00:52,  1.39it/s]Extractor Estimating: 75it [00:53,  1.37it/s]Extractor Estimating: 76it [00:54,  1.31it/s]Extractor Estimating: 77it [00:55,  1.32it/s]Extractor Estimating: 78it [00:55,  1.37it/s]Extractor Estimating: 79it [00:56,  1.40it/s]Extractor Estimating: 80it [00:57,  1.40it/s]Extractor Estimating: 81it [00:58,  1.35it/s]Extractor Estimating: 82it [00:58,  1.37it/s]Extractor Estimating: 83it [00:59,  1.41it/s]Extractor Estimating: 84it [01:00,  1.34it/s]Extractor Estimating: 85it [01:01,  1.32it/s]Extractor Estimating: 86it [01:01,  1.33it/s]Extractor Estimating: 87it [01:02,  1.32it/s]Extractor Estimating: 88it [01:03,  1.33it/s]Extractor Estimating: 89it [01:04,  1.36it/s]Extractor Estimating: 90it [01:04,  1.39it/s]Extractor Estimating: 91it [01:05,  1.36it/s]Extractor Estimating: 92it [01:06,  1.35it/s]Extractor Estimating: 93it [01:07,  1.34it/s]Extractor Estimating: 94it [01:07,  1.39it/s]Extractor Estimating: 95it [01:08,  1.43it/s]Extractor Estimating: 96it [01:09,  1.39it/s]Extractor Estimating: 97it [01:09,  1.38it/s]Extractor Estimating: 98it [01:10,  1.34it/s]Extractor Estimating: 99it [01:11,  1.40it/s]Extractor Estimating: 100it [01:12,  1.38it/s]Extractor Estimating: 101it [01:12,  1.42it/s]Extractor Estimating: 102it [01:13,  1.40it/s]Extractor Estimating: 103it [01:14,  1.47it/s]Extractor Estimating: 104it [01:14,  1.50it/s]Extractor Estimating: 105it [01:15,  1.52it/s]Extractor Estimating: 106it [01:15,  1.53it/s]Extractor Estimating: 107it [01:16,  1.53it/s]Extractor Estimating: 108it [01:17,  1.57it/s]Extractor Estimating: 109it [01:17,  1.61it/s]Extractor Estimating: 110it [01:18,  1.61it/s]Extractor Estimating: 111it [01:19,  1.65it/s]Extractor Estimating: 112it [01:19,  1.58it/s]Extractor Estimating: 113it [01:20,  1.47it/s]Extractor Estimating: 114it [01:21,  1.51it/s]Extractor Estimating: 115it [01:21,  1.54it/s]Extractor Estimating: 116it [01:22,  1.55it/s]Extractor Estimating: 117it [01:22,  1.57it/s]Extractor Estimating: 118it [01:23,  1.63it/s]Extractor Estimating: 119it [01:24,  1.64it/s]Extractor Estimating: 120it [01:24,  1.66it/s]Extractor Estimating: 121it [01:25,  1.64it/s]Extractor Estimating: 122it [01:26,  1.55it/s]Extractor Estimating: 123it [01:26,  1.60it/s]Extractor Estimating: 124it [01:27,  1.62it/s]Extractor Estimating: 125it [01:27,  1.64it/s]Extractor Estimating: 126it [01:28,  1.64it/s]Extractor Estimating: 127it [01:29,  1.67it/s]Extractor Estimating: 128it [01:29,  1.62it/s]Extractor Estimating: 129it [01:30,  1.63it/s]Extractor Estimating: 130it [01:30,  1.58it/s]Extractor Estimating: 131it [01:31,  1.57it/s]Extractor Estimating: 132it [01:32,  1.59it/s]Extractor Estimating: 133it [01:32,  1.61it/s]Extractor Estimating: 134it [01:33,  1.58it/s]Extractor Estimating: 135it [01:34,  1.47it/s]Extractor Estimating: 136it [01:34,  1.53it/s]Extractor Estimating: 137it [01:35,  1.53it/s]Extractor Estimating: 138it [01:36,  1.52it/s]Extractor Estimating: 139it [01:36,  1.53it/s]Extractor Estimating: 140it [01:37,  1.49it/s]Extractor Estimating: 141it [01:38,  1.52it/s]Extractor Estimating: 142it [01:38,  1.59it/s]Extractor Estimating: 143it [01:39,  1.61it/s]Extractor Estimating: 144it [01:39,  1.60it/s]Extractor Estimating: 145it [01:40,  1.55it/s]Extractor Estimating: 146it [01:41,  1.49it/s]Extractor Estimating: 147it [01:42,  1.52it/s]Extractor Estimating: 148it [01:42,  1.52it/s]Extractor Estimating: 149it [01:43,  1.54it/s]Extractor Estimating: 150it [01:44,  1.49it/s]Extractor Estimating: 151it [01:44,  1.45it/s]Extractor Estimating: 152it [01:45,  1.50it/s]Extractor Estimating: 153it [01:46,  1.51it/s]Extractor Estimating: 154it [01:46,  1.51it/s]Extractor Estimating: 155it [01:47,  1.47it/s]Extractor Estimating: 156it [01:48,  1.50it/s]Extractor Estimating: 157it [01:48,  1.53it/s]Extractor Estimating: 158it [01:49,  1.50it/s]Extractor Estimating: 159it [01:50,  1.49it/s]Extractor Estimating: 160it [01:50,  1.46it/s]Extractor Estimating: 161it [01:51,  1.52it/s]Extractor Estimating: 162it [01:52,  1.49it/s]Extractor Estimating: 163it [01:52,  1.48it/s]Extractor Estimating: 164it [01:53,  1.47it/s]Extractor Estimating: 165it [01:54,  1.50it/s]Extractor Estimating: 166it [01:54,  1.50it/s]Extractor Estimating: 167it [01:55,  1.53it/s]Extractor Estimating: 168it [01:55,  1.55it/s]Extractor Estimating: 169it [01:56,  1.44it/s]Extractor Estimating: 170it [01:57,  1.52it/s]Extractor Estimating: 171it [01:58,  1.51it/s]Extractor Estimating: 172it [01:58,  1.49it/s]Extractor Estimating: 173it [01:59,  1.49it/s]Extractor Estimating: 174it [02:00,  1.49it/s]Extractor Estimating: 175it [02:00,  1.49it/s]Extractor Estimating: 176it [02:01,  1.51it/s]Extractor Estimating: 177it [02:02,  1.47it/s]Extractor Estimating: 178it [02:02,  1.39it/s]Extractor Estimating: 179it [02:03,  1.41it/s]Extractor Estimating: 180it [02:04,  1.40it/s]Extractor Estimating: 181it [02:05,  1.36it/s]Extractor Estimating: 182it [02:05,  1.38it/s]Extractor Estimating: 183it [02:06,  1.41it/s]Extractor Estimating: 184it [02:07,  1.37it/s]Extractor Estimating: 185it [02:08,  1.36it/s]Extractor Estimating: 186it [02:08,  1.38it/s]Extractor Estimating: 187it [02:09,  1.44it/s]Extractor Estimating: 188it [02:10,  1.42it/s]Extractor Estimating: 189it [02:10,  1.43it/s]Extractor Estimating: 190it [02:11,  1.23it/s]Extractor Estimating: 191it [02:12,  1.23it/s]Extractor Estimating: 192it [02:13,  1.27it/s]Extractor Estimating: 193it [02:14,  1.26it/s]Extractor Estimating: 194it [02:14,  1.28it/s]Extractor Estimating: 195it [02:15,  1.36it/s]Extractor Estimating: 196it [02:16,  1.39it/s]Extractor Estimating: 197it [02:16,  1.40it/s]Extractor Estimating: 198it [02:17,  1.30it/s]Extractor Estimating: 199it [02:18,  1.29it/s]Extractor Estimating: 200it [02:19,  1.28it/s]Extractor Estimating: 201it [02:20,  1.32it/s]Extractor Estimating: 202it [02:20,  1.30it/s]Extractor Estimating: 203it [02:21,  1.34it/s]Extractor Estimating: 204it [02:22,  1.35it/s]Extractor Estimating: 205it [02:22,  1.40it/s]Extractor Estimating: 206it [02:23,  1.42it/s]Extractor Estimating: 207it [02:24,  1.27it/s]Extractor Estimating: 208it [02:25,  1.30it/s]Extractor Estimating: 209it [02:26,  1.32it/s]Extractor Estimating: 210it [02:26,  1.34it/s]Extractor Estimating: 211it [02:27,  1.36it/s]Extractor Estimating: 212it [02:28,  1.38it/s]Extractor Estimating: 213it [02:29,  1.30it/s]Extractor Estimating: 214it [02:29,  1.31it/s]Extractor Estimating: 215it [02:30,  1.26it/s]Extractor Estimating: 216it [02:31,  1.30it/s]Extractor Estimating: 217it [02:32,  1.34it/s]Extractor Estimating: 218it [02:32,  1.34it/s]Extractor Estimating: 219it [02:33,  1.36it/s]Extractor Estimating: 220it [02:34,  1.32it/s]Extractor Estimating: 221it [02:35,  1.35it/s]Extractor Estimating: 222it [02:35,  1.32it/s]Extractor Estimating: 223it [02:36,  1.26it/s]Extractor Estimating: 224it [02:37,  1.28it/s]Extractor Estimating: 225it [02:38,  1.33it/s]Extractor Estimating: 226it [02:38,  1.35it/s]Extractor Estimating: 227it [02:39,  1.41it/s]Extractor Estimating: 228it [02:40,  1.40it/s]Extractor Estimating: 229it [02:40,  1.42it/s]Extractor Estimating: 230it [02:41,  1.38it/s]Extractor Estimating: 231it [02:42,  1.38it/s]Extractor Estimating: 232it [02:43,  1.37it/s]Extractor Estimating: 233it [02:43,  1.40it/s]Extractor Estimating: 234it [02:44,  1.44it/s]Extractor Estimating: 235it [02:45,  1.46it/s]Extractor Estimating: 236it [02:45,  1.45it/s]Extractor Estimating: 237it [02:46,  1.47it/s]Extractor Estimating: 238it [02:47,  1.43it/s]Extractor Estimating: 239it [02:48,  1.39it/s]Extractor Estimating: 240it [02:48,  1.42it/s]Extractor Estimating: 241it [02:49,  1.41it/s]Extractor Estimating: 242it [02:50,  1.38it/s]Extractor Estimating: 243it [02:50,  1.38it/s]Extractor Estimating: 244it [02:51,  1.41it/s]Extractor Estimating: 245it [02:52,  1.40it/s]Extractor Estimating: 246it [02:53,  1.33it/s]Extractor Estimating: 247it [02:53,  1.35it/s]Extractor Estimating: 248it [02:54,  1.36it/s]Extractor Estimating: 249it [02:55,  1.40it/s]Extractor Estimating: 250it [02:56,  1.38it/s]Extractor Estimating: 251it [02:56,  1.41it/s]Extractor Estimating: 252it [02:57,  1.39it/s]Extractor Estimating: 253it [02:58,  1.39it/s]Extractor Estimating: 254it [02:58,  1.41it/s]Extractor Estimating: 255it [02:59,  1.44it/s]Extractor Estimating: 256it [03:00,  1.41it/s]Extractor Estimating: 257it [03:00,  1.42it/s]Extractor Estimating: 258it [03:01,  1.40it/s]Extractor Estimating: 259it [03:02,  1.43it/s]Extractor Estimating: 260it [03:03,  1.42it/s]Extractor Estimating: 261it [03:03,  1.46it/s]Extractor Estimating: 262it [03:04,  1.46it/s]Extractor Estimating: 263it [03:05,  1.44it/s]Extractor Estimating: 264it [03:05,  1.41it/s]Extractor Estimating: 265it [03:06,  1.42it/s]Extractor Estimating: 266it [03:07,  1.40it/s]Extractor Estimating: 267it [03:08,  1.31it/s]Extractor Estimating: 268it [03:08,  1.32it/s]Extractor Estimating: 269it [03:09,  1.37it/s]Extractor Estimating: 270it [03:10,  1.42it/s]Extractor Estimating: 271it [03:10,  1.41it/s]Extractor Estimating: 272it [03:11,  1.43it/s]Extractor Estimating: 273it [03:12,  1.42it/s]Extractor Estimating: 274it [03:13,  1.35it/s]Extractor Estimating: 275it [03:13,  1.43it/s]Extractor Estimating: 276it [03:14,  1.47it/s]Extractor Estimating: 277it [03:15,  1.48it/s]Extractor Estimating: 278it [03:15,  1.50it/s]Extractor Estimating: 279it [03:16,  1.48it/s]Extractor Estimating: 280it [03:17,  1.46it/s]Extractor Estimating: 281it [03:17,  1.47it/s]Extractor Estimating: 282it [03:18,  1.56it/s]Extractor Estimating: 283it [03:18,  1.58it/s]Extractor Estimating: 284it [03:19,  1.55it/s]Extractor Estimating: 285it [03:20,  1.54it/s]Extractor Estimating: 286it [03:20,  1.53it/s]Extractor Estimating: 287it [03:21,  1.51it/s]Extractor Estimating: 288it [03:22,  1.49it/s]Extractor Estimating: 289it [03:22,  1.48it/s]Extractor Estimating: 290it [03:23,  1.46it/s]Extractor Estimating: 291it [03:24,  1.47it/s]Extractor Estimating: 292it [03:24,  1.49it/s]Extractor Estimating: 293it [03:25,  1.54it/s]Extractor Estimating: 294it [03:26,  1.51it/s]Extractor Estimating: 295it [03:27,  1.48it/s]Extractor Estimating: 296it [03:27,  1.37it/s]Extractor Estimating: 297it [03:28,  1.38it/s]Extractor Estimating: 298it [03:29,  1.43it/s]Extractor Estimating: 299it [03:29,  1.44it/s]Extractor Estimating: 300it [03:30,  1.51it/s]Extractor Estimating: 301it [03:31,  1.47it/s]Extractor Estimating: 302it [03:31,  1.43it/s]Extractor Estimating: 303it [03:32,  1.42it/s]Extractor Estimating: 304it [03:33,  1.42it/s]Extractor Estimating: 305it [03:34,  1.44it/s]Extractor Estimating: 306it [03:34,  1.38it/s]Extractor Estimating: 307it [03:35,  1.39it/s]Extractor Estimating: 308it [03:36,  1.42it/s]Extractor Estimating: 309it [03:36,  1.39it/s]Extractor Estimating: 310it [03:37,  1.40it/s]Extractor Estimating: 311it [03:38,  1.44it/s]Extractor Estimating: 312it [03:39,  1.40it/s]Extractor Estimating: 313it [03:39,  1.42it/s]Extractor Estimating: 314it [03:40,  1.37it/s]Extractor Estimating: 315it [03:41,  1.35it/s]Extractor Estimating: 316it [03:42,  1.38it/s]Extractor Estimating: 317it [03:42,  1.37it/s]Extractor Estimating: 318it [03:43,  1.38it/s]Extractor Estimating: 319it [03:44,  1.39it/s]Extractor Estimating: 320it [03:44,  1.36it/s]Extractor Estimating: 321it [03:45,  1.39it/s]Extractor Estimating: 322it [03:46,  1.37it/s]Extractor Estimating: 323it [03:47,  1.40it/s]Extractor Estimating: 324it [03:47,  1.40it/s]Extractor Estimating: 325it [03:48,  1.41it/s]Extractor Estimating: 326it [03:49,  1.43it/s]Extractor Estimating: 327it [03:49,  1.37it/s]Extractor Estimating: 328it [03:50,  1.39it/s]Extractor Estimating: 329it [03:51,  1.42it/s]Extractor Estimating: 330it [03:51,  1.44it/s]Extractor Estimating: 331it [03:52,  1.42it/s]Extractor Estimating: 332it [03:53,  1.42it/s]Extractor Estimating: 333it [03:54,  1.45it/s]Extractor Estimating: 334it [03:54,  1.38it/s]Extractor Estimating: 335it [03:55,  1.43it/s]Extractor Estimating: 336it [03:56,  1.46it/s]Extractor Estimating: 337it [03:57,  1.36it/s]Extractor Estimating: 338it [03:57,  1.37it/s]Extractor Estimating: 339it [03:58,  1.39it/s]Extractor Estimating: 340it [03:59,  1.36it/s]Extractor Estimating: 341it [04:00,  1.32it/s]Extractor Estimating: 342it [04:00,  1.38it/s]Extractor Estimating: 343it [04:01,  1.40it/s]Extractor Estimating: 344it [04:02,  1.40it/s]Extractor Estimating: 345it [04:02,  1.39it/s]Extractor Estimating: 346it [04:03,  1.40it/s]Extractor Estimating: 347it [04:04,  1.37it/s]Extractor Estimating: 348it [04:04,  1.36it/s]Extractor Estimating: 349it [04:05,  1.37it/s]Extractor Estimating: 350it [04:06,  1.40it/s]Extractor Estimating: 351it [04:07,  1.43it/s]Extractor Estimating: 352it [04:07,  1.46it/s]Extractor Estimating: 353it [04:08,  1.46it/s]Extractor Estimating: 354it [04:09,  1.51it/s]Extractor Estimating: 355it [04:09,  1.56it/s]Extractor Estimating: 356it [04:10,  1.47it/s]Extractor Estimating: 357it [04:11,  1.43it/s]Extractor Estimating: 358it [04:11,  1.44it/s]Extractor Estimating: 359it [04:12,  1.45it/s]Extractor Estimating: 360it [04:13,  1.44it/s]Extractor Estimating: 361it [04:13,  1.43it/s]Extractor Estimating: 362it [04:14,  1.45it/s]Extractor Estimating: 363it [04:15,  1.46it/s]Extractor Estimating: 364it [04:15,  1.48it/s]Extractor Estimating: 365it [04:16,  1.47it/s]Extractor Estimating: 366it [04:17,  1.43it/s]Extractor Estimating: 367it [04:18,  1.25it/s]Extractor Estimating: 368it [04:19,  1.27it/s]Extractor Estimating: 369it [04:19,  1.30it/s]Extractor Estimating: 370it [04:20,  1.31it/s]Extractor Estimating: 371it [04:21,  1.34it/s]Extractor Estimating: 372it [04:22,  1.37it/s]Extractor Estimating: 373it [04:22,  1.38it/s]Extractor Estimating: 374it [04:23,  1.42it/s]Extractor Estimating: 375it [04:24,  1.46it/s]Extractor Estimating: 375it [04:24,  1.42it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:36,192 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:36,200 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:36,200 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:36,200 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:36,200 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:15:36,928 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:15:36,929 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:15:37,512 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:15:38,570 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:15:38,570 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:41,487 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:41,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:41,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:41,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:41,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:15:42,139 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:15:42,140 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:15:42,720 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:15:42,902 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:15:42,902 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 20:43:34,901 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 20:43:34,915 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7491 mean pseudo reward: 0.9224641854877347
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
train vocab size: 22300
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22400, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22400, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.134, loss:690.4347
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.141, loss:655.0473
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.141, loss:669.9330
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.124, loss:640.6813
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.142, loss:684.6854
>> valid entity prec:0.5567, rec:0.5696, f1:0.5631
>> valid relation prec:0.2702, rec:0.0996, f1:0.1456
>> valid relation with NER prec:0.2702, rec:0.0996, f1:0.1456
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.604, loss:654.0675
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.124, loss:617.0664
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.148, loss:664.6126
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.137, loss:688.8232
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.129, loss:669.8276
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5579, rec:0.5854, f1:0.5713
>> valid relation prec:0.2150, rec:0.0904, f1:0.1273
>> valid relation with NER prec:0.2150, rec:0.0904, f1:0.1273
new max entity f1 on valid!
g_step 1100, step 161, avg_time 2.595, loss:654.4397
g_step 1200, step 261, avg_time 1.137, loss:666.4869
g_step 1300, step 48, avg_time 1.130, loss:640.8608
g_step 1400, step 148, avg_time 1.137, loss:628.6793
g_step 1500, step 248, avg_time 1.146, loss:636.9011
>> valid entity prec:0.6179, rec:0.5045, f1:0.5555
>> valid relation prec:0.2452, rec:0.0586, f1:0.0945
>> valid relation with NER prec:0.2452, rec:0.0586, f1:0.0945
g_step 1600, step 35, avg_time 2.579, loss:625.8907
g_step 1700, step 135, avg_time 1.136, loss:579.7196
g_step 1800, step 235, avg_time 1.136, loss:615.3141
g_step 1900, step 22, avg_time 1.144, loss:618.1411
g_step 2000, step 122, avg_time 1.126, loss:563.8554
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5424, rec:0.5907, f1:0.5655
>> valid relation prec:0.2187, rec:0.0867, f1:0.1242
>> valid relation with NER prec:0.2187, rec:0.0867, f1:0.1242
g_step 2100, step 222, avg_time 2.615, loss:584.5445
g_step 2200, step 9, avg_time 1.121, loss:568.4739
g_step 2300, step 109, avg_time 1.143, loss:535.6292
g_step 2400, step 209, avg_time 1.128, loss:528.2731
g_step 2500, step 309, avg_time 1.154, loss:584.1604
>> valid entity prec:0.5585, rec:0.5527, f1:0.5556
>> valid relation prec:0.2757, rec:0.0884, f1:0.1339
>> valid relation with NER prec:0.2757, rec:0.0884, f1:0.1339
g_step 2600, step 96, avg_time 2.607, loss:495.1763
g_step 2700, step 196, avg_time 1.143, loss:528.4605
g_step 2800, step 296, avg_time 1.139, loss:551.9667
g_step 2900, step 83, avg_time 1.138, loss:484.9892
g_step 3000, step 183, avg_time 1.136, loss:507.2083
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5776, rec:0.5360, f1:0.5560
>> valid relation prec:0.1953, rec:0.0841, f1:0.1176
>> valid relation with NER prec:0.1953, rec:0.0841, f1:0.1176
g_step 3100, step 283, avg_time 2.587, loss:513.8413
g_step 3200, step 70, avg_time 1.127, loss:478.5155
g_step 3300, step 170, avg_time 1.142, loss:474.2820
g_step 3400, step 270, avg_time 1.153, loss:482.9866
g_step 3500, step 57, avg_time 1.122, loss:465.9265
>> valid entity prec:0.5479, rec:0.5453, f1:0.5466
>> valid relation prec:0.2069, rec:0.0916, f1:0.1269
>> valid relation with NER prec:0.2069, rec:0.0916, f1:0.1269
g_step 3600, step 157, avg_time 2.602, loss:448.4594
g_step 3700, step 257, avg_time 1.137, loss:482.6000
g_step 3800, step 44, avg_time 1.131, loss:454.1277
g_step 3900, step 144, avg_time 1.139, loss:450.8555
g_step 4000, step 244, avg_time 1.146, loss:462.8837
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5614, rec:0.5363, f1:0.5485
>> valid relation prec:0.1981, rec:0.0847, f1:0.1186
>> valid relation with NER prec:0.1981, rec:0.0847, f1:0.1186
g_step 4100, step 31, avg_time 2.592, loss:432.8211
g_step 4200, step 131, avg_time 1.146, loss:422.4969
g_step 4300, step 231, avg_time 1.129, loss:438.5905
g_step 4400, step 18, avg_time 1.133, loss:423.7967
g_step 4500, step 118, avg_time 1.142, loss:408.6996
>> valid entity prec:0.5258, rec:0.5744, f1:0.5490
>> valid relation prec:0.1764, rec:0.0939, f1:0.1225
>> valid relation with NER prec:0.1764, rec:0.0939, f1:0.1225
g_step 4600, step 218, avg_time 2.613, loss:428.6962
g_step 4700, step 5, avg_time 1.127, loss:430.6627
g_step 4800, step 105, avg_time 1.131, loss:384.3853
g_step 4900, step 205, avg_time 1.121, loss:405.7873
g_step 5000, step 305, avg_time 1.158, loss:413.8080
learning rate was adjusted to 0.0008
>> valid entity prec:0.5558, rec:0.4889, f1:0.5202
>> valid relation prec:0.2003, rec:0.0775, f1:0.1118
>> valid relation with NER prec:0.2003, rec:0.0775, f1:0.1118
g_step 5100, step 92, avg_time 2.572, loss:383.8347
g_step 5200, step 192, avg_time 1.136, loss:378.7620
g_step 5300, step 292, avg_time 1.148, loss:416.9427
g_step 5400, step 79, avg_time 1.131, loss:349.9742
g_step 5500, step 179, avg_time 1.132, loss:378.9666
>> valid entity prec:0.5619, rec:0.5198, f1:0.5400
>> valid relation prec:0.1776, rec:0.0827, f1:0.1128
>> valid relation with NER prec:0.1776, rec:0.0827, f1:0.1128
g_step 5600, step 279, avg_time 2.602, loss:385.1305
g_step 5700, step 66, avg_time 1.139, loss:360.4597
g_step 5800, step 166, avg_time 1.127, loss:356.7857
g_step 5900, step 266, avg_time 1.141, loss:374.6344
g_step 6000, step 53, avg_time 1.128, loss:348.5383
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5236, rec:0.5280, f1:0.5258
>> valid relation prec:0.1508, rec:0.0763, f1:0.1014
>> valid relation with NER prec:0.1508, rec:0.0763, f1:0.1014
g_step 6100, step 153, avg_time 2.590, loss:357.4398
g_step 6200, step 253, avg_time 1.155, loss:367.3365
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 20:43:34 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 20:43:34 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_20-43-34_ctolab11.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 20:43:35 - WARNING - datasets.builder -   Using custom data configuration default-de25448de13237d5
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-de25448de13237d5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 20:43:39,548 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:43:40,215 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 20:43:40,216 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:43:40,217 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 20:43:40,779 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:43:40,846 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:43:40,846 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:43:40,846 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:43:40,846 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:43:40,846 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:43:40,846 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 20:43:41,003 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 20:43:44,044 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 20:43:44,054 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-de25448de13237d5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.38ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.19ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.51ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.64ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.69ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.73ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.76ba/s]100%|██████████| 8/8 [00:01<00:00,  5.66ba/s]100%|██████████| 8/8 [00:01<00:00,  4.90ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.93ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.25ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.38ba/s]100%|██████████| 4/4 [00:00<00:00,  5.49ba/s]100%|██████████| 4/4 [00:00<00:00,  4.95ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  7.78ba/s] 25%|██▌       | 2/8 [00:00<00:00,  8.75ba/s] 50%|█████     | 4/8 [00:00<00:00, 10.08ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  7.41ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  8.87ba/s]100%|██████████| 8/8 [00:00<00:00,  9.43ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  6.45ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.41ba/s]100%|██████████| 4/4 [00:00<00:00, 10.56ba/s]
[INFO|trainer.py:414] 2023-08-28 20:43:48,540 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 20:43:48,586 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 20:43:48,586 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-28 20:43:48,586 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 20:43:48,586 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 20:43:48,586 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 20:43:48,586 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 20:43:48,586 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:56,  3.30it/s]  0%|          | 2/585 [00:00<02:53,  3.37it/s]  1%|          | 3/585 [00:00<02:51,  3.40it/s]  1%|          | 4/585 [00:01<02:50,  3.41it/s]  1%|          | 5/585 [00:01<02:49,  3.41it/s]  1%|          | 6/585 [00:01<02:49,  3.42it/s]  1%|          | 7/585 [00:02<02:49,  3.41it/s]  1%|▏         | 8/585 [00:02<02:49,  3.41it/s]  2%|▏         | 9/585 [00:02<02:49,  3.39it/s]  2%|▏         | 10/585 [00:02<02:49,  3.38it/s]  2%|▏         | 11/585 [00:03<02:50,  3.37it/s]  2%|▏         | 12/585 [00:03<02:50,  3.36it/s]  2%|▏         | 13/585 [00:03<02:50,  3.35it/s]  2%|▏         | 14/585 [00:04<02:50,  3.35it/s]  3%|▎         | 15/585 [00:04<02:50,  3.35it/s]  3%|▎         | 16/585 [00:04<02:50,  3.35it/s]  3%|▎         | 17/585 [00:05<02:49,  3.35it/s]  3%|▎         | 18/585 [00:05<02:51,  3.30it/s]  3%|▎         | 19/585 [00:05<02:50,  3.32it/s]  3%|▎         | 20/585 [00:05<02:49,  3.33it/s]  4%|▎         | 21/585 [00:06<02:49,  3.34it/s]  4%|▍         | 22/585 [00:06<02:48,  3.34it/s]  4%|▍         | 23/585 [00:06<02:48,  3.34it/s]  4%|▍         | 24/585 [00:07<02:47,  3.34it/s]  4%|▍         | 25/585 [00:07<02:47,  3.34it/s]  4%|▍         | 26/585 [00:07<02:47,  3.35it/s]  5%|▍         | 27/585 [00:08<02:46,  3.35it/s]  5%|▍         | 28/585 [00:08<02:46,  3.34it/s]  5%|▍         | 29/585 [00:08<02:49,  3.28it/s]  5%|▌         | 30/585 [00:08<02:48,  3.30it/s]  5%|▌         | 31/585 [00:09<02:47,  3.31it/s]  5%|▌         | 32/585 [00:09<02:46,  3.33it/s]  6%|▌         | 33/585 [00:09<02:45,  3.33it/s]  6%|▌         | 34/585 [00:10<02:45,  3.34it/s]  6%|▌         | 35/585 [00:10<02:44,  3.35it/s]  6%|▌         | 36/585 [00:10<02:43,  3.35it/s]  6%|▋         | 37/585 [00:11<02:43,  3.35it/s]  6%|▋         | 38/585 [00:11<02:43,  3.35it/s]  7%|▋         | 39/585 [00:11<02:44,  3.32it/s]  7%|▋         | 40/585 [00:11<02:43,  3.33it/s]  7%|▋         | 41/585 [00:12<02:43,  3.34it/s]  7%|▋         | 42/585 [00:12<02:42,  3.33it/s]  7%|▋         | 43/585 [00:12<02:42,  3.34it/s]  8%|▊         | 44/585 [00:13<02:41,  3.34it/s]  8%|▊         | 45/585 [00:13<02:41,  3.35it/s]  8%|▊         | 46/585 [00:13<02:41,  3.35it/s]  8%|▊         | 47/585 [00:14<02:40,  3.34it/s]  8%|▊         | 48/585 [00:14<02:40,  3.34it/s]  8%|▊         | 49/585 [00:14<02:41,  3.33it/s]  9%|▊         | 50/585 [00:14<02:40,  3.33it/s]  9%|▊         | 51/585 [00:15<02:40,  3.34it/s]  9%|▉         | 52/585 [00:15<02:39,  3.34it/s]  9%|▉         | 53/585 [00:15<02:39,  3.34it/s]  9%|▉         | 54/585 [00:16<02:39,  3.34it/s]  9%|▉         | 55/585 [00:16<02:38,  3.34it/s] 10%|▉         | 56/585 [00:16<02:38,  3.34it/s] 10%|▉         | 57/585 [00:17<02:38,  3.33it/s] 10%|▉         | 58/585 [00:17<02:37,  3.34it/s] 10%|█         | 59/585 [00:17<02:37,  3.34it/s] 10%|█         | 60/585 [00:17<02:37,  3.33it/s] 10%|█         | 61/585 [00:18<02:37,  3.33it/s] 11%|█         | 62/585 [00:18<02:36,  3.33it/s] 11%|█         | 63/585 [00:18<02:36,  3.33it/s] 11%|█         | 64/585 [00:19<02:36,  3.33it/s] 11%|█         | 65/585 [00:19<02:35,  3.33it/s] 11%|█▏        | 66/585 [00:19<03:15,  2.66it/s] 11%|█▏        | 67/585 [00:20<03:03,  2.83it/s] 12%|█▏        | 68/585 [00:20<02:54,  2.96it/s] 12%|█▏        | 69/585 [00:20<02:48,  3.07it/s] 12%|█▏        | 70/585 [00:21<02:43,  3.14it/s] 12%|█▏        | 71/585 [00:21<02:40,  3.20it/s] 12%|█▏        | 72/585 [00:21<02:38,  3.24it/s] 12%|█▏        | 73/585 [00:22<02:36,  3.27it/s] 13%|█▎        | 74/585 [00:22<02:35,  3.29it/s] 13%|█▎        | 75/585 [00:22<02:34,  3.30it/s] 13%|█▎        | 76/585 [00:22<02:34,  3.30it/s] 13%|█▎        | 77/585 [00:23<02:33,  3.31it/s] 13%|█▎        | 78/585 [00:23<02:32,  3.32it/s] 14%|█▎        | 79/585 [00:23<02:32,  3.32it/s] 14%|█▎        | 80/585 [00:24<02:31,  3.33it/s] 14%|█▍        | 81/585 [00:24<02:31,  3.33it/s] 14%|█▍        | 82/585 [00:24<02:31,  3.33it/s] 14%|█▍        | 83/585 [00:25<02:30,  3.33it/s] 14%|█▍        | 84/585 [00:25<02:30,  3.33it/s] 15%|█▍        | 85/585 [00:25<02:30,  3.33it/s] 15%|█▍        | 86/585 [00:25<02:30,  3.32it/s] 15%|█▍        | 87/585 [00:26<02:29,  3.33it/s] 15%|█▌        | 88/585 [00:26<02:29,  3.33it/s] 15%|█▌        | 89/585 [00:26<02:28,  3.33it/s] 15%|█▌        | 90/585 [00:27<02:28,  3.33it/s] 16%|█▌        | 91/585 [00:27<02:28,  3.33it/s] 16%|█▌        | 92/585 [00:27<02:27,  3.33it/s] 16%|█▌        | 93/585 [00:28<02:27,  3.33it/s] 16%|█▌        | 94/585 [00:28<02:27,  3.33it/s] 16%|█▌        | 95/585 [00:28<02:27,  3.33it/s] 16%|█▋        | 96/585 [00:29<02:29,  3.27it/s] 17%|█▋        | 97/585 [00:29<02:28,  3.29it/s] 17%|█▋        | 98/585 [00:29<02:27,  3.30it/s] 17%|█▋        | 99/585 [00:29<02:26,  3.31it/s] 17%|█▋        | 100/585 [00:30<02:26,  3.32it/s] 17%|█▋        | 101/585 [00:30<02:24,  3.34it/s] 17%|█▋        | 102/585 [00:30<02:23,  3.36it/s] 18%|█▊        | 103/585 [00:31<02:22,  3.38it/s] 18%|█▊        | 104/585 [00:31<02:21,  3.39it/s] 18%|█▊        | 105/585 [00:31<02:21,  3.40it/s] 18%|█▊        | 106/585 [00:31<02:20,  3.40it/s] 18%|█▊        | 107/585 [00:32<02:23,  3.34it/s] 18%|█▊        | 108/585 [00:32<02:21,  3.37it/s] 19%|█▊        | 109/585 [00:32<02:20,  3.38it/s] 19%|█▉        | 110/585 [00:33<02:21,  3.35it/s] 19%|█▉        | 111/585 [00:33<02:20,  3.37it/s] 19%|█▉        | 112/585 [00:34<03:16,  2.40it/s] 19%|█▉        | 113/585 [00:34<02:58,  2.64it/s] 19%|█▉        | 114/585 [00:34<02:46,  2.83it/s] 20%|█▉        | 115/585 [00:35<02:37,  2.98it/s] 20%|█▉        | 116/585 [00:35<02:33,  3.05it/s] 20%|██        | 117/585 [00:35<02:28,  3.15it/s][INFO|trainer.py:2140] 2023-08-28 20:44:24,283 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:44:24,284 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 20:44:24,284 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.73it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.62it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.63it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.84it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.10it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.62it/s][A
  8%|▊         | 37/436 [00:00<00:09, 44.29it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.06it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.20it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.29it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.28it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.11it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.06it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.20it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 43.95it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.74it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 43.87it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.01it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.19it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.33it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.35it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 43.65it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.90it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 43.90it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.99it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.14it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.22it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.19it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.24it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.21it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.35it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.27it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.20it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.00it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.15it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.27it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.30it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.29it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.26it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.36it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.26it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.24it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.23it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.13it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.19it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.28it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.32it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.32it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 42.85it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 43.33it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.62it/s][A
 60%|██████    | 262/436 [00:05<00:03, 43.73it/s][A
 61%|██████    | 267/436 [00:06<00:03, 43.82it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.08it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.19it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.19it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 43.99it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 43.91it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.21it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.21it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.03it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.20it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.21it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.35it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.23it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.09it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 43.93it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 43.75it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.27it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.27it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.27it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.35it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.42it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.28it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.13it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 43.81it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 43.96it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.06it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.19it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.32it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.32it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.38it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.26it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.14it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.06it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.17it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.17it/s][A 20%|██        | 117/585 [00:45<02:28,  3.15it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:44:34,266 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 20:44:34,389 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:44:39,070 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:44:39,106 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:44:39,125 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:58<53:58,  6.93s/it] 20%|██        | 119/585 [00:58<38:24,  4.95s/it] 21%|██        | 120/585 [00:58<27:31,  3.55s/it] 21%|██        | 121/585 [00:58<19:55,  2.58s/it] 21%|██        | 122/585 [00:59<14:36,  1.89s/it] 21%|██        | 123/585 [00:59<10:53,  1.42s/it] 21%|██        | 124/585 [00:59<08:18,  1.08s/it] 21%|██▏       | 125/585 [01:00<06:29,  1.18it/s] 22%|██▏       | 126/585 [01:00<05:13,  1.47it/s] 22%|██▏       | 127/585 [01:00<04:19,  1.76it/s] 22%|██▏       | 128/585 [01:01<03:42,  2.05it/s] 22%|██▏       | 129/585 [01:01<03:17,  2.31it/s] 22%|██▏       | 130/585 [01:01<02:58,  2.54it/s] 22%|██▏       | 131/585 [01:01<02:45,  2.74it/s] 23%|██▎       | 132/585 [01:02<02:36,  2.90it/s] 23%|██▎       | 133/585 [01:02<02:29,  3.02it/s] 23%|██▎       | 134/585 [01:02<02:25,  3.11it/s] 23%|██▎       | 135/585 [01:03<02:21,  3.17it/s] 23%|██▎       | 136/585 [01:03<02:19,  3.22it/s] 23%|██▎       | 137/585 [01:03<02:17,  3.25it/s] 24%|██▎       | 138/585 [01:04<02:16,  3.28it/s] 24%|██▍       | 139/585 [01:04<02:16,  3.26it/s] 24%|██▍       | 140/585 [01:04<02:15,  3.28it/s] 24%|██▍       | 141/585 [01:04<02:14,  3.30it/s] 24%|██▍       | 142/585 [01:05<02:13,  3.31it/s] 24%|██▍       | 143/585 [01:05<02:13,  3.32it/s] 25%|██▍       | 144/585 [01:05<02:12,  3.32it/s] 25%|██▍       | 145/585 [01:06<02:12,  3.32it/s] 25%|██▍       | 146/585 [01:06<02:11,  3.33it/s] 25%|██▌       | 147/585 [01:06<02:11,  3.33it/s] 25%|██▌       | 148/585 [01:07<02:10,  3.34it/s] 25%|██▌       | 149/585 [01:07<02:14,  3.25it/s] 26%|██▌       | 150/585 [01:07<02:12,  3.27it/s] 26%|██▌       | 151/585 [01:07<02:11,  3.29it/s] 26%|██▌       | 152/585 [01:08<02:11,  3.30it/s] 26%|██▌       | 153/585 [01:08<02:10,  3.31it/s] 26%|██▋       | 154/585 [01:08<02:10,  3.32it/s] 26%|██▋       | 155/585 [01:09<02:09,  3.32it/s] 27%|██▋       | 156/585 [01:09<02:09,  3.32it/s] 27%|██▋       | 157/585 [01:09<02:08,  3.32it/s] 27%|██▋       | 158/585 [01:10<02:08,  3.32it/s] 27%|██▋       | 159/585 [01:10<02:08,  3.32it/s] 27%|██▋       | 160/585 [01:10<02:07,  3.32it/s] 28%|██▊       | 161/585 [01:10<02:07,  3.33it/s] 28%|██▊       | 162/585 [01:11<02:07,  3.33it/s] 28%|██▊       | 163/585 [01:11<02:06,  3.33it/s] 28%|██▊       | 164/585 [01:11<02:06,  3.33it/s] 28%|██▊       | 165/585 [01:12<02:06,  3.33it/s] 28%|██▊       | 166/585 [01:12<02:05,  3.34it/s] 29%|██▊       | 167/585 [01:12<02:05,  3.33it/s] 29%|██▊       | 168/585 [01:13<02:05,  3.33it/s] 29%|██▉       | 169/585 [01:13<02:13,  3.12it/s] 29%|██▉       | 170/585 [01:13<02:10,  3.18it/s] 29%|██▉       | 171/585 [01:14<02:08,  3.22it/s] 29%|██▉       | 172/585 [01:14<02:06,  3.25it/s] 30%|██▉       | 173/585 [01:14<02:05,  3.28it/s] 30%|██▉       | 174/585 [01:14<02:04,  3.29it/s] 30%|██▉       | 175/585 [01:15<02:04,  3.30it/s] 30%|███       | 176/585 [01:15<02:03,  3.31it/s] 30%|███       | 177/585 [01:15<02:03,  3.31it/s] 30%|███       | 178/585 [01:16<02:02,  3.32it/s] 31%|███       | 179/585 [01:16<02:04,  3.27it/s] 31%|███       | 180/585 [01:16<02:03,  3.28it/s] 31%|███       | 181/585 [01:17<02:02,  3.30it/s] 31%|███       | 182/585 [01:17<02:01,  3.31it/s] 31%|███▏      | 183/585 [01:17<02:01,  3.31it/s] 31%|███▏      | 184/585 [01:17<02:01,  3.31it/s] 32%|███▏      | 185/585 [01:18<02:00,  3.32it/s] 32%|███▏      | 186/585 [01:18<02:00,  3.32it/s] 32%|███▏      | 187/585 [01:18<01:59,  3.33it/s] 32%|███▏      | 188/585 [01:19<01:59,  3.33it/s] 32%|███▏      | 189/585 [01:19<01:58,  3.33it/s] 32%|███▏      | 190/585 [01:19<01:58,  3.33it/s] 33%|███▎      | 191/585 [01:20<01:58,  3.33it/s] 33%|███▎      | 192/585 [01:20<01:58,  3.33it/s] 33%|███▎      | 193/585 [01:20<01:57,  3.33it/s] 33%|███▎      | 194/585 [01:20<01:57,  3.33it/s] 33%|███▎      | 195/585 [01:21<02:01,  3.22it/s] 34%|███▎      | 196/585 [01:21<01:59,  3.25it/s] 34%|███▎      | 197/585 [01:21<01:58,  3.27it/s] 34%|███▍      | 198/585 [01:22<01:57,  3.29it/s] 34%|███▍      | 199/585 [01:22<01:56,  3.30it/s] 34%|███▍      | 200/585 [01:22<01:56,  3.31it/s] 34%|███▍      | 201/585 [01:23<01:55,  3.32it/s] 35%|███▍      | 202/585 [01:23<01:55,  3.32it/s] 35%|███▍      | 203/585 [01:23<01:54,  3.32it/s] 35%|███▍      | 204/585 [01:24<01:54,  3.33it/s] 35%|███▌      | 205/585 [01:24<02:11,  2.88it/s] 35%|███▌      | 206/585 [01:24<02:06,  3.00it/s] 35%|███▌      | 207/585 [01:25<02:02,  3.09it/s] 36%|███▌      | 208/585 [01:25<01:59,  3.16it/s] 36%|███▌      | 209/585 [01:25<01:57,  3.21it/s] 36%|███▌      | 210/585 [01:25<01:55,  3.24it/s] 36%|███▌      | 211/585 [01:26<01:54,  3.27it/s] 36%|███▌      | 212/585 [01:26<01:53,  3.29it/s] 36%|███▋      | 213/585 [01:26<01:52,  3.30it/s] 37%|███▋      | 214/585 [01:27<01:52,  3.31it/s] 37%|███▋      | 215/585 [01:27<01:52,  3.28it/s] 37%|███▋      | 216/585 [01:27<01:52,  3.29it/s] 37%|███▋      | 217/585 [01:28<01:51,  3.30it/s] 37%|███▋      | 218/585 [01:28<01:50,  3.31it/s] 37%|███▋      | 219/585 [01:28<01:50,  3.31it/s] 38%|███▊      | 220/585 [01:28<01:50,  3.32it/s] 38%|███▊      | 221/585 [01:29<01:49,  3.32it/s] 38%|███▊      | 222/585 [01:29<01:49,  3.32it/s] 38%|███▊      | 223/585 [01:29<01:48,  3.32it/s] 38%|███▊      | 224/585 [01:30<01:48,  3.33it/s] 38%|███▊      | 225/585 [01:30<01:51,  3.22it/s] 39%|███▊      | 226/585 [01:30<01:50,  3.25it/s] 39%|███▉      | 227/585 [01:31<01:49,  3.27it/s] 39%|███▉      | 228/585 [01:31<01:48,  3.29it/s] 39%|███▉      | 229/585 [01:31<01:47,  3.30it/s] 39%|███▉      | 230/585 [01:32<01:47,  3.31it/s] 39%|███▉      | 231/585 [01:32<01:46,  3.31it/s] 40%|███▉      | 232/585 [01:32<01:46,  3.32it/s] 40%|███▉      | 233/585 [01:32<01:45,  3.32it/s] 40%|████      | 234/585 [01:33<01:47,  3.28it/s][INFO|trainer.py:2140] 2023-08-28 20:45:21,882 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:45:21,882 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 20:45:21,882 >>   Batch size = 8
{'eval_loss': 0.9767536520957947, 'eval_runtime': 9.8942, 'eval_samples_per_second': 352.53, 'eval_steps_per_second': 44.066, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.02it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.11it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.43it/s][A
  5%|▌         | 22/436 [00:00<00:09, 43.11it/s][A
  6%|▌         | 27/436 [00:00<00:10, 39.84it/s][A
  7%|▋         | 32/436 [00:00<00:11, 36.02it/s][A
  8%|▊         | 36/436 [00:00<00:11, 35.99it/s][A
  9%|▉         | 41/436 [00:01<00:10, 38.48it/s][A
 11%|█         | 46/436 [00:01<00:09, 40.27it/s][A
 12%|█▏        | 51/436 [00:01<00:09, 41.43it/s][A
 13%|█▎        | 56/436 [00:01<00:08, 42.40it/s][A
 14%|█▍        | 61/436 [00:01<00:08, 43.09it/s][A
 15%|█▌        | 66/436 [00:01<00:08, 43.57it/s][A
 16%|█▋        | 71/436 [00:01<00:08, 43.78it/s][A
 17%|█▋        | 76/436 [00:01<00:08, 43.53it/s][A
 19%|█▊        | 81/436 [00:01<00:08, 43.56it/s][A
 20%|█▉        | 86/436 [00:02<00:07, 43.95it/s][A
 21%|██        | 91/436 [00:02<00:07, 44.13it/s][A
 22%|██▏       | 96/436 [00:02<00:07, 44.21it/s][A
 23%|██▎       | 101/436 [00:02<00:07, 44.27it/s][A
 24%|██▍       | 106/436 [00:02<00:07, 44.22it/s][A
 25%|██▌       | 111/436 [00:02<00:07, 44.24it/s][A
 27%|██▋       | 116/436 [00:02<00:07, 44.03it/s][A
 28%|██▊       | 121/436 [00:02<00:07, 43.79it/s][A
 29%|██▉       | 126/436 [00:02<00:07, 43.80it/s][A
 30%|███       | 131/436 [00:03<00:06, 44.04it/s][A
 31%|███       | 136/436 [00:03<00:06, 44.11it/s][A
 32%|███▏      | 141/436 [00:03<00:06, 44.35it/s][A
 33%|███▎      | 146/436 [00:03<00:06, 44.39it/s][A
 35%|███▍      | 151/436 [00:03<00:06, 44.35it/s][A
 36%|███▌      | 156/436 [00:03<00:06, 44.34it/s][A
 37%|███▋      | 161/436 [00:03<00:06, 43.97it/s][A
 38%|███▊      | 166/436 [00:03<00:06, 43.92it/s][A
 39%|███▉      | 171/436 [00:03<00:06, 43.90it/s][A
 40%|████      | 176/436 [00:04<00:05, 44.10it/s][A
 42%|████▏     | 181/436 [00:04<00:05, 44.27it/s][A
 43%|████▎     | 186/436 [00:04<00:05, 44.34it/s][A
 44%|████▍     | 191/436 [00:04<00:05, 44.34it/s][A
 45%|████▍     | 196/436 [00:04<00:05, 44.06it/s][A
 46%|████▌     | 201/436 [00:04<00:05, 44.24it/s][A
 47%|████▋     | 206/436 [00:04<00:05, 44.08it/s][A
 48%|████▊     | 211/436 [00:04<00:05, 44.04it/s][A
 50%|████▉     | 216/436 [00:04<00:05, 43.96it/s][A
 51%|█████     | 221/436 [00:05<00:04, 44.20it/s][A
 52%|█████▏    | 226/436 [00:05<00:04, 44.26it/s][A
 53%|█████▎    | 231/436 [00:05<00:04, 44.42it/s][A
 54%|█████▍    | 236/436 [00:05<00:04, 44.44it/s][A
 55%|█████▌    | 241/436 [00:05<00:04, 44.37it/s][A
 56%|█████▋    | 246/436 [00:05<00:04, 44.08it/s][A
 58%|█████▊    | 251/436 [00:05<00:04, 44.02it/s][A
 59%|█████▊    | 256/436 [00:05<00:04, 44.02it/s][A
 60%|█████▉    | 261/436 [00:06<00:03, 44.06it/s][A
 61%|██████    | 266/436 [00:06<00:03, 44.22it/s][A
 62%|██████▏   | 271/436 [00:06<00:03, 44.29it/s][A
 63%|██████▎   | 276/436 [00:06<00:03, 44.30it/s][A
 64%|██████▍   | 281/436 [00:06<00:03, 44.33it/s][A
 66%|██████▌   | 286/436 [00:06<00:03, 44.12it/s][A
 67%|██████▋   | 291/436 [00:06<00:03, 44.03it/s][A
 68%|██████▊   | 296/436 [00:06<00:03, 44.07it/s][A
 69%|██████▉   | 301/436 [00:06<00:03, 44.04it/s][A
 70%|███████   | 306/436 [00:07<00:02, 44.03it/s][A
 71%|███████▏  | 311/436 [00:07<00:02, 44.18it/s][A
 72%|███████▏  | 316/436 [00:07<00:02, 44.11it/s][A
 74%|███████▎  | 321/436 [00:07<00:02, 44.20it/s][A
 75%|███████▍  | 326/436 [00:07<00:02, 44.24it/s][A
 76%|███████▌  | 331/436 [00:07<00:02, 44.27it/s][A
 77%|███████▋  | 336/436 [00:07<00:02, 44.20it/s][A
 78%|███████▊  | 341/436 [00:07<00:02, 44.12it/s][A
 79%|███████▉  | 346/436 [00:07<00:02, 44.11it/s][A
 81%|████████  | 351/436 [00:08<00:01, 44.06it/s][A
 82%|████████▏ | 356/436 [00:08<00:01, 44.18it/s][A
 83%|████████▎ | 361/436 [00:08<00:01, 44.07it/s][A
 84%|████████▍ | 366/436 [00:08<00:01, 44.16it/s][A
 85%|████████▌ | 371/436 [00:08<00:01, 44.30it/s][A
 86%|████████▌ | 376/436 [00:08<00:01, 44.20it/s][A
 87%|████████▋ | 381/436 [00:08<00:01, 44.23it/s][A
 89%|████████▊ | 386/436 [00:08<00:01, 44.12it/s][A
 90%|████████▉ | 391/436 [00:08<00:01, 44.01it/s][A
 91%|█████████ | 396/436 [00:09<00:00, 44.21it/s][A
 92%|█████████▏| 401/436 [00:09<00:00, 44.12it/s][A
 93%|█████████▎| 406/436 [00:09<00:00, 44.14it/s][A
 94%|█████████▍| 411/436 [00:09<00:00, 44.27it/s][A
 95%|█████████▌| 416/436 [00:09<00:00, 44.28it/s][A
 97%|█████████▋| 421/436 [00:09<00:00, 44.17it/s][A
 98%|█████████▊| 426/436 [00:09<00:00, 44.12it/s][A
 99%|█████████▉| 431/436 [00:09<00:00, 43.99it/s][A
100%|██████████| 436/436 [00:09<00:00, 44.01it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.01it/s][A 40%|████      | 234/585 [01:43<01:47,  3.28it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:45:31,954 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 20:45:31,992 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:45:36,676 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:45:36,885 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:45:36,901 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:57<43:58,  7.54s/it] 40%|████      | 236/585 [01:57<31:15,  5.38s/it] 41%|████      | 237/585 [01:58<22:20,  3.85s/it] 41%|████      | 238/585 [01:58<16:07,  2.79s/it] 41%|████      | 239/585 [01:58<11:46,  2.04s/it] 41%|████      | 240/585 [01:59<08:43,  1.52s/it] 41%|████      | 241/585 [01:59<06:36,  1.15s/it] 41%|████▏     | 242/585 [01:59<05:07,  1.11it/s] 42%|████▏     | 243/585 [02:00<04:05,  1.39it/s] 42%|████▏     | 244/585 [02:00<03:22,  1.69it/s] 42%|████▏     | 245/585 [02:00<02:51,  1.98it/s] 42%|████▏     | 246/585 [02:01<02:33,  2.21it/s] 42%|████▏     | 247/585 [02:01<02:17,  2.46it/s] 42%|████▏     | 248/585 [02:01<02:06,  2.67it/s] 43%|████▎     | 249/585 [02:01<01:58,  2.84it/s] 43%|████▎     | 250/585 [02:02<01:52,  2.97it/s] 43%|████▎     | 251/585 [02:02<01:48,  3.07it/s] 43%|████▎     | 252/585 [02:02<01:45,  3.14it/s] 43%|████▎     | 253/585 [02:03<01:43,  3.20it/s] 43%|████▎     | 254/585 [02:03<01:42,  3.24it/s] 44%|████▎     | 255/585 [02:03<01:41,  3.26it/s] 44%|████▍     | 256/585 [02:04<01:42,  3.21it/s] 44%|████▍     | 257/585 [02:04<01:40,  3.25it/s] 44%|████▍     | 258/585 [02:04<01:39,  3.28it/s] 44%|████▍     | 259/585 [02:04<01:38,  3.29it/s] 44%|████▍     | 260/585 [02:05<01:38,  3.31it/s] 45%|████▍     | 261/585 [02:05<01:37,  3.32it/s] 45%|████▍     | 262/585 [02:05<01:37,  3.32it/s] 45%|████▍     | 263/585 [02:06<01:36,  3.33it/s] 45%|████▌     | 264/585 [02:06<01:36,  3.33it/s] 45%|████▌     | 265/585 [02:06<01:36,  3.33it/s] 45%|████▌     | 266/585 [02:07<01:38,  3.25it/s] 46%|████▌     | 267/585 [02:07<01:37,  3.27it/s] 46%|████▌     | 268/585 [02:07<01:36,  3.29it/s] 46%|████▌     | 269/585 [02:07<01:35,  3.30it/s] 46%|████▌     | 270/585 [02:08<01:35,  3.31it/s] 46%|████▋     | 271/585 [02:08<01:34,  3.32it/s] 46%|████▋     | 272/585 [02:08<01:34,  3.32it/s] 47%|████▋     | 273/585 [02:09<01:33,  3.33it/s] 47%|████▋     | 274/585 [02:09<01:33,  3.33it/s] 47%|████▋     | 275/585 [02:09<01:33,  3.33it/s] 47%|████▋     | 276/585 [02:10<01:37,  3.17it/s] 47%|████▋     | 277/585 [02:10<01:35,  3.22it/s] 48%|████▊     | 278/585 [02:10<01:34,  3.25it/s] 48%|████▊     | 279/585 [02:11<01:33,  3.27it/s] 48%|████▊     | 280/585 [02:11<01:32,  3.29it/s] 48%|████▊     | 281/585 [02:11<01:32,  3.30it/s] 48%|████▊     | 282/585 [02:11<01:31,  3.31it/s] 48%|████▊     | 283/585 [02:12<01:31,  3.32it/s] 49%|████▊     | 284/585 [02:12<01:30,  3.32it/s] 49%|████▊     | 285/585 [02:12<01:30,  3.32it/s] 49%|████▉     | 286/585 [02:13<01:30,  3.30it/s] 49%|████▉     | 287/585 [02:13<01:30,  3.31it/s] 49%|████▉     | 288/585 [02:13<01:29,  3.32it/s] 49%|████▉     | 289/585 [02:14<01:29,  3.32it/s] 50%|████▉     | 290/585 [02:14<01:28,  3.32it/s] 50%|████▉     | 291/585 [02:14<01:28,  3.32it/s] 50%|████▉     | 292/585 [02:14<01:28,  3.32it/s] 50%|█████     | 293/585 [02:15<01:28,  3.32it/s] 50%|█████     | 294/585 [02:15<01:27,  3.32it/s] 50%|█████     | 295/585 [02:15<01:27,  3.32it/s] 51%|█████     | 296/585 [02:16<01:28,  3.25it/s] 51%|█████     | 297/585 [02:16<01:27,  3.28it/s] 51%|█████     | 298/585 [02:16<01:27,  3.29it/s] 51%|█████     | 299/585 [02:17<01:26,  3.30it/s] 51%|█████▏    | 300/585 [02:17<01:26,  3.31it/s] 51%|█████▏    | 301/585 [02:17<01:25,  3.32it/s] 52%|█████▏    | 302/585 [02:17<01:25,  3.32it/s] 52%|█████▏    | 303/585 [02:18<01:24,  3.33it/s] 52%|█████▏    | 304/585 [02:18<01:24,  3.33it/s] 52%|█████▏    | 305/585 [02:18<01:24,  3.33it/s] 52%|█████▏    | 306/585 [02:19<01:25,  3.27it/s] 52%|█████▏    | 307/585 [02:19<01:24,  3.29it/s] 53%|█████▎    | 308/585 [02:19<01:23,  3.30it/s] 53%|█████▎    | 309/585 [02:20<01:23,  3.31it/s] 53%|█████▎    | 310/585 [02:20<01:22,  3.32it/s] 53%|█████▎    | 311/585 [02:20<01:22,  3.32it/s] 53%|█████▎    | 312/585 [02:20<01:22,  3.33it/s] 54%|█████▎    | 313/585 [02:21<01:21,  3.33it/s] 54%|█████▎    | 314/585 [02:21<01:21,  3.33it/s] 54%|█████▍    | 315/585 [02:21<01:21,  3.33it/s] 54%|█████▍    | 316/585 [02:22<01:20,  3.33it/s] 54%|█████▍    | 317/585 [02:22<01:20,  3.33it/s] 54%|█████▍    | 318/585 [02:22<01:24,  3.16it/s] 55%|█████▍    | 319/585 [02:23<01:22,  3.21it/s] 55%|█████▍    | 320/585 [02:23<01:21,  3.24it/s] 55%|█████▍    | 321/585 [02:23<01:20,  3.27it/s] 55%|█████▌    | 322/585 [02:24<01:19,  3.29it/s] 55%|█████▌    | 323/585 [02:24<01:19,  3.30it/s] 55%|█████▌    | 324/585 [02:24<01:18,  3.31it/s] 56%|█████▌    | 325/585 [02:24<01:18,  3.32it/s] 56%|█████▌    | 326/585 [02:25<01:17,  3.32it/s] 56%|█████▌    | 327/585 [02:25<01:17,  3.33it/s] 56%|█████▌    | 328/585 [02:25<01:21,  3.14it/s] 56%|█████▌    | 329/585 [02:26<01:20,  3.19it/s] 56%|█████▋    | 330/585 [02:26<01:18,  3.23it/s] 57%|█████▋    | 331/585 [02:26<01:17,  3.26it/s] 57%|█████▋    | 332/585 [02:27<01:17,  3.28it/s] 57%|█████▋    | 333/585 [02:27<01:16,  3.29it/s] 57%|█████▋    | 334/585 [02:27<01:15,  3.31it/s] 57%|█████▋    | 335/585 [02:27<01:15,  3.31it/s] 57%|█████▋    | 336/585 [02:28<01:15,  3.31it/s] 58%|█████▊    | 337/585 [02:28<01:14,  3.32it/s] 58%|█████▊    | 338/585 [02:28<01:22,  3.00it/s] 58%|█████▊    | 339/585 [02:29<01:19,  3.09it/s] 58%|█████▊    | 340/585 [02:29<01:17,  3.16it/s] 58%|█████▊    | 341/585 [02:29<01:16,  3.20it/s] 58%|█████▊    | 342/585 [02:30<01:15,  3.24it/s] 59%|█████▊    | 343/585 [02:30<01:14,  3.26it/s] 59%|█████▉    | 344/585 [02:30<01:13,  3.28it/s] 59%|█████▉    | 345/585 [02:31<01:12,  3.29it/s] 59%|█████▉    | 346/585 [02:31<01:12,  3.31it/s] 59%|█████▉    | 347/585 [02:31<01:11,  3.31it/s] 59%|█████▉    | 348/585 [02:32<01:11,  3.30it/s] 60%|█████▉    | 349/585 [02:32<01:11,  3.31it/s] 60%|█████▉    | 350/585 [02:32<01:11,  3.31it/s] 60%|██████    | 351/585 [02:32<01:10,  3.31it/s][INFO|trainer.py:2140] 2023-08-28 20:46:21,554 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:46:21,554 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 20:46:21,554 >>   Batch size = 8
{'eval_loss': 0.980851948261261, 'eval_runtime': 9.997, 'eval_samples_per_second': 348.906, 'eval_steps_per_second': 43.613, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.03it/s][A
  3%|▎         | 12/436 [00:00<00:09, 45.78it/s][A
  4%|▍         | 17/436 [00:00<00:09, 45.59it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.42it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.75it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.51it/s][A
  8%|▊         | 37/436 [00:00<00:09, 44.23it/s][A
 10%|▉         | 42/436 [00:00<00:09, 43.49it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.13it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 43.98it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.41it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.58it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.52it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.36it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.28it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 41.35it/s][A
 20%|█▉        | 87/436 [00:01<00:08, 42.38it/s][A
 21%|██        | 92/436 [00:02<00:07, 43.01it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 43.39it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 43.75it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 43.93it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.10it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.02it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 43.79it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.89it/s][A
 30%|███       | 132/436 [00:02<00:06, 43.96it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.15it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.24it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.30it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.33it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.19it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.07it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.01it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.02it/s][A
 41%|████      | 177/436 [00:04<00:05, 44.10it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.23it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.26it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.26it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.32it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.28it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.15it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.00it/s][A
 50%|████▉     | 217/436 [00:04<00:05, 41.31it/s][A
 51%|█████     | 222/436 [00:05<00:05, 42.29it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 43.03it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 43.44it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 43.78it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 43.86it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 43.91it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 43.92it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.68it/s][A
 60%|██████    | 262/436 [00:05<00:03, 43.80it/s][A
 61%|██████    | 267/436 [00:06<00:03, 43.88it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.13it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.27it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.32it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.39it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.19it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.03it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 43.90it/s][A
 70%|███████   | 307/436 [00:06<00:02, 43.94it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.07it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.27it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.28it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.43it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.39it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.25it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.03it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.03it/s][A
 81%|████████  | 352/436 [00:07<00:01, 43.93it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.03it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.24it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.31it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.38it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.36it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.17it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.04it/s][A
 90%|████████▉ | 392/436 [00:08<00:01, 43.87it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 44.00it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.09it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.13it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.31it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.29it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.26it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.24it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.10it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.10it/s][A 60%|██████    | 351/585 [02:42<01:10,  3.31it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:46:31,504 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 20:46:31,531 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:46:36,646 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:46:36,731 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:46:36,765 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:56<28:28,  7.33s/it] 60%|██████    | 353/585 [02:56<20:12,  5.23s/it] 61%|██████    | 354/585 [02:57<14:26,  3.75s/it] 61%|██████    | 355/585 [02:57<10:24,  2.71s/it] 61%|██████    | 356/585 [02:57<07:35,  1.99s/it] 61%|██████    | 357/585 [02:58<05:38,  1.48s/it] 61%|██████    | 358/585 [02:58<04:16,  1.13s/it] 61%|██████▏   | 359/585 [02:58<03:18,  1.14it/s] 62%|██████▏   | 360/585 [02:59<02:38,  1.42it/s] 62%|██████▏   | 361/585 [02:59<02:10,  1.71it/s] 62%|██████▏   | 362/585 [02:59<01:51,  2.00it/s] 62%|██████▏   | 363/585 [02:59<01:38,  2.26it/s] 62%|██████▏   | 364/585 [03:00<01:28,  2.50it/s] 62%|██████▏   | 365/585 [03:00<01:21,  2.70it/s] 63%|██████▎   | 366/585 [03:00<01:16,  2.87it/s] 63%|██████▎   | 367/585 [03:01<01:12,  2.99it/s] 63%|██████▎   | 368/585 [03:01<01:10,  3.09it/s] 63%|██████▎   | 369/585 [03:01<01:08,  3.16it/s] 63%|██████▎   | 370/585 [03:02<01:06,  3.21it/s] 63%|██████▎   | 371/585 [03:02<01:05,  3.26it/s] 64%|██████▎   | 372/585 [03:02<01:04,  3.28it/s] 64%|██████▍   | 373/585 [03:03<01:08,  3.08it/s] 64%|██████▍   | 374/585 [03:03<01:07,  3.15it/s] 64%|██████▍   | 375/585 [03:03<01:05,  3.20it/s] 64%|██████▍   | 376/585 [03:03<01:04,  3.24it/s] 64%|██████▍   | 377/585 [03:04<01:03,  3.27it/s] 65%|██████▍   | 378/585 [03:04<01:03,  3.28it/s] 65%|██████▍   | 379/585 [03:04<01:02,  3.29it/s] 65%|██████▍   | 380/585 [03:05<01:02,  3.30it/s] 65%|██████▌   | 381/585 [03:05<01:01,  3.31it/s] 65%|██████▌   | 382/585 [03:05<01:01,  3.31it/s] 65%|██████▌   | 383/585 [03:06<01:01,  3.31it/s] 66%|██████▌   | 384/585 [03:06<01:00,  3.31it/s] 66%|██████▌   | 385/585 [03:06<01:00,  3.32it/s] 66%|██████▌   | 386/585 [03:06<00:59,  3.33it/s] 66%|██████▌   | 387/585 [03:07<00:59,  3.32it/s] 66%|██████▋   | 388/585 [03:07<00:59,  3.33it/s] 66%|██████▋   | 389/585 [03:07<00:58,  3.33it/s] 67%|██████▋   | 390/585 [03:08<00:58,  3.33it/s] 67%|██████▋   | 391/585 [03:08<00:58,  3.33it/s] 67%|██████▋   | 392/585 [03:08<00:57,  3.33it/s] 67%|██████▋   | 393/585 [03:09<00:57,  3.32it/s] 67%|██████▋   | 394/585 [03:09<00:57,  3.33it/s] 68%|██████▊   | 395/585 [03:09<00:57,  3.33it/s] 68%|██████▊   | 396/585 [03:09<00:56,  3.33it/s] 68%|██████▊   | 397/585 [03:10<00:56,  3.33it/s] 68%|██████▊   | 398/585 [03:10<00:56,  3.33it/s] 68%|██████▊   | 399/585 [03:10<00:55,  3.33it/s] 68%|██████▊   | 400/585 [03:11<00:55,  3.33it/s] 69%|██████▊   | 401/585 [03:11<00:55,  3.33it/s] 69%|██████▊   | 402/585 [03:11<00:54,  3.33it/s] 69%|██████▉   | 403/585 [03:12<00:55,  3.28it/s] 69%|██████▉   | 404/585 [03:12<00:54,  3.30it/s] 69%|██████▉   | 405/585 [03:12<00:54,  3.31it/s] 69%|██████▉   | 406/585 [03:12<00:53,  3.32it/s] 70%|██████▉   | 407/585 [03:13<00:53,  3.32it/s] 70%|██████▉   | 408/585 [03:13<00:53,  3.32it/s] 70%|██████▉   | 409/585 [03:13<00:52,  3.33it/s] 70%|███████   | 410/585 [03:14<00:52,  3.32it/s] 70%|███████   | 411/585 [03:14<00:52,  3.32it/s] 70%|███████   | 412/585 [03:14<00:52,  3.33it/s] 71%|███████   | 413/585 [03:15<00:51,  3.32it/s] 71%|███████   | 414/585 [03:15<00:51,  3.32it/s] 71%|███████   | 415/585 [03:15<00:51,  3.32it/s] 71%|███████   | 416/585 [03:15<00:50,  3.32it/s] 71%|███████▏  | 417/585 [03:16<00:50,  3.32it/s] 71%|███████▏  | 418/585 [03:16<00:50,  3.32it/s] 72%|███████▏  | 419/585 [03:16<00:49,  3.32it/s] 72%|███████▏  | 420/585 [03:17<00:49,  3.32it/s] 72%|███████▏  | 421/585 [03:17<00:49,  3.32it/s] 72%|███████▏  | 422/585 [03:17<00:48,  3.33it/s] 72%|███████▏  | 423/585 [03:18<00:48,  3.32it/s] 72%|███████▏  | 424/585 [03:18<00:48,  3.32it/s] 73%|███████▎  | 425/585 [03:18<00:48,  3.33it/s] 73%|███████▎  | 426/585 [03:18<00:47,  3.33it/s] 73%|███████▎  | 427/585 [03:19<00:47,  3.33it/s] 73%|███████▎  | 428/585 [03:19<00:47,  3.33it/s] 73%|███████▎  | 429/585 [03:19<00:46,  3.33it/s] 74%|███████▎  | 430/585 [03:20<00:46,  3.33it/s] 74%|███████▎  | 431/585 [03:20<00:46,  3.33it/s] 74%|███████▍  | 432/585 [03:20<00:45,  3.33it/s] 74%|███████▍  | 433/585 [03:21<00:45,  3.32it/s] 74%|███████▍  | 434/585 [03:21<00:45,  3.32it/s] 74%|███████▍  | 435/585 [03:21<00:45,  3.32it/s] 75%|███████▍  | 436/585 [03:21<00:44,  3.32it/s] 75%|███████▍  | 437/585 [03:22<00:44,  3.32it/s] 75%|███████▍  | 438/585 [03:22<00:44,  3.32it/s] 75%|███████▌  | 439/585 [03:22<00:43,  3.33it/s] 75%|███████▌  | 440/585 [03:23<00:43,  3.32it/s] 75%|███████▌  | 441/585 [03:23<00:43,  3.33it/s] 76%|███████▌  | 442/585 [03:23<00:43,  3.32it/s] 76%|███████▌  | 443/585 [03:24<00:43,  3.29it/s] 76%|███████▌  | 444/585 [03:24<00:42,  3.30it/s] 76%|███████▌  | 445/585 [03:24<00:42,  3.31it/s] 76%|███████▌  | 446/585 [03:25<00:41,  3.31it/s] 76%|███████▋  | 447/585 [03:25<00:41,  3.32it/s] 77%|███████▋  | 448/585 [03:25<00:41,  3.32it/s] 77%|███████▋  | 449/585 [03:25<00:40,  3.32it/s] 77%|███████▋  | 450/585 [03:26<00:40,  3.32it/s] 77%|███████▋  | 451/585 [03:26<00:40,  3.32it/s] 77%|███████▋  | 452/585 [03:26<00:40,  3.32it/s] 77%|███████▋  | 453/585 [03:27<00:39,  3.31it/s] 78%|███████▊  | 454/585 [03:27<00:39,  3.32it/s] 78%|███████▊  | 455/585 [03:27<00:39,  3.32it/s] 78%|███████▊  | 456/585 [03:28<00:38,  3.32it/s] 78%|███████▊  | 457/585 [03:28<00:38,  3.32it/s] 78%|███████▊  | 458/585 [03:28<00:38,  3.33it/s] 78%|███████▊  | 459/585 [03:28<00:37,  3.33it/s] 79%|███████▊  | 460/585 [03:29<00:37,  3.33it/s] 79%|███████▉  | 461/585 [03:29<00:37,  3.33it/s] 79%|███████▉  | 462/585 [03:29<00:36,  3.33it/s] 79%|███████▉  | 463/585 [03:30<00:37,  3.26it/s] 79%|███████▉  | 464/585 [03:30<00:36,  3.28it/s] 79%|███████▉  | 465/585 [03:30<00:36,  3.30it/s] 80%|███████▉  | 466/585 [03:31<00:35,  3.31it/s] 80%|███████▉  | 467/585 [03:31<00:35,  3.31it/s] 80%|████████  | 468/585 [03:31<00:35,  3.31it/s][INFO|trainer.py:2140] 2023-08-28 20:47:20,297 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:47:20,297 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 20:47:20,297 >>   Batch size = 8
{'eval_loss': 0.9942866563796997, 'eval_runtime': 9.9348, 'eval_samples_per_second': 351.09, 'eval_steps_per_second': 43.886, 'epoch': 3.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.27it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.01it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.81it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.90it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.27it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.70it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.50it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.05it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.17it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.41it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 43.65it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 43.95it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 43.39it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.09it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.09it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.91it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 43.83it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.10it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 43.48it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.49it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 43.78it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.28it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.23it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.09it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.94it/s][A
 30%|███       | 132/436 [00:02<00:06, 43.95it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.05it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.19it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.16it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.33it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.30it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.21it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.15it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.97it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.03it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.05it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.18it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.29it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.35it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.31it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.27it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.09it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.08it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.13it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.09it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.21it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.18it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.32it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.29it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.22it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.22it/s][A
 60%|██████    | 262/436 [00:05<00:03, 43.98it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.07it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.17it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.18it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.31it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.23it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.20it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.31it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.20it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.19it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.13it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.19it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.10it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.17it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.26it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.19it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.20it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.09it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.06it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.19it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.23it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.15it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.27it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.24it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.13it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.08it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.07it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.06it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.09it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.19it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.20it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.24it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.23it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.16it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.23it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.23it/s][A 80%|████████  | 468/585 [03:41<00:35,  3.31it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:47:30,228 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 20:47:30,343 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:47:34,205 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:47:34,223 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:47:34,232 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:54<13:50,  7.16s/it] 80%|████████  | 470/585 [03:55<09:50,  5.13s/it] 81%|████████  | 471/585 [03:55<06:59,  3.68s/it] 81%|████████  | 472/585 [03:55<05:01,  2.67s/it] 81%|████████  | 473/585 [03:56<03:39,  1.96s/it] 81%|████████  | 474/585 [03:56<02:42,  1.46s/it] 81%|████████  | 475/585 [03:56<02:02,  1.11s/it] 81%|████████▏ | 476/585 [03:57<01:34,  1.15it/s] 82%|████████▏ | 477/585 [03:57<01:15,  1.43it/s] 82%|████████▏ | 478/585 [03:57<01:01,  1.73it/s] 82%|████████▏ | 479/585 [03:57<00:52,  2.02it/s] 82%|████████▏ | 480/585 [03:58<00:46,  2.28it/s] 82%|████████▏ | 481/585 [03:58<00:41,  2.52it/s] 82%|████████▏ | 482/585 [03:58<00:37,  2.72it/s] 83%|████████▎ | 483/585 [03:59<00:35,  2.88it/s] 83%|████████▎ | 484/585 [03:59<00:33,  3.00it/s] 83%|████████▎ | 485/585 [03:59<00:32,  3.09it/s] 83%|████████▎ | 486/585 [04:00<00:31,  3.16it/s] 83%|████████▎ | 487/585 [04:00<00:30,  3.20it/s] 83%|████████▎ | 488/585 [04:00<00:29,  3.24it/s] 84%|████████▎ | 489/585 [04:00<00:29,  3.27it/s] 84%|████████▍ | 490/585 [04:01<00:28,  3.28it/s] 84%|████████▍ | 491/585 [04:01<00:28,  3.29it/s] 84%|████████▍ | 492/585 [04:01<00:28,  3.31it/s] 84%|████████▍ | 493/585 [04:02<00:27,  3.31it/s] 84%|████████▍ | 494/585 [04:02<00:27,  3.32it/s] 85%|████████▍ | 495/585 [04:02<00:27,  3.32it/s] 85%|████████▍ | 496/585 [04:03<00:26,  3.32it/s] 85%|████████▍ | 497/585 [04:03<00:26,  3.32it/s] 85%|████████▌ | 498/585 [04:03<00:26,  3.33it/s] 85%|████████▌ | 499/585 [04:03<00:25,  3.33it/s] 85%|████████▌ | 500/585 [04:04<00:26,  3.25it/s]                                                  85%|████████▌ | 500/585 [04:04<00:26,  3.25it/s] 86%|████████▌ | 501/585 [04:04<00:25,  3.27it/s] 86%|████████▌ | 502/585 [04:04<00:25,  3.29it/s] 86%|████████▌ | 503/585 [04:05<00:24,  3.30it/s] 86%|████████▌ | 504/585 [04:05<00:24,  3.31it/s] 86%|████████▋ | 505/585 [04:05<00:24,  3.31it/s] 86%|████████▋ | 506/585 [04:06<00:23,  3.32it/s] 87%|████████▋ | 507/585 [04:06<00:23,  3.32it/s] 87%|████████▋ | 508/585 [04:06<00:23,  3.32it/s] 87%|████████▋ | 509/585 [04:06<00:22,  3.32it/s] 87%|████████▋ | 510/585 [04:07<00:22,  3.29it/s] 87%|████████▋ | 511/585 [04:07<00:22,  3.30it/s] 88%|████████▊ | 512/585 [04:07<00:22,  3.31it/s] 88%|████████▊ | 513/585 [04:08<00:21,  3.31it/s] 88%|████████▊ | 514/585 [04:08<00:21,  3.32it/s] 88%|████████▊ | 515/585 [04:08<00:21,  3.32it/s] 88%|████████▊ | 516/585 [04:09<00:20,  3.32it/s] 88%|████████▊ | 517/585 [04:09<00:20,  3.32it/s] 89%|████████▊ | 518/585 [04:09<00:20,  3.32it/s] 89%|████████▊ | 519/585 [04:09<00:19,  3.32it/s] 89%|████████▉ | 520/585 [04:10<00:20,  3.19it/s] 89%|████████▉ | 521/585 [04:10<00:19,  3.23it/s] 89%|████████▉ | 522/585 [04:10<00:19,  3.26it/s] 89%|████████▉ | 523/585 [04:11<00:18,  3.28it/s] 90%|████████▉ | 524/585 [04:11<00:18,  3.29it/s] 90%|████████▉ | 525/585 [04:11<00:18,  3.30it/s] 90%|████████▉ | 526/585 [04:12<00:17,  3.31it/s] 90%|█████████ | 527/585 [04:12<00:17,  3.32it/s] 90%|█████████ | 528/585 [04:12<00:17,  3.32it/s] 90%|█████████ | 529/585 [04:13<00:16,  3.32it/s] 91%|█████████ | 530/585 [04:13<00:16,  3.30it/s] 91%|█████████ | 531/585 [04:13<00:16,  3.31it/s] 91%|█████████ | 532/585 [04:13<00:15,  3.32it/s] 91%|█████████ | 533/585 [04:14<00:15,  3.32it/s] 91%|█████████▏| 534/585 [04:14<00:15,  3.32it/s] 91%|█████████▏| 535/585 [04:14<00:15,  3.32it/s] 92%|█████████▏| 536/585 [04:15<00:14,  3.32it/s] 92%|█████████▏| 537/585 [04:15<00:14,  3.32it/s] 92%|█████████▏| 538/585 [04:15<00:14,  3.32it/s] 92%|█████████▏| 539/585 [04:16<00:13,  3.32it/s] 92%|█████████▏| 540/585 [04:16<00:13,  3.26it/s] 92%|█████████▏| 541/585 [04:16<00:13,  3.28it/s] 93%|█████████▎| 542/585 [04:16<00:13,  3.30it/s] 93%|█████████▎| 543/585 [04:17<00:12,  3.31it/s] 93%|█████████▎| 544/585 [04:17<00:12,  3.31it/s] 93%|█████████▎| 545/585 [04:17<00:12,  3.32it/s] 93%|█████████▎| 546/585 [04:18<00:11,  3.32it/s] 94%|█████████▎| 547/585 [04:18<00:11,  3.33it/s] 94%|█████████▎| 548/585 [04:18<00:11,  3.33it/s] 94%|█████████▍| 549/585 [04:19<00:10,  3.33it/s] 94%|█████████▍| 550/585 [04:19<00:10,  3.32it/s] 94%|█████████▍| 551/585 [04:19<00:10,  3.33it/s] 94%|█████████▍| 552/585 [04:19<00:09,  3.33it/s] 95%|█████████▍| 553/585 [04:20<00:09,  3.33it/s] 95%|█████████▍| 554/585 [04:20<00:09,  3.33it/s] 95%|█████████▍| 555/585 [04:20<00:09,  3.33it/s] 95%|█████████▌| 556/585 [04:21<00:08,  3.33it/s] 95%|█████████▌| 557/585 [04:21<00:08,  3.33it/s] 95%|█████████▌| 558/585 [04:21<00:08,  3.33it/s] 96%|█████████▌| 559/585 [04:22<00:07,  3.33it/s] 96%|█████████▌| 560/585 [04:22<00:07,  3.25it/s] 96%|█████████▌| 561/585 [04:22<00:07,  3.27it/s] 96%|█████████▌| 562/585 [04:22<00:06,  3.29it/s] 96%|█████████▌| 563/585 [04:23<00:06,  3.30it/s] 96%|█████████▋| 564/585 [04:23<00:06,  3.31it/s] 97%|█████████▋| 565/585 [04:23<00:06,  3.31it/s] 97%|█████████▋| 566/585 [04:24<00:05,  3.31it/s] 97%|█████████▋| 567/585 [04:24<00:05,  3.31it/s] 97%|█████████▋| 568/585 [04:24<00:05,  3.32it/s] 97%|█████████▋| 569/585 [04:25<00:04,  3.32it/s] 97%|█████████▋| 570/585 [04:25<00:04,  3.33it/s] 98%|█████████▊| 571/585 [04:25<00:04,  3.29it/s] 98%|█████████▊| 572/585 [04:26<00:03,  3.30it/s] 98%|█████████▊| 573/585 [04:26<00:03,  3.31it/s] 98%|█████████▊| 574/585 [04:26<00:03,  3.31it/s] 98%|█████████▊| 575/585 [04:26<00:03,  3.32it/s] 98%|█████████▊| 576/585 [04:27<00:02,  3.32it/s] 99%|█████████▊| 577/585 [04:27<00:02,  3.32it/s] 99%|█████████▉| 578/585 [04:27<00:02,  3.32it/s] 99%|█████████▉| 579/585 [04:28<00:01,  3.32it/s] 99%|█████████▉| 580/585 [04:28<00:01,  3.32it/s] 99%|█████████▉| 581/585 [04:28<00:01,  3.26it/s] 99%|█████████▉| 582/585 [04:29<00:00,  3.29it/s]100%|█████████▉| 583/585 [04:29<00:00,  3.30it/s]100%|█████████▉| 584/585 [04:29<00:00,  3.31it/s]100%|██████████| 585/585 [04:29<00:00,  3.31it/s][INFO|trainer.py:2140] 2023-08-28 20:48:18,538 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:48:18,538 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 20:48:18,538 >>   Batch size = 8
{'eval_loss': 0.9988937973976135, 'eval_runtime': 9.8871, 'eval_samples_per_second': 352.781, 'eval_steps_per_second': 44.098, 'epoch': 4.0}
{'loss': 0.6557, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.46it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.51it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.88it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.99it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.31it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.74it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.53it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.27it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.20it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.39it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.31it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.49it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.44it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 43.79it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 43.86it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.94it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 43.92it/s][A
 21%|██        | 92/436 [00:02<00:07, 43.97it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.27it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.35it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.21it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.01it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.99it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.09it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 44.07it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.14it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.20it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.20it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.29it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.21it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.15it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.17it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.09it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 41.57it/s][A
 41%|████      | 177/436 [00:04<00:06, 39.76it/s][A
 42%|████▏     | 182/436 [00:04<00:06, 36.95it/s][A
 43%|████▎     | 187/436 [00:04<00:06, 39.79it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 41.30it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 42.23it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 42.98it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 43.53it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 43.75it/s][A
 50%|████▉     | 217/436 [00:04<00:05, 43.60it/s][A
 51%|█████     | 222/436 [00:05<00:04, 43.63it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 43.59it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 43.78it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.05it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.18it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.38it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.41it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.21it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.01it/s][A
 61%|██████    | 267/436 [00:06<00:03, 43.77it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 43.72it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 43.94it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.09it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.37it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.42it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.36it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.32it/s][A
 70%|███████   | 307/436 [00:07<00:02, 44.12it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 43.98it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 43.85it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.01it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.13it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.35it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 43.29it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 43.56it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 43.61it/s][A
 81%|████████  | 352/436 [00:08<00:01, 43.62it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 43.72it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 43.77it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 43.91it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.13it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 43.95it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.15it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.27it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.18it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 44.10it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 43.88it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 43.88it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 43.93it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.25it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.33it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.28it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.36it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.36it/s][A100%|██████████| 585/585 [04:39<00:00,  3.31it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:48:28,558 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 20:48:28,668 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:48:32,536 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:48:32,550 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:48:32,558 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 20:48:41,406 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 20:48:41,411 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-117 (score: 0.9767536520957947).
                                                 100%|██████████| 585/585 [04:56<00:00,  3.31it/s]100%|██████████| 585/585 [04:56<00:00,  1.97it/s]
[INFO|trainer.py:1894] 2023-08-28 20:48:45,598 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 20:48:45,741 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:48:50,261 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:48:50,341 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:48:50,403 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 20:48:50,661 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:48:50,661 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:48:50,661 >>   train_loss               =     0.6511
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:48:50,661 >>   train_runtime            = 0:04:56.97
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:48:50,661 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:48:50,661 >>   train_samples_per_second =    126.274
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:48:50,661 >>   train_steps_per_second   =       1.97
{'eval_loss': 1.003662109375, 'eval_runtime': 9.9648, 'eval_samples_per_second': 350.033, 'eval_steps_per_second': 43.754, 'epoch': 5.0}
{'train_runtime': 296.9732, 'train_samples_per_second': 126.274, 'train_steps_per_second': 1.97, 'train_loss': 0.6511223621857471, 'epoch': 5.0}
08/28/2023 20:48:50 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 20:48:50,709 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:48:50,709 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 20:48:50,709 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 54.91it/s]  3%|▎         | 12/436 [00:00<00:08, 48.30it/s]  4%|▍         | 17/436 [00:00<00:08, 46.90it/s]  5%|▌         | 22/436 [00:00<00:08, 46.14it/s]  6%|▌         | 27/436 [00:00<00:08, 45.53it/s]  7%|▋         | 32/436 [00:00<00:08, 45.24it/s]  8%|▊         | 37/436 [00:00<00:08, 45.16it/s] 10%|▉         | 42/436 [00:00<00:08, 44.67it/s] 11%|█         | 47/436 [00:01<00:08, 44.11it/s] 12%|█▏        | 52/436 [00:01<00:08, 44.01it/s] 13%|█▎        | 57/436 [00:01<00:08, 44.15it/s] 14%|█▍        | 62/436 [00:01<00:08, 44.32it/s] 15%|█▌        | 67/436 [00:01<00:08, 44.50it/s] 17%|█▋        | 72/436 [00:01<00:08, 44.48it/s] 18%|█▊        | 77/436 [00:01<00:08, 44.63it/s] 19%|█▉        | 82/436 [00:01<00:07, 44.49it/s] 20%|█▉        | 87/436 [00:01<00:07, 44.16it/s] 21%|██        | 92/436 [00:02<00:07, 43.94it/s] 22%|██▏       | 97/436 [00:02<00:07, 43.99it/s] 23%|██▎       | 102/436 [00:02<00:07, 44.07it/s] 25%|██▍       | 107/436 [00:02<00:07, 44.16it/s] 26%|██▌       | 112/436 [00:02<00:07, 44.31it/s] 27%|██▋       | 117/436 [00:02<00:07, 44.48it/s] 28%|██▊       | 122/436 [00:02<00:07, 44.48it/s] 29%|██▉       | 127/436 [00:02<00:06, 44.37it/s] 30%|███       | 132/436 [00:02<00:06, 44.14it/s] 31%|███▏      | 137/436 [00:03<00:06, 43.86it/s] 33%|███▎      | 142/436 [00:03<00:06, 43.91it/s] 34%|███▎      | 147/436 [00:03<00:06, 44.01it/s] 35%|███▍      | 152/436 [00:03<00:06, 44.26it/s] 36%|███▌      | 157/436 [00:03<00:06, 44.35it/s] 37%|███▋      | 162/436 [00:03<00:06, 44.36it/s] 38%|███▊      | 167/436 [00:03<00:06, 44.32it/s] 39%|███▉      | 172/436 [00:03<00:05, 44.23it/s] 41%|████      | 177/436 [00:03<00:05, 44.08it/s] 42%|████▏     | 182/436 [00:04<00:05, 43.88it/s] 43%|████▎     | 187/436 [00:04<00:05, 43.91it/s] 44%|████▍     | 192/436 [00:04<00:05, 43.92it/s] 45%|████▌     | 197/436 [00:04<00:05, 44.17it/s] 46%|████▋     | 202/436 [00:04<00:05, 44.36it/s] 47%|████▋     | 207/436 [00:04<00:05, 44.44it/s] 49%|████▊     | 212/436 [00:04<00:05, 44.52it/s] 50%|████▉     | 217/436 [00:04<00:04, 44.27it/s] 51%|█████     | 222/436 [00:04<00:04, 44.08it/s] 52%|█████▏    | 227/436 [00:05<00:04, 43.93it/s] 53%|█████▎    | 232/436 [00:05<00:04, 43.91it/s] 54%|█████▍    | 237/436 [00:05<00:04, 44.06it/s] 56%|█████▌    | 242/436 [00:05<00:04, 44.24it/s] 57%|█████▋    | 247/436 [00:05<00:04, 44.29it/s] 58%|█████▊    | 252/436 [00:05<00:04, 44.45it/s] 59%|█████▉    | 257/436 [00:05<00:04, 44.20it/s] 60%|██████    | 262/436 [00:05<00:03, 44.15it/s] 61%|██████    | 267/436 [00:06<00:03, 44.01it/s] 62%|██████▏   | 272/436 [00:06<00:03, 43.95it/s] 64%|██████▎   | 277/436 [00:06<00:03, 43.89it/s] 65%|██████▍   | 282/436 [00:06<00:03, 44.15it/s] 66%|██████▌   | 287/436 [00:06<00:03, 44.27it/s] 67%|██████▋   | 292/436 [00:06<00:03, 44.39it/s] 68%|██████▊   | 297/436 [00:06<00:03, 44.42it/s] 69%|██████▉   | 302/436 [00:06<00:03, 44.27it/s] 70%|███████   | 307/436 [00:06<00:02, 44.21it/s] 72%|███████▏  | 312/436 [00:07<00:02, 44.09it/s] 73%|███████▎  | 317/436 [00:07<00:02, 44.00it/s] 74%|███████▍  | 322/436 [00:07<00:02, 44.05it/s] 75%|███████▌  | 327/436 [00:07<00:02, 44.09it/s] 76%|███████▌  | 332/436 [00:07<00:02, 44.14it/s] 77%|███████▋  | 337/436 [00:07<00:02, 44.33it/s] 78%|███████▊  | 342/436 [00:07<00:02, 44.31it/s] 80%|███████▉  | 347/436 [00:07<00:02, 44.35it/s] 81%|████████  | 352/436 [00:07<00:01, 44.21it/s] 82%|████████▏ | 357/436 [00:08<00:01, 44.00it/s] 83%|████████▎ | 362/436 [00:08<00:01, 44.08it/s] 84%|████████▍ | 367/436 [00:08<00:01, 44.10it/s] 85%|████████▌ | 372/436 [00:08<00:01, 44.20it/s] 86%|████████▋ | 377/436 [00:08<00:01, 44.32it/s] 88%|████████▊ | 382/436 [00:08<00:01, 44.36it/s] 89%|████████▉ | 387/436 [00:08<00:01, 44.27it/s] 90%|████████▉ | 392/436 [00:08<00:01, 38.77it/s] 91%|█████████ | 397/436 [00:09<00:00, 40.37it/s] 92%|█████████▏| 402/436 [00:09<00:00, 41.59it/s] 93%|█████████▎| 407/436 [00:09<00:00, 42.48it/s] 94%|█████████▍| 412/436 [00:09<00:00, 43.03it/s] 96%|█████████▌| 417/436 [00:09<00:00, 43.51it/s] 97%|█████████▋| 422/436 [00:09<00:00, 43.93it/s] 98%|█████████▊| 427/436 [00:09<00:00, 43.87it/s] 99%|█████████▉| 432/436 [00:09<00:00, 43.57it/s]100%|██████████| 436/436 [00:09<00:00, 44.04it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 20:49:00,627 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:00,627 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:00,627 >>   eval_loss               =     0.9768
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:00,627 >>   eval_runtime            = 0:00:09.91
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:00,627 >>   eval_samples            =       3488
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:00,627 >>   eval_samples_per_second =    351.707
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:00,627 >>   eval_steps_per_second   =     43.963
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:49:00,627 >>   perplexity              =     2.6558
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:06,596 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:06,602 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:06,602 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:06,602 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:06,602 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:49:06,887 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:49:06,888 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:49:07,142 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:49:08,196 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:49:08,197 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:09,965 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:09,970 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:09,970 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:09,970 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:49:09,970 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:49:10,285 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:49:10,286 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:49:10,556 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:49:10,700 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:49:10,700 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/checkpoint-351
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11910
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12010, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.34it/s]Extractor Predicting: 2it [00:01,  1.30it/s]Extractor Predicting: 3it [00:02,  1.31it/s]Extractor Predicting: 4it [00:02,  1.35it/s]Extractor Predicting: 5it [00:03,  1.38it/s]Extractor Predicting: 6it [00:04,  1.37it/s]Extractor Predicting: 7it [00:05,  1.39it/s]Extractor Predicting: 8it [00:05,  1.43it/s]Extractor Predicting: 9it [00:06,  1.39it/s]Extractor Predicting: 10it [00:07,  1.39it/s]Extractor Predicting: 11it [00:07,  1.40it/s]Extractor Predicting: 12it [00:08,  1.38it/s]Extractor Predicting: 13it [00:09,  1.38it/s]Extractor Predicting: 14it [00:10,  1.36it/s]Extractor Predicting: 15it [00:10,  1.37it/s]Extractor Predicting: 16it [00:11,  1.36it/s]Extractor Predicting: 17it [00:12,  1.36it/s]Extractor Predicting: 18it [00:13,  1.38it/s]Extractor Predicting: 19it [00:13,  1.41it/s]Extractor Predicting: 20it [00:14,  1.40it/s]Extractor Predicting: 21it [00:15,  1.38it/s]Extractor Predicting: 22it [00:15,  1.39it/s]Extractor Predicting: 23it [00:16,  1.40it/s]Extractor Predicting: 24it [00:17,  1.40it/s]Extractor Predicting: 25it [00:18,  1.37it/s]Extractor Predicting: 26it [00:18,  1.37it/s]Extractor Predicting: 27it [00:19,  1.38it/s]Extractor Predicting: 28it [00:20,  1.38it/s]Extractor Predicting: 29it [00:21,  1.37it/s]Extractor Predicting: 30it [00:21,  1.35it/s]Extractor Predicting: 31it [00:22,  1.35it/s]Extractor Predicting: 32it [00:23,  1.36it/s]Extractor Predicting: 33it [00:24,  1.26it/s]Extractor Predicting: 34it [00:24,  1.27it/s]Extractor Predicting: 35it [00:25,  1.27it/s]Extractor Predicting: 36it [00:26,  1.27it/s]Extractor Predicting: 37it [00:27,  1.29it/s]Extractor Predicting: 38it [00:28,  1.29it/s]Extractor Predicting: 39it [00:28,  1.30it/s]Extractor Predicting: 40it [00:29,  1.29it/s]Extractor Predicting: 41it [00:30,  1.30it/s]Extractor Predicting: 42it [00:31,  1.30it/s]Extractor Predicting: 43it [00:31,  1.28it/s]Extractor Predicting: 44it [00:32,  1.28it/s]Extractor Predicting: 45it [00:33,  1.27it/s]Extractor Predicting: 46it [00:34,  1.24it/s]Extractor Predicting: 47it [00:35,  1.29it/s]Extractor Predicting: 48it [00:35,  1.30it/s]Extractor Predicting: 49it [00:36,  1.33it/s]Extractor Predicting: 50it [00:37,  1.29it/s]Extractor Predicting: 51it [00:38,  1.32it/s]Extractor Predicting: 52it [00:38,  1.33it/s]Extractor Predicting: 53it [00:39,  1.32it/s]Extractor Predicting: 54it [00:40,  1.32it/s]Extractor Predicting: 55it [00:41,  1.32it/s]Extractor Predicting: 56it [00:41,  1.32it/s]Extractor Predicting: 57it [00:42,  1.35it/s]Extractor Predicting: 58it [00:43,  1.35it/s]Extractor Predicting: 59it [00:44,  1.31it/s]Extractor Predicting: 60it [00:44,  1.30it/s]Extractor Predicting: 61it [00:45,  1.28it/s]Extractor Predicting: 62it [00:46,  1.28it/s]Extractor Predicting: 63it [00:47,  1.29it/s]Extractor Predicting: 64it [00:48,  1.29it/s]Extractor Predicting: 65it [00:48,  1.31it/s]Extractor Predicting: 66it [00:49,  1.32it/s]Extractor Predicting: 67it [00:50,  1.31it/s]Extractor Predicting: 68it [00:51,  1.33it/s]Extractor Predicting: 69it [00:51,  1.32it/s]Extractor Predicting: 70it [00:52,  1.32it/s]Extractor Predicting: 71it [00:53,  1.28it/s]Extractor Predicting: 72it [00:54,  1.31it/s]Extractor Predicting: 73it [00:54,  1.30it/s]Extractor Predicting: 74it [00:55,  1.32it/s]Extractor Predicting: 75it [00:56,  1.32it/s]Extractor Predicting: 76it [00:57,  1.32it/s]Extractor Predicting: 77it [00:57,  1.35it/s]Extractor Predicting: 78it [00:58,  1.32it/s]Extractor Predicting: 79it [00:59,  1.29it/s]Extractor Predicting: 80it [01:00,  1.29it/s]Extractor Predicting: 81it [01:01,  1.28it/s]Extractor Predicting: 82it [01:01,  1.28it/s]Extractor Predicting: 83it [01:02,  1.31it/s]Extractor Predicting: 84it [01:03,  1.30it/s]Extractor Predicting: 85it [01:04,  1.29it/s]Extractor Predicting: 86it [01:04,  1.30it/s]Extractor Predicting: 87it [01:05,  1.32it/s]Extractor Predicting: 88it [01:06,  1.34it/s]Extractor Predicting: 89it [01:07,  1.33it/s]Extractor Predicting: 90it [01:07,  1.37it/s]Extractor Predicting: 91it [01:08,  1.39it/s]Extractor Predicting: 92it [01:09,  1.40it/s]Extractor Predicting: 93it [01:09,  1.38it/s]Extractor Predicting: 94it [01:10,  1.39it/s]Extractor Predicting: 95it [01:11,  1.40it/s]Extractor Predicting: 96it [01:12,  1.39it/s]Extractor Predicting: 97it [01:12,  1.38it/s]Extractor Predicting: 98it [01:13,  1.35it/s]Extractor Predicting: 99it [01:14,  1.33it/s]Extractor Predicting: 100it [01:15,  1.31it/s]Extractor Predicting: 101it [01:15,  1.36it/s]Extractor Predicting: 102it [01:16,  1.40it/s]Extractor Predicting: 103it [01:17,  1.39it/s]Extractor Predicting: 104it [01:17,  1.40it/s]Extractor Predicting: 105it [01:18,  1.39it/s]Extractor Predicting: 106it [01:19,  1.40it/s]Extractor Predicting: 107it [01:20,  1.39it/s]Extractor Predicting: 108it [01:20,  1.38it/s]Extractor Predicting: 109it [01:21,  1.38it/s]Extractor Predicting: 110it [01:22,  1.36it/s]Extractor Predicting: 111it [01:22,  1.40it/s]Extractor Predicting: 112it [01:23,  1.41it/s]Extractor Predicting: 113it [01:24,  1.45it/s]Extractor Predicting: 114it [01:25,  1.43it/s]Extractor Predicting: 115it [01:25,  1.33it/s]Extractor Predicting: 116it [01:26,  1.35it/s]Extractor Predicting: 117it [01:27,  1.34it/s]Extractor Predicting: 118it [01:28,  1.33it/s]Extractor Predicting: 119it [01:28,  1.31it/s]Extractor Predicting: 120it [01:29,  1.30it/s]Extractor Predicting: 121it [01:30,  1.34it/s]Extractor Predicting: 122it [01:31,  1.34it/s]Extractor Predicting: 123it [01:31,  1.34it/s]Extractor Predicting: 124it [01:32,  1.32it/s]Extractor Predicting: 125it [01:33,  1.31it/s]Extractor Predicting: 126it [01:34,  1.29it/s]Extractor Predicting: 127it [01:35,  1.30it/s]Extractor Predicting: 128it [01:35,  1.35it/s]Extractor Predicting: 129it [01:36,  1.32it/s]Extractor Predicting: 130it [01:37,  1.35it/s]Extractor Predicting: 131it [01:37,  1.34it/s]Extractor Predicting: 132it [01:38,  1.34it/s]Extractor Predicting: 133it [01:39,  1.28it/s]Extractor Predicting: 134it [01:40,  1.26it/s]Extractor Predicting: 135it [01:41,  1.28it/s]Extractor Predicting: 136it [01:41,  1.30it/s]Extractor Predicting: 137it [01:42,  1.32it/s]Extractor Predicting: 138it [01:43,  1.32it/s]Extractor Predicting: 139it [01:44,  1.31it/s]Extractor Predicting: 140it [01:44,  1.34it/s]Extractor Predicting: 141it [01:45,  1.32it/s]Extractor Predicting: 142it [01:46,  1.29it/s]Extractor Predicting: 143it [01:47,  1.31it/s]Extractor Predicting: 144it [01:47,  1.59it/s]Extractor Predicting: 144it [01:47,  1.34it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:06,515 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:06,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:06,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:06,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:06,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:51:07,300 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:51:07,301 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:51:07,559 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:51:08,601 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:51:08,602 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:10,372 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:10,386 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:10,386 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:10,386 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:10,386 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:51:10,704 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:51:10,705 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:51:11,006 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:51:11,173 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:51:11,173 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3871297242083759,
  "recall": 0.10865825688073394,
  "score": 0.16968882919185133,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19834
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19934, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.45it/s]Extractor Predicting: 2it [00:01,  1.27it/s]Extractor Predicting: 3it [00:02,  1.29it/s]Extractor Predicting: 4it [00:03,  1.29it/s]Extractor Predicting: 5it [00:03,  1.28it/s]Extractor Predicting: 6it [00:04,  1.29it/s]Extractor Predicting: 7it [00:05,  1.32it/s]Extractor Predicting: 8it [00:06,  1.31it/s]Extractor Predicting: 9it [00:06,  1.34it/s]Extractor Predicting: 10it [00:07,  1.30it/s]Extractor Predicting: 11it [00:08,  1.30it/s]Extractor Predicting: 12it [00:09,  1.30it/s]Extractor Predicting: 13it [00:09,  1.32it/s]Extractor Predicting: 14it [00:10,  1.32it/s]Extractor Predicting: 15it [00:11,  1.31it/s]Extractor Predicting: 16it [00:12,  1.31it/s]Extractor Predicting: 17it [00:13,  1.29it/s]Extractor Predicting: 18it [00:13,  1.31it/s]Extractor Predicting: 19it [00:14,  1.34it/s]Extractor Predicting: 20it [00:15,  1.34it/s]Extractor Predicting: 21it [00:15,  1.34it/s]Extractor Predicting: 22it [00:16,  1.37it/s]Extractor Predicting: 23it [00:17,  1.38it/s]Extractor Predicting: 24it [00:18,  1.35it/s]Extractor Predicting: 25it [00:18,  1.32it/s]Extractor Predicting: 26it [00:19,  1.35it/s]Extractor Predicting: 27it [00:20,  1.33it/s]Extractor Predicting: 28it [00:21,  1.32it/s]Extractor Predicting: 29it [00:21,  1.32it/s]Extractor Predicting: 30it [00:22,  1.29it/s]Extractor Predicting: 31it [00:23,  1.35it/s]Extractor Predicting: 32it [00:24,  1.40it/s]Extractor Predicting: 33it [00:24,  1.43it/s]Extractor Predicting: 34it [00:25,  1.45it/s]Extractor Predicting: 35it [00:26,  1.47it/s]Extractor Predicting: 36it [00:26,  1.47it/s]Extractor Predicting: 37it [00:27,  1.48it/s]Extractor Predicting: 38it [00:28,  1.50it/s]Extractor Predicting: 39it [00:28,  1.53it/s]Extractor Predicting: 40it [00:29,  1.55it/s]Extractor Predicting: 41it [00:29,  1.53it/s]Extractor Predicting: 42it [00:30,  1.54it/s]Extractor Predicting: 43it [00:31,  1.53it/s]Extractor Predicting: 44it [00:31,  1.53it/s]Extractor Predicting: 45it [00:32,  1.53it/s]Extractor Predicting: 46it [00:33,  1.48it/s]Extractor Predicting: 47it [00:33,  1.51it/s]Extractor Predicting: 48it [00:34,  1.55it/s]Extractor Predicting: 49it [00:35,  1.56it/s]Extractor Predicting: 50it [00:35,  1.54it/s]Extractor Predicting: 51it [00:36,  1.55it/s]Extractor Predicting: 52it [00:37,  1.52it/s]Extractor Predicting: 53it [00:37,  1.52it/s]Extractor Predicting: 54it [00:38,  1.50it/s]Extractor Predicting: 55it [00:39,  1.47it/s]Extractor Predicting: 56it [00:39,  1.45it/s]Extractor Predicting: 57it [00:40,  1.45it/s]Extractor Predicting: 58it [00:41,  1.43it/s]Extractor Predicting: 59it [00:42,  1.40it/s]Extractor Predicting: 60it [00:42,  1.36it/s]Extractor Predicting: 61it [00:43,  1.31it/s]Extractor Predicting: 62it [00:44,  1.27it/s]Extractor Predicting: 63it [00:45,  1.29it/s]Extractor Predicting: 64it [00:46,  1.27it/s]Extractor Predicting: 65it [00:46,  1.30it/s]Extractor Predicting: 66it [00:47,  1.30it/s]Extractor Predicting: 67it [00:48,  1.31it/s]Extractor Predicting: 68it [00:49,  1.28it/s]Extractor Predicting: 69it [00:50,  1.26it/s]Extractor Predicting: 70it [00:50,  1.25it/s]Extractor Predicting: 71it [00:51,  1.23it/s]Extractor Predicting: 72it [00:52,  1.26it/s]Extractor Predicting: 73it [00:53,  1.25it/s]Extractor Predicting: 74it [00:54,  1.27it/s]Extractor Predicting: 75it [00:54,  1.28it/s]Extractor Predicting: 76it [00:55,  1.29it/s]Extractor Predicting: 77it [00:56,  1.28it/s]Extractor Predicting: 78it [00:57,  1.26it/s]Extractor Predicting: 79it [00:57,  1.25it/s]Extractor Predicting: 80it [00:58,  1.26it/s]Extractor Predicting: 81it [00:59,  1.25it/s]Extractor Predicting: 82it [01:00,  1.25it/s]Extractor Predicting: 83it [01:01,  1.25it/s]Extractor Predicting: 84it [01:01,  1.28it/s]Extractor Predicting: 85it [01:02,  1.26it/s]Extractor Predicting: 86it [01:03,  1.23it/s]Extractor Predicting: 87it [01:04,  1.24it/s]Extractor Predicting: 88it [01:05,  1.26it/s]Extractor Predicting: 89it [01:05,  1.27it/s]Extractor Predicting: 90it [01:06,  1.30it/s]Extractor Predicting: 91it [01:07,  1.27it/s]Extractor Predicting: 92it [01:08,  1.25it/s]Extractor Predicting: 93it [01:09,  1.18it/s]Extractor Predicting: 94it [01:10,  1.21it/s]Extractor Predicting: 95it [01:10,  1.25it/s]Extractor Predicting: 96it [01:11,  1.29it/s]Extractor Predicting: 97it [01:12,  1.30it/s]Extractor Predicting: 98it [01:12,  1.34it/s]Extractor Predicting: 99it [01:13,  1.33it/s]Extractor Predicting: 100it [01:14,  1.32it/s]Extractor Predicting: 101it [01:15,  1.38it/s]Extractor Predicting: 102it [01:15,  1.38it/s]Extractor Predicting: 103it [01:16,  1.35it/s]Extractor Predicting: 104it [01:17,  1.31it/s]Extractor Predicting: 105it [01:18,  1.31it/s]Extractor Predicting: 106it [01:18,  1.30it/s]Extractor Predicting: 107it [01:19,  1.32it/s]Extractor Predicting: 108it [01:20,  1.31it/s]Extractor Predicting: 109it [01:21,  1.30it/s]Extractor Predicting: 110it [01:22,  1.28it/s]Extractor Predicting: 111it [01:22,  1.31it/s]Extractor Predicting: 112it [01:23,  1.29it/s]Extractor Predicting: 113it [01:24,  1.32it/s]Extractor Predicting: 114it [01:25,  1.29it/s]Extractor Predicting: 115it [01:25,  1.30it/s]Extractor Predicting: 116it [01:26,  1.35it/s]Extractor Predicting: 117it [01:27,  1.34it/s]Extractor Predicting: 118it [01:27,  1.38it/s]Extractor Predicting: 119it [01:28,  1.40it/s]Extractor Predicting: 120it [01:29,  1.42it/s]Extractor Predicting: 121it [01:30,  1.45it/s]Extractor Predicting: 122it [01:30,  1.44it/s]Extractor Predicting: 123it [01:31,  1.45it/s]Extractor Predicting: 124it [01:32,  1.46it/s]Extractor Predicting: 125it [01:32,  1.47it/s]Extractor Predicting: 126it [01:33,  1.45it/s]Extractor Predicting: 127it [01:34,  1.44it/s]Extractor Predicting: 128it [01:34,  1.49it/s]Extractor Predicting: 129it [01:35,  1.48it/s]Extractor Predicting: 130it [01:36,  1.55it/s]Extractor Predicting: 131it [01:36,  1.57it/s]Extractor Predicting: 132it [01:37,  1.53it/s]Extractor Predicting: 133it [01:38,  1.46it/s]Extractor Predicting: 134it [01:38,  1.45it/s]Extractor Predicting: 135it [01:39,  1.47it/s]Extractor Predicting: 136it [01:40,  1.50it/s]Extractor Predicting: 137it [01:40,  1.50it/s]Extractor Predicting: 138it [01:41,  1.49it/s]Extractor Predicting: 139it [01:42,  1.46it/s]Extractor Predicting: 140it [01:42,  1.49it/s]Extractor Predicting: 141it [01:43,  1.48it/s]Extractor Predicting: 142it [01:44,  1.53it/s]Extractor Predicting: 143it [01:44,  1.51it/s]Extractor Predicting: 144it [01:45,  1.49it/s]Extractor Predicting: 145it [01:46,  1.47it/s]Extractor Predicting: 146it [01:46,  1.43it/s]Extractor Predicting: 147it [01:47,  1.41it/s]Extractor Predicting: 148it [01:48,  1.41it/s]Extractor Predicting: 149it [01:49,  1.38it/s]Extractor Predicting: 150it [01:49,  1.37it/s]Extractor Predicting: 151it [01:50,  1.37it/s]Extractor Predicting: 152it [01:51,  1.38it/s]Extractor Predicting: 153it [01:52,  1.34it/s]Extractor Predicting: 154it [01:52,  1.37it/s]Extractor Predicting: 155it [01:53,  1.39it/s]Extractor Predicting: 156it [01:54,  1.36it/s]Extractor Predicting: 157it [01:54,  1.36it/s]Extractor Predicting: 158it [01:55,  1.34it/s]Extractor Predicting: 159it [01:56,  1.34it/s]Extractor Predicting: 160it [01:57,  1.33it/s]Extractor Predicting: 161it [01:58,  1.33it/s]Extractor Predicting: 162it [01:58,  1.31it/s]Extractor Predicting: 163it [01:59,  1.34it/s]Extractor Predicting: 164it [02:00,  1.34it/s]Extractor Predicting: 165it [02:01,  1.32it/s]Extractor Predicting: 166it [02:01,  1.29it/s]Extractor Predicting: 167it [02:02,  1.31it/s]Extractor Predicting: 168it [02:03,  1.31it/s]Extractor Predicting: 169it [02:04,  1.33it/s]Extractor Predicting: 170it [02:04,  1.32it/s]Extractor Predicting: 171it [02:05,  1.32it/s]Extractor Predicting: 172it [02:06,  1.34it/s]Extractor Predicting: 173it [02:07,  1.32it/s]Extractor Predicting: 174it [02:07,  1.32it/s]Extractor Predicting: 175it [02:08,  1.34it/s]Extractor Predicting: 176it [02:09,  1.33it/s]Extractor Predicting: 177it [02:10,  1.31it/s]Extractor Predicting: 178it [02:10,  1.32it/s]Extractor Predicting: 179it [02:11,  1.32it/s]Extractor Predicting: 180it [02:12,  1.32it/s]Extractor Predicting: 181it [02:13,  1.32it/s]Extractor Predicting: 182it [02:13,  1.32it/s]Extractor Predicting: 183it [02:14,  1.32it/s]Extractor Predicting: 184it [02:15,  1.27it/s]Extractor Predicting: 185it [02:16,  1.30it/s]Extractor Predicting: 186it [02:17,  1.29it/s]Extractor Predicting: 187it [02:17,  1.31it/s]Extractor Predicting: 188it [02:18,  1.31it/s]Extractor Predicting: 189it [02:19,  1.33it/s]Extractor Predicting: 190it [02:20,  1.33it/s]Extractor Predicting: 191it [02:20,  1.35it/s]Extractor Predicting: 192it [02:21,  1.23it/s]Extractor Predicting: 193it [02:22,  1.28it/s]Extractor Predicting: 194it [02:23,  1.28it/s]Extractor Predicting: 195it [02:23,  1.30it/s]Extractor Predicting: 196it [02:24,  1.31it/s]Extractor Predicting: 197it [02:25,  1.32it/s]Extractor Predicting: 198it [02:26,  1.29it/s]Extractor Predicting: 199it [02:27,  1.30it/s]Extractor Predicting: 200it [02:27,  1.31it/s]Extractor Predicting: 201it [02:28,  1.35it/s]Extractor Predicting: 202it [02:29,  1.33it/s]Extractor Predicting: 203it [02:29,  1.36it/s]Extractor Predicting: 204it [02:30,  1.34it/s]Extractor Predicting: 205it [02:31,  1.33it/s]Extractor Predicting: 206it [02:32,  1.38it/s]Extractor Predicting: 207it [02:32,  1.38it/s]Extractor Predicting: 208it [02:33,  1.39it/s]Extractor Predicting: 209it [02:34,  1.38it/s]Extractor Predicting: 210it [02:35,  1.37it/s]Extractor Predicting: 211it [02:35,  1.35it/s]Extractor Predicting: 212it [02:36,  1.32it/s]Extractor Predicting: 213it [02:37,  1.33it/s]Extractor Predicting: 214it [02:38,  1.33it/s]Extractor Predicting: 215it [02:38,  1.31it/s]Extractor Predicting: 216it [02:39,  1.30it/s]Extractor Predicting: 217it [02:40,  1.27it/s]Extractor Predicting: 218it [02:41,  1.29it/s]Extractor Predicting: 219it [02:42,  1.30it/s]Extractor Predicting: 220it [02:42,  1.33it/s]Extractor Predicting: 221it [02:43,  1.33it/s]Extractor Predicting: 222it [02:44,  1.33it/s]Extractor Predicting: 223it [02:44,  1.34it/s]Extractor Predicting: 224it [02:45,  1.33it/s]Extractor Predicting: 225it [02:46,  1.31it/s]Extractor Predicting: 226it [02:47,  1.33it/s]Extractor Predicting: 227it [02:48,  1.31it/s]Extractor Predicting: 228it [02:48,  1.35it/s]Extractor Predicting: 229it [02:49,  1.32it/s]Extractor Predicting: 230it [02:50,  1.34it/s]Extractor Predicting: 231it [02:51,  1.30it/s]Extractor Predicting: 232it [02:51,  1.30it/s]Extractor Predicting: 233it [02:52,  1.31it/s]Extractor Predicting: 234it [02:53,  1.31it/s]Extractor Predicting: 235it [02:54,  1.33it/s]Extractor Predicting: 236it [02:54,  1.34it/s]Extractor Predicting: 237it [02:55,  1.28it/s]Extractor Predicting: 238it [02:56,  1.26it/s]Extractor Predicting: 239it [02:57,  1.26it/s]Extractor Predicting: 240it [02:58,  1.28it/s]Extractor Predicting: 241it [02:58,  1.27it/s]Extractor Predicting: 242it [02:59,  1.29it/s]Extractor Predicting: 243it [03:00,  1.29it/s]Extractor Predicting: 244it [03:01,  1.30it/s]Extractor Predicting: 245it [03:01,  1.30it/s]Extractor Predicting: 246it [03:02,  1.27it/s]Extractor Predicting: 247it [03:03,  1.24it/s]Extractor Predicting: 248it [03:04,  1.25it/s]Extractor Predicting: 249it [03:05,  1.30it/s]Extractor Predicting: 250it [03:05,  1.33it/s]Extractor Predicting: 251it [03:06,  1.31it/s]Extractor Predicting: 252it [03:07,  1.34it/s]Extractor Predicting: 253it [03:08,  1.34it/s]Extractor Predicting: 254it [03:08,  1.31it/s]Extractor Predicting: 255it [03:09,  1.30it/s]Extractor Predicting: 256it [03:10,  1.29it/s]Extractor Predicting: 257it [03:11,  1.29it/s]Extractor Predicting: 258it [03:11,  1.29it/s]Extractor Predicting: 259it [03:12,  1.31it/s]Extractor Predicting: 260it [03:13,  1.31it/s]Extractor Predicting: 261it [03:14,  1.34it/s]Extractor Predicting: 262it [03:14,  1.34it/s]Extractor Predicting: 263it [03:15,  1.33it/s]Extractor Predicting: 264it [03:16,  1.34it/s]Extractor Predicting: 265it [03:17,  1.31it/s]Extractor Predicting: 266it [03:17,  1.32it/s]Extractor Predicting: 267it [03:18,  1.34it/s]Extractor Predicting: 268it [03:19,  1.34it/s]Extractor Predicting: 269it [03:20,  1.34it/s]Extractor Predicting: 270it [03:20,  1.31it/s]Extractor Predicting: 271it [03:21,  1.35it/s]Extractor Predicting: 272it [03:22,  1.23it/s]Extractor Predicting: 273it [03:23,  1.28it/s]Extractor Predicting: 274it [03:24,  1.30it/s]Extractor Predicting: 275it [03:24,  1.33it/s]Extractor Predicting: 276it [03:25,  1.32it/s]Extractor Predicting: 277it [03:26,  1.33it/s]Extractor Predicting: 278it [03:27,  1.32it/s]Extractor Predicting: 279it [03:27,  1.33it/s]Extractor Predicting: 280it [03:28,  1.32it/s]Extractor Predicting: 281it [03:29,  1.35it/s]Extractor Predicting: 282it [03:30,  1.35it/s]Extractor Predicting: 283it [03:30,  1.28it/s]Extractor Predicting: 284it [03:31,  1.49it/s]Extractor Predicting: 284it [03:31,  1.34it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:49,980 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:49,989 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:49,989 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:49,989 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:49,989 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:54:50,369 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:54:50,370 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:54:50,625 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:54:51,706 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:54:51,706 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:54,206 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:54,211 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:54,211 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:54,211 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:54:54,211 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:54:54,925 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:54:54,926 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:54:55,502 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:54:55,676 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:54:55,676 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.597860962566845,
  "recall": 0.0821817112613937,
  "score": 0.1445004523717203,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1045
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1145, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.32it/s]Extractor Predicting: 2it [00:01,  1.19it/s]Extractor Predicting: 3it [00:02,  1.21it/s]Extractor Predicting: 4it [00:03,  1.24it/s]Extractor Predicting: 5it [00:03,  1.69it/s]Extractor Predicting: 5it [00:03,  1.45it/s]
[INFO|configuration_utils.py:515] 2023-08-28 20:54:59,769 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:54:59,770 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 20:54:59,779 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:54:59,780 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 20:54:59,793 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 20:55:06,537 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 20:55:06,541 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 20:55:06,552 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:55:06,553 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 20:55:06,562 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:55:06,567 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:55:06,567 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:55:06,567 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:55:06,567 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:55:06,567 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:55:06,567 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5,
  "recall": 0.005050505050505051,
  "score": 0.01,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 20:55:06,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:07,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:08,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:08,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:09,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:10,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:11,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:11,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:12,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:13,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:13,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:14,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:15,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:16,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:16,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:17,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:18,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:18,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:19,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:20,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:21,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:22,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:22,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:23,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:17<04:07, 17.69s/it][WARNING|generation_utils.py:914] 2023-08-28 20:55:24,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:25,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:25,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:26,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:27,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:28,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:28,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:29,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:30,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:30,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:31,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:32,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:32,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:33,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:34,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:35,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:35,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:36,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:36,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:37,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:38,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:32<03:24, 15.76s/it][WARNING|generation_utils.py:914] 2023-08-28 20:55:38,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:39,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:40,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:41,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:42,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:43,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:43,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:44,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:45,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:45,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:46,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:47,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:48,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:48,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:49,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:50,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:51,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:51,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:52,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:52,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:53,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:47<03:08, 15.71s/it][WARNING|generation_utils.py:914] 2023-08-28 20:55:54,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:55,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:56,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:56,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:57,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:58,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:58,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:59,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:00,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:01,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:02,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:02,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:03,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:04,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:05,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:06,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:06,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:07,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:08,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:09,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:10,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:10,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:04<02:57, 16.10s/it][WARNING|generation_utils.py:914] 2023-08-28 20:56:11,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:12,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:12,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:13,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:14,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:14,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:15,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:15,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:16,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:17,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:17,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:18,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:19,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:19,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:20,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:20,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:21,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:22,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:23,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:23,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:24,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:25,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:18<02:35, 15.53s/it][WARNING|generation_utils.py:914] 2023-08-28 20:56:25,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:26,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:27,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:27,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:28,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:28,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:29,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:30,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:30,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:31,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:31,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:32,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:33,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:34,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:34,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:35,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:35,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:36,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:37,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:38,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:38,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:39,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:33<02:15, 15.04s/it][WARNING|generation_utils.py:914] 2023-08-28 20:56:39,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:40,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:41,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:41,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:42,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:43,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:43,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:44,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:45,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:46,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:46,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:47,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:48,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:48,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:49,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:50,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:50,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:51,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:52,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:52,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:53,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:53,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:54,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:55,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:49<02:03, 15.38s/it][WARNING|generation_utils.py:914] 2023-08-28 20:56:56,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:56,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:57,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:58,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:59,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:56:59,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:00,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:01,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:02,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:02,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:03,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:05,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:05,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:06,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:07,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:08,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:09,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:09,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:10,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:11,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:12,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:13,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:14,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:14,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:08<01:56, 16.69s/it][WARNING|generation_utils.py:914] 2023-08-28 20:57:15,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:16,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:16,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:17,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:18,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:19,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:20,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:21,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:21,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:22,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:23,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:24,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:24,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:25,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:26,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:27,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:27,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:28,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:29,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:30,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:30,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:31,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:25<01:40, 16.76s/it][WARNING|generation_utils.py:914] 2023-08-28 20:57:32,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:33,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:33,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:34,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:35,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:35,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:36,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:37,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:38,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:38,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:39,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:40,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:40,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:41,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:42,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:43,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:43,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:44,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:45,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:45,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:46,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:40<01:20, 16.17s/it][WARNING|generation_utils.py:914] 2023-08-28 20:57:47,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:47,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:48,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:49,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:49,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:50,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:51,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:51,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:52,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:52,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:53,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:54,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:54,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:55,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:56,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:57,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:57,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:58,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:59,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:57:59,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:00,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:54<01:01, 15.49s/it][WARNING|generation_utils.py:914] 2023-08-28 20:58:01,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:02,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:02,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:03,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:04,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:04,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:05,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:06,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:07,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:07,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:08,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:09,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:10,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:11,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:11,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:12,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:13,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:13,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:14,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:15,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:16,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:17,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:11<00:47, 15.86s/it][WARNING|generation_utils.py:914] 2023-08-28 20:58:17,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:18,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:19,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:20,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:20,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:21,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:22,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:22,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:23,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:24,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:25,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:25,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:26,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:26,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:27,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:28,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:29,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:30,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:30,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:31,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:32,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:32,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:26<00:31, 15.74s/it][WARNING|generation_utils.py:914] 2023-08-28 20:58:33,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:34,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:34,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:35,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:36,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:36,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:37,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:38,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:39,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:39,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:40,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:41,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:42,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:43,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:43,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:44,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:45,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:45,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:46,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:47,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:47,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:48,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:42<00:15, 15.86s/it][WARNING|generation_utils.py:914] 2023-08-28 20:58:49,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:50,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:50,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:51,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:52,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:53,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:54,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:54,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:55,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:56,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:57,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:57,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:58,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:58:59,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:59:00,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:59:00,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:59:01,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:59:01,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:59:02,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:59:03,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:59:03,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:57<00:00, 15.61s/it]Generating: 100%|██████████| 15/15 [03:57<00:00, 15.84s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:09,530 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:09,611 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:09,611 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:09,611 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:09,611 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:59:09,911 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:59:09,912 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:59:10,585 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:59:11,635 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:59:11,635 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:14,457 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:14,467 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:14,467 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:14,467 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:14,467 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:59:14,833 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:59:14,834 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:59:15,090 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:59:15,264 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:59:15,264 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : genre . Context : Later in the year , the band formed the single " The Walking Dead " . Head Entity : The Walking Dead , Tail Entity : horror .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 563, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : genre .', 'success_rate': 0.796875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9285714285714286, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9315476190476191, 'errors': {'', '(\'ZX850\', \'manufacturer\', \'\', \'The company released a replacement battery as the basis of the compact compact compact compact eFotek ZX850 smartphone , known as " the ZX850 " .\')'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : participant in .', 'success_rate': 0.8565340909090909, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8792613636363636, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8735795454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 462, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8072916666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 378, 'raw': 480}
{'target': 600, 'success': 404, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 481, 'raw': 608}
{'target': 600, 'success': 509, 'raw': 640}
{'target': 600, 'success': 535, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 590, 'raw': 736}
{'target': 600, 'success': 615, 'raw': 768}
{'prompt': 'Relation : father .', 'success_rate': 0.80078125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8735795454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9166666666666666, 'errors': {''}}
['Relation : located in the administrative territorial entity . Context : The city is located in the municipality of Saint-Martin ( French : Saint-Martin de Saint-Martin ) in Saint - Marie ( French : Saint-Mémoire ) , on the Ritz & Marriott Hotel . Head Entity : Saint Martin de Saint - Martin , Tail Entity : Saint-Marie .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.8892045454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8735795454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8835227272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.9330357142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/synthetic/3_ext.jsonl'}}
estimate vocab size: 10266
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10366, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.21it/s]Extractor Estimating: 2it [00:01,  1.19it/s]Extractor Estimating: 3it [00:02,  1.28it/s]Extractor Estimating: 4it [00:03,  1.16it/s]Extractor Estimating: 5it [00:04,  1.21it/s]Extractor Estimating: 6it [00:04,  1.22it/s]Extractor Estimating: 7it [00:05,  1.28it/s]Extractor Estimating: 8it [00:06,  1.32it/s]Extractor Estimating: 9it [00:07,  1.37it/s]Extractor Estimating: 10it [00:07,  1.34it/s]Extractor Estimating: 11it [00:08,  1.33it/s]Extractor Estimating: 12it [00:09,  1.38it/s]Extractor Estimating: 13it [00:09,  1.36it/s]Extractor Estimating: 14it [00:10,  1.34it/s]Extractor Estimating: 15it [00:11,  1.31it/s]Extractor Estimating: 16it [00:12,  1.30it/s]Extractor Estimating: 17it [00:13,  1.35it/s]Extractor Estimating: 18it [00:13,  1.35it/s]Extractor Estimating: 19it [00:14,  1.33it/s]Extractor Estimating: 20it [00:15,  1.33it/s]Extractor Estimating: 21it [00:15,  1.35it/s]Extractor Estimating: 22it [00:16,  1.35it/s]Extractor Estimating: 23it [00:17,  1.36it/s]Extractor Estimating: 24it [00:18,  1.32it/s]Extractor Estimating: 25it [00:19,  1.18it/s]Extractor Estimating: 26it [00:19,  1.27it/s]Extractor Estimating: 27it [00:20,  1.33it/s]Extractor Estimating: 28it [00:21,  1.37it/s]Extractor Estimating: 29it [00:21,  1.42it/s]Extractor Estimating: 30it [00:22,  1.53it/s]Extractor Estimating: 31it [00:23,  1.60it/s]Extractor Estimating: 32it [00:23,  1.61it/s]Extractor Estimating: 33it [00:24,  1.63it/s]Extractor Estimating: 34it [00:24,  1.68it/s]Extractor Estimating: 35it [00:25,  1.69it/s]Extractor Estimating: 36it [00:26,  1.68it/s]Extractor Estimating: 37it [00:26,  1.64it/s]Extractor Estimating: 38it [00:27,  1.67it/s]Extractor Estimating: 39it [00:27,  1.68it/s]Extractor Estimating: 40it [00:28,  1.69it/s]Extractor Estimating: 41it [00:29,  1.66it/s]Extractor Estimating: 42it [00:29,  1.60it/s]Extractor Estimating: 43it [00:30,  1.60it/s]Extractor Estimating: 44it [00:30,  1.64it/s]Extractor Estimating: 45it [00:31,  1.66it/s]Extractor Estimating: 46it [00:32,  1.68it/s]Extractor Estimating: 47it [00:32,  1.66it/s]Extractor Estimating: 48it [00:33,  1.65it/s]Extractor Estimating: 49it [00:34,  1.53it/s]Extractor Estimating: 50it [00:34,  1.56it/s]Extractor Estimating: 51it [00:35,  1.54it/s]Extractor Estimating: 52it [00:35,  1.54it/s]Extractor Estimating: 53it [00:36,  1.47it/s]Extractor Estimating: 54it [00:37,  1.43it/s]Extractor Estimating: 55it [00:38,  1.45it/s]Extractor Estimating: 56it [00:38,  1.39it/s]Extractor Estimating: 57it [00:39,  1.40it/s]Extractor Estimating: 58it [00:40,  1.44it/s]Extractor Estimating: 59it [00:41,  1.41it/s]Extractor Estimating: 60it [00:41,  1.42it/s]Extractor Estimating: 61it [00:42,  1.44it/s]Extractor Estimating: 62it [00:43,  1.44it/s]Extractor Estimating: 63it [00:43,  1.40it/s]Extractor Estimating: 64it [00:44,  1.37it/s]Extractor Estimating: 65it [00:45,  1.38it/s]Extractor Estimating: 66it [00:46,  1.37it/s]Extractor Estimating: 67it [00:46,  1.46it/s]Extractor Estimating: 68it [00:47,  1.34it/s]Extractor Estimating: 69it [00:48,  1.38it/s]Extractor Estimating: 70it [00:48,  1.43it/s]Extractor Estimating: 71it [00:49,  1.43it/s]Extractor Estimating: 72it [00:50,  1.43it/s]Extractor Estimating: 73it [00:50,  1.49it/s]Extractor Estimating: 74it [00:51,  1.45it/s]Extractor Estimating: 75it [00:52,  1.42it/s]Extractor Estimating: 76it [00:53,  1.43it/s]Extractor Estimating: 77it [00:53,  1.45it/s]Extractor Estimating: 78it [00:54,  1.45it/s]Extractor Estimating: 79it [00:55,  1.48it/s]Extractor Estimating: 80it [00:55,  1.51it/s]Extractor Estimating: 81it [00:56,  1.44it/s]Extractor Estimating: 82it [00:57,  1.42it/s]Extractor Estimating: 83it [00:57,  1.44it/s]Extractor Estimating: 84it [00:58,  1.44it/s]Extractor Estimating: 85it [00:59,  1.47it/s]Extractor Estimating: 86it [00:59,  1.46it/s]Extractor Estimating: 87it [01:00,  1.43it/s]Extractor Estimating: 88it [01:01,  1.45it/s]Extractor Estimating: 89it [01:01,  1.44it/s]Extractor Estimating: 90it [01:02,  1.41it/s]Extractor Estimating: 91it [01:03,  1.47it/s]Extractor Estimating: 92it [01:03,  1.47it/s]Extractor Estimating: 93it [01:04,  1.46it/s]Extractor Estimating: 94it [01:05,  1.48it/s]Extractor Estimating: 95it [01:06,  1.39it/s]Extractor Estimating: 96it [01:06,  1.42it/s]Extractor Estimating: 97it [01:07,  1.43it/s]Extractor Estimating: 98it [01:08,  1.42it/s]Extractor Estimating: 99it [01:08,  1.47it/s]Extractor Estimating: 100it [01:09,  1.46it/s]Extractor Estimating: 101it [01:10,  1.49it/s]Extractor Estimating: 102it [01:10,  1.53it/s]Extractor Estimating: 103it [01:11,  1.57it/s]Extractor Estimating: 104it [01:12,  1.57it/s]Extractor Estimating: 105it [01:12,  1.60it/s]Extractor Estimating: 106it [01:13,  1.61it/s]Extractor Estimating: 107it [01:13,  1.59it/s]Extractor Estimating: 108it [01:14,  1.60it/s]Extractor Estimating: 109it [01:15,  1.59it/s]Extractor Estimating: 110it [01:15,  1.63it/s]Extractor Estimating: 111it [01:16,  1.63it/s]Extractor Estimating: 112it [01:16,  1.65it/s]Extractor Estimating: 113it [01:17,  1.70it/s]Extractor Estimating: 114it [01:18,  1.66it/s]Extractor Estimating: 115it [01:18,  1.69it/s]Extractor Estimating: 116it [01:19,  1.63it/s]Extractor Estimating: 117it [01:19,  1.62it/s]Extractor Estimating: 118it [01:20,  1.65it/s]Extractor Estimating: 119it [01:21,  1.59it/s]Extractor Estimating: 120it [01:21,  1.53it/s]Extractor Estimating: 121it [01:22,  1.52it/s]Extractor Estimating: 122it [01:23,  1.41it/s]Extractor Estimating: 123it [01:24,  1.47it/s]Extractor Estimating: 124it [01:24,  1.53it/s]Extractor Estimating: 125it [01:25,  1.55it/s]Extractor Estimating: 126it [01:25,  1.61it/s]Extractor Estimating: 127it [01:26,  1.60it/s]Extractor Estimating: 128it [01:27,  1.62it/s]Extractor Estimating: 129it [01:27,  1.67it/s]Extractor Estimating: 130it [01:28,  1.71it/s]Extractor Estimating: 131it [01:28,  1.71it/s]Extractor Estimating: 132it [01:29,  1.72it/s]Extractor Estimating: 133it [01:29,  1.66it/s]Extractor Estimating: 134it [01:30,  1.71it/s]Extractor Estimating: 135it [01:31,  1.76it/s]Extractor Estimating: 136it [01:31,  1.79it/s]Extractor Estimating: 137it [01:32,  1.76it/s]Extractor Estimating: 138it [01:32,  1.72it/s]Extractor Estimating: 139it [01:33,  1.67it/s]Extractor Estimating: 140it [01:34,  1.64it/s]Extractor Estimating: 141it [01:34,  1.69it/s]Extractor Estimating: 142it [01:35,  1.64it/s]Extractor Estimating: 143it [01:35,  1.71it/s]Extractor Estimating: 144it [01:36,  1.68it/s]Extractor Estimating: 145it [01:36,  1.72it/s]Extractor Estimating: 146it [01:37,  1.68it/s]Extractor Estimating: 147it [01:38,  1.70it/s]Extractor Estimating: 148it [01:38,  1.65it/s]Extractor Estimating: 149it [01:39,  1.63it/s]Extractor Estimating: 150it [01:40,  1.66it/s]Extractor Estimating: 151it [01:40,  1.63it/s]Extractor Estimating: 152it [01:41,  1.67it/s]Extractor Estimating: 153it [01:41,  1.62it/s]Extractor Estimating: 154it [01:42,  1.61it/s]Extractor Estimating: 155it [01:43,  1.59it/s]Extractor Estimating: 156it [01:43,  1.59it/s]Extractor Estimating: 157it [01:44,  1.57it/s]Extractor Estimating: 158it [01:45,  1.54it/s]Extractor Estimating: 159it [01:45,  1.55it/s]Extractor Estimating: 160it [01:46,  1.52it/s]Extractor Estimating: 161it [01:47,  1.58it/s]Extractor Estimating: 162it [01:47,  1.60it/s]Extractor Estimating: 163it [01:48,  1.63it/s]Extractor Estimating: 164it [01:48,  1.62it/s]Extractor Estimating: 165it [01:49,  1.65it/s]Extractor Estimating: 166it [01:50,  1.61it/s]Extractor Estimating: 167it [01:50,  1.55it/s]Extractor Estimating: 168it [01:51,  1.58it/s]Extractor Estimating: 169it [01:52,  1.58it/s]Extractor Estimating: 170it [01:52,  1.58it/s]Extractor Estimating: 171it [01:53,  1.61it/s]Extractor Estimating: 172it [01:53,  1.63it/s]Extractor Estimating: 173it [01:54,  1.62it/s]Extractor Estimating: 174it [01:55,  1.64it/s]Extractor Estimating: 175it [01:55,  1.59it/s]Extractor Estimating: 176it [01:56,  1.57it/s]Extractor Estimating: 177it [01:57,  1.48it/s]Extractor Estimating: 178it [01:57,  1.46it/s]Extractor Estimating: 179it [01:58,  1.43it/s]Extractor Estimating: 180it [01:59,  1.41it/s]Extractor Estimating: 181it [02:00,  1.38it/s]Extractor Estimating: 182it [02:00,  1.42it/s]Extractor Estimating: 183it [02:01,  1.43it/s]Extractor Estimating: 184it [02:02,  1.45it/s]Extractor Estimating: 185it [02:02,  1.43it/s]Extractor Estimating: 186it [02:03,  1.46it/s]Extractor Estimating: 187it [02:04,  1.45it/s]Extractor Estimating: 188it [02:04,  1.49it/s]Extractor Estimating: 189it [02:05,  1.41it/s]Extractor Estimating: 190it [02:06,  1.43it/s]Extractor Estimating: 191it [02:06,  1.46it/s]Extractor Estimating: 192it [02:07,  1.47it/s]Extractor Estimating: 193it [02:08,  1.48it/s]Extractor Estimating: 194it [02:08,  1.47it/s]Extractor Estimating: 195it [02:09,  1.51it/s]Extractor Estimating: 196it [02:10,  1.51it/s]Extractor Estimating: 197it [02:10,  1.55it/s]Extractor Estimating: 198it [02:11,  1.50it/s]Extractor Estimating: 199it [02:12,  1.45it/s]Extractor Estimating: 200it [02:12,  1.48it/s]Extractor Estimating: 201it [02:13,  1.47it/s]Extractor Estimating: 202it [02:14,  1.47it/s]Extractor Estimating: 203it [02:15,  1.42it/s]Extractor Estimating: 204it [02:15,  1.45it/s]Extractor Estimating: 205it [02:16,  1.37it/s]Extractor Estimating: 206it [02:17,  1.39it/s]Extractor Estimating: 207it [02:17,  1.42it/s]Extractor Estimating: 208it [02:18,  1.41it/s]Extractor Estimating: 209it [02:19,  1.43it/s]Extractor Estimating: 210it [02:19,  1.48it/s]Extractor Estimating: 211it [02:20,  1.38it/s]Extractor Estimating: 212it [02:21,  1.35it/s]Extractor Estimating: 213it [02:22,  1.40it/s]Extractor Estimating: 214it [02:22,  1.41it/s]Extractor Estimating: 215it [02:23,  1.46it/s]Extractor Estimating: 216it [02:24,  1.42it/s]Extractor Estimating: 217it [02:24,  1.44it/s]Extractor Estimating: 218it [02:25,  1.44it/s]Extractor Estimating: 219it [02:26,  1.26it/s]Extractor Estimating: 220it [02:27,  1.32it/s]Extractor Estimating: 221it [02:28,  1.34it/s]Extractor Estimating: 222it [02:28,  1.40it/s]Extractor Estimating: 223it [02:29,  1.39it/s]Extractor Estimating: 224it [02:30,  1.38it/s]Extractor Estimating: 225it [02:30,  1.42it/s]Extractor Estimating: 226it [02:31,  1.43it/s]Extractor Estimating: 227it [02:32,  1.46it/s]Extractor Estimating: 228it [02:32,  1.46it/s]Extractor Estimating: 229it [02:33,  1.45it/s]Extractor Estimating: 230it [02:34,  1.50it/s]Extractor Estimating: 231it [02:34,  1.53it/s]Extractor Estimating: 232it [02:35,  1.48it/s]Extractor Estimating: 233it [02:36,  1.53it/s]Extractor Estimating: 234it [02:36,  1.51it/s]Extractor Estimating: 235it [02:37,  1.56it/s]Extractor Estimating: 236it [02:38,  1.56it/s]Extractor Estimating: 237it [02:38,  1.51it/s]Extractor Estimating: 238it [02:39,  1.52it/s]Extractor Estimating: 239it [02:40,  1.49it/s]Extractor Estimating: 240it [02:40,  1.50it/s]Extractor Estimating: 241it [02:41,  1.49it/s]Extractor Estimating: 242it [02:42,  1.45it/s]Extractor Estimating: 243it [02:42,  1.50it/s]Extractor Estimating: 244it [02:43,  1.47it/s]Extractor Estimating: 245it [02:44,  1.46it/s]Extractor Estimating: 246it [02:44,  1.47it/s]Extractor Estimating: 247it [02:45,  1.48it/s]Extractor Estimating: 248it [02:46,  1.49it/s]Extractor Estimating: 249it [02:46,  1.51it/s]Extractor Estimating: 250it [02:47,  1.45it/s]Extractor Estimating: 251it [02:48,  1.48it/s]Extractor Estimating: 252it [02:48,  1.49it/s]Extractor Estimating: 253it [02:49,  1.50it/s]Extractor Estimating: 254it [02:50,  1.55it/s]Extractor Estimating: 255it [02:50,  1.56it/s]Extractor Estimating: 256it [02:51,  1.56it/s]Extractor Estimating: 257it [02:52,  1.58it/s]Extractor Estimating: 258it [02:52,  1.55it/s]Extractor Estimating: 259it [02:53,  1.53it/s]Extractor Estimating: 260it [02:53,  1.55it/s]Extractor Estimating: 261it [02:54,  1.54it/s]Extractor Estimating: 262it [02:55,  1.52it/s]Extractor Estimating: 263it [02:55,  1.53it/s]Extractor Estimating: 264it [02:56,  1.55it/s]Extractor Estimating: 265it [02:57,  1.39it/s]Extractor Estimating: 266it [02:58,  1.38it/s]Extractor Estimating: 267it [02:58,  1.39it/s]Extractor Estimating: 268it [02:59,  1.46it/s]Extractor Estimating: 269it [03:00,  1.37it/s]Extractor Estimating: 270it [03:01,  1.43it/s]Extractor Estimating: 271it [03:01,  1.49it/s]Extractor Estimating: 272it [03:02,  1.51it/s]Extractor Estimating: 273it [03:02,  1.47it/s]Extractor Estimating: 274it [03:03,  1.52it/s]Extractor Estimating: 275it [03:04,  1.53it/s]Extractor Estimating: 276it [03:04,  1.56it/s]Extractor Estimating: 277it [03:05,  1.51it/s]Extractor Estimating: 278it [03:06,  1.53it/s]Extractor Estimating: 279it [03:06,  1.52it/s]Extractor Estimating: 280it [03:07,  1.53it/s]Extractor Estimating: 281it [03:08,  1.57it/s]Extractor Estimating: 282it [03:08,  1.59it/s]Extractor Estimating: 283it [03:09,  1.52it/s]Extractor Estimating: 284it [03:10,  1.51it/s]Extractor Estimating: 285it [03:10,  1.56it/s]Extractor Estimating: 286it [03:11,  1.59it/s]Extractor Estimating: 287it [03:11,  1.60it/s]Extractor Estimating: 288it [03:12,  1.63it/s]Extractor Estimating: 289it [03:13,  1.61it/s]Extractor Estimating: 290it [03:13,  1.53it/s]Extractor Estimating: 291it [03:14,  1.52it/s]Extractor Estimating: 292it [03:15,  1.56it/s]Extractor Estimating: 293it [03:15,  1.60it/s]Extractor Estimating: 294it [03:16,  1.62it/s]Extractor Estimating: 295it [03:16,  1.70it/s]Extractor Estimating: 296it [03:17,  1.59it/s]Extractor Estimating: 297it [03:18,  1.59it/s]Extractor Estimating: 298it [03:18,  1.56it/s]Extractor Estimating: 299it [03:19,  1.48it/s]Extractor Estimating: 300it [03:20,  1.51it/s]Extractor Estimating: 301it [03:20,  1.55it/s]Extractor Estimating: 302it [03:21,  1.58it/s]Extractor Estimating: 303it [03:22,  1.52it/s]Extractor Estimating: 304it [03:22,  1.54it/s]Extractor Estimating: 305it [03:23,  1.57it/s]Extractor Estimating: 306it [03:24,  1.59it/s]Extractor Estimating: 307it [03:24,  1.58it/s]Extractor Estimating: 308it [03:25,  1.53it/s]Extractor Estimating: 309it [03:25,  1.59it/s]Extractor Estimating: 310it [03:26,  1.57it/s]Extractor Estimating: 311it [03:27,  1.43it/s]Extractor Estimating: 312it [03:28,  1.42it/s]Extractor Estimating: 313it [03:28,  1.48it/s]Extractor Estimating: 314it [03:29,  1.47it/s]Extractor Estimating: 315it [03:30,  1.45it/s]Extractor Estimating: 316it [03:30,  1.54it/s]Extractor Estimating: 317it [03:31,  1.53it/s]Extractor Estimating: 318it [03:32,  1.49it/s]Extractor Estimating: 319it [03:32,  1.49it/s]Extractor Estimating: 320it [03:33,  1.45it/s]Extractor Estimating: 321it [03:34,  1.53it/s]Extractor Estimating: 322it [03:34,  1.53it/s]Extractor Estimating: 323it [03:35,  1.54it/s]Extractor Estimating: 324it [03:35,  1.56it/s]Extractor Estimating: 325it [03:36,  1.53it/s]Extractor Estimating: 326it [03:37,  1.54it/s]Extractor Estimating: 327it [03:37,  1.55it/s]Extractor Estimating: 328it [03:38,  1.50it/s]Extractor Estimating: 329it [03:39,  1.46it/s]Extractor Estimating: 330it [03:40,  1.42it/s]Extractor Estimating: 331it [03:40,  1.47it/s]Extractor Estimating: 332it [03:41,  1.45it/s]Extractor Estimating: 333it [03:42,  1.45it/s]Extractor Estimating: 334it [03:42,  1.50it/s]Extractor Estimating: 335it [03:43,  1.49it/s]Extractor Estimating: 336it [03:44,  1.54it/s]Extractor Estimating: 337it [03:44,  1.55it/s]Extractor Estimating: 338it [03:45,  1.39it/s]Extractor Estimating: 339it [03:46,  1.37it/s]Extractor Estimating: 340it [03:47,  1.41it/s]Extractor Estimating: 341it [03:47,  1.49it/s]Extractor Estimating: 342it [03:48,  1.52it/s]Extractor Estimating: 343it [03:48,  1.54it/s]Extractor Estimating: 344it [03:49,  1.49it/s]Extractor Estimating: 345it [03:50,  1.48it/s]Extractor Estimating: 346it [03:50,  1.51it/s]Extractor Estimating: 347it [03:51,  1.53it/s]Extractor Estimating: 348it [03:52,  1.55it/s]Extractor Estimating: 349it [03:52,  1.51it/s]Extractor Estimating: 350it [03:53,  1.45it/s]Extractor Estimating: 351it [03:54,  1.46it/s]Extractor Estimating: 352it [03:54,  1.49it/s]Extractor Estimating: 353it [03:55,  1.39it/s]Extractor Estimating: 354it [03:56,  1.36it/s]Extractor Estimating: 355it [03:57,  1.38it/s]Extractor Estimating: 356it [03:57,  1.40it/s]Extractor Estimating: 357it [03:58,  1.36it/s]Extractor Estimating: 358it [03:59,  1.40it/s]Extractor Estimating: 359it [03:59,  1.47it/s]Extractor Estimating: 360it [04:00,  1.48it/s]Extractor Estimating: 361it [04:01,  1.29it/s]Extractor Estimating: 362it [04:02,  1.33it/s]Extractor Estimating: 363it [04:03,  1.36it/s]Extractor Estimating: 364it [04:03,  1.40it/s]Extractor Estimating: 365it [04:04,  1.41it/s]Extractor Estimating: 366it [04:05,  1.40it/s]Extractor Estimating: 367it [04:05,  1.41it/s]Extractor Estimating: 368it [04:06,  1.45it/s]Extractor Estimating: 369it [04:07,  1.48it/s]Extractor Estimating: 370it [04:07,  1.50it/s]Extractor Estimating: 371it [04:08,  1.43it/s]Extractor Estimating: 372it [04:09,  1.44it/s]Extractor Estimating: 373it [04:09,  1.41it/s]Extractor Estimating: 374it [04:10,  1.44it/s]Extractor Estimating: 375it [04:11,  1.44it/s]Extractor Estimating: 375it [04:11,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:38,943 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:38,953 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:38,953 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:38,953 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:38,953 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:03:39,611 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:03:39,612 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:03:40,155 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:03:41,266 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:03:41,266 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:44,262 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:44,268 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:44,268 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:44,268 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:03:44,268 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:03:45,024 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:03:45,025 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:03:45,609 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:03:45,833 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:03:45,833 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 23:28:52,508 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 23:28:52,547 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7497 mean pseudo reward: 0.9250314093078056
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
train vocab size: 19305
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19405, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=19405, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.090, loss:679.4720
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.116, loss:661.8617
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.111, loss:625.5802
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.108, loss:636.8132
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.113, loss:626.6582
>> valid entity prec:0.5615, rec:0.5950, f1:0.5778
>> valid relation prec:0.2659, rec:0.0947, f1:0.1397
>> valid relation with NER prec:0.2659, rec:0.0947, f1:0.1397
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.578, loss:636.3817
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.093, loss:591.4582
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.115, loss:600.4557
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.113, loss:682.7128
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.091, loss:604.8007
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5410, rec:0.6242, f1:0.5796
>> valid relation prec:0.2076, rec:0.0827, f1:0.1183
>> valid relation with NER prec:0.2076, rec:0.0827, f1:0.1183
new max entity f1 on valid!
g_step 1100, step 161, avg_time 2.564, loss:627.6485
g_step 1200, step 261, avg_time 1.107, loss:618.1910
g_step 1300, step 48, avg_time 1.133, loss:581.2677
g_step 1400, step 148, avg_time 1.104, loss:586.5815
g_step 1500, step 248, avg_time 1.120, loss:584.8075
>> valid entity prec:0.5271, rec:0.6070, f1:0.5642
>> valid relation prec:0.2893, rec:0.0956, f1:0.1437
>> valid relation with NER prec:0.2893, rec:0.0956, f1:0.1437
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 35, avg_time 2.566, loss:547.3334
g_step 1700, step 135, avg_time 1.120, loss:544.3634
g_step 1800, step 235, avg_time 1.109, loss:577.9591
g_step 1900, step 22, avg_time 1.107, loss:545.3047
g_step 2000, step 122, avg_time 1.105, loss:522.9769
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5371, rec:0.6025, f1:0.5679
>> valid relation prec:0.2446, rec:0.0939, f1:0.1357
>> valid relation with NER prec:0.2446, rec:0.0939, f1:0.1357
g_step 2100, step 222, avg_time 2.571, loss:536.9966
g_step 2200, step 9, avg_time 1.122, loss:520.1106
g_step 2300, step 109, avg_time 1.110, loss:492.7946
g_step 2400, step 209, avg_time 1.120, loss:505.3908
g_step 2500, step 309, avg_time 1.107, loss:512.7385
>> valid entity prec:0.5419, rec:0.5488, f1:0.5453
>> valid relation prec:0.2150, rec:0.0824, f1:0.1191
>> valid relation with NER prec:0.2150, rec:0.0824, f1:0.1191
g_step 2600, step 96, avg_time 2.567, loss:455.7669
g_step 2700, step 196, avg_time 1.114, loss:481.0475
g_step 2800, step 296, avg_time 1.105, loss:493.0495
g_step 2900, step 83, avg_time 1.108, loss:454.0691
g_step 3000, step 183, avg_time 1.101, loss:453.7435
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5438, rec:0.6056, f1:0.5730
>> valid relation prec:0.1937, rec:0.0870, f1:0.1200
>> valid relation with NER prec:0.1937, rec:0.0870, f1:0.1200
g_step 3100, step 283, avg_time 2.589, loss:471.7148
g_step 3200, step 70, avg_time 1.105, loss:445.2681
g_step 3300, step 170, avg_time 1.108, loss:430.8958
g_step 3400, step 270, avg_time 1.121, loss:438.3245
g_step 3500, step 57, avg_time 1.108, loss:435.4341
>> valid entity prec:0.5199, rec:0.5913, f1:0.5533
>> valid relation prec:0.1919, rec:0.0801, f1:0.1130
>> valid relation with NER prec:0.1919, rec:0.0801, f1:0.1130
g_step 3600, step 157, avg_time 2.580, loss:426.3234
g_step 3700, step 257, avg_time 1.111, loss:429.6180
g_step 3800, step 44, avg_time 1.099, loss:413.0607
g_step 3900, step 144, avg_time 1.120, loss:399.1359
g_step 4000, step 244, avg_time 1.115, loss:420.6833
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5600, rec:0.5591, f1:0.5595
>> valid relation prec:0.1895, rec:0.0815, f1:0.1140
>> valid relation with NER prec:0.1895, rec:0.0815, f1:0.1140
g_step 4100, step 31, avg_time 2.578, loss:382.9443
g_step 4200, step 131, avg_time 1.103, loss:382.8659
g_step 4300, step 231, avg_time 1.119, loss:403.3198
g_step 4400, step 18, avg_time 1.113, loss:415.0612
g_step 4500, step 118, avg_time 1.122, loss:375.5282
>> valid entity prec:0.5368, rec:0.5285, f1:0.5326
>> valid relation prec:0.1831, rec:0.0746, f1:0.1060
>> valid relation with NER prec:0.1831, rec:0.0746, f1:0.1060
g_step 4600, step 218, avg_time 2.562, loss:375.9631
g_step 4700, step 5, avg_time 1.102, loss:381.9068
g_step 4800, step 105, avg_time 1.123, loss:354.6089
g_step 4900, step 205, avg_time 1.111, loss:369.2222
g_step 5000, step 305, avg_time 1.104, loss:388.2116
learning rate was adjusted to 0.0008
>> valid entity prec:0.5667, rec:0.5325, f1:0.5491
>> valid relation prec:0.2173, rec:0.0973, f1:0.1344
>> valid relation with NER prec:0.2173, rec:0.0973, f1:0.1344
g_step 5100, step 92, avg_time 2.575, loss:335.5477
g_step 5200, step 192, avg_time 1.104, loss:352.3651
g_step 5300, step 292, avg_time 1.115, loss:363.7614
g_step 5400, step 79, avg_time 1.120, loss:323.3348
g_step 5500, step 179, avg_time 1.108, loss:347.9646
>> valid entity prec:0.5493, rec:0.5383, f1:0.5437
>> valid relation prec:0.1877, rec:0.0844, f1:0.1164
>> valid relation with NER prec:0.1877, rec:0.0844, f1:0.1164
g_step 5600, step 279, avg_time 2.581, loss:358.1906
g_step 5700, step 66, avg_time 1.082, loss:321.6550
g_step 5800, step 166, avg_time 1.113, loss:331.3107
g_step 5900, step 266, avg_time 1.120, loss:343.6270
g_step 6000, step 53, avg_time 1.102, loss:315.5756
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5448, rec:0.4775, f1:0.5089
>> valid relation prec:0.1930, rec:0.0743, f1:0.1073
>> valid relation with NER prec:0.1930, rec:0.0743, f1:0.1073
g_step 6100, step 153, avg_time 2.554, loss:315.0626
g_step 6200, step 253, avg_time 1.114, loss:316.4661
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 23:28:52 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 23:28:52 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_23-28-52_ctolab11.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 23:28:53 - WARNING - datasets.builder -   Using custom data configuration default-7ccae59fd19223eb
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-7ccae59fd19223eb/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 23:28:53,997 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:28:53,998 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:28:53,998 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:28:53,999 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:28:54,011 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:28:54,016 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:28:54,016 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:28:54,016 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:28:54,016 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:28:54,016 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:28:54,017 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 23:28:54,155 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:28:57,223 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 23:28:57,229 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-7ccae59fd19223eb/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.05ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.01ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.45ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.69ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.82ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.89ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.94ba/s]100%|██████████| 8/8 [00:01<00:00,  5.87ba/s]100%|██████████| 8/8 [00:01<00:00,  4.97ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.10ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.33ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.80ba/s]100%|██████████| 4/4 [00:00<00:00,  4.93ba/s]100%|██████████| 4/4 [00:00<00:00,  4.38ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.27ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.56ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.84ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 11.07ba/s]100%|██████████| 8/8 [00:00<00:00, 11.61ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.92ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.23ba/s]100%|██████████| 4/4 [00:00<00:00, 11.56ba/s]
[INFO|trainer.py:414] 2023-08-28 23:29:01,241 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 23:29:01,253 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 23:29:01,253 >>   Num examples = 7499
[INFO|trainer.py:1149] 2023-08-28 23:29:01,253 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 23:29:01,253 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 23:29:01,253 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 23:29:01,253 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 23:29:01,253 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:58,  3.28it/s]  0%|          | 2/585 [00:00<02:53,  3.35it/s]  1%|          | 3/585 [00:00<02:52,  3.37it/s]  1%|          | 4/585 [00:01<02:51,  3.39it/s]  1%|          | 5/585 [00:01<02:50,  3.40it/s]  1%|          | 6/585 [00:01<02:50,  3.40it/s]  1%|          | 7/585 [00:02<02:49,  3.41it/s]  1%|▏         | 8/585 [00:02<02:49,  3.40it/s]  2%|▏         | 9/585 [00:02<02:49,  3.40it/s]  2%|▏         | 10/585 [00:02<02:48,  3.41it/s]  2%|▏         | 11/585 [00:03<02:48,  3.41it/s]  2%|▏         | 12/585 [00:03<02:47,  3.41it/s]  2%|▏         | 13/585 [00:03<02:47,  3.42it/s]  2%|▏         | 14/585 [00:04<02:47,  3.41it/s]  3%|▎         | 15/585 [00:04<02:46,  3.41it/s]  3%|▎         | 16/585 [00:04<02:46,  3.41it/s]  3%|▎         | 17/585 [00:04<02:46,  3.41it/s]  3%|▎         | 18/585 [00:05<02:46,  3.41it/s]  3%|▎         | 19/585 [00:05<02:47,  3.38it/s]  3%|▎         | 20/585 [00:05<02:46,  3.40it/s]  4%|▎         | 21/585 [00:06<02:45,  3.40it/s]  4%|▍         | 22/585 [00:06<02:45,  3.40it/s]  4%|▍         | 23/585 [00:06<02:44,  3.41it/s]  4%|▍         | 24/585 [00:07<02:44,  3.41it/s]  4%|▍         | 25/585 [00:07<02:44,  3.41it/s]  4%|▍         | 26/585 [00:07<02:43,  3.41it/s]  5%|▍         | 27/585 [00:07<02:43,  3.41it/s]  5%|▍         | 28/585 [00:08<02:43,  3.41it/s]  5%|▍         | 29/585 [00:08<02:42,  3.41it/s]  5%|▌         | 30/585 [00:08<02:47,  3.31it/s]  5%|▌         | 31/585 [00:09<02:46,  3.33it/s]  5%|▌         | 32/585 [00:09<02:44,  3.35it/s]  6%|▌         | 33/585 [00:09<02:43,  3.37it/s]  6%|▌         | 34/585 [00:10<02:42,  3.39it/s]  6%|▌         | 35/585 [00:10<02:42,  3.39it/s]  6%|▌         | 36/585 [00:10<02:41,  3.40it/s]  6%|▋         | 37/585 [00:10<02:41,  3.40it/s]  6%|▋         | 38/585 [00:11<02:40,  3.41it/s]  7%|▋         | 39/585 [00:11<02:40,  3.41it/s]  7%|▋         | 40/585 [00:11<02:39,  3.41it/s]  7%|▋         | 41/585 [00:12<02:40,  3.39it/s]  7%|▋         | 42/585 [00:12<02:39,  3.40it/s]  7%|▋         | 43/585 [00:12<02:39,  3.40it/s]  8%|▊         | 44/585 [00:12<02:39,  3.40it/s]  8%|▊         | 45/585 [00:13<02:38,  3.40it/s]  8%|▊         | 46/585 [00:13<02:38,  3.40it/s]  8%|▊         | 47/585 [00:13<02:37,  3.41it/s]  8%|▊         | 48/585 [00:14<02:37,  3.41it/s]  8%|▊         | 49/585 [00:14<02:37,  3.41it/s]  9%|▊         | 50/585 [00:14<02:36,  3.41it/s]  9%|▊         | 51/585 [00:15<02:36,  3.41it/s]  9%|▉         | 52/585 [00:15<02:42,  3.27it/s]  9%|▉         | 53/585 [00:15<02:40,  3.31it/s]  9%|▉         | 54/585 [00:15<02:38,  3.34it/s]  9%|▉         | 55/585 [00:16<02:37,  3.37it/s] 10%|▉         | 56/585 [00:16<02:36,  3.38it/s] 10%|▉         | 57/585 [00:16<02:35,  3.39it/s] 10%|▉         | 58/585 [00:17<02:35,  3.40it/s] 10%|█         | 59/585 [00:17<02:34,  3.40it/s] 10%|█         | 60/585 [00:17<02:34,  3.41it/s] 10%|█         | 61/585 [00:17<02:33,  3.41it/s] 11%|█         | 62/585 [00:18<02:33,  3.41it/s] 11%|█         | 63/585 [00:18<02:33,  3.39it/s] 11%|█         | 64/585 [00:18<02:33,  3.40it/s] 11%|█         | 65/585 [00:19<02:32,  3.40it/s] 11%|█▏        | 66/585 [00:19<02:32,  3.40it/s] 11%|█▏        | 67/585 [00:19<02:32,  3.40it/s] 12%|█▏        | 68/585 [00:20<02:31,  3.40it/s] 12%|█▏        | 69/585 [00:20<02:31,  3.40it/s] 12%|█▏        | 70/585 [00:20<02:31,  3.40it/s] 12%|█▏        | 71/585 [00:20<02:30,  3.41it/s] 12%|█▏        | 72/585 [00:21<02:30,  3.41it/s] 12%|█▏        | 73/585 [00:21<02:30,  3.41it/s] 13%|█▎        | 74/585 [00:21<02:32,  3.35it/s] 13%|█▎        | 75/585 [00:22<02:31,  3.37it/s] 13%|█▎        | 76/585 [00:22<02:30,  3.38it/s] 13%|█▎        | 77/585 [00:22<02:29,  3.39it/s] 13%|█▎        | 78/585 [00:22<02:29,  3.39it/s] 14%|█▎        | 79/585 [00:23<02:28,  3.40it/s] 14%|█▎        | 80/585 [00:23<02:28,  3.40it/s] 14%|█▍        | 81/585 [00:23<02:28,  3.40it/s] 14%|█▍        | 82/585 [00:24<02:27,  3.40it/s] 14%|█▍        | 83/585 [00:24<02:27,  3.40it/s] 14%|█▍        | 84/585 [00:24<02:27,  3.40it/s] 15%|█▍        | 85/585 [00:25<02:29,  3.35it/s] 15%|█▍        | 86/585 [00:25<02:28,  3.37it/s] 15%|█▍        | 87/585 [00:25<02:27,  3.39it/s] 15%|█▌        | 88/585 [00:25<02:26,  3.39it/s] 15%|█▌        | 89/585 [00:26<02:26,  3.40it/s] 15%|█▌        | 90/585 [00:26<02:25,  3.40it/s] 16%|█▌        | 91/585 [00:26<02:25,  3.40it/s] 16%|█▌        | 92/585 [00:27<02:24,  3.40it/s] 16%|█▌        | 93/585 [00:27<02:24,  3.41it/s] 16%|█▌        | 94/585 [00:27<02:24,  3.40it/s] 16%|█▌        | 95/585 [00:27<02:23,  3.40it/s] 16%|█▋        | 96/585 [00:28<02:23,  3.40it/s] 17%|█▋        | 97/585 [00:28<02:23,  3.40it/s] 17%|█▋        | 98/585 [00:28<02:23,  3.40it/s] 17%|█▋        | 99/585 [00:29<02:22,  3.40it/s] 17%|█▋        | 100/585 [00:29<02:22,  3.40it/s] 17%|█▋        | 101/585 [00:29<02:22,  3.41it/s] 17%|█▋        | 102/585 [00:30<02:21,  3.41it/s] 18%|█▊        | 103/585 [00:30<02:21,  3.41it/s] 18%|█▊        | 104/585 [00:30<02:21,  3.41it/s] 18%|█▊        | 105/585 [00:30<02:21,  3.40it/s] 18%|█▊        | 106/585 [00:31<02:20,  3.40it/s] 18%|█▊        | 107/585 [00:31<02:21,  3.37it/s] 18%|█▊        | 108/585 [00:31<02:21,  3.38it/s] 19%|█▊        | 109/585 [00:32<02:20,  3.39it/s] 19%|█▉        | 110/585 [00:32<02:20,  3.39it/s] 19%|█▉        | 111/585 [00:32<02:19,  3.40it/s] 19%|█▉        | 112/585 [00:32<02:19,  3.40it/s] 19%|█▉        | 113/585 [00:33<02:18,  3.40it/s] 19%|█▉        | 114/585 [00:33<02:18,  3.40it/s] 20%|█▉        | 115/585 [00:33<02:17,  3.41it/s] 20%|█▉        | 116/585 [00:34<02:17,  3.40it/s] 20%|██        | 117/585 [00:34<02:17,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 23:29:35,768 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:29:35,768 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 23:29:35,768 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.54it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.43it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.49it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.15it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.69it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.41it/s][A
  8%|▊         | 37/436 [00:00<00:09, 44.21it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.03it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.12it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.23it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.37it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.41it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.25it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.02it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 43.93it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.90it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 43.95it/s][A
 21%|██        | 92/436 [00:02<00:07, 43.99it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.07it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.27it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.36it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.23it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.11it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.06it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.92it/s][A
 30%|███       | 132/436 [00:02<00:06, 43.91it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 43.98it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.21it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.35it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.31it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.18it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.22it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.08it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.91it/s][A
 41%|████      | 177/436 [00:03<00:05, 43.91it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.12it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.23it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.29it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.25it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.23it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.19it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.00it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 43.96it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.02it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.15it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.20it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.25it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.20it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.12it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 43.89it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.86it/s][A
 60%|██████    | 262/436 [00:05<00:03, 43.61it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.01it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 42.62it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 43.29it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 43.71it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 43.92it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 43.86it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 43.77it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 43.90it/s][A
 70%|███████   | 307/436 [00:06<00:02, 43.93it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 43.80it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 43.84it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.10it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.27it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.28it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.26it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.26it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.26it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.09it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.02it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.02it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.18it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.25it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.22it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.16it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.18it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.06it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.01it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 43.95it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 43.91it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.07it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.25it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.20it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.27it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.19it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.19it/s][A 20%|██        | 117/585 [00:44<02:17,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:29:45,691 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 23:29:45,715 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:29:48,616 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:29:48,642 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:29:48,654 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:52<44:32,  5.72s/it] 20%|██        | 119/585 [00:53<31:48,  4.10s/it] 21%|██        | 120/585 [00:53<22:54,  2.96s/it] 21%|██        | 121/585 [00:53<16:42,  2.16s/it] 21%|██        | 122/585 [00:54<12:21,  1.60s/it] 21%|██        | 123/585 [00:54<09:19,  1.21s/it] 21%|██        | 124/585 [00:54<07:12,  1.07it/s] 21%|██▏       | 125/585 [00:54<05:43,  1.34it/s] 22%|██▏       | 126/585 [00:55<04:41,  1.63it/s] 22%|██▏       | 127/585 [00:55<03:57,  1.93it/s] 22%|██▏       | 128/585 [00:55<03:27,  2.20it/s] 22%|██▏       | 129/585 [00:56<03:06,  2.45it/s] 22%|██▏       | 130/585 [00:56<02:51,  2.66it/s] 22%|██▏       | 131/585 [00:56<02:40,  2.83it/s] 23%|██▎       | 132/585 [00:57<02:32,  2.96it/s] 23%|██▎       | 133/585 [00:57<02:27,  3.06it/s] 23%|██▎       | 134/585 [00:57<02:23,  3.14it/s] 23%|██▎       | 135/585 [00:57<02:21,  3.19it/s] 23%|██▎       | 136/585 [00:58<02:19,  3.23it/s] 23%|██▎       | 137/585 [00:58<02:17,  3.25it/s] 24%|██▎       | 138/585 [00:58<02:17,  3.26it/s] 24%|██▍       | 139/585 [00:59<02:15,  3.28it/s] 24%|██▍       | 140/585 [00:59<02:14,  3.30it/s] 24%|██▍       | 141/585 [00:59<02:14,  3.31it/s] 24%|██▍       | 142/585 [01:00<02:13,  3.31it/s] 24%|██▍       | 143/585 [01:00<02:13,  3.32it/s] 25%|██▍       | 144/585 [01:00<02:13,  3.31it/s] 25%|██▍       | 145/585 [01:00<02:12,  3.32it/s] 25%|██▍       | 146/585 [01:01<02:12,  3.32it/s] 25%|██▌       | 147/585 [01:01<02:11,  3.33it/s] 25%|██▌       | 148/585 [01:01<02:12,  3.30it/s] 25%|██▌       | 149/585 [01:02<02:11,  3.31it/s] 26%|██▌       | 150/585 [01:02<02:11,  3.31it/s] 26%|██▌       | 151/585 [01:02<02:10,  3.31it/s] 26%|██▌       | 152/585 [01:03<02:10,  3.32it/s] 26%|██▌       | 153/585 [01:03<02:10,  3.32it/s] 26%|██▋       | 154/585 [01:03<02:09,  3.32it/s] 26%|██▋       | 155/585 [01:03<02:09,  3.32it/s] 27%|██▋       | 156/585 [01:04<02:09,  3.32it/s] 27%|██▋       | 157/585 [01:04<02:08,  3.32it/s] 27%|██▋       | 158/585 [01:04<02:10,  3.28it/s] 27%|██▋       | 159/585 [01:05<02:09,  3.29it/s] 27%|██▋       | 160/585 [01:05<02:08,  3.30it/s] 28%|██▊       | 161/585 [01:05<02:08,  3.30it/s] 28%|██▊       | 162/585 [01:06<02:07,  3.31it/s] 28%|██▊       | 163/585 [01:06<02:07,  3.32it/s] 28%|██▊       | 164/585 [01:06<02:06,  3.32it/s] 28%|██▊       | 165/585 [01:07<02:06,  3.32it/s] 28%|██▊       | 166/585 [01:07<02:06,  3.32it/s] 29%|██▊       | 167/585 [01:07<02:05,  3.32it/s] 29%|██▊       | 168/585 [01:07<02:09,  3.23it/s] 29%|██▉       | 169/585 [01:08<02:07,  3.26it/s] 29%|██▉       | 170/585 [01:08<02:06,  3.28it/s] 29%|██▉       | 171/585 [01:08<02:05,  3.29it/s] 29%|██▉       | 172/585 [01:09<02:05,  3.30it/s] 30%|██▉       | 173/585 [01:09<02:04,  3.31it/s] 30%|██▉       | 174/585 [01:09<02:04,  3.31it/s] 30%|██▉       | 175/585 [01:10<02:03,  3.32it/s] 30%|███       | 176/585 [01:10<02:03,  3.32it/s] 30%|███       | 177/585 [01:10<02:02,  3.32it/s] 30%|███       | 178/585 [01:10<02:03,  3.29it/s] 31%|███       | 179/585 [01:11<02:03,  3.30it/s] 31%|███       | 180/585 [01:11<02:02,  3.30it/s] 31%|███       | 181/585 [01:11<02:02,  3.31it/s] 31%|███       | 182/585 [01:12<02:01,  3.31it/s] 31%|███▏      | 183/585 [01:12<02:01,  3.32it/s] 31%|███▏      | 184/585 [01:12<02:00,  3.32it/s] 32%|███▏      | 185/585 [01:13<02:00,  3.32it/s] 32%|███▏      | 186/585 [01:13<02:00,  3.32it/s] 32%|███▏      | 187/585 [01:13<02:00,  3.32it/s] 32%|███▏      | 188/585 [01:13<01:59,  3.31it/s] 32%|███▏      | 189/585 [01:14<01:59,  3.31it/s] 32%|███▏      | 190/585 [01:14<01:59,  3.31it/s] 33%|███▎      | 191/585 [01:14<01:58,  3.32it/s] 33%|███▎      | 192/585 [01:15<01:58,  3.32it/s] 33%|███▎      | 193/585 [01:15<01:58,  3.32it/s] 33%|███▎      | 194/585 [01:15<01:57,  3.32it/s] 33%|███▎      | 195/585 [01:16<01:57,  3.32it/s] 34%|███▎      | 196/585 [01:16<01:57,  3.31it/s] 34%|███▎      | 197/585 [01:16<01:56,  3.32it/s] 34%|███▍      | 198/585 [01:16<01:57,  3.29it/s] 34%|███▍      | 199/585 [01:17<01:57,  3.30it/s] 34%|███▍      | 200/585 [01:17<01:56,  3.30it/s] 34%|███▍      | 201/585 [01:17<01:56,  3.31it/s] 35%|███▍      | 202/585 [01:18<01:55,  3.31it/s] 35%|███▍      | 203/585 [01:18<01:55,  3.32it/s] 35%|███▍      | 204/585 [01:18<01:54,  3.32it/s] 35%|███▌      | 205/585 [01:19<01:54,  3.32it/s] 35%|███▌      | 206/585 [01:19<01:54,  3.32it/s] 35%|███▌      | 207/585 [01:19<01:53,  3.33it/s] 36%|███▌      | 208/585 [01:20<01:54,  3.29it/s] 36%|███▌      | 209/585 [01:20<01:54,  3.29it/s] 36%|███▌      | 210/585 [01:20<01:53,  3.30it/s] 36%|███▌      | 211/585 [01:20<01:53,  3.31it/s] 36%|███▌      | 212/585 [01:21<01:52,  3.32it/s] 36%|███▋      | 213/585 [01:21<01:51,  3.32it/s] 37%|███▋      | 214/585 [01:21<01:51,  3.32it/s] 37%|███▋      | 215/585 [01:22<01:51,  3.33it/s] 37%|███▋      | 216/585 [01:22<01:50,  3.33it/s] 37%|███▋      | 217/585 [01:22<01:50,  3.33it/s] 37%|███▋      | 218/585 [01:23<01:51,  3.28it/s] 37%|███▋      | 219/585 [01:23<01:51,  3.30it/s] 38%|███▊      | 220/585 [01:23<01:50,  3.30it/s] 38%|███▊      | 221/585 [01:23<01:50,  3.30it/s] 38%|███▊      | 222/585 [01:24<01:49,  3.31it/s] 38%|███▊      | 223/585 [01:24<01:49,  3.32it/s] 38%|███▊      | 224/585 [01:24<01:48,  3.32it/s] 38%|███▊      | 225/585 [01:25<01:48,  3.32it/s] 39%|███▊      | 226/585 [01:25<01:47,  3.33it/s] 39%|███▉      | 227/585 [01:25<01:47,  3.33it/s] 39%|███▉      | 228/585 [01:26<01:47,  3.33it/s] 39%|███▉      | 229/585 [01:26<01:46,  3.33it/s] 39%|███▉      | 230/585 [01:26<01:47,  3.32it/s] 39%|███▉      | 231/585 [01:26<01:46,  3.32it/s] 40%|███▉      | 232/585 [01:27<01:46,  3.32it/s] 40%|███▉      | 233/585 [01:27<01:46,  3.32it/s] 40%|████      | 234/585 [01:27<01:45,  3.32it/s][INFO|trainer.py:2140] 2023-08-28 23:30:29,160 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:30:29,160 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 23:30:29,160 >>   Batch size = 8
{'eval_loss': 1.0010502338409424, 'eval_runtime': 9.9068, 'eval_samples_per_second': 352.081, 'eval_steps_per_second': 44.01, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 54.99it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.05it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.31it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.33it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.57it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.49it/s][A
  8%|▊         | 37/436 [00:00<00:09, 44.30it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.13it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.09it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.26it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.39it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.44it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.14it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 43.94it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 43.97it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.96it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 43.94it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.06it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.15it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.23it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.27it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.15it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.00it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.04it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.92it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.06it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.06it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.19it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.22it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.19it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 43.95it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.93it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.81it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.98it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.05it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.11it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.24it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.32it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.21it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.09it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 43.94it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.04it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 43.94it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.08it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.19it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.26it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.22it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.13it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.05it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.04it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.12it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.10it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.11it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.21it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.25it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.15it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.10it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 43.98it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.12it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.15it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.05it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.04it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.17it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.17it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.11it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.06it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.16it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.13it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.06it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.06it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.04it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.06it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.06it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.09it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.10it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.18it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.22it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.15it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.09it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.01it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 43.97it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.00it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.06it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.22it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.23it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.26it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.26it/s][A 40%|████      | 234/585 [01:37<01:45,  3.32it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:30:39,072 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 23:30:39,089 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:30:41,620 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:30:41,640 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:30:41,652 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:45<32:41,  5.61s/it] 40%|████      | 236/585 [01:46<23:22,  4.02s/it] 41%|████      | 237/585 [01:46<16:50,  2.90s/it] 41%|████      | 238/585 [01:46<12:16,  2.12s/it] 41%|████      | 239/585 [01:47<09:05,  1.58s/it] 41%|████      | 240/585 [01:47<06:51,  1.19s/it] 41%|████      | 241/585 [01:47<05:18,  1.08it/s] 41%|████▏     | 242/585 [01:47<04:13,  1.35it/s] 42%|████▏     | 243/585 [01:48<03:27,  1.65it/s] 42%|████▏     | 244/585 [01:48<02:55,  1.94it/s] 42%|████▏     | 245/585 [01:48<02:33,  2.22it/s] 42%|████▏     | 246/585 [01:49<02:18,  2.46it/s] 42%|████▏     | 247/585 [01:49<02:06,  2.66it/s] 42%|████▏     | 248/585 [01:49<01:59,  2.83it/s] 43%|████▎     | 249/585 [01:50<01:53,  2.96it/s] 43%|████▎     | 250/585 [01:50<01:49,  3.06it/s] 43%|████▎     | 251/585 [01:50<01:46,  3.13it/s] 43%|████▎     | 252/585 [01:50<01:44,  3.18it/s] 43%|████▎     | 253/585 [01:51<01:43,  3.22it/s] 43%|████▎     | 254/585 [01:51<01:41,  3.25it/s] 44%|████▎     | 255/585 [01:51<01:40,  3.27it/s] 44%|████▍     | 256/585 [01:52<01:40,  3.28it/s] 44%|████▍     | 257/585 [01:52<01:39,  3.29it/s] 44%|████▍     | 258/585 [01:52<01:41,  3.21it/s] 44%|████▍     | 259/585 [01:53<01:40,  3.24it/s] 44%|████▍     | 260/585 [01:53<01:39,  3.26it/s] 45%|████▍     | 261/585 [01:53<01:38,  3.28it/s] 45%|████▍     | 262/585 [01:54<01:38,  3.29it/s] 45%|████▍     | 263/585 [01:54<01:37,  3.30it/s] 45%|████▌     | 264/585 [01:54<01:37,  3.30it/s] 45%|████▌     | 265/585 [01:54<01:36,  3.31it/s] 45%|████▌     | 266/585 [01:55<01:37,  3.29it/s] 46%|████▌     | 267/585 [01:55<01:36,  3.29it/s] 46%|████▌     | 268/585 [01:55<01:35,  3.30it/s] 46%|████▌     | 269/585 [01:56<01:35,  3.31it/s] 46%|████▌     | 270/585 [01:56<01:35,  3.31it/s] 46%|████▋     | 271/585 [01:56<01:34,  3.32it/s] 46%|████▋     | 272/585 [01:57<01:34,  3.32it/s] 47%|████▋     | 273/585 [01:57<01:34,  3.32it/s] 47%|████▋     | 274/585 [01:57<01:33,  3.32it/s] 47%|████▋     | 275/585 [01:57<01:33,  3.32it/s] 47%|████▋     | 276/585 [01:58<01:33,  3.31it/s] 47%|████▋     | 277/585 [01:58<01:32,  3.31it/s] 48%|████▊     | 278/585 [01:58<01:32,  3.32it/s] 48%|████▊     | 279/585 [01:59<01:32,  3.33it/s] 48%|████▊     | 280/585 [01:59<01:31,  3.33it/s] 48%|████▊     | 281/585 [01:59<01:31,  3.33it/s] 48%|████▊     | 282/585 [02:00<01:31,  3.33it/s] 48%|████▊     | 283/585 [02:00<01:30,  3.32it/s] 49%|████▊     | 284/585 [02:00<01:30,  3.32it/s] 49%|████▊     | 285/585 [02:00<01:30,  3.32it/s] 49%|████▉     | 286/585 [02:01<01:30,  3.31it/s] 49%|████▉     | 287/585 [02:01<01:29,  3.31it/s] 49%|████▉     | 288/585 [02:01<01:29,  3.32it/s] 49%|████▉     | 289/585 [02:02<01:29,  3.31it/s] 50%|████▉     | 290/585 [02:02<01:29,  3.31it/s] 50%|████▉     | 291/585 [02:02<01:28,  3.31it/s] 50%|████▉     | 292/585 [02:03<01:28,  3.31it/s] 50%|█████     | 293/585 [02:03<01:28,  3.31it/s] 50%|█████     | 294/585 [02:03<01:27,  3.31it/s] 50%|█████     | 295/585 [02:03<01:27,  3.31it/s] 51%|█████     | 296/585 [02:04<01:27,  3.29it/s] 51%|█████     | 297/585 [02:04<01:27,  3.29it/s] 51%|█████     | 298/585 [02:04<01:27,  3.30it/s] 51%|█████     | 299/585 [02:05<01:26,  3.30it/s] 51%|█████▏    | 300/585 [02:05<01:26,  3.30it/s] 51%|█████▏    | 301/585 [02:05<01:25,  3.31it/s] 52%|█████▏    | 302/585 [02:06<01:25,  3.31it/s] 52%|█████▏    | 303/585 [02:06<01:24,  3.32it/s] 52%|█████▏    | 304/585 [02:06<01:24,  3.32it/s] 52%|█████▏    | 305/585 [02:06<01:24,  3.32it/s] 52%|█████▏    | 306/585 [02:07<01:24,  3.32it/s] 52%|█████▏    | 307/585 [02:07<01:24,  3.29it/s] 53%|█████▎    | 308/585 [02:07<01:23,  3.30it/s] 53%|█████▎    | 309/585 [02:08<01:23,  3.30it/s] 53%|█████▎    | 310/585 [02:08<01:23,  3.31it/s] 53%|█████▎    | 311/585 [02:08<01:22,  3.31it/s] 53%|█████▎    | 312/585 [02:09<01:22,  3.31it/s] 54%|█████▎    | 313/585 [02:09<01:21,  3.32it/s] 54%|█████▎    | 314/585 [02:09<01:21,  3.32it/s] 54%|█████▍    | 315/585 [02:10<01:21,  3.33it/s] 54%|█████▍    | 316/585 [02:10<01:20,  3.33it/s] 54%|█████▍    | 317/585 [02:10<01:20,  3.32it/s] 54%|█████▍    | 318/585 [02:10<01:20,  3.32it/s] 55%|█████▍    | 319/585 [02:11<01:20,  3.32it/s] 55%|█████▍    | 320/585 [02:11<01:19,  3.32it/s] 55%|█████▍    | 321/585 [02:11<01:19,  3.32it/s] 55%|█████▌    | 322/585 [02:12<01:19,  3.32it/s] 55%|█████▌    | 323/585 [02:12<01:18,  3.32it/s] 55%|█████▌    | 324/585 [02:12<01:18,  3.32it/s] 56%|█████▌    | 325/585 [02:13<01:18,  3.33it/s] 56%|█████▌    | 326/585 [02:13<01:17,  3.33it/s] 56%|█████▌    | 327/585 [02:13<01:18,  3.27it/s] 56%|█████▌    | 328/585 [02:13<01:18,  3.29it/s] 56%|█████▌    | 329/585 [02:14<01:17,  3.30it/s] 56%|█████▋    | 330/585 [02:14<01:17,  3.31it/s] 57%|█████▋    | 331/585 [02:14<01:16,  3.31it/s] 57%|█████▋    | 332/585 [02:15<01:16,  3.31it/s] 57%|█████▋    | 333/585 [02:15<01:15,  3.32it/s] 57%|█████▋    | 334/585 [02:15<01:15,  3.32it/s] 57%|█████▋    | 335/585 [02:16<01:15,  3.32it/s] 57%|█████▋    | 336/585 [02:16<01:14,  3.33it/s] 58%|█████▊    | 337/585 [02:16<01:14,  3.32it/s] 58%|█████▊    | 338/585 [02:16<01:14,  3.32it/s] 58%|█████▊    | 339/585 [02:17<01:13,  3.33it/s] 58%|█████▊    | 340/585 [02:17<01:13,  3.33it/s] 58%|█████▊    | 341/585 [02:17<01:13,  3.33it/s] 58%|█████▊    | 342/585 [02:18<01:13,  3.33it/s] 59%|█████▊    | 343/585 [02:18<01:12,  3.33it/s] 59%|█████▉    | 344/585 [02:18<01:12,  3.33it/s] 59%|█████▉    | 345/585 [02:19<01:12,  3.33it/s] 59%|█████▉    | 346/585 [02:19<01:11,  3.33it/s] 59%|█████▉    | 347/585 [02:19<01:12,  3.30it/s] 59%|█████▉    | 348/585 [02:19<01:11,  3.31it/s] 60%|█████▉    | 349/585 [02:20<01:11,  3.31it/s] 60%|█████▉    | 350/585 [02:20<01:10,  3.31it/s] 60%|██████    | 351/585 [02:20<01:10,  3.32it/s][INFO|trainer.py:2140] 2023-08-28 23:31:22,169 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:31:22,170 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 23:31:22,170 >>   Batch size = 8
{'eval_loss': 1.0098392963409424, 'eval_runtime': 9.8935, 'eval_samples_per_second': 352.556, 'eval_steps_per_second': 44.07, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 54.54it/s][A
  3%|▎         | 12/436 [00:00<00:08, 47.48it/s][A
  4%|▍         | 17/436 [00:00<00:09, 45.92it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.18it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.70it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.53it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.37it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.10it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.08it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.28it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.34it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.26it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.16it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.28it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.17it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 44.03it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 43.86it/s][A
 21%|██        | 92/436 [00:02<00:07, 43.99it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.19it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.29it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.14it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.19it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.20it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.10it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.92it/s][A
 30%|███       | 132/436 [00:02<00:06, 43.95it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.01it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.24it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.27it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.22it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.22it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.21it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.03it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.99it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.05it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.12it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.17it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.24it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.25it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.25it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.14it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 43.83it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 43.84it/s][A
 51%|█████     | 222/436 [00:05<00:04, 43.97it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.12it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.10it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.32it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.34it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.32it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.15it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.93it/s][A
 60%|██████    | 262/436 [00:05<00:03, 43.95it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.05it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.11it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.23it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.40it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.35it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.25it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.07it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 43.74it/s][A
 70%|███████   | 307/436 [00:06<00:02, 43.77it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 43.88it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.03it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.25it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.32it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.25it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.19it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.05it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.01it/s][A
 81%|████████  | 352/436 [00:07<00:01, 43.77it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.04it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.25it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.33it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.34it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.25it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.20it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.14it/s][A
 90%|████████▉ | 392/436 [00:08<00:01, 43.88it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.06it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.17it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.26it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.20it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.22it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.17it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.02it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 43.96it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 43.96it/s][A 60%|██████    | 351/585 [02:30<01:10,  3.32it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:31:32,094 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 23:31:32,117 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:31:34,728 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:31:34,749 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:31:34,760 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:38<21:44,  5.60s/it] 60%|██████    | 353/585 [02:39<15:31,  4.01s/it] 61%|██████    | 354/585 [02:39<11:09,  2.90s/it] 61%|██████    | 355/585 [02:39<08:07,  2.12s/it] 61%|██████    | 356/585 [02:40<06:00,  1.57s/it] 61%|██████    | 357/585 [02:40<04:31,  1.19s/it] 61%|██████    | 358/585 [02:40<03:29,  1.08it/s] 61%|██████▏   | 359/585 [02:40<02:46,  1.36it/s] 62%|██████▏   | 360/585 [02:41<02:16,  1.65it/s] 62%|██████▏   | 361/585 [02:41<01:55,  1.95it/s] 62%|██████▏   | 362/585 [02:41<01:40,  2.22it/s] 62%|██████▏   | 363/585 [02:42<01:30,  2.45it/s] 62%|██████▏   | 364/585 [02:42<01:23,  2.66it/s] 62%|██████▏   | 365/585 [02:42<01:17,  2.83it/s] 63%|██████▎   | 366/585 [02:43<01:13,  2.96it/s] 63%|██████▎   | 367/585 [02:43<01:11,  3.07it/s] 63%|██████▎   | 368/585 [02:43<01:09,  3.14it/s] 63%|██████▎   | 369/585 [02:43<01:07,  3.19it/s] 63%|██████▎   | 370/585 [02:44<01:06,  3.24it/s] 63%|██████▎   | 371/585 [02:44<01:05,  3.26it/s] 64%|██████▎   | 372/585 [02:44<01:04,  3.29it/s] 64%|██████▍   | 373/585 [02:45<01:04,  3.29it/s] 64%|██████▍   | 374/585 [02:45<01:03,  3.30it/s] 64%|██████▍   | 375/585 [02:45<01:03,  3.31it/s] 64%|██████▍   | 376/585 [02:46<01:02,  3.32it/s] 64%|██████▍   | 377/585 [02:46<01:02,  3.32it/s] 65%|██████▍   | 378/585 [02:46<01:02,  3.33it/s] 65%|██████▍   | 379/585 [02:46<01:01,  3.33it/s] 65%|██████▍   | 380/585 [02:47<01:01,  3.33it/s] 65%|██████▌   | 381/585 [02:47<01:01,  3.33it/s] 65%|██████▌   | 382/585 [02:47<01:00,  3.33it/s] 65%|██████▌   | 383/585 [02:48<01:01,  3.31it/s] 66%|██████▌   | 384/585 [02:48<01:00,  3.31it/s] 66%|██████▌   | 385/585 [02:48<01:00,  3.32it/s] 66%|██████▌   | 386/585 [02:49<00:59,  3.32it/s] 66%|██████▌   | 387/585 [02:49<00:59,  3.32it/s] 66%|██████▋   | 388/585 [02:49<00:59,  3.32it/s] 66%|██████▋   | 389/585 [02:49<00:59,  3.32it/s] 67%|██████▋   | 390/585 [02:50<00:58,  3.32it/s] 67%|██████▋   | 391/585 [02:50<00:58,  3.32it/s] 67%|██████▋   | 392/585 [02:50<00:58,  3.33it/s] 67%|██████▋   | 393/585 [02:51<00:57,  3.32it/s] 67%|██████▋   | 394/585 [02:51<00:57,  3.33it/s] 68%|██████▊   | 395/585 [02:51<00:57,  3.33it/s] 68%|██████▊   | 396/585 [02:52<00:56,  3.33it/s] 68%|██████▊   | 397/585 [02:52<00:56,  3.33it/s] 68%|██████▊   | 398/585 [02:52<00:56,  3.33it/s] 68%|██████▊   | 399/585 [02:52<00:55,  3.33it/s] 68%|██████▊   | 400/585 [02:53<00:55,  3.33it/s] 69%|██████▊   | 401/585 [02:53<00:55,  3.34it/s] 69%|██████▊   | 402/585 [02:53<00:54,  3.33it/s] 69%|██████▉   | 403/585 [02:54<00:54,  3.32it/s] 69%|██████▉   | 404/585 [02:54<00:54,  3.33it/s] 69%|██████▉   | 405/585 [02:54<00:54,  3.33it/s] 69%|██████▉   | 406/585 [02:55<00:53,  3.33it/s] 70%|██████▉   | 407/585 [02:55<00:53,  3.33it/s] 70%|██████▉   | 408/585 [02:55<00:53,  3.33it/s] 70%|██████▉   | 409/585 [02:55<00:52,  3.33it/s] 70%|███████   | 410/585 [02:56<00:52,  3.34it/s] 70%|███████   | 411/585 [02:56<00:52,  3.33it/s] 70%|███████   | 412/585 [02:56<00:51,  3.34it/s] 71%|███████   | 413/585 [02:57<00:51,  3.32it/s] 71%|███████   | 414/585 [02:57<00:51,  3.32it/s] 71%|███████   | 415/585 [02:57<00:51,  3.32it/s] 71%|███████   | 416/585 [02:58<00:50,  3.32it/s] 71%|███████▏  | 417/585 [02:58<00:50,  3.33it/s] 71%|███████▏  | 418/585 [02:58<00:50,  3.33it/s] 72%|███████▏  | 419/585 [02:58<00:49,  3.32it/s] 72%|███████▏  | 420/585 [02:59<00:49,  3.32it/s] 72%|███████▏  | 421/585 [02:59<00:49,  3.32it/s] 72%|███████▏  | 422/585 [02:59<00:49,  3.32it/s] 72%|███████▏  | 423/585 [03:00<00:48,  3.32it/s] 72%|███████▏  | 424/585 [03:00<00:48,  3.32it/s] 73%|███████▎  | 425/585 [03:00<00:48,  3.32it/s] 73%|███████▎  | 426/585 [03:01<00:47,  3.32it/s] 73%|███████▎  | 427/585 [03:01<00:47,  3.32it/s] 73%|███████▎  | 428/585 [03:01<00:47,  3.32it/s] 73%|███████▎  | 429/585 [03:01<00:46,  3.33it/s] 74%|███████▎  | 430/585 [03:02<00:46,  3.33it/s] 74%|███████▎  | 431/585 [03:02<00:46,  3.33it/s] 74%|███████▍  | 432/585 [03:02<00:45,  3.33it/s] 74%|███████▍  | 433/585 [03:03<00:45,  3.33it/s] 74%|███████▍  | 434/585 [03:03<00:45,  3.33it/s] 74%|███████▍  | 435/585 [03:03<00:45,  3.32it/s] 75%|███████▍  | 436/585 [03:04<00:44,  3.32it/s] 75%|███████▍  | 437/585 [03:04<00:44,  3.32it/s] 75%|███████▍  | 438/585 [03:04<00:44,  3.32it/s] 75%|███████▌  | 439/585 [03:04<00:43,  3.33it/s] 75%|███████▌  | 440/585 [03:05<00:43,  3.32it/s] 75%|███████▌  | 441/585 [03:05<00:43,  3.32it/s] 76%|███████▌  | 442/585 [03:05<00:43,  3.32it/s] 76%|███████▌  | 443/585 [03:06<00:42,  3.32it/s] 76%|███████▌  | 444/585 [03:06<00:42,  3.32it/s] 76%|███████▌  | 445/585 [03:06<00:42,  3.31it/s] 76%|███████▌  | 446/585 [03:07<00:41,  3.31it/s] 76%|███████▋  | 447/585 [03:07<00:41,  3.32it/s] 77%|███████▋  | 448/585 [03:07<00:41,  3.32it/s] 77%|███████▋  | 449/585 [03:08<00:40,  3.32it/s] 77%|███████▋  | 450/585 [03:08<00:40,  3.32it/s] 77%|███████▋  | 451/585 [03:08<00:40,  3.32it/s] 77%|███████▋  | 452/585 [03:08<00:40,  3.32it/s] 77%|███████▋  | 453/585 [03:09<00:39,  3.32it/s] 78%|███████▊  | 454/585 [03:09<00:39,  3.32it/s] 78%|███████▊  | 455/585 [03:09<00:40,  3.25it/s] 78%|███████▊  | 456/585 [03:10<00:39,  3.27it/s] 78%|███████▊  | 457/585 [03:10<00:39,  3.28it/s] 78%|███████▊  | 458/585 [03:10<00:38,  3.28it/s] 78%|███████▊  | 459/585 [03:11<00:38,  3.30it/s] 79%|███████▊  | 460/585 [03:11<00:37,  3.33it/s] 79%|███████▉  | 461/585 [03:11<00:37,  3.35it/s] 79%|███████▉  | 462/585 [03:11<00:36,  3.36it/s] 79%|███████▉  | 463/585 [03:12<00:36,  3.38it/s] 79%|███████▉  | 464/585 [03:12<00:35,  3.38it/s] 79%|███████▉  | 465/585 [03:12<00:38,  3.14it/s] 80%|███████▉  | 466/585 [03:13<00:37,  3.21it/s] 80%|███████▉  | 467/585 [03:13<00:36,  3.26it/s] 80%|████████  | 468/585 [03:13<00:35,  3.30it/s][INFO|trainer.py:2140] 2023-08-28 23:32:15,077 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:32:15,077 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 23:32:15,077 >>   Batch size = 8
{'eval_loss': 1.0224156379699707, 'eval_runtime': 9.8939, 'eval_samples_per_second': 352.54, 'eval_steps_per_second': 44.067, 'epoch': 3.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.05it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.26it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.37it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.71it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.98it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.70it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.40it/s][A
 10%|▉         | 42/436 [00:00<00:08, 43.97it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.05it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.28it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.29it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.14it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.32it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.39it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.27it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 40.28it/s][A
 20%|█▉        | 87/436 [00:01<00:08, 41.48it/s][A
 21%|██        | 92/436 [00:02<00:08, 42.36it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 43.01it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 43.41it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 43.80it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 43.97it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.99it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 43.67it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.80it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.00it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.06it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.15it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.06it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.22it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.26it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.99it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.85it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.96it/s][A
 41%|████      | 177/436 [00:04<00:05, 44.11it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.30it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.22it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.25it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.21it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.18it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.07it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 43.89it/s][A
 50%|████▉     | 217/436 [00:04<00:05, 43.74it/s][A
 51%|█████     | 222/436 [00:05<00:04, 43.86it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.06it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.18it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.12it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.23it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.17it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 43.98it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.87it/s][A
 60%|██████    | 262/436 [00:05<00:03, 43.98it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.14it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.25it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.22it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.30it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.24it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.15it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 43.93it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.03it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.07it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.02it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.21it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.18it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.16it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.09it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 43.97it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 43.91it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 43.93it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.05it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.09it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.25it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.29it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.24it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.15it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.03it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 43.99it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.02it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 44.06it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.08it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.25it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.32it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.18it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.14it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.01it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.02it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.02it/s][A 80%|████████  | 468/585 [03:23<00:35,  3.30it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:32:25,018 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 23:32:25,044 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:32:27,586 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:32:27,605 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:32:27,616 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:31<10:48,  5.59s/it] 80%|████████  | 470/585 [03:31<07:40,  4.00s/it] 81%|████████  | 471/585 [03:32<05:29,  2.89s/it] 81%|████████  | 472/585 [03:32<03:58,  2.11s/it] 81%|████████  | 473/585 [03:32<02:55,  1.57s/it] 81%|████████  | 474/585 [03:33<02:11,  1.19s/it] 81%|████████  | 475/585 [03:33<01:41,  1.08it/s] 81%|████████▏ | 476/585 [03:33<01:20,  1.36it/s] 82%|████████▏ | 477/585 [03:34<01:05,  1.65it/s] 82%|████████▏ | 478/585 [03:34<00:54,  1.95it/s] 82%|████████▏ | 479/585 [03:34<00:47,  2.22it/s] 82%|████████▏ | 480/585 [03:35<00:42,  2.47it/s] 82%|████████▏ | 481/585 [03:35<00:38,  2.68it/s] 82%|████████▏ | 482/585 [03:35<00:36,  2.85it/s] 83%|████████▎ | 483/585 [03:35<00:34,  2.98it/s] 83%|████████▎ | 484/585 [03:36<00:32,  3.07it/s] 83%|████████▎ | 485/585 [03:36<00:31,  3.15it/s] 83%|████████▎ | 486/585 [03:36<00:30,  3.20it/s] 83%|████████▎ | 487/585 [03:37<00:30,  3.24it/s] 83%|████████▎ | 488/585 [03:37<00:29,  3.27it/s] 84%|████████▎ | 489/585 [03:37<00:29,  3.24it/s] 84%|████████▍ | 490/585 [03:38<00:29,  3.27it/s] 84%|████████▍ | 491/585 [03:38<00:28,  3.28it/s] 84%|████████▍ | 492/585 [03:38<00:28,  3.30it/s] 84%|████████▍ | 493/585 [03:38<00:27,  3.31it/s] 84%|████████▍ | 494/585 [03:39<00:27,  3.31it/s] 85%|████████▍ | 495/585 [03:39<00:27,  3.33it/s] 85%|████████▍ | 496/585 [03:39<00:26,  3.35it/s] 85%|████████▍ | 497/585 [03:40<00:26,  3.37it/s] 85%|████████▌ | 498/585 [03:40<00:25,  3.38it/s] 85%|████████▌ | 499/585 [03:40<00:25,  3.39it/s] 85%|████████▌ | 500/585 [03:40<00:25,  3.37it/s]                                                  85%|████████▌ | 500/585 [03:40<00:25,  3.37it/s] 86%|████████▌ | 501/585 [03:41<00:24,  3.38it/s] 86%|████████▌ | 502/585 [03:41<00:24,  3.39it/s] 86%|████████▌ | 503/585 [03:41<00:24,  3.40it/s] 86%|████████▌ | 504/585 [03:42<00:23,  3.39it/s] 86%|████████▋ | 505/585 [03:42<00:23,  3.40it/s] 86%|████████▋ | 506/585 [03:42<00:23,  3.40it/s] 87%|████████▋ | 507/585 [03:43<00:22,  3.40it/s] 87%|████████▋ | 508/585 [03:43<00:22,  3.41it/s] 87%|████████▋ | 509/585 [03:43<00:22,  3.40it/s] 87%|████████▋ | 510/585 [03:43<00:22,  3.40it/s] 87%|████████▋ | 511/585 [03:44<00:21,  3.38it/s] 88%|████████▊ | 512/585 [03:44<00:21,  3.39it/s] 88%|████████▊ | 513/585 [03:44<00:21,  3.40it/s] 88%|████████▊ | 514/585 [03:45<00:20,  3.40it/s] 88%|████████▊ | 515/585 [03:45<00:20,  3.39it/s] 88%|████████▊ | 516/585 [03:45<00:20,  3.39it/s] 88%|████████▊ | 517/585 [03:45<00:20,  3.40it/s] 89%|████████▊ | 518/585 [03:46<00:19,  3.40it/s] 89%|████████▊ | 519/585 [03:46<00:19,  3.40it/s] 89%|████████▉ | 520/585 [03:46<00:19,  3.41it/s] 89%|████████▉ | 521/585 [03:47<00:18,  3.41it/s] 89%|████████▉ | 522/585 [03:47<00:18,  3.37it/s] 89%|████████▉ | 523/585 [03:47<00:18,  3.38it/s] 90%|████████▉ | 524/585 [03:48<00:17,  3.39it/s] 90%|████████▉ | 525/585 [03:48<00:17,  3.40it/s] 90%|████████▉ | 526/585 [03:48<00:17,  3.40it/s] 90%|█████████ | 527/585 [03:48<00:17,  3.41it/s] 90%|█████████ | 528/585 [03:49<00:16,  3.40it/s] 90%|█████████ | 529/585 [03:49<00:16,  3.40it/s] 91%|█████████ | 530/585 [03:49<00:16,  3.40it/s] 91%|█████████ | 531/585 [03:50<00:15,  3.41it/s] 91%|█████████ | 532/585 [03:50<00:15,  3.40it/s] 91%|█████████ | 533/585 [03:50<00:15,  3.38it/s] 91%|█████████▏| 534/585 [03:50<00:15,  3.39it/s] 91%|█████████▏| 535/585 [03:51<00:14,  3.40it/s] 92%|█████████▏| 536/585 [03:51<00:14,  3.40it/s] 92%|█████████▏| 537/585 [03:51<00:14,  3.40it/s] 92%|█████████▏| 538/585 [03:52<00:13,  3.41it/s] 92%|█████████▏| 539/585 [03:52<00:13,  3.41it/s] 92%|█████████▏| 540/585 [03:52<00:13,  3.41it/s] 92%|█████████▏| 541/585 [03:53<00:12,  3.41it/s] 93%|█████████▎| 542/585 [03:53<00:12,  3.41it/s] 93%|█████████▎| 543/585 [03:53<00:12,  3.41it/s] 93%|█████████▎| 544/585 [03:53<00:12,  3.40it/s] 93%|█████████▎| 545/585 [03:54<00:11,  3.40it/s] 93%|█████████▎| 546/585 [03:54<00:11,  3.40it/s] 94%|█████████▎| 547/585 [03:54<00:11,  3.40it/s] 94%|█████████▎| 548/585 [03:55<00:10,  3.41it/s] 94%|█████████▍| 549/585 [03:55<00:10,  3.40it/s] 94%|█████████▍| 550/585 [03:55<00:10,  3.41it/s] 94%|█████████▍| 551/585 [03:55<00:09,  3.41it/s] 94%|█████████▍| 552/585 [03:56<00:09,  3.41it/s] 95%|█████████▍| 553/585 [03:56<00:09,  3.41it/s] 95%|█████████▍| 554/585 [03:56<00:09,  3.41it/s] 95%|█████████▍| 555/585 [03:57<00:08,  3.39it/s] 95%|█████████▌| 556/585 [03:57<00:08,  3.39it/s] 95%|█████████▌| 557/585 [03:57<00:08,  3.40it/s] 95%|█████████▌| 558/585 [03:58<00:07,  3.40it/s] 96%|█████████▌| 559/585 [03:58<00:07,  3.41it/s] 96%|█████████▌| 560/585 [03:58<00:07,  3.41it/s] 96%|█████████▌| 561/585 [03:58<00:07,  3.41it/s] 96%|█████████▌| 562/585 [03:59<00:06,  3.41it/s] 96%|█████████▌| 563/585 [03:59<00:06,  3.41it/s] 96%|█████████▋| 564/585 [03:59<00:06,  3.41it/s] 97%|█████████▋| 565/585 [04:00<00:05,  3.41it/s] 97%|█████████▋| 566/585 [04:00<00:05,  3.37it/s] 97%|█████████▋| 567/585 [04:00<00:05,  3.38it/s] 97%|█████████▋| 568/585 [04:00<00:05,  3.39it/s] 97%|█████████▋| 569/585 [04:01<00:04,  3.40it/s] 97%|█████████▋| 570/585 [04:01<00:04,  3.40it/s] 98%|█████████▊| 571/585 [04:01<00:04,  3.40it/s] 98%|█████████▊| 572/585 [04:02<00:03,  3.40it/s] 98%|█████████▊| 573/585 [04:02<00:03,  3.40it/s] 98%|█████████▊| 574/585 [04:02<00:03,  3.41it/s] 98%|█████████▊| 575/585 [04:03<00:02,  3.41it/s] 98%|█████████▊| 576/585 [04:03<00:02,  3.41it/s] 99%|█████████▊| 577/585 [04:03<00:02,  3.41it/s] 99%|█████████▉| 578/585 [04:03<00:02,  3.41it/s] 99%|█████████▉| 579/585 [04:04<00:01,  3.41it/s] 99%|█████████▉| 580/585 [04:04<00:01,  3.41it/s] 99%|█████████▉| 581/585 [04:04<00:01,  3.41it/s] 99%|█████████▉| 582/585 [04:05<00:00,  3.41it/s]100%|█████████▉| 583/585 [04:05<00:00,  3.39it/s]100%|█████████▉| 584/585 [04:05<00:00,  3.40it/s]100%|██████████| 585/585 [04:05<00:00,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 23:33:07,238 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:33:07,238 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 23:33:07,238 >>   Batch size = 8
{'eval_loss': 1.0306529998779297, 'eval_runtime': 9.9248, 'eval_samples_per_second': 351.443, 'eval_steps_per_second': 43.93, 'epoch': 4.0}
{'loss': 0.5297, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.61it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.22it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.33it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.46it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.93it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.55it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.35it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.12it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.25it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.36it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.34it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.37it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.23it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.17it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.08it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.97it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.02it/s][A
 21%|██        | 92/436 [00:02<00:07, 43.97it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.27it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.31it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.31it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.25it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.10it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.03it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.51it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.12it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.17it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.21it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.33it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.32it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.30it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.20it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.02it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.05it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.03it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.15it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.23it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.23it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.28it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.23it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.13it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.08it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 43.89it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.10it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.26it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.27it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.32it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.28it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.24it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.19it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.95it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.00it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.11it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.28it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.33it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.32it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.31it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.15it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.06it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 43.95it/s][A
 70%|███████   | 307/436 [00:06<00:02, 43.95it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.11it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.17it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.18it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.29it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.19it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 43.95it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 43.97it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 43.93it/s][A
 81%|████████  | 352/436 [00:07<00:01, 43.98it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.15it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.29it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.27it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 43.98it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.07it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.00it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 43.94it/s][A
 90%|████████▉ | 392/436 [00:08<00:01, 43.92it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 43.95it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.14it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 43.91it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.08it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 43.96it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 43.95it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.00it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 43.67it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 43.67it/s][A100%|██████████| 585/585 [04:15<00:00,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:33:17,373 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 23:33:17,519 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:33:20,311 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:33:20,331 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:33:20,341 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 23:33:28,029 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 23:33:28,034 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117 (score: 1.0010502338409424).
                                                 100%|██████████| 585/585 [04:29<00:00,  3.40it/s]100%|██████████| 585/585 [04:29<00:00,  2.17it/s]
[INFO|trainer.py:1894] 2023-08-28 23:33:30,644 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 23:33:30,664 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:33:32,925 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:33:32,942 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:33:32,949 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:33:33,231 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:33:33,231 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:33:33,231 >>   train_loss               =      0.526
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:33:33,231 >>   train_runtime            = 0:04:29.38
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:33:33,231 >>   train_samples            =       7499
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:33:33,231 >>   train_samples_per_second =    139.188
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:33:33,231 >>   train_steps_per_second   =      2.172
{'eval_loss': 1.0356247425079346, 'eval_runtime': 9.8852, 'eval_samples_per_second': 352.85, 'eval_steps_per_second': 44.106, 'epoch': 5.0}
{'train_runtime': 269.3832, 'train_samples_per_second': 139.188, 'train_steps_per_second': 2.172, 'train_loss': 0.5260368151542468, 'epoch': 5.0}
08/28/2023 23:33:33 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 23:33:33,298 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:33:33,298 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 23:33:33,298 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 55.56it/s]  3%|▎         | 12/436 [00:00<00:08, 48.74it/s]  4%|▍         | 17/436 [00:00<00:08, 46.79it/s]  5%|▌         | 22/436 [00:00<00:09, 45.90it/s]  6%|▌         | 27/436 [00:00<00:08, 45.53it/s]  7%|▋         | 32/436 [00:00<00:08, 45.25it/s]  8%|▊         | 37/436 [00:00<00:08, 45.06it/s] 10%|▉         | 42/436 [00:00<00:08, 44.58it/s] 11%|█         | 47/436 [00:01<00:08, 43.91it/s] 12%|█▏        | 52/436 [00:01<00:08, 43.99it/s] 13%|█▎        | 57/436 [00:01<00:08, 44.09it/s] 14%|█▍        | 62/436 [00:01<00:08, 44.23it/s] 15%|█▌        | 67/436 [00:01<00:08, 44.39it/s] 17%|█▋        | 72/436 [00:01<00:08, 44.38it/s] 18%|█▊        | 77/436 [00:01<00:08, 44.44it/s] 19%|█▉        | 82/436 [00:01<00:07, 44.31it/s] 20%|█▉        | 87/436 [00:01<00:07, 43.92it/s] 21%|██        | 92/436 [00:02<00:07, 43.75it/s] 22%|██▏       | 97/436 [00:02<00:07, 43.71it/s] 23%|██▎       | 102/436 [00:02<00:07, 43.87it/s] 25%|██▍       | 107/436 [00:02<00:07, 44.12it/s] 26%|██▌       | 112/436 [00:02<00:07, 44.29it/s] 27%|██▋       | 117/436 [00:02<00:07, 44.39it/s] 28%|██▊       | 122/436 [00:02<00:07, 44.31it/s] 29%|██▉       | 127/436 [00:02<00:06, 44.32it/s] 30%|███       | 132/436 [00:02<00:06, 44.05it/s] 31%|███▏      | 137/436 [00:03<00:06, 43.79it/s] 33%|███▎      | 142/436 [00:03<00:06, 43.80it/s] 34%|███▎      | 147/436 [00:03<00:06, 43.88it/s] 35%|███▍      | 152/436 [00:03<00:06, 44.14it/s] 36%|███▌      | 157/436 [00:03<00:06, 44.26it/s] 37%|███▋      | 162/436 [00:03<00:06, 44.35it/s] 38%|███▊      | 167/436 [00:03<00:06, 44.43it/s] 39%|███▉      | 172/436 [00:03<00:05, 44.25it/s] 41%|████      | 177/436 [00:03<00:05, 44.02it/s] 42%|████▏     | 182/436 [00:04<00:05, 43.92it/s] 43%|████▎     | 187/436 [00:04<00:05, 43.83it/s] 44%|████▍     | 192/436 [00:04<00:05, 43.93it/s] 45%|████▌     | 197/436 [00:04<00:05, 44.10it/s] 46%|████▋     | 202/436 [00:04<00:05, 44.19it/s] 47%|████▋     | 207/436 [00:04<00:05, 44.35it/s] 49%|████▊     | 212/436 [00:04<00:05, 44.30it/s] 50%|████▉     | 217/436 [00:04<00:04, 44.26it/s] 51%|█████     | 222/436 [00:05<00:04, 44.04it/s] 52%|█████▏    | 227/436 [00:05<00:04, 43.90it/s] 53%|█████▎    | 232/436 [00:05<00:04, 43.92it/s] 54%|█████▍    | 237/436 [00:05<00:04, 43.96it/s] 56%|█████▌    | 242/436 [00:05<00:04, 44.10it/s] 57%|█████▋    | 247/436 [00:05<00:04, 44.20it/s] 58%|█████▊    | 252/436 [00:05<00:04, 44.34it/s] 59%|█████▉    | 257/436 [00:05<00:04, 44.31it/s] 60%|██████    | 262/436 [00:05<00:03, 44.14it/s] 61%|██████    | 267/436 [00:06<00:03, 44.04it/s] 62%|██████▏   | 272/436 [00:06<00:03, 44.01it/s] 64%|██████▎   | 277/436 [00:06<00:03, 44.02it/s] 65%|██████▍   | 282/436 [00:06<00:03, 44.07it/s] 66%|██████▌   | 287/436 [00:06<00:03, 44.13it/s] 67%|██████▋   | 292/436 [00:06<00:03, 44.17it/s] 68%|██████▊   | 297/436 [00:06<00:03, 44.32it/s] 69%|██████▉   | 302/436 [00:06<00:03, 44.25it/s] 70%|███████   | 307/436 [00:06<00:02, 44.12it/s] 72%|███████▏  | 312/436 [00:07<00:02, 44.06it/s] 73%|███████▎  | 317/436 [00:07<00:02, 43.86it/s] 74%|███████▍  | 322/436 [00:07<00:02, 44.02it/s] 75%|███████▌  | 327/436 [00:07<00:02, 44.00it/s] 76%|███████▌  | 332/436 [00:07<00:02, 44.11it/s] 77%|███████▋  | 337/436 [00:07<00:02, 44.24it/s] 78%|███████▊  | 342/436 [00:07<00:02, 44.23it/s] 80%|███████▉  | 347/436 [00:07<00:02, 44.26it/s] 81%|████████  | 352/436 [00:07<00:01, 44.03it/s] 82%|████████▏ | 357/436 [00:08<00:01, 44.03it/s] 83%|████████▎ | 362/436 [00:08<00:01, 44.00it/s] 84%|████████▍ | 367/436 [00:08<00:01, 43.99it/s] 85%|████████▌ | 372/436 [00:08<00:01, 44.11it/s] 86%|████████▋ | 377/436 [00:08<00:01, 44.05it/s] 88%|████████▊ | 382/436 [00:08<00:01, 44.18it/s] 89%|████████▉ | 387/436 [00:08<00:01, 44.23it/s] 90%|████████▉ | 392/436 [00:08<00:00, 44.14it/s] 91%|█████████ | 397/436 [00:08<00:00, 44.13it/s] 92%|█████████▏| 402/436 [00:09<00:00, 44.04it/s] 93%|█████████▎| 407/436 [00:09<00:00, 44.04it/s] 94%|█████████▍| 412/436 [00:09<00:00, 44.12it/s] 96%|█████████▌| 417/436 [00:09<00:00, 44.11it/s] 97%|█████████▋| 422/436 [00:09<00:00, 44.10it/s] 98%|█████████▊| 427/436 [00:09<00:00, 44.08it/s] 99%|█████████▉| 432/436 [00:09<00:00, 43.45it/s]100%|██████████| 436/436 [00:09<00:00, 44.17it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:33:43,186 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:33:43,186 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:33:43,186 >>   eval_loss               =     1.0011
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:33:43,186 >>   eval_runtime            = 0:00:09.88
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:33:43,186 >>   eval_samples            =       3488
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:33:43,186 >>   eval_samples_per_second =    352.773
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:33:43,186 >>   eval_steps_per_second   =     44.097
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:33:43,186 >>   perplexity              =     2.7211
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:48,984 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:48,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:48,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:48,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:48,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:33:49,705 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:33:49,705 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:33:49,976 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:33:51,013 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:33:51,013 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:52,693 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:52,698 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:52,698 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:52,698 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:52,698 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:33:53,021 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:33:53,022 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:33:53,691 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:33:53,858 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:33:53,858 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/checkpoint-351
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11910
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12010, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.32it/s]Extractor Predicting: 2it [00:01,  1.29it/s]Extractor Predicting: 3it [00:02,  1.31it/s]Extractor Predicting: 4it [00:02,  1.35it/s]Extractor Predicting: 5it [00:03,  1.38it/s]Extractor Predicting: 6it [00:04,  1.38it/s]Extractor Predicting: 7it [00:05,  1.40it/s]Extractor Predicting: 8it [00:05,  1.44it/s]Extractor Predicting: 9it [00:06,  1.39it/s]Extractor Predicting: 10it [00:07,  1.40it/s]Extractor Predicting: 11it [00:07,  1.40it/s]Extractor Predicting: 12it [00:08,  1.37it/s]Extractor Predicting: 13it [00:09,  1.37it/s]Extractor Predicting: 14it [00:10,  1.36it/s]Extractor Predicting: 15it [00:10,  1.39it/s]Extractor Predicting: 16it [00:11,  1.37it/s]Extractor Predicting: 17it [00:12,  1.36it/s]Extractor Predicting: 18it [00:13,  1.38it/s]Extractor Predicting: 19it [00:13,  1.40it/s]Extractor Predicting: 20it [00:14,  1.41it/s]Extractor Predicting: 21it [00:15,  1.37it/s]Extractor Predicting: 22it [00:15,  1.39it/s]Extractor Predicting: 23it [00:16,  1.39it/s]Extractor Predicting: 24it [00:17,  1.39it/s]Extractor Predicting: 25it [00:18,  1.37it/s]Extractor Predicting: 26it [00:18,  1.37it/s]Extractor Predicting: 27it [00:19,  1.37it/s]Extractor Predicting: 28it [00:20,  1.38it/s]Extractor Predicting: 29it [00:21,  1.35it/s]Extractor Predicting: 30it [00:21,  1.34it/s]Extractor Predicting: 31it [00:22,  1.33it/s]Extractor Predicting: 32it [00:23,  1.34it/s]Extractor Predicting: 33it [00:24,  1.33it/s]Extractor Predicting: 34it [00:24,  1.32it/s]Extractor Predicting: 35it [00:25,  1.30it/s]Extractor Predicting: 36it [00:26,  1.29it/s]Extractor Predicting: 37it [00:27,  1.30it/s]Extractor Predicting: 38it [00:27,  1.30it/s]Extractor Predicting: 39it [00:28,  1.31it/s]Extractor Predicting: 40it [00:29,  1.30it/s]Extractor Predicting: 41it [00:30,  1.30it/s]Extractor Predicting: 42it [00:31,  1.23it/s]Extractor Predicting: 43it [00:32,  1.22it/s]Extractor Predicting: 44it [00:32,  1.23it/s]Extractor Predicting: 45it [00:33,  1.21it/s]Extractor Predicting: 46it [00:34,  1.22it/s]Extractor Predicting: 47it [00:35,  1.27it/s]Extractor Predicting: 48it [00:36,  1.26it/s]Extractor Predicting: 49it [00:36,  1.26it/s]Extractor Predicting: 50it [00:37,  1.24it/s]Extractor Predicting: 51it [00:38,  1.27it/s]Extractor Predicting: 52it [00:39,  1.29it/s]Extractor Predicting: 53it [00:40,  1.21it/s]Extractor Predicting: 54it [00:40,  1.24it/s]Extractor Predicting: 55it [00:41,  1.26it/s]Extractor Predicting: 56it [00:42,  1.28it/s]Extractor Predicting: 57it [00:43,  1.31it/s]Extractor Predicting: 58it [00:43,  1.32it/s]Extractor Predicting: 59it [00:44,  1.30it/s]Extractor Predicting: 60it [00:45,  1.29it/s]Extractor Predicting: 61it [00:46,  1.28it/s]Extractor Predicting: 62it [00:47,  1.28it/s]Extractor Predicting: 63it [00:47,  1.30it/s]Extractor Predicting: 64it [00:48,  1.29it/s]Extractor Predicting: 65it [00:49,  1.32it/s]Extractor Predicting: 66it [00:49,  1.33it/s]Extractor Predicting: 67it [00:50,  1.33it/s]Extractor Predicting: 68it [00:51,  1.33it/s]Extractor Predicting: 69it [00:52,  1.32it/s]Extractor Predicting: 70it [00:53,  1.32it/s]Extractor Predicting: 71it [00:53,  1.29it/s]Extractor Predicting: 72it [00:54,  1.30it/s]Extractor Predicting: 73it [00:55,  1.30it/s]Extractor Predicting: 74it [00:56,  1.31it/s]Extractor Predicting: 75it [00:56,  1.32it/s]Extractor Predicting: 76it [00:57,  1.32it/s]Extractor Predicting: 77it [00:58,  1.35it/s]Extractor Predicting: 78it [00:59,  1.32it/s]Extractor Predicting: 79it [00:59,  1.31it/s]Extractor Predicting: 80it [01:00,  1.30it/s]Extractor Predicting: 81it [01:01,  1.30it/s]Extractor Predicting: 82it [01:02,  1.31it/s]Extractor Predicting: 83it [01:02,  1.34it/s]Extractor Predicting: 84it [01:03,  1.32it/s]Extractor Predicting: 85it [01:04,  1.31it/s]Extractor Predicting: 86it [01:05,  1.31it/s]Extractor Predicting: 87it [01:05,  1.32it/s]Extractor Predicting: 88it [01:06,  1.33it/s]Extractor Predicting: 89it [01:07,  1.33it/s]Extractor Predicting: 90it [01:08,  1.36it/s]Extractor Predicting: 91it [01:08,  1.39it/s]Extractor Predicting: 92it [01:09,  1.40it/s]Extractor Predicting: 93it [01:10,  1.38it/s]Extractor Predicting: 94it [01:10,  1.40it/s]Extractor Predicting: 95it [01:11,  1.40it/s]Extractor Predicting: 96it [01:12,  1.39it/s]Extractor Predicting: 97it [01:13,  1.38it/s]Extractor Predicting: 98it [01:13,  1.36it/s]Extractor Predicting: 99it [01:14,  1.32it/s]Extractor Predicting: 100it [01:15,  1.32it/s]Extractor Predicting: 101it [01:16,  1.36it/s]Extractor Predicting: 102it [01:16,  1.40it/s]Extractor Predicting: 103it [01:17,  1.39it/s]Extractor Predicting: 104it [01:18,  1.38it/s]Extractor Predicting: 105it [01:19,  1.37it/s]Extractor Predicting: 106it [01:19,  1.39it/s]Extractor Predicting: 107it [01:20,  1.37it/s]Extractor Predicting: 108it [01:21,  1.37it/s]Extractor Predicting: 109it [01:21,  1.37it/s]Extractor Predicting: 110it [01:22,  1.25it/s]Extractor Predicting: 111it [01:23,  1.32it/s]Extractor Predicting: 112it [01:24,  1.34it/s]Extractor Predicting: 113it [01:24,  1.39it/s]Extractor Predicting: 114it [01:25,  1.39it/s]Extractor Predicting: 115it [01:26,  1.41it/s]Extractor Predicting: 116it [01:27,  1.40it/s]Extractor Predicting: 117it [01:27,  1.38it/s]Extractor Predicting: 118it [01:28,  1.36it/s]Extractor Predicting: 119it [01:29,  1.33it/s]Extractor Predicting: 120it [01:30,  1.32it/s]Extractor Predicting: 121it [01:30,  1.35it/s]Extractor Predicting: 122it [01:31,  1.35it/s]Extractor Predicting: 123it [01:32,  1.34it/s]Extractor Predicting: 124it [01:33,  1.32it/s]Extractor Predicting: 125it [01:33,  1.31it/s]Extractor Predicting: 126it [01:34,  1.29it/s]Extractor Predicting: 127it [01:35,  1.30it/s]Extractor Predicting: 128it [01:36,  1.35it/s]Extractor Predicting: 129it [01:36,  1.32it/s]Extractor Predicting: 130it [01:37,  1.35it/s]Extractor Predicting: 131it [01:38,  1.33it/s]Extractor Predicting: 132it [01:39,  1.32it/s]Extractor Predicting: 133it [01:39,  1.32it/s]Extractor Predicting: 134it [01:40,  1.29it/s]Extractor Predicting: 135it [01:41,  1.29it/s]Extractor Predicting: 136it [01:42,  1.31it/s]Extractor Predicting: 137it [01:43,  1.31it/s]Extractor Predicting: 138it [01:43,  1.31it/s]Extractor Predicting: 139it [01:44,  1.30it/s]Extractor Predicting: 140it [01:45,  1.33it/s]Extractor Predicting: 141it [01:46,  1.32it/s]Extractor Predicting: 142it [01:46,  1.29it/s]Extractor Predicting: 143it [01:47,  1.32it/s]Extractor Predicting: 144it [01:47,  1.59it/s]Extractor Predicting: 144it [01:47,  1.33it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:35:49,586 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:35:49,592 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:35:49,592 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:35:49,592 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:35:49,592 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:35:49,890 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:35:49,891 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:35:50,555 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:35:51,582 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:35:51,582 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:35:53,721 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:35:53,726 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:35:53,726 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:35:53,726 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:35:53,726 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:35:54,045 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:35:54,046 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:35:54,320 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:35:54,483 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:35:54,483 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4043243243243243,
  "recall": 0.10722477064220183,
  "score": 0.16949920688873782,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19834
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19934, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.44it/s]Extractor Predicting: 2it [00:01,  1.27it/s]Extractor Predicting: 3it [00:02,  1.28it/s]Extractor Predicting: 4it [00:03,  1.28it/s]Extractor Predicting: 5it [00:03,  1.27it/s]Extractor Predicting: 6it [00:04,  1.28it/s]Extractor Predicting: 7it [00:05,  1.31it/s]Extractor Predicting: 8it [00:06,  1.31it/s]Extractor Predicting: 9it [00:06,  1.34it/s]Extractor Predicting: 10it [00:07,  1.30it/s]Extractor Predicting: 11it [00:08,  1.29it/s]Extractor Predicting: 12it [00:09,  1.31it/s]Extractor Predicting: 13it [00:09,  1.32it/s]Extractor Predicting: 14it [00:10,  1.32it/s]Extractor Predicting: 15it [00:11,  1.31it/s]Extractor Predicting: 16it [00:12,  1.31it/s]Extractor Predicting: 17it [00:13,  1.29it/s]Extractor Predicting: 18it [00:13,  1.31it/s]Extractor Predicting: 19it [00:14,  1.33it/s]Extractor Predicting: 20it [00:15,  1.33it/s]Extractor Predicting: 21it [00:16,  1.33it/s]Extractor Predicting: 22it [00:16,  1.36it/s]Extractor Predicting: 23it [00:17,  1.37it/s]Extractor Predicting: 24it [00:18,  1.34it/s]Extractor Predicting: 25it [00:19,  1.31it/s]Extractor Predicting: 26it [00:19,  1.34it/s]Extractor Predicting: 27it [00:20,  1.33it/s]Extractor Predicting: 28it [00:21,  1.31it/s]Extractor Predicting: 29it [00:22,  1.32it/s]Extractor Predicting: 30it [00:22,  1.29it/s]Extractor Predicting: 31it [00:23,  1.35it/s]Extractor Predicting: 32it [00:24,  1.41it/s]Extractor Predicting: 33it [00:24,  1.44it/s]Extractor Predicting: 34it [00:25,  1.45it/s]Extractor Predicting: 35it [00:26,  1.47it/s]Extractor Predicting: 36it [00:26,  1.47it/s]Extractor Predicting: 37it [00:27,  1.48it/s]Extractor Predicting: 38it [00:28,  1.51it/s]Extractor Predicting: 39it [00:28,  1.53it/s]Extractor Predicting: 40it [00:29,  1.55it/s]Extractor Predicting: 41it [00:30,  1.53it/s]Extractor Predicting: 42it [00:30,  1.54it/s]Extractor Predicting: 43it [00:31,  1.53it/s]Extractor Predicting: 44it [00:31,  1.55it/s]Extractor Predicting: 45it [00:32,  1.54it/s]Extractor Predicting: 46it [00:33,  1.49it/s]Extractor Predicting: 47it [00:33,  1.52it/s]Extractor Predicting: 48it [00:34,  1.56it/s]Extractor Predicting: 49it [00:35,  1.56it/s]Extractor Predicting: 50it [00:35,  1.54it/s]Extractor Predicting: 51it [00:36,  1.57it/s]Extractor Predicting: 52it [00:37,  1.54it/s]Extractor Predicting: 53it [00:37,  1.53it/s]Extractor Predicting: 54it [00:38,  1.52it/s]Extractor Predicting: 55it [00:39,  1.49it/s]Extractor Predicting: 56it [00:39,  1.50it/s]Extractor Predicting: 57it [00:40,  1.49it/s]Extractor Predicting: 58it [00:41,  1.46it/s]Extractor Predicting: 59it [00:42,  1.42it/s]Extractor Predicting: 60it [00:42,  1.37it/s]Extractor Predicting: 61it [00:43,  1.32it/s]Extractor Predicting: 62it [00:44,  1.27it/s]Extractor Predicting: 63it [00:45,  1.27it/s]Extractor Predicting: 64it [00:46,  1.20it/s]Extractor Predicting: 65it [00:46,  1.24it/s]Extractor Predicting: 66it [00:47,  1.25it/s]Extractor Predicting: 67it [00:48,  1.26it/s]Extractor Predicting: 68it [00:49,  1.24it/s]Extractor Predicting: 69it [00:50,  1.23it/s]Extractor Predicting: 70it [00:51,  1.22it/s]Extractor Predicting: 71it [00:51,  1.19it/s]Extractor Predicting: 72it [00:52,  1.20it/s]Extractor Predicting: 73it [00:53,  1.21it/s]Extractor Predicting: 74it [00:54,  1.25it/s]Extractor Predicting: 75it [00:55,  1.27it/s]Extractor Predicting: 76it [00:55,  1.28it/s]Extractor Predicting: 77it [00:56,  1.28it/s]Extractor Predicting: 78it [00:57,  1.26it/s]Extractor Predicting: 79it [00:58,  1.25it/s]Extractor Predicting: 80it [00:59,  1.26it/s]Extractor Predicting: 81it [00:59,  1.25it/s]Extractor Predicting: 82it [01:00,  1.25it/s]Extractor Predicting: 83it [01:01,  1.26it/s]Extractor Predicting: 84it [01:02,  1.28it/s]Extractor Predicting: 85it [01:02,  1.27it/s]Extractor Predicting: 86it [01:03,  1.25it/s]Extractor Predicting: 87it [01:04,  1.25it/s]Extractor Predicting: 88it [01:05,  1.16it/s]Extractor Predicting: 89it [01:06,  1.19it/s]Extractor Predicting: 90it [01:07,  1.24it/s]Extractor Predicting: 91it [01:07,  1.23it/s]Extractor Predicting: 92it [01:08,  1.21it/s]Extractor Predicting: 93it [01:09,  1.26it/s]Extractor Predicting: 94it [01:10,  1.26it/s]Extractor Predicting: 95it [01:11,  1.29it/s]Extractor Predicting: 96it [01:11,  1.33it/s]Extractor Predicting: 97it [01:12,  1.33it/s]Extractor Predicting: 98it [01:13,  1.35it/s]Extractor Predicting: 99it [01:13,  1.34it/s]Extractor Predicting: 100it [01:14,  1.34it/s]Extractor Predicting: 101it [01:15,  1.39it/s]Extractor Predicting: 102it [01:16,  1.39it/s]Extractor Predicting: 103it [01:16,  1.35it/s]Extractor Predicting: 104it [01:17,  1.31it/s]Extractor Predicting: 105it [01:18,  1.32it/s]Extractor Predicting: 106it [01:19,  1.31it/s]Extractor Predicting: 107it [01:19,  1.32it/s]Extractor Predicting: 108it [01:20,  1.31it/s]Extractor Predicting: 109it [01:21,  1.30it/s]Extractor Predicting: 110it [01:22,  1.28it/s]Extractor Predicting: 111it [01:23,  1.31it/s]Extractor Predicting: 112it [01:23,  1.29it/s]Extractor Predicting: 113it [01:24,  1.32it/s]Extractor Predicting: 114it [01:25,  1.29it/s]Extractor Predicting: 115it [01:26,  1.30it/s]Extractor Predicting: 116it [01:26,  1.35it/s]Extractor Predicting: 117it [01:27,  1.35it/s]Extractor Predicting: 118it [01:28,  1.40it/s]Extractor Predicting: 119it [01:28,  1.41it/s]Extractor Predicting: 120it [01:29,  1.41it/s]Extractor Predicting: 121it [01:30,  1.44it/s]Extractor Predicting: 122it [01:30,  1.43it/s]Extractor Predicting: 123it [01:31,  1.45it/s]Extractor Predicting: 124it [01:32,  1.46it/s]Extractor Predicting: 125it [01:33,  1.47it/s]Extractor Predicting: 126it [01:33,  1.45it/s]Extractor Predicting: 127it [01:34,  1.45it/s]Extractor Predicting: 128it [01:35,  1.49it/s]Extractor Predicting: 129it [01:35,  1.47it/s]Extractor Predicting: 130it [01:36,  1.54it/s]Extractor Predicting: 131it [01:36,  1.56it/s]Extractor Predicting: 132it [01:37,  1.52it/s]Extractor Predicting: 133it [01:38,  1.47it/s]Extractor Predicting: 134it [01:39,  1.46it/s]Extractor Predicting: 135it [01:39,  1.48it/s]Extractor Predicting: 136it [01:40,  1.51it/s]Extractor Predicting: 137it [01:41,  1.51it/s]Extractor Predicting: 138it [01:41,  1.50it/s]Extractor Predicting: 139it [01:42,  1.47it/s]Extractor Predicting: 140it [01:43,  1.49it/s]Extractor Predicting: 141it [01:43,  1.47it/s]Extractor Predicting: 142it [01:44,  1.53it/s]Extractor Predicting: 143it [01:45,  1.51it/s]Extractor Predicting: 144it [01:45,  1.50it/s]Extractor Predicting: 145it [01:46,  1.48it/s]Extractor Predicting: 146it [01:47,  1.44it/s]Extractor Predicting: 147it [01:47,  1.41it/s]Extractor Predicting: 148it [01:48,  1.41it/s]Extractor Predicting: 149it [01:49,  1.38it/s]Extractor Predicting: 150it [01:50,  1.37it/s]Extractor Predicting: 151it [01:50,  1.37it/s]Extractor Predicting: 152it [01:51,  1.37it/s]Extractor Predicting: 153it [01:52,  1.34it/s]Extractor Predicting: 154it [01:53,  1.36it/s]Extractor Predicting: 155it [01:53,  1.38it/s]Extractor Predicting: 156it [01:54,  1.36it/s]Extractor Predicting: 157it [01:55,  1.36it/s]Extractor Predicting: 158it [01:55,  1.35it/s]Extractor Predicting: 159it [01:56,  1.33it/s]Extractor Predicting: 160it [01:57,  1.33it/s]Extractor Predicting: 161it [01:58,  1.32it/s]Extractor Predicting: 162it [01:59,  1.32it/s]Extractor Predicting: 163it [01:59,  1.34it/s]Extractor Predicting: 164it [02:00,  1.34it/s]Extractor Predicting: 165it [02:01,  1.33it/s]Extractor Predicting: 166it [02:02,  1.30it/s]Extractor Predicting: 167it [02:02,  1.32it/s]Extractor Predicting: 168it [02:03,  1.31it/s]Extractor Predicting: 169it [02:04,  1.34it/s]Extractor Predicting: 170it [02:05,  1.33it/s]Extractor Predicting: 171it [02:06,  1.21it/s]Extractor Predicting: 172it [02:06,  1.25it/s]Extractor Predicting: 173it [02:07,  1.25it/s]Extractor Predicting: 174it [02:08,  1.26it/s]Extractor Predicting: 175it [02:09,  1.29it/s]Extractor Predicting: 176it [02:09,  1.30it/s]Extractor Predicting: 177it [02:10,  1.29it/s]Extractor Predicting: 178it [02:11,  1.30it/s]Extractor Predicting: 179it [02:12,  1.30it/s]Extractor Predicting: 180it [02:12,  1.30it/s]Extractor Predicting: 181it [02:13,  1.29it/s]Extractor Predicting: 182it [02:14,  1.30it/s]Extractor Predicting: 183it [02:15,  1.29it/s]Extractor Predicting: 184it [02:16,  1.30it/s]Extractor Predicting: 185it [02:16,  1.31it/s]Extractor Predicting: 186it [02:17,  1.29it/s]Extractor Predicting: 187it [02:18,  1.31it/s]Extractor Predicting: 188it [02:19,  1.31it/s]Extractor Predicting: 189it [02:19,  1.33it/s]Extractor Predicting: 190it [02:20,  1.32it/s]Extractor Predicting: 191it [02:21,  1.34it/s]Extractor Predicting: 192it [02:22,  1.34it/s]Extractor Predicting: 193it [02:22,  1.37it/s]Extractor Predicting: 194it [02:23,  1.34it/s]Extractor Predicting: 195it [02:24,  1.34it/s]Extractor Predicting: 196it [02:25,  1.34it/s]Extractor Predicting: 197it [02:25,  1.34it/s]Extractor Predicting: 198it [02:26,  1.30it/s]Extractor Predicting: 199it [02:27,  1.31it/s]Extractor Predicting: 200it [02:28,  1.31it/s]Extractor Predicting: 201it [02:28,  1.35it/s]Extractor Predicting: 202it [02:29,  1.33it/s]Extractor Predicting: 203it [02:30,  1.36it/s]Extractor Predicting: 204it [02:31,  1.34it/s]Extractor Predicting: 205it [02:31,  1.34it/s]Extractor Predicting: 206it [02:32,  1.39it/s]Extractor Predicting: 207it [02:33,  1.38it/s]Extractor Predicting: 208it [02:33,  1.39it/s]Extractor Predicting: 209it [02:34,  1.38it/s]Extractor Predicting: 210it [02:35,  1.37it/s]Extractor Predicting: 211it [02:36,  1.35it/s]Extractor Predicting: 212it [02:36,  1.32it/s]Extractor Predicting: 213it [02:37,  1.32it/s]Extractor Predicting: 214it [02:38,  1.32it/s]Extractor Predicting: 215it [02:39,  1.30it/s]Extractor Predicting: 216it [02:40,  1.31it/s]Extractor Predicting: 217it [02:40,  1.27it/s]Extractor Predicting: 218it [02:41,  1.29it/s]Extractor Predicting: 219it [02:42,  1.30it/s]Extractor Predicting: 220it [02:43,  1.34it/s]Extractor Predicting: 221it [02:43,  1.33it/s]Extractor Predicting: 222it [02:44,  1.32it/s]Extractor Predicting: 223it [02:45,  1.34it/s]Extractor Predicting: 224it [02:46,  1.33it/s]Extractor Predicting: 225it [02:46,  1.32it/s]Extractor Predicting: 226it [02:47,  1.33it/s]Extractor Predicting: 227it [02:48,  1.31it/s]Extractor Predicting: 228it [02:49,  1.35it/s]Extractor Predicting: 229it [02:49,  1.32it/s]Extractor Predicting: 230it [02:50,  1.34it/s]Extractor Predicting: 231it [02:51,  1.30it/s]Extractor Predicting: 232it [02:52,  1.29it/s]Extractor Predicting: 233it [02:52,  1.30it/s]Extractor Predicting: 234it [02:53,  1.32it/s]Extractor Predicting: 235it [02:54,  1.33it/s]Extractor Predicting: 236it [02:55,  1.34it/s]Extractor Predicting: 237it [02:56,  1.28it/s]Extractor Predicting: 238it [02:56,  1.27it/s]Extractor Predicting: 239it [02:57,  1.26it/s]Extractor Predicting: 240it [02:58,  1.28it/s]Extractor Predicting: 241it [02:59,  1.28it/s]Extractor Predicting: 242it [02:59,  1.30it/s]Extractor Predicting: 243it [03:00,  1.30it/s]Extractor Predicting: 244it [03:01,  1.31it/s]Extractor Predicting: 245it [03:02,  1.30it/s]Extractor Predicting: 246it [03:02,  1.28it/s]Extractor Predicting: 247it [03:03,  1.24it/s]Extractor Predicting: 248it [03:04,  1.26it/s]Extractor Predicting: 249it [03:05,  1.31it/s]Extractor Predicting: 250it [03:06,  1.33it/s]Extractor Predicting: 251it [03:06,  1.31it/s]Extractor Predicting: 252it [03:07,  1.34it/s]Extractor Predicting: 253it [03:08,  1.34it/s]Extractor Predicting: 254it [03:09,  1.31it/s]Extractor Predicting: 255it [03:09,  1.31it/s]Extractor Predicting: 256it [03:10,  1.29it/s]Extractor Predicting: 257it [03:11,  1.30it/s]Extractor Predicting: 258it [03:12,  1.29it/s]Extractor Predicting: 259it [03:12,  1.32it/s]Extractor Predicting: 260it [03:13,  1.33it/s]Extractor Predicting: 261it [03:14,  1.34it/s]Extractor Predicting: 262it [03:15,  1.34it/s]Extractor Predicting: 263it [03:15,  1.33it/s]Extractor Predicting: 264it [03:16,  1.34it/s]Extractor Predicting: 265it [03:17,  1.34it/s]Extractor Predicting: 266it [03:18,  1.34it/s]Extractor Predicting: 267it [03:18,  1.36it/s]Extractor Predicting: 268it [03:19,  1.35it/s]Extractor Predicting: 269it [03:20,  1.34it/s]Extractor Predicting: 270it [03:21,  1.31it/s]Extractor Predicting: 271it [03:21,  1.34it/s]Extractor Predicting: 272it [03:22,  1.35it/s]Extractor Predicting: 273it [03:23,  1.37it/s]Extractor Predicting: 274it [03:23,  1.38it/s]Extractor Predicting: 275it [03:24,  1.39it/s]Extractor Predicting: 276it [03:25,  1.36it/s]Extractor Predicting: 277it [03:26,  1.24it/s]Extractor Predicting: 278it [03:27,  1.28it/s]Extractor Predicting: 279it [03:27,  1.29it/s]Extractor Predicting: 280it [03:28,  1.29it/s]Extractor Predicting: 281it [03:29,  1.33it/s]Extractor Predicting: 282it [03:30,  1.32it/s]Extractor Predicting: 283it [03:31,  1.29it/s]Extractor Predicting: 284it [03:31,  1.49it/s]Extractor Predicting: 284it [03:31,  1.34it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:39:32,695 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:39:32,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:39:32,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:39:32,698 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:39:32,698 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:39:33,049 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:39:33,050 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:39:33,307 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:39:34,374 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:39:34,374 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:39:35,659 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:39:35,664 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:39:35,664 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:39:35,664 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:39:35,664 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:39:35,995 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:39:35,995 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:39:36,262 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:39:36,407 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:39:36,407 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5160965794768612,
  "recall": 0.07541899441340782,
  "score": 0.13160595177013854,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1045
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1145, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.28it/s]Extractor Predicting: 2it [00:01,  1.17it/s]Extractor Predicting: 3it [00:02,  1.20it/s]Extractor Predicting: 4it [00:03,  1.22it/s]Extractor Predicting: 5it [00:03,  1.67it/s]Extractor Predicting: 5it [00:03,  1.43it/s]
[INFO|configuration_utils.py:515] 2023-08-28 23:39:40,343 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:39:40,346 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:39:40,351 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:39:40,351 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 23:39:40,353 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:39:44,227 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 23:39:44,227 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 23:39:44,247 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:39:44,248 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:39:44,255 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:39:44,266 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:39:44,266 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:39:44,266 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:39:44,266 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:39:44,266 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:39:44,266 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 23:39:44,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:45,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:45,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:46,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:47,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:48,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:48,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:49,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:50,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:50,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:51,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:52,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:52,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:53,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:54,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:54,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:55,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:56,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:57,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:57,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:58,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:39:59,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:00,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:48, 16.33s/it][WARNING|generation_utils.py:914] 2023-08-28 23:40:00,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:01,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:02,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:02,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:03,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:04,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:05,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:05,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:06,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:06,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:07,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:08,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:08,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:09,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:09,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:10,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:11,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:11,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:12,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:13,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:13,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:29<03:11, 14.72s/it][WARNING|generation_utils.py:914] 2023-08-28 23:40:14,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:15,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:15,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:16,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:17,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:18,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:19,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:19,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:20,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:21,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:22,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:23,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:23,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:24,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:25,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:25,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:26,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:27,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:28,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:29,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:29,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:45<03:02, 15.18s/it][WARNING|generation_utils.py:914] 2023-08-28 23:40:30,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:31,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:31,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:32,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:33,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:34,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:34,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:35,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:36,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:37,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:37,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:38,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:38,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:39,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:40,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:41,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:41,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:42,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:42,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:43,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:44,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:45,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:45,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:02<02:52, 15.67s/it][WARNING|generation_utils.py:914] 2023-08-28 23:40:46,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:47,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:47,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:48,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:49,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:50,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:50,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:51,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:52,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:52,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:53,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:54,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:54,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:55,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:55,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:56,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:57,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:57,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:58,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:59,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:40:59,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:15<02:29, 14.91s/it][WARNING|generation_utils.py:914] 2023-08-28 23:41:00,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:00,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:01,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:02,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:02,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:03,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:03,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:04,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:05,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:05,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:06,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:07,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:07,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:08,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:08,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:09,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:10,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:10,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:11,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:12,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:12,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:13,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:29<02:11, 14.57s/it][WARNING|generation_utils.py:914] 2023-08-28 23:41:14,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:14,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:15,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:15,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:16,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:17,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:18,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:18,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:19,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:19,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:20,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:21,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:21,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:22,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:23,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:23,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:24,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:24,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:25,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:26,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:26,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:27,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:28,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:44<01:57, 14.68s/it][WARNING|generation_utils.py:914] 2023-08-28 23:41:28,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:29,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:30,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:30,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:31,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:32,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:33,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:34,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:34,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:35,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:36,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:37,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:38,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:38,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:39,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:40,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:41,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:41,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:42,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:43,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:44,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:44,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:45,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:01<01:48, 15.56s/it][WARNING|generation_utils.py:914] 2023-08-28 23:41:46,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:47,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:47,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:48,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:49,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:49,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:50,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:51,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:52,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:52,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:53,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:54,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:55,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:56,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:57,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:57,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:58,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:59,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:41:59,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:00,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:01,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:17<01:33, 15.62s/it][WARNING|generation_utils.py:914] 2023-08-28 23:42:02,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:02,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:03,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:04,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:04,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:05,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:06,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:06,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:07,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:08,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:08,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:09,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:10,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:10,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:11,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:12,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:12,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:13,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:14,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:14,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:15,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:31<01:15, 15.08s/it][WARNING|generation_utils.py:914] 2023-08-28 23:42:16,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:16,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:17,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:17,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:18,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:19,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:19,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:20,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:21,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:21,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:22,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:23,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:23,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:24,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:25,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:25,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:26,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:26,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:27,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:28,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:44<00:57, 14.41s/it][WARNING|generation_utils.py:914] 2023-08-28 23:42:28,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:29,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:30,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:31,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:31,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:32,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:33,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:33,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:34,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:35,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:36,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:37,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:37,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:38,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:39,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:40,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:40,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:41,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:42,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:42,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:43,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:00<00:44, 14.82s/it][WARNING|generation_utils.py:914] 2023-08-28 23:42:44,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:45,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:46,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:46,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:47,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:48,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:48,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:49,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:50,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:50,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:51,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:52,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:53,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:54,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:55,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:55,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:56,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:56,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:57,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:58,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:42:58,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:15<00:29, 14.87s/it][WARNING|generation_utils.py:914] 2023-08-28 23:42:59,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:00,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:00,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:01,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:02,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:03,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:03,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:04,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:05,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:05,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:06,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:07,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:08,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:08,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:09,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:10,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:10,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:11,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:12,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:12,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:13,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:29<00:14, 14.85s/it][WARNING|generation_utils.py:914] 2023-08-28 23:43:14,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:15,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:15,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:16,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:17,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:17,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:18,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:19,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:19,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:20,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:21,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:21,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:22,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:23,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:23,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:24,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:25,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:26,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:26,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:27,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:43<00:00, 14.49s/it]Generating: 100%|██████████| 15/15 [03:43<00:00, 14.91s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:43:33,081 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:43:33,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:43:33,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:43:33,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:43:33,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:43:33,411 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:43:33,412 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:43:33,686 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:43:34,717 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:43:34,717 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:43:36,429 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:43:36,434 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:43:36,434 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:43:36,435 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:43:36,435 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:43:36,765 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:43:36,766 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:43:37,026 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:43:37,194 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:43:37,195 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 560, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : genre .', 'success_rate': 0.8383152173913043, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9345238095238095, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : participant in .', 'success_rate': 0.84375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : participating team .', 'success_rate': 0.9047619047619048, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : competition class .', 'success_rate': 0.890625, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8315217391304348, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 565, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : father .', 'success_rate': 0.84375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.9077380952380952, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.946875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.9300595238095238, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8973214285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.9032738095238095, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.953125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/synthetic/4_ext.jsonl'}}
estimate vocab size: 8891
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8991, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.06it/s]Extractor Estimating: 2it [00:01,  1.24it/s]Extractor Estimating: 3it [00:02,  1.29it/s]Extractor Estimating: 4it [00:03,  1.35it/s]Extractor Estimating: 5it [00:03,  1.34it/s]Extractor Estimating: 6it [00:04,  1.41it/s]Extractor Estimating: 7it [00:05,  1.36it/s]Extractor Estimating: 8it [00:05,  1.39it/s]Extractor Estimating: 9it [00:06,  1.37it/s]Extractor Estimating: 10it [00:07,  1.41it/s]Extractor Estimating: 11it [00:08,  1.34it/s]Extractor Estimating: 12it [00:08,  1.34it/s]Extractor Estimating: 13it [00:09,  1.41it/s]Extractor Estimating: 14it [00:10,  1.46it/s]Extractor Estimating: 15it [00:10,  1.39it/s]Extractor Estimating: 16it [00:11,  1.43it/s]Extractor Estimating: 17it [00:12,  1.47it/s]Extractor Estimating: 18it [00:13,  1.42it/s]Extractor Estimating: 19it [00:13,  1.39it/s]Extractor Estimating: 20it [00:14,  1.40it/s]Extractor Estimating: 21it [00:15,  1.35it/s]Extractor Estimating: 22it [00:16,  1.36it/s]Extractor Estimating: 23it [00:16,  1.34it/s]Extractor Estimating: 24it [00:17,  1.34it/s]Extractor Estimating: 25it [00:18,  1.40it/s]Extractor Estimating: 26it [00:18,  1.46it/s]Extractor Estimating: 27it [00:19,  1.56it/s]Extractor Estimating: 28it [00:19,  1.56it/s]Extractor Estimating: 29it [00:20,  1.57it/s]Extractor Estimating: 30it [00:21,  1.56it/s]Extractor Estimating: 31it [00:21,  1.65it/s]Extractor Estimating: 32it [00:22,  1.65it/s]Extractor Estimating: 33it [00:23,  1.60it/s]Extractor Estimating: 34it [00:23,  1.73it/s]Extractor Estimating: 35it [00:24,  1.67it/s]Extractor Estimating: 36it [00:24,  1.71it/s]Extractor Estimating: 37it [00:25,  1.74it/s]Extractor Estimating: 38it [00:25,  1.75it/s]Extractor Estimating: 39it [00:26,  1.78it/s]Extractor Estimating: 40it [00:26,  1.79it/s]Extractor Estimating: 41it [00:27,  1.78it/s]Extractor Estimating: 42it [00:28,  1.77it/s]Extractor Estimating: 43it [00:28,  1.82it/s]Extractor Estimating: 44it [00:29,  1.80it/s]Extractor Estimating: 45it [00:29,  1.82it/s]Extractor Estimating: 46it [00:30,  1.81it/s]Extractor Estimating: 47it [00:30,  1.71it/s]Extractor Estimating: 48it [00:31,  1.71it/s]Extractor Estimating: 49it [00:31,  1.80it/s]Extractor Estimating: 50it [00:32,  1.75it/s]Extractor Estimating: 51it [00:33,  1.64it/s]Extractor Estimating: 52it [00:34,  1.52it/s]Extractor Estimating: 53it [00:34,  1.45it/s]Extractor Estimating: 54it [00:35,  1.48it/s]Extractor Estimating: 55it [00:36,  1.46it/s]Extractor Estimating: 56it [00:36,  1.48it/s]Extractor Estimating: 57it [00:37,  1.52it/s]Extractor Estimating: 58it [00:38,  1.50it/s]Extractor Estimating: 59it [00:38,  1.49it/s]Extractor Estimating: 60it [00:39,  1.45it/s]Extractor Estimating: 61it [00:40,  1.41it/s]Extractor Estimating: 62it [00:41,  1.41it/s]Extractor Estimating: 63it [00:41,  1.41it/s]Extractor Estimating: 64it [00:42,  1.27it/s]Extractor Estimating: 65it [00:43,  1.32it/s]Extractor Estimating: 66it [00:44,  1.38it/s]Extractor Estimating: 67it [00:44,  1.41it/s]Extractor Estimating: 68it [00:45,  1.40it/s]Extractor Estimating: 69it [00:46,  1.36it/s]Extractor Estimating: 70it [00:46,  1.37it/s]Extractor Estimating: 71it [00:47,  1.40it/s]Extractor Estimating: 72it [00:48,  1.38it/s]Extractor Estimating: 73it [00:49,  1.34it/s]Extractor Estimating: 74it [00:49,  1.36it/s]Extractor Estimating: 75it [00:50,  1.43it/s]Extractor Estimating: 76it [00:51,  1.35it/s]Extractor Estimating: 77it [00:51,  1.38it/s]Extractor Estimating: 78it [00:52,  1.42it/s]Extractor Estimating: 79it [00:53,  1.46it/s]Extractor Estimating: 80it [00:53,  1.52it/s]Extractor Estimating: 81it [00:54,  1.55it/s]Extractor Estimating: 82it [00:55,  1.52it/s]Extractor Estimating: 83it [00:55,  1.50it/s]Extractor Estimating: 84it [00:56,  1.52it/s]Extractor Estimating: 85it [00:57,  1.51it/s]Extractor Estimating: 86it [00:57,  1.55it/s]Extractor Estimating: 87it [00:58,  1.50it/s]Extractor Estimating: 88it [00:59,  1.51it/s]Extractor Estimating: 89it [00:59,  1.56it/s]Extractor Estimating: 90it [01:00,  1.57it/s]Extractor Estimating: 91it [01:01,  1.55it/s]Extractor Estimating: 92it [01:01,  1.58it/s]Extractor Estimating: 93it [01:02,  1.60it/s]Extractor Estimating: 94it [01:02,  1.62it/s]Extractor Estimating: 95it [01:03,  1.59it/s]Extractor Estimating: 96it [01:04,  1.59it/s]Extractor Estimating: 97it [01:04,  1.57it/s]Extractor Estimating: 98it [01:05,  1.54it/s]Extractor Estimating: 99it [01:06,  1.53it/s]Extractor Estimating: 100it [01:06,  1.57it/s]Extractor Estimating: 101it [01:07,  1.60it/s]Extractor Estimating: 102it [01:07,  1.65it/s]Extractor Estimating: 103it [01:08,  1.66it/s]Extractor Estimating: 104it [01:09,  1.67it/s]Extractor Estimating: 105it [01:09,  1.67it/s]Extractor Estimating: 106it [01:10,  1.54it/s]Extractor Estimating: 107it [01:11,  1.57it/s]Extractor Estimating: 108it [01:11,  1.60it/s]Extractor Estimating: 109it [01:12,  1.66it/s]Extractor Estimating: 110it [01:12,  1.66it/s]Extractor Estimating: 111it [01:13,  1.66it/s]Extractor Estimating: 112it [01:14,  1.65it/s]Extractor Estimating: 113it [01:14,  1.68it/s]Extractor Estimating: 114it [01:15,  1.67it/s]Extractor Estimating: 115it [01:15,  1.69it/s]Extractor Estimating: 116it [01:16,  1.67it/s]Extractor Estimating: 117it [01:17,  1.64it/s]Extractor Estimating: 118it [01:17,  1.69it/s]Extractor Estimating: 119it [01:18,  1.70it/s]Extractor Estimating: 120it [01:18,  1.69it/s]Extractor Estimating: 121it [01:19,  1.76it/s]Extractor Estimating: 122it [01:19,  1.71it/s]Extractor Estimating: 123it [01:20,  1.45it/s]Extractor Estimating: 124it [01:21,  1.52it/s]Extractor Estimating: 125it [01:21,  1.58it/s]Extractor Estimating: 126it [01:22,  1.63it/s]Extractor Estimating: 127it [01:23,  1.66it/s]Extractor Estimating: 128it [01:23,  1.69it/s]Extractor Estimating: 129it [01:24,  1.72it/s]Extractor Estimating: 130it [01:24,  1.75it/s]Extractor Estimating: 131it [01:25,  1.72it/s]Extractor Estimating: 132it [01:26,  1.71it/s]Extractor Estimating: 133it [01:26,  1.72it/s]Extractor Estimating: 134it [01:27,  1.74it/s]Extractor Estimating: 135it [01:27,  1.78it/s]Extractor Estimating: 136it [01:28,  1.79it/s]Extractor Estimating: 137it [01:28,  1.80it/s]Extractor Estimating: 138it [01:29,  1.76it/s]Extractor Estimating: 139it [01:29,  1.79it/s]Extractor Estimating: 140it [01:30,  1.79it/s]Extractor Estimating: 141it [01:31,  1.75it/s]Extractor Estimating: 142it [01:31,  1.71it/s]Extractor Estimating: 143it [01:32,  1.76it/s]Extractor Estimating: 144it [01:32,  1.77it/s]Extractor Estimating: 145it [01:33,  1.77it/s]Extractor Estimating: 146it [01:33,  1.74it/s]Extractor Estimating: 147it [01:34,  1.79it/s]Extractor Estimating: 148it [01:34,  1.83it/s]Extractor Estimating: 149it [01:35,  1.76it/s]Extractor Estimating: 150it [01:36,  1.72it/s]Extractor Estimating: 151it [01:36,  1.71it/s]Extractor Estimating: 152it [01:37,  1.70it/s]Extractor Estimating: 153it [01:37,  1.72it/s]Extractor Estimating: 154it [01:38,  1.66it/s]Extractor Estimating: 155it [01:39,  1.69it/s]Extractor Estimating: 156it [01:39,  1.61it/s]Extractor Estimating: 157it [01:40,  1.66it/s]Extractor Estimating: 158it [01:40,  1.70it/s]Extractor Estimating: 159it [01:41,  1.74it/s]Extractor Estimating: 160it [01:42,  1.74it/s]Extractor Estimating: 161it [01:42,  1.68it/s]Extractor Estimating: 162it [01:43,  1.70it/s]Extractor Estimating: 163it [01:43,  1.79it/s]Extractor Estimating: 164it [01:44,  1.74it/s]Extractor Estimating: 165it [01:45,  1.67it/s]Extractor Estimating: 166it [01:45,  1.67it/s]Extractor Estimating: 167it [01:46,  1.69it/s]Extractor Estimating: 168it [01:46,  1.66it/s]Extractor Estimating: 169it [01:47,  1.69it/s]Extractor Estimating: 170it [01:48,  1.64it/s]Extractor Estimating: 171it [01:48,  1.70it/s]Extractor Estimating: 172it [01:49,  1.67it/s]Extractor Estimating: 173it [01:49,  1.64it/s]Extractor Estimating: 174it [01:50,  1.64it/s]Extractor Estimating: 175it [01:51,  1.63it/s]Extractor Estimating: 176it [01:51,  1.62it/s]Extractor Estimating: 177it [01:52,  1.56it/s]Extractor Estimating: 178it [01:53,  1.53it/s]Extractor Estimating: 179it [01:53,  1.51it/s]Extractor Estimating: 180it [01:54,  1.54it/s]Extractor Estimating: 181it [01:55,  1.53it/s]Extractor Estimating: 182it [01:55,  1.58it/s]Extractor Estimating: 183it [01:56,  1.56it/s]Extractor Estimating: 184it [01:56,  1.59it/s]Extractor Estimating: 185it [01:57,  1.57it/s]Extractor Estimating: 186it [01:58,  1.59it/s]Extractor Estimating: 187it [01:58,  1.59it/s]Extractor Estimating: 188it [01:59,  1.62it/s]Extractor Estimating: 189it [02:00,  1.58it/s]Extractor Estimating: 190it [02:00,  1.56it/s]Extractor Estimating: 191it [02:01,  1.48it/s]Extractor Estimating: 192it [02:02,  1.46it/s]Extractor Estimating: 193it [02:02,  1.44it/s]Extractor Estimating: 194it [02:03,  1.48it/s]Extractor Estimating: 195it [02:04,  1.51it/s]Extractor Estimating: 196it [02:04,  1.48it/s]Extractor Estimating: 197it [02:05,  1.51it/s]Extractor Estimating: 198it [02:06,  1.54it/s]Extractor Estimating: 199it [02:06,  1.53it/s]Extractor Estimating: 200it [02:07,  1.42it/s]Extractor Estimating: 201it [02:08,  1.42it/s]Extractor Estimating: 202it [02:08,  1.45it/s]Extractor Estimating: 203it [02:09,  1.48it/s]Extractor Estimating: 204it [02:10,  1.51it/s]Extractor Estimating: 205it [02:10,  1.47it/s]Extractor Estimating: 206it [02:11,  1.43it/s]Extractor Estimating: 207it [02:12,  1.43it/s]Extractor Estimating: 208it [02:13,  1.42it/s]Extractor Estimating: 209it [02:13,  1.45it/s]Extractor Estimating: 210it [02:14,  1.45it/s]Extractor Estimating: 211it [02:15,  1.43it/s]Extractor Estimating: 212it [02:15,  1.41it/s]Extractor Estimating: 213it [02:16,  1.34it/s]Extractor Estimating: 214it [02:17,  1.37it/s]Extractor Estimating: 215it [02:18,  1.36it/s]Extractor Estimating: 216it [02:18,  1.37it/s]Extractor Estimating: 217it [02:19,  1.38it/s]Extractor Estimating: 218it [02:20,  1.41it/s]Extractor Estimating: 219it [02:20,  1.45it/s]Extractor Estimating: 220it [02:21,  1.43it/s]Extractor Estimating: 221it [02:22,  1.41it/s]Extractor Estimating: 222it [02:23,  1.39it/s]Extractor Estimating: 223it [02:23,  1.35it/s]Extractor Estimating: 224it [02:24,  1.39it/s]Extractor Estimating: 225it [02:25,  1.33it/s]Extractor Estimating: 226it [02:26,  1.37it/s]Extractor Estimating: 227it [02:26,  1.41it/s]Extractor Estimating: 228it [02:27,  1.45it/s]Extractor Estimating: 229it [02:27,  1.53it/s]Extractor Estimating: 230it [02:28,  1.58it/s]Extractor Estimating: 231it [02:29,  1.59it/s]Extractor Estimating: 232it [02:29,  1.58it/s]Extractor Estimating: 233it [02:30,  1.55it/s]Extractor Estimating: 234it [02:31,  1.59it/s]Extractor Estimating: 235it [02:31,  1.53it/s]Extractor Estimating: 236it [02:32,  1.61it/s]Extractor Estimating: 237it [02:33,  1.56it/s]Extractor Estimating: 238it [02:33,  1.56it/s]Extractor Estimating: 239it [02:34,  1.58it/s]Extractor Estimating: 240it [02:34,  1.62it/s]Extractor Estimating: 241it [02:35,  1.69it/s]Extractor Estimating: 242it [02:36,  1.64it/s]Extractor Estimating: 243it [02:36,  1.68it/s]Extractor Estimating: 244it [02:37,  1.65it/s]Extractor Estimating: 245it [02:37,  1.62it/s]Extractor Estimating: 246it [02:38,  1.59it/s]Extractor Estimating: 247it [02:39,  1.66it/s]Extractor Estimating: 248it [02:39,  1.66it/s]Extractor Estimating: 249it [02:40,  1.66it/s]Extractor Estimating: 250it [02:40,  1.60it/s]Extractor Estimating: 251it [02:41,  1.58it/s]Extractor Estimating: 252it [02:42,  1.50it/s]Extractor Estimating: 253it [02:43,  1.45it/s]Extractor Estimating: 254it [02:43,  1.47it/s]Extractor Estimating: 255it [02:44,  1.48it/s]Extractor Estimating: 256it [02:45,  1.54it/s]Extractor Estimating: 257it [02:45,  1.46it/s]Extractor Estimating: 258it [02:46,  1.51it/s]Extractor Estimating: 259it [02:47,  1.45it/s]Extractor Estimating: 260it [02:47,  1.47it/s]Extractor Estimating: 261it [02:48,  1.46it/s]Extractor Estimating: 262it [02:49,  1.40it/s]Extractor Estimating: 263it [02:50,  1.38it/s]Extractor Estimating: 264it [02:50,  1.41it/s]Extractor Estimating: 265it [02:51,  1.48it/s]Extractor Estimating: 266it [02:51,  1.50it/s]Extractor Estimating: 267it [02:52,  1.56it/s]Extractor Estimating: 268it [02:53,  1.54it/s]Extractor Estimating: 269it [02:53,  1.48it/s]Extractor Estimating: 270it [02:54,  1.51it/s]Extractor Estimating: 271it [02:55,  1.46it/s]Extractor Estimating: 272it [02:55,  1.54it/s]Extractor Estimating: 273it [02:56,  1.52it/s]Extractor Estimating: 274it [02:57,  1.49it/s]Extractor Estimating: 275it [02:58,  1.43it/s]Extractor Estimating: 276it [02:58,  1.53it/s]Extractor Estimating: 277it [02:59,  1.52it/s]Extractor Estimating: 278it [02:59,  1.53it/s]Extractor Estimating: 279it [03:00,  1.61it/s]Extractor Estimating: 280it [03:00,  1.69it/s]Extractor Estimating: 281it [03:01,  1.64it/s]Extractor Estimating: 282it [03:02,  1.69it/s]Extractor Estimating: 283it [03:02,  1.73it/s]Extractor Estimating: 284it [03:03,  1.69it/s]Extractor Estimating: 285it [03:03,  1.68it/s]Extractor Estimating: 286it [03:04,  1.71it/s]Extractor Estimating: 287it [03:05,  1.74it/s]Extractor Estimating: 288it [03:05,  1.71it/s]Extractor Estimating: 289it [03:06,  1.63it/s]Extractor Estimating: 290it [03:06,  1.65it/s]Extractor Estimating: 291it [03:07,  1.69it/s]Extractor Estimating: 292it [03:08,  1.65it/s]Extractor Estimating: 293it [03:08,  1.73it/s]Extractor Estimating: 294it [03:09,  1.67it/s]Extractor Estimating: 295it [03:09,  1.64it/s]Extractor Estimating: 296it [03:10,  1.65it/s]Extractor Estimating: 297it [03:11,  1.58it/s]Extractor Estimating: 298it [03:11,  1.59it/s]Extractor Estimating: 299it [03:12,  1.51it/s]Extractor Estimating: 300it [03:13,  1.55it/s]Extractor Estimating: 301it [03:13,  1.55it/s]Extractor Estimating: 302it [03:14,  1.53it/s]Extractor Estimating: 303it [03:15,  1.59it/s]Extractor Estimating: 304it [03:15,  1.61it/s]Extractor Estimating: 305it [03:16,  1.61it/s]Extractor Estimating: 306it [03:16,  1.63it/s]Extractor Estimating: 307it [03:17,  1.63it/s]Extractor Estimating: 308it [03:18,  1.63it/s]Extractor Estimating: 309it [03:18,  1.63it/s]Extractor Estimating: 310it [03:19,  1.59it/s]Extractor Estimating: 311it [03:19,  1.62it/s]Extractor Estimating: 312it [03:20,  1.60it/s]Extractor Estimating: 313it [03:21,  1.63it/s]Extractor Estimating: 314it [03:21,  1.56it/s]Extractor Estimating: 315it [03:22,  1.56it/s]Extractor Estimating: 316it [03:23,  1.43it/s]Extractor Estimating: 317it [03:23,  1.50it/s]Extractor Estimating: 318it [03:24,  1.59it/s]Extractor Estimating: 319it [03:24,  1.70it/s]Extractor Estimating: 320it [03:25,  1.66it/s]Extractor Estimating: 321it [03:26,  1.61it/s]Extractor Estimating: 322it [03:26,  1.64it/s]Extractor Estimating: 323it [03:27,  1.64it/s]Extractor Estimating: 324it [03:28,  1.64it/s]Extractor Estimating: 325it [03:28,  1.62it/s]Extractor Estimating: 326it [03:29,  1.61it/s]Extractor Estimating: 327it [03:29,  1.65it/s]Extractor Estimating: 328it [03:30,  1.60it/s]Extractor Estimating: 329it [03:31,  1.55it/s]Extractor Estimating: 330it [03:31,  1.59it/s]Extractor Estimating: 331it [03:32,  1.58it/s]Extractor Estimating: 332it [03:33,  1.62it/s]Extractor Estimating: 333it [03:33,  1.55it/s]Extractor Estimating: 334it [03:34,  1.62it/s]Extractor Estimating: 335it [03:35,  1.54it/s]Extractor Estimating: 336it [03:35,  1.55it/s]Extractor Estimating: 337it [03:36,  1.61it/s]Extractor Estimating: 338it [03:36,  1.61it/s]Extractor Estimating: 339it [03:37,  1.59it/s]Extractor Estimating: 340it [03:38,  1.62it/s]Extractor Estimating: 341it [03:38,  1.55it/s]Extractor Estimating: 342it [03:39,  1.56it/s]Extractor Estimating: 343it [03:40,  1.54it/s]Extractor Estimating: 344it [03:40,  1.51it/s]Extractor Estimating: 345it [03:41,  1.53it/s]Extractor Estimating: 346it [03:42,  1.54it/s]Extractor Estimating: 347it [03:42,  1.54it/s]Extractor Estimating: 348it [03:43,  1.55it/s]Extractor Estimating: 349it [03:44,  1.44it/s]Extractor Estimating: 350it [03:44,  1.49it/s]Extractor Estimating: 351it [03:45,  1.48it/s]Extractor Estimating: 352it [03:46,  1.45it/s]Extractor Estimating: 353it [03:46,  1.48it/s]Extractor Estimating: 354it [03:47,  1.48it/s]Extractor Estimating: 355it [03:48,  1.47it/s]Extractor Estimating: 356it [03:48,  1.43it/s]Extractor Estimating: 357it [03:49,  1.47it/s]Extractor Estimating: 358it [03:50,  1.44it/s]Extractor Estimating: 359it [03:51,  1.45it/s]Extractor Estimating: 360it [03:51,  1.44it/s]Extractor Estimating: 361it [03:52,  1.46it/s]Extractor Estimating: 362it [03:53,  1.50it/s]Extractor Estimating: 363it [03:53,  1.49it/s]Extractor Estimating: 364it [03:54,  1.47it/s]Extractor Estimating: 365it [03:55,  1.46it/s]Extractor Estimating: 366it [03:55,  1.46it/s]Extractor Estimating: 367it [03:56,  1.49it/s]Extractor Estimating: 368it [03:57,  1.45it/s]Extractor Estimating: 369it [03:57,  1.44it/s]Extractor Estimating: 370it [03:58,  1.45it/s]Extractor Estimating: 371it [03:59,  1.48it/s]Extractor Estimating: 372it [03:59,  1.48it/s]Extractor Estimating: 373it [04:00,  1.45it/s]Extractor Estimating: 374it [04:01,  1.44it/s]Extractor Estimating: 375it [04:02,  1.41it/s]Extractor Estimating: 375it [04:02,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:49,938 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:49,946 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:49,946 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:49,946 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:49,946 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:47:50,637 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:47:50,638 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:47:50,889 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:47:51,938 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:47:51,938 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:53,660 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:53,665 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:53,665 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:53,665 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:53,665 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:47:54,391 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:47:54,392 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:47:54,661 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:47:54,826 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:47:54,826 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 02:12:58,395 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 02:12:58,422 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7498 mean pseudo reward: 0.8954809806027423
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
train vocab size: 15866
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15966, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15966, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.114, loss:651.4912
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.106, loss:624.6631
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.144, loss:615.8949
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.090, loss:562.3208
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.122, loss:580.0098
>> valid entity prec:0.5763, rec:0.5546, f1:0.5652
>> valid relation prec:0.2960, rec:0.0881, f1:0.1358
>> valid relation with NER prec:0.2960, rec:0.0881, f1:0.1358
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.590, loss:606.3612
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.099, loss:549.7725
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.122, loss:577.3792
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.096, loss:553.8338
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.096, loss:556.2530
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5273, rec:0.6102, f1:0.5657
>> valid relation prec:0.2467, rec:0.0967, f1:0.1390
>> valid relation with NER prec:0.2467, rec:0.0967, f1:0.1390
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.590, loss:552.6778
g_step 1200, step 261, avg_time 1.100, loss:553.7844
g_step 1300, step 48, avg_time 1.112, loss:505.1231
g_step 1400, step 148, avg_time 1.097, loss:506.1503
g_step 1500, step 248, avg_time 1.100, loss:521.1860
>> valid entity prec:0.5140, rec:0.5799, f1:0.5449
>> valid relation prec:0.2547, rec:0.0847, f1:0.1271
>> valid relation with NER prec:0.2547, rec:0.0847, f1:0.1271
g_step 1600, step 35, avg_time 2.587, loss:494.8298
g_step 1700, step 135, avg_time 1.120, loss:493.4287
g_step 1800, step 235, avg_time 1.114, loss:495.3944
g_step 1900, step 22, avg_time 1.103, loss:479.6246
g_step 2000, step 122, avg_time 1.113, loss:449.2123
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5438, rec:0.6135, f1:0.5766
>> valid relation prec:0.2512, rec:0.1073, f1:0.1504
>> valid relation with NER prec:0.2512, rec:0.1073, f1:0.1504
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 222, avg_time 2.555, loss:455.4991
g_step 2200, step 9, avg_time 1.122, loss:481.4871
g_step 2300, step 109, avg_time 1.105, loss:426.3017
g_step 2400, step 209, avg_time 1.113, loss:434.2966
g_step 2500, step 309, avg_time 1.103, loss:449.7129
>> valid entity prec:0.5218, rec:0.5527, f1:0.5368
>> valid relation prec:0.2095, rec:0.0985, f1:0.1340
>> valid relation with NER prec:0.2095, rec:0.0985, f1:0.1340
g_step 2600, step 96, avg_time 2.589, loss:400.2930
g_step 2700, step 196, avg_time 1.107, loss:405.7541
g_step 2800, step 296, avg_time 1.102, loss:440.6788
g_step 2900, step 83, avg_time 1.108, loss:388.1425
g_step 3000, step 183, avg_time 1.099, loss:402.0916
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5254, rec:0.5416, f1:0.5334
>> valid relation prec:0.1934, rec:0.0841, f1:0.1172
>> valid relation with NER prec:0.1934, rec:0.0841, f1:0.1172
g_step 3100, step 283, avg_time 2.578, loss:415.1675
g_step 3200, step 70, avg_time 1.091, loss:386.7137
g_step 3300, step 170, avg_time 1.106, loss:386.5453
g_step 3400, step 270, avg_time 1.098, loss:393.2581
g_step 3500, step 57, avg_time 1.141, loss:383.0890
>> valid entity prec:0.5432, rec:0.5402, f1:0.5417
>> valid relation prec:0.2066, rec:0.1162, f1:0.1488
>> valid relation with NER prec:0.2066, rec:0.1162, f1:0.1488
g_step 3600, step 157, avg_time 2.582, loss:377.5556
g_step 3700, step 257, avg_time 1.117, loss:374.9546
g_step 3800, step 44, avg_time 1.110, loss:347.5860
g_step 3900, step 144, avg_time 1.123, loss:351.2561
g_step 4000, step 244, avg_time 1.091, loss:363.0130
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5634, rec:0.5689, f1:0.5662
>> valid relation prec:0.1956, rec:0.1048, f1:0.1364
>> valid relation with NER prec:0.1956, rec:0.1048, f1:0.1364
g_step 4100, step 31, avg_time 2.575, loss:348.5112
g_step 4200, step 131, avg_time 1.117, loss:324.5146
g_step 4300, step 231, avg_time 1.112, loss:345.0985
g_step 4400, step 18, avg_time 1.083, loss:341.6413
g_step 4500, step 118, avg_time 1.095, loss:326.4819
>> valid entity prec:0.5417, rec:0.5269, f1:0.5342
>> valid relation prec:0.2151, rec:0.1022, f1:0.1385
>> valid relation with NER prec:0.2151, rec:0.1022, f1:0.1385
g_step 4600, step 218, avg_time 2.568, loss:357.3050
g_step 4700, step 5, avg_time 1.121, loss:323.2091
g_step 4800, step 105, avg_time 1.121, loss:291.3253
g_step 4900, step 205, avg_time 1.117, loss:324.4117
g_step 5000, step 305, avg_time 1.089, loss:331.3725
learning rate was adjusted to 0.0008
>> valid entity prec:0.5507, rec:0.5425, f1:0.5466
>> valid relation prec:0.1769, rec:0.0962, f1:0.1246
>> valid relation with NER prec:0.1769, rec:0.0962, f1:0.1246
g_step 5100, step 92, avg_time 2.584, loss:299.9837
g_step 5200, step 192, avg_time 1.103, loss:308.3350
g_step 5300, step 292, avg_time 1.108, loss:319.9031
g_step 5400, step 79, avg_time 1.097, loss:272.8706
g_step 5500, step 179, avg_time 1.106, loss:297.2842
>> valid entity prec:0.5342, rec:0.5646, f1:0.5490
>> valid relation prec:0.1541, rec:0.0918, f1:0.1151
>> valid relation with NER prec:0.1541, rec:0.0918, f1:0.1151
g_step 5600, step 279, avg_time 2.581, loss:311.2168
g_step 5700, step 66, avg_time 1.095, loss:280.7046
g_step 5800, step 166, avg_time 1.113, loss:283.5392
g_step 5900, step 266, avg_time 1.112, loss:287.7318
g_step 6000, step 53, avg_time 1.096, loss:283.3705
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5300, rec:0.5338, f1:0.5319
>> valid relation prec:0.1792, rec:0.1048, f1:0.1322
>> valid relation with NER prec:0.1792, rec:0.1048, f1:0.1322
g_step 6100, step 153, avg_time 2.577, loss:277.2456
g_step 6200, step 253, avg_time 1.125, loss:276.8387
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 02:12:58 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 02:12:58 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_02-12-58_ctolab11.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 02:12:59 - WARNING - datasets.builder -   Using custom data configuration default-3105fbfbc5ca7047
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-3105fbfbc5ca7047/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 02:12:59,781 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:12:59,782 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 02:12:59,783 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:12:59,784 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 02:12:59,804 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:12:59,810 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:12:59,810 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:12:59,810 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:12:59,810 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:12:59,810 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:12:59,810 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 02:12:59,974 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 02:13:03,032 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 02:13:03,035 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-3105fbfbc5ca7047/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.44ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.28ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.65ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.87ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.28ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.55ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.75ba/s]100%|██████████| 8/8 [00:01<00:00,  4.78ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.19ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.43ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.49ba/s]100%|██████████| 4/4 [00:00<00:00,  5.61ba/s]100%|██████████| 4/4 [00:00<00:00,  5.10ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.20ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.77ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 11.03ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 11.05ba/s]100%|██████████| 8/8 [00:00<00:00, 11.67ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.32ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.77ba/s]100%|██████████| 4/4 [00:00<00:00, 12.26ba/s]
[INFO|trainer.py:414] 2023-08-29 02:13:06,919 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 02:13:06,932 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 02:13:06,932 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 02:13:06,932 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 02:13:06,932 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 02:13:06,932 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 02:13:06,932 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 02:13:06,932 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:55,  3.32it/s]  0%|          | 2/585 [00:00<02:52,  3.38it/s]  1%|          | 3/585 [00:00<02:51,  3.40it/s]  1%|          | 4/585 [00:01<02:50,  3.40it/s]  1%|          | 5/585 [00:01<02:50,  3.41it/s]  1%|          | 6/585 [00:01<02:49,  3.41it/s]  1%|          | 7/585 [00:02<02:49,  3.41it/s]  1%|▏         | 8/585 [00:02<02:49,  3.41it/s]  2%|▏         | 9/585 [00:02<02:50,  3.39it/s]  2%|▏         | 10/585 [00:02<02:49,  3.39it/s]  2%|▏         | 11/585 [00:03<02:49,  3.40it/s]  2%|▏         | 12/585 [00:03<02:48,  3.40it/s]  2%|▏         | 13/585 [00:03<02:48,  3.40it/s]  2%|▏         | 14/585 [00:04<02:47,  3.41it/s]  3%|▎         | 15/585 [00:04<02:47,  3.41it/s]  3%|▎         | 16/585 [00:04<02:46,  3.41it/s]  3%|▎         | 17/585 [00:04<02:46,  3.40it/s]  3%|▎         | 18/585 [00:05<02:46,  3.41it/s]  3%|▎         | 19/585 [00:05<02:46,  3.41it/s]  3%|▎         | 20/585 [00:05<02:46,  3.40it/s]  4%|▎         | 21/585 [00:06<02:45,  3.40it/s]  4%|▍         | 22/585 [00:06<02:45,  3.41it/s]  4%|▍         | 23/585 [00:06<02:45,  3.41it/s]  4%|▍         | 24/585 [00:07<02:44,  3.41it/s]  4%|▍         | 25/585 [00:07<02:44,  3.41it/s]  4%|▍         | 26/585 [00:07<02:44,  3.41it/s]  5%|▍         | 27/585 [00:07<02:43,  3.41it/s]  5%|▍         | 28/585 [00:08<02:43,  3.41it/s]  5%|▍         | 29/585 [00:08<02:43,  3.41it/s]  5%|▌         | 30/585 [00:08<02:42,  3.41it/s]  5%|▌         | 31/585 [00:09<02:42,  3.41it/s]  5%|▌         | 32/585 [00:09<02:42,  3.41it/s]  6%|▌         | 33/585 [00:09<02:42,  3.40it/s]  6%|▌         | 34/585 [00:09<02:42,  3.40it/s]  6%|▌         | 35/585 [00:10<02:41,  3.40it/s]  6%|▌         | 36/585 [00:10<02:41,  3.40it/s]  6%|▋         | 37/585 [00:10<02:41,  3.40it/s]  6%|▋         | 38/585 [00:11<02:40,  3.41it/s]  7%|▋         | 39/585 [00:11<02:40,  3.40it/s]  7%|▋         | 40/585 [00:11<02:40,  3.41it/s]  7%|▋         | 41/585 [00:12<02:39,  3.41it/s]  7%|▋         | 42/585 [00:12<02:39,  3.41it/s]  7%|▋         | 43/585 [00:12<02:38,  3.41it/s]  8%|▊         | 44/585 [00:12<02:38,  3.41it/s]  8%|▊         | 45/585 [00:13<02:38,  3.42it/s]  8%|▊         | 46/585 [00:13<02:37,  3.41it/s]  8%|▊         | 47/585 [00:13<02:37,  3.41it/s]  8%|▊         | 48/585 [00:14<02:37,  3.41it/s]  8%|▊         | 49/585 [00:14<02:37,  3.41it/s]  9%|▊         | 50/585 [00:14<02:36,  3.41it/s]  9%|▊         | 51/585 [00:14<02:36,  3.41it/s]  9%|▉         | 52/585 [00:15<02:37,  3.39it/s]  9%|▉         | 53/585 [00:15<02:36,  3.40it/s]  9%|▉         | 54/585 [00:15<02:36,  3.40it/s]  9%|▉         | 55/585 [00:16<02:35,  3.41it/s] 10%|▉         | 56/585 [00:16<02:35,  3.41it/s] 10%|▉         | 57/585 [00:16<02:35,  3.40it/s] 10%|▉         | 58/585 [00:17<02:34,  3.40it/s] 10%|█         | 59/585 [00:17<02:34,  3.40it/s] 10%|█         | 60/585 [00:17<02:34,  3.41it/s] 10%|█         | 61/585 [00:17<02:33,  3.40it/s] 11%|█         | 62/585 [00:18<02:33,  3.41it/s] 11%|█         | 63/585 [00:18<02:33,  3.41it/s] 11%|█         | 64/585 [00:18<02:33,  3.40it/s] 11%|█         | 65/585 [00:19<02:32,  3.40it/s] 11%|█▏        | 66/585 [00:19<02:32,  3.41it/s] 11%|█▏        | 67/585 [00:19<02:32,  3.41it/s] 12%|█▏        | 68/585 [00:19<02:31,  3.41it/s] 12%|█▏        | 69/585 [00:20<02:32,  3.39it/s] 12%|█▏        | 70/585 [00:20<02:31,  3.40it/s] 12%|█▏        | 71/585 [00:20<02:31,  3.40it/s] 12%|█▏        | 72/585 [00:21<02:30,  3.40it/s] 12%|█▏        | 73/585 [00:21<02:30,  3.40it/s] 13%|█▎        | 74/585 [00:21<02:30,  3.40it/s] 13%|█▎        | 75/585 [00:22<02:29,  3.40it/s] 13%|█▎        | 76/585 [00:22<02:29,  3.41it/s] 13%|█▎        | 77/585 [00:22<02:29,  3.41it/s] 13%|█▎        | 78/585 [00:22<02:28,  3.40it/s] 14%|█▎        | 79/585 [00:23<02:28,  3.41it/s] 14%|█▎        | 80/585 [00:23<02:28,  3.41it/s] 14%|█▍        | 81/585 [00:23<02:27,  3.41it/s] 14%|█▍        | 82/585 [00:24<02:27,  3.41it/s] 14%|█▍        | 83/585 [00:24<02:27,  3.41it/s] 14%|█▍        | 84/585 [00:24<02:27,  3.40it/s] 15%|█▍        | 85/585 [00:24<02:26,  3.40it/s] 15%|█▍        | 86/585 [00:25<02:26,  3.40it/s] 15%|█▍        | 87/585 [00:25<02:26,  3.40it/s] 15%|█▌        | 88/585 [00:25<02:26,  3.40it/s] 15%|█▌        | 89/585 [00:26<02:25,  3.40it/s] 15%|█▌        | 90/585 [00:26<02:25,  3.40it/s] 16%|█▌        | 91/585 [00:26<02:25,  3.40it/s] 16%|█▌        | 92/585 [00:27<02:24,  3.41it/s] 16%|█▌        | 93/585 [00:27<02:24,  3.40it/s] 16%|█▌        | 94/585 [00:27<02:24,  3.40it/s] 16%|█▌        | 95/585 [00:27<02:24,  3.40it/s] 16%|█▋        | 96/585 [00:28<02:23,  3.40it/s] 17%|█▋        | 97/585 [00:28<02:23,  3.40it/s] 17%|█▋        | 98/585 [00:28<02:23,  3.41it/s] 17%|█▋        | 99/585 [00:29<02:22,  3.41it/s] 17%|█▋        | 100/585 [00:29<02:22,  3.40it/s] 17%|█▋        | 101/585 [00:29<02:22,  3.40it/s] 17%|█▋        | 102/585 [00:29<02:21,  3.41it/s] 18%|█▊        | 103/585 [00:30<02:21,  3.40it/s] 18%|█▊        | 104/585 [00:30<02:21,  3.39it/s] 18%|█▊        | 105/585 [00:30<02:21,  3.40it/s] 18%|█▊        | 106/585 [00:31<02:20,  3.40it/s] 18%|█▊        | 107/585 [00:31<02:20,  3.40it/s] 18%|█▊        | 108/585 [00:31<02:20,  3.40it/s] 19%|█▊        | 109/585 [00:32<02:20,  3.40it/s] 19%|█▉        | 110/585 [00:32<02:19,  3.40it/s] 19%|█▉        | 111/585 [00:32<02:19,  3.40it/s] 19%|█▉        | 112/585 [00:32<02:19,  3.40it/s] 19%|█▉        | 113/585 [00:33<02:18,  3.41it/s] 19%|█▉        | 114/585 [00:33<02:18,  3.40it/s] 20%|█▉        | 115/585 [00:33<02:18,  3.40it/s] 20%|█▉        | 116/585 [00:34<02:17,  3.40it/s] 20%|██        | 117/585 [00:34<02:17,  3.40it/s][INFO|trainer.py:2140] 2023-08-29 02:13:41,353 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:13:41,353 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-29 02:13:41,353 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 54.69it/s][A
  3%|▎         | 12/436 [00:00<00:08, 47.81it/s][A
  4%|▍         | 17/436 [00:00<00:09, 45.87it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.11it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.58it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.32it/s][A
  8%|▊         | 37/436 [00:00<00:09, 43.96it/s][A
 10%|▉         | 42/436 [00:00<00:08, 43.89it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.01it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.19it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.07it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.06it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 43.95it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 43.92it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 43.89it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.65it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 43.78it/s][A
 21%|██        | 92/436 [00:02<00:07, 43.90it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.01it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.01it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.02it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.05it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.95it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 43.84it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.70it/s][A
 30%|███       | 132/436 [00:02<00:06, 43.75it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 43.93it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 43.97it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.10it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.03it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.04it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.97it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.79it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.69it/s][A
 41%|████      | 177/436 [00:04<00:05, 43.82it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 43.92it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.02it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.07it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.04it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 43.97it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 43.88it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 43.71it/s][A
 50%|████▉     | 217/436 [00:04<00:05, 43.77it/s][A
 51%|█████     | 222/436 [00:05<00:04, 43.85it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 43.78it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 43.77it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.06it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.12it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.06it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 43.75it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.87it/s][A
 60%|██████    | 262/436 [00:05<00:03, 43.78it/s][A
 61%|██████    | 267/436 [00:06<00:03, 43.85it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 43.85it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 43.97it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.02it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.02it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 43.92it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 43.71it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 43.69it/s][A
 70%|███████   | 307/436 [00:06<00:02, 43.76it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 43.87it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 43.81it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 43.89it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.02it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.02it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 43.95it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 43.82it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 43.73it/s][A
 81%|████████  | 352/436 [00:07<00:01, 43.90it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 43.96it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 43.90it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 43.95it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.00it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 43.82it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 43.77it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 43.72it/s][A
 90%|████████▉ | 392/436 [00:08<00:01, 43.67it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 43.78it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 43.91it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.01it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.01it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.03it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 43.88it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 43.80it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 43.76it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 43.76it/s][A 20%|██        | 117/585 [00:44<02:17,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:13:51,322 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 02:13:51,344 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:13:53,227 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:13:53,248 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:13:53,258 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:50<39:41,  5.10s/it] 20%|██        | 119/585 [00:50<28:26,  3.66s/it] 21%|██        | 120/585 [00:51<20:33,  2.65s/it] 21%|██        | 121/585 [00:51<15:07,  1.95s/it] 21%|██        | 122/585 [00:51<11:15,  1.46s/it] 21%|██        | 123/585 [00:52<08:33,  1.11s/it] 21%|██        | 124/585 [00:52<06:40,  1.15it/s] 21%|██▏       | 125/585 [00:52<05:20,  1.43it/s] 22%|██▏       | 126/585 [00:53<04:25,  1.73it/s] 22%|██▏       | 127/585 [00:53<03:46,  2.02it/s] 22%|██▏       | 128/585 [00:53<03:19,  2.29it/s] 22%|██▏       | 129/585 [00:54<03:02,  2.50it/s] 22%|██▏       | 130/585 [00:54<02:48,  2.70it/s] 22%|██▏       | 131/585 [00:54<02:38,  2.87it/s] 23%|██▎       | 132/585 [00:54<02:31,  2.99it/s] 23%|██▎       | 133/585 [00:55<02:26,  3.08it/s] 23%|██▎       | 134/585 [00:55<02:22,  3.16it/s] 23%|██▎       | 135/585 [00:55<02:20,  3.21it/s] 23%|██▎       | 136/585 [00:56<02:18,  3.24it/s] 23%|██▎       | 137/585 [00:56<02:17,  3.27it/s] 24%|██▎       | 138/585 [00:56<02:15,  3.29it/s] 24%|██▍       | 139/585 [00:57<02:17,  3.24it/s] 24%|██▍       | 140/585 [00:57<02:16,  3.27it/s] 24%|██▍       | 141/585 [00:57<02:15,  3.29it/s] 24%|██▍       | 142/585 [00:57<02:14,  3.30it/s] 24%|██▍       | 143/585 [00:58<02:13,  3.31it/s] 25%|██▍       | 144/585 [00:58<02:13,  3.31it/s] 25%|██▍       | 145/585 [00:58<02:12,  3.31it/s] 25%|██▍       | 146/585 [00:59<02:12,  3.32it/s] 25%|██▌       | 147/585 [00:59<02:11,  3.32it/s] 25%|██▌       | 148/585 [00:59<02:11,  3.33it/s] 25%|██▌       | 149/585 [01:00<02:11,  3.32it/s] 26%|██▌       | 150/585 [01:00<02:11,  3.32it/s] 26%|██▌       | 151/585 [01:00<02:10,  3.32it/s] 26%|██▌       | 152/585 [01:00<02:10,  3.32it/s] 26%|██▌       | 153/585 [01:01<02:10,  3.32it/s] 26%|██▋       | 154/585 [01:01<02:09,  3.32it/s] 26%|██▋       | 155/585 [01:01<02:09,  3.32it/s] 27%|██▋       | 156/585 [01:02<02:09,  3.32it/s] 27%|██▋       | 157/585 [01:02<02:09,  3.32it/s] 27%|██▋       | 158/585 [01:02<02:08,  3.32it/s] 27%|██▋       | 159/585 [01:03<02:08,  3.31it/s] 27%|██▋       | 160/585 [01:03<02:08,  3.32it/s] 28%|██▊       | 161/585 [01:03<02:07,  3.32it/s] 28%|██▊       | 162/585 [01:03<02:07,  3.32it/s] 28%|██▊       | 163/585 [01:04<02:07,  3.32it/s] 28%|██▊       | 164/585 [01:04<02:06,  3.32it/s] 28%|██▊       | 165/585 [01:04<02:06,  3.32it/s] 28%|██▊       | 166/585 [01:05<02:06,  3.32it/s] 29%|██▊       | 167/585 [01:05<02:05,  3.32it/s] 29%|██▊       | 168/585 [01:05<02:05,  3.32it/s] 29%|██▉       | 169/585 [01:06<02:05,  3.31it/s] 29%|██▉       | 170/585 [01:06<02:05,  3.31it/s] 29%|██▉       | 171/585 [01:06<02:04,  3.31it/s] 29%|██▉       | 172/585 [01:06<02:04,  3.32it/s] 30%|██▉       | 173/585 [01:07<02:04,  3.32it/s] 30%|██▉       | 174/585 [01:07<02:03,  3.32it/s] 30%|██▉       | 175/585 [01:07<02:03,  3.32it/s] 30%|███       | 176/585 [01:08<02:02,  3.33it/s] 30%|███       | 177/585 [01:08<02:02,  3.33it/s] 30%|███       | 178/585 [01:08<02:02,  3.33it/s] 31%|███       | 179/585 [01:09<02:02,  3.31it/s] 31%|███       | 180/585 [01:09<02:02,  3.32it/s] 31%|███       | 181/585 [01:09<02:01,  3.32it/s] 31%|███       | 182/585 [01:10<02:01,  3.32it/s] 31%|███▏      | 183/585 [01:10<02:01,  3.32it/s] 31%|███▏      | 184/585 [01:10<02:01,  3.31it/s] 32%|███▏      | 185/585 [01:10<02:00,  3.31it/s] 32%|███▏      | 186/585 [01:11<02:00,  3.31it/s] 32%|███▏      | 187/585 [01:11<01:59,  3.32it/s] 32%|███▏      | 188/585 [01:11<01:59,  3.32it/s] 32%|███▏      | 189/585 [01:12<01:59,  3.32it/s] 32%|███▏      | 190/585 [01:12<01:59,  3.32it/s] 33%|███▎      | 191/585 [01:12<01:58,  3.32it/s] 33%|███▎      | 192/585 [01:13<01:58,  3.32it/s] 33%|███▎      | 193/585 [01:13<01:57,  3.32it/s] 33%|███▎      | 194/585 [01:13<01:57,  3.32it/s] 33%|███▎      | 195/585 [01:13<01:57,  3.32it/s] 34%|███▎      | 196/585 [01:14<01:56,  3.33it/s] 34%|███▎      | 197/585 [01:14<01:56,  3.32it/s] 34%|███▍      | 198/585 [01:14<01:56,  3.32it/s] 34%|███▍      | 199/585 [01:15<01:56,  3.32it/s] 34%|███▍      | 200/585 [01:15<01:56,  3.31it/s] 34%|███▍      | 201/585 [01:15<01:55,  3.31it/s] 35%|███▍      | 202/585 [01:16<01:55,  3.32it/s] 35%|███▍      | 203/585 [01:16<01:55,  3.32it/s] 35%|███▍      | 204/585 [01:16<01:54,  3.31it/s] 35%|███▌      | 205/585 [01:16<01:54,  3.32it/s] 35%|███▌      | 206/585 [01:17<01:54,  3.32it/s] 35%|███▌      | 207/585 [01:17<01:53,  3.32it/s] 36%|███▌      | 208/585 [01:17<01:53,  3.32it/s] 36%|███▌      | 209/585 [01:18<01:53,  3.32it/s] 36%|███▌      | 210/585 [01:18<01:52,  3.32it/s] 36%|███▌      | 211/585 [01:18<01:52,  3.32it/s] 36%|███▌      | 212/585 [01:19<01:52,  3.32it/s] 36%|███▋      | 213/585 [01:19<01:52,  3.32it/s] 37%|███▋      | 214/585 [01:19<01:52,  3.31it/s] 37%|███▋      | 215/585 [01:19<01:51,  3.31it/s] 37%|███▋      | 216/585 [01:20<01:51,  3.31it/s] 37%|███▋      | 217/585 [01:20<01:51,  3.31it/s] 37%|███▋      | 218/585 [01:20<01:50,  3.32it/s] 37%|███▋      | 219/585 [01:21<01:50,  3.32it/s] 38%|███▊      | 220/585 [01:21<01:49,  3.32it/s] 38%|███▊      | 221/585 [01:21<01:49,  3.32it/s] 38%|███▊      | 222/585 [01:22<01:49,  3.33it/s] 38%|███▊      | 223/585 [01:22<01:48,  3.32it/s] 38%|███▊      | 224/585 [01:22<01:48,  3.31it/s] 38%|███▊      | 225/585 [01:22<01:48,  3.31it/s] 39%|███▊      | 226/585 [01:23<01:48,  3.31it/s] 39%|███▉      | 227/585 [01:23<01:48,  3.31it/s] 39%|███▉      | 228/585 [01:23<01:47,  3.32it/s] 39%|███▉      | 229/585 [01:24<01:47,  3.32it/s] 39%|███▉      | 230/585 [01:24<01:46,  3.32it/s] 39%|███▉      | 231/585 [01:24<01:46,  3.32it/s] 40%|███▉      | 232/585 [01:25<01:46,  3.32it/s] 40%|███▉      | 233/585 [01:25<01:45,  3.32it/s] 40%|████      | 234/585 [01:25<01:46,  3.31it/s][INFO|trainer.py:2140] 2023-08-29 02:14:32,659 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:14:32,659 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-29 02:14:32,659 >>   Batch size = 8
{'eval_loss': 1.0543519258499146, 'eval_runtime': 9.9483, 'eval_samples_per_second': 350.612, 'eval_steps_per_second': 43.826, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 54.55it/s][A
  3%|▎         | 12/436 [00:00<00:08, 47.32it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.30it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.47it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.84it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.37it/s][A
  8%|▊         | 37/436 [00:00<00:09, 44.27it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.11it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.17it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.29it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.31it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.37it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.07it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.11it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.04it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.84it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 43.97it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.05it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.09it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.23it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.19it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.17it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.02it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.01it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.90it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.04it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.00it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.08it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.22it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.17it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.09it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.04it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.98it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.97it/s][A
 41%|████      | 177/436 [00:03<00:05, 43.95it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.06it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.09it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.22it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.20it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.08it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 43.99it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 43.99it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.08it/s][A
 51%|█████     | 222/436 [00:05<00:04, 43.89it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.07it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.17it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.24it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.19it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.11it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.04it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.12it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.04it/s][A
 61%|██████    | 267/436 [00:06<00:03, 43.94it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.09it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.12it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.11it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.06it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.08it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.11it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.17it/s][A
 70%|███████   | 307/436 [00:06<00:02, 43.98it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 43.96it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.04it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.18it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.06it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.06it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.11it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.13it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.06it/s][A
 81%|████████  | 352/436 [00:07<00:01, 43.93it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.03it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.20it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.15it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.12it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.10it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.14it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.04it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.03it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 43.97it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 43.99it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.12it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.16it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.03it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.14it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.10it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.11it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.11it/s][A 40%|████      | 234/585 [01:35<01:46,  3.31it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:14:42,576 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 02:14:42,601 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:14:44,344 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:14:44,364 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:14:44,374 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:41<28:33,  4.90s/it] 40%|████      | 236/585 [01:41<20:28,  3.52s/it] 41%|████      | 237/585 [01:41<14:48,  2.55s/it] 41%|████      | 238/585 [01:42<10:51,  1.88s/it] 41%|████      | 239/585 [01:42<08:06,  1.40s/it] 41%|████      | 240/585 [01:42<06:10,  1.07s/it] 41%|████      | 241/585 [01:43<04:49,  1.19it/s] 41%|████▏     | 242/585 [01:43<03:52,  1.47it/s] 42%|████▏     | 243/585 [01:43<03:13,  1.77it/s] 42%|████▏     | 244/585 [01:44<02:45,  2.06it/s] 42%|████▏     | 245/585 [01:44<02:26,  2.32it/s] 42%|████▏     | 246/585 [01:44<02:13,  2.55it/s] 42%|████▏     | 247/585 [01:44<02:03,  2.74it/s] 42%|████▏     | 248/585 [01:45<01:56,  2.89it/s] 43%|████▎     | 249/585 [01:45<01:51,  3.01it/s] 43%|████▎     | 250/585 [01:45<01:48,  3.09it/s] 43%|████▎     | 251/585 [01:46<01:45,  3.15it/s] 43%|████▎     | 252/585 [01:46<01:44,  3.20it/s] 43%|████▎     | 253/585 [01:46<01:42,  3.23it/s] 43%|████▎     | 254/585 [01:47<01:41,  3.26it/s] 44%|████▎     | 255/585 [01:47<01:40,  3.28it/s] 44%|████▍     | 256/585 [01:47<01:40,  3.28it/s] 44%|████▍     | 257/585 [01:47<01:39,  3.30it/s] 44%|████▍     | 258/585 [01:48<01:38,  3.31it/s] 44%|████▍     | 259/585 [01:48<01:38,  3.31it/s] 44%|████▍     | 260/585 [01:48<01:37,  3.32it/s] 45%|████▍     | 261/585 [01:49<01:37,  3.32it/s] 45%|████▍     | 262/585 [01:49<01:37,  3.32it/s] 45%|████▍     | 263/585 [01:49<01:36,  3.32it/s] 45%|████▌     | 264/585 [01:50<01:36,  3.32it/s] 45%|████▌     | 265/585 [01:50<01:36,  3.32it/s] 45%|████▌     | 266/585 [01:50<01:36,  3.30it/s] 46%|████▌     | 267/585 [01:50<01:36,  3.31it/s] 46%|████▌     | 268/585 [01:51<01:35,  3.31it/s] 46%|████▌     | 269/585 [01:51<01:35,  3.32it/s] 46%|████▌     | 270/585 [01:51<01:37,  3.24it/s] 46%|████▋     | 271/585 [01:52<01:36,  3.26it/s] 46%|████▋     | 272/585 [01:52<01:35,  3.28it/s] 47%|████▋     | 273/585 [01:52<01:34,  3.29it/s] 47%|████▋     | 274/585 [01:53<01:34,  3.30it/s] 47%|████▋     | 275/585 [01:53<01:33,  3.30it/s] 47%|████▋     | 276/585 [01:53<01:34,  3.29it/s] 47%|████▋     | 277/585 [01:53<01:33,  3.30it/s] 48%|████▊     | 278/585 [01:54<01:32,  3.31it/s] 48%|████▊     | 279/585 [01:54<01:32,  3.31it/s] 48%|████▊     | 280/585 [01:54<01:32,  3.32it/s] 48%|████▊     | 281/585 [01:55<01:31,  3.32it/s] 48%|████▊     | 282/585 [01:55<01:31,  3.32it/s] 48%|████▊     | 283/585 [01:55<01:30,  3.32it/s] 49%|████▊     | 284/585 [01:56<01:30,  3.32it/s] 49%|████▊     | 285/585 [01:56<01:30,  3.32it/s] 49%|████▉     | 286/585 [01:56<01:30,  3.31it/s] 49%|████▉     | 287/585 [01:56<01:30,  3.31it/s] 49%|████▉     | 288/585 [01:57<01:29,  3.31it/s] 49%|████▉     | 289/585 [01:57<01:29,  3.32it/s] 50%|████▉     | 290/585 [01:57<01:28,  3.32it/s] 50%|████▉     | 291/585 [01:58<01:28,  3.32it/s] 50%|████▉     | 292/585 [01:58<01:28,  3.32it/s] 50%|█████     | 293/585 [01:58<01:27,  3.32it/s] 50%|█████     | 294/585 [01:59<01:27,  3.32it/s] 50%|█████     | 295/585 [01:59<01:27,  3.33it/s] 51%|█████     | 296/585 [01:59<01:27,  3.31it/s] 51%|█████     | 297/585 [02:00<01:26,  3.31it/s] 51%|█████     | 298/585 [02:00<01:26,  3.31it/s] 51%|█████     | 299/585 [02:00<01:26,  3.31it/s] 51%|█████▏    | 300/585 [02:00<01:26,  3.31it/s] 51%|█████▏    | 301/585 [02:01<01:25,  3.31it/s] 52%|█████▏    | 302/585 [02:01<01:25,  3.32it/s] 52%|█████▏    | 303/585 [02:01<01:24,  3.32it/s] 52%|█████▏    | 304/585 [02:02<01:24,  3.32it/s] 52%|█████▏    | 305/585 [02:02<01:24,  3.32it/s] 52%|█████▏    | 306/585 [02:02<01:24,  3.32it/s] 52%|█████▏    | 307/585 [02:03<01:23,  3.33it/s] 53%|█████▎    | 308/585 [02:03<01:22,  3.35it/s] 53%|█████▎    | 309/585 [02:03<01:21,  3.37it/s] 53%|█████▎    | 310/585 [02:03<01:21,  3.38it/s] 53%|█████▎    | 311/585 [02:04<01:20,  3.39it/s] 53%|█████▎    | 312/585 [02:04<01:20,  3.39it/s] 54%|█████▎    | 313/585 [02:04<01:20,  3.39it/s] 54%|█████▎    | 314/585 [02:05<01:19,  3.40it/s] 54%|█████▍    | 315/585 [02:05<01:19,  3.40it/s] 54%|█████▍    | 316/585 [02:05<01:19,  3.40it/s] 54%|█████▍    | 317/585 [02:05<01:18,  3.40it/s] 54%|█████▍    | 318/585 [02:06<01:18,  3.40it/s] 55%|█████▍    | 319/585 [02:06<01:18,  3.40it/s] 55%|█████▍    | 320/585 [02:06<01:17,  3.40it/s] 55%|█████▍    | 321/585 [02:07<01:17,  3.40it/s] 55%|█████▌    | 322/585 [02:07<01:17,  3.40it/s] 55%|█████▌    | 323/585 [02:07<01:17,  3.38it/s] 55%|█████▌    | 324/585 [02:08<01:17,  3.39it/s] 56%|█████▌    | 325/585 [02:08<01:16,  3.39it/s] 56%|█████▌    | 326/585 [02:08<01:16,  3.39it/s] 56%|█████▌    | 327/585 [02:08<01:16,  3.39it/s] 56%|█████▌    | 328/585 [02:09<01:15,  3.39it/s] 56%|█████▌    | 329/585 [02:09<01:15,  3.40it/s] 56%|█████▋    | 330/585 [02:09<01:15,  3.40it/s] 57%|█████▋    | 331/585 [02:10<01:14,  3.40it/s] 57%|█████▋    | 332/585 [02:10<01:14,  3.40it/s] 57%|█████▋    | 333/585 [02:10<01:14,  3.40it/s] 57%|█████▋    | 334/585 [02:10<01:14,  3.39it/s] 57%|█████▋    | 335/585 [02:11<01:13,  3.39it/s] 57%|█████▋    | 336/585 [02:11<01:13,  3.39it/s] 58%|█████▊    | 337/585 [02:11<01:13,  3.40it/s] 58%|█████▊    | 338/585 [02:12<01:12,  3.39it/s] 58%|█████▊    | 339/585 [02:12<01:12,  3.40it/s] 58%|█████▊    | 340/585 [02:12<01:12,  3.40it/s] 58%|█████▊    | 341/585 [02:13<01:11,  3.40it/s] 58%|█████▊    | 342/585 [02:13<01:11,  3.40it/s] 59%|█████▊    | 343/585 [02:13<01:11,  3.41it/s] 59%|█████▉    | 344/585 [02:13<01:10,  3.41it/s] 59%|█████▉    | 345/585 [02:14<01:10,  3.39it/s] 59%|█████▉    | 346/585 [02:14<01:10,  3.39it/s] 59%|█████▉    | 347/585 [02:14<01:10,  3.40it/s] 59%|█████▉    | 348/585 [02:15<01:09,  3.40it/s] 60%|█████▉    | 349/585 [02:15<01:09,  3.40it/s] 60%|█████▉    | 350/585 [02:15<01:09,  3.40it/s] 60%|██████    | 351/585 [02:15<01:08,  3.40it/s][INFO|trainer.py:2140] 2023-08-29 02:15:22,949 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:15:22,949 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-29 02:15:22,949 >>   Batch size = 8
{'eval_loss': 1.0713313817977905, 'eval_runtime': 9.9043, 'eval_samples_per_second': 352.169, 'eval_steps_per_second': 44.021, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 54.89it/s][A
  3%|▎         | 12/436 [00:00<00:08, 47.71it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.01it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.13it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.68it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.34it/s][A
  8%|▊         | 37/436 [00:00<00:09, 44.32it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.09it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.24it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.19it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.27it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.21it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.09it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 43.97it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 43.99it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 44.05it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.03it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.17it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.12it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.11it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.00it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.01it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.82it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 43.98it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 44.05it/s][A
 30%|███       | 132/436 [00:02<00:06, 43.93it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.05it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.11it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.09it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 43.89it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 43.93it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.79it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.87it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.06it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.16it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.11it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.12it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.03it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 43.90it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 43.84it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 43.78it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 43.84it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.09it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.27it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.21it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.18it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 43.97it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 43.83it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 43.60it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 43.74it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.93it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.00it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.24it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.27it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.25it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.01it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 43.87it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 43.82it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 43.80it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.02it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.04it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.22it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.27it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.10it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.07it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 43.92it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 43.93it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 43.83it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.00it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.08it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.17it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.12it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.04it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 43.76it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 43.88it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 43.81it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 43.78it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.03it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.23it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.23it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.25it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.08it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 43.89it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 43.97it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 43.89it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.03it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.03it/s][A 60%|██████    | 351/585 [02:25<01:08,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:15:32,890 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 02:15:32,908 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:15:34,800 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:15:34,814 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:15:34,825 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:31<19:24,  5.00s/it] 60%|██████    | 353/585 [02:32<13:53,  3.59s/it] 61%|██████    | 354/585 [02:32<10:01,  2.60s/it] 61%|██████    | 355/585 [02:32<07:20,  1.91s/it] 61%|██████    | 356/585 [02:33<05:27,  1.43s/it] 61%|██████    | 357/585 [02:33<04:08,  1.09s/it] 61%|██████    | 358/585 [02:33<03:13,  1.17it/s] 61%|██████▏   | 359/585 [02:34<02:35,  1.45it/s] 62%|██████▏   | 360/585 [02:34<02:08,  1.75it/s] 62%|██████▏   | 361/585 [02:34<01:49,  2.04it/s] 62%|██████▏   | 362/585 [02:34<01:36,  2.30it/s] 62%|██████▏   | 363/585 [02:35<01:27,  2.53it/s] 62%|██████▏   | 364/585 [02:35<01:21,  2.72it/s] 62%|██████▏   | 365/585 [02:35<01:16,  2.88it/s] 63%|██████▎   | 366/585 [02:36<01:12,  3.00it/s] 63%|██████▎   | 367/585 [02:36<01:10,  3.09it/s] 63%|██████▎   | 368/585 [02:36<01:08,  3.16it/s] 63%|██████▎   | 369/585 [02:37<01:07,  3.21it/s] 63%|██████▎   | 370/585 [02:37<01:06,  3.24it/s] 63%|██████▎   | 371/585 [02:37<01:05,  3.27it/s] 64%|██████▎   | 372/585 [02:37<01:04,  3.29it/s] 64%|██████▍   | 373/585 [02:38<01:04,  3.27it/s] 64%|██████▍   | 374/585 [02:38<01:04,  3.28it/s] 64%|██████▍   | 375/585 [02:38<01:03,  3.30it/s] 64%|██████▍   | 376/585 [02:39<01:03,  3.30it/s] 64%|██████▍   | 377/585 [02:39<01:02,  3.31it/s] 65%|██████▍   | 378/585 [02:39<01:02,  3.32it/s] 65%|██████▍   | 379/585 [02:40<01:02,  3.32it/s] 65%|██████▍   | 380/585 [02:40<01:01,  3.32it/s] 65%|██████▌   | 381/585 [02:40<01:01,  3.32it/s] 65%|██████▌   | 382/585 [02:40<01:01,  3.33it/s] 65%|██████▌   | 383/585 [02:41<01:00,  3.32it/s] 66%|██████▌   | 384/585 [02:41<01:00,  3.32it/s] 66%|██████▌   | 385/585 [02:41<01:00,  3.32it/s] 66%|██████▌   | 386/585 [02:42<00:59,  3.32it/s] 66%|██████▌   | 387/585 [02:42<00:59,  3.32it/s] 66%|██████▋   | 388/585 [02:42<00:59,  3.31it/s] 66%|██████▋   | 389/585 [02:43<00:59,  3.31it/s] 67%|██████▋   | 390/585 [02:43<00:58,  3.31it/s] 67%|██████▋   | 391/585 [02:43<00:58,  3.32it/s] 67%|██████▋   | 392/585 [02:44<00:58,  3.32it/s] 67%|██████▋   | 393/585 [02:44<00:57,  3.32it/s] 67%|██████▋   | 394/585 [02:44<00:57,  3.32it/s] 68%|██████▊   | 395/585 [02:44<00:57,  3.32it/s] 68%|██████▊   | 396/585 [02:45<00:56,  3.33it/s] 68%|██████▊   | 397/585 [02:45<00:56,  3.33it/s] 68%|██████▊   | 398/585 [02:45<00:56,  3.33it/s] 68%|██████▊   | 399/585 [02:46<00:55,  3.33it/s] 68%|██████▊   | 400/585 [02:46<00:55,  3.32it/s] 69%|██████▊   | 401/585 [02:46<00:55,  3.32it/s] 69%|██████▊   | 402/585 [02:47<00:55,  3.32it/s] 69%|██████▉   | 403/585 [02:47<00:54,  3.31it/s] 69%|██████▉   | 404/585 [02:47<00:54,  3.31it/s] 69%|██████▉   | 405/585 [02:47<00:54,  3.31it/s] 69%|██████▉   | 406/585 [02:48<00:53,  3.32it/s] 70%|██████▉   | 407/585 [02:48<00:53,  3.32it/s] 70%|██████▉   | 408/585 [02:48<00:53,  3.32it/s] 70%|██████▉   | 409/585 [02:49<00:52,  3.32it/s] 70%|███████   | 410/585 [02:49<00:52,  3.33it/s] 70%|███████   | 411/585 [02:49<00:52,  3.32it/s] 70%|███████   | 412/585 [02:50<00:52,  3.33it/s] 71%|███████   | 413/585 [02:50<00:51,  3.32it/s] 71%|███████   | 414/585 [02:50<00:51,  3.31it/s] 71%|███████   | 415/585 [02:50<00:51,  3.30it/s] 71%|███████   | 416/585 [02:51<00:51,  3.31it/s] 71%|███████▏  | 417/585 [02:51<00:50,  3.31it/s] 71%|███████▏  | 418/585 [02:51<00:50,  3.31it/s] 72%|███████▏  | 419/585 [02:52<00:51,  3.24it/s] 72%|███████▏  | 420/585 [02:52<00:50,  3.27it/s] 72%|███████▏  | 421/585 [02:52<00:49,  3.28it/s] 72%|███████▏  | 422/585 [02:53<00:49,  3.30it/s] 72%|███████▏  | 423/585 [02:53<00:49,  3.29it/s] 72%|███████▏  | 424/585 [02:53<00:48,  3.30it/s] 73%|███████▎  | 425/585 [02:53<00:48,  3.30it/s] 73%|███████▎  | 426/585 [02:54<00:48,  3.31it/s] 73%|███████▎  | 427/585 [02:54<00:47,  3.31it/s] 73%|███████▎  | 428/585 [02:54<00:47,  3.31it/s] 73%|███████▎  | 429/585 [02:55<00:47,  3.31it/s] 74%|███████▎  | 430/585 [02:55<00:46,  3.32it/s] 74%|███████▎  | 431/585 [02:55<00:46,  3.32it/s] 74%|███████▍  | 432/585 [02:56<00:46,  3.32it/s] 74%|███████▍  | 433/585 [02:56<00:45,  3.33it/s] 74%|███████▍  | 434/585 [02:56<00:45,  3.33it/s] 74%|███████▍  | 435/585 [02:56<00:45,  3.32it/s] 75%|███████▍  | 436/585 [02:57<00:44,  3.32it/s] 75%|███████▍  | 437/585 [02:57<00:44,  3.32it/s] 75%|███████▍  | 438/585 [02:57<00:44,  3.32it/s] 75%|███████▌  | 439/585 [02:58<00:44,  3.32it/s] 75%|███████▌  | 440/585 [02:58<00:43,  3.32it/s] 75%|███████▌  | 441/585 [02:58<00:43,  3.32it/s] 76%|███████▌  | 442/585 [02:59<00:43,  3.31it/s] 76%|███████▌  | 443/585 [02:59<00:42,  3.32it/s] 76%|███████▌  | 444/585 [02:59<00:42,  3.32it/s] 76%|███████▌  | 445/585 [02:59<00:42,  3.32it/s] 76%|███████▌  | 446/585 [03:00<00:41,  3.32it/s] 76%|███████▋  | 447/585 [03:00<00:41,  3.33it/s] 77%|███████▋  | 448/585 [03:00<00:41,  3.33it/s] 77%|███████▋  | 449/585 [03:01<00:40,  3.33it/s] 77%|███████▋  | 450/585 [03:01<00:40,  3.33it/s] 77%|███████▋  | 451/585 [03:01<00:40,  3.33it/s] 77%|███████▋  | 452/585 [03:02<00:39,  3.33it/s] 77%|███████▋  | 453/585 [03:02<00:39,  3.33it/s] 78%|███████▊  | 454/585 [03:02<00:39,  3.33it/s] 78%|███████▊  | 455/585 [03:03<00:39,  3.33it/s] 78%|███████▊  | 456/585 [03:03<00:38,  3.32it/s] 78%|███████▊  | 457/585 [03:03<00:38,  3.33it/s] 78%|███████▊  | 458/585 [03:03<00:38,  3.32it/s] 78%|███████▊  | 459/585 [03:04<00:38,  3.29it/s] 79%|███████▊  | 460/585 [03:04<00:37,  3.30it/s] 79%|███████▉  | 461/585 [03:04<00:37,  3.31it/s] 79%|███████▉  | 462/585 [03:05<00:37,  3.31it/s] 79%|███████▉  | 463/585 [03:05<00:36,  3.32it/s] 79%|███████▉  | 464/585 [03:05<00:36,  3.32it/s] 79%|███████▉  | 465/585 [03:06<00:36,  3.32it/s] 80%|███████▉  | 466/585 [03:06<00:35,  3.32it/s] 80%|███████▉  | 467/585 [03:06<00:35,  3.32it/s] 80%|████████  | 468/585 [03:06<00:35,  3.32it/s][INFO|trainer.py:2140] 2023-08-29 02:16:13,907 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:16:13,907 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-29 02:16:13,907 >>   Batch size = 8
{'eval_loss': 1.081719160079956, 'eval_runtime': 9.9174, 'eval_samples_per_second': 351.706, 'eval_steps_per_second': 43.963, 'epoch': 3.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 54.80it/s][A
  3%|▎         | 12/436 [00:00<00:08, 47.80it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.35it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.44it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.90it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.42it/s][A
  8%|▊         | 37/436 [00:00<00:09, 44.14it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.04it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.16it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.37it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.39it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.25it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.11it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.02it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.09it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.80it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 43.95it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.08it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.24it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.20it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.05it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 43.87it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.89it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 43.90it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.73it/s][A
 30%|███       | 132/436 [00:02<00:06, 43.98it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.14it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.17it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.27it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.15it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.01it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.92it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.78it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.84it/s][A
 41%|████      | 177/436 [00:03<00:05, 43.83it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.18it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.30it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.36it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.26it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.08it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.09it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 43.85it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 43.83it/s][A
 51%|█████     | 222/436 [00:05<00:04, 43.89it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.18it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.29it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.26it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.22it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.12it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 43.99it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.91it/s][A
 60%|██████    | 262/436 [00:05<00:03, 43.97it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.01it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.15it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.22it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.17it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.08it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.13it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 43.97it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.04it/s][A
 70%|███████   | 307/436 [00:06<00:02, 43.96it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.03it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.11it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.20it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.13it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.13it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.09it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 43.97it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 43.82it/s][A
 81%|████████  | 352/436 [00:07<00:01, 43.89it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.00it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.12it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.17it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.13it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 43.97it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.05it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 43.97it/s][A
 90%|████████▉ | 392/436 [00:08<00:01, 44.00it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.11it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.02it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.18it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.11it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.15it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.09it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 43.90it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.02it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.02it/s][A 80%|████████  | 468/585 [03:16<00:35,  3.32it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:16:23,832 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 02:16:23,857 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:16:25,537 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:16:25,552 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:16:25,565 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:22<09:15,  4.79s/it] 80%|████████  | 470/585 [03:22<06:36,  3.44s/it] 81%|████████  | 471/585 [03:22<04:45,  2.50s/it] 81%|████████  | 472/585 [03:23<03:27,  1.84s/it] 81%|████████  | 473/585 [03:23<02:34,  1.38s/it] 81%|████████  | 474/585 [03:23<01:57,  1.06s/it] 81%|████████  | 475/585 [03:23<01:31,  1.21it/s] 81%|████████▏ | 476/585 [03:24<01:13,  1.49it/s] 82%|████████▏ | 477/585 [03:24<01:00,  1.79it/s] 82%|████████▏ | 478/585 [03:24<00:51,  2.08it/s] 82%|████████▏ | 479/585 [03:25<00:45,  2.34it/s] 82%|████████▏ | 480/585 [03:25<00:40,  2.56it/s] 82%|████████▏ | 481/585 [03:25<00:37,  2.75it/s] 82%|████████▏ | 482/585 [03:26<00:35,  2.90it/s] 83%|████████▎ | 483/585 [03:26<00:33,  3.02it/s] 83%|████████▎ | 484/585 [03:26<00:32,  3.10it/s] 83%|████████▎ | 485/585 [03:27<00:31,  3.16it/s] 83%|████████▎ | 486/585 [03:27<00:30,  3.21it/s] 83%|████████▎ | 487/585 [03:27<00:30,  3.24it/s] 83%|████████▎ | 488/585 [03:27<00:29,  3.26it/s] 84%|████████▎ | 489/585 [03:28<00:29,  3.28it/s] 84%|████████▍ | 490/585 [03:28<00:28,  3.28it/s] 84%|████████▍ | 491/585 [03:28<00:28,  3.30it/s] 84%|████████▍ | 492/585 [03:29<00:28,  3.31it/s] 84%|████████▍ | 493/585 [03:29<00:27,  3.31it/s] 84%|████████▍ | 494/585 [03:29<00:27,  3.32it/s] 85%|████████▍ | 495/585 [03:30<00:27,  3.32it/s] 85%|████████▍ | 496/585 [03:30<00:26,  3.32it/s] 85%|████████▍ | 497/585 [03:30<00:26,  3.32it/s] 85%|████████▌ | 498/585 [03:30<00:26,  3.32it/s] 85%|████████▌ | 499/585 [03:31<00:25,  3.33it/s] 85%|████████▌ | 500/585 [03:31<00:25,  3.32it/s]                                                  85%|████████▌ | 500/585 [03:31<00:25,  3.32it/s] 86%|████████▌ | 501/585 [03:31<00:25,  3.32it/s] 86%|████████▌ | 502/585 [03:32<00:25,  3.32it/s] 86%|████████▌ | 503/585 [03:32<00:24,  3.31it/s] 86%|████████▌ | 504/585 [03:32<00:24,  3.31it/s] 86%|████████▋ | 505/585 [03:33<00:24,  3.31it/s] 86%|████████▋ | 506/585 [03:33<00:23,  3.32it/s] 87%|████████▋ | 507/585 [03:33<00:23,  3.32it/s] 87%|████████▋ | 508/585 [03:33<00:23,  3.32it/s] 87%|████████▋ | 509/585 [03:34<00:22,  3.32it/s] 87%|████████▋ | 510/585 [03:34<00:22,  3.32it/s] 87%|████████▋ | 511/585 [03:34<00:22,  3.32it/s] 88%|████████▊ | 512/585 [03:35<00:21,  3.32it/s] 88%|████████▊ | 513/585 [03:35<00:21,  3.32it/s] 88%|████████▊ | 514/585 [03:35<00:21,  3.33it/s] 88%|████████▊ | 515/585 [03:36<00:21,  3.32it/s] 88%|████████▊ | 516/585 [03:36<00:20,  3.33it/s] 88%|████████▊ | 517/585 [03:36<00:20,  3.32it/s] 89%|████████▊ | 518/585 [03:36<00:20,  3.32it/s] 89%|████████▊ | 519/585 [03:37<00:19,  3.32it/s] 89%|████████▉ | 520/585 [03:37<00:19,  3.31it/s] 89%|████████▉ | 521/585 [03:37<00:19,  3.31it/s] 89%|████████▉ | 522/585 [03:38<00:19,  3.32it/s] 89%|████████▉ | 523/585 [03:38<00:18,  3.32it/s] 90%|████████▉ | 524/585 [03:38<00:18,  3.32it/s] 90%|████████▉ | 525/585 [03:39<00:18,  3.32it/s] 90%|████████▉ | 526/585 [03:39<00:17,  3.32it/s] 90%|█████████ | 527/585 [03:39<00:17,  3.33it/s] 90%|█████████ | 528/585 [03:39<00:17,  3.33it/s] 90%|█████████ | 529/585 [03:40<00:16,  3.33it/s] 91%|█████████ | 530/585 [03:40<00:16,  3.31it/s] 91%|█████████ | 531/585 [03:40<00:16,  3.31it/s] 91%|█████████ | 532/585 [03:41<00:15,  3.32it/s] 91%|█████████ | 533/585 [03:41<00:15,  3.32it/s] 91%|█████████▏| 534/585 [03:41<00:15,  3.32it/s] 91%|█████████▏| 535/585 [03:42<00:15,  3.32it/s] 92%|█████████▏| 536/585 [03:42<00:14,  3.32it/s] 92%|█████████▏| 537/585 [03:42<00:14,  3.32it/s] 92%|█████████▏| 538/585 [03:42<00:14,  3.32it/s] 92%|█████████▏| 539/585 [03:43<00:13,  3.32it/s] 92%|█████████▏| 540/585 [03:43<00:13,  3.31it/s] 92%|█████████▏| 541/585 [03:43<00:13,  3.32it/s] 93%|█████████▎| 542/585 [03:44<00:12,  3.32it/s] 93%|█████████▎| 543/585 [03:44<00:12,  3.32it/s] 93%|█████████▎| 544/585 [03:44<00:12,  3.32it/s] 93%|█████████▎| 545/585 [03:45<00:12,  3.32it/s] 93%|█████████▎| 546/585 [03:45<00:11,  3.32it/s] 94%|█████████▎| 547/585 [03:45<00:11,  3.32it/s] 94%|█████████▎| 548/585 [03:45<00:11,  3.32it/s] 94%|█████████▍| 549/585 [03:46<00:10,  3.32it/s] 94%|█████████▍| 550/585 [03:46<00:10,  3.31it/s] 94%|█████████▍| 551/585 [03:46<00:10,  3.31it/s] 94%|█████████▍| 552/585 [03:47<00:09,  3.32it/s] 95%|█████████▍| 553/585 [03:47<00:09,  3.32it/s] 95%|█████████▍| 554/585 [03:47<00:09,  3.32it/s] 95%|█████████▍| 555/585 [03:48<00:09,  3.32it/s] 95%|█████████▌| 556/585 [03:48<00:08,  3.33it/s] 95%|█████████▌| 557/585 [03:48<00:08,  3.34it/s] 95%|█████████▌| 558/585 [03:48<00:08,  3.36it/s] 96%|█████████▌| 559/585 [03:49<00:07,  3.37it/s] 96%|█████████▌| 560/585 [03:49<00:07,  3.38it/s] 96%|█████████▌| 561/585 [03:49<00:07,  3.39it/s] 96%|█████████▌| 562/585 [03:50<00:06,  3.37it/s] 96%|█████████▌| 563/585 [03:50<00:06,  3.38it/s] 96%|█████████▋| 564/585 [03:50<00:06,  3.39it/s] 97%|█████████▋| 565/585 [03:51<00:05,  3.37it/s] 97%|█████████▋| 566/585 [03:51<00:05,  3.38it/s] 97%|█████████▋| 567/585 [03:51<00:05,  3.39it/s] 97%|█████████▋| 568/585 [03:51<00:05,  3.38it/s] 97%|█████████▋| 569/585 [03:52<00:04,  3.26it/s] 97%|█████████▋| 570/585 [03:52<00:04,  3.30it/s] 98%|█████████▊| 571/585 [03:52<00:04,  3.33it/s] 98%|█████████▊| 572/585 [03:53<00:03,  3.35it/s] 98%|█████████▊| 573/585 [03:53<00:03,  3.36it/s] 98%|█████████▊| 574/585 [03:53<00:03,  3.37it/s] 98%|█████████▊| 575/585 [03:54<00:02,  3.38it/s] 98%|█████████▊| 576/585 [03:54<00:02,  3.39it/s] 99%|█████████▊| 577/585 [03:54<00:02,  3.39it/s] 99%|█████████▉| 578/585 [03:54<00:02,  3.39it/s] 99%|█████████▉| 579/585 [03:55<00:01,  3.39it/s] 99%|█████████▉| 580/585 [03:55<00:01,  3.40it/s] 99%|█████████▉| 581/585 [03:55<00:01,  3.40it/s] 99%|█████████▉| 582/585 [03:56<00:00,  3.40it/s]100%|█████████▉| 583/585 [03:56<00:00,  3.40it/s]100%|█████████▉| 584/585 [03:56<00:00,  3.39it/s]100%|██████████| 585/585 [03:56<00:00,  3.39it/s][INFO|trainer.py:2140] 2023-08-29 02:17:03,908 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:17:03,908 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-29 02:17:03,908 >>   Batch size = 8
{'eval_loss': 1.0960367918014526, 'eval_runtime': 9.907, 'eval_samples_per_second': 352.076, 'eval_steps_per_second': 44.01, 'epoch': 4.0}
{'loss': 0.3958, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.11it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.42it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.32it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.38it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.97it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.51it/s][A
  8%|▊         | 37/436 [00:00<00:09, 44.28it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.02it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.12it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.30it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.30it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.30it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.20it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.16it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.16it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.92it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 43.81it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.04it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.19it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.12it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.19it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.14it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.08it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.04it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.97it/s][A
 30%|███       | 132/436 [00:02<00:06, 43.83it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 43.98it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.18it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.24it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.33it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.21it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.22it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.15it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.98it/s][A
 41%|████      | 177/436 [00:03<00:05, 43.81it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.05it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.14it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.19it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.16it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.22it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.19it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.10it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.00it/s][A
 51%|█████     | 222/436 [00:05<00:04, 43.82it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.01it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.05it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.16it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.17it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.20it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.10it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.96it/s][A
 60%|██████    | 262/436 [00:05<00:03, 43.91it/s][A
 61%|██████    | 267/436 [00:06<00:03, 43.89it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.10it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.17it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.17it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.25it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.23it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.15it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.03it/s][A
 70%|███████   | 307/436 [00:06<00:02, 43.92it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.01it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 43.99it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.19it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.23it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.18it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.18it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.07it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 43.91it/s][A
 81%|████████  | 352/436 [00:07<00:01, 43.85it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 43.94it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.07it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.22it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.21it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.27it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.24it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.12it/s][A
 90%|████████▉ | 392/436 [00:08<00:01, 43.99it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 43.93it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 43.93it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.11it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.22it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.21it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.28it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.22it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.06it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.06it/s][A100%|██████████| 585/585 [04:06<00:00,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:17:13,823 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 02:17:13,846 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:17:15,554 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:17:15,573 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:17:15,603 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 02:17:19,475 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 02:17:19,483 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117 (score: 1.0543519258499146).
                                                 100%|██████████| 585/585 [04:14<00:00,  3.39it/s]100%|██████████| 585/585 [04:14<00:00,  2.30it/s]
[INFO|trainer.py:1894] 2023-08-29 02:17:21,398 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 02:17:21,419 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:17:23,304 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:17:23,320 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:17:23,335 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:17:23,536 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:17:23,537 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:17:23,537 >>   train_loss               =     0.3928
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:17:23,537 >>   train_runtime            = 0:04:14.46
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:17:23,537 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:17:23,537 >>   train_samples_per_second =     147.37
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:17:23,537 >>   train_steps_per_second   =      2.299
{'eval_loss': 1.1011836528778076, 'eval_runtime': 9.8899, 'eval_samples_per_second': 352.682, 'eval_steps_per_second': 44.085, 'epoch': 5.0}
{'train_runtime': 254.4616, 'train_samples_per_second': 147.37, 'train_steps_per_second': 2.299, 'train_loss': 0.3927700988247863, 'epoch': 5.0}
08/29/2023 02:17:23 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 02:17:23,593 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:17:23,593 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-29 02:17:23,593 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 54.80it/s]  3%|▎         | 12/436 [00:00<00:08, 48.09it/s]  4%|▍         | 17/436 [00:00<00:08, 46.62it/s]  5%|▌         | 22/436 [00:00<00:09, 45.83it/s]  6%|▌         | 27/436 [00:00<00:09, 45.40it/s]  7%|▋         | 32/436 [00:00<00:08, 45.14it/s]  8%|▊         | 37/436 [00:00<00:08, 44.91it/s] 10%|▉         | 42/436 [00:00<00:08, 44.53it/s] 11%|█         | 47/436 [00:01<00:08, 43.95it/s] 12%|█▏        | 52/436 [00:01<00:08, 43.90it/s] 13%|█▎        | 57/436 [00:01<00:08, 44.07it/s] 14%|█▍        | 62/436 [00:01<00:08, 44.09it/s] 15%|█▌        | 67/436 [00:01<00:08, 44.24it/s] 17%|█▋        | 72/436 [00:01<00:08, 44.26it/s] 18%|█▊        | 77/436 [00:01<00:08, 44.36it/s] 19%|█▉        | 82/436 [00:01<00:08, 44.10it/s] 20%|█▉        | 87/436 [00:01<00:07, 43.90it/s] 21%|██        | 92/436 [00:02<00:07, 43.54it/s] 22%|██▏       | 97/436 [00:02<00:07, 43.72it/s] 23%|██▎       | 102/436 [00:02<00:07, 43.93it/s] 25%|██▍       | 107/436 [00:02<00:07, 44.02it/s] 26%|██▌       | 112/436 [00:02<00:07, 44.19it/s] 27%|██▋       | 117/436 [00:02<00:07, 44.30it/s] 28%|██▊       | 122/436 [00:02<00:07, 44.29it/s] 29%|██▉       | 127/436 [00:02<00:07, 44.11it/s] 30%|███       | 132/436 [00:02<00:06, 43.95it/s] 31%|███▏      | 137/436 [00:03<00:06, 43.74it/s] 33%|███▎      | 142/436 [00:03<00:06, 43.77it/s] 34%|███▎      | 147/436 [00:03<00:06, 43.79it/s] 35%|███▍      | 152/436 [00:03<00:06, 44.11it/s] 36%|███▌      | 157/436 [00:03<00:06, 44.27it/s] 37%|███▋      | 162/436 [00:03<00:06, 44.34it/s] 38%|███▊      | 167/436 [00:03<00:06, 44.27it/s] 39%|███▉      | 172/436 [00:03<00:05, 44.06it/s] 41%|████      | 177/436 [00:03<00:05, 43.94it/s] 42%|████▏     | 182/436 [00:04<00:05, 43.73it/s] 43%|████▎     | 187/436 [00:04<00:05, 43.83it/s] 44%|████▍     | 192/436 [00:04<00:05, 43.88it/s] 45%|████▌     | 197/436 [00:04<00:05, 44.13it/s] 46%|████▋     | 202/436 [00:04<00:05, 44.30it/s] 47%|████▋     | 207/436 [00:04<00:05, 44.25it/s] 49%|████▊     | 212/436 [00:04<00:05, 44.17it/s] 50%|████▉     | 217/436 [00:04<00:04, 44.08it/s] 51%|█████     | 222/436 [00:05<00:04, 43.79it/s] 52%|█████▏    | 227/436 [00:05<00:04, 43.81it/s] 53%|█████▎    | 232/436 [00:05<00:04, 43.79it/s] 54%|█████▍    | 237/436 [00:05<00:04, 43.86it/s] 56%|█████▌    | 242/436 [00:05<00:04, 44.23it/s] 57%|█████▋    | 247/436 [00:05<00:04, 44.24it/s] 58%|█████▊    | 252/436 [00:05<00:04, 44.22it/s] 59%|█████▉    | 257/436 [00:05<00:04, 44.16it/s] 60%|██████    | 262/436 [00:05<00:03, 44.06it/s] 61%|██████    | 267/436 [00:06<00:03, 43.84it/s] 62%|██████▏   | 272/436 [00:06<00:03, 43.81it/s] 64%|██████▎   | 277/436 [00:06<00:03, 43.87it/s] 65%|██████▍   | 282/436 [00:06<00:03, 44.02it/s] 66%|██████▌   | 287/436 [00:06<00:03, 44.15it/s] 67%|██████▋   | 292/436 [00:06<00:03, 44.22it/s] 68%|██████▊   | 297/436 [00:06<00:03, 44.15it/s] 69%|██████▉   | 302/436 [00:06<00:03, 44.05it/s] 70%|███████   | 307/436 [00:06<00:02, 43.95it/s] 72%|███████▏  | 312/436 [00:07<00:02, 43.87it/s] 73%|███████▎  | 317/436 [00:07<00:02, 43.85it/s] 74%|███████▍  | 322/436 [00:07<00:02, 43.93it/s] 75%|███████▌  | 327/436 [00:07<00:02, 44.05it/s] 76%|███████▌  | 332/436 [00:07<00:02, 44.11it/s] 77%|███████▋  | 337/436 [00:07<00:02, 44.09it/s] 78%|███████▊  | 342/436 [00:07<00:02, 44.01it/s] 80%|███████▉  | 347/436 [00:07<00:02, 43.75it/s] 81%|████████  | 352/436 [00:07<00:01, 43.66it/s] 82%|████████▏ | 357/436 [00:08<00:01, 43.78it/s] 83%|████████▎ | 362/436 [00:08<00:01, 43.73it/s] 84%|████████▍ | 367/436 [00:08<00:01, 43.97it/s] 85%|████████▌ | 372/436 [00:08<00:01, 44.13it/s] 86%|████████▋ | 377/436 [00:08<00:01, 44.18it/s] 88%|████████▊ | 382/436 [00:08<00:01, 44.21it/s] 89%|████████▉ | 387/436 [00:08<00:01, 44.11it/s] 90%|████████▉ | 392/436 [00:08<00:00, 44.04it/s] 91%|█████████ | 397/436 [00:08<00:00, 43.80it/s] 92%|█████████▏| 402/436 [00:09<00:00, 43.88it/s] 93%|█████████▎| 407/436 [00:09<00:00, 43.93it/s] 94%|█████████▍| 412/436 [00:09<00:00, 43.98it/s] 96%|█████████▌| 417/436 [00:09<00:00, 44.12it/s] 97%|█████████▋| 422/436 [00:09<00:00, 44.14it/s] 98%|█████████▊| 427/436 [00:09<00:00, 44.05it/s] 99%|█████████▉| 432/436 [00:09<00:00, 44.06it/s]100%|██████████| 436/436 [00:09<00:00, 44.10it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:17:33,502 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:17:33,502 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:17:33,502 >>   eval_loss               =     1.0544
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:17:33,502 >>   eval_runtime            = 0:00:09.90
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:17:33,502 >>   eval_samples            =       3488
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:17:33,502 >>   eval_samples_per_second =    352.009
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:17:33,502 >>   eval_steps_per_second   =     44.001
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:17:33,502 >>   perplexity              =     2.8701
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:39,578 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:39,584 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:39,584 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:39,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:39,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:17:40,292 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:17:40,293 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:17:40,960 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:17:41,979 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:17:41,979 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:44,489 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:44,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:44,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:44,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:44,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:17:45,235 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:17:45,236 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:17:45,492 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:17:45,646 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:17:45,646 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11910
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12010, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.28it/s]Extractor Predicting: 2it [00:01,  1.28it/s]Extractor Predicting: 3it [00:02,  1.30it/s]Extractor Predicting: 4it [00:03,  1.35it/s]Extractor Predicting: 5it [00:03,  1.38it/s]Extractor Predicting: 6it [00:04,  1.38it/s]Extractor Predicting: 7it [00:05,  1.40it/s]Extractor Predicting: 8it [00:05,  1.44it/s]Extractor Predicting: 9it [00:06,  1.40it/s]Extractor Predicting: 10it [00:07,  1.34it/s]Extractor Predicting: 11it [00:08,  1.36it/s]Extractor Predicting: 12it [00:08,  1.35it/s]Extractor Predicting: 13it [00:09,  1.36it/s]Extractor Predicting: 14it [00:10,  1.35it/s]Extractor Predicting: 15it [00:10,  1.38it/s]Extractor Predicting: 16it [00:11,  1.36it/s]Extractor Predicting: 17it [00:12,  1.36it/s]Extractor Predicting: 18it [00:13,  1.38it/s]Extractor Predicting: 19it [00:13,  1.41it/s]Extractor Predicting: 20it [00:14,  1.40it/s]Extractor Predicting: 21it [00:15,  1.37it/s]Extractor Predicting: 22it [00:16,  1.39it/s]Extractor Predicting: 23it [00:16,  1.40it/s]Extractor Predicting: 24it [00:17,  1.40it/s]Extractor Predicting: 25it [00:18,  1.37it/s]Extractor Predicting: 26it [00:18,  1.37it/s]Extractor Predicting: 27it [00:19,  1.37it/s]Extractor Predicting: 28it [00:20,  1.38it/s]Extractor Predicting: 29it [00:21,  1.36it/s]Extractor Predicting: 30it [00:21,  1.35it/s]Extractor Predicting: 31it [00:22,  1.33it/s]Extractor Predicting: 32it [00:23,  1.34it/s]Extractor Predicting: 33it [00:24,  1.32it/s]Extractor Predicting: 34it [00:24,  1.31it/s]Extractor Predicting: 35it [00:25,  1.29it/s]Extractor Predicting: 36it [00:26,  1.28it/s]Extractor Predicting: 37it [00:27,  1.29it/s]Extractor Predicting: 38it [00:28,  1.29it/s]Extractor Predicting: 39it [00:28,  1.30it/s]Extractor Predicting: 40it [00:29,  1.29it/s]Extractor Predicting: 41it [00:30,  1.29it/s]Extractor Predicting: 42it [00:31,  1.29it/s]Extractor Predicting: 43it [00:32,  1.27it/s]Extractor Predicting: 44it [00:32,  1.27it/s]Extractor Predicting: 45it [00:33,  1.26it/s]Extractor Predicting: 46it [00:34,  1.26it/s]Extractor Predicting: 47it [00:35,  1.30it/s]Extractor Predicting: 48it [00:35,  1.30it/s]Extractor Predicting: 49it [00:36,  1.33it/s]Extractor Predicting: 50it [00:37,  1.29it/s]Extractor Predicting: 51it [00:38,  1.32it/s]Extractor Predicting: 52it [00:38,  1.33it/s]Extractor Predicting: 53it [00:39,  1.32it/s]Extractor Predicting: 54it [00:40,  1.32it/s]Extractor Predicting: 55it [00:41,  1.32it/s]Extractor Predicting: 56it [00:41,  1.31it/s]Extractor Predicting: 57it [00:42,  1.34it/s]Extractor Predicting: 58it [00:43,  1.34it/s]Extractor Predicting: 59it [00:44,  1.30it/s]Extractor Predicting: 60it [00:45,  1.28it/s]Extractor Predicting: 61it [00:45,  1.27it/s]Extractor Predicting: 62it [00:46,  1.28it/s]Extractor Predicting: 63it [00:47,  1.30it/s]Extractor Predicting: 64it [00:48,  1.28it/s]Extractor Predicting: 65it [00:48,  1.31it/s]Extractor Predicting: 66it [00:49,  1.32it/s]Extractor Predicting: 67it [00:50,  1.32it/s]Extractor Predicting: 68it [00:51,  1.33it/s]Extractor Predicting: 69it [00:51,  1.32it/s]Extractor Predicting: 70it [00:52,  1.32it/s]Extractor Predicting: 71it [00:53,  1.29it/s]Extractor Predicting: 72it [00:54,  1.31it/s]Extractor Predicting: 73it [00:54,  1.30it/s]Extractor Predicting: 74it [00:55,  1.32it/s]Extractor Predicting: 75it [00:56,  1.32it/s]Extractor Predicting: 76it [00:57,  1.33it/s]Extractor Predicting: 77it [00:57,  1.35it/s]Extractor Predicting: 78it [00:58,  1.32it/s]Extractor Predicting: 79it [00:59,  1.32it/s]Extractor Predicting: 80it [01:00,  1.30it/s]Extractor Predicting: 81it [01:01,  1.29it/s]Extractor Predicting: 82it [01:01,  1.30it/s]Extractor Predicting: 83it [01:02,  1.33it/s]Extractor Predicting: 84it [01:03,  1.31it/s]Extractor Predicting: 85it [01:04,  1.30it/s]Extractor Predicting: 86it [01:04,  1.31it/s]Extractor Predicting: 87it [01:05,  1.22it/s]Extractor Predicting: 88it [01:06,  1.26it/s]Extractor Predicting: 89it [01:07,  1.28it/s]Extractor Predicting: 90it [01:07,  1.32it/s]Extractor Predicting: 91it [01:08,  1.35it/s]Extractor Predicting: 92it [01:09,  1.37it/s]Extractor Predicting: 93it [01:10,  1.35it/s]Extractor Predicting: 94it [01:10,  1.37it/s]Extractor Predicting: 95it [01:11,  1.38it/s]Extractor Predicting: 96it [01:12,  1.37it/s]Extractor Predicting: 97it [01:13,  1.36it/s]Extractor Predicting: 98it [01:13,  1.34it/s]Extractor Predicting: 99it [01:14,  1.31it/s]Extractor Predicting: 100it [01:15,  1.30it/s]Extractor Predicting: 101it [01:16,  1.35it/s]Extractor Predicting: 102it [01:16,  1.39it/s]Extractor Predicting: 103it [01:17,  1.37it/s]Extractor Predicting: 104it [01:18,  1.38it/s]Extractor Predicting: 105it [01:18,  1.37it/s]Extractor Predicting: 106it [01:19,  1.39it/s]Extractor Predicting: 107it [01:20,  1.37it/s]Extractor Predicting: 108it [01:21,  1.36it/s]Extractor Predicting: 109it [01:21,  1.36it/s]Extractor Predicting: 110it [01:22,  1.34it/s]Extractor Predicting: 111it [01:23,  1.38it/s]Extractor Predicting: 112it [01:24,  1.38it/s]Extractor Predicting: 113it [01:24,  1.42it/s]Extractor Predicting: 114it [01:25,  1.40it/s]Extractor Predicting: 115it [01:26,  1.41it/s]Extractor Predicting: 116it [01:26,  1.40it/s]Extractor Predicting: 117it [01:27,  1.37it/s]Extractor Predicting: 118it [01:28,  1.35it/s]Extractor Predicting: 119it [01:29,  1.33it/s]Extractor Predicting: 120it [01:29,  1.31it/s]Extractor Predicting: 121it [01:30,  1.34it/s]Extractor Predicting: 122it [01:31,  1.34it/s]Extractor Predicting: 123it [01:32,  1.33it/s]Extractor Predicting: 124it [01:33,  1.31it/s]Extractor Predicting: 125it [01:33,  1.30it/s]Extractor Predicting: 126it [01:34,  1.28it/s]Extractor Predicting: 127it [01:35,  1.29it/s]Extractor Predicting: 128it [01:36,  1.34it/s]Extractor Predicting: 129it [01:36,  1.32it/s]Extractor Predicting: 130it [01:37,  1.35it/s]Extractor Predicting: 131it [01:38,  1.34it/s]Extractor Predicting: 132it [01:39,  1.34it/s]Extractor Predicting: 133it [01:39,  1.33it/s]Extractor Predicting: 134it [01:40,  1.30it/s]Extractor Predicting: 135it [01:41,  1.30it/s]Extractor Predicting: 136it [01:42,  1.32it/s]Extractor Predicting: 137it [01:42,  1.33it/s]Extractor Predicting: 138it [01:43,  1.33it/s]Extractor Predicting: 139it [01:44,  1.31it/s]Extractor Predicting: 140it [01:45,  1.33it/s]Extractor Predicting: 141it [01:45,  1.32it/s]Extractor Predicting: 142it [01:46,  1.29it/s]Extractor Predicting: 143it [01:47,  1.31it/s]Extractor Predicting: 144it [01:47,  1.59it/s]Extractor Predicting: 144it [01:47,  1.34it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:19:41,698 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:19:41,702 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:19:41,703 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:19:41,703 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:19:41,703 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:19:42,332 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:19:42,333 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:19:42,902 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:19:43,923 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:19:43,923 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:19:46,742 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:19:46,747 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:19:46,747 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:19:46,747 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:19:46,747 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:19:47,400 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:19:47,401 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:19:47,962 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:19:48,121 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:19:48,121 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.31916537867078826,
  "recall": 0.1184059633027523,
  "score": 0.1727310748640736,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19834
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19934, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.45it/s]Extractor Predicting: 2it [00:01,  1.39it/s]Extractor Predicting: 3it [00:02,  1.35it/s]Extractor Predicting: 4it [00:02,  1.32it/s]Extractor Predicting: 5it [00:03,  1.29it/s]Extractor Predicting: 6it [00:04,  1.30it/s]Extractor Predicting: 7it [00:05,  1.32it/s]Extractor Predicting: 8it [00:06,  1.32it/s]Extractor Predicting: 9it [00:06,  1.34it/s]Extractor Predicting: 10it [00:07,  1.30it/s]Extractor Predicting: 11it [00:08,  1.30it/s]Extractor Predicting: 12it [00:09,  1.30it/s]Extractor Predicting: 13it [00:09,  1.31it/s]Extractor Predicting: 14it [00:10,  1.31it/s]Extractor Predicting: 15it [00:11,  1.30it/s]Extractor Predicting: 16it [00:12,  1.29it/s]Extractor Predicting: 17it [00:13,  1.27it/s]Extractor Predicting: 18it [00:13,  1.30it/s]Extractor Predicting: 19it [00:14,  1.32it/s]Extractor Predicting: 20it [00:15,  1.32it/s]Extractor Predicting: 21it [00:15,  1.32it/s]Extractor Predicting: 22it [00:16,  1.35it/s]Extractor Predicting: 23it [00:17,  1.36it/s]Extractor Predicting: 24it [00:18,  1.32it/s]Extractor Predicting: 25it [00:19,  1.30it/s]Extractor Predicting: 26it [00:19,  1.33it/s]Extractor Predicting: 27it [00:20,  1.31it/s]Extractor Predicting: 28it [00:21,  1.29it/s]Extractor Predicting: 29it [00:22,  1.30it/s]Extractor Predicting: 30it [00:22,  1.26it/s]Extractor Predicting: 31it [00:23,  1.32it/s]Extractor Predicting: 32it [00:24,  1.38it/s]Extractor Predicting: 33it [00:24,  1.40it/s]Extractor Predicting: 34it [00:25,  1.41it/s]Extractor Predicting: 35it [00:26,  1.43it/s]Extractor Predicting: 36it [00:26,  1.44it/s]Extractor Predicting: 37it [00:27,  1.44it/s]Extractor Predicting: 38it [00:28,  1.47it/s]Extractor Predicting: 39it [00:28,  1.50it/s]Extractor Predicting: 40it [00:29,  1.52it/s]Extractor Predicting: 41it [00:30,  1.51it/s]Extractor Predicting: 42it [00:30,  1.52it/s]Extractor Predicting: 43it [00:31,  1.50it/s]Extractor Predicting: 44it [00:32,  1.53it/s]Extractor Predicting: 45it [00:32,  1.52it/s]Extractor Predicting: 46it [00:33,  1.47it/s]Extractor Predicting: 47it [00:34,  1.50it/s]Extractor Predicting: 48it [00:34,  1.53it/s]Extractor Predicting: 49it [00:35,  1.55it/s]Extractor Predicting: 50it [00:36,  1.52it/s]Extractor Predicting: 51it [00:36,  1.55it/s]Extractor Predicting: 52it [00:37,  1.52it/s]Extractor Predicting: 53it [00:38,  1.51it/s]Extractor Predicting: 54it [00:38,  1.50it/s]Extractor Predicting: 55it [00:39,  1.47it/s]Extractor Predicting: 56it [00:40,  1.49it/s]Extractor Predicting: 57it [00:40,  1.48it/s]Extractor Predicting: 58it [00:41,  1.45it/s]Extractor Predicting: 59it [00:42,  1.41it/s]Extractor Predicting: 60it [00:43,  1.36it/s]Extractor Predicting: 61it [00:43,  1.31it/s]Extractor Predicting: 62it [00:44,  1.27it/s]Extractor Predicting: 63it [00:45,  1.28it/s]Extractor Predicting: 64it [00:46,  1.26it/s]Extractor Predicting: 65it [00:47,  1.29it/s]Extractor Predicting: 66it [00:47,  1.29it/s]Extractor Predicting: 67it [00:48,  1.29it/s]Extractor Predicting: 68it [00:49,  1.26it/s]Extractor Predicting: 69it [00:50,  1.24it/s]Extractor Predicting: 70it [00:51,  1.23it/s]Extractor Predicting: 71it [00:52,  1.21it/s]Extractor Predicting: 72it [00:52,  1.24it/s]Extractor Predicting: 73it [00:53,  1.24it/s]Extractor Predicting: 74it [00:54,  1.26it/s]Extractor Predicting: 75it [00:55,  1.27it/s]Extractor Predicting: 76it [00:55,  1.28it/s]Extractor Predicting: 77it [00:56,  1.27it/s]Extractor Predicting: 78it [00:57,  1.25it/s]Extractor Predicting: 79it [00:58,  1.23it/s]Extractor Predicting: 80it [00:59,  1.24it/s]Extractor Predicting: 81it [01:00,  1.23it/s]Extractor Predicting: 82it [01:00,  1.23it/s]Extractor Predicting: 83it [01:01,  1.23it/s]Extractor Predicting: 84it [01:02,  1.26it/s]Extractor Predicting: 85it [01:03,  1.25it/s]Extractor Predicting: 86it [01:04,  1.23it/s]Extractor Predicting: 87it [01:04,  1.23it/s]Extractor Predicting: 88it [01:05,  1.25it/s]Extractor Predicting: 89it [01:06,  1.26it/s]Extractor Predicting: 90it [01:07,  1.30it/s]Extractor Predicting: 91it [01:07,  1.27it/s]Extractor Predicting: 92it [01:08,  1.25it/s]Extractor Predicting: 93it [01:09,  1.28it/s]Extractor Predicting: 94it [01:10,  1.28it/s]Extractor Predicting: 95it [01:11,  1.31it/s]Extractor Predicting: 96it [01:11,  1.34it/s]Extractor Predicting: 97it [01:12,  1.34it/s]Extractor Predicting: 98it [01:13,  1.36it/s]Extractor Predicting: 99it [01:13,  1.35it/s]Extractor Predicting: 100it [01:14,  1.35it/s]Extractor Predicting: 101it [01:15,  1.40it/s]Extractor Predicting: 102it [01:16,  1.40it/s]Extractor Predicting: 103it [01:16,  1.36it/s]Extractor Predicting: 104it [01:17,  1.22it/s]Extractor Predicting: 105it [01:18,  1.25it/s]Extractor Predicting: 106it [01:19,  1.26it/s]Extractor Predicting: 107it [01:20,  1.28it/s]Extractor Predicting: 108it [01:20,  1.29it/s]Extractor Predicting: 109it [01:21,  1.28it/s]Extractor Predicting: 110it [01:22,  1.27it/s]Extractor Predicting: 111it [01:23,  1.29it/s]Extractor Predicting: 112it [01:24,  1.27it/s]Extractor Predicting: 113it [01:24,  1.31it/s]Extractor Predicting: 114it [01:25,  1.27it/s]Extractor Predicting: 115it [01:26,  1.28it/s]Extractor Predicting: 116it [01:27,  1.33it/s]Extractor Predicting: 117it [01:27,  1.33it/s]Extractor Predicting: 118it [01:28,  1.38it/s]Extractor Predicting: 119it [01:29,  1.40it/s]Extractor Predicting: 120it [01:29,  1.41it/s]Extractor Predicting: 121it [01:30,  1.43it/s]Extractor Predicting: 122it [01:31,  1.42it/s]Extractor Predicting: 123it [01:31,  1.43it/s]Extractor Predicting: 124it [01:32,  1.44it/s]Extractor Predicting: 125it [01:33,  1.46it/s]Extractor Predicting: 126it [01:34,  1.43it/s]Extractor Predicting: 127it [01:34,  1.43it/s]Extractor Predicting: 128it [01:35,  1.47it/s]Extractor Predicting: 129it [01:36,  1.46it/s]Extractor Predicting: 130it [01:36,  1.53it/s]Extractor Predicting: 131it [01:37,  1.54it/s]Extractor Predicting: 132it [01:37,  1.50it/s]Extractor Predicting: 133it [01:38,  1.45it/s]Extractor Predicting: 134it [01:39,  1.44it/s]Extractor Predicting: 135it [01:40,  1.46it/s]Extractor Predicting: 136it [01:40,  1.49it/s]Extractor Predicting: 137it [01:41,  1.49it/s]Extractor Predicting: 138it [01:42,  1.48it/s]Extractor Predicting: 139it [01:42,  1.45it/s]Extractor Predicting: 140it [01:43,  1.48it/s]Extractor Predicting: 141it [01:44,  1.46it/s]Extractor Predicting: 142it [01:44,  1.51it/s]Extractor Predicting: 143it [01:45,  1.50it/s]Extractor Predicting: 144it [01:46,  1.48it/s]Extractor Predicting: 145it [01:46,  1.46it/s]Extractor Predicting: 146it [01:47,  1.42it/s]Extractor Predicting: 147it [01:48,  1.40it/s]Extractor Predicting: 148it [01:49,  1.40it/s]Extractor Predicting: 149it [01:49,  1.38it/s]Extractor Predicting: 150it [01:50,  1.36it/s]Extractor Predicting: 151it [01:51,  1.36it/s]Extractor Predicting: 152it [01:52,  1.37it/s]Extractor Predicting: 153it [01:52,  1.34it/s]Extractor Predicting: 154it [01:53,  1.35it/s]Extractor Predicting: 155it [01:54,  1.38it/s]Extractor Predicting: 156it [01:54,  1.35it/s]Extractor Predicting: 157it [01:55,  1.36it/s]Extractor Predicting: 158it [01:56,  1.34it/s]Extractor Predicting: 159it [01:57,  1.33it/s]Extractor Predicting: 160it [01:58,  1.32it/s]Extractor Predicting: 161it [01:58,  1.32it/s]Extractor Predicting: 162it [01:59,  1.31it/s]Extractor Predicting: 163it [02:00,  1.33it/s]Extractor Predicting: 164it [02:01,  1.33it/s]Extractor Predicting: 165it [02:01,  1.32it/s]Extractor Predicting: 166it [02:02,  1.29it/s]Extractor Predicting: 167it [02:03,  1.32it/s]Extractor Predicting: 168it [02:04,  1.31it/s]Extractor Predicting: 169it [02:04,  1.33it/s]Extractor Predicting: 170it [02:05,  1.32it/s]Extractor Predicting: 171it [02:06,  1.32it/s]Extractor Predicting: 172it [02:07,  1.34it/s]Extractor Predicting: 173it [02:07,  1.32it/s]Extractor Predicting: 174it [02:08,  1.31it/s]Extractor Predicting: 175it [02:09,  1.33it/s]Extractor Predicting: 176it [02:10,  1.32it/s]Extractor Predicting: 177it [02:10,  1.31it/s]Extractor Predicting: 178it [02:11,  1.32it/s]Extractor Predicting: 179it [02:12,  1.32it/s]Extractor Predicting: 180it [02:13,  1.32it/s]Extractor Predicting: 181it [02:13,  1.31it/s]Extractor Predicting: 182it [02:14,  1.31it/s]Extractor Predicting: 183it [02:15,  1.31it/s]Extractor Predicting: 184it [02:16,  1.32it/s]Extractor Predicting: 185it [02:16,  1.33it/s]Extractor Predicting: 186it [02:17,  1.31it/s]Extractor Predicting: 187it [02:18,  1.33it/s]Extractor Predicting: 188it [02:19,  1.33it/s]Extractor Predicting: 189it [02:19,  1.35it/s]Extractor Predicting: 190it [02:20,  1.34it/s]Extractor Predicting: 191it [02:21,  1.36it/s]Extractor Predicting: 192it [02:22,  1.35it/s]Extractor Predicting: 193it [02:22,  1.38it/s]Extractor Predicting: 194it [02:23,  1.35it/s]Extractor Predicting: 195it [02:24,  1.35it/s]Extractor Predicting: 196it [02:25,  1.35it/s]Extractor Predicting: 197it [02:25,  1.35it/s]Extractor Predicting: 198it [02:26,  1.31it/s]Extractor Predicting: 199it [02:27,  1.31it/s]Extractor Predicting: 200it [02:28,  1.32it/s]Extractor Predicting: 201it [02:28,  1.35it/s]Extractor Predicting: 202it [02:29,  1.34it/s]Extractor Predicting: 203it [02:30,  1.36it/s]Extractor Predicting: 204it [02:31,  1.35it/s]Extractor Predicting: 205it [02:31,  1.34it/s]Extractor Predicting: 206it [02:32,  1.39it/s]Extractor Predicting: 207it [02:33,  1.26it/s]Extractor Predicting: 208it [02:34,  1.31it/s]Extractor Predicting: 209it [02:34,  1.31it/s]Extractor Predicting: 210it [02:35,  1.32it/s]Extractor Predicting: 211it [02:36,  1.31it/s]Extractor Predicting: 212it [02:37,  1.29it/s]Extractor Predicting: 213it [02:38,  1.30it/s]Extractor Predicting: 214it [02:38,  1.31it/s]Extractor Predicting: 215it [02:39,  1.29it/s]Extractor Predicting: 216it [02:40,  1.29it/s]Extractor Predicting: 217it [02:41,  1.26it/s]Extractor Predicting: 218it [02:41,  1.28it/s]Extractor Predicting: 219it [02:42,  1.28it/s]Extractor Predicting: 220it [02:43,  1.31it/s]Extractor Predicting: 221it [02:44,  1.31it/s]Extractor Predicting: 222it [02:44,  1.30it/s]Extractor Predicting: 223it [02:45,  1.32it/s]Extractor Predicting: 224it [02:46,  1.30it/s]Extractor Predicting: 225it [02:47,  1.30it/s]Extractor Predicting: 226it [02:48,  1.31it/s]Extractor Predicting: 227it [02:48,  1.30it/s]Extractor Predicting: 228it [02:49,  1.34it/s]Extractor Predicting: 229it [02:50,  1.30it/s]Extractor Predicting: 230it [02:51,  1.31it/s]Extractor Predicting: 231it [02:51,  1.28it/s]Extractor Predicting: 232it [02:52,  1.28it/s]Extractor Predicting: 233it [02:53,  1.28it/s]Extractor Predicting: 234it [02:54,  1.31it/s]Extractor Predicting: 235it [02:54,  1.32it/s]Extractor Predicting: 236it [02:55,  1.33it/s]Extractor Predicting: 237it [02:56,  1.26it/s]Extractor Predicting: 238it [02:57,  1.25it/s]Extractor Predicting: 239it [02:58,  1.25it/s]Extractor Predicting: 240it [02:58,  1.27it/s]Extractor Predicting: 241it [02:59,  1.26it/s]Extractor Predicting: 242it [03:00,  1.29it/s]Extractor Predicting: 243it [03:01,  1.28it/s]Extractor Predicting: 244it [03:02,  1.28it/s]Extractor Predicting: 245it [03:02,  1.29it/s]Extractor Predicting: 246it [03:03,  1.26it/s]Extractor Predicting: 247it [03:04,  1.23it/s]Extractor Predicting: 248it [03:05,  1.25it/s]Extractor Predicting: 249it [03:05,  1.29it/s]Extractor Predicting: 250it [03:06,  1.32it/s]Extractor Predicting: 251it [03:07,  1.30it/s]Extractor Predicting: 252it [03:08,  1.32it/s]Extractor Predicting: 253it [03:08,  1.33it/s]Extractor Predicting: 254it [03:09,  1.30it/s]Extractor Predicting: 255it [03:10,  1.30it/s]Extractor Predicting: 256it [03:11,  1.28it/s]Extractor Predicting: 257it [03:12,  1.29it/s]Extractor Predicting: 258it [03:12,  1.28it/s]Extractor Predicting: 259it [03:13,  1.31it/s]Extractor Predicting: 260it [03:14,  1.33it/s]Extractor Predicting: 261it [03:15,  1.35it/s]Extractor Predicting: 262it [03:15,  1.35it/s]Extractor Predicting: 263it [03:16,  1.33it/s]Extractor Predicting: 264it [03:17,  1.35it/s]Extractor Predicting: 265it [03:18,  1.35it/s]Extractor Predicting: 266it [03:18,  1.34it/s]Extractor Predicting: 267it [03:19,  1.36it/s]Extractor Predicting: 268it [03:20,  1.36it/s]Extractor Predicting: 269it [03:21,  1.35it/s]Extractor Predicting: 270it [03:21,  1.32it/s]Extractor Predicting: 271it [03:22,  1.36it/s]Extractor Predicting: 272it [03:23,  1.36it/s]Extractor Predicting: 273it [03:23,  1.38it/s]Extractor Predicting: 274it [03:24,  1.39it/s]Extractor Predicting: 275it [03:25,  1.39it/s]Extractor Predicting: 276it [03:26,  1.36it/s]Extractor Predicting: 277it [03:26,  1.36it/s]Extractor Predicting: 278it [03:27,  1.37it/s]Extractor Predicting: 279it [03:28,  1.36it/s]Extractor Predicting: 280it [03:29,  1.34it/s]Extractor Predicting: 281it [03:29,  1.37it/s]Extractor Predicting: 282it [03:30,  1.24it/s]Extractor Predicting: 283it [03:31,  1.23it/s]Extractor Predicting: 284it [03:32,  1.44it/s]Extractor Predicting: 284it [03:32,  1.34it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:23:27,428 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:23:27,433 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:23:27,433 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:23:27,433 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:23:27,433 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:23:28,050 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:23:28,051 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:23:28,635 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:23:29,647 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:23:29,648 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:23:32,490 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:23:32,496 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:23:32,496 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:23:32,496 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:23:32,496 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:23:33,158 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:23:33,160 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:23:33,735 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:23:33,894 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:23:33,894 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3395973154362416,
  "recall": 0.11158482799176712,
  "score": 0.1679760982626978,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1045
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1145, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.28it/s]Extractor Predicting: 2it [00:01,  1.16it/s]Extractor Predicting: 3it [00:02,  1.19it/s]Extractor Predicting: 4it [00:03,  1.22it/s]Extractor Predicting: 5it [00:03,  1.66it/s]Extractor Predicting: 5it [00:03,  1.42it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3157894736842105,
  "recall": 0.030303030303030304,
  "score": 0.055299539170506916,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_3/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_3', 'type': 'train', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/synthetic/1_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/synthetic/2_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/synthetic/3_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/synthetic/4_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_3/extractor/iter1/results_single_is_eval_True_limit5000.json'
