/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_3', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/1_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/2_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11910
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12010, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Predicting: 1it [00:15, 15.43s/it]Extractor Predicting: 2it [00:16,  7.07s/it]Extractor Predicting: 3it [00:17,  4.13s/it]Extractor Predicting: 4it [00:17,  2.72s/it]Extractor Predicting: 5it [00:18,  1.95s/it]Extractor Predicting: 6it [00:19,  1.49s/it]Extractor Predicting: 7it [00:19,  1.19s/it]Extractor Predicting: 8it [00:20,  1.02it/s]Extractor Predicting: 9it [00:20,  1.16it/s]Extractor Predicting: 10it [00:21,  1.29it/s]Extractor Predicting: 11it [00:21,  1.38it/s]Extractor Predicting: 12it [00:22,  1.45it/s]Extractor Predicting: 13it [00:23,  1.52it/s]Extractor Predicting: 14it [00:23,  1.55it/s]Extractor Predicting: 15it [00:24,  1.62it/s]Extractor Predicting: 16it [00:24,  1.62it/s]Extractor Predicting: 17it [00:25,  1.64it/s]Extractor Predicting: 18it [00:26,  1.68it/s]Extractor Predicting: 19it [00:26,  1.72it/s]Extractor Predicting: 20it [00:27,  1.72it/s]Extractor Predicting: 21it [00:27,  1.69it/s]Extractor Predicting: 22it [00:28,  1.70it/s]Extractor Predicting: 23it [00:28,  1.71it/s]Extractor Predicting: 24it [00:29,  1.73it/s]Extractor Predicting: 25it [00:30,  1.68it/s]Extractor Predicting: 26it [00:30,  1.69it/s]Extractor Predicting: 27it [00:31,  1.70it/s]Extractor Predicting: 28it [00:31,  1.71it/s]Extractor Predicting: 29it [00:32,  1.69it/s]Extractor Predicting: 30it [00:33,  1.66it/s]Extractor Predicting: 31it [00:33,  1.66it/s]Extractor Predicting: 32it [00:34,  1.68it/s]Extractor Predicting: 33it [00:34,  1.66it/s]Extractor Predicting: 34it [00:35,  1.63it/s]Extractor Predicting: 35it [00:36,  1.61it/s]Extractor Predicting: 36it [00:36,  1.61it/s]Extractor Predicting: 37it [00:37,  1.62it/s]Extractor Predicting: 38it [00:38,  1.60it/s]Extractor Predicting: 39it [00:38,  1.59it/s]Extractor Predicting: 40it [00:39,  1.59it/s]Extractor Predicting: 41it [00:40,  1.58it/s]Extractor Predicting: 42it [00:40,  1.58it/s]Extractor Predicting: 43it [00:41,  1.55it/s]Extractor Predicting: 44it [00:41,  1.54it/s]Extractor Predicting: 45it [00:42,  1.54it/s]Extractor Predicting: 46it [00:43,  1.54it/s]Extractor Predicting: 47it [00:43,  1.60it/s]Extractor Predicting: 48it [00:44,  1.58it/s]Extractor Predicting: 49it [00:45,  1.60it/s]Extractor Predicting: 50it [00:45,  1.56it/s]Extractor Predicting: 51it [00:46,  1.60it/s]Extractor Predicting: 52it [00:46,  1.63it/s]Extractor Predicting: 53it [00:47,  1.49it/s]Extractor Predicting: 54it [00:48,  1.50it/s]Extractor Predicting: 55it [00:49,  1.53it/s]Extractor Predicting: 56it [00:49,  1.55it/s]Extractor Predicting: 57it [00:50,  1.59it/s]Extractor Predicting: 58it [00:50,  1.60it/s]Extractor Predicting: 59it [00:51,  1.55it/s]Extractor Predicting: 60it [00:52,  1.54it/s]Extractor Predicting: 61it [00:52,  1.52it/s]Extractor Predicting: 62it [00:53,  1.54it/s]Extractor Predicting: 63it [00:54,  1.55it/s]Extractor Predicting: 64it [00:54,  1.55it/s]Extractor Predicting: 65it [00:55,  1.58it/s]Extractor Predicting: 66it [00:56,  1.59it/s]Extractor Predicting: 67it [00:56,  1.58it/s]Extractor Predicting: 68it [00:57,  1.61it/s]Extractor Predicting: 69it [00:57,  1.60it/s]Extractor Predicting: 70it [00:58,  1.60it/s]Extractor Predicting: 71it [00:59,  1.55it/s]Extractor Predicting: 72it [00:59,  1.57it/s]Extractor Predicting: 73it [01:00,  1.57it/s]Extractor Predicting: 74it [01:01,  1.60it/s]Extractor Predicting: 75it [01:01,  1.59it/s]Extractor Predicting: 76it [01:02,  1.60it/s]Extractor Predicting: 77it [01:02,  1.64it/s]Extractor Predicting: 78it [01:03,  1.60it/s]Extractor Predicting: 79it [01:04,  1.61it/s]Extractor Predicting: 80it [01:04,  1.57it/s]Extractor Predicting: 81it [01:05,  1.57it/s]Extractor Predicting: 82it [01:06,  1.58it/s]Extractor Predicting: 83it [01:06,  1.63it/s]Extractor Predicting: 84it [01:07,  1.59it/s]Extractor Predicting: 85it [01:08,  1.57it/s]Extractor Predicting: 86it [01:08,  1.59it/s]Extractor Predicting: 87it [01:09,  1.60it/s]Extractor Predicting: 88it [01:09,  1.62it/s]Extractor Predicting: 89it [01:10,  1.59it/s]Extractor Predicting: 90it [01:11,  1.63it/s]Extractor Predicting: 91it [01:11,  1.66it/s]Extractor Predicting: 92it [01:12,  1.67it/s]Extractor Predicting: 93it [01:12,  1.65it/s]Extractor Predicting: 94it [01:13,  1.70it/s]Extractor Predicting: 95it [01:13,  1.70it/s]Extractor Predicting: 96it [01:14,  1.67it/s]Extractor Predicting: 97it [01:15,  1.65it/s]Extractor Predicting: 98it [01:15,  1.62it/s]Extractor Predicting: 99it [01:16,  1.60it/s]Extractor Predicting: 100it [01:17,  1.58it/s]Extractor Predicting: 101it [01:17,  1.63it/s]Extractor Predicting: 102it [01:18,  1.69it/s]Extractor Predicting: 103it [01:18,  1.68it/s]Extractor Predicting: 104it [01:19,  1.70it/s]Extractor Predicting: 105it [01:20,  1.67it/s]Extractor Predicting: 106it [01:20,  1.63it/s]Extractor Predicting: 107it [01:21,  1.62it/s]Extractor Predicting: 108it [01:21,  1.63it/s]Extractor Predicting: 109it [01:22,  1.64it/s]Extractor Predicting: 110it [01:23,  1.63it/s]Extractor Predicting: 111it [01:23,  1.66it/s]Extractor Predicting: 112it [01:24,  1.67it/s]Extractor Predicting: 113it [01:24,  1.73it/s]Extractor Predicting: 114it [01:25,  1.73it/s]Extractor Predicting: 115it [01:26,  1.75it/s]Extractor Predicting: 116it [01:26,  1.71it/s]Extractor Predicting: 117it [01:27,  1.69it/s]Extractor Predicting: 118it [01:28,  1.54it/s]Extractor Predicting: 119it [01:28,  1.56it/s]Extractor Predicting: 120it [01:29,  1.57it/s]Extractor Predicting: 121it [01:29,  1.62it/s]Extractor Predicting: 122it [01:30,  1.63it/s]Extractor Predicting: 123it [01:31,  1.64it/s]Extractor Predicting: 124it [01:31,  1.62it/s]Extractor Predicting: 125it [01:32,  1.60it/s]Extractor Predicting: 126it [01:32,  1.56it/s]Extractor Predicting: 127it [01:33,  1.58it/s]Extractor Predicting: 128it [01:34,  1.65it/s]Extractor Predicting: 129it [01:34,  1.62it/s]Extractor Predicting: 130it [01:35,  1.67it/s]Extractor Predicting: 131it [01:35,  1.66it/s]Extractor Predicting: 132it [01:36,  1.57it/s]Extractor Predicting: 133it [01:37,  1.58it/s]Extractor Predicting: 134it [01:37,  1.56it/s]Extractor Predicting: 135it [01:38,  1.58it/s]Extractor Predicting: 136it [01:39,  1.61it/s]Extractor Predicting: 137it [01:39,  1.63it/s]Extractor Predicting: 138it [01:40,  1.63it/s]Extractor Predicting: 139it [01:41,  1.61it/s]Extractor Predicting: 140it [01:41,  1.64it/s]Extractor Predicting: 141it [01:42,  1.61it/s]Extractor Predicting: 142it [01:42,  1.58it/s]Extractor Predicting: 143it [01:43,  1.62it/s]Extractor Predicting: 144it [01:44,  1.58it/s]Extractor Predicting: 144it [01:44,  1.38it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3143544506816359,
  "recall": 0.11238532110091744,
  "score": 0.16557550158394935,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19834
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19934, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.77it/s]Extractor Predicting: 2it [00:01,  1.73it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.65it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.68it/s]Extractor Predicting: 10it [00:06,  1.63it/s]Extractor Predicting: 11it [00:06,  1.63it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:07,  1.64it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:12,  1.63it/s]Extractor Predicting: 22it [00:13,  1.68it/s]Extractor Predicting: 23it [00:13,  1.70it/s]Extractor Predicting: 24it [00:14,  1.56it/s]Extractor Predicting: 25it [00:15,  1.56it/s]Extractor Predicting: 26it [00:15,  1.61it/s]Extractor Predicting: 27it [00:16,  1.61it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:17,  1.62it/s]Extractor Predicting: 30it [00:18,  1.57it/s]Extractor Predicting: 31it [00:19,  1.65it/s]Extractor Predicting: 32it [00:19,  1.72it/s]Extractor Predicting: 33it [00:20,  1.75it/s]Extractor Predicting: 34it [00:20,  1.78it/s]Extractor Predicting: 35it [00:21,  1.81it/s]Extractor Predicting: 36it [00:21,  1.81it/s]Extractor Predicting: 37it [00:22,  1.81it/s]Extractor Predicting: 38it [00:22,  1.84it/s]Extractor Predicting: 39it [00:23,  1.88it/s]Extractor Predicting: 40it [00:23,  1.90it/s]Extractor Predicting: 41it [00:24,  1.87it/s]Extractor Predicting: 42it [00:24,  1.88it/s]Extractor Predicting: 43it [00:25,  1.84it/s]Extractor Predicting: 44it [00:25,  1.87it/s]Extractor Predicting: 45it [00:26,  1.87it/s]Extractor Predicting: 46it [00:27,  1.81it/s]Extractor Predicting: 47it [00:27,  1.85it/s]Extractor Predicting: 48it [00:28,  1.89it/s]Extractor Predicting: 49it [00:28,  1.92it/s]Extractor Predicting: 50it [00:29,  1.89it/s]Extractor Predicting: 51it [00:29,  1.92it/s]Extractor Predicting: 52it [00:30,  1.89it/s]Extractor Predicting: 53it [00:30,  1.89it/s]Extractor Predicting: 54it [00:31,  1.87it/s]Extractor Predicting: 55it [00:31,  1.83it/s]Extractor Predicting: 56it [00:32,  1.83it/s]Extractor Predicting: 57it [00:32,  1.81it/s]Extractor Predicting: 58it [00:33,  1.77it/s]Extractor Predicting: 59it [00:34,  1.73it/s]Extractor Predicting: 60it [00:34,  1.68it/s]Extractor Predicting: 61it [00:35,  1.61it/s]Extractor Predicting: 62it [00:36,  1.56it/s]Extractor Predicting: 63it [00:36,  1.56it/s]Extractor Predicting: 64it [00:37,  1.53it/s]Extractor Predicting: 65it [00:38,  1.57it/s]Extractor Predicting: 66it [00:38,  1.58it/s]Extractor Predicting: 67it [00:39,  1.59it/s]Extractor Predicting: 68it [00:40,  1.54it/s]Extractor Predicting: 69it [00:40,  1.52it/s]Extractor Predicting: 70it [00:41,  1.51it/s]Extractor Predicting: 71it [00:42,  1.49it/s]Extractor Predicting: 72it [00:42,  1.54it/s]Extractor Predicting: 73it [00:43,  1.51it/s]Extractor Predicting: 74it [00:43,  1.54it/s]Extractor Predicting: 75it [00:44,  1.56it/s]Extractor Predicting: 76it [00:45,  1.57it/s]Extractor Predicting: 77it [00:45,  1.56it/s]Extractor Predicting: 78it [00:46,  1.53it/s]Extractor Predicting: 79it [00:47,  1.51it/s]Extractor Predicting: 80it [00:47,  1.52it/s]Extractor Predicting: 81it [00:48,  1.52it/s]Extractor Predicting: 82it [00:49,  1.51it/s]Extractor Predicting: 83it [00:49,  1.51it/s]Extractor Predicting: 84it [00:50,  1.55it/s]Extractor Predicting: 85it [00:51,  1.53it/s]Extractor Predicting: 86it [00:51,  1.51it/s]Extractor Predicting: 87it [00:52,  1.51it/s]Extractor Predicting: 88it [00:53,  1.53it/s]Extractor Predicting: 89it [00:53,  1.53it/s]Extractor Predicting: 90it [00:54,  1.57it/s]Extractor Predicting: 91it [00:55,  1.54it/s]Extractor Predicting: 92it [00:55,  1.51it/s]Extractor Predicting: 93it [00:56,  1.55it/s]Extractor Predicting: 94it [00:57,  1.55it/s]Extractor Predicting: 95it [00:57,  1.59it/s]Extractor Predicting: 96it [00:58,  1.63it/s]Extractor Predicting: 97it [00:58,  1.63it/s]Extractor Predicting: 98it [00:59,  1.66it/s]Extractor Predicting: 99it [01:00,  1.63it/s]Extractor Predicting: 100it [01:00,  1.62it/s]Extractor Predicting: 101it [01:01,  1.69it/s]Extractor Predicting: 102it [01:01,  1.70it/s]Extractor Predicting: 103it [01:02,  1.66it/s]Extractor Predicting: 104it [01:03,  1.60it/s]Extractor Predicting: 105it [01:03,  1.60it/s]Extractor Predicting: 106it [01:04,  1.61it/s]Extractor Predicting: 107it [01:04,  1.61it/s]Extractor Predicting: 108it [01:05,  1.60it/s]Extractor Predicting: 109it [01:06,  1.59it/s]Extractor Predicting: 110it [01:06,  1.58it/s]Extractor Predicting: 111it [01:07,  1.60it/s]Extractor Predicting: 112it [01:08,  1.57it/s]Extractor Predicting: 113it [01:08,  1.60it/s]Extractor Predicting: 114it [01:09,  1.55it/s]Extractor Predicting: 115it [01:10,  1.56it/s]Extractor Predicting: 116it [01:10,  1.63it/s]Extractor Predicting: 117it [01:11,  1.62it/s]Extractor Predicting: 118it [01:11,  1.54it/s]Extractor Predicting: 119it [01:12,  1.61it/s]Extractor Predicting: 120it [01:13,  1.66it/s]Extractor Predicting: 121it [01:13,  1.73it/s]Extractor Predicting: 122it [01:14,  1.73it/s]Extractor Predicting: 123it [01:14,  1.76it/s]Extractor Predicting: 124it [01:15,  1.77it/s]Extractor Predicting: 125it [01:15,  1.79it/s]Extractor Predicting: 126it [01:16,  1.77it/s]Extractor Predicting: 127it [01:16,  1.76it/s]Extractor Predicting: 128it [01:17,  1.81it/s]Extractor Predicting: 129it [01:18,  1.80it/s]Extractor Predicting: 130it [01:18,  1.89it/s]Extractor Predicting: 131it [01:19,  1.90it/s]Extractor Predicting: 132it [01:19,  1.84it/s]Extractor Predicting: 133it [01:20,  1.78it/s]Extractor Predicting: 134it [01:20,  1.76it/s]Extractor Predicting: 135it [01:21,  1.79it/s]Extractor Predicting: 136it [01:21,  1.84it/s]Extractor Predicting: 137it [01:22,  1.84it/s]Extractor Predicting: 138it [01:22,  1.83it/s]Extractor Predicting: 139it [01:23,  1.79it/s]Extractor Predicting: 140it [01:24,  1.84it/s]Extractor Predicting: 141it [01:24,  1.82it/s]Extractor Predicting: 142it [01:25,  1.88it/s]Extractor Predicting: 143it [01:25,  1.85it/s]Extractor Predicting: 144it [01:26,  1.82it/s]Extractor Predicting: 145it [01:26,  1.78it/s]Extractor Predicting: 146it [01:27,  1.73it/s]Extractor Predicting: 147it [01:28,  1.70it/s]Extractor Predicting: 148it [01:28,  1.70it/s]Extractor Predicting: 149it [01:29,  1.66it/s]Extractor Predicting: 150it [01:29,  1.64it/s]Extractor Predicting: 151it [01:30,  1.66it/s]Extractor Predicting: 152it [01:31,  1.66it/s]Extractor Predicting: 153it [01:31,  1.63it/s]Extractor Predicting: 154it [01:32,  1.65it/s]Extractor Predicting: 155it [01:32,  1.67it/s]Extractor Predicting: 156it [01:33,  1.63it/s]Extractor Predicting: 157it [01:34,  1.65it/s]Extractor Predicting: 158it [01:34,  1.63it/s]Extractor Predicting: 159it [01:35,  1.60it/s]Extractor Predicting: 160it [01:36,  1.59it/s]Extractor Predicting: 161it [01:36,  1.59it/s]Extractor Predicting: 162it [01:37,  1.58it/s]Extractor Predicting: 163it [01:37,  1.61it/s]Extractor Predicting: 164it [01:38,  1.60it/s]Extractor Predicting: 165it [01:39,  1.59it/s]Extractor Predicting: 166it [01:39,  1.57it/s]Extractor Predicting: 167it [01:40,  1.60it/s]Extractor Predicting: 168it [01:41,  1.58it/s]Extractor Predicting: 169it [01:41,  1.61it/s]Extractor Predicting: 170it [01:42,  1.60it/s]Extractor Predicting: 171it [01:42,  1.62it/s]Extractor Predicting: 172it [01:43,  1.64it/s]Extractor Predicting: 173it [01:44,  1.61it/s]Extractor Predicting: 174it [01:44,  1.60it/s]Extractor Predicting: 175it [01:45,  1.62it/s]Extractor Predicting: 176it [01:46,  1.62it/s]Extractor Predicting: 177it [01:46,  1.61it/s]Extractor Predicting: 178it [01:47,  1.62it/s]Extractor Predicting: 179it [01:47,  1.61it/s]Extractor Predicting: 180it [01:48,  1.59it/s]Extractor Predicting: 181it [01:49,  1.58it/s]Extractor Predicting: 182it [01:49,  1.60it/s]Extractor Predicting: 183it [01:50,  1.36it/s]Extractor Predicting: 184it [01:51,  1.42it/s]Extractor Predicting: 185it [01:52,  1.47it/s]Extractor Predicting: 186it [01:52,  1.48it/s]Extractor Predicting: 187it [01:53,  1.54it/s]Extractor Predicting: 188it [01:53,  1.55it/s]Extractor Predicting: 189it [01:54,  1.59it/s]Extractor Predicting: 190it [01:55,  1.58it/s]Extractor Predicting: 191it [01:55,  1.61it/s]Extractor Predicting: 192it [01:56,  1.62it/s]Extractor Predicting: 193it [01:56,  1.67it/s]Extractor Predicting: 194it [01:57,  1.64it/s]Extractor Predicting: 195it [01:58,  1.65it/s]Extractor Predicting: 196it [01:58,  1.64it/s]Extractor Predicting: 197it [01:59,  1.64it/s]Extractor Predicting: 198it [02:00,  1.61it/s]Extractor Predicting: 199it [02:00,  1.62it/s]Extractor Predicting: 200it [02:01,  1.63it/s]Extractor Predicting: 201it [02:01,  1.67it/s]Extractor Predicting: 202it [02:02,  1.63it/s]Extractor Predicting: 203it [02:02,  1.68it/s]Extractor Predicting: 204it [02:03,  1.51it/s]Extractor Predicting: 205it [02:04,  1.53it/s]Extractor Predicting: 206it [02:04,  1.63it/s]Extractor Predicting: 207it [02:05,  1.65it/s]Extractor Predicting: 208it [02:06,  1.69it/s]Extractor Predicting: 209it [02:06,  1.67it/s]Extractor Predicting: 210it [02:07,  1.67it/s]Extractor Predicting: 211it [02:07,  1.66it/s]Extractor Predicting: 212it [02:08,  1.63it/s]Extractor Predicting: 213it [02:09,  1.64it/s]Extractor Predicting: 214it [02:09,  1.63it/s]Extractor Predicting: 215it [02:10,  1.61it/s]Extractor Predicting: 216it [02:11,  1.60it/s]Extractor Predicting: 217it [02:11,  1.55it/s]Extractor Predicting: 218it [02:12,  1.58it/s]Extractor Predicting: 219it [02:13,  1.58it/s]Extractor Predicting: 220it [02:13,  1.61it/s]Extractor Predicting: 221it [02:14,  1.60it/s]Extractor Predicting: 222it [02:14,  1.60it/s]Extractor Predicting: 223it [02:15,  1.63it/s]Extractor Predicting: 224it [02:16,  1.63it/s]Extractor Predicting: 225it [02:16,  1.63it/s]Extractor Predicting: 226it [02:17,  1.63it/s]Extractor Predicting: 227it [02:17,  1.60it/s]Extractor Predicting: 228it [02:18,  1.66it/s]Extractor Predicting: 229it [02:19,  1.63it/s]Extractor Predicting: 230it [02:19,  1.64it/s]Extractor Predicting: 231it [02:20,  1.58it/s]Extractor Predicting: 232it [02:21,  1.57it/s]Extractor Predicting: 233it [02:21,  1.56it/s]Extractor Predicting: 234it [02:22,  1.58it/s]Extractor Predicting: 235it [02:22,  1.59it/s]Extractor Predicting: 236it [02:23,  1.60it/s]Extractor Predicting: 237it [02:24,  1.53it/s]Extractor Predicting: 238it [02:24,  1.52it/s]Extractor Predicting: 239it [02:25,  1.51it/s]Extractor Predicting: 240it [02:26,  1.53it/s]Extractor Predicting: 241it [02:26,  1.52it/s]Extractor Predicting: 242it [02:27,  1.55it/s]Extractor Predicting: 243it [02:28,  1.54it/s]Extractor Predicting: 244it [02:28,  1.55it/s]Extractor Predicting: 245it [02:29,  1.55it/s]Extractor Predicting: 246it [02:30,  1.53it/s]Extractor Predicting: 247it [02:30,  1.49it/s]Extractor Predicting: 248it [02:31,  1.51it/s]Extractor Predicting: 249it [02:32,  1.56it/s]Extractor Predicting: 250it [02:32,  1.59it/s]Extractor Predicting: 251it [02:33,  1.59it/s]Extractor Predicting: 252it [02:33,  1.61it/s]Extractor Predicting: 253it [02:34,  1.61it/s]Extractor Predicting: 254it [02:35,  1.59it/s]Extractor Predicting: 255it [02:35,  1.59it/s]Extractor Predicting: 256it [02:36,  1.58it/s]Extractor Predicting: 257it [02:37,  1.59it/s]Extractor Predicting: 258it [02:37,  1.57it/s]Extractor Predicting: 259it [02:38,  1.62it/s]Extractor Predicting: 260it [02:38,  1.63it/s]Extractor Predicting: 261it [02:39,  1.66it/s]Extractor Predicting: 262it [02:40,  1.66it/s]Extractor Predicting: 263it [02:40,  1.63it/s]Extractor Predicting: 264it [02:41,  1.65it/s]Extractor Predicting: 265it [02:41,  1.65it/s]Extractor Predicting: 266it [02:42,  1.64it/s]Extractor Predicting: 267it [02:43,  1.67it/s]Extractor Predicting: 268it [02:43,  1.66it/s]Extractor Predicting: 269it [02:44,  1.64it/s]Extractor Predicting: 270it [02:44,  1.62it/s]Extractor Predicting: 271it [02:45,  1.66it/s]Extractor Predicting: 272it [02:46,  1.66it/s]Extractor Predicting: 273it [02:46,  1.67it/s]Extractor Predicting: 274it [02:47,  1.68it/s]Extractor Predicting: 275it [02:47,  1.69it/s]Extractor Predicting: 276it [02:48,  1.65it/s]Extractor Predicting: 277it [02:49,  1.66it/s]Extractor Predicting: 278it [02:49,  1.66it/s]Extractor Predicting: 279it [02:50,  1.65it/s]Extractor Predicting: 280it [02:51,  1.46it/s]Extractor Predicting: 281it [02:51,  1.55it/s]Extractor Predicting: 282it [02:52,  1.57it/s]Extractor Predicting: 283it [02:53,  1.56it/s]Extractor Predicting: 284it [02:53,  1.80it/s]Extractor Predicting: 284it [02:53,  1.64it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4002659574468085,
  "recall": 0.13275507203763598,
  "score": 0.19938176197836166,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1045
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1145, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:01,  1.51it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:02,  2.11it/s]Extractor Predicting: 5it [00:02,  1.81it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.38461538461538464,
  "recall": 0.050505050505050504,
  "score": 0.08928571428571427,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:18, 14.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:26<02:46, 12.81s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:40<02:44, 13.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:55<02:33, 13.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:08<02:15, 13.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:21<02:02, 13.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:35<01:50, 13.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:51<01:39, 14.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:05<01:26, 14.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:20<01:11, 14.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:33<00:55, 13.98s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:48<00:43, 14.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:01<00:28, 14.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:16<00:14, 14.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:28<00:00, 13.64s/it]Generating: 100%|██████████| 15/15 [03:28<00:00, 13.91s/it]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 512, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 563, 'raw': 704}
{'target': 600, 'success': 589, 'raw': 736}
{'target': 600, 'success': 607, 'raw': 768}
{'prompt': 'Relation : genre .', 'success_rate': 0.7903645833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9241071428571429, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8650568181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : participant in .', 'success_rate': 0.8369565217391305, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : participating team .', 'success_rate': 0.890625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 560, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8288043478260869, 'errors': {''}}
['Relation : country of citizenship . Context : Following his election in 2001 , his father , former Argentine Prime Minister Cristina Fernández de Kirchner , was born in Buenos Aires . Head Entity : Cristina Fernández de Kirchner , Tail Entity : Argentina .\n']
['Relation : country of citizenship . Context : Following his election in 2001 , his father , former Argentine Prime Minister Cristina Fernández de Kirchner , was born in Buenos Aires . Head Entity : Cristina Fernández de Kirchner , Tail Entity : Argentina .\n', 'Relation : country of citizenship . Context : He was born in Argentina where he was a citizen of Argentina . Head Entity : Argentine , Tail Entity : Argentina .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 431, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 535, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 590, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.80859375, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : father .', 'success_rate': 0.8315217391304348, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.8988095238095238, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9136904761904762, 'errors': {''}}
['Relation : located in the administrative territorial entity . Context : The city is located in the municipality of Saint-Martin ( French : Saint-Martin de Saint-Martin ) in Saint - Marie ( French : Saint-Métro ) , and is designated as a district by the French government . Head Entity : Saint-Martin de Saint-Martin de Saint-Marie , Tail Entity : Saint - Marie .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.890625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8707386363636364, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.9255952380952381, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/3_ext.jsonl'}}
estimate vocab size: 10296
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10396, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:02,  2.98s/it]Extractor Estimating: 2it [00:03,  1.62s/it]Extractor Estimating: 3it [00:04,  1.14s/it]Extractor Estimating: 4it [00:05,  1.15s/it]Extractor Estimating: 5it [00:05,  1.05it/s]Extractor Estimating: 6it [00:06,  1.25it/s]Extractor Estimating: 7it [00:07,  1.39it/s]Extractor Estimating: 8it [00:07,  1.39it/s]Extractor Estimating: 9it [00:08,  1.44it/s]Extractor Estimating: 10it [00:08,  1.53it/s]Extractor Estimating: 11it [00:10,  1.02it/s]Extractor Estimating: 12it [00:11,  1.07it/s]Extractor Estimating: 13it [00:12,  1.19it/s]Extractor Estimating: 14it [00:12,  1.35it/s]Extractor Estimating: 15it [00:13,  1.43it/s]Extractor Estimating: 16it [00:13,  1.47it/s]Extractor Estimating: 17it [00:14,  1.55it/s]Extractor Estimating: 18it [00:16,  1.18s/it]Extractor Estimating: 19it [00:17,  1.01s/it]Extractor Estimating: 20it [00:18,  1.03it/s]Extractor Estimating: 21it [00:18,  1.16it/s]Extractor Estimating: 22it [00:19,  1.29it/s]Extractor Estimating: 23it [00:20,  1.39it/s]Extractor Estimating: 24it [00:20,  1.43it/s]Extractor Estimating: 25it [00:22,  1.14it/s]Extractor Estimating: 26it [00:22,  1.24it/s]Extractor Estimating: 27it [00:23,  1.41it/s]Extractor Estimating: 28it [00:23,  1.57it/s]Extractor Estimating: 29it [00:24,  1.65it/s]Extractor Estimating: 30it [00:24,  1.57it/s]Extractor Estimating: 31it [00:25,  1.70it/s]Extractor Estimating: 32it [00:25,  1.77it/s]Extractor Estimating: 33it [00:26,  1.86it/s]Extractor Estimating: 34it [00:26,  1.95it/s]Extractor Estimating: 35it [00:27,  1.98it/s]Extractor Estimating: 36it [00:27,  1.89it/s]Extractor Estimating: 37it [00:28,  1.95it/s]Extractor Estimating: 38it [00:28,  1.97it/s]Extractor Estimating: 39it [00:29,  1.96it/s]Extractor Estimating: 40it [00:29,  1.99it/s]Extractor Estimating: 41it [00:30,  2.05it/s]Extractor Estimating: 42it [00:31,  1.60it/s]Extractor Estimating: 43it [00:31,  1.73it/s]Extractor Estimating: 44it [00:32,  1.76it/s]Extractor Estimating: 45it [00:32,  1.78it/s]Extractor Estimating: 46it [00:33,  1.83it/s]Extractor Estimating: 47it [00:34,  1.72it/s]Extractor Estimating: 48it [00:34,  1.79it/s]Extractor Estimating: 49it [00:34,  1.89it/s]Extractor Estimating: 50it [00:35,  1.79it/s]Extractor Estimating: 51it [00:36,  1.68it/s]Extractor Estimating: 52it [00:38,  1.04it/s]Extractor Estimating: 53it [00:38,  1.15it/s]Extractor Estimating: 54it [00:39,  1.25it/s]Extractor Estimating: 55it [00:40,  1.32it/s]Extractor Estimating: 56it [00:40,  1.45it/s]Extractor Estimating: 57it [00:41,  1.51it/s]Extractor Estimating: 58it [00:41,  1.50it/s]Extractor Estimating: 59it [00:42,  1.57it/s]Extractor Estimating: 60it [00:42,  1.64it/s]Extractor Estimating: 61it [00:43,  1.68it/s]Extractor Estimating: 62it [00:44,  1.70it/s]Extractor Estimating: 63it [00:44,  1.71it/s]Extractor Estimating: 64it [00:45,  1.64it/s]Extractor Estimating: 65it [00:45,  1.62it/s]Extractor Estimating: 66it [00:46,  1.65it/s]Extractor Estimating: 67it [00:47,  1.52it/s]Extractor Estimating: 68it [00:47,  1.56it/s]Extractor Estimating: 69it [00:48,  1.59it/s]Extractor Estimating: 70it [00:49,  1.59it/s]Extractor Estimating: 71it [00:49,  1.65it/s]Extractor Estimating: 72it [00:50,  1.72it/s]Extractor Estimating: 73it [00:50,  1.76it/s]Extractor Estimating: 74it [00:52,  1.02s/it]Extractor Estimating: 75it [00:53,  1.10it/s]Extractor Estimating: 76it [00:54,  1.25it/s]Extractor Estimating: 77it [00:54,  1.34it/s]Extractor Estimating: 78it [00:55,  1.39it/s]Extractor Estimating: 79it [00:55,  1.50it/s]Extractor Estimating: 80it [00:56,  1.56it/s]Extractor Estimating: 81it [00:56,  1.65it/s]Extractor Estimating: 82it [00:57,  1.75it/s]Extractor Estimating: 83it [00:58,  1.75it/s]Extractor Estimating: 84it [00:58,  1.69it/s]Extractor Estimating: 85it [00:59,  1.67it/s]Extractor Estimating: 86it [00:59,  1.67it/s]Extractor Estimating: 87it [01:00,  1.68it/s]Extractor Estimating: 88it [01:01,  1.68it/s]Extractor Estimating: 89it [01:01,  1.76it/s]Extractor Estimating: 90it [01:02,  1.73it/s]Extractor Estimating: 91it [01:02,  1.56it/s]Extractor Estimating: 92it [01:03,  1.57it/s]Extractor Estimating: 93it [01:04,  1.67it/s]Extractor Estimating: 94it [01:04,  1.72it/s]Extractor Estimating: 95it [01:05,  1.66it/s]Extractor Estimating: 96it [01:05,  1.69it/s]Extractor Estimating: 97it [01:06,  1.66it/s]Extractor Estimating: 98it [01:07,  1.66it/s]Extractor Estimating: 99it [01:07,  1.63it/s]Extractor Estimating: 100it [01:08,  1.61it/s]Extractor Estimating: 101it [01:08,  1.71it/s]Extractor Estimating: 102it [01:09,  1.68it/s]Extractor Estimating: 103it [01:10,  1.74it/s]Extractor Estimating: 104it [01:10,  1.74it/s]Extractor Estimating: 105it [01:11,  1.79it/s]Extractor Estimating: 106it [01:11,  1.81it/s]Extractor Estimating: 107it [01:12,  1.85it/s]Extractor Estimating: 108it [01:12,  1.84it/s]Extractor Estimating: 109it [01:13,  1.86it/s]Extractor Estimating: 110it [01:13,  1.88it/s]Extractor Estimating: 111it [01:14,  1.92it/s]Extractor Estimating: 112it [01:14,  1.93it/s]Extractor Estimating: 113it [01:15,  1.90it/s]Extractor Estimating: 114it [01:15,  1.75it/s]Extractor Estimating: 115it [01:16,  1.76it/s]Extractor Estimating: 116it [01:17,  1.74it/s]Extractor Estimating: 117it [01:17,  1.74it/s]Extractor Estimating: 118it [01:18,  1.76it/s]Extractor Estimating: 119it [01:18,  1.80it/s]Extractor Estimating: 120it [01:19,  1.70it/s]Extractor Estimating: 121it [01:20,  1.73it/s]Extractor Estimating: 122it [01:20,  1.76it/s]Extractor Estimating: 123it [01:21,  1.73it/s]Extractor Estimating: 124it [01:21,  1.82it/s]Extractor Estimating: 125it [01:22,  1.85it/s]Extractor Estimating: 126it [01:23,  1.46it/s]Extractor Estimating: 127it [01:23,  1.59it/s]Extractor Estimating: 128it [01:24,  1.68it/s]Extractor Estimating: 129it [01:24,  1.75it/s]Extractor Estimating: 130it [01:25,  1.80it/s]Extractor Estimating: 131it [01:26,  1.27it/s]Extractor Estimating: 132it [01:27,  1.41it/s]Extractor Estimating: 133it [01:27,  1.54it/s]Extractor Estimating: 134it [01:28,  1.61it/s]Extractor Estimating: 135it [01:28,  1.74it/s]Extractor Estimating: 136it [01:29,  1.38it/s]Extractor Estimating: 137it [01:30,  1.54it/s]Extractor Estimating: 138it [01:30,  1.65it/s]Extractor Estimating: 139it [01:31,  1.79it/s]Extractor Estimating: 140it [01:31,  1.81it/s]Extractor Estimating: 141it [01:32,  1.51it/s]Extractor Estimating: 142it [01:33,  1.65it/s]Extractor Estimating: 143it [01:33,  1.66it/s]Extractor Estimating: 144it [01:34,  1.68it/s]Extractor Estimating: 145it [01:34,  1.75it/s]Extractor Estimating: 146it [01:35,  1.54it/s]Extractor Estimating: 147it [01:36,  1.66it/s]Extractor Estimating: 148it [01:36,  1.79it/s]Extractor Estimating: 149it [01:37,  1.31it/s]Extractor Estimating: 150it [01:38,  1.45it/s]Extractor Estimating: 151it [01:38,  1.56it/s]Extractor Estimating: 152it [01:39,  1.67it/s]Extractor Estimating: 153it [01:40,  1.32it/s]Extractor Estimating: 154it [01:40,  1.46it/s]Extractor Estimating: 155it [01:41,  1.57it/s]Extractor Estimating: 156it [01:41,  1.73it/s]Extractor Estimating: 157it [01:42,  1.73it/s]Extractor Estimating: 158it [01:43,  1.60it/s]Extractor Estimating: 159it [01:43,  1.69it/s]Extractor Estimating: 160it [01:44,  1.76it/s]Extractor Estimating: 161it [01:44,  1.85it/s]Extractor Estimating: 162it [01:45,  1.83it/s]Extractor Estimating: 163it [01:45,  1.85it/s]Extractor Estimating: 164it [01:46,  1.48it/s]Extractor Estimating: 165it [01:47,  1.63it/s]Extractor Estimating: 166it [01:47,  1.72it/s]Extractor Estimating: 167it [01:48,  1.74it/s]Extractor Estimating: 168it [01:48,  1.79it/s]Extractor Estimating: 169it [01:49,  1.68it/s]Extractor Estimating: 170it [01:50,  1.79it/s]Extractor Estimating: 171it [01:50,  1.82it/s]Extractor Estimating: 172it [01:51,  1.83it/s]Extractor Estimating: 173it [01:51,  1.85it/s]Extractor Estimating: 174it [01:52,  1.84it/s]Extractor Estimating: 175it [01:52,  1.78it/s]Extractor Estimating: 176it [01:53,  1.83it/s]Extractor Estimating: 177it [01:53,  1.78it/s]Extractor Estimating: 178it [01:54,  1.75it/s]Extractor Estimating: 179it [01:55,  1.70it/s]Extractor Estimating: 180it [01:55,  1.74it/s]Extractor Estimating: 181it [01:56,  1.63it/s]Extractor Estimating: 182it [01:56,  1.62it/s]Extractor Estimating: 183it [01:57,  1.63it/s]Extractor Estimating: 184it [01:58,  1.55it/s]Extractor Estimating: 185it [01:59,  1.49it/s]Extractor Estimating: 186it [01:59,  1.49it/s]Extractor Estimating: 187it [02:00,  1.58it/s]Extractor Estimating: 188it [02:00,  1.61it/s]Extractor Estimating: 189it [02:01,  1.66it/s]Extractor Estimating: 190it [02:01,  1.67it/s]Extractor Estimating: 191it [02:02,  1.66it/s]Extractor Estimating: 192it [02:03,  1.45it/s]Extractor Estimating: 193it [02:04,  1.52it/s]Extractor Estimating: 194it [02:04,  1.53it/s]Extractor Estimating: 195it [02:05,  1.65it/s]Extractor Estimating: 196it [02:05,  1.69it/s]Extractor Estimating: 197it [02:06,  1.59it/s]Extractor Estimating: 198it [02:07,  1.63it/s]Extractor Estimating: 199it [02:07,  1.61it/s]Extractor Estimating: 200it [02:08,  1.63it/s]Extractor Estimating: 201it [02:08,  1.59it/s]Extractor Estimating: 202it [02:09,  1.62it/s]Extractor Estimating: 203it [02:10,  1.63it/s]Extractor Estimating: 204it [02:10,  1.66it/s]Extractor Estimating: 205it [02:11,  1.53it/s]Extractor Estimating: 206it [02:12,  1.58it/s]Extractor Estimating: 207it [02:12,  1.60it/s]Extractor Estimating: 208it [02:13,  1.61it/s]Extractor Estimating: 209it [02:13,  1.61it/s]Extractor Estimating: 210it [02:14,  1.34it/s]Extractor Estimating: 211it [02:15,  1.40it/s]Extractor Estimating: 212it [02:16,  1.47it/s]Extractor Estimating: 213it [02:16,  1.53it/s]Extractor Estimating: 214it [02:17,  1.57it/s]Extractor Estimating: 215it [02:18,  1.45it/s]Extractor Estimating: 216it [02:18,  1.52it/s]Extractor Estimating: 217it [02:19,  1.57it/s]Extractor Estimating: 218it [02:19,  1.59it/s]Extractor Estimating: 219it [02:20,  1.61it/s]Extractor Estimating: 220it [02:21,  1.38it/s]Extractor Estimating: 221it [02:22,  1.44it/s]Extractor Estimating: 222it [02:22,  1.56it/s]Extractor Estimating: 223it [02:23,  1.55it/s]Extractor Estimating: 224it [02:23,  1.57it/s]Extractor Estimating: 225it [02:24,  1.51it/s]Extractor Estimating: 226it [02:25,  1.55it/s]Extractor Estimating: 227it [02:25,  1.58it/s]Extractor Estimating: 228it [02:26,  1.61it/s]Extractor Estimating: 229it [02:27,  1.60it/s]Extractor Estimating: 230it [02:27,  1.63it/s]Extractor Estimating: 231it [02:28,  1.67it/s]Extractor Estimating: 232it [02:28,  1.69it/s]Extractor Estimating: 233it [02:29,  1.69it/s]Extractor Estimating: 234it [02:30,  1.68it/s]Extractor Estimating: 235it [02:30,  1.71it/s]Extractor Estimating: 236it [02:31,  1.59it/s]Extractor Estimating: 237it [02:32,  1.56it/s]Extractor Estimating: 238it [02:32,  1.60it/s]Extractor Estimating: 239it [02:33,  1.68it/s]Extractor Estimating: 240it [02:33,  1.73it/s]Extractor Estimating: 241it [02:34,  1.29it/s]Extractor Estimating: 242it [02:35,  1.43it/s]Extractor Estimating: 243it [02:35,  1.53it/s]Extractor Estimating: 244it [02:36,  1.58it/s]Extractor Estimating: 245it [02:37,  1.62it/s]Extractor Estimating: 246it [02:37,  1.54it/s]Extractor Estimating: 247it [02:38,  1.63it/s]Extractor Estimating: 248it [02:39,  1.33it/s]Extractor Estimating: 249it [02:39,  1.47it/s]Extractor Estimating: 250it [02:40,  1.52it/s]Extractor Estimating: 251it [02:41,  1.60it/s]Extractor Estimating: 252it [02:41,  1.62it/s]Extractor Estimating: 253it [02:42,  1.56it/s]Extractor Estimating: 254it [02:42,  1.66it/s]Extractor Estimating: 255it [02:43,  1.67it/s]Extractor Estimating: 256it [02:44,  1.74it/s]Extractor Estimating: 257it [02:44,  1.74it/s]Extractor Estimating: 258it [02:45,  1.76it/s]Extractor Estimating: 259it [02:46,  1.36it/s]Extractor Estimating: 260it [02:46,  1.42it/s]Extractor Estimating: 261it [02:47,  1.51it/s]Extractor Estimating: 262it [02:48,  1.54it/s]Extractor Estimating: 263it [02:48,  1.62it/s]Extractor Estimating: 264it [02:49,  1.52it/s]Extractor Estimating: 265it [02:50,  1.55it/s]Extractor Estimating: 266it [02:50,  1.53it/s]Extractor Estimating: 267it [02:51,  1.56it/s]Extractor Estimating: 268it [02:51,  1.62it/s]Extractor Estimating: 269it [02:52,  1.31it/s]Extractor Estimating: 270it [02:53,  1.40it/s]Extractor Estimating: 271it [02:54,  1.48it/s]Extractor Estimating: 272it [02:54,  1.44it/s]Extractor Estimating: 273it [02:55,  1.38it/s]Extractor Estimating: 274it [02:56,  1.46it/s]Extractor Estimating: 275it [02:56,  1.57it/s]Extractor Estimating: 276it [02:57,  1.63it/s]Extractor Estimating: 277it [02:57,  1.67it/s]Extractor Estimating: 278it [02:58,  1.72it/s]Extractor Estimating: 279it [02:59,  1.72it/s]Extractor Estimating: 280it [02:59,  1.75it/s]Extractor Estimating: 281it [03:00,  1.78it/s]Extractor Estimating: 282it [03:00,  1.85it/s]Extractor Estimating: 283it [03:01,  1.80it/s]Extractor Estimating: 284it [03:02,  1.39it/s]Extractor Estimating: 285it [03:02,  1.48it/s]Extractor Estimating: 286it [03:03,  1.60it/s]Extractor Estimating: 287it [03:03,  1.68it/s]Extractor Estimating: 288it [03:04,  1.71it/s]Extractor Estimating: 289it [03:05,  1.68it/s]Extractor Estimating: 290it [03:05,  1.75it/s]Extractor Estimating: 291it [03:06,  1.78it/s]Extractor Estimating: 292it [03:06,  1.76it/s]Extractor Estimating: 293it [03:07,  1.78it/s]Extractor Estimating: 294it [03:07,  1.77it/s]Extractor Estimating: 295it [03:08,  1.67it/s]Extractor Estimating: 296it [03:09,  1.69it/s]Extractor Estimating: 297it [03:09,  1.76it/s]Extractor Estimating: 298it [03:10,  1.81it/s]Extractor Estimating: 299it [03:10,  1.82it/s]Extractor Estimating: 300it [03:11,  1.82it/s]Extractor Estimating: 301it [03:11,  1.82it/s]Extractor Estimating: 302it [03:12,  1.76it/s]Extractor Estimating: 303it [03:13,  1.69it/s]Extractor Estimating: 304it [03:13,  1.73it/s]Extractor Estimating: 305it [03:14,  1.74it/s]Extractor Estimating: 306it [03:14,  1.70it/s]Extractor Estimating: 307it [03:15,  1.70it/s]Extractor Estimating: 308it [03:15,  1.73it/s]Extractor Estimating: 309it [03:16,  1.71it/s]Extractor Estimating: 310it [03:17,  1.76it/s]Extractor Estimating: 311it [03:17,  1.79it/s]Extractor Estimating: 312it [03:18,  1.81it/s]Extractor Estimating: 313it [03:18,  1.75it/s]Extractor Estimating: 314it [03:19,  1.70it/s]Extractor Estimating: 315it [03:19,  1.69it/s]Extractor Estimating: 316it [03:20,  1.69it/s]Extractor Estimating: 317it [03:21,  1.72it/s]Extractor Estimating: 318it [03:21,  1.78it/s]Extractor Estimating: 319it [03:22,  1.81it/s]Extractor Estimating: 320it [03:22,  1.81it/s]Extractor Estimating: 321it [03:23,  1.67it/s]Extractor Estimating: 322it [03:24,  1.67it/s]Extractor Estimating: 323it [03:24,  1.68it/s]Extractor Estimating: 324it [03:25,  1.75it/s]Extractor Estimating: 325it [03:25,  1.79it/s]Extractor Estimating: 326it [03:26,  1.84it/s]Extractor Estimating: 327it [03:26,  1.64it/s]Extractor Estimating: 328it [03:27,  1.63it/s]Extractor Estimating: 329it [03:28,  1.67it/s]Extractor Estimating: 330it [03:28,  1.68it/s]Extractor Estimating: 331it [03:29,  1.66it/s]Extractor Estimating: 332it [03:30,  1.58it/s]Extractor Estimating: 333it [03:30,  1.60it/s]Extractor Estimating: 334it [03:31,  1.62it/s]Extractor Estimating: 335it [03:31,  1.64it/s]Extractor Estimating: 336it [03:32,  1.68it/s]Extractor Estimating: 337it [03:33,  1.40it/s]Extractor Estimating: 338it [03:33,  1.49it/s]Extractor Estimating: 339it [03:34,  1.59it/s]Extractor Estimating: 340it [03:35,  1.62it/s]Extractor Estimating: 341it [03:35,  1.54it/s]Extractor Estimating: 342it [03:36,  1.45it/s]Extractor Estimating: 343it [03:37,  1.53it/s]Extractor Estimating: 344it [03:37,  1.60it/s]Extractor Estimating: 345it [03:38,  1.59it/s]Extractor Estimating: 346it [03:38,  1.67it/s]Extractor Estimating: 347it [03:39,  1.64it/s]Extractor Estimating: 348it [03:40,  1.73it/s]Extractor Estimating: 349it [03:40,  1.70it/s]Extractor Estimating: 350it [03:41,  1.74it/s]Extractor Estimating: 351it [03:41,  1.70it/s]Extractor Estimating: 352it [03:42,  1.69it/s]Extractor Estimating: 353it [03:43,  1.65it/s]Extractor Estimating: 354it [03:43,  1.68it/s]Extractor Estimating: 355it [03:44,  1.73it/s]Extractor Estimating: 356it [03:44,  1.64it/s]Extractor Estimating: 357it [03:45,  1.64it/s]Extractor Estimating: 358it [03:46,  1.64it/s]Extractor Estimating: 359it [03:46,  1.64it/s]Extractor Estimating: 360it [03:47,  1.66it/s]Extractor Estimating: 361it [03:47,  1.72it/s]Extractor Estimating: 362it [03:48,  1.77it/s]Extractor Estimating: 363it [03:48,  1.76it/s]Extractor Estimating: 364it [03:49,  1.72it/s]Extractor Estimating: 365it [03:50,  1.71it/s]Extractor Estimating: 366it [03:50,  1.61it/s]Extractor Estimating: 367it [03:51,  1.59it/s]Extractor Estimating: 368it [03:52,  1.57it/s]Extractor Estimating: 369it [03:52,  1.63it/s]Extractor Estimating: 370it [03:53,  1.65it/s]Extractor Estimating: 371it [03:53,  1.58it/s]Extractor Estimating: 372it [03:54,  1.60it/s]Extractor Estimating: 373it [03:55,  1.59it/s]Extractor Estimating: 374it [03:55,  1.64it/s]Extractor Estimating: 375it [03:56,  1.66it/s]Extractor Estimating: 375it [03:56,  1.59it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7495 mean pseudo reward: 0.9442429505909726
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
train vocab size: 19384
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19484, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=19484, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.312, loss:755.0166
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.954, loss:690.5071
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.952, loss:700.5336
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 0.961, loss:650.2090
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 0.964, loss:661.1411
>> valid entity prec:0.5806, rec:0.4934, f1:0.5334
>> valid relation prec:0.2354, rec:0.0729, f1:0.1113
>> valid relation with NER prec:0.2354, rec:0.0729, f1:0.1113
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.250, loss:688.6343
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 0.953, loss:639.0010
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 0.972, loss:643.4446
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 0.950, loss:655.1654
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 0.960, loss:615.5967
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5036, rec:0.5263, f1:0.5147
>> valid relation prec:0.1866, rec:0.0778, f1:0.1098
>> valid relation with NER prec:0.1866, rec:0.0778, f1:0.1098
g_step 1100, step 161, avg_time 2.184, loss:652.5343
g_step 1200, step 261, avg_time 0.932, loss:660.4571
g_step 1300, step 48, avg_time 0.954, loss:627.2309
g_step 1400, step 148, avg_time 0.949, loss:581.1294
g_step 1500, step 248, avg_time 0.957, loss:622.8545
>> valid entity prec:0.5581, rec:0.5241, f1:0.5406
>> valid relation prec:0.1941, rec:0.0680, f1:0.1007
>> valid relation with NER prec:0.1941, rec:0.0680, f1:0.1007
new max entity f1 on valid!
g_step 1600, step 35, avg_time 2.208, loss:573.9497
g_step 1700, step 135, avg_time 0.933, loss:544.4537
g_step 1800, step 235, avg_time 0.946, loss:590.5265
g_step 1900, step 22, avg_time 0.960, loss:558.4140
g_step 2000, step 122, avg_time 0.953, loss:532.1109
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5887, rec:0.4775, f1:0.5273
>> valid relation prec:0.2045, rec:0.0606, f1:0.0934
>> valid relation with NER prec:0.2045, rec:0.0606, f1:0.0934
g_step 2100, step 222, avg_time 2.194, loss:537.6550
g_step 2200, step 9, avg_time 0.941, loss:553.3417
g_step 2300, step 109, avg_time 0.951, loss:501.7215
g_step 2400, step 209, avg_time 0.966, loss:499.9341
g_step 2500, step 309, avg_time 0.956, loss:518.5398
>> valid entity prec:0.5543, rec:0.5747, f1:0.5643
>> valid relation prec:0.1974, rec:0.0795, f1:0.1134
>> valid relation with NER prec:0.1974, rec:0.0795, f1:0.1134
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 96, avg_time 2.254, loss:476.4592
g_step 2700, step 196, avg_time 0.963, loss:488.2172
g_step 2800, step 296, avg_time 0.968, loss:519.7418
g_step 2900, step 83, avg_time 0.959, loss:457.6019
g_step 3000, step 183, avg_time 0.954, loss:469.8853
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6044, rec:0.4806, f1:0.5355
>> valid relation prec:0.1707, rec:0.0686, f1:0.0979
>> valid relation with NER prec:0.1707, rec:0.0686, f1:0.0979
g_step 3100, step 283, avg_time 2.222, loss:482.9229
g_step 3200, step 70, avg_time 0.949, loss:441.3829
g_step 3300, step 170, avg_time 0.961, loss:447.1046
g_step 3400, step 270, avg_time 0.952, loss:442.4677
g_step 3500, step 57, avg_time 0.961, loss:425.8083
>> valid entity prec:0.5620, rec:0.5534, f1:0.5577
>> valid relation prec:0.1987, rec:0.0861, f1:0.1201
>> valid relation with NER prec:0.1987, rec:0.0861, f1:0.1201
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 157, avg_time 2.252, loss:404.9842
g_step 3700, step 257, avg_time 0.961, loss:443.5869
g_step 3800, step 44, avg_time 0.952, loss:406.8400
g_step 3900, step 144, avg_time 0.964, loss:413.1919
g_step 4000, step 244, avg_time 0.958, loss:408.2423
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5364, rec:0.5677, f1:0.5516
>> valid relation prec:0.1525, rec:0.0858, f1:0.1098
>> valid relation with NER prec:0.1525, rec:0.0858, f1:0.1098
g_step 4100, step 31, avg_time 2.229, loss:385.1898
g_step 4200, step 131, avg_time 0.966, loss:372.7362
g_step 4300, step 231, avg_time 0.955, loss:388.0968
g_step 4400, step 18, avg_time 0.951, loss:381.8519
g_step 4500, step 118, avg_time 0.945, loss:354.5033
>> valid entity prec:0.5534, rec:0.5104, f1:0.5310
>> valid relation prec:0.1467, rec:0.0706, f1:0.0953
>> valid relation with NER prec:0.1467, rec:0.0706, f1:0.0953
g_step 4600, step 218, avg_time 2.228, loss:357.9006
g_step 4700, step 5, avg_time 0.963, loss:388.1122
g_step 4800, step 105, avg_time 0.952, loss:346.8203
g_step 4900, step 205, avg_time 0.967, loss:357.3138
g_step 5000, step 305, avg_time 0.971, loss:356.3162
learning rate was adjusted to 0.0008
>> valid entity prec:0.5529, rec:0.5025, f1:0.5265
>> valid relation prec:0.1587, rec:0.0631, f1:0.0903
>> valid relation with NER prec:0.1587, rec:0.0631, f1:0.0903
g_step 5100, step 92, avg_time 2.193, loss:330.2934
g_step 5200, step 192, avg_time 0.961, loss:336.2210
g_step 5300, step 292, avg_time 0.963, loss:357.3984
g_step 5400, step 79, avg_time 0.957, loss:321.8409
g_step 5500, step 179, avg_time 0.967, loss:314.9282
>> valid entity prec:0.5371, rec:0.5075, f1:0.5219
>> valid relation prec:0.1382, rec:0.0692, f1:0.0922
>> valid relation with NER prec:0.1382, rec:0.0692, f1:0.0922
g_step 5600, step 279, avg_time 2.208, loss:339.4627
g_step 5700, step 66, avg_time 0.965, loss:326.8470
g_step 5800, step 166, avg_time 0.969, loss:304.6510
g_step 5900, step 266, avg_time 0.933, loss:318.0106
g_step 6000, step 53, avg_time 0.935, loss:285.5337
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5656, rec:0.4607, f1:0.5078
>> valid relation prec:0.1516, rec:0.0657, f1:0.0917
>> valid relation with NER prec:0.1516, rec:0.0657, f1:0.0917
g_step 6100, step 153, avg_time 2.187, loss:289.8745
g_step 6200, step 253, avg_time 0.950, loss:315.1148
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 21:23:10 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 21:23:10 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_21-23-10_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 21:23:13 - WARNING - datasets.builder -   Using custom data configuration default-ae4864a2213d385e
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-ae4864a2213d385e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 21:23:22,653 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:23:22,738 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:23:22,739 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:23:22,740 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:23:22,892 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:23:22,964 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:23:22,964 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:23:22,964 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:23:22,964 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:23:22,964 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:23:22,964 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 21:23:24,071 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:23:27,446 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 21:23:27,545 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-ae4864a2213d385e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 21:23:27 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x150c85f8e440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:01<00:10,  1.47s/ba] 25%|██▌       | 2/8 [00:01<00:04,  1.39ba/s] 38%|███▊      | 3/8 [00:01<00:02,  2.08ba/s] 50%|█████     | 4/8 [00:02<00:01,  2.73ba/s] 62%|██████▎   | 5/8 [00:02<00:00,  3.28ba/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.71ba/s] 88%|████████▊ | 7/8 [00:02<00:00,  4.06ba/s]100%|██████████| 8/8 [00:02<00:00,  4.99ba/s]100%|██████████| 8/8 [00:02<00:00,  2.91ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:02,  1.43ba/s] 50%|█████     | 2/4 [00:01<00:00,  2.05ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.73ba/s]100%|██████████| 4/4 [00:01<00:00,  3.77ba/s]100%|██████████| 4/4 [00:01<00:00,  2.92ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  5.02ba/s] 25%|██▌       | 2/8 [00:00<00:00,  6.21ba/s] 50%|█████     | 4/8 [00:00<00:00,  8.57ba/s] 75%|███████▌  | 6/8 [00:00<00:00,  9.62ba/s]100%|██████████| 8/8 [00:00<00:00, 11.13ba/s]100%|██████████| 8/8 [00:00<00:00,  9.62ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:02,  1.34ba/s] 50%|█████     | 2/4 [00:01<00:00,  2.15ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  3.33ba/s]100%|██████████| 4/4 [00:01<00:00,  3.44ba/s]
[INFO|trainer.py:414] 2023-08-28 21:23:39,023 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 21:23:39,205 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 21:23:39,205 >>   Num examples = 7499
[INFO|trainer.py:1149] 2023-08-28 21:23:39,205 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 21:23:39,205 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 21:23:39,205 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 21:23:39,205 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 21:23:39,205 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:02<22:17,  2.29s/it]  0%|          | 2/585 [00:02<12:27,  1.28s/it]  1%|          | 3/585 [00:03<09:22,  1.04it/s]  1%|          | 4/585 [00:03<07:30,  1.29it/s]  1%|          | 5/585 [00:04<05:48,  1.66it/s]  1%|          | 6/585 [00:04<05:10,  1.86it/s]  1%|          | 7/585 [00:05<05:22,  1.79it/s]  1%|▏         | 8/585 [00:05<05:32,  1.74it/s]  2%|▏         | 9/585 [00:06<04:50,  1.98it/s]  2%|▏         | 10/585 [00:06<04:11,  2.29it/s]  2%|▏         | 11/585 [00:06<03:44,  2.55it/s]  2%|▏         | 12/585 [00:07<03:26,  2.78it/s]  2%|▏         | 13/585 [00:07<03:13,  2.96it/s]  2%|▏         | 14/585 [00:07<03:04,  3.10it/s]  3%|▎         | 15/585 [00:07<02:57,  3.20it/s]  3%|▎         | 16/585 [00:08<02:53,  3.28it/s]  3%|▎         | 17/585 [00:08<02:49,  3.34it/s]  3%|▎         | 18/585 [00:08<03:07,  3.03it/s]  3%|▎         | 19/585 [00:09<02:59,  3.15it/s]  3%|▎         | 20/585 [00:09<02:54,  3.24it/s]  4%|▎         | 21/585 [00:09<02:50,  3.31it/s]  4%|▍         | 22/585 [00:10<02:47,  3.36it/s]  4%|▍         | 23/585 [00:10<02:44,  3.41it/s]  4%|▍         | 24/585 [00:10<02:43,  3.43it/s]  4%|▍         | 25/585 [00:10<02:42,  3.45it/s]  4%|▍         | 26/585 [00:11<02:41,  3.45it/s]  5%|▍         | 27/585 [00:11<02:41,  3.46it/s]  5%|▍         | 28/585 [00:11<02:40,  3.47it/s]  5%|▍         | 29/585 [00:12<04:18,  2.15it/s]  5%|▌         | 30/585 [00:12<03:48,  2.43it/s]  5%|▌         | 31/585 [00:13<03:27,  2.67it/s]  5%|▌         | 32/585 [00:13<03:12,  2.87it/s]  6%|▌         | 33/585 [00:13<03:01,  3.04it/s]  6%|▌         | 34/585 [00:14<02:54,  3.16it/s]  6%|▌         | 35/585 [00:14<02:49,  3.25it/s]  6%|▌         | 36/585 [00:14<02:45,  3.31it/s]  6%|▋         | 37/585 [00:14<02:43,  3.36it/s]  6%|▋         | 38/585 [00:15<03:22,  2.69it/s]  7%|▋         | 39/585 [00:15<03:09,  2.89it/s]  7%|▋         | 40/585 [00:16<02:59,  3.04it/s]  7%|▋         | 41/585 [00:16<02:52,  3.16it/s]  7%|▋         | 42/585 [00:16<02:46,  3.25it/s]  7%|▋         | 43/585 [00:16<02:43,  3.32it/s]  8%|▊         | 44/585 [00:17<02:40,  3.37it/s]  8%|▊         | 45/585 [00:17<02:38,  3.40it/s]  8%|▊         | 46/585 [00:17<02:37,  3.42it/s]  8%|▊         | 47/585 [00:18<02:36,  3.43it/s]  8%|▊         | 48/585 [00:18<04:10,  2.14it/s]  8%|▊         | 49/585 [00:19<03:41,  2.42it/s]  9%|▊         | 50/585 [00:19<03:20,  2.66it/s]  9%|▊         | 51/585 [00:19<03:06,  2.86it/s]  9%|▉         | 52/585 [00:20<02:56,  3.02it/s]  9%|▉         | 53/585 [00:20<02:49,  3.15it/s]  9%|▉         | 54/585 [00:20<02:43,  3.24it/s]  9%|▉         | 55/585 [00:20<02:40,  3.31it/s] 10%|▉         | 56/585 [00:21<02:37,  3.35it/s] 10%|▉         | 57/585 [00:21<02:40,  3.30it/s] 10%|▉         | 58/585 [00:21<02:37,  3.34it/s] 10%|█         | 59/585 [00:22<02:35,  3.38it/s] 10%|█         | 60/585 [00:22<02:34,  3.40it/s] 10%|█         | 61/585 [00:22<02:32,  3.43it/s] 11%|█         | 62/585 [00:23<02:31,  3.45it/s] 11%|█         | 63/585 [00:23<02:31,  3.45it/s] 11%|█         | 64/585 [00:23<02:31,  3.45it/s] 11%|█         | 65/585 [00:23<02:30,  3.46it/s] 11%|█▏        | 66/585 [00:24<02:29,  3.47it/s] 11%|█▏        | 67/585 [00:24<02:29,  3.47it/s] 12%|█▏        | 68/585 [00:24<02:40,  3.23it/s] 12%|█▏        | 69/585 [00:25<02:36,  3.29it/s] 12%|█▏        | 70/585 [00:25<02:34,  3.34it/s] 12%|█▏        | 71/585 [00:25<02:31,  3.39it/s] 12%|█▏        | 72/585 [00:26<03:16,  2.61it/s] 12%|█▏        | 73/585 [00:26<03:01,  2.82it/s] 13%|█▎        | 74/585 [00:26<02:51,  2.98it/s] 13%|█▎        | 75/585 [00:27<02:43,  3.12it/s] 13%|█▎        | 76/585 [00:27<02:37,  3.22it/s] 13%|█▎        | 77/585 [00:27<02:33,  3.30it/s] 13%|█▎        | 78/585 [00:28<02:58,  2.84it/s] 14%|█▎        | 79/585 [00:28<02:48,  3.00it/s] 14%|█▎        | 80/585 [00:28<02:41,  3.13it/s] 14%|█▍        | 81/585 [00:30<05:09,  1.63it/s] 14%|█▍        | 82/585 [00:30<04:19,  1.94it/s] 14%|█▍        | 83/585 [00:30<04:17,  1.95it/s] 14%|█▍        | 84/585 [00:31<05:39,  1.47it/s] 15%|█▍        | 85/585 [00:32<04:40,  1.78it/s] 15%|█▍        | 86/585 [00:32<03:58,  2.09it/s] 15%|█▍        | 87/585 [00:32<03:29,  2.37it/s] 15%|█▌        | 88/585 [00:33<03:26,  2.40it/s] 15%|█▌        | 89/585 [00:33<03:06,  2.65it/s] 15%|█▌        | 90/585 [00:33<02:52,  2.87it/s] 16%|█▌        | 91/585 [00:34<02:42,  3.03it/s] 16%|█▌        | 92/585 [00:34<02:36,  3.15it/s] 16%|█▌        | 93/585 [00:34<02:31,  3.25it/s] 16%|█▌        | 94/585 [00:34<02:27,  3.32it/s] 16%|█▌        | 95/585 [00:35<02:25,  3.37it/s] 16%|█▋        | 96/585 [00:35<02:23,  3.41it/s] 17%|█▋        | 97/585 [00:35<02:21,  3.44it/s] 17%|█▋        | 98/585 [00:36<02:20,  3.46it/s] 17%|█▋        | 99/585 [00:36<03:43,  2.18it/s] 17%|█▋        | 100/585 [00:37<03:17,  2.45it/s] 17%|█▋        | 101/585 [00:37<02:59,  2.69it/s] 17%|█▋        | 102/585 [00:37<02:46,  2.89it/s] 18%|█▊        | 103/585 [00:38<02:37,  3.05it/s] 18%|█▊        | 104/585 [00:38<02:31,  3.18it/s] 18%|█▊        | 105/585 [00:38<02:26,  3.27it/s] 18%|█▊        | 106/585 [00:38<02:23,  3.34it/s] 18%|█▊        | 107/585 [00:39<02:21,  3.38it/s] 18%|█▊        | 108/585 [00:39<02:44,  2.90it/s] 19%|█▊        | 109/585 [00:39<02:35,  3.05it/s] 19%|█▉        | 110/585 [00:40<02:29,  3.18it/s] 19%|█▉        | 111/585 [00:40<02:25,  3.27it/s] 19%|█▉        | 112/585 [00:40<02:21,  3.33it/s] 19%|█▉        | 113/585 [00:41<02:19,  3.38it/s] 19%|█▉        | 114/585 [00:41<02:17,  3.41it/s] 20%|█▉        | 115/585 [00:41<02:16,  3.43it/s] 20%|█▉        | 116/585 [00:41<02:15,  3.46it/s] 20%|██        | 117/585 [00:42<02:14,  3.47it/s][INFO|trainer.py:2140] 2023-08-28 21:24:21,801 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:24:21,802 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 21:24:21,802 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.75it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.60it/s][A
  4%|▍         | 18/436 [00:00<00:08, 47.40it/s][A
  5%|▌         | 23/436 [00:00<00:08, 46.70it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.36it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.04it/s][A
  9%|▊         | 38/436 [00:00<00:08, 45.63it/s][A
 10%|▉         | 43/436 [00:00<00:08, 44.98it/s][A
 11%|█         | 48/436 [00:01<00:08, 44.69it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 44.71it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 44.88it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 45.08it/s][A
 16%|█▌        | 68/436 [00:01<00:08, 45.23it/s][A
 17%|█▋        | 73/436 [00:01<00:08, 45.29it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 45.25it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 44.93it/s][A
 20%|██        | 88/436 [00:01<00:07, 44.55it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 44.46it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 44.49it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 44.54it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 44.80it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 45.11it/s][A
 27%|██▋       | 118/436 [00:02<00:07, 45.22it/s][A
 28%|██▊       | 123/436 [00:03<00:13, 23.57it/s][A
 29%|██▉       | 128/436 [00:03<00:11, 27.64it/s][A
 31%|███       | 133/436 [00:03<00:09, 31.27it/s][A
 32%|███▏      | 138/436 [00:03<00:08, 34.54it/s][A
 33%|███▎      | 143/436 [00:03<00:07, 37.34it/s][A
 34%|███▍      | 148/436 [00:03<00:07, 39.45it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 41.14it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 42.20it/s][A
 37%|███▋      | 163/436 [00:03<00:06, 42.60it/s][A
 39%|███▊      | 168/436 [00:04<00:06, 42.93it/s][A
 40%|███▉      | 173/436 [00:04<00:06, 43.52it/s][A
 41%|████      | 178/436 [00:04<00:05, 43.77it/s][A
 42%|████▏     | 183/436 [00:04<00:05, 44.18it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 44.61it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 44.79it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.06it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.09it/s][A
 48%|████▊     | 208/436 [00:04<00:05, 44.83it/s][A
 49%|████▉     | 213/436 [00:05<00:05, 44.52it/s][A
 50%|█████     | 218/436 [00:05<00:04, 44.55it/s][A
 51%|█████     | 223/436 [00:05<00:04, 44.69it/s][A
 52%|█████▏    | 228/436 [00:05<00:04, 44.90it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.05it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.23it/s][A
 56%|█████▌    | 243/436 [00:06<00:08, 22.28it/s][A
 57%|█████▋    | 248/436 [00:06<00:07, 26.31it/s][A
 58%|█████▊    | 253/436 [00:06<00:06, 30.15it/s][A
 59%|█████▉    | 258/436 [00:06<00:05, 33.51it/s][A
 60%|██████    | 263/436 [00:06<00:04, 36.41it/s][A
 61%|██████▏   | 268/436 [00:06<00:04, 38.76it/s][A
 63%|██████▎   | 273/436 [00:06<00:04, 40.54it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 41.81it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 42.25it/s][A
 66%|██████▌   | 288/436 [00:07<00:03, 42.73it/s][A
 67%|██████▋   | 293/436 [00:07<00:03, 43.24it/s][A
 68%|██████▊   | 298/436 [00:07<00:03, 43.75it/s][A
 69%|██████▉   | 303/436 [00:07<00:03, 44.23it/s][A
 71%|███████   | 308/436 [00:07<00:02, 44.65it/s][A
 72%|███████▏  | 313/436 [00:07<00:02, 44.95it/s][A
 73%|███████▎  | 318/436 [00:07<00:02, 45.13it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 44.86it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 44.57it/s][A
 76%|███████▋  | 333/436 [00:08<00:02, 44.35it/s][A
 78%|███████▊  | 338/436 [00:08<00:02, 44.37it/s][A
 79%|███████▊  | 343/436 [00:08<00:02, 44.52it/s][A
 80%|███████▉  | 348/436 [00:08<00:01, 44.69it/s][A
 81%|████████  | 353/436 [00:08<00:01, 44.93it/s][A
 82%|████████▏ | 358/436 [00:08<00:01, 45.06it/s][A
 83%|████████▎ | 363/436 [00:08<00:01, 40.65it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 42.12it/s][A
 86%|████████▌ | 373/436 [00:09<00:01, 43.01it/s][A
 87%|████████▋ | 378/436 [00:09<00:01, 43.53it/s][A
 88%|████████▊ | 383/436 [00:09<00:01, 43.92it/s][A
 89%|████████▉ | 388/436 [00:09<00:01, 43.96it/s][A
 90%|█████████ | 393/436 [00:09<00:00, 44.40it/s][A
 91%|█████████▏| 398/436 [00:09<00:00, 44.81it/s][A
 92%|█████████▏| 403/436 [00:09<00:00, 44.47it/s][A
 94%|█████████▎| 408/436 [00:09<00:00, 44.55it/s][A
 95%|█████████▍| 413/436 [00:09<00:00, 44.67it/s][A
 96%|█████████▌| 418/436 [00:10<00:00, 44.88it/s][A
 97%|█████████▋| 423/436 [00:10<00:00, 44.89it/s][A
 98%|█████████▊| 428/436 [00:10<00:00, 44.86it/s][A
 99%|█████████▉| 433/436 [00:10<00:00, 44.85it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:53<02:14,  3.47it/s]
100%|██████████| 436/436 [00:10<00:00, 44.85it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:24:33,177 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 21:24:34,620 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:24:43,786 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:24:44,242 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:24:44,323 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:27<1:47:58, 13.87s/it] 20%|██        | 119/585 [01:28<1:16:22,  9.83s/it] 21%|██        | 120/585 [01:28<54:01,  6.97s/it]   21%|██        | 121/585 [01:28<38:24,  4.97s/it] 21%|██        | 122/585 [01:29<27:51,  3.61s/it] 21%|██        | 123/585 [01:29<20:36,  2.68s/it] 21%|██        | 124/585 [01:30<15:13,  1.98s/it] 21%|██▏       | 125/585 [01:30<11:18,  1.47s/it] 22%|██▏       | 126/585 [01:30<08:38,  1.13s/it] 22%|██▏       | 127/585 [01:30<06:41,  1.14it/s] 22%|██▏       | 128/585 [01:31<06:03,  1.26it/s] 22%|██▏       | 129/585 [01:31<04:53,  1.55it/s] 22%|██▏       | 130/585 [01:32<04:05,  1.86it/s] 22%|██▏       | 131/585 [01:32<03:30,  2.15it/s] 23%|██▎       | 132/585 [01:32<03:06,  2.42it/s] 23%|██▎       | 133/585 [01:33<02:49,  2.66it/s] 23%|██▎       | 134/585 [01:33<02:38,  2.85it/s] 23%|██▎       | 135/585 [01:33<02:29,  3.01it/s] 23%|██▎       | 136/585 [01:33<02:23,  3.12it/s] 23%|██▎       | 137/585 [01:34<02:19,  3.20it/s] 24%|██▎       | 138/585 [01:34<02:39,  2.80it/s] 24%|██▍       | 139/585 [01:34<02:30,  2.96it/s] 24%|██▍       | 140/585 [01:35<02:23,  3.09it/s] 24%|██▍       | 141/585 [01:35<02:19,  3.18it/s] 24%|██▍       | 142/585 [01:35<02:16,  3.25it/s] 24%|██▍       | 143/585 [01:36<02:13,  3.30it/s] 25%|██▍       | 144/585 [01:36<02:12,  3.34it/s] 25%|██▍       | 145/585 [01:37<03:32,  2.08it/s] 25%|██▍       | 146/585 [01:37<03:31,  2.08it/s] 25%|██▌       | 147/585 [01:38<03:05,  2.36it/s] 25%|██▌       | 148/585 [01:38<02:48,  2.60it/s] 25%|██▌       | 149/585 [01:38<02:35,  2.80it/s] 26%|██▌       | 150/585 [01:38<02:26,  2.96it/s] 26%|██▌       | 151/585 [01:39<02:20,  3.09it/s] 26%|██▌       | 152/585 [01:39<02:16,  3.18it/s] 26%|██▌       | 153/585 [01:39<02:12,  3.25it/s] 26%|██▋       | 154/585 [01:40<02:10,  3.30it/s] 26%|██▋       | 155/585 [01:40<02:08,  3.34it/s] 27%|██▋       | 156/585 [01:40<02:17,  3.11it/s] 27%|██▋       | 157/585 [01:41<02:13,  3.20it/s] 27%|██▋       | 158/585 [01:41<02:10,  3.27it/s] 27%|██▋       | 159/585 [01:41<02:08,  3.31it/s] 27%|██▋       | 160/585 [01:41<02:06,  3.35it/s] 28%|██▊       | 161/585 [01:42<02:05,  3.37it/s] 28%|██▊       | 162/585 [01:42<02:04,  3.39it/s] 28%|██▊       | 163/585 [01:42<02:04,  3.40it/s] 28%|██▊       | 164/585 [01:43<02:03,  3.41it/s] 28%|██▊       | 165/585 [01:43<02:02,  3.41it/s] 28%|██▊       | 166/585 [01:43<02:02,  3.42it/s] 29%|██▊       | 167/585 [01:44<02:30,  2.77it/s] 29%|██▊       | 168/585 [01:44<02:22,  2.94it/s] 29%|██▉       | 169/585 [01:44<02:15,  3.07it/s] 29%|██▉       | 170/585 [01:45<02:11,  3.17it/s] 29%|██▉       | 171/585 [01:45<02:07,  3.24it/s] 29%|██▉       | 172/585 [01:45<02:05,  3.29it/s] 30%|██▉       | 173/585 [01:45<02:04,  3.32it/s] 30%|██▉       | 174/585 [01:46<02:02,  3.34it/s] 30%|██▉       | 175/585 [01:46<02:01,  3.37it/s] 30%|███       | 176/585 [01:46<02:00,  3.38it/s] 30%|███       | 177/585 [01:47<02:22,  2.86it/s] 30%|███       | 178/585 [01:47<02:15,  3.01it/s] 31%|███       | 179/585 [01:47<02:10,  3.12it/s] 31%|███       | 180/585 [01:48<02:06,  3.21it/s] 31%|███       | 181/585 [01:48<02:03,  3.27it/s] 31%|███       | 182/585 [01:48<02:01,  3.31it/s] 31%|███▏      | 183/585 [01:49<02:00,  3.35it/s] 31%|███▏      | 184/585 [01:49<01:59,  3.37it/s] 32%|███▏      | 185/585 [01:49<01:58,  3.38it/s] 32%|███▏      | 186/585 [01:49<01:57,  3.39it/s] 32%|███▏      | 187/585 [01:50<02:02,  3.25it/s] 32%|███▏      | 188/585 [01:50<02:00,  3.30it/s] 32%|███▏      | 189/585 [01:50<01:58,  3.34it/s] 32%|███▏      | 190/585 [01:51<01:57,  3.36it/s] 33%|███▎      | 191/585 [01:51<01:56,  3.38it/s] 33%|███▎      | 192/585 [01:51<01:55,  3.39it/s] 33%|███▎      | 193/585 [01:52<01:55,  3.40it/s] 33%|███▎      | 194/585 [01:52<01:54,  3.41it/s] 33%|███▎      | 195/585 [01:52<01:54,  3.40it/s] 34%|███▎      | 196/585 [01:52<01:54,  3.41it/s] 34%|███▎      | 197/585 [01:53<01:53,  3.42it/s] 34%|███▍      | 198/585 [01:53<02:20,  2.75it/s] 34%|███▍      | 199/585 [01:54<02:12,  2.92it/s] 34%|███▍      | 200/585 [01:54<02:05,  3.06it/s] 34%|███▍      | 201/585 [01:54<02:01,  3.16it/s] 35%|███▍      | 202/585 [01:54<01:58,  3.23it/s] 35%|███▍      | 203/585 [01:55<01:56,  3.29it/s] 35%|███▍      | 204/585 [01:55<01:54,  3.33it/s] 35%|███▌      | 205/585 [01:55<01:53,  3.35it/s] 35%|███▌      | 206/585 [01:56<01:52,  3.37it/s] 35%|███▌      | 207/585 [01:56<01:51,  3.38it/s] 36%|███▌      | 208/585 [01:56<01:56,  3.23it/s] 36%|███▌      | 209/585 [01:57<01:54,  3.28it/s] 36%|███▌      | 210/585 [01:57<01:53,  3.32it/s] 36%|███▌      | 211/585 [01:57<01:51,  3.35it/s] 36%|███▌      | 212/585 [01:57<01:50,  3.37it/s] 36%|███▋      | 213/585 [01:58<01:49,  3.39it/s] 37%|███▋      | 214/585 [01:58<01:49,  3.40it/s] 37%|███▋      | 215/585 [01:58<01:48,  3.40it/s] 37%|███▋      | 216/585 [01:59<02:32,  2.41it/s] 37%|███▋      | 217/585 [02:00<02:43,  2.24it/s] 37%|███▋      | 218/585 [02:00<02:26,  2.50it/s] 37%|███▋      | 219/585 [02:00<02:14,  2.72it/s] 38%|███▊      | 220/585 [02:00<02:06,  2.89it/s] 38%|███▊      | 221/585 [02:01<02:00,  3.03it/s] 38%|███▊      | 222/585 [02:01<01:55,  3.13it/s] 38%|███▊      | 223/585 [02:01<01:52,  3.22it/s] 38%|███▊      | 224/585 [02:02<01:50,  3.27it/s] 38%|███▊      | 225/585 [02:02<01:48,  3.31it/s] 39%|███▊      | 226/585 [02:02<01:47,  3.34it/s] 39%|███▉      | 227/585 [02:03<02:00,  2.98it/s] 39%|███▉      | 228/585 [02:03<01:55,  3.10it/s] 39%|███▉      | 229/585 [02:03<01:51,  3.18it/s] 39%|███▉      | 230/585 [02:03<01:49,  3.25it/s] 39%|███▉      | 231/585 [02:04<01:47,  3.29it/s] 40%|███▉      | 232/585 [02:04<01:46,  3.32it/s] 40%|███▉      | 233/585 [02:04<01:45,  3.34it/s] 40%|████      | 234/585 [02:05<01:44,  3.37it/s][INFO|trainer.py:2140] 2023-08-28 21:25:44,372 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:25:44,372 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 21:25:44,372 >>   Batch size = 8
{'eval_loss': 1.0065582990646362, 'eval_runtime': 10.4593, 'eval_samples_per_second': 333.482, 'eval_steps_per_second': 41.685, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.84it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.06it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.90it/s][A
  5%|▌         | 22/436 [00:00<00:08, 46.07it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.44it/s][A
  7%|▋         | 32/436 [00:00<00:08, 45.13it/s][A
  8%|▊         | 37/436 [00:00<00:08, 45.01it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.80it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.87it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 45.00it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 45.06it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.82it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.78it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.69it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.54it/s][A
 19%|█▉        | 82/436 [00:01<00:11, 30.74it/s][A
 20%|█▉        | 87/436 [00:02<00:10, 34.22it/s][A
 21%|██        | 92/436 [00:02<00:09, 36.99it/s][A
 22%|██▏       | 97/436 [00:02<00:08, 39.13it/s][A
 23%|██▎       | 102/436 [00:02<00:08, 40.69it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 41.88it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 43.09it/s][A
 27%|██▋       | 117/436 [00:03<00:07, 43.73it/s][A
 28%|██▊       | 122/436 [00:03<00:15, 20.40it/s][A
 29%|██▉       | 127/436 [00:03<00:12, 24.45it/s][A
 30%|███       | 132/436 [00:03<00:10, 28.38it/s][A
 31%|███▏      | 137/436 [00:03<00:09, 31.99it/s][A
 33%|███▎      | 142/436 [00:03<00:08, 35.11it/s][A
 34%|███▎      | 147/436 [00:03<00:07, 37.63it/s][A
 35%|███▍      | 152/436 [00:03<00:07, 39.53it/s][A
 36%|███▌      | 157/436 [00:04<00:06, 40.84it/s][A
 37%|███▋      | 162/436 [00:04<00:06, 41.68it/s][A
 38%|███▊      | 167/436 [00:04<00:06, 42.37it/s][A
 39%|███▉      | 172/436 [00:04<00:06, 42.88it/s][A
 41%|████      | 177/436 [00:04<00:05, 43.63it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.15it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.58it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.76it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.86it/s][A
 46%|████▋     | 202/436 [00:05<00:05, 44.57it/s][A
 47%|████▋     | 207/436 [00:05<00:05, 44.24it/s][A
 49%|████▊     | 212/436 [00:05<00:05, 44.16it/s][A
 50%|████▉     | 217/436 [00:05<00:04, 44.22it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.43it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.68it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.75it/s][A
 54%|█████▍    | 237/436 [00:05<00:05, 38.97it/s][A
 56%|█████▌    | 242/436 [00:06<00:04, 40.67it/s][A
 57%|█████▋    | 247/436 [00:06<00:04, 41.91it/s][A
 58%|█████▊    | 252/436 [00:06<00:04, 42.92it/s][A
 59%|█████▉    | 257/436 [00:06<00:04, 43.55it/s][A
 60%|██████    | 262/436 [00:06<00:03, 44.16it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.52it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.65it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.24it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.08it/s][A
 66%|██████▌   | 287/436 [00:07<00:03, 44.20it/s][A
 67%|██████▋   | 292/436 [00:07<00:03, 44.57it/s][A
 68%|██████▊   | 297/436 [00:07<00:03, 44.67it/s][A
 69%|██████▉   | 302/436 [00:07<00:02, 44.94it/s][A
 70%|███████   | 307/436 [00:07<00:02, 45.03it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 45.10it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.84it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.55it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.28it/s][A
 76%|███████▌  | 332/436 [00:08<00:02, 44.33it/s][A
 77%|███████▋  | 337/436 [00:08<00:02, 44.54it/s][A
 78%|███████▊  | 342/436 [00:08<00:02, 44.79it/s][A
 80%|███████▉  | 347/436 [00:08<00:01, 44.64it/s][A
 81%|████████  | 352/436 [00:08<00:01, 45.16it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.99it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.93it/s][A
 84%|████████▍ | 367/436 [00:09<00:01, 44.78it/s][A
 85%|████████▌ | 372/436 [00:09<00:02, 26.19it/s][A
 86%|████████▋ | 377/436 [00:09<00:01, 29.95it/s][A
 88%|████████▊ | 382/436 [00:09<00:01, 33.44it/s][A
 89%|████████▉ | 387/436 [00:09<00:01, 36.33it/s][A
 90%|████████▉ | 392/436 [00:09<00:01, 38.52it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 40.42it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 41.75it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 42.52it/s][A
 94%|█████████▍| 412/436 [00:10<00:00, 42.89it/s][A
 96%|█████████▌| 417/436 [00:10<00:00, 43.15it/s][A
 97%|█████████▋| 422/436 [00:10<00:00, 43.40it/s][A
 98%|█████████▊| 427/436 [00:10<00:00, 44.16it/s][A
 99%|█████████▉| 432/436 [00:10<00:00, 44.41it/s][A                                                 
                                                 [A 40%|████      | 234/585 [02:15<01:44,  3.37it/s]
100%|██████████| 436/436 [00:10<00:00, 44.41it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:25:55,603 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 21:25:56,072 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:26:07,185 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:26:08,298 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:26:08,956 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:59<1:36:53, 16.61s/it] 40%|████      | 236/585 [03:00<1:08:12, 11.73s/it] 41%|████      | 237/585 [03:00<48:06,  8.30s/it]   41%|████      | 238/585 [03:00<34:05,  5.89s/it] 41%|████      | 239/585 [03:01<24:17,  4.21s/it] 41%|████      | 240/585 [03:01<17:27,  3.04s/it] 41%|████      | 241/585 [03:01<12:41,  2.21s/it] 41%|████▏     | 242/585 [03:01<09:21,  1.64s/it] 42%|████▏     | 243/585 [03:02<07:01,  1.23s/it] 42%|████▏     | 244/585 [03:02<05:23,  1.05it/s] 42%|████▏     | 245/585 [03:02<04:15,  1.33it/s] 42%|████▏     | 246/585 [03:03<03:27,  1.64it/s] 42%|████▏     | 247/585 [03:03<03:20,  1.69it/s] 42%|████▏     | 248/585 [03:03<02:49,  1.99it/s] 43%|████▎     | 249/585 [03:04<02:26,  2.29it/s] 43%|████▎     | 250/585 [03:04<02:11,  2.55it/s] 43%|████▎     | 251/585 [03:04<02:00,  2.77it/s] 43%|████▎     | 252/585 [03:05<01:52,  2.95it/s] 43%|████▎     | 253/585 [03:05<01:47,  3.10it/s] 43%|████▎     | 254/585 [03:05<01:43,  3.21it/s] 44%|████▎     | 255/585 [03:05<01:40,  3.29it/s] 44%|████▍     | 256/585 [03:06<01:38,  3.34it/s] 44%|████▍     | 257/585 [03:06<01:41,  3.23it/s] 44%|████▍     | 258/585 [03:07<02:55,  1.86it/s] 44%|████▍     | 259/585 [03:07<02:30,  2.16it/s] 44%|████▍     | 260/585 [03:08<02:13,  2.44it/s] 45%|████▍     | 261/585 [03:08<02:00,  2.68it/s] 45%|████▍     | 262/585 [03:08<01:52,  2.88it/s] 45%|████▍     | 263/585 [03:08<01:45,  3.05it/s] 45%|████▌     | 264/585 [03:09<01:41,  3.16it/s] 45%|████▌     | 265/585 [03:09<01:38,  3.26it/s] 45%|████▌     | 266/585 [03:09<01:40,  3.18it/s] 46%|████▌     | 267/585 [03:10<01:37,  3.27it/s] 46%|████▌     | 268/585 [03:10<01:35,  3.33it/s] 46%|████▌     | 269/585 [03:10<01:33,  3.37it/s] 46%|████▌     | 270/585 [03:11<01:32,  3.40it/s] 46%|████▋     | 271/585 [03:11<01:31,  3.43it/s] 46%|████▋     | 272/585 [03:11<01:31,  3.43it/s] 47%|████▋     | 273/585 [03:11<01:30,  3.45it/s] 47%|████▋     | 274/585 [03:12<01:29,  3.46it/s] 47%|████▋     | 275/585 [03:12<01:29,  3.46it/s] 47%|████▋     | 276/585 [03:12<01:29,  3.47it/s] 47%|████▋     | 277/585 [03:13<01:37,  3.16it/s] 48%|████▊     | 278/585 [03:13<01:34,  3.25it/s] 48%|████▊     | 279/585 [03:13<01:32,  3.32it/s] 48%|████▊     | 280/585 [03:14<01:30,  3.37it/s] 48%|████▊     | 281/585 [03:14<01:29,  3.40it/s] 48%|████▊     | 282/585 [03:14<01:28,  3.43it/s] 48%|████▊     | 283/585 [03:14<01:27,  3.44it/s] 49%|████▊     | 284/585 [03:15<01:27,  3.45it/s] 49%|████▊     | 285/585 [03:15<01:26,  3.46it/s] 49%|████▉     | 286/585 [03:15<01:26,  3.46it/s] 49%|████▉     | 287/585 [03:16<01:26,  3.46it/s] 49%|████▉     | 288/585 [03:16<02:01,  2.44it/s] 49%|████▉     | 289/585 [03:17<01:50,  2.68it/s] 50%|████▉     | 290/585 [03:17<01:42,  2.88it/s] 50%|████▉     | 291/585 [03:17<01:36,  3.04it/s] 50%|████▉     | 292/585 [03:17<01:33,  3.15it/s] 50%|█████     | 293/585 [03:18<01:30,  3.24it/s] 50%|█████     | 294/585 [03:18<01:27,  3.31it/s] 50%|█████     | 295/585 [03:18<01:26,  3.35it/s] 51%|█████     | 296/585 [03:19<01:25,  3.38it/s] 51%|█████     | 297/585 [03:19<01:24,  3.40it/s] 51%|█████     | 298/585 [03:19<01:35,  3.00it/s] 51%|█████     | 299/585 [03:20<01:31,  3.13it/s] 51%|█████▏    | 300/585 [03:20<01:28,  3.23it/s] 51%|█████▏    | 301/585 [03:20<01:26,  3.29it/s] 52%|█████▏    | 302/585 [03:20<01:24,  3.34it/s] 52%|█████▏    | 303/585 [03:21<01:23,  3.38it/s] 52%|█████▏    | 304/585 [03:21<01:22,  3.41it/s] 52%|█████▏    | 305/585 [03:21<01:21,  3.43it/s] 52%|█████▏    | 306/585 [03:22<01:20,  3.44it/s] 52%|█████▏    | 307/585 [03:22<01:20,  3.45it/s] 53%|█████▎    | 308/585 [03:22<01:22,  3.34it/s] 53%|█████▎    | 309/585 [03:22<01:21,  3.38it/s] 53%|█████▎    | 310/585 [03:23<01:20,  3.41it/s] 53%|█████▎    | 311/585 [03:23<01:19,  3.43it/s] 53%|█████▎    | 312/585 [03:23<01:19,  3.44it/s] 54%|█████▎    | 313/585 [03:24<01:18,  3.45it/s] 54%|█████▎    | 314/585 [03:24<01:18,  3.45it/s] 54%|█████▍    | 315/585 [03:24<01:17,  3.46it/s] 54%|█████▍    | 316/585 [03:24<01:17,  3.46it/s] 54%|█████▍    | 317/585 [03:25<01:17,  3.47it/s] 54%|█████▍    | 318/585 [03:25<01:17,  3.46it/s] 55%|█████▍    | 319/585 [03:26<01:32,  2.88it/s] 55%|█████▍    | 320/585 [03:26<01:27,  3.04it/s] 55%|█████▍    | 321/585 [03:26<01:23,  3.15it/s] 55%|█████▌    | 322/585 [03:26<01:21,  3.23it/s] 55%|█████▌    | 323/585 [03:27<01:19,  3.30it/s] 55%|█████▌    | 324/585 [03:27<01:17,  3.35it/s] 56%|█████▌    | 325/585 [03:27<01:16,  3.38it/s] 56%|█████▌    | 326/585 [03:28<01:22,  3.14it/s] 56%|█████▌    | 327/585 [03:28<01:19,  3.23it/s] 56%|█████▌    | 328/585 [03:28<01:17,  3.30it/s] 56%|█████▌    | 329/585 [03:29<01:40,  2.55it/s] 56%|█████▋    | 330/585 [03:29<01:31,  2.77it/s] 57%|█████▋    | 331/585 [03:29<01:26,  2.95it/s] 57%|█████▋    | 332/585 [03:30<01:21,  3.09it/s] 57%|█████▋    | 333/585 [03:30<01:19,  3.19it/s] 57%|█████▋    | 334/585 [03:30<01:16,  3.27it/s] 57%|█████▋    | 335/585 [03:31<01:15,  3.33it/s] 57%|█████▋    | 336/585 [03:31<01:47,  2.32it/s] 58%|█████▊    | 337/585 [03:32<01:44,  2.37it/s] 58%|█████▊    | 338/585 [03:32<01:34,  2.62it/s] 58%|█████▊    | 339/585 [03:32<01:28,  2.78it/s] 58%|█████▊    | 340/585 [03:33<01:23,  2.95it/s] 58%|█████▊    | 341/585 [03:33<01:18,  3.09it/s] 58%|█████▊    | 342/585 [03:33<01:15,  3.20it/s] 59%|█████▊    | 343/585 [03:33<01:14,  3.27it/s] 59%|█████▉    | 344/585 [03:34<01:12,  3.33it/s] 59%|█████▉    | 345/585 [03:34<01:11,  3.36it/s] 59%|█████▉    | 346/585 [03:34<01:10,  3.39it/s] 59%|█████▉    | 347/585 [03:35<01:25,  2.79it/s] 59%|█████▉    | 348/585 [03:35<01:20,  2.96it/s] 60%|█████▉    | 349/585 [03:35<01:16,  3.10it/s] 60%|█████▉    | 350/585 [03:36<01:13,  3.19it/s] 60%|██████    | 351/585 [03:36<01:11,  3.27it/s][INFO|trainer.py:2140] 2023-08-28 21:27:15,700 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:27:15,700 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 21:27:15,700 >>   Batch size = 8
{'eval_loss': 1.019442081451416, 'eval_runtime': 10.657, 'eval_samples_per_second': 327.297, 'eval_steps_per_second': 40.912, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.96it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.82it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.22it/s][A
  5%|▌         | 22/436 [00:00<00:08, 46.21it/s][A
  6%|▌         | 27/436 [00:00<00:08, 45.50it/s][A
  7%|▋         | 32/436 [00:00<00:15, 26.87it/s][A
  8%|▊         | 37/436 [00:01<00:12, 30.89it/s][A
 10%|▉         | 42/436 [00:01<00:11, 34.21it/s][A
 11%|█         | 47/436 [00:01<00:10, 37.10it/s][A
 12%|█▏        | 52/436 [00:01<00:09, 39.34it/s][A
 13%|█▎        | 57/436 [00:01<00:09, 41.08it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 42.21it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 43.06it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 43.14it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 43.31it/s][A
 19%|█▉        | 82/436 [00:02<00:08, 43.68it/s][A
 20%|█▉        | 87/436 [00:02<00:07, 44.06it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.49it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.73it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.80it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.95it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.95it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.56it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.49it/s][A
 29%|██▉       | 127/436 [00:03<00:06, 44.45it/s][A
 30%|███       | 132/436 [00:03<00:06, 44.64it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.75it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.82it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.98it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 45.11it/s][A
 36%|███▌      | 157/436 [00:03<00:07, 39.35it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 41.06it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 42.29it/s][A
 39%|███▉      | 172/436 [00:04<00:06, 43.14it/s][A
 41%|████      | 177/436 [00:04<00:05, 43.81it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.21it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.49it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.70it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.23it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 43.99it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.24it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.56it/s][A
 50%|████▉     | 217/436 [00:05<00:04, 44.63it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.82it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.87it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 45.03it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.97it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.64it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.51it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.45it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.62it/s][A
 60%|██████    | 262/436 [00:06<00:03, 44.72it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.93it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 45.07it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 45.15it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.89it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.70it/s][A
 67%|██████▋   | 292/436 [00:06<00:05, 28.33it/s][A
 68%|██████▊   | 297/436 [00:07<00:04, 31.94it/s][A
 69%|██████▉   | 302/436 [00:07<00:03, 35.14it/s][A
 70%|███████   | 307/436 [00:07<00:03, 37.67it/s][A
 72%|███████▏  | 312/436 [00:07<00:03, 39.68it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 41.13it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 42.28it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 43.03it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 43.11it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 43.33it/s][A
 78%|███████▊  | 342/436 [00:08<00:02, 43.66it/s][A
 80%|███████▉  | 347/436 [00:08<00:02, 44.01it/s][A
 81%|████████  | 352/436 [00:08<00:01, 44.40it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.71it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.92it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.99it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.88it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.18it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.32it/s][A
 89%|████████▉ | 387/436 [00:09<00:01, 44.40it/s][A
 90%|████████▉ | 392/436 [00:09<00:00, 44.47it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 44.72it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.86it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 45.09it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 45.13it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 36.81it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 38.97it/s][A
 98%|█████████▊| 427/436 [00:10<00:00, 40.59it/s][A
 99%|█████████▉| 432/436 [00:10<00:00, 41.90it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [03:46<01:11,  3.27it/s]
100%|██████████| 436/436 [00:10<00:00, 41.90it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:27:27,243 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 21:27:28,007 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:27:49,192 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:27:49,896 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:27:50,706 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [04:46<1:21:57, 21.10s/it] 60%|██████    | 353/585 [04:46<57:33, 14.88s/it]   61%|██████    | 354/585 [04:46<40:26, 10.51s/it] 61%|██████    | 355/585 [04:47<28:31,  7.44s/it] 61%|██████    | 356/585 [04:47<20:13,  5.30s/it] 61%|██████    | 357/585 [04:47<14:25,  3.79s/it] 61%|██████    | 358/585 [04:47<10:22,  2.74s/it] 61%|██████▏   | 359/585 [04:48<07:33,  2.01s/it] 62%|██████▏   | 360/585 [04:48<05:35,  1.49s/it] 62%|██████▏   | 361/585 [04:48<04:13,  1.13s/it] 62%|██████▏   | 362/585 [04:49<03:16,  1.14it/s] 62%|██████▏   | 363/585 [04:49<02:36,  1.42it/s] 62%|██████▏   | 364/585 [04:50<02:35,  1.43it/s] 62%|██████▏   | 365/585 [04:50<02:07,  1.73it/s] 63%|██████▎   | 366/585 [04:50<01:47,  2.03it/s] 63%|██████▎   | 367/585 [04:50<01:34,  2.31it/s] 63%|██████▎   | 368/585 [04:51<01:24,  2.57it/s] 63%|██████▎   | 369/585 [04:51<01:17,  2.77it/s] 63%|██████▎   | 370/585 [04:51<01:12,  2.95it/s] 63%|██████▎   | 371/585 [04:52<01:09,  3.08it/s] 64%|██████▎   | 372/585 [04:52<01:06,  3.18it/s] 64%|██████▍   | 373/585 [04:52<01:16,  2.78it/s] 64%|██████▍   | 374/585 [04:53<01:11,  2.95it/s] 64%|██████▍   | 375/585 [04:53<01:08,  3.08it/s] 64%|██████▍   | 376/585 [04:53<01:05,  3.17it/s] 64%|██████▍   | 377/585 [04:54<01:04,  3.25it/s] 65%|██████▍   | 378/585 [04:54<01:02,  3.30it/s] 65%|██████▍   | 379/585 [04:54<01:01,  3.34it/s] 65%|██████▍   | 380/585 [04:54<01:01,  3.36it/s] 65%|██████▌   | 381/585 [04:55<01:00,  3.38it/s] 65%|██████▌   | 382/585 [04:55<00:59,  3.40it/s] 65%|██████▌   | 383/585 [04:56<01:26,  2.33it/s] 66%|██████▌   | 384/585 [04:56<01:17,  2.58it/s] 66%|██████▌   | 385/585 [04:56<01:11,  2.78it/s] 66%|██████▌   | 386/585 [04:57<01:07,  2.95it/s] 66%|██████▌   | 387/585 [04:57<01:04,  3.07it/s] 66%|██████▋   | 388/585 [04:57<01:02,  3.17it/s] 66%|██████▋   | 389/585 [04:57<01:00,  3.24it/s] 67%|██████▋   | 390/585 [04:58<00:59,  3.29it/s] 67%|██████▋   | 391/585 [04:58<00:58,  3.34it/s] 67%|██████▋   | 392/585 [04:58<01:00,  3.19it/s] 67%|██████▋   | 393/585 [04:59<00:58,  3.26it/s] 67%|██████▋   | 394/585 [04:59<00:57,  3.31it/s] 68%|██████▊   | 395/585 [04:59<00:56,  3.36it/s] 68%|██████▊   | 396/585 [05:00<00:55,  3.38it/s] 68%|██████▊   | 397/585 [05:00<00:55,  3.39it/s] 68%|██████▊   | 398/585 [05:00<00:55,  3.40it/s] 68%|██████▊   | 399/585 [05:00<00:54,  3.40it/s] 68%|██████▊   | 400/585 [05:01<00:54,  3.41it/s] 69%|██████▊   | 401/585 [05:01<00:53,  3.42it/s] 69%|██████▊   | 402/585 [05:01<00:53,  3.42it/s] 69%|██████▉   | 403/585 [05:02<00:55,  3.30it/s] 69%|██████▉   | 404/585 [05:02<00:54,  3.33it/s] 69%|██████▉   | 405/585 [05:02<00:53,  3.36it/s] 69%|██████▉   | 406/585 [05:03<00:52,  3.38it/s] 70%|██████▉   | 407/585 [05:03<00:52,  3.40it/s] 70%|██████▉   | 408/585 [05:03<00:52,  3.40it/s] 70%|██████▉   | 409/585 [05:03<00:51,  3.42it/s] 70%|███████   | 410/585 [05:04<00:51,  3.42it/s] 70%|███████   | 411/585 [05:04<00:50,  3.43it/s] 70%|███████   | 412/585 [05:04<00:50,  3.43it/s] 71%|███████   | 413/585 [05:05<00:50,  3.42it/s] 71%|███████   | 414/585 [05:06<01:32,  1.85it/s] 71%|███████   | 415/585 [05:06<01:19,  2.15it/s] 71%|███████   | 416/585 [05:06<01:09,  2.41it/s] 71%|███████▏  | 417/585 [05:07<01:03,  2.65it/s] 71%|███████▏  | 418/585 [05:07<00:58,  2.84it/s] 72%|███████▏  | 419/585 [05:07<00:55,  2.99it/s] 72%|███████▏  | 420/585 [05:07<00:53,  3.11it/s] 72%|███████▏  | 421/585 [05:08<00:51,  3.20it/s] 72%|███████▏  | 422/585 [05:08<00:58,  2.77it/s] 72%|███████▏  | 423/585 [05:09<00:55,  2.94it/s] 72%|███████▏  | 424/585 [05:09<00:52,  3.07it/s] 73%|███████▎  | 425/585 [05:10<01:27,  1.82it/s] 73%|███████▎  | 426/585 [05:10<01:14,  2.12it/s] 73%|███████▎  | 427/585 [05:10<01:05,  2.40it/s] 73%|███████▎  | 428/585 [05:11<00:59,  2.64it/s] 73%|███████▎  | 429/585 [05:11<00:54,  2.85it/s] 74%|███████▎  | 430/585 [05:11<00:51,  3.01it/s] 74%|███████▎  | 431/585 [05:12<00:49,  3.13it/s] 74%|███████▍  | 432/585 [05:12<00:47,  3.23it/s] 74%|███████▍  | 433/585 [05:13<01:22,  1.84it/s] 74%|███████▍  | 434/585 [05:13<01:10,  2.15it/s] 74%|███████▍  | 435/585 [05:14<01:01,  2.42it/s] 75%|███████▍  | 436/585 [05:14<00:55,  2.66it/s] 75%|███████▍  | 437/585 [05:14<00:51,  2.87it/s] 75%|███████▍  | 438/585 [05:14<00:48,  3.03it/s] 75%|███████▌  | 439/585 [05:15<01:17,  1.88it/s] 75%|███████▌  | 440/585 [05:16<01:08,  2.12it/s] 75%|███████▌  | 441/585 [05:16<00:59,  2.40it/s] 76%|███████▌  | 442/585 [05:16<00:53,  2.65it/s] 76%|███████▌  | 443/585 [05:17<00:49,  2.85it/s] 76%|███████▌  | 444/585 [05:17<00:46,  3.02it/s] 76%|███████▌  | 445/585 [05:17<00:44,  3.15it/s] 76%|███████▌  | 446/585 [05:17<00:42,  3.24it/s] 76%|███████▋  | 447/585 [05:18<00:41,  3.30it/s] 77%|███████▋  | 448/585 [05:18<00:40,  3.36it/s] 77%|███████▋  | 449/585 [05:18<00:39,  3.40it/s] 77%|███████▋  | 450/585 [05:19<00:39,  3.43it/s] 77%|███████▋  | 451/585 [05:19<00:49,  2.71it/s] 77%|███████▋  | 452/585 [05:19<00:45,  2.90it/s] 77%|███████▋  | 453/585 [05:20<00:43,  3.06it/s] 78%|███████▊  | 454/585 [05:20<00:41,  3.18it/s] 78%|███████▊  | 455/585 [05:20<00:39,  3.27it/s] 78%|███████▊  | 456/585 [05:21<00:38,  3.33it/s] 78%|███████▊  | 457/585 [05:21<00:37,  3.37it/s] 78%|███████▊  | 458/585 [05:21<00:37,  3.40it/s] 78%|███████▊  | 459/585 [05:21<00:36,  3.42it/s] 79%|███████▊  | 460/585 [05:22<00:36,  3.45it/s] 79%|███████▉  | 461/585 [05:22<00:45,  2.71it/s] 79%|███████▉  | 462/585 [05:23<00:42,  2.90it/s] 79%|███████▉  | 463/585 [05:23<00:39,  3.06it/s] 79%|███████▉  | 464/585 [05:23<00:38,  3.18it/s] 79%|███████▉  | 465/585 [05:23<00:36,  3.27it/s] 80%|███████▉  | 466/585 [05:24<00:35,  3.33it/s] 80%|███████▉  | 467/585 [05:24<00:35,  3.37it/s] 80%|████████  | 468/585 [05:24<00:34,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 21:29:04,039 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:29:04,039 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 21:29:04,039 >>   Batch size = 8
{'eval_loss': 1.0301934480667114, 'eval_runtime': 10.3078, 'eval_samples_per_second': 338.384, 'eval_steps_per_second': 42.298, 'epoch': 3.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.07it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.66it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.83it/s][A
  5%|▌         | 22/436 [00:00<00:08, 46.12it/s][A
  6%|▌         | 27/436 [00:00<00:16, 24.46it/s][A
  7%|▋         | 32/436 [00:00<00:13, 28.87it/s][A
  8%|▊         | 37/436 [00:01<00:12, 32.70it/s][A
 10%|▉         | 42/436 [00:01<00:10, 35.85it/s][A
 11%|█         | 47/436 [00:01<00:10, 38.33it/s][A
 12%|█▏        | 52/436 [00:01<00:09, 40.32it/s][A
 13%|█▎        | 57/436 [00:01<00:09, 41.75it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 42.73it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 42.88it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 43.26it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 43.70it/s][A
 19%|█▉        | 82/436 [00:02<00:08, 44.22it/s][A
 20%|█▉        | 87/436 [00:02<00:07, 44.43it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.58it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.98it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 45.17it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.98it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.62it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.57it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.60it/s][A
 29%|██▉       | 127/436 [00:03<00:06, 44.73it/s][A
 30%|███       | 132/436 [00:03<00:06, 44.67it/s][A
 31%|███▏      | 137/436 [00:03<00:12, 23.34it/s][A
 32%|███▏      | 141/436 [00:03<00:11, 24.97it/s][A
 33%|███▎      | 146/436 [00:03<00:09, 29.09it/s][A
 35%|███▍      | 151/436 [00:03<00:08, 32.64it/s][A
 36%|███▌      | 156/436 [00:04<00:07, 35.76it/s][A
 37%|███▋      | 161/436 [00:04<00:07, 38.30it/s][A
 38%|███▊      | 166/436 [00:04<00:06, 40.25it/s][A
 39%|███▉      | 171/436 [00:04<00:06, 41.74it/s][A
 40%|████      | 176/436 [00:04<00:06, 42.50it/s][A
 42%|████▏     | 181/436 [00:04<00:05, 42.81it/s][A
 43%|████▎     | 186/436 [00:04<00:05, 43.27it/s][A
 44%|████▍     | 191/436 [00:04<00:05, 43.67it/s][A
 45%|████▍     | 196/436 [00:04<00:05, 44.18it/s][A
 46%|████▌     | 201/436 [00:05<00:05, 44.48it/s][A
 47%|████▋     | 206/436 [00:05<00:05, 44.78it/s][A
 48%|████▊     | 211/436 [00:05<00:04, 45.01it/s][A
 50%|████▉     | 216/436 [00:05<00:04, 45.10it/s][A
 51%|█████     | 221/436 [00:05<00:04, 44.98it/s][A
 52%|█████▏    | 226/436 [00:05<00:04, 44.58it/s][A
 53%|█████▎    | 231/436 [00:05<00:04, 44.63it/s][A
 54%|█████▍    | 236/436 [00:05<00:04, 44.58it/s][A
 55%|█████▌    | 241/436 [00:05<00:04, 44.75it/s][A
 56%|█████▋    | 246/436 [00:06<00:04, 44.86it/s][A
 58%|█████▊    | 251/436 [00:06<00:04, 45.00it/s][A
 59%|█████▊    | 256/436 [00:06<00:03, 45.18it/s][A
 60%|█████▉    | 261/436 [00:06<00:03, 45.14it/s][A
 61%|██████    | 266/436 [00:06<00:03, 45.06it/s][A
 62%|██████▏   | 271/436 [00:06<00:03, 44.70it/s][A
 63%|██████▎   | 276/436 [00:07<00:06, 25.83it/s][A
 64%|██████▍   | 281/436 [00:07<00:05, 29.70it/s][A
 66%|██████▌   | 286/436 [00:07<00:04, 33.17it/s][A
 67%|██████▋   | 291/436 [00:07<00:04, 36.07it/s][A
 68%|██████▊   | 296/436 [00:07<00:03, 38.49it/s][A
 69%|██████▉   | 301/436 [00:07<00:03, 40.34it/s][A
 70%|███████   | 306/436 [00:07<00:03, 41.70it/s][A
 71%|███████▏  | 311/436 [00:07<00:02, 42.73it/s][A
 72%|███████▏  | 316/436 [00:07<00:02, 42.95it/s][A
 74%|███████▎  | 321/436 [00:08<00:02, 43.28it/s][A
 75%|███████▍  | 326/436 [00:08<00:02, 43.65it/s][A
 76%|███████▌  | 331/436 [00:08<00:02, 44.15it/s][A
 77%|███████▋  | 336/436 [00:08<00:02, 44.47it/s][A
 78%|███████▊  | 341/436 [00:08<00:02, 44.79it/s][A
 79%|███████▉  | 346/436 [00:08<00:02, 44.98it/s][A
 81%|████████  | 351/436 [00:08<00:01, 45.17it/s][A
 82%|████████▏ | 356/436 [00:08<00:01, 44.79it/s][A
 83%|████████▎ | 361/436 [00:08<00:01, 44.50it/s][A
 84%|████████▍ | 366/436 [00:09<00:01, 44.56it/s][A
 85%|████████▌ | 371/436 [00:09<00:01, 44.50it/s][A
 86%|████████▌ | 376/436 [00:09<00:01, 44.61it/s][A
 87%|████████▋ | 381/436 [00:09<00:01, 44.84it/s][A
 89%|████████▊ | 386/436 [00:09<00:01, 45.06it/s][A
 90%|████████▉ | 391/436 [00:09<00:00, 45.17it/s][A
 91%|█████████ | 396/436 [00:09<00:00, 45.22it/s][A
 92%|█████████▏| 401/436 [00:10<00:01, 26.56it/s][A
 93%|█████████▎| 406/436 [00:10<00:00, 30.36it/s][A
 94%|█████████▍| 411/436 [00:10<00:00, 33.70it/s][A
 95%|█████████▌| 416/436 [00:10<00:00, 36.45it/s][A
 97%|█████████▋| 421/436 [00:10<00:00, 38.71it/s][A
 98%|█████████▊| 426/436 [00:10<00:00, 40.42it/s][A
 99%|█████████▉| 431/436 [00:10<00:00, 41.81it/s][A
100%|██████████| 436/436 [00:10<00:00, 42.63it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [05:37<00:34,  3.41it/s]
100%|██████████| 436/436 [00:12<00:00, 42.63it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:29:17,895 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 21:29:19,986 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:29:37,053 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:29:37,703 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:29:38,753 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [06:24<34:54, 18.06s/it] 80%|████████  | 470/585 [06:24<24:32, 12.81s/it] 81%|████████  | 471/585 [06:25<17:11,  9.05s/it] 81%|████████  | 472/585 [06:25<12:05,  6.42s/it] 81%|████████  | 473/585 [06:25<08:33,  4.58s/it] 81%|████████  | 474/585 [06:26<06:05,  3.29s/it] 81%|████████  | 475/585 [06:26<04:23,  2.39s/it] 81%|████████▏ | 476/585 [06:26<03:12,  1.76s/it] 82%|████████▏ | 477/585 [06:26<02:22,  1.32s/it] 82%|████████▏ | 478/585 [06:27<01:48,  1.01s/it] 82%|████████▏ | 479/585 [06:27<01:24,  1.26it/s] 82%|████████▏ | 480/585 [06:27<01:09,  1.51it/s] 82%|████████▏ | 481/585 [06:28<00:57,  1.82it/s] 82%|████████▏ | 482/585 [06:28<01:05,  1.57it/s] 83%|████████▎ | 483/585 [06:29<00:54,  1.88it/s] 83%|████████▎ | 484/585 [06:29<00:46,  2.17it/s] 83%|████████▎ | 485/585 [06:29<00:40,  2.44it/s] 83%|████████▎ | 486/585 [06:30<00:37,  2.68it/s] 83%|████████▎ | 487/585 [06:30<00:34,  2.87it/s] 83%|████████▎ | 488/585 [06:30<00:32,  3.02it/s] 84%|████████▎ | 489/585 [06:31<00:32,  2.93it/s] 84%|████████▍ | 490/585 [06:31<00:31,  3.06it/s] 84%|████████▍ | 491/585 [06:31<00:29,  3.16it/s] 84%|████████▍ | 492/585 [06:31<00:28,  3.24it/s] 84%|████████▍ | 493/585 [06:32<00:27,  3.29it/s] 84%|████████▍ | 494/585 [06:32<00:27,  3.33it/s] 85%|████████▍ | 495/585 [06:32<00:26,  3.36it/s] 85%|████████▍ | 496/585 [06:33<00:26,  3.38it/s] 85%|████████▍ | 497/585 [06:33<00:25,  3.39it/s] 85%|████████▌ | 498/585 [06:33<00:25,  3.41it/s] 85%|████████▌ | 499/585 [06:33<00:25,  3.42it/s] 85%|████████▌ | 500/585 [06:34<00:27,  3.05it/s]                                                  85%|████████▌ | 500/585 [06:34<00:27,  3.05it/s] 86%|████████▌ | 501/585 [06:34<00:26,  3.16it/s] 86%|████████▌ | 502/585 [06:34<00:25,  3.24it/s] 86%|████████▌ | 503/585 [06:35<00:24,  3.30it/s] 86%|████████▌ | 504/585 [06:35<00:24,  3.33it/s] 86%|████████▋ | 505/585 [06:35<00:23,  3.36it/s] 86%|████████▋ | 506/585 [06:36<00:36,  2.15it/s] 87%|████████▋ | 507/585 [06:37<00:32,  2.37it/s] 87%|████████▋ | 508/585 [06:37<00:38,  1.99it/s] 87%|████████▋ | 509/585 [06:37<00:33,  2.28it/s] 87%|████████▋ | 510/585 [06:38<00:29,  2.55it/s] 87%|████████▋ | 511/585 [06:38<00:26,  2.77it/s] 88%|████████▊ | 512/585 [06:38<00:24,  2.95it/s] 88%|████████▊ | 513/585 [06:39<00:23,  3.08it/s] 88%|████████▊ | 514/585 [06:39<00:22,  3.20it/s] 88%|████████▊ | 515/585 [06:39<00:21,  3.28it/s] 88%|████████▊ | 516/585 [06:39<00:20,  3.35it/s] 88%|████████▊ | 517/585 [06:40<00:20,  3.38it/s] 89%|████████▊ | 518/585 [06:40<00:25,  2.63it/s] 89%|████████▊ | 519/585 [06:41<00:23,  2.85it/s] 89%|████████▉ | 520/585 [06:41<00:21,  3.01it/s] 89%|████████▉ | 521/585 [06:41<00:21,  2.92it/s] 89%|████████▉ | 522/585 [06:42<00:20,  3.07it/s] 89%|████████▉ | 523/585 [06:42<00:19,  3.19it/s] 90%|████████▉ | 524/585 [06:42<00:18,  3.28it/s] 90%|████████▉ | 525/585 [06:42<00:17,  3.34it/s] 90%|████████▉ | 526/585 [06:43<00:17,  3.39it/s] 90%|█████████ | 527/585 [06:43<00:16,  3.42it/s] 90%|█████████ | 528/585 [06:43<00:16,  3.44it/s] 90%|█████████ | 529/585 [06:44<00:16,  3.46it/s] 91%|█████████ | 530/585 [06:44<00:15,  3.48it/s] 91%|█████████ | 531/585 [06:44<00:15,  3.49it/s] 91%|█████████ | 532/585 [06:45<00:19,  2.69it/s] 91%|█████████ | 533/585 [06:45<00:17,  2.89it/s] 91%|█████████▏| 534/585 [06:45<00:16,  3.05it/s] 91%|█████████▏| 535/585 [06:46<00:15,  3.17it/s] 92%|█████████▏| 536/585 [06:46<00:15,  3.26it/s] 92%|█████████▏| 537/585 [06:46<00:14,  3.33it/s] 92%|█████████▏| 538/585 [06:46<00:13,  3.38it/s] 92%|█████████▏| 539/585 [06:47<00:13,  3.41it/s] 92%|█████████▏| 540/585 [06:47<00:13,  3.44it/s] 92%|█████████▏| 541/585 [06:47<00:12,  3.45it/s] 93%|█████████▎| 542/585 [06:48<00:17,  2.44it/s] 93%|█████████▎| 543/585 [06:48<00:15,  2.68it/s] 93%|█████████▎| 544/585 [06:49<00:14,  2.89it/s] 93%|█████████▎| 545/585 [06:49<00:13,  3.05it/s] 93%|█████████▎| 546/585 [06:49<00:12,  3.17it/s] 94%|█████████▎| 547/585 [06:49<00:11,  3.26it/s] 94%|█████████▎| 548/585 [06:50<00:11,  3.32it/s] 94%|█████████▍| 549/585 [06:50<00:10,  3.36it/s] 94%|█████████▍| 550/585 [06:50<00:10,  3.39it/s] 94%|█████████▍| 551/585 [06:51<00:09,  3.42it/s] 94%|█████████▍| 552/585 [06:51<00:15,  2.18it/s] 95%|█████████▍| 553/585 [06:52<00:13,  2.46it/s] 95%|█████████▍| 554/585 [06:52<00:11,  2.69it/s] 95%|█████████▍| 555/585 [06:52<00:10,  2.88it/s] 95%|█████████▌| 556/585 [06:53<00:09,  3.04it/s] 95%|█████████▌| 557/585 [06:53<00:08,  3.17it/s] 95%|█████████▌| 558/585 [06:53<00:08,  3.26it/s] 96%|█████████▌| 559/585 [06:53<00:07,  3.32it/s] 96%|█████████▌| 560/585 [06:54<00:07,  3.36it/s] 96%|█████████▌| 561/585 [06:54<00:08,  2.71it/s] 96%|█████████▌| 562/585 [06:55<00:07,  2.90it/s] 96%|█████████▌| 563/585 [06:55<00:07,  3.05it/s] 96%|█████████▋| 564/585 [06:55<00:06,  3.18it/s] 97%|█████████▋| 565/585 [06:55<00:06,  3.26it/s] 97%|█████████▋| 566/585 [06:56<00:05,  3.33it/s] 97%|█████████▋| 567/585 [06:56<00:05,  3.37it/s] 97%|█████████▋| 568/585 [06:56<00:04,  3.41it/s] 97%|█████████▋| 569/585 [06:57<00:04,  3.43it/s] 97%|█████████▋| 570/585 [06:57<00:04,  3.44it/s] 98%|█████████▊| 571/585 [06:57<00:04,  2.97it/s] 98%|█████████▊| 572/585 [06:58<00:04,  3.11it/s] 98%|█████████▊| 573/585 [06:58<00:03,  3.21it/s] 98%|█████████▊| 574/585 [06:58<00:03,  3.29it/s] 98%|█████████▊| 575/585 [06:58<00:02,  3.35it/s] 98%|█████████▊| 576/585 [06:59<00:02,  3.39it/s] 99%|█████████▊| 577/585 [06:59<00:02,  3.42it/s] 99%|█████████▉| 578/585 [06:59<00:02,  3.44it/s] 99%|█████████▉| 579/585 [07:00<00:01,  3.45it/s] 99%|█████████▉| 580/585 [07:00<00:01,  3.46it/s] 99%|█████████▉| 581/585 [07:00<00:01,  2.97it/s] 99%|█████████▉| 582/585 [07:01<00:00,  3.11it/s]100%|█████████▉| 583/585 [07:01<00:00,  3.21it/s]100%|█████████▉| 584/585 [07:01<00:00,  3.28it/s]100%|██████████| 585/585 [07:01<00:00,  3.34it/s][INFO|trainer.py:2140] 2023-08-28 21:30:41,170 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:30:41,170 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 21:30:41,170 >>   Batch size = 8
{'eval_loss': 1.0383707284927368, 'eval_runtime': 12.3071, 'eval_samples_per_second': 283.414, 'eval_steps_per_second': 35.427, 'epoch': 4.0}
{'loss': 0.5309, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.28it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.72it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.86it/s][A
  5%|▌         | 22/436 [00:00<00:08, 46.12it/s][A
  6%|▌         | 27/436 [00:00<00:08, 45.48it/s][A
  7%|▋         | 32/436 [00:00<00:08, 45.22it/s][A
  8%|▊         | 37/436 [00:00<00:08, 45.22it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.95it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.99it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.99it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 45.04it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.93it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.79it/s][A
 17%|█▋        | 72/436 [00:01<00:10, 35.35it/s][A
 18%|█▊        | 77/436 [00:01<00:09, 37.78it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 39.72it/s][A
 20%|█▉        | 87/436 [00:02<00:08, 41.26it/s][A
 21%|██        | 92/436 [00:02<00:08, 42.36it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 43.10it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 43.87it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.06it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 43.86it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.85it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.24it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.48it/s][A
 30%|███       | 132/436 [00:03<00:06, 44.68it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.85it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.90it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 45.04it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 45.03it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.66it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.63it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.64it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.72it/s][A
 41%|████      | 177/436 [00:04<00:05, 44.81it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.94it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.95it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.94it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 45.04it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.89it/s][A
 47%|████▋     | 207/436 [00:04<00:06, 36.00it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 38.31it/s][A
 50%|████▉     | 217/436 [00:05<00:05, 40.13it/s][A
 51%|█████     | 222/436 [00:05<00:05, 41.47it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 42.49it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 43.26it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 43.82it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.11it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.13it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.20it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.47it/s][A
 60%|██████    | 262/436 [00:06<00:03, 44.71it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.80it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.95it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.82it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.99it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.88it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.70it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.75it/s][A
 69%|██████▉   | 302/436 [00:06<00:02, 44.84it/s][A
 70%|███████   | 307/436 [00:07<00:02, 44.86it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.66it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.80it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.88it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.80it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.90it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.86it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 39.81it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 41.39it/s][A
 81%|████████  | 352/436 [00:08<00:01, 42.50it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 43.36it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 43.97it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.38it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.55it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.67it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.41it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.07it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.40it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 44.58it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.81it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.99it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 45.09it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 45.14it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.72it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.23it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.16it/s][A                                                 
                                                 [A100%|██████████| 585/585 [07:11<00:00,  3.34it/s]
100%|██████████| 436/436 [00:09<00:00, 44.16it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:30:51,567 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 21:30:51,903 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:30:58,209 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:30:58,727 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:30:59,015 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:31:12,299 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:31:12,327 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117 (score: 1.0065582990646362).
                                                 100%|██████████| 585/585 [08:06<00:00,  3.34it/s]100%|██████████| 585/585 [08:06<00:00,  1.20it/s]
[INFO|trainer.py:1894] 2023-08-28 21:31:45,865 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 21:31:45,996 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:31:54,224 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:31:54,683 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:31:54,786 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:31:55,809 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:31:55,818 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:31:55,818 >>   train_loss               =      0.527
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:31:55,818 >>   train_runtime            = 0:08:06.33
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:31:55,818 >>   train_samples            =       7499
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:31:55,818 >>   train_samples_per_second =     77.098
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:31:55,818 >>   train_steps_per_second   =      1.203
{'eval_loss': 1.0426628589630127, 'eval_runtime': 9.9581, 'eval_samples_per_second': 350.267, 'eval_steps_per_second': 43.783, 'epoch': 5.0}
{'train_runtime': 486.3302, 'train_samples_per_second': 77.098, 'train_steps_per_second': 1.203, 'train_loss': 0.5269911024305556, 'epoch': 5.0}
08/28/2023 21:31:56 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:31:56,060 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:31:56,060 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 21:31:56,060 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 56.06it/s]  3%|▎         | 12/436 [00:00<00:08, 49.40it/s]  4%|▍         | 17/436 [00:00<00:08, 47.69it/s]  5%|▌         | 22/436 [00:00<00:08, 46.87it/s]  6%|▌         | 27/436 [00:00<00:08, 46.29it/s]  7%|▋         | 32/436 [00:00<00:08, 45.95it/s]  8%|▊         | 37/436 [00:00<00:08, 45.74it/s] 10%|▉         | 42/436 [00:00<00:08, 45.40it/s] 11%|█         | 47/436 [00:01<00:08, 44.83it/s] 12%|█▏        | 52/436 [00:01<00:08, 44.82it/s] 13%|█▎        | 57/436 [00:01<00:08, 44.95it/s] 14%|█▍        | 62/436 [00:01<00:08, 45.03it/s] 15%|█▌        | 67/436 [00:01<00:08, 45.20it/s] 17%|█▋        | 72/436 [00:01<00:08, 45.32it/s] 18%|█▊        | 77/436 [00:01<00:07, 45.35it/s] 19%|█▉        | 82/436 [00:01<00:07, 45.31it/s] 20%|█▉        | 87/436 [00:01<00:07, 45.15it/s] 21%|██        | 92/436 [00:02<00:07, 44.84it/s] 22%|██▏       | 97/436 [00:02<00:07, 44.64it/s] 23%|██▎       | 102/436 [00:02<00:07, 44.66it/s] 25%|██▍       | 107/436 [00:02<00:07, 44.63it/s] 26%|██▌       | 112/436 [00:02<00:07, 44.91it/s] 27%|██▋       | 117/436 [00:02<00:07, 45.12it/s] 28%|██▊       | 122/436 [00:02<00:06, 45.32it/s] 29%|██▉       | 127/436 [00:02<00:06, 45.39it/s] 30%|███       | 132/436 [00:02<00:06, 45.15it/s] 31%|███▏      | 137/436 [00:03<00:06, 45.05it/s] 33%|███▎      | 142/436 [00:03<00:06, 45.13it/s] 34%|███▎      | 147/436 [00:03<00:06, 45.09it/s] 35%|███▍      | 152/436 [00:03<00:06, 45.31it/s] 36%|███▌      | 157/436 [00:03<00:06, 45.34it/s] 37%|███▋      | 162/436 [00:03<00:06, 45.33it/s] 38%|███▊      | 167/436 [00:03<00:05, 45.27it/s] 39%|███▉      | 172/436 [00:03<00:05, 45.40it/s] 41%|████      | 177/436 [00:03<00:05, 45.15it/s] 42%|████▏     | 182/436 [00:04<00:05, 45.04it/s] 43%|████▎     | 187/436 [00:04<00:05, 45.18it/s] 44%|████▍     | 192/436 [00:04<00:05, 45.02it/s] 45%|████▌     | 197/436 [00:04<00:05, 45.08it/s] 46%|████▋     | 202/436 [00:04<00:05, 45.32it/s] 47%|████▋     | 207/436 [00:04<00:05, 45.48it/s] 49%|████▊     | 212/436 [00:04<00:04, 45.51it/s] 50%|████▉     | 217/436 [00:04<00:04, 45.55it/s] 51%|█████     | 222/436 [00:04<00:04, 45.32it/s] 52%|█████▏    | 227/436 [00:05<00:04, 45.22it/s] 53%|█████▎    | 232/436 [00:05<00:04, 45.18it/s] 54%|█████▍    | 237/436 [00:05<00:04, 45.25it/s] 56%|█████▌    | 242/436 [00:05<00:04, 45.10it/s] 57%|█████▋    | 247/436 [00:05<00:04, 45.21it/s] 58%|█████▊    | 252/436 [00:05<00:04, 45.26it/s] 59%|█████▉    | 257/436 [00:05<00:03, 45.44it/s] 60%|██████    | 262/436 [00:05<00:03, 45.39it/s] 61%|██████    | 267/436 [00:05<00:03, 45.40it/s] 62%|██████▏   | 272/436 [00:06<00:04, 40.82it/s] 64%|██████▎   | 277/436 [00:06<00:03, 42.22it/s] 65%|██████▍   | 282/436 [00:06<00:03, 43.26it/s] 66%|██████▌   | 287/436 [00:06<00:03, 44.01it/s] 67%|██████▋   | 292/436 [00:06<00:03, 44.39it/s] 68%|██████▊   | 297/436 [00:06<00:03, 44.61it/s] 69%|██████▉   | 302/436 [00:06<00:02, 44.93it/s] 70%|███████   | 307/436 [00:06<00:02, 45.13it/s] 72%|███████▏  | 312/436 [00:06<00:02, 44.80it/s] 73%|███████▎  | 317/436 [00:07<00:02, 44.53it/s] 74%|███████▍  | 322/436 [00:07<00:02, 44.66it/s] 75%|███████▌  | 327/436 [00:07<00:02, 44.93it/s] 76%|███████▌  | 332/436 [00:07<00:02, 45.30it/s] 77%|███████▋  | 337/436 [00:07<00:02, 45.44it/s] 78%|███████▊  | 342/436 [00:07<00:02, 45.50it/s] 80%|███████▉  | 347/436 [00:07<00:01, 45.56it/s] 81%|████████  | 352/436 [00:07<00:01, 45.42it/s] 82%|████████▏ | 357/436 [00:07<00:01, 45.21it/s] 83%|████████▎ | 362/436 [00:08<00:01, 44.95it/s] 84%|████████▍ | 367/436 [00:08<00:01, 44.73it/s] 85%|████████▌ | 372/436 [00:08<00:01, 45.06it/s] 86%|████████▋ | 377/436 [00:08<00:01, 45.10it/s] 88%|████████▊ | 382/436 [00:08<00:01, 45.27it/s] 89%|████████▉ | 387/436 [00:08<00:01, 45.43it/s] 90%|████████▉ | 392/436 [00:08<00:00, 45.42it/s] 91%|█████████ | 397/436 [00:08<00:00, 45.32it/s] 92%|█████████▏| 402/436 [00:08<00:00, 45.24it/s] 93%|█████████▎| 407/436 [00:09<00:00, 36.60it/s] 94%|█████████▍| 412/436 [00:09<00:00, 38.94it/s] 96%|█████████▌| 417/436 [00:09<00:00, 40.78it/s] 97%|█████████▋| 422/436 [00:09<00:00, 42.14it/s] 98%|█████████▊| 427/436 [00:09<00:00, 43.11it/s] 99%|█████████▉| 432/436 [00:09<00:00, 43.96it/s]100%|██████████| 436/436 [00:09<00:00, 44.72it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:32:05,829 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:32:05,829 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:32:05,829 >>   eval_loss               =     1.0066
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:32:05,829 >>   eval_runtime            = 0:00:09.76
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:32:05,829 >>   eval_samples            =       3488
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:32:05,829 >>   eval_samples_per_second =    357.057
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:32:05,829 >>   eval_steps_per_second   =     44.632
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:32:05,829 >>   perplexity              =     2.7362
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:16,261 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:16,284 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:16,284 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:16,284 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:16,284 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:32:16,807 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:32:16,808 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:32:17,558 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:32:18,676 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:32:18,676 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:20,875 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:20,893 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:20,893 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:20,893 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:20,893 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:32:22,274 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:32:22,275 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:32:23,032 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:32:24,060 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:32:24,063 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11910
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12010, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.54it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.47it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.62it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.62it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:10,  1.62it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:11,  1.64it/s]Extractor Predicting: 19it [00:11,  1.68it/s]Extractor Predicting: 20it [00:12,  1.67it/s]Extractor Predicting: 21it [00:13,  1.62it/s]Extractor Predicting: 22it [00:13,  1.64it/s]Extractor Predicting: 23it [00:14,  1.65it/s]Extractor Predicting: 24it [00:14,  1.65it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:16,  1.60it/s]Extractor Predicting: 27it [00:16,  1.61it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:18,  1.61it/s]Extractor Predicting: 30it [00:18,  1.60it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:19,  1.60it/s]Extractor Predicting: 33it [00:20,  1.59it/s]Extractor Predicting: 34it [00:21,  1.57it/s]Extractor Predicting: 35it [00:21,  1.56it/s]Extractor Predicting: 36it [00:22,  1.41it/s]Extractor Predicting: 37it [00:23,  1.36it/s]Extractor Predicting: 38it [00:24,  1.42it/s]Extractor Predicting: 39it [00:24,  1.47it/s]Extractor Predicting: 40it [00:25,  1.49it/s]Extractor Predicting: 41it [00:26,  1.50it/s]Extractor Predicting: 42it [00:26,  1.50it/s]Extractor Predicting: 43it [00:27,  1.49it/s]Extractor Predicting: 44it [00:28,  1.50it/s]Extractor Predicting: 45it [00:28,  1.51it/s]Extractor Predicting: 46it [00:29,  1.50it/s]Extractor Predicting: 47it [00:30,  1.44it/s]Extractor Predicting: 48it [00:30,  1.47it/s]Extractor Predicting: 49it [00:31,  1.53it/s]Extractor Predicting: 50it [00:32,  1.51it/s]Extractor Predicting: 51it [00:32,  1.56it/s]Extractor Predicting: 52it [00:33,  1.55it/s]Extractor Predicting: 53it [00:33,  1.56it/s]Extractor Predicting: 54it [00:34,  1.56it/s]Extractor Predicting: 55it [00:35,  1.55it/s]Extractor Predicting: 56it [00:35,  1.54it/s]Extractor Predicting: 57it [00:36,  1.55it/s]Extractor Predicting: 58it [00:37,  1.55it/s]Extractor Predicting: 59it [00:37,  1.53it/s]Extractor Predicting: 60it [00:38,  1.51it/s]Extractor Predicting: 61it [00:39,  1.50it/s]Extractor Predicting: 62it [00:39,  1.48it/s]Extractor Predicting: 63it [00:40,  1.51it/s]Extractor Predicting: 64it [00:41,  1.49it/s]Extractor Predicting: 65it [00:41,  1.53it/s]Extractor Predicting: 66it [00:42,  1.56it/s]Extractor Predicting: 67it [00:43,  1.41it/s]Extractor Predicting: 68it [00:43,  1.47it/s]Extractor Predicting: 69it [00:44,  1.47it/s]Extractor Predicting: 70it [00:45,  1.50it/s]Extractor Predicting: 71it [00:45,  1.49it/s]Extractor Predicting: 72it [00:46,  1.50it/s]Extractor Predicting: 73it [00:47,  1.50it/s]Extractor Predicting: 74it [00:47,  1.51it/s]Extractor Predicting: 75it [00:48,  1.53it/s]Extractor Predicting: 76it [00:49,  1.53it/s]Extractor Predicting: 77it [00:49,  1.45it/s]Extractor Predicting: 78it [00:50,  1.46it/s]Extractor Predicting: 79it [00:51,  1.47it/s]Extractor Predicting: 80it [00:51,  1.48it/s]Extractor Predicting: 81it [00:52,  1.49it/s]Extractor Predicting: 82it [00:53,  1.14it/s]Extractor Predicting: 83it [00:54,  1.26it/s]Extractor Predicting: 84it [00:55,  1.33it/s]Extractor Predicting: 85it [00:55,  1.37it/s]Extractor Predicting: 86it [00:56,  1.42it/s]Extractor Predicting: 87it [00:57,  1.29it/s]Extractor Predicting: 88it [00:58,  1.37it/s]Extractor Predicting: 89it [00:58,  1.42it/s]Extractor Predicting: 90it [00:59,  1.49it/s]Extractor Predicting: 91it [00:59,  1.56it/s]Extractor Predicting: 92it [01:00,  1.59it/s]Extractor Predicting: 93it [01:01,  1.58it/s]Extractor Predicting: 94it [01:01,  1.61it/s]Extractor Predicting: 95it [01:02,  1.46it/s]Extractor Predicting: 96it [01:03,  1.50it/s]Extractor Predicting: 97it [01:03,  1.52it/s]Extractor Predicting: 98it [01:04,  1.53it/s]Extractor Predicting: 99it [01:05,  1.52it/s]Extractor Predicting: 100it [01:05,  1.50it/s]Extractor Predicting: 101it [01:06,  1.56it/s]Extractor Predicting: 102it [01:07,  1.62it/s]Extractor Predicting: 103it [01:07,  1.62it/s]Extractor Predicting: 104it [01:08,  1.63it/s]Extractor Predicting: 105it [01:08,  1.60it/s]Extractor Predicting: 106it [01:09,  1.62it/s]Extractor Predicting: 107it [01:10,  1.47it/s]Extractor Predicting: 108it [01:10,  1.51it/s]Extractor Predicting: 109it [01:11,  1.53it/s]Extractor Predicting: 110it [01:12,  1.48it/s]Extractor Predicting: 111it [01:12,  1.55it/s]Extractor Predicting: 112it [01:13,  1.58it/s]Extractor Predicting: 113it [01:14,  1.63it/s]Extractor Predicting: 114it [01:14,  1.63it/s]Extractor Predicting: 115it [01:15,  1.54it/s]Extractor Predicting: 116it [01:16,  1.56it/s]Extractor Predicting: 117it [01:16,  1.56it/s]Extractor Predicting: 118it [01:17,  1.56it/s]Extractor Predicting: 119it [01:17,  1.55it/s]Extractor Predicting: 120it [01:18,  1.41it/s]Extractor Predicting: 121it [01:19,  1.48it/s]Extractor Predicting: 122it [01:20,  1.51it/s]Extractor Predicting: 123it [01:20,  1.52it/s]Extractor Predicting: 124it [01:21,  1.51it/s]Extractor Predicting: 125it [01:22,  1.45it/s]Extractor Predicting: 126it [01:22,  1.47it/s]Extractor Predicting: 127it [01:23,  1.49it/s]Extractor Predicting: 128it [01:24,  1.55it/s]Extractor Predicting: 129it [01:24,  1.53it/s]Extractor Predicting: 130it [01:25,  1.54it/s]Extractor Predicting: 131it [01:25,  1.55it/s]Extractor Predicting: 132it [01:26,  1.55it/s]Extractor Predicting: 133it [01:27,  1.54it/s]Extractor Predicting: 134it [01:27,  1.52it/s]Extractor Predicting: 135it [01:28,  1.50it/s]Extractor Predicting: 136it [01:29,  1.53it/s]Extractor Predicting: 137it [01:29,  1.54it/s]Extractor Predicting: 138it [01:30,  1.54it/s]Extractor Predicting: 139it [01:31,  1.52it/s]Extractor Predicting: 140it [01:31,  1.56it/s]Extractor Predicting: 141it [01:32,  1.54it/s]Extractor Predicting: 142it [01:33,  1.45it/s]Extractor Predicting: 143it [01:33,  1.51it/s]Extractor Predicting: 144it [01:34,  1.83it/s]Extractor Predicting: 144it [01:34,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:35,249 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:36,065 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:36,066 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:36,066 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:36,066 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:34:36,924 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:34:36,925 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:34:38,238 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:34:39,296 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:34:39,357 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:45,793 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:45,845 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:45,846 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:45,846 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:45,846 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:34:47,284 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:34:47,285 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:34:48,786 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:34:48,952 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:34:49,049 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.31940818102698,
  "recall": 0.10521788990825688,
  "score": 0.15829199913737332,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19834
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19934, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.40it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:04,  1.31it/s]Extractor Predicting: 7it [00:04,  1.40it/s]Extractor Predicting: 8it [00:05,  1.44it/s]Extractor Predicting: 9it [00:06,  1.51it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.45it/s]Extractor Predicting: 12it [00:08,  1.49it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:10,  1.53it/s]Extractor Predicting: 16it [00:10,  1.47it/s]Extractor Predicting: 17it [00:11,  1.46it/s]Extractor Predicting: 18it [00:12,  1.50it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:13,  1.55it/s]Extractor Predicting: 21it [00:14,  1.53it/s]Extractor Predicting: 22it [00:14,  1.59it/s]Extractor Predicting: 23it [00:15,  1.60it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:17,  1.56it/s]Extractor Predicting: 27it [00:17,  1.55it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:19,  1.56it/s]Extractor Predicting: 30it [00:19,  1.51it/s]Extractor Predicting: 31it [00:20,  1.56it/s]Extractor Predicting: 32it [00:21,  1.63it/s]Extractor Predicting: 33it [00:21,  1.67it/s]Extractor Predicting: 34it [00:22,  1.69it/s]Extractor Predicting: 35it [00:22,  1.72it/s]Extractor Predicting: 36it [00:23,  1.72it/s]Extractor Predicting: 37it [00:23,  1.67it/s]Extractor Predicting: 38it [00:24,  1.73it/s]Extractor Predicting: 39it [00:25,  1.64it/s]Extractor Predicting: 40it [00:25,  1.69it/s]Extractor Predicting: 41it [00:26,  1.71it/s]Extractor Predicting: 42it [00:26,  1.74it/s]Extractor Predicting: 43it [00:27,  1.75it/s]Extractor Predicting: 44it [00:28,  1.67it/s]Extractor Predicting: 45it [00:28,  1.70it/s]Extractor Predicting: 46it [00:29,  1.67it/s]Extractor Predicting: 47it [00:29,  1.73it/s]Extractor Predicting: 48it [00:30,  1.78it/s]Extractor Predicting: 49it [00:30,  1.81it/s]Extractor Predicting: 50it [00:31,  1.54it/s]Extractor Predicting: 51it [00:32,  1.63it/s]Extractor Predicting: 52it [00:32,  1.67it/s]Extractor Predicting: 53it [00:33,  1.70it/s]Extractor Predicting: 54it [00:33,  1.72it/s]Extractor Predicting: 55it [00:35,  1.22it/s]Extractor Predicting: 56it [00:35,  1.36it/s]Extractor Predicting: 57it [00:36,  1.45it/s]Extractor Predicting: 58it [00:37,  1.51it/s]Extractor Predicting: 59it [00:37,  1.45it/s]Extractor Predicting: 60it [00:38,  1.47it/s]Extractor Predicting: 61it [00:39,  1.46it/s]Extractor Predicting: 62it [00:39,  1.45it/s]Extractor Predicting: 63it [00:40,  1.47it/s]Extractor Predicting: 64it [00:41,  1.39it/s]Extractor Predicting: 65it [00:41,  1.46it/s]Extractor Predicting: 66it [00:42,  1.47it/s]Extractor Predicting: 67it [00:43,  1.50it/s]Extractor Predicting: 68it [00:43,  1.47it/s]Extractor Predicting: 69it [00:44,  1.46it/s]Extractor Predicting: 70it [00:45,  1.44it/s]Extractor Predicting: 71it [00:46,  1.42it/s]Extractor Predicting: 72it [00:46,  1.46it/s]Extractor Predicting: 73it [00:47,  1.45it/s]Extractor Predicting: 74it [00:48,  1.41it/s]Extractor Predicting: 75it [00:48,  1.44it/s]Extractor Predicting: 76it [00:49,  1.46it/s]Extractor Predicting: 77it [00:50,  1.47it/s]Extractor Predicting: 78it [00:50,  1.46it/s]Extractor Predicting: 79it [00:51,  1.27it/s]Extractor Predicting: 80it [00:52,  1.32it/s]Extractor Predicting: 81it [00:53,  1.36it/s]Extractor Predicting: 82it [00:53,  1.39it/s]Extractor Predicting: 83it [00:54,  1.33it/s]Extractor Predicting: 84it [00:55,  1.40it/s]Extractor Predicting: 85it [00:56,  1.22it/s]Extractor Predicting: 86it [00:57,  1.27it/s]Extractor Predicting: 87it [00:57,  1.32it/s]Extractor Predicting: 88it [00:58,  1.36it/s]Extractor Predicting: 89it [00:59,  1.37it/s]Extractor Predicting: 90it [00:59,  1.43it/s]Extractor Predicting: 91it [01:00,  1.42it/s]Extractor Predicting: 92it [01:01,  1.41it/s]Extractor Predicting: 93it [01:01,  1.46it/s]Extractor Predicting: 94it [01:02,  1.42it/s]Extractor Predicting: 95it [01:03,  1.47it/s]Extractor Predicting: 96it [01:03,  1.52it/s]Extractor Predicting: 97it [01:04,  1.53it/s]Extractor Predicting: 98it [01:05,  1.58it/s]Extractor Predicting: 99it [01:05,  1.49it/s]Extractor Predicting: 100it [01:06,  1.52it/s]Extractor Predicting: 101it [01:07,  1.60it/s]Extractor Predicting: 102it [01:07,  1.61it/s]Extractor Predicting: 103it [01:08,  1.43it/s]Extractor Predicting: 104it [01:09,  1.37it/s]Extractor Predicting: 105it [01:10,  1.43it/s]Extractor Predicting: 106it [01:10,  1.46it/s]Extractor Predicting: 107it [01:11,  1.49it/s]Extractor Predicting: 108it [01:11,  1.51it/s]Extractor Predicting: 109it [01:12,  1.46it/s]Extractor Predicting: 110it [01:13,  1.47it/s]Extractor Predicting: 111it [01:13,  1.51it/s]Extractor Predicting: 112it [01:14,  1.48it/s]Extractor Predicting: 113it [01:15,  1.53it/s]Extractor Predicting: 114it [01:16,  1.43it/s]Extractor Predicting: 115it [01:16,  1.46it/s]Extractor Predicting: 116it [01:17,  1.54it/s]Extractor Predicting: 117it [01:17,  1.54it/s]Extractor Predicting: 118it [01:18,  1.60it/s]Extractor Predicting: 119it [01:19,  1.56it/s]Extractor Predicting: 120it [01:19,  1.60it/s]Extractor Predicting: 121it [01:20,  1.63it/s]Extractor Predicting: 122it [01:20,  1.64it/s]Extractor Predicting: 123it [01:21,  1.67it/s]Extractor Predicting: 124it [01:22,  1.65it/s]Extractor Predicting: 125it [01:22,  1.68it/s]Extractor Predicting: 126it [01:23,  1.67it/s]Extractor Predicting: 127it [01:23,  1.67it/s]Extractor Predicting: 128it [01:24,  1.73it/s]Extractor Predicting: 129it [01:25,  1.71it/s]Extractor Predicting: 130it [01:25,  1.76it/s]Extractor Predicting: 131it [01:26,  1.78it/s]Extractor Predicting: 132it [01:26,  1.76it/s]Extractor Predicting: 133it [01:27,  1.71it/s]Extractor Predicting: 134it [01:27,  1.69it/s]Extractor Predicting: 135it [01:28,  1.73it/s]Extractor Predicting: 136it [01:29,  1.76it/s]Extractor Predicting: 137it [01:29,  1.56it/s]Extractor Predicting: 138it [01:30,  1.62it/s]Extractor Predicting: 139it [01:31,  1.62it/s]Extractor Predicting: 140it [01:31,  1.68it/s]Extractor Predicting: 141it [01:32,  1.69it/s]Extractor Predicting: 142it [01:33,  1.45it/s]Extractor Predicting: 143it [01:33,  1.53it/s]Extractor Predicting: 144it [01:34,  1.58it/s]Extractor Predicting: 145it [01:34,  1.61it/s]Extractor Predicting: 146it [01:35,  1.61it/s]Extractor Predicting: 147it [01:36,  1.50it/s]Extractor Predicting: 148it [01:36,  1.54it/s]Extractor Predicting: 149it [01:37,  1.55it/s]Extractor Predicting: 150it [01:38,  1.55it/s]Extractor Predicting: 151it [01:38,  1.57it/s]Extractor Predicting: 152it [01:39,  1.56it/s]Extractor Predicting: 153it [01:40,  1.54it/s]Extractor Predicting: 154it [01:40,  1.57it/s]Extractor Predicting: 155it [01:41,  1.60it/s]Extractor Predicting: 156it [01:41,  1.57it/s]Extractor Predicting: 157it [01:42,  1.39it/s]Extractor Predicting: 158it [01:43,  1.43it/s]Extractor Predicting: 159it [01:44,  1.46it/s]Extractor Predicting: 160it [01:44,  1.48it/s]Extractor Predicting: 161it [01:45,  1.50it/s]Extractor Predicting: 162it [01:46,  1.46it/s]Extractor Predicting: 163it [01:46,  1.51it/s]Extractor Predicting: 164it [01:47,  1.52it/s]Extractor Predicting: 165it [01:48,  1.52it/s]Extractor Predicting: 166it [01:48,  1.48it/s]Extractor Predicting: 167it [01:49,  1.50it/s]Extractor Predicting: 168it [01:50,  1.50it/s]Extractor Predicting: 169it [01:50,  1.54it/s]Extractor Predicting: 170it [01:51,  1.54it/s]Extractor Predicting: 171it [01:52,  1.54it/s]Extractor Predicting: 172it [01:52,  1.51it/s]Extractor Predicting: 173it [01:53,  1.50it/s]Extractor Predicting: 174it [01:54,  1.51it/s]Extractor Predicting: 175it [01:54,  1.54it/s]Extractor Predicting: 176it [01:55,  1.54it/s]Extractor Predicting: 177it [01:56,  1.50it/s]Extractor Predicting: 178it [01:56,  1.51it/s]Extractor Predicting: 179it [01:57,  1.53it/s]Extractor Predicting: 180it [01:57,  1.53it/s]Extractor Predicting: 181it [01:58,  1.53it/s]Extractor Predicting: 182it [01:59,  1.40it/s]Extractor Predicting: 183it [02:00,  1.43it/s]Extractor Predicting: 184it [02:00,  1.39it/s]Extractor Predicting: 185it [02:01,  1.44it/s]Extractor Predicting: 186it [02:02,  1.46it/s]Extractor Predicting: 187it [02:02,  1.50it/s]Extractor Predicting: 188it [02:03,  1.51it/s]Extractor Predicting: 189it [02:04,  1.52it/s]Extractor Predicting: 190it [02:04,  1.54it/s]Extractor Predicting: 191it [02:05,  1.57it/s]Extractor Predicting: 192it [02:05,  1.57it/s]Extractor Predicting: 193it [02:06,  1.60it/s]Extractor Predicting: 194it [02:07,  1.55it/s]Extractor Predicting: 195it [02:07,  1.55it/s]Extractor Predicting: 196it [02:08,  1.57it/s]Extractor Predicting: 197it [02:09,  1.58it/s]Extractor Predicting: 198it [02:09,  1.52it/s]Extractor Predicting: 199it [02:10,  1.48it/s]Extractor Predicting: 200it [02:11,  1.51it/s]Extractor Predicting: 201it [02:11,  1.56it/s]Extractor Predicting: 202it [02:12,  1.54it/s]Extractor Predicting: 203it [02:13,  1.57it/s]Extractor Predicting: 204it [02:13,  1.46it/s]Extractor Predicting: 205it [02:14,  1.49it/s]Extractor Predicting: 206it [02:15,  1.56it/s]Extractor Predicting: 207it [02:15,  1.57it/s]Extractor Predicting: 208it [02:16,  1.61it/s]Extractor Predicting: 209it [02:17,  1.34it/s]Extractor Predicting: 210it [02:17,  1.40it/s]Extractor Predicting: 211it [02:18,  1.44it/s]Extractor Predicting: 212it [02:19,  1.46it/s]Extractor Predicting: 213it [02:19,  1.50it/s]Extractor Predicting: 214it [02:20,  1.49it/s]Extractor Predicting: 215it [02:21,  1.49it/s]Extractor Predicting: 216it [02:21,  1.51it/s]Extractor Predicting: 217it [02:22,  1.47it/s]Extractor Predicting: 218it [02:23,  1.51it/s]Extractor Predicting: 219it [02:24,  1.41it/s]Extractor Predicting: 220it [02:24,  1.47it/s]Extractor Predicting: 221it [02:25,  1.49it/s]Extractor Predicting: 222it [02:25,  1.51it/s]Extractor Predicting: 223it [02:26,  1.55it/s]Extractor Predicting: 224it [02:27,  1.51it/s]Extractor Predicting: 225it [02:27,  1.52it/s]Extractor Predicting: 226it [02:28,  1.54it/s]Extractor Predicting: 227it [02:29,  1.53it/s]Extractor Predicting: 228it [02:29,  1.58it/s]Extractor Predicting: 229it [02:30,  1.56it/s]Extractor Predicting: 230it [02:31,  1.36it/s]Extractor Predicting: 231it [02:32,  1.40it/s]Extractor Predicting: 232it [02:32,  1.43it/s]Extractor Predicting: 233it [02:33,  1.48it/s]Extractor Predicting: 234it [02:34,  1.51it/s]Extractor Predicting: 235it [02:34,  1.48it/s]Extractor Predicting: 236it [02:35,  1.51it/s]Extractor Predicting: 237it [02:36,  1.47it/s]Extractor Predicting: 238it [02:36,  1.47it/s]Extractor Predicting: 239it [02:37,  1.49it/s]Extractor Predicting: 240it [02:38,  1.38it/s]Extractor Predicting: 241it [02:38,  1.42it/s]Extractor Predicting: 242it [02:39,  1.47it/s]Extractor Predicting: 243it [02:40,  1.49it/s]Extractor Predicting: 244it [02:40,  1.50it/s]Extractor Predicting: 245it [02:41,  1.40it/s]Extractor Predicting: 246it [02:42,  1.42it/s]Extractor Predicting: 247it [02:43,  1.41it/s]Extractor Predicting: 248it [02:43,  1.45it/s]Extractor Predicting: 249it [02:44,  1.52it/s]Extractor Predicting: 250it [02:45,  1.22it/s]Extractor Predicting: 251it [02:46,  1.29it/s]Extractor Predicting: 252it [02:46,  1.37it/s]Extractor Predicting: 253it [02:47,  1.43it/s]Extractor Predicting: 254it [02:48,  1.43it/s]Extractor Predicting: 255it [02:48,  1.45it/s]Extractor Predicting: 256it [02:49,  1.46it/s]Extractor Predicting: 257it [02:50,  1.47it/s]Extractor Predicting: 258it [02:50,  1.48it/s]Extractor Predicting: 259it [02:51,  1.45it/s]Extractor Predicting: 260it [02:52,  1.49it/s]Extractor Predicting: 261it [02:52,  1.53it/s]Extractor Predicting: 262it [02:53,  1.53it/s]Extractor Predicting: 263it [02:54,  1.53it/s]Extractor Predicting: 264it [02:55,  1.34it/s]Extractor Predicting: 265it [02:55,  1.40it/s]Extractor Predicting: 266it [02:56,  1.44it/s]Extractor Predicting: 267it [02:56,  1.51it/s]Extractor Predicting: 268it [02:57,  1.53it/s]Extractor Predicting: 269it [02:58,  1.52it/s]Extractor Predicting: 270it [02:58,  1.51it/s]Extractor Predicting: 271it [02:59,  1.56it/s]Extractor Predicting: 272it [03:00,  1.56it/s]Extractor Predicting: 273it [03:00,  1.60it/s]Extractor Predicting: 274it [03:01,  1.60it/s]Extractor Predicting: 275it [03:01,  1.59it/s]Extractor Predicting: 276it [03:02,  1.57it/s]Extractor Predicting: 277it [03:03,  1.58it/s]Extractor Predicting: 278it [03:03,  1.59it/s]Extractor Predicting: 279it [03:04,  1.59it/s]Extractor Predicting: 280it [03:05,  1.49it/s]Extractor Predicting: 281it [03:05,  1.55it/s]Extractor Predicting: 282it [03:06,  1.56it/s]Extractor Predicting: 283it [03:07,  1.54it/s]Extractor Predicting: 284it [03:07,  1.77it/s]Extractor Predicting: 284it [03:07,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:38:19,900 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:38:19,928 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:38:19,929 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:38:19,929 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:38:19,929 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:38:20,421 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:38:20,422 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:38:21,016 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:38:22,090 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:38:22,090 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:38:24,639 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:38:24,677 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:38:24,677 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:38:24,677 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:38:24,677 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:38:25,197 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:38:25,198 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:38:25,961 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:38:26,122 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:38:26,122 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4340758579169175,
  "recall": 0.10599823581299618,
  "score": 0.17038875103391235,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1045
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1145, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:02,  1.45it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:02,  2.00it/s]Extractor Predicting: 5it [00:02,  1.72it/s]
[INFO|configuration_utils.py:515] 2023-08-28 21:38:34,645 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:38:34,952 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:38:35,599 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:38:35,601 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 21:38:35,734 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:39:09,883 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 21:39:09,958 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 21:39:10,780 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:39:10,781 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:39:11,056 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:39:11,134 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:39:11,134 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:39:11,134 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:39:11,134 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:39:11,134 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:39:11,134 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.42857142857142855,
  "recall": 0.015151515151515152,
  "score": 0.02926829268292683,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 21:39:12,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:12,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:13,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:14,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:15,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:16,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:16,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:17,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:17,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:18,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:18,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:19,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:20,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:21,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:21,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:22,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:22,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:23,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:24,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:24,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:25,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:26,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:26,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:37, 15.52s/it][WARNING|generation_utils.py:914] 2023-08-28 21:39:27,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:28,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:28,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:29,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:29,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:30,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:31,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:31,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:32,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:32,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:33,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:33,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:34,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:34,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:35,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:35,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:36,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:36,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:37,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:37,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:26<02:43, 12.60s/it][WARNING|generation_utils.py:914] 2023-08-28 21:39:38,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:38,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:39,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:40,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:41,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:41,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:42,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:43,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:44,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:44,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:45,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:45,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:46,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:47,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:47,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:48,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:48,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:49,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:50,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:51,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:51,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:39<02:38, 13.18s/it][WARNING|generation_utils.py:914] 2023-08-28 21:39:52,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:52,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:53,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:54,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:55,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:55,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:56,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:57,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:58,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:58,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:59,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:59,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:00,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:01,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:02,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:02,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:03,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:03,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:04,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:04,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:05,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:06,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:54<02:30, 13.70s/it][WARNING|generation_utils.py:914] 2023-08-28 21:40:07,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:07,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:08,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:08,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:09,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:09,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:10,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:11,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:11,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:11,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:12,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:13,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:13,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:14,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:14,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:15,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:15,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:16,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:16,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:17,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:18,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:06<02:10, 13.01s/it][WARNING|generation_utils.py:914] 2023-08-28 21:40:18,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:19,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:19,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:20,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:20,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:21,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:21,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:22,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:22,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:23,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:24,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:24,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:25,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:25,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:26,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:26,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:27,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:27,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:28,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:29,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:29,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:30,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:30,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:19<01:56, 12.99s/it][WARNING|generation_utils.py:914] 2023-08-28 21:40:31,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:32,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:32,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:33,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:33,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:34,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:35,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:36,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:36,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:37,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:37,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:38,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:39,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:39,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:40,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:40,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:41,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:42,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:42,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:43,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:43,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:44,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:32<01:45, 13.16s/it][WARNING|generation_utils.py:914] 2023-08-28 21:40:45,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:45,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:46,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:47,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:48,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:48,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:49,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:50,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:51,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:52,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:53,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:53,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:54,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:55,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:55,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:56,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:57,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:57,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:58,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:58,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:59,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:00,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:48<01:37, 13.96s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:00,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:01,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:01,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:02,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:03,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:04,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:04,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:05,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:05,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:06,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:07,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:07,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:08,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:09,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:09,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:10,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:11,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:11,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:12,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:13,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:14,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:14,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:03<01:25, 14.23s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:15,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:16,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:16,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:17,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:18,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:19,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:19,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:20,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:21,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:21,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:22,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:23,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:23,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:24,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:25,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:25,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:26,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:27,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:27,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:28,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:29,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:17<01:10, 14.16s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:29,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:30,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:30,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:31,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:32,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:32,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:33,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:33,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:34,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:34,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:35,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:36,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:36,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:37,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:38,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:38,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:39,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:39,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:40,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:41,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:41,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:29<00:54, 13.73s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:42,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:43,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:43,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:44,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:45,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:45,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:46,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:47,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:47,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:48,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:49,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:50,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:50,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:51,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:52,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:52,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:53,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:54,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:54,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:55,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:56,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:44<00:41, 13.98s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:56,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:57,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:58,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:58,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:59,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:59,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:00,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:01,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:01,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:02,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:03,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:03,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:04,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:04,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:05,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:05,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:06,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:07,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:07,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:08,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:08,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:57<00:27, 13.54s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:09,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:09,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:10,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:11,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:11,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:12,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:12,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:13,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:14,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:14,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:15,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:16,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:16,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:17,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:18,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:18,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:19,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:19,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:20,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:20,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:21,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:09<00:13, 13.31s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:22,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:22,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:23,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:24,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:24,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:25,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:26,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:27,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:27,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:28,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:28,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:29,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:30,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:30,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:31,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:31,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:32,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:33,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:33,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:34,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:22<00:00, 13.14s/it]Generating: 100%|██████████| 15/15 [03:22<00:00, 13.50s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:52,326 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:52,328 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:52,329 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:52,329 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:52,329 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:42:54,992 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:42:55,063 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:42:55,776 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:42:57,342 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:42:57,342 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:43:01,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:43:01,791 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:43:01,791 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:43:01,791 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:43:01,791 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:43:03,597 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:43:03,598 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:43:04,317 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:43:04,703 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:43:04,764 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : genre . Context : Later in the year , the album was released by Columbia Records and was released by Universal Music Group in 2010 . Head Entity : Columbia Records , Tail Entity : anthology .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : genre .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 611, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9546875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : participant in .', 'success_rate': 0.8792613636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : participating team . Context : Following his promotion to the American League under New York Red Bulls Head Coach Fredy Montero , FC New York received two second place finishes , one in the 1996 MLS All - World Cup . Head Entity : 1996 MLS All - World Cup , Tail Entity : FC New York Red Bulls .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : participating team .', 'success_rate': 0.9002976190476191, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 627, 'raw': 736}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8519021739130435, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : father .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8664772727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.9047619047619048, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9300595238095238, 'errors': {''}}
['Relation : located in the administrative territorial entity . Context : The city is located in the municipality of Saint-Martin ( French : Saint-Martin de Saint-Martin ) in Saint - Marie ( French : Saint-Mémoire ) , on the municipality of Nantes . Head Entity : Saint - Martin de Saint - Martin de Saint-Mémoire , Tail Entity : Saint - Marie .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.9196428571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.9092261904761905, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.9047619047619048, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('German Military Academy', 'occupation', '', 'A later building was built as the headquarters for the German Military Academy in Dresden , where it was studied and developed until it merged with the German Academy for Military Sciences at Leipzig , in 1905 .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.94375, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/4_ext.jsonl'}}
estimate vocab size: 9009
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9109, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.53it/s]Extractor Estimating: 2it [00:01,  1.61it/s]Extractor Estimating: 3it [00:01,  1.56it/s]Extractor Estimating: 4it [00:02,  1.45it/s]Extractor Estimating: 5it [00:03,  1.46it/s]Extractor Estimating: 6it [00:04,  1.34it/s]Extractor Estimating: 7it [00:04,  1.41it/s]Extractor Estimating: 8it [00:05,  1.49it/s]Extractor Estimating: 9it [00:06,  1.54it/s]Extractor Estimating: 10it [00:06,  1.55it/s]Extractor Estimating: 11it [00:07,  1.33it/s]Extractor Estimating: 12it [00:08,  1.40it/s]Extractor Estimating: 13it [00:09,  1.38it/s]Extractor Estimating: 14it [00:09,  1.42it/s]Extractor Estimating: 15it [00:10,  1.46it/s]Extractor Estimating: 16it [00:10,  1.52it/s]Extractor Estimating: 17it [00:11,  1.57it/s]Extractor Estimating: 18it [00:12,  1.60it/s]Extractor Estimating: 19it [00:12,  1.49it/s]Extractor Estimating: 20it [00:13,  1.49it/s]Extractor Estimating: 21it [00:14,  1.55it/s]Extractor Estimating: 22it [00:14,  1.52it/s]Extractor Estimating: 23it [00:15,  1.47it/s]Extractor Estimating: 24it [00:16,  1.39it/s]Extractor Estimating: 25it [00:17,  1.45it/s]Extractor Estimating: 26it [00:17,  1.60it/s]Extractor Estimating: 27it [00:17,  1.71it/s]Extractor Estimating: 28it [00:18,  1.84it/s]Extractor Estimating: 29it [00:18,  2.00it/s]Extractor Estimating: 30it [00:19,  1.77it/s]Extractor Estimating: 31it [00:20,  1.83it/s]Extractor Estimating: 32it [00:20,  1.91it/s]Extractor Estimating: 33it [00:20,  1.96it/s]Extractor Estimating: 34it [00:21,  1.95it/s]Extractor Estimating: 35it [00:21,  2.07it/s]Extractor Estimating: 36it [00:22,  2.04it/s]Extractor Estimating: 37it [00:22,  2.04it/s]Extractor Estimating: 38it [00:23,  2.10it/s]Extractor Estimating: 39it [00:23,  2.08it/s]Extractor Estimating: 40it [00:24,  2.08it/s]Extractor Estimating: 41it [00:24,  2.12it/s]Extractor Estimating: 42it [00:25,  2.09it/s]Extractor Estimating: 43it [00:25,  1.95it/s]Extractor Estimating: 44it [00:26,  1.98it/s]Extractor Estimating: 45it [00:26,  1.86it/s]Extractor Estimating: 46it [00:27,  1.98it/s]Extractor Estimating: 47it [00:27,  2.08it/s]Extractor Estimating: 48it [00:28,  2.10it/s]Extractor Estimating: 49it [00:28,  1.84it/s]Extractor Estimating: 50it [00:29,  1.88it/s]Extractor Estimating: 51it [00:30,  1.86it/s]Extractor Estimating: 52it [00:30,  1.62it/s]Extractor Estimating: 53it [00:31,  1.60it/s]Extractor Estimating: 54it [00:32,  1.49it/s]Extractor Estimating: 55it [00:32,  1.57it/s]Extractor Estimating: 56it [00:33,  1.50it/s]Extractor Estimating: 57it [00:34,  1.52it/s]Extractor Estimating: 58it [00:35,  1.40it/s]Extractor Estimating: 59it [00:35,  1.36it/s]Extractor Estimating: 60it [00:36,  1.45it/s]Extractor Estimating: 61it [00:36,  1.54it/s]Extractor Estimating: 62it [00:37,  1.59it/s]Extractor Estimating: 63it [00:38,  1.62it/s]Extractor Estimating: 64it [00:38,  1.63it/s]Extractor Estimating: 65it [00:39,  1.66it/s]Extractor Estimating: 66it [00:39,  1.66it/s]Extractor Estimating: 67it [00:40,  1.65it/s]Extractor Estimating: 68it [00:41,  1.72it/s]Extractor Estimating: 69it [00:41,  1.67it/s]Extractor Estimating: 70it [00:42,  1.65it/s]Extractor Estimating: 71it [00:43,  1.59it/s]Extractor Estimating: 72it [00:43,  1.55it/s]Extractor Estimating: 73it [00:44,  1.56it/s]Extractor Estimating: 74it [00:44,  1.61it/s]Extractor Estimating: 75it [00:45,  1.63it/s]Extractor Estimating: 76it [00:46,  1.61it/s]Extractor Estimating: 77it [00:47,  1.38it/s]Extractor Estimating: 78it [00:47,  1.45it/s]Extractor Estimating: 79it [00:48,  1.48it/s]Extractor Estimating: 80it [00:48,  1.58it/s]Extractor Estimating: 81it [00:49,  1.61it/s]Extractor Estimating: 82it [00:50,  1.55it/s]Extractor Estimating: 83it [00:50,  1.57it/s]Extractor Estimating: 84it [00:51,  1.57it/s]Extractor Estimating: 85it [00:52,  1.51it/s]Extractor Estimating: 86it [00:52,  1.58it/s]Extractor Estimating: 87it [00:53,  1.53it/s]Extractor Estimating: 88it [00:53,  1.61it/s]Extractor Estimating: 89it [00:54,  1.70it/s]Extractor Estimating: 90it [00:55,  1.66it/s]Extractor Estimating: 91it [00:55,  1.64it/s]Extractor Estimating: 92it [00:56,  1.63it/s]Extractor Estimating: 93it [00:56,  1.65it/s]Extractor Estimating: 94it [00:57,  1.71it/s]Extractor Estimating: 95it [00:58,  1.66it/s]Extractor Estimating: 96it [00:58,  1.68it/s]Extractor Estimating: 97it [00:59,  1.69it/s]Extractor Estimating: 98it [00:59,  1.68it/s]Extractor Estimating: 99it [01:00,  1.70it/s]Extractor Estimating: 100it [01:01,  1.65it/s]Extractor Estimating: 101it [01:01,  1.70it/s]Extractor Estimating: 102it [01:02,  1.71it/s]Extractor Estimating: 103it [01:02,  1.74it/s]Extractor Estimating: 104it [01:03,  1.69it/s]Extractor Estimating: 105it [01:03,  1.76it/s]Extractor Estimating: 106it [01:04,  1.81it/s]Extractor Estimating: 107it [01:04,  1.82it/s]Extractor Estimating: 108it [01:05,  1.80it/s]Extractor Estimating: 109it [01:06,  1.81it/s]Extractor Estimating: 110it [01:06,  1.64it/s]Extractor Estimating: 111it [01:07,  1.78it/s]Extractor Estimating: 112it [01:07,  1.80it/s]Extractor Estimating: 113it [01:08,  1.78it/s]Extractor Estimating: 114it [01:08,  1.77it/s]Extractor Estimating: 115it [01:09,  1.82it/s]Extractor Estimating: 116it [01:10,  1.59it/s]Extractor Estimating: 117it [01:10,  1.67it/s]Extractor Estimating: 118it [01:11,  1.70it/s]Extractor Estimating: 119it [01:11,  1.77it/s]Extractor Estimating: 120it [01:12,  1.79it/s]Extractor Estimating: 121it [01:12,  1.84it/s]Extractor Estimating: 122it [01:13,  1.79it/s]Extractor Estimating: 123it [01:14,  1.53it/s]Extractor Estimating: 124it [01:14,  1.60it/s]Extractor Estimating: 125it [01:15,  1.64it/s]Extractor Estimating: 126it [01:16,  1.78it/s]Extractor Estimating: 127it [01:16,  1.84it/s]Extractor Estimating: 128it [01:16,  2.01it/s]Extractor Estimating: 129it [01:17,  1.96it/s]Extractor Estimating: 130it [01:17,  1.99it/s]Extractor Estimating: 131it [01:18,  2.05it/s]Extractor Estimating: 132it [01:18,  2.13it/s]Extractor Estimating: 133it [01:19,  2.09it/s]Extractor Estimating: 134it [01:19,  2.10it/s]Extractor Estimating: 135it [01:20,  2.10it/s]Extractor Estimating: 136it [01:20,  1.99it/s]Extractor Estimating: 137it [01:21,  1.96it/s]Extractor Estimating: 138it [01:21,  2.02it/s]Extractor Estimating: 139it [01:22,  2.03it/s]Extractor Estimating: 140it [01:22,  1.99it/s]Extractor Estimating: 141it [01:23,  2.04it/s]Extractor Estimating: 142it [01:23,  1.97it/s]Extractor Estimating: 143it [01:24,  2.03it/s]Extractor Estimating: 144it [01:24,  2.01it/s]Extractor Estimating: 145it [01:25,  2.02it/s]Extractor Estimating: 146it [01:25,  1.99it/s]Extractor Estimating: 147it [01:26,  1.99it/s]Extractor Estimating: 148it [01:26,  1.99it/s]Extractor Estimating: 149it [01:27,  2.06it/s]Extractor Estimating: 150it [01:27,  1.98it/s]Extractor Estimating: 151it [01:28,  1.98it/s]Extractor Estimating: 152it [01:29,  1.78it/s]Extractor Estimating: 153it [01:29,  1.86it/s]Extractor Estimating: 154it [01:30,  1.58it/s]Extractor Estimating: 155it [01:30,  1.69it/s]Extractor Estimating: 156it [01:31,  1.77it/s]Extractor Estimating: 157it [01:31,  1.90it/s]Extractor Estimating: 158it [01:32,  1.89it/s]Extractor Estimating: 159it [01:32,  1.90it/s]Extractor Estimating: 160it [01:33,  1.66it/s]Extractor Estimating: 161it [01:34,  1.71it/s]Extractor Estimating: 162it [01:34,  1.74it/s]Extractor Estimating: 163it [01:35,  1.78it/s]Extractor Estimating: 164it [01:35,  1.82it/s]Extractor Estimating: 165it [01:36,  1.87it/s]Extractor Estimating: 166it [01:36,  1.73it/s]Extractor Estimating: 167it [01:37,  1.77it/s]Extractor Estimating: 168it [01:37,  1.82it/s]Extractor Estimating: 169it [01:38,  1.93it/s]Extractor Estimating: 170it [01:38,  1.97it/s]Extractor Estimating: 171it [01:39,  1.97it/s]Extractor Estimating: 172it [01:40,  1.83it/s]Extractor Estimating: 173it [01:40,  1.84it/s]Extractor Estimating: 174it [01:41,  1.91it/s]Extractor Estimating: 175it [01:41,  1.90it/s]Extractor Estimating: 176it [01:42,  1.88it/s]Extractor Estimating: 177it [01:42,  1.68it/s]Extractor Estimating: 178it [01:43,  1.65it/s]Extractor Estimating: 179it [01:44,  1.65it/s]Extractor Estimating: 180it [01:44,  1.55it/s]Extractor Estimating: 181it [01:45,  1.58it/s]Extractor Estimating: 182it [01:45,  1.68it/s]Extractor Estimating: 183it [01:46,  1.59it/s]Extractor Estimating: 184it [01:47,  1.62it/s]Extractor Estimating: 185it [01:48,  1.41it/s]Extractor Estimating: 186it [01:48,  1.50it/s]Extractor Estimating: 187it [01:49,  1.52it/s]Extractor Estimating: 188it [01:49,  1.60it/s]Extractor Estimating: 189it [01:50,  1.68it/s]Extractor Estimating: 190it [01:51,  1.53it/s]Extractor Estimating: 191it [01:51,  1.56it/s]Extractor Estimating: 192it [01:52,  1.65it/s]Extractor Estimating: 193it [01:52,  1.67it/s]Extractor Estimating: 194it [01:53,  1.71it/s]Extractor Estimating: 195it [01:54,  1.63it/s]Extractor Estimating: 196it [01:54,  1.68it/s]Extractor Estimating: 197it [01:55,  1.68it/s]Extractor Estimating: 198it [01:55,  1.74it/s]Extractor Estimating: 199it [01:56,  1.71it/s]Extractor Estimating: 200it [01:57,  1.78it/s]Extractor Estimating: 201it [01:57,  1.63it/s]Extractor Estimating: 202it [01:58,  1.61it/s]Extractor Estimating: 203it [01:58,  1.64it/s]Extractor Estimating: 204it [01:59,  1.70it/s]Extractor Estimating: 205it [02:00,  1.67it/s]Extractor Estimating: 206it [02:01,  1.31it/s]Extractor Estimating: 207it [02:01,  1.39it/s]Extractor Estimating: 208it [02:02,  1.49it/s]Extractor Estimating: 209it [02:03,  1.50it/s]Extractor Estimating: 210it [02:03,  1.54it/s]Extractor Estimating: 211it [02:04,  1.39it/s]Extractor Estimating: 212it [02:05,  1.50it/s]Extractor Estimating: 213it [02:05,  1.54it/s]Extractor Estimating: 214it [02:06,  1.50it/s]Extractor Estimating: 215it [02:07,  1.51it/s]Extractor Estimating: 216it [02:07,  1.46it/s]Extractor Estimating: 217it [02:08,  1.51it/s]Extractor Estimating: 218it [02:09,  1.57it/s]Extractor Estimating: 219it [02:09,  1.56it/s]Extractor Estimating: 220it [02:10,  1.50it/s]Extractor Estimating: 221it [02:11,  1.41it/s]Extractor Estimating: 222it [02:11,  1.45it/s]Extractor Estimating: 223it [02:12,  1.49it/s]Extractor Estimating: 224it [02:13,  1.53it/s]Extractor Estimating: 225it [02:13,  1.56it/s]Extractor Estimating: 226it [02:14,  1.60it/s]Extractor Estimating: 227it [02:14,  1.63it/s]Extractor Estimating: 228it [02:15,  1.65it/s]Extractor Estimating: 229it [02:16,  1.70it/s]Extractor Estimating: 230it [02:16,  1.75it/s]Extractor Estimating: 231it [02:17,  1.82it/s]Extractor Estimating: 232it [02:17,  1.66it/s]Extractor Estimating: 233it [02:18,  1.76it/s]Extractor Estimating: 234it [02:18,  1.74it/s]Extractor Estimating: 235it [02:19,  1.75it/s]Extractor Estimating: 236it [02:20,  1.72it/s]Extractor Estimating: 237it [02:20,  1.69it/s]Extractor Estimating: 238it [02:21,  1.75it/s]Extractor Estimating: 239it [02:21,  1.75it/s]Extractor Estimating: 240it [02:22,  1.56it/s]Extractor Estimating: 241it [02:23,  1.63it/s]Extractor Estimating: 242it [02:23,  1.70it/s]Extractor Estimating: 243it [02:24,  1.70it/s]Extractor Estimating: 244it [02:24,  1.73it/s]Extractor Estimating: 245it [02:25,  1.63it/s]Extractor Estimating: 246it [02:26,  1.66it/s]Extractor Estimating: 247it [02:26,  1.67it/s]Extractor Estimating: 248it [02:27,  1.71it/s]Extractor Estimating: 249it [02:27,  1.66it/s]Extractor Estimating: 250it [02:28,  1.57it/s]Extractor Estimating: 251it [02:29,  1.59it/s]Extractor Estimating: 252it [02:29,  1.59it/s]Extractor Estimating: 253it [02:30,  1.58it/s]Extractor Estimating: 254it [02:31,  1.60it/s]Extractor Estimating: 255it [02:32,  1.28it/s]Extractor Estimating: 256it [02:32,  1.34it/s]Extractor Estimating: 257it [02:33,  1.44it/s]Extractor Estimating: 258it [02:33,  1.52it/s]Extractor Estimating: 259it [02:34,  1.54it/s]Extractor Estimating: 260it [02:35,  1.46it/s]Extractor Estimating: 261it [02:35,  1.55it/s]Extractor Estimating: 262it [02:36,  1.57it/s]Extractor Estimating: 263it [02:37,  1.58it/s]Extractor Estimating: 264it [02:37,  1.63it/s]Extractor Estimating: 265it [02:38,  1.52it/s]Extractor Estimating: 266it [02:39,  1.41it/s]Extractor Estimating: 267it [02:39,  1.53it/s]Extractor Estimating: 268it [02:40,  1.56it/s]Extractor Estimating: 269it [02:41,  1.62it/s]Extractor Estimating: 270it [02:41,  1.62it/s]Extractor Estimating: 271it [02:42,  1.65it/s]Extractor Estimating: 272it [02:42,  1.66it/s]Extractor Estimating: 273it [02:43,  1.69it/s]Extractor Estimating: 274it [02:44,  1.66it/s]Extractor Estimating: 275it [02:44,  1.69it/s]Extractor Estimating: 276it [02:45,  1.76it/s]Extractor Estimating: 277it [02:45,  1.75it/s]Extractor Estimating: 278it [02:46,  1.63it/s]Extractor Estimating: 279it [02:46,  1.65it/s]Extractor Estimating: 280it [02:47,  1.71it/s]Extractor Estimating: 281it [02:47,  1.81it/s]Extractor Estimating: 282it [02:48,  1.81it/s]Extractor Estimating: 283it [02:49,  1.77it/s]Extractor Estimating: 284it [02:49,  1.70it/s]Extractor Estimating: 285it [02:50,  1.76it/s]Extractor Estimating: 286it [02:50,  1.74it/s]Extractor Estimating: 287it [02:51,  1.72it/s]Extractor Estimating: 288it [02:52,  1.77it/s]Extractor Estimating: 289it [02:52,  1.88it/s]Extractor Estimating: 290it [02:53,  1.61it/s]Extractor Estimating: 291it [02:53,  1.67it/s]Extractor Estimating: 292it [02:54,  1.66it/s]Extractor Estimating: 293it [02:54,  1.73it/s]Extractor Estimating: 294it [02:55,  1.78it/s]Extractor Estimating: 295it [02:56,  1.68it/s]Extractor Estimating: 296it [02:56,  1.77it/s]Extractor Estimating: 297it [02:57,  1.76it/s]Extractor Estimating: 298it [02:57,  1.80it/s]Extractor Estimating: 299it [02:58,  1.82it/s]Extractor Estimating: 300it [02:58,  1.82it/s]Extractor Estimating: 301it [02:59,  1.73it/s]Extractor Estimating: 302it [03:00,  1.74it/s]Extractor Estimating: 303it [03:00,  1.74it/s]Extractor Estimating: 304it [03:01,  1.84it/s]Extractor Estimating: 305it [03:01,  1.89it/s]Extractor Estimating: 306it [03:02,  1.91it/s]Extractor Estimating: 307it [03:02,  1.74it/s]Extractor Estimating: 308it [03:03,  1.85it/s]Extractor Estimating: 309it [03:03,  1.78it/s]Extractor Estimating: 310it [03:04,  1.70it/s]Extractor Estimating: 311it [03:05,  1.71it/s]Extractor Estimating: 312it [03:05,  1.67it/s]Extractor Estimating: 313it [03:06,  1.64it/s]Extractor Estimating: 314it [03:06,  1.72it/s]Extractor Estimating: 315it [03:07,  1.71it/s]Extractor Estimating: 316it [03:08,  1.74it/s]Extractor Estimating: 317it [03:08,  1.68it/s]Extractor Estimating: 318it [03:09,  1.72it/s]Extractor Estimating: 319it [03:09,  1.56it/s]Extractor Estimating: 320it [03:10,  1.59it/s]Extractor Estimating: 321it [03:11,  1.54it/s]Extractor Estimating: 322it [03:11,  1.60it/s]Extractor Estimating: 323it [03:12,  1.69it/s]Extractor Estimating: 324it [03:12,  1.66it/s]Extractor Estimating: 325it [03:13,  1.71it/s]Extractor Estimating: 326it [03:14,  1.71it/s]Extractor Estimating: 327it [03:14,  1.74it/s]Extractor Estimating: 328it [03:15,  1.73it/s]Extractor Estimating: 329it [03:15,  1.81it/s]Extractor Estimating: 330it [03:16,  1.80it/s]Extractor Estimating: 331it [03:16,  1.70it/s]Extractor Estimating: 332it [03:17,  1.69it/s]Extractor Estimating: 333it [03:18,  1.75it/s]Extractor Estimating: 334it [03:18,  1.78it/s]Extractor Estimating: 335it [03:19,  1.77it/s]Extractor Estimating: 336it [03:19,  1.80it/s]Extractor Estimating: 337it [03:20,  1.54it/s]Extractor Estimating: 338it [03:21,  1.58it/s]Extractor Estimating: 339it [03:21,  1.62it/s]Extractor Estimating: 340it [03:22,  1.63it/s]Extractor Estimating: 341it [03:23,  1.63it/s]Extractor Estimating: 342it [03:23,  1.66it/s]Extractor Estimating: 343it [03:24,  1.70it/s]Extractor Estimating: 344it [03:24,  1.71it/s]Extractor Estimating: 345it [03:25,  1.76it/s]Extractor Estimating: 346it [03:25,  1.80it/s]Extractor Estimating: 347it [03:26,  1.82it/s]Extractor Estimating: 348it [03:27,  1.49it/s]Extractor Estimating: 349it [03:27,  1.58it/s]Extractor Estimating: 350it [03:28,  1.55it/s]Extractor Estimating: 351it [03:29,  1.59it/s]Extractor Estimating: 352it [03:29,  1.61it/s]Extractor Estimating: 353it [03:30,  1.38it/s]Extractor Estimating: 354it [03:31,  1.48it/s]Extractor Estimating: 355it [03:31,  1.59it/s]Extractor Estimating: 356it [03:32,  1.62it/s]Extractor Estimating: 357it [03:32,  1.63it/s]Extractor Estimating: 358it [03:33,  1.60it/s]Extractor Estimating: 359it [03:34,  1.66it/s]Extractor Estimating: 360it [03:34,  1.72it/s]Extractor Estimating: 361it [03:35,  1.73it/s]Extractor Estimating: 362it [03:35,  1.78it/s]Extractor Estimating: 363it [03:36,  1.74it/s]Extractor Estimating: 364it [03:37,  1.49it/s]Extractor Estimating: 365it [03:37,  1.60it/s]Extractor Estimating: 366it [03:38,  1.60it/s]Extractor Estimating: 367it [03:39,  1.60it/s]Extractor Estimating: 368it [03:39,  1.57it/s]Extractor Estimating: 369it [03:40,  1.38it/s]Extractor Estimating: 370it [03:41,  1.47it/s]Extractor Estimating: 371it [03:41,  1.53it/s]Extractor Estimating: 372it [03:42,  1.63it/s]Extractor Estimating: 373it [03:42,  1.61it/s]Extractor Estimating: 374it [03:43,  1.56it/s]Extractor Estimating: 375it [03:44,  1.73it/s]Extractor Estimating: 375it [03:44,  1.67it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:17,344 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:17,449 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:17,449 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:17,450 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:17,450 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:47:18,981 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:47:18,983 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:47:20,566 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:47:22,242 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:47:22,242 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:30,777 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:31,058 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:31,058 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:31,058 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:31,058 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:47:32,980 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:47:32,982 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:47:33,946 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:47:34,367 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:47:34,368 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 23:55:14,504 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 23:55:14,775 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7492 mean pseudo reward: 0.9462812734070469
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
train vocab size: 15998
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16098, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16098, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.968, loss:676.7842
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.956, loss:622.5970
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.951, loss:545.0064
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 0.968, loss:551.2339
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 0.954, loss:546.7300
>> valid entity prec:0.5501, rec:0.5612, f1:0.5556
>> valid relation prec:0.2100, rec:0.0990, f1:0.1346
>> valid relation with NER prec:0.2100, rec:0.0990, f1:0.1346
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.287, loss:559.5836
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 0.958, loss:504.9283
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 0.963, loss:507.2633
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 0.977, loss:543.3013
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 0.972, loss:513.2331
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5628, rec:0.5127, f1:0.5366
>> valid relation prec:0.2018, rec:0.1036, f1:0.1369
>> valid relation with NER prec:0.2018, rec:0.1036, f1:0.1369
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.249, loss:504.2654
g_step 1200, step 261, avg_time 0.962, loss:516.4837
g_step 1300, step 48, avg_time 0.944, loss:474.4456
g_step 1400, step 148, avg_time 0.972, loss:474.7466
g_step 1500, step 248, avg_time 0.975, loss:487.3780
>> valid entity prec:0.5070, rec:0.4949, f1:0.5009
>> valid relation prec:0.1660, rec:0.0887, f1:0.1156
>> valid relation with NER prec:0.1660, rec:0.0887, f1:0.1156
g_step 1600, step 35, avg_time 2.215, loss:466.6134
g_step 1700, step 135, avg_time 0.965, loss:446.2654
g_step 1800, step 235, avg_time 0.966, loss:449.9934
g_step 1900, step 22, avg_time 0.975, loss:468.6057
g_step 2000, step 122, avg_time 0.970, loss:431.5536
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5310, rec:0.5287, f1:0.5299
>> valid relation prec:0.1917, rec:0.0999, f1:0.1313
>> valid relation with NER prec:0.1917, rec:0.0999, f1:0.1313
g_step 2100, step 222, avg_time 2.232, loss:424.2097
g_step 2200, step 9, avg_time 0.962, loss:446.6209
g_step 2300, step 109, avg_time 0.966, loss:389.1950
g_step 2400, step 209, avg_time 0.977, loss:425.5312
g_step 2500, step 309, avg_time 0.955, loss:426.5678
>> valid entity prec:0.5278, rec:0.5217, f1:0.5247
>> valid relation prec:0.1883, rec:0.1145, f1:0.1424
>> valid relation with NER prec:0.1883, rec:0.1145, f1:0.1424
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 96, avg_time 2.227, loss:374.6803
g_step 2700, step 196, avg_time 0.979, loss:391.2314
g_step 2800, step 296, avg_time 0.987, loss:402.2030
g_step 2900, step 83, avg_time 0.955, loss:359.8318
g_step 3000, step 183, avg_time 0.976, loss:388.1378
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5006, rec:0.4973, f1:0.4989
>> valid relation prec:0.1655, rec:0.0976, f1:0.1228
>> valid relation with NER prec:0.1655, rec:0.0976, f1:0.1228
g_step 3100, step 283, avg_time 2.249, loss:389.4389
g_step 3200, step 70, avg_time 0.949, loss:338.4477
g_step 3300, step 170, avg_time 0.979, loss:337.5336
g_step 3400, step 270, avg_time 0.978, loss:373.2462
g_step 3500, step 57, avg_time 0.976, loss:342.4821
>> valid entity prec:0.5269, rec:0.4851, f1:0.5052
>> valid relation prec:0.1737, rec:0.0999, f1:0.1268
>> valid relation with NER prec:0.1737, rec:0.0999, f1:0.1268
g_step 3600, step 157, avg_time 2.240, loss:345.3930
g_step 3700, step 257, avg_time 0.995, loss:353.7942
g_step 3800, step 44, avg_time 0.978, loss:333.7224
g_step 3900, step 144, avg_time 0.962, loss:328.8153
g_step 4000, step 244, avg_time 1.002, loss:351.3680
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5313, rec:0.5594, f1:0.5450
>> valid relation prec:0.2195, rec:0.1294, f1:0.1628
>> valid relation with NER prec:0.2195, rec:0.1294, f1:0.1628
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4100, step 31, avg_time 2.253, loss:336.5198
g_step 4200, step 131, avg_time 0.994, loss:311.4136
g_step 4300, step 231, avg_time 0.961, loss:333.6058
g_step 4400, step 18, avg_time 0.968, loss:315.0297
g_step 4500, step 118, avg_time 0.986, loss:288.4863
>> valid entity prec:0.5111, rec:0.4802, f1:0.4952
>> valid relation prec:0.1852, rec:0.1091, f1:0.1373
>> valid relation with NER prec:0.1852, rec:0.1091, f1:0.1373
g_step 4600, step 218, avg_time 2.258, loss:320.7752
g_step 4700, step 5, avg_time 0.968, loss:315.0276
g_step 4800, step 105, avg_time 0.982, loss:272.2373
g_step 4900, step 205, avg_time 0.968, loss:320.2380
g_step 5000, step 305, avg_time 0.998, loss:320.8561
learning rate was adjusted to 0.0008
>> valid entity prec:0.5565, rec:0.5053, f1:0.5297
>> valid relation prec:0.1949, rec:0.1056, f1:0.1370
>> valid relation with NER prec:0.1949, rec:0.1056, f1:0.1370
g_step 5100, step 92, avg_time 2.260, loss:274.6006
g_step 5200, step 192, avg_time 1.002, loss:303.6109
g_step 5300, step 292, avg_time 0.984, loss:291.3586
g_step 5400, step 79, avg_time 0.987, loss:252.3993
g_step 5500, step 179, avg_time 0.985, loss:291.7978
>> valid entity prec:0.5274, rec:0.5107, f1:0.5189
>> valid relation prec:0.1906, rec:0.1257, f1:0.1515
>> valid relation with NER prec:0.1906, rec:0.1257, f1:0.1515
g_step 5600, step 279, avg_time 2.291, loss:289.5042
g_step 5700, step 66, avg_time 1.014, loss:266.8407
g_step 5800, step 166, avg_time 1.007, loss:262.3818
g_step 5900, step 266, avg_time 0.998, loss:286.3054
g_step 6000, step 53, avg_time 0.988, loss:256.1047
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5111, rec:0.4843, f1:0.4973
>> valid relation prec:0.1738, rec:0.1062, f1:0.1318
>> valid relation with NER prec:0.1738, rec:0.1062, f1:0.1318
g_step 6100, step 153, avg_time 2.310, loss:269.2673
g_step 6200, step 253, avg_time 1.023, loss:257.3881
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 23:55:14 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 23:55:14 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_23-55-14_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 23:55:16 - WARNING - datasets.builder -   Using custom data configuration default-fa8a508a1c0c4b1e
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-fa8a508a1c0c4b1e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 23:55:18,282 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:55:18,305 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:55:18,306 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:55:18,307 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:55:18,381 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:55:18,421 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:55:18,421 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:55:18,422 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:55:18,422 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:55:18,422 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:55:18,422 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 23:55:18,737 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:55:21,950 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 23:55:21,950 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-fa8a508a1c0c4b1e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.70ba/s] 25%|██▌       | 2/8 [00:00<00:02,  2.71ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.45ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.94ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.24ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.49ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.63ba/s]100%|██████████| 8/8 [00:01<00:00,  5.58ba/s]100%|██████████| 8/8 [00:01<00:00,  4.36ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.88ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.67ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.02ba/s]100%|██████████| 4/4 [00:00<00:00,  5.14ba/s]100%|██████████| 4/4 [00:00<00:00,  4.45ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  4.20ba/s] 38%|███▊      | 3/8 [00:00<00:00,  7.70ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.07ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  9.85ba/s]100%|██████████| 8/8 [00:00<00:00,  9.61ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  6.44ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.20ba/s]100%|██████████| 4/4 [00:00<00:00, 10.35ba/s]
[INFO|trainer.py:414] 2023-08-28 23:55:27,464 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 23:55:27,549 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 23:55:27,549 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-28 23:55:27,549 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 23:55:27,549 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 23:55:27,550 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 23:55:27,550 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 23:55:27,550 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:55,  3.33it/s]  0%|          | 2/585 [00:00<02:51,  3.40it/s]  1%|          | 3/585 [00:00<02:49,  3.43it/s]  1%|          | 4/585 [00:01<02:56,  3.30it/s]  1%|          | 5/585 [00:01<02:53,  3.34it/s]  1%|          | 6/585 [00:01<02:52,  3.36it/s]  1%|          | 7/585 [00:02<02:51,  3.37it/s]  1%|▏         | 8/585 [00:02<02:50,  3.38it/s]  2%|▏         | 9/585 [00:02<02:49,  3.39it/s]  2%|▏         | 10/585 [00:02<02:49,  3.40it/s]  2%|▏         | 11/585 [00:03<02:49,  3.39it/s]  2%|▏         | 12/585 [00:03<02:48,  3.40it/s]  2%|▏         | 13/585 [00:03<02:48,  3.40it/s]  2%|▏         | 14/585 [00:04<02:47,  3.40it/s]  3%|▎         | 15/585 [00:04<02:53,  3.29it/s]  3%|▎         | 16/585 [00:04<02:51,  3.32it/s]  3%|▎         | 17/585 [00:05<02:49,  3.34it/s]  3%|▎         | 18/585 [00:05<02:48,  3.36it/s]  3%|▎         | 19/585 [00:05<02:47,  3.38it/s]  3%|▎         | 20/585 [00:05<02:47,  3.38it/s]  4%|▎         | 21/585 [00:06<02:46,  3.39it/s]  4%|▍         | 22/585 [00:06<02:45,  3.39it/s]  4%|▍         | 23/585 [00:06<02:45,  3.39it/s]  4%|▍         | 24/585 [00:07<02:45,  3.39it/s]  4%|▍         | 25/585 [00:07<02:45,  3.39it/s]  4%|▍         | 26/585 [00:07<02:53,  3.23it/s]  5%|▍         | 27/585 [00:08<02:50,  3.28it/s]  5%|▍         | 28/585 [00:08<02:48,  3.32it/s]  5%|▍         | 29/585 [00:08<02:46,  3.34it/s]  5%|▌         | 30/585 [00:08<02:45,  3.35it/s]  5%|▌         | 31/585 [00:09<02:44,  3.37it/s]  5%|▌         | 32/585 [00:09<02:43,  3.38it/s]  6%|▌         | 33/585 [00:09<02:43,  3.38it/s]  6%|▌         | 34/585 [00:10<02:42,  3.39it/s]  6%|▌         | 35/585 [00:10<02:41,  3.40it/s]  6%|▌         | 36/585 [00:10<02:41,  3.40it/s]  6%|▋         | 37/585 [00:11<02:49,  3.23it/s]  6%|▋         | 38/585 [00:11<02:47,  3.27it/s]  7%|▋         | 39/585 [00:11<02:44,  3.31it/s]  7%|▋         | 40/585 [00:11<02:43,  3.34it/s]  7%|▋         | 41/585 [00:12<02:42,  3.36it/s]  7%|▋         | 42/585 [00:12<02:41,  3.37it/s]  7%|▋         | 43/585 [00:12<02:40,  3.38it/s]  8%|▊         | 44/585 [00:13<02:39,  3.39it/s]  8%|▊         | 45/585 [00:13<02:39,  3.39it/s]  8%|▊         | 46/585 [00:13<02:38,  3.39it/s]  8%|▊         | 47/585 [00:13<02:38,  3.40it/s]  8%|▊         | 48/585 [00:14<02:44,  3.26it/s]  8%|▊         | 49/585 [00:14<02:42,  3.30it/s]  9%|▊         | 50/585 [00:14<02:40,  3.33it/s]  9%|▊         | 51/585 [00:15<02:39,  3.35it/s]  9%|▉         | 52/585 [00:15<02:38,  3.36it/s]  9%|▉         | 53/585 [00:15<02:37,  3.37it/s]  9%|▉         | 54/585 [00:16<02:36,  3.38it/s]  9%|▉         | 55/585 [00:16<02:36,  3.39it/s] 10%|▉         | 56/585 [00:16<02:36,  3.39it/s] 10%|▉         | 57/585 [00:16<02:35,  3.40it/s] 10%|▉         | 58/585 [00:17<02:35,  3.40it/s] 10%|█         | 59/585 [00:17<02:45,  3.19it/s] 10%|█         | 60/585 [00:17<02:41,  3.24it/s] 10%|█         | 61/585 [00:18<02:39,  3.29it/s] 11%|█         | 62/585 [00:18<02:37,  3.32it/s] 11%|█         | 63/585 [00:18<02:36,  3.34it/s] 11%|█         | 64/585 [00:19<02:35,  3.36it/s] 11%|█         | 65/585 [00:19<02:34,  3.37it/s] 11%|█▏        | 66/585 [00:19<02:33,  3.38it/s] 11%|█▏        | 67/585 [00:19<02:33,  3.38it/s] 12%|█▏        | 68/585 [00:20<02:32,  3.39it/s] 12%|█▏        | 69/585 [00:20<02:42,  3.18it/s] 12%|█▏        | 70/585 [00:20<02:38,  3.25it/s] 12%|█▏        | 71/585 [00:21<02:36,  3.29it/s] 12%|█▏        | 72/585 [00:21<02:34,  3.32it/s] 12%|█▏        | 73/585 [00:21<02:32,  3.36it/s] 13%|█▎        | 74/585 [00:22<02:30,  3.39it/s] 13%|█▎        | 75/585 [00:22<02:29,  3.40it/s] 13%|█▎        | 76/585 [00:22<02:29,  3.41it/s] 13%|█▎        | 77/585 [00:22<02:28,  3.42it/s] 13%|█▎        | 78/585 [00:23<02:27,  3.43it/s] 14%|█▎        | 79/585 [00:23<02:27,  3.43it/s] 14%|█▎        | 80/585 [00:23<02:26,  3.44it/s] 14%|█▍        | 81/585 [00:24<02:26,  3.44it/s] 14%|█▍        | 82/585 [00:24<02:26,  3.44it/s] 14%|█▍        | 83/585 [00:24<02:25,  3.44it/s] 14%|█▍        | 84/585 [00:24<02:25,  3.45it/s] 15%|█▍        | 85/585 [00:25<02:25,  3.45it/s] 15%|█▍        | 86/585 [00:25<02:24,  3.45it/s] 15%|█▍        | 87/585 [00:25<02:28,  3.35it/s] 15%|█▌        | 88/585 [00:26<02:27,  3.37it/s] 15%|█▌        | 89/585 [00:26<02:26,  3.40it/s] 15%|█▌        | 90/585 [00:26<02:25,  3.41it/s] 16%|█▌        | 91/585 [00:27<02:24,  3.42it/s] 16%|█▌        | 92/585 [00:27<02:23,  3.43it/s] 16%|█▌        | 93/585 [00:27<02:23,  3.43it/s] 16%|█▌        | 94/585 [00:27<02:22,  3.44it/s] 16%|█▌        | 95/585 [00:28<02:22,  3.44it/s] 16%|█▋        | 96/585 [00:28<02:22,  3.44it/s] 17%|█▋        | 97/585 [00:28<02:21,  3.44it/s] 17%|█▋        | 98/585 [00:29<02:26,  3.33it/s] 17%|█▋        | 99/585 [00:29<02:24,  3.36it/s] 17%|█▋        | 100/585 [00:29<02:23,  3.39it/s] 17%|█▋        | 101/585 [00:29<02:22,  3.40it/s] 17%|█▋        | 102/585 [00:30<02:21,  3.42it/s] 18%|█▊        | 103/585 [00:30<02:20,  3.42it/s] 18%|█▊        | 104/585 [00:30<02:20,  3.43it/s] 18%|█▊        | 105/585 [00:31<02:19,  3.43it/s] 18%|█▊        | 106/585 [00:31<02:19,  3.44it/s] 18%|█▊        | 107/585 [00:31<02:19,  3.44it/s] 18%|█▊        | 108/585 [00:32<02:18,  3.44it/s] 19%|█▊        | 109/585 [00:32<02:21,  3.37it/s] 19%|█▉        | 110/585 [00:32<02:19,  3.40it/s] 19%|█▉        | 111/585 [00:32<02:19,  3.41it/s] 19%|█▉        | 112/585 [00:33<02:18,  3.42it/s] 19%|█▉        | 113/585 [00:33<02:17,  3.43it/s] 19%|█▉        | 114/585 [00:33<02:17,  3.43it/s] 20%|█▉        | 115/585 [00:34<02:16,  3.44it/s] 20%|█▉        | 116/585 [00:34<02:16,  3.44it/s] 20%|██        | 117/585 [00:34<02:16,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 23:56:02,242 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:56:02,242 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 23:56:02,242 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.79it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.49it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.38it/s][A
  5%|▌         | 22/436 [00:00<00:09, 42.35it/s][A
  6%|▌         | 27/436 [00:00<00:09, 43.28it/s][A
  7%|▋         | 32/436 [00:00<00:09, 43.86it/s][A
  8%|▊         | 37/436 [00:00<00:09, 44.01it/s][A
 10%|▉         | 42/436 [00:00<00:08, 43.97it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.06it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 43.12it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 43.42it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 43.74it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 43.81it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.29it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.48it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.47it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.58it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.30it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.49it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.66it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.60it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.75it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.76it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.77it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.69it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.71it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.49it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.52it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.53it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.45it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 40.86it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 42.16it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.07it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.58it/s][A
 41%|████      | 177/436 [00:04<00:06, 39.62it/s][A
 42%|████▏     | 182/436 [00:04<00:06, 41.03it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 42.24it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 42.87it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 43.24it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 43.65it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.05it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.36it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.29it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.17it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.30it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.46it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.48it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.31it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.34it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.74it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.46it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.45it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.41it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.56it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.71it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.73it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.77it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 43.17it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 43.74it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 41.30it/s][A
 70%|███████   | 307/436 [00:06<00:03, 42.36it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 43.24it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 43.50it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 43.71it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 43.86it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.08it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.37it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 43.98it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.23it/s][A
 81%|████████  | 352/436 [00:08<00:01, 44.51it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.50it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.76it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.74it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.74it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.69it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.47it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.29it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.49it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 44.47it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.59it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.77it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.75it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.70it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.53it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.07it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.12it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.12it/s][A 20%|██        | 117/585 [00:44<02:16,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:56:12,327 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 23:56:12,504 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:56:15,829 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:56:16,119 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:56:16,236 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:56<52:03,  6.69s/it] 20%|██        | 119/585 [00:56<37:08,  4.78s/it] 21%|██        | 120/585 [00:56<26:37,  3.44s/it] 21%|██        | 121/585 [00:57<19:16,  2.49s/it] 21%|██        | 122/585 [00:57<14:08,  1.83s/it] 21%|██        | 123/585 [00:57<10:33,  1.37s/it] 21%|██        | 124/585 [00:58<08:03,  1.05s/it] 21%|██▏       | 125/585 [00:58<06:18,  1.22it/s] 22%|██▏       | 126/585 [00:58<05:04,  1.51it/s] 22%|██▏       | 127/585 [00:58<04:13,  1.81it/s] 22%|██▏       | 128/585 [00:59<03:36,  2.11it/s] 22%|██▏       | 129/585 [00:59<03:11,  2.38it/s] 22%|██▏       | 130/585 [00:59<02:57,  2.57it/s] 22%|██▏       | 131/585 [01:00<02:43,  2.78it/s] 23%|██▎       | 132/585 [01:00<02:33,  2.95it/s] 23%|██▎       | 133/585 [01:00<02:26,  3.09it/s] 23%|██▎       | 134/585 [01:01<02:21,  3.19it/s] 23%|██▎       | 135/585 [01:01<02:18,  3.26it/s] 23%|██▎       | 136/585 [01:01<02:15,  3.31it/s] 23%|██▎       | 137/585 [01:01<02:13,  3.35it/s] 24%|██▎       | 138/585 [01:02<02:12,  3.38it/s] 24%|██▍       | 139/585 [01:02<02:11,  3.40it/s] 24%|██▍       | 140/585 [01:02<02:10,  3.41it/s] 24%|██▍       | 141/585 [01:03<02:11,  3.37it/s] 24%|██▍       | 142/585 [01:03<02:10,  3.40it/s] 24%|██▍       | 143/585 [01:03<02:09,  3.41it/s] 25%|██▍       | 144/585 [01:03<02:08,  3.43it/s] 25%|██▍       | 145/585 [01:04<02:08,  3.43it/s] 25%|██▍       | 146/585 [01:04<02:07,  3.44it/s] 25%|██▌       | 147/585 [01:04<02:07,  3.44it/s] 25%|██▌       | 148/585 [01:05<02:06,  3.44it/s] 25%|██▌       | 149/585 [01:05<02:06,  3.44it/s] 26%|██▌       | 150/585 [01:05<02:06,  3.45it/s] 26%|██▌       | 151/585 [01:05<02:05,  3.45it/s] 26%|██▌       | 152/585 [01:06<02:09,  3.34it/s] 26%|██▌       | 153/585 [01:06<02:08,  3.37it/s] 26%|██▋       | 154/585 [01:06<02:07,  3.39it/s] 26%|██▋       | 155/585 [01:07<02:06,  3.41it/s] 27%|██▋       | 156/585 [01:07<02:05,  3.42it/s] 27%|██▋       | 157/585 [01:07<02:04,  3.43it/s] 27%|██▋       | 158/585 [01:08<02:04,  3.44it/s] 27%|██▋       | 159/585 [01:08<02:03,  3.44it/s] 27%|██▋       | 160/585 [01:08<02:03,  3.44it/s] 28%|██▊       | 161/585 [01:08<02:03,  3.44it/s] 28%|██▊       | 162/585 [01:09<02:02,  3.45it/s] 28%|██▊       | 163/585 [01:09<02:06,  3.34it/s] 28%|██▊       | 164/585 [01:09<02:04,  3.37it/s] 28%|██▊       | 165/585 [01:10<02:03,  3.39it/s] 28%|██▊       | 166/585 [01:10<02:02,  3.41it/s] 29%|██▊       | 167/585 [01:10<02:02,  3.42it/s] 29%|██▊       | 168/585 [01:10<02:01,  3.43it/s] 29%|██▉       | 169/585 [01:11<02:00,  3.44it/s] 29%|██▉       | 170/585 [01:11<02:00,  3.45it/s] 29%|██▉       | 171/585 [01:11<02:00,  3.45it/s] 29%|██▉       | 172/585 [01:12<01:59,  3.45it/s] 30%|██▉       | 173/585 [01:12<01:59,  3.45it/s] 30%|██▉       | 174/585 [01:12<02:07,  3.21it/s] 30%|██▉       | 175/585 [01:13<02:04,  3.28it/s] 30%|███       | 176/585 [01:13<02:02,  3.33it/s] 30%|███       | 177/585 [01:13<02:01,  3.37it/s] 30%|███       | 178/585 [01:13<02:00,  3.39it/s] 31%|███       | 179/585 [01:14<01:59,  3.41it/s] 31%|███       | 180/585 [01:14<01:58,  3.42it/s] 31%|███       | 181/585 [01:14<01:57,  3.43it/s] 31%|███       | 182/585 [01:15<01:57,  3.43it/s] 31%|███▏      | 183/585 [01:15<01:56,  3.44it/s] 31%|███▏      | 184/585 [01:15<01:56,  3.45it/s] 32%|███▏      | 185/585 [01:15<02:02,  3.26it/s] 32%|███▏      | 186/585 [01:16<02:00,  3.31it/s] 32%|███▏      | 187/585 [01:16<01:58,  3.35it/s] 32%|███▏      | 188/585 [01:16<01:57,  3.39it/s] 32%|███▏      | 189/585 [01:17<01:56,  3.41it/s] 32%|███▏      | 190/585 [01:17<01:55,  3.42it/s] 33%|███▎      | 191/585 [01:17<01:54,  3.43it/s] 33%|███▎      | 192/585 [01:18<01:54,  3.44it/s] 33%|███▎      | 193/585 [01:18<01:53,  3.44it/s] 33%|███▎      | 194/585 [01:18<01:53,  3.45it/s] 33%|███▎      | 195/585 [01:18<01:53,  3.45it/s] 34%|███▎      | 196/585 [01:19<01:58,  3.27it/s] 34%|███▎      | 197/585 [01:19<01:56,  3.32it/s] 34%|███▍      | 198/585 [01:19<01:55,  3.36it/s] 34%|███▍      | 199/585 [01:20<01:53,  3.39it/s] 34%|███▍      | 200/585 [01:20<01:53,  3.41it/s] 34%|███▍      | 201/585 [01:20<01:52,  3.42it/s] 35%|███▍      | 202/585 [01:20<01:51,  3.43it/s] 35%|███▍      | 203/585 [01:21<01:51,  3.44it/s] 35%|███▍      | 204/585 [01:21<01:50,  3.44it/s] 35%|███▌      | 205/585 [01:21<01:50,  3.44it/s] 35%|███▌      | 206/585 [01:22<01:50,  3.44it/s] 35%|███▌      | 207/585 [01:22<01:57,  3.22it/s] 36%|███▌      | 208/585 [01:22<01:54,  3.29it/s] 36%|███▌      | 209/585 [01:23<01:52,  3.34it/s] 36%|███▌      | 210/585 [01:23<01:51,  3.37it/s] 36%|███▌      | 211/585 [01:23<01:50,  3.39it/s] 36%|███▌      | 212/585 [01:23<01:49,  3.41it/s] 36%|███▋      | 213/585 [01:24<01:48,  3.42it/s] 37%|███▋      | 214/585 [01:24<01:48,  3.43it/s] 37%|███▋      | 215/585 [01:24<01:47,  3.43it/s] 37%|███▋      | 216/585 [01:25<01:47,  3.44it/s] 37%|███▋      | 217/585 [01:25<01:47,  3.44it/s] 37%|███▋      | 218/585 [01:25<01:46,  3.44it/s] 37%|███▋      | 219/585 [01:25<01:46,  3.44it/s] 38%|███▊      | 220/585 [01:26<01:45,  3.45it/s] 38%|███▊      | 221/585 [01:26<01:45,  3.44it/s] 38%|███▊      | 222/585 [01:26<01:45,  3.45it/s] 38%|███▊      | 223/585 [01:27<01:50,  3.29it/s] 38%|███▊      | 224/585 [01:27<01:48,  3.33it/s] 38%|███▊      | 225/585 [01:27<01:46,  3.37it/s] 39%|███▊      | 226/585 [01:28<01:46,  3.39it/s] 39%|███▉      | 227/585 [01:28<01:45,  3.40it/s] 39%|███▉      | 228/585 [01:28<01:44,  3.40it/s] 39%|███▉      | 229/585 [01:28<01:44,  3.41it/s] 39%|███▉      | 230/585 [01:29<01:43,  3.42it/s] 39%|███▉      | 231/585 [01:29<01:43,  3.43it/s] 40%|███▉      | 232/585 [01:29<01:42,  3.43it/s] 40%|███▉      | 233/585 [01:30<01:42,  3.43it/s] 40%|████      | 234/585 [01:30<01:45,  3.32it/s][INFO|trainer.py:2140] 2023-08-28 23:56:58,001 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:56:58,001 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 23:56:58,001 >>   Batch size = 8
{'eval_loss': 1.057356357574463, 'eval_runtime': 9.9377, 'eval_samples_per_second': 350.986, 'eval_steps_per_second': 43.873, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.49it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.24it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.33it/s][A
  5%|▌         | 22/436 [00:00<00:08, 46.52it/s][A
  6%|▌         | 27/436 [00:00<00:08, 45.59it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.79it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.57it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.32it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.44it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.65it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.92it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.99it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 45.15it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.79it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.48it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.40it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.02it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.24it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.49it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.67it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.68it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.78it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.83it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.74it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 42.81it/s][A
 30%|███       | 132/436 [00:02<00:07, 43.33it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 43.68it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 43.94it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.13it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.32it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.56it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.47it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.35it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.41it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.53it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.44it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.41it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.49it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.59it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.60it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.47it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.47it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.64it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.43it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.56it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.53it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.61it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.70it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.65it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.63it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.67it/s][A
 60%|██████    | 262/436 [00:05<00:04, 43.45it/s][A
 61%|██████    | 267/436 [00:05<00:03, 43.89it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.03it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.22it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.37it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.51it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.38it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.61it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.55it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.55it/s][A
 72%|███████▏  | 312/436 [00:06<00:02, 44.59it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.69it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.56it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.54it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.63it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.70it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.71it/s][A
 80%|███████▉  | 347/436 [00:07<00:01, 44.51it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.53it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.56it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.67it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.40it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.36it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.56it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.54it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.57it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.56it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.38it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.45it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.45it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.45it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.41it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.31it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.64it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.45it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.45it/s][A 40%|████      | 234/585 [01:40<01:45,  3.32it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:57:07,890 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 23:57:08,082 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:57:11,210 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:57:11,418 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:57:11,516 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:50<36:39,  6.28s/it] 40%|████      | 236/585 [01:50<26:08,  4.50s/it] 41%|████      | 237/585 [01:51<18:45,  3.24s/it] 41%|████      | 238/585 [01:51<13:36,  2.35s/it] 41%|████      | 239/585 [01:51<10:00,  1.74s/it] 41%|████      | 240/585 [01:52<07:29,  1.30s/it] 41%|████      | 241/585 [01:52<05:44,  1.00s/it] 41%|████▏     | 242/585 [01:52<04:30,  1.27it/s] 42%|████▏     | 243/585 [01:53<03:38,  1.56it/s] 42%|████▏     | 244/585 [01:53<03:02,  1.86it/s] 42%|████▏     | 245/585 [01:53<02:37,  2.16it/s] 42%|████▏     | 246/585 [01:53<02:20,  2.42it/s] 42%|████▏     | 247/585 [01:54<02:10,  2.58it/s] 42%|████▏     | 248/585 [01:54<02:01,  2.78it/s] 43%|████▎     | 249/585 [01:54<01:54,  2.94it/s] 43%|████▎     | 250/585 [01:55<01:49,  3.07it/s] 43%|████▎     | 251/585 [01:55<01:45,  3.15it/s] 43%|████▎     | 252/585 [01:55<01:43,  3.22it/s] 43%|████▎     | 253/585 [01:56<01:41,  3.27it/s] 43%|████▎     | 254/585 [01:56<01:40,  3.31it/s] 44%|████▎     | 255/585 [01:56<01:39,  3.33it/s] 44%|████▍     | 256/585 [01:56<01:38,  3.35it/s] 44%|████▍     | 257/585 [01:57<01:37,  3.37it/s] 44%|████▍     | 258/585 [01:57<01:36,  3.38it/s] 44%|████▍     | 259/585 [01:57<01:36,  3.38it/s] 44%|████▍     | 260/585 [01:58<01:38,  3.29it/s] 45%|████▍     | 261/585 [01:58<01:37,  3.32it/s] 45%|████▍     | 262/585 [01:58<01:36,  3.34it/s] 45%|████▍     | 263/585 [01:58<01:35,  3.36it/s] 45%|████▌     | 264/585 [01:59<01:35,  3.37it/s] 45%|████▌     | 265/585 [01:59<01:34,  3.38it/s] 45%|████▌     | 266/585 [01:59<01:34,  3.38it/s] 46%|████▌     | 267/585 [02:00<01:33,  3.39it/s] 46%|████▌     | 268/585 [02:00<01:33,  3.39it/s] 46%|████▌     | 269/585 [02:00<01:33,  3.39it/s] 46%|████▌     | 270/585 [02:01<01:32,  3.39it/s] 46%|████▋     | 271/585 [02:01<01:34,  3.33it/s] 46%|████▋     | 272/585 [02:01<01:33,  3.35it/s] 47%|████▋     | 273/585 [02:01<01:32,  3.37it/s] 47%|████▋     | 274/585 [02:02<01:32,  3.38it/s] 47%|████▋     | 275/585 [02:02<01:31,  3.38it/s] 47%|████▋     | 276/585 [02:02<01:31,  3.39it/s] 47%|████▋     | 277/585 [02:03<01:30,  3.39it/s] 48%|████▊     | 278/585 [02:03<01:30,  3.39it/s] 48%|████▊     | 279/585 [02:03<01:30,  3.39it/s] 48%|████▊     | 280/585 [02:04<01:29,  3.40it/s] 48%|████▊     | 281/585 [02:04<01:29,  3.40it/s] 48%|████▊     | 282/585 [02:04<01:33,  3.24it/s] 48%|████▊     | 283/585 [02:04<01:31,  3.29it/s] 49%|████▊     | 284/585 [02:05<01:30,  3.32it/s] 49%|████▊     | 285/585 [02:05<01:29,  3.35it/s] 49%|████▉     | 286/585 [02:05<01:28,  3.36it/s] 49%|████▉     | 287/585 [02:06<01:28,  3.36it/s] 49%|████▉     | 288/585 [02:06<01:28,  3.37it/s] 49%|████▉     | 289/585 [02:06<01:27,  3.38it/s] 50%|████▉     | 290/585 [02:07<01:27,  3.39it/s] 50%|████▉     | 291/585 [02:07<01:26,  3.39it/s] 50%|████▉     | 292/585 [02:07<01:26,  3.39it/s] 50%|█████     | 293/585 [02:07<01:28,  3.31it/s] 50%|█████     | 294/585 [02:08<01:27,  3.34it/s] 50%|█████     | 295/585 [02:08<01:26,  3.35it/s] 51%|█████     | 296/585 [02:08<01:25,  3.37it/s] 51%|█████     | 297/585 [02:09<01:25,  3.38it/s] 51%|█████     | 298/585 [02:09<01:24,  3.38it/s] 51%|█████     | 299/585 [02:09<01:24,  3.38it/s] 51%|█████▏    | 300/585 [02:09<01:24,  3.39it/s] 51%|█████▏    | 301/585 [02:10<01:23,  3.39it/s] 52%|█████▏    | 302/585 [02:10<01:23,  3.39it/s] 52%|█████▏    | 303/585 [02:10<01:23,  3.39it/s] 52%|█████▏    | 304/585 [02:11<01:25,  3.30it/s] 52%|█████▏    | 305/585 [02:11<01:24,  3.33it/s] 52%|█████▏    | 306/585 [02:11<01:23,  3.35it/s] 52%|█████▏    | 307/585 [02:12<01:22,  3.36it/s] 53%|█████▎    | 308/585 [02:12<01:22,  3.37it/s] 53%|█████▎    | 309/585 [02:12<01:21,  3.38it/s] 53%|█████▎    | 310/585 [02:12<01:21,  3.39it/s] 53%|█████▎    | 311/585 [02:13<01:20,  3.39it/s] 53%|█████▎    | 312/585 [02:13<01:20,  3.39it/s] 54%|█████▎    | 313/585 [02:13<01:20,  3.40it/s] 54%|█████▎    | 314/585 [02:14<01:19,  3.40it/s] 54%|█████▍    | 315/585 [02:14<01:23,  3.23it/s] 54%|█████▍    | 316/585 [02:14<01:21,  3.28it/s] 54%|█████▍    | 317/585 [02:15<01:20,  3.32it/s] 54%|█████▍    | 318/585 [02:15<01:19,  3.34it/s] 55%|█████▍    | 319/585 [02:15<01:19,  3.36it/s] 55%|█████▍    | 320/585 [02:15<01:18,  3.37it/s] 55%|█████▍    | 321/585 [02:16<01:18,  3.37it/s] 55%|█████▌    | 322/585 [02:16<01:17,  3.38it/s] 55%|█████▌    | 323/585 [02:16<01:17,  3.39it/s] 55%|█████▌    | 324/585 [02:17<01:16,  3.39it/s] 56%|█████▌    | 325/585 [02:17<01:16,  3.40it/s] 56%|█████▌    | 326/585 [02:17<01:21,  3.16it/s] 56%|█████▌    | 327/585 [02:18<01:19,  3.23it/s] 56%|█████▌    | 328/585 [02:18<01:18,  3.29it/s] 56%|█████▌    | 329/585 [02:18<01:17,  3.32it/s] 56%|█████▋    | 330/585 [02:18<01:16,  3.34it/s] 57%|█████▋    | 331/585 [02:19<01:15,  3.36it/s] 57%|█████▋    | 332/585 [02:19<01:15,  3.37it/s] 57%|█████▋    | 333/585 [02:19<01:14,  3.38it/s] 57%|█████▋    | 334/585 [02:20<01:14,  3.39it/s] 57%|█████▋    | 335/585 [02:20<01:13,  3.40it/s] 57%|█████▋    | 336/585 [02:20<01:19,  3.13it/s] 58%|█████▊    | 337/585 [02:21<01:17,  3.21it/s] 58%|█████▊    | 338/585 [02:21<01:15,  3.27it/s] 58%|█████▊    | 339/585 [02:21<01:14,  3.30it/s] 58%|█████▊    | 340/585 [02:21<01:13,  3.33it/s] 58%|█████▊    | 341/585 [02:22<01:12,  3.35it/s] 58%|█████▊    | 342/585 [02:22<01:12,  3.37it/s] 59%|█████▊    | 343/585 [02:22<01:11,  3.38it/s] 59%|█████▉    | 344/585 [02:23<01:11,  3.39it/s] 59%|█████▉    | 345/585 [02:23<01:10,  3.39it/s] 59%|█████▉    | 346/585 [02:23<01:17,  3.07it/s] 59%|█████▉    | 347/585 [02:24<01:15,  3.16it/s] 59%|█████▉    | 348/585 [02:24<01:13,  3.23it/s] 60%|█████▉    | 349/585 [02:24<01:11,  3.28it/s] 60%|█████▉    | 350/585 [02:25<01:10,  3.31it/s] 60%|██████    | 351/585 [02:25<01:09,  3.34it/s][INFO|trainer.py:2140] 2023-08-28 23:57:53,024 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:57:53,024 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 23:57:53,024 >>   Batch size = 8
{'eval_loss': 1.0802971124649048, 'eval_runtime': 9.8234, 'eval_samples_per_second': 355.072, 'eval_steps_per_second': 44.384, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.88it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.15it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.29it/s][A
  5%|▌         | 22/436 [00:00<00:08, 46.48it/s][A
  6%|▌         | 27/436 [00:00<00:08, 46.27it/s][A
  7%|▋         | 32/436 [00:00<00:08, 45.89it/s][A
  8%|▊         | 37/436 [00:00<00:08, 45.39it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.77it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.29it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.19it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.57it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.94it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.79it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.97it/s][A
 18%|█▊        | 77/436 [00:01<00:07, 44.96it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.74it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.31it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.19it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.20it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.50it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.73it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.88it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.93it/s][A
 28%|██▊       | 122/436 [00:02<00:06, 45.01it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.76it/s][A
 30%|███       | 132/436 [00:02<00:07, 41.45it/s][A
 31%|███▏      | 137/436 [00:03<00:07, 42.44it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 42.92it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 43.37it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 43.76it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.40it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.29it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.48it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.19it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.13it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.44it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.48it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.47it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.61it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.67it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.93it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.78it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.47it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.49it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.58it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.45it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.32it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.46it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.66it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.81it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.70it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.59it/s][A
 61%|██████    | 267/436 [00:06<00:03, 42.34it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 43.11it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 43.53it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 43.96it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.06it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.37it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.51it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.52it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.22it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.24it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.21it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.38it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.51it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.64it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.68it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.56it/s][A
 80%|███████▉  | 347/436 [00:07<00:01, 44.60it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.29it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.31it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.44it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.27it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.52it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.66it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.64it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.63it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.60it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.30it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 43.41it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 43.92it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.11it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.36it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.18it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.37it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.27it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.27it/s][A 60%|██████    | 351/585 [02:35<01:09,  3.34it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:58:02,942 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 23:58:03,082 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:58:06,376 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:58:06,551 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:58:06,662 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:46<25:50,  6.65s/it] 60%|██████    | 353/585 [02:47<18:21,  4.75s/it] 61%|██████    | 354/585 [02:47<13:08,  3.41s/it] 61%|██████    | 355/585 [02:47<09:29,  2.48s/it] 61%|██████    | 356/585 [02:47<06:57,  1.82s/it] 61%|██████    | 357/585 [02:48<05:10,  1.36s/it] 61%|██████    | 358/585 [02:48<03:56,  1.04s/it] 61%|██████▏   | 359/585 [02:48<03:05,  1.22it/s] 62%|██████▏   | 360/585 [02:49<02:28,  1.51it/s] 62%|██████▏   | 361/585 [02:49<02:03,  1.81it/s] 62%|██████▏   | 362/585 [02:49<01:46,  2.09it/s] 62%|██████▏   | 363/585 [02:50<01:46,  2.08it/s] 62%|██████▏   | 364/585 [02:50<01:35,  2.31it/s] 62%|██████▏   | 365/585 [02:50<01:26,  2.56it/s] 63%|██████▎   | 366/585 [02:51<01:19,  2.77it/s] 63%|██████▎   | 367/585 [02:51<01:14,  2.94it/s] 63%|██████▎   | 368/585 [02:51<01:10,  3.08it/s] 63%|██████▎   | 369/585 [02:52<01:07,  3.18it/s] 63%|██████▎   | 370/585 [02:52<01:06,  3.26it/s] 63%|██████▎   | 371/585 [02:52<01:04,  3.31it/s] 64%|██████▎   | 372/585 [02:52<01:03,  3.35it/s] 64%|██████▍   | 373/585 [02:53<01:02,  3.37it/s] 64%|██████▍   | 374/585 [02:53<01:02,  3.40it/s] 64%|██████▍   | 375/585 [02:53<01:03,  3.32it/s] 64%|██████▍   | 376/585 [02:54<01:02,  3.35it/s] 64%|██████▍   | 377/585 [02:54<01:01,  3.38it/s] 65%|██████▍   | 378/585 [02:54<01:00,  3.40it/s] 65%|██████▍   | 379/585 [02:54<01:00,  3.41it/s] 65%|██████▍   | 380/585 [02:55<00:59,  3.43it/s] 65%|██████▌   | 381/585 [02:55<00:59,  3.43it/s] 65%|██████▌   | 382/585 [02:55<00:59,  3.44it/s] 65%|██████▌   | 383/585 [02:56<00:58,  3.44it/s] 66%|██████▌   | 384/585 [02:56<00:58,  3.44it/s] 66%|██████▌   | 385/585 [02:56<00:58,  3.44it/s] 66%|██████▌   | 386/585 [02:56<00:57,  3.44it/s] 66%|██████▌   | 387/585 [02:57<00:57,  3.45it/s] 66%|██████▋   | 388/585 [02:57<00:57,  3.44it/s] 66%|██████▋   | 389/585 [02:57<00:56,  3.44it/s] 67%|██████▋   | 390/585 [02:58<00:56,  3.44it/s] 67%|██████▋   | 391/585 [02:58<00:56,  3.44it/s] 67%|██████▋   | 392/585 [02:58<00:56,  3.45it/s] 67%|██████▋   | 393/585 [02:59<00:55,  3.45it/s] 67%|██████▋   | 394/585 [02:59<00:57,  3.29it/s] 68%|██████▊   | 395/585 [02:59<00:56,  3.33it/s] 68%|██████▊   | 396/585 [02:59<00:56,  3.37it/s] 68%|██████▊   | 397/585 [03:00<00:55,  3.39it/s] 68%|██████▊   | 398/585 [03:00<00:54,  3.41it/s] 68%|██████▊   | 399/585 [03:00<00:54,  3.42it/s] 68%|██████▊   | 400/585 [03:01<00:54,  3.43it/s] 69%|██████▊   | 401/585 [03:01<00:53,  3.43it/s] 69%|██████▊   | 402/585 [03:01<00:53,  3.43it/s] 69%|██████▉   | 403/585 [03:01<00:53,  3.43it/s] 69%|██████▉   | 404/585 [03:02<00:52,  3.44it/s] 69%|██████▉   | 405/585 [03:02<00:53,  3.37it/s] 69%|██████▉   | 406/585 [03:02<00:52,  3.39it/s] 70%|██████▉   | 407/585 [03:03<00:52,  3.41it/s] 70%|██████▉   | 408/585 [03:03<00:51,  3.42it/s] 70%|██████▉   | 409/585 [03:03<00:51,  3.42it/s] 70%|███████   | 410/585 [03:04<00:51,  3.43it/s] 70%|███████   | 411/585 [03:04<00:50,  3.43it/s] 70%|███████   | 412/585 [03:04<00:50,  3.43it/s] 71%|███████   | 413/585 [03:04<00:50,  3.44it/s] 71%|███████   | 414/585 [03:05<00:49,  3.44it/s] 71%|███████   | 415/585 [03:05<00:49,  3.44it/s] 71%|███████   | 416/585 [03:05<00:50,  3.36it/s] 71%|███████▏  | 417/585 [03:06<00:49,  3.38it/s] 71%|███████▏  | 418/585 [03:06<00:49,  3.40it/s] 72%|███████▏  | 419/585 [03:06<00:48,  3.41it/s] 72%|███████▏  | 420/585 [03:06<00:48,  3.42it/s] 72%|███████▏  | 421/585 [03:07<00:47,  3.43it/s] 72%|███████▏  | 422/585 [03:07<00:47,  3.43it/s] 72%|███████▏  | 423/585 [03:07<00:47,  3.43it/s] 72%|███████▏  | 424/585 [03:08<00:46,  3.44it/s] 73%|███████▎  | 425/585 [03:08<00:46,  3.44it/s] 73%|███████▎  | 426/585 [03:08<00:46,  3.43it/s] 73%|███████▎  | 427/585 [03:09<00:47,  3.35it/s] 73%|███████▎  | 428/585 [03:09<00:46,  3.38it/s] 73%|███████▎  | 429/585 [03:09<00:45,  3.40it/s] 74%|███████▎  | 430/585 [03:09<00:45,  3.41it/s] 74%|███████▎  | 431/585 [03:10<00:45,  3.42it/s] 74%|███████▍  | 432/585 [03:10<00:44,  3.42it/s] 74%|███████▍  | 433/585 [03:10<00:44,  3.43it/s] 74%|███████▍  | 434/585 [03:11<00:44,  3.43it/s] 74%|███████▍  | 435/585 [03:11<00:43,  3.43it/s] 75%|███████▍  | 436/585 [03:11<00:43,  3.43it/s] 75%|███████▍  | 437/585 [03:11<00:43,  3.43it/s] 75%|███████▍  | 438/585 [03:12<00:44,  3.29it/s] 75%|███████▌  | 439/585 [03:12<00:43,  3.33it/s] 75%|███████▌  | 440/585 [03:12<00:43,  3.36it/s] 75%|███████▌  | 441/585 [03:13<00:42,  3.38it/s] 76%|███████▌  | 442/585 [03:13<00:42,  3.40it/s] 76%|███████▌  | 443/585 [03:13<00:41,  3.41it/s] 76%|███████▌  | 444/585 [03:13<00:41,  3.42it/s] 76%|███████▌  | 445/585 [03:14<00:40,  3.43it/s] 76%|███████▌  | 446/585 [03:14<00:40,  3.44it/s] 76%|███████▋  | 447/585 [03:14<00:40,  3.44it/s] 77%|███████▋  | 448/585 [03:15<00:39,  3.44it/s] 77%|███████▋  | 449/585 [03:15<00:41,  3.28it/s] 77%|███████▋  | 450/585 [03:15<00:40,  3.33it/s] 77%|███████▋  | 451/585 [03:16<00:39,  3.36it/s] 77%|███████▋  | 452/585 [03:16<00:39,  3.39it/s] 77%|███████▋  | 453/585 [03:16<00:38,  3.41it/s] 78%|███████▊  | 454/585 [03:16<00:38,  3.42it/s] 78%|███████▊  | 455/585 [03:17<00:37,  3.42it/s] 78%|███████▊  | 456/585 [03:17<00:37,  3.43it/s] 78%|███████▊  | 457/585 [03:17<00:37,  3.43it/s] 78%|███████▊  | 458/585 [03:18<00:36,  3.43it/s] 78%|███████▊  | 459/585 [03:18<00:36,  3.44it/s] 79%|███████▊  | 460/585 [03:18<00:39,  3.18it/s] 79%|███████▉  | 461/585 [03:19<00:38,  3.25it/s] 79%|███████▉  | 462/585 [03:19<00:37,  3.30it/s] 79%|███████▉  | 463/585 [03:19<00:36,  3.34it/s] 79%|███████▉  | 464/585 [03:19<00:35,  3.37it/s] 79%|███████▉  | 465/585 [03:20<00:35,  3.40it/s] 80%|███████▉  | 466/585 [03:20<00:34,  3.41it/s] 80%|███████▉  | 467/585 [03:20<00:34,  3.43it/s] 80%|████████  | 468/585 [03:21<00:34,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 23:58:48,687 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:58:48,688 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 23:58:48,688 >>   Batch size = 8
{'eval_loss': 1.0890531539916992, 'eval_runtime': 9.8455, 'eval_samples_per_second': 354.275, 'eval_steps_per_second': 44.284, 'epoch': 3.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.06it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.54it/s][A
  4%|▍         | 17/436 [00:00<00:10, 39.15it/s][A
  5%|▌         | 22/436 [00:00<00:10, 41.14it/s][A
  6%|▌         | 27/436 [00:00<00:09, 42.48it/s][A
  7%|▋         | 32/436 [00:00<00:09, 43.46it/s][A
  8%|▊         | 37/436 [00:00<00:09, 43.85it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.20it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.49it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.53it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.23it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.05it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.05it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.11it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.38it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.78it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.91it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.78it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.74it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.53it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.33it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.16it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.16it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.32it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.61it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.68it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.89it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.72it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.66it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 41.20it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 42.25it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.14it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.68it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.05it/s][A
 41%|████      | 177/436 [00:04<00:05, 44.45it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.64it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.44it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.15it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.11it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.33it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.52it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.52it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.76it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.91it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.98it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.57it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.32it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.36it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.33it/s][A
 58%|█████▊    | 252/436 [00:05<00:05, 35.23it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 37.77it/s][A
 60%|██████    | 262/436 [00:06<00:04, 39.78it/s][A
 61%|██████    | 267/436 [00:06<00:04, 41.35it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 42.36it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 43.14it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 43.74it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 43.93it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 43.61it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 43.52it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 43.85it/s][A
 70%|███████   | 307/436 [00:07<00:02, 44.21it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.45it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.69it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.74it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.75it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.63it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.40it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.15it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.04it/s][A
 81%|████████  | 352/436 [00:08<00:01, 44.19it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.50it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.48it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.83it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.76it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.62it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 42.16it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 42.77it/s][A
 90%|████████▉ | 392/436 [00:08<00:01, 43.20it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 43.69it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.02it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.47it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.74it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.68it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.19it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 43.87it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 43.79it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:31<00:34,  3.43it/s]
100%|██████████| 436/436 [00:09<00:00, 43.79it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:58:58,935 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 23:58:59,159 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:59:02,230 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:59:02,373 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:59:02,439 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:42<12:54,  6.68s/it] 80%|████████  | 470/585 [03:42<09:08,  4.77s/it] 81%|████████  | 471/585 [03:43<06:30,  3.43s/it] 81%|████████  | 472/585 [03:43<04:40,  2.49s/it] 81%|████████  | 473/585 [03:43<03:24,  1.83s/it] 81%|████████  | 474/585 [03:44<02:31,  1.37s/it] 81%|████████  | 475/585 [03:44<01:55,  1.05s/it] 81%|████████▏ | 476/585 [03:44<01:29,  1.22it/s] 82%|████████▏ | 477/585 [03:45<01:11,  1.51it/s] 82%|████████▏ | 478/585 [03:45<00:58,  1.81it/s] 82%|████████▏ | 479/585 [03:45<00:50,  2.11it/s] 82%|████████▏ | 480/585 [03:45<00:43,  2.39it/s] 82%|████████▏ | 481/585 [03:46<00:41,  2.51it/s] 82%|████████▏ | 482/585 [03:46<00:37,  2.74it/s] 83%|████████▎ | 483/585 [03:46<00:34,  2.92it/s] 83%|████████▎ | 484/585 [03:47<00:33,  3.06it/s] 83%|████████▎ | 485/585 [03:47<00:31,  3.17it/s] 83%|████████▎ | 486/585 [03:47<00:30,  3.25it/s] 83%|████████▎ | 487/585 [03:48<00:29,  3.30it/s] 83%|████████▎ | 488/585 [03:48<00:28,  3.35it/s] 84%|████████▎ | 489/585 [03:48<00:28,  3.37it/s] 84%|████████▍ | 490/585 [03:48<00:27,  3.40it/s] 84%|████████▍ | 491/585 [03:49<00:27,  3.41it/s] 84%|████████▍ | 492/585 [03:49<00:28,  3.30it/s] 84%|████████▍ | 493/585 [03:49<00:28,  3.23it/s] 84%|████████▍ | 494/585 [03:50<00:35,  2.59it/s] 85%|████████▍ | 495/585 [03:50<00:32,  2.80it/s] 85%|████████▍ | 496/585 [03:50<00:30,  2.97it/s] 85%|████████▍ | 497/585 [03:51<00:28,  3.10it/s] 85%|████████▌ | 498/585 [03:51<00:27,  3.19it/s] 85%|████████▌ | 499/585 [03:51<00:26,  3.27it/s] 85%|████████▌ | 500/585 [03:52<00:25,  3.31it/s]                                                  85%|████████▌ | 500/585 [03:52<00:25,  3.31it/s] 86%|████████▌ | 501/585 [03:52<00:25,  3.34it/s] 86%|████████▌ | 502/585 [03:52<00:25,  3.31it/s] 86%|████████▌ | 503/585 [03:53<00:24,  3.35it/s] 86%|████████▌ | 504/585 [03:53<00:23,  3.38it/s] 86%|████████▋ | 505/585 [03:53<00:23,  3.40it/s] 86%|████████▋ | 506/585 [03:53<00:23,  3.41it/s] 87%|████████▋ | 507/585 [03:54<00:22,  3.42it/s] 87%|████████▋ | 508/585 [03:54<00:22,  3.43it/s] 87%|████████▋ | 509/585 [03:54<00:22,  3.43it/s] 87%|████████▋ | 510/585 [03:55<00:21,  3.44it/s] 87%|████████▋ | 511/585 [03:55<00:21,  3.44it/s] 88%|████████▊ | 512/585 [03:55<00:21,  3.44it/s] 88%|████████▊ | 513/585 [03:55<00:21,  3.33it/s] 88%|████████▊ | 514/585 [03:56<00:21,  3.36it/s] 88%|████████▊ | 515/585 [03:56<00:20,  3.39it/s] 88%|████████▊ | 516/585 [03:56<00:20,  3.40it/s] 88%|████████▊ | 517/585 [03:57<00:19,  3.41it/s] 89%|████████▊ | 518/585 [03:57<00:19,  3.42it/s] 89%|████████▊ | 519/585 [03:57<00:19,  3.43it/s] 89%|████████▉ | 520/585 [03:57<00:18,  3.43it/s] 89%|████████▉ | 521/585 [03:58<00:18,  3.43it/s] 89%|████████▉ | 522/585 [03:58<00:18,  3.44it/s] 89%|████████▉ | 523/585 [03:58<00:18,  3.43it/s] 90%|████████▉ | 524/585 [03:59<00:18,  3.31it/s] 90%|████████▉ | 525/585 [03:59<00:17,  3.35it/s] 90%|████████▉ | 526/585 [03:59<00:17,  3.38it/s] 90%|█████████ | 527/585 [04:00<00:17,  3.40it/s] 90%|█████████ | 528/585 [04:00<00:16,  3.41it/s] 90%|█████████ | 529/585 [04:00<00:16,  3.42it/s] 91%|█████████ | 530/585 [04:00<00:16,  3.43it/s] 91%|█████████ | 531/585 [04:01<00:15,  3.43it/s] 91%|█████████ | 532/585 [04:01<00:15,  3.44it/s] 91%|█████████ | 533/585 [04:01<00:15,  3.44it/s] 91%|█████████▏| 534/585 [04:02<00:14,  3.44it/s] 91%|█████████▏| 535/585 [04:02<00:15,  3.25it/s] 92%|█████████▏| 536/585 [04:02<00:14,  3.30it/s] 92%|█████████▏| 537/585 [04:03<00:14,  3.34it/s] 92%|█████████▏| 538/585 [04:03<00:13,  3.37it/s] 92%|█████████▏| 539/585 [04:03<00:13,  3.39it/s] 92%|█████████▏| 540/585 [04:03<00:13,  3.41it/s] 92%|█████████▏| 541/585 [04:04<00:12,  3.42it/s] 93%|█████████▎| 542/585 [04:04<00:12,  3.43it/s] 93%|█████████▎| 543/585 [04:04<00:12,  3.43it/s] 93%|█████████▎| 544/585 [04:05<00:11,  3.44it/s] 93%|█████████▎| 545/585 [04:05<00:11,  3.44it/s] 93%|█████████▎| 546/585 [04:05<00:11,  3.44it/s] 94%|█████████▎| 547/585 [04:05<00:11,  3.30it/s] 94%|█████████▎| 548/585 [04:06<00:11,  3.34it/s] 94%|█████████▍| 549/585 [04:06<00:10,  3.37it/s] 94%|█████████▍| 550/585 [04:06<00:10,  3.39it/s] 94%|█████████▍| 551/585 [04:07<00:09,  3.41it/s] 94%|█████████▍| 552/585 [04:07<00:09,  3.42it/s] 95%|█████████▍| 553/585 [04:07<00:09,  3.43it/s] 95%|█████████▍| 554/585 [04:08<00:09,  3.43it/s] 95%|█████████▍| 555/585 [04:08<00:08,  3.43it/s] 95%|█████████▌| 556/585 [04:08<00:08,  3.43it/s] 95%|█████████▌| 557/585 [04:08<00:08,  3.44it/s] 95%|█████████▌| 558/585 [04:09<00:08,  3.26it/s] 96%|█████████▌| 559/585 [04:09<00:07,  3.32it/s] 96%|█████████▌| 560/585 [04:09<00:07,  3.35it/s] 96%|█████████▌| 561/585 [04:10<00:07,  3.38it/s] 96%|█████████▌| 562/585 [04:10<00:06,  3.40it/s] 96%|█████████▌| 563/585 [04:10<00:06,  3.41it/s] 96%|█████████▋| 564/585 [04:10<00:06,  3.42it/s] 97%|█████████▋| 565/585 [04:11<00:05,  3.43it/s] 97%|█████████▋| 566/585 [04:11<00:05,  3.43it/s] 97%|█████████▋| 567/585 [04:11<00:05,  3.44it/s] 97%|█████████▋| 568/585 [04:12<00:04,  3.43it/s] 97%|█████████▋| 569/585 [04:12<00:04,  3.38it/s] 97%|█████████▋| 570/585 [04:12<00:04,  3.40it/s] 98%|█████████▊| 571/585 [04:13<00:04,  3.41it/s] 98%|█████████▊| 572/585 [04:13<00:03,  3.42it/s] 98%|█████████▊| 573/585 [04:13<00:03,  3.43it/s] 98%|█████████▊| 574/585 [04:13<00:03,  3.43it/s] 98%|█████████▊| 575/585 [04:14<00:02,  3.44it/s] 98%|█████████▊| 576/585 [04:14<00:02,  3.44it/s] 99%|█████████▊| 577/585 [04:14<00:02,  3.44it/s] 99%|█████████▉| 578/585 [04:15<00:02,  3.44it/s] 99%|█████████▉| 579/585 [04:15<00:01,  3.44it/s] 99%|█████████▉| 580/585 [04:15<00:01,  3.23it/s] 99%|█████████▉| 581/585 [04:15<00:01,  3.29it/s] 99%|█████████▉| 582/585 [04:16<00:00,  3.34it/s]100%|█████████▉| 583/585 [04:16<00:00,  3.37it/s]100%|█████████▉| 584/585 [04:16<00:00,  3.39it/s]100%|██████████| 585/585 [04:17<00:00,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 23:59:44,693 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:59:44,693 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-28 23:59:44,693 >>   Batch size = 8
{'eval_loss': 1.098463535308838, 'eval_runtime': 9.9879, 'eval_samples_per_second': 349.223, 'eval_steps_per_second': 43.653, 'epoch': 4.0}
{'loss': 0.4007, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.74it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.93it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.94it/s][A
  5%|▌         | 22/436 [00:00<00:08, 46.06it/s][A
  6%|▌         | 27/436 [00:00<00:08, 45.50it/s][A
  7%|▋         | 32/436 [00:00<00:08, 45.04it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.94it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.71it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.77it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.83it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.87it/s][A
 14%|█▍        | 62/436 [00:01<00:09, 40.02it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 41.37it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 42.25it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 42.99it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.62it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 43.96it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.05it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.17it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.03it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 43.91it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.30it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.45it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.57it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.68it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.70it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.68it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.39it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.24it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.23it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.41it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.56it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.74it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.89it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.96it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.88it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.60it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.41it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 40.92it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 42.02it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 42.98it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 43.70it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.08it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.48it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.45it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.40it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.26it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 43.92it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.23it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.52it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.74it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.80it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.96it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.90it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.66it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.37it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.39it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.33it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.53it/s][A
 69%|██████▉   | 302/436 [00:06<00:02, 44.77it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.87it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.89it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.96it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.72it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.65it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 37.53it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 39.62it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 41.09it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 42.28it/s][A
 81%|████████  | 352/436 [00:07<00:01, 43.08it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 43.57it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.04it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.27it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 43.88it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 43.92it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.21it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.34it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.57it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 44.81it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.86it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.84it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.81it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.31it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.24it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.37it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.58it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:27<00:00,  3.40it/s]
100%|██████████| 436/436 [00:09<00:00, 44.58it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:59:55,171 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 23:59:55,940 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:00:00,784 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:00:01,196 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:00:01,323 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 00:00:10,919 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 00:00:10,952 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117 (score: 1.057356357574463).
                                                 100%|██████████| 585/585 [04:54<00:00,  3.40it/s]100%|██████████| 585/585 [04:54<00:00,  1.98it/s]
[INFO|trainer.py:1894] 2023-08-29 00:00:22,714 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 00:00:22,953 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:00:27,238 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:00:27,524 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:00:27,644 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:00:28,236 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:00:28,236 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:00:28,236 >>   train_loss               =     0.3979
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:00:28,236 >>   train_runtime            = 0:04:54.68
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:00:28,237 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:00:28,237 >>   train_samples_per_second =    127.254
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:00:28,237 >>   train_steps_per_second   =      1.985
{'eval_loss': 1.1044410467147827, 'eval_runtime': 9.9109, 'eval_samples_per_second': 351.936, 'eval_steps_per_second': 43.992, 'epoch': 5.0}
{'train_runtime': 294.6856, 'train_samples_per_second': 127.254, 'train_steps_per_second': 1.985, 'train_loss': 0.3978761395837507, 'epoch': 5.0}
08/29/2023 00:00:28 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 00:00:28,556 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:00:28,556 >>   Num examples = 3488
[INFO|trainer.py:2145] 2023-08-29 00:00:28,557 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 55.46it/s]  3%|▎         | 12/436 [00:00<00:08, 49.33it/s]  4%|▍         | 17/436 [00:00<00:08, 47.69it/s]  5%|▌         | 22/436 [00:00<00:08, 46.89it/s]  6%|▌         | 27/436 [00:00<00:08, 46.42it/s]  7%|▋         | 32/436 [00:00<00:08, 45.97it/s]  8%|▊         | 37/436 [00:00<00:08, 45.73it/s] 10%|▉         | 42/436 [00:00<00:08, 45.32it/s] 11%|█         | 47/436 [00:01<00:08, 44.74it/s] 12%|█▏        | 52/436 [00:01<00:08, 44.59it/s] 13%|█▎        | 57/436 [00:01<00:08, 44.63it/s] 14%|█▍        | 62/436 [00:01<00:08, 44.88it/s] 15%|█▌        | 67/436 [00:01<00:08, 44.90it/s] 17%|█▋        | 72/436 [00:01<00:08, 44.84it/s] 18%|█▊        | 77/436 [00:01<00:07, 45.13it/s] 19%|█▉        | 82/436 [00:01<00:08, 42.34it/s] 20%|█▉        | 87/436 [00:01<00:08, 43.14it/s] 21%|██        | 92/436 [00:02<00:07, 43.62it/s] 22%|██▏       | 97/436 [00:02<00:07, 43.86it/s] 23%|██▎       | 102/436 [00:02<00:07, 44.15it/s] 25%|██▍       | 107/436 [00:02<00:07, 44.42it/s] 26%|██▌       | 112/436 [00:02<00:07, 44.66it/s] 27%|██▋       | 117/436 [00:02<00:07, 44.87it/s] 28%|██▊       | 122/436 [00:02<00:07, 44.61it/s] 29%|██▉       | 127/436 [00:02<00:06, 44.67it/s] 30%|███       | 132/436 [00:02<00:06, 44.69it/s] 31%|███▏      | 137/436 [00:03<00:06, 44.79it/s] 33%|███▎      | 142/436 [00:03<00:06, 44.88it/s] 34%|███▎      | 147/436 [00:03<00:06, 44.80it/s] 35%|███▍      | 152/436 [00:03<00:06, 45.00it/s] 36%|███▌      | 157/436 [00:03<00:06, 45.08it/s] 37%|███▋      | 162/436 [00:03<00:06, 45.07it/s] 38%|███▊      | 167/436 [00:03<00:05, 45.06it/s] 39%|███▉      | 172/436 [00:03<00:05, 44.87it/s] 41%|████      | 177/436 [00:03<00:05, 44.91it/s] 42%|████▏     | 182/436 [00:04<00:05, 45.09it/s] 43%|████▎     | 187/436 [00:04<00:05, 44.94it/s] 44%|████▍     | 192/436 [00:04<00:05, 45.00it/s] 45%|████▌     | 197/436 [00:04<00:05, 44.99it/s] 46%|████▋     | 202/436 [00:04<00:05, 44.86it/s] 47%|████▋     | 207/436 [00:04<00:05, 45.09it/s] 49%|████▊     | 212/436 [00:04<00:04, 44.87it/s] 50%|████▉     | 217/436 [00:04<00:04, 44.94it/s] 51%|█████     | 222/436 [00:04<00:04, 44.94it/s] 52%|█████▏    | 227/436 [00:05<00:04, 44.99it/s] 53%|█████▎    | 232/436 [00:05<00:04, 44.92it/s] 54%|█████▍    | 237/436 [00:05<00:04, 44.79it/s] 56%|█████▌    | 242/436 [00:05<00:04, 44.81it/s] 57%|█████▋    | 247/436 [00:05<00:04, 44.85it/s] 58%|█████▊    | 252/436 [00:05<00:04, 44.95it/s] 59%|█████▉    | 257/436 [00:05<00:03, 44.94it/s] 60%|██████    | 262/436 [00:05<00:03, 44.80it/s] 61%|██████    | 267/436 [00:05<00:03, 44.92it/s] 62%|██████▏   | 272/436 [00:06<00:03, 44.67it/s] 64%|██████▎   | 277/436 [00:06<00:03, 40.04it/s] 65%|██████▍   | 282/436 [00:06<00:03, 41.46it/s] 66%|██████▌   | 287/436 [00:06<00:03, 42.63it/s] 67%|██████▋   | 292/436 [00:06<00:03, 43.33it/s] 68%|██████▊   | 297/436 [00:06<00:03, 43.98it/s] 69%|██████▉   | 302/436 [00:06<00:03, 44.44it/s] 70%|███████   | 307/436 [00:06<00:02, 44.60it/s] 72%|███████▏  | 312/436 [00:06<00:02, 44.56it/s] 73%|███████▎  | 317/436 [00:07<00:02, 44.20it/s] 74%|███████▍  | 322/436 [00:07<00:02, 43.96it/s] 75%|███████▌  | 327/436 [00:07<00:02, 44.28it/s] 76%|███████▌  | 332/436 [00:07<00:02, 44.59it/s] 77%|███████▋  | 337/436 [00:07<00:02, 44.84it/s] 78%|███████▊  | 342/436 [00:07<00:02, 45.11it/s] 80%|███████▉  | 347/436 [00:07<00:01, 45.07it/s] 81%|████████  | 352/436 [00:07<00:01, 45.31it/s] 82%|████████▏ | 357/436 [00:07<00:01, 45.00it/s] 83%|████████▎ | 362/436 [00:08<00:01, 44.66it/s] 84%|████████▍ | 367/436 [00:08<00:01, 44.54it/s] 85%|████████▌ | 372/436 [00:08<00:01, 44.56it/s] 86%|████████▋ | 377/436 [00:08<00:01, 44.67it/s] 88%|████████▊ | 382/436 [00:08<00:01, 44.93it/s] 89%|████████▉ | 387/436 [00:08<00:01, 45.14it/s] 90%|████████▉ | 392/436 [00:08<00:00, 45.17it/s] 91%|█████████ | 397/436 [00:08<00:00, 45.01it/s] 92%|█████████▏| 402/436 [00:08<00:00, 44.75it/s] 93%|█████████▎| 407/436 [00:09<00:00, 44.37it/s] 94%|█████████▍| 412/436 [00:09<00:00, 42.43it/s] 96%|█████████▌| 417/436 [00:09<00:00, 43.06it/s] 97%|█████████▋| 422/436 [00:09<00:00, 43.61it/s] 98%|█████████▊| 427/436 [00:09<00:00, 44.10it/s] 99%|█████████▉| 432/436 [00:09<00:00, 44.54it/s]100%|██████████| 436/436 [00:09<00:00, 44.58it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:00:38,355 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:00:38,355 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:00:38,355 >>   eval_loss               =     1.0574
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:00:38,355 >>   eval_runtime            = 0:00:09.79
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:00:38,355 >>   eval_samples            =       3488
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:00:38,355 >>   eval_samples_per_second =     355.97
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:00:38,355 >>   eval_steps_per_second   =     44.496
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:00:38,355 >>   perplexity              =     2.8788
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:00:51,214 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:00:51,267 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:00:51,267 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:00:51,267 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:00:51,267 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:00:52,304 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:00:52,305 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:00:53,020 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:00:54,384 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:00:54,433 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:00:57,827 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:00:57,875 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:00:57,875 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:00:57,875 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:00:57,875 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:00:58,883 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:00:58,885 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:00:59,515 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:00:59,760 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:00:59,760 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11910
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12010, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.40it/s]Extractor Predicting: 2it [00:01,  1.44it/s]Extractor Predicting: 3it [00:02,  1.43it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:05,  1.62it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:07,  1.60it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.58it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.60it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:14,  1.60it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:15,  1.60it/s]Extractor Predicting: 25it [00:15,  1.56it/s]Extractor Predicting: 26it [00:16,  1.57it/s]Extractor Predicting: 27it [00:17,  1.57it/s]Extractor Predicting: 28it [00:17,  1.57it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:19,  1.53it/s]Extractor Predicting: 32it [00:20,  1.54it/s]Extractor Predicting: 33it [00:21,  1.51it/s]Extractor Predicting: 34it [00:21,  1.51it/s]Extractor Predicting: 35it [00:22,  1.49it/s]Extractor Predicting: 36it [00:23,  1.49it/s]Extractor Predicting: 37it [00:23,  1.49it/s]Extractor Predicting: 38it [00:24,  1.47it/s]Extractor Predicting: 39it [00:25,  1.48it/s]Extractor Predicting: 40it [00:25,  1.48it/s]Extractor Predicting: 41it [00:26,  1.48it/s]Extractor Predicting: 42it [00:27,  1.47it/s]Extractor Predicting: 43it [00:28,  1.45it/s]Extractor Predicting: 44it [00:28,  1.45it/s]Extractor Predicting: 45it [00:29,  1.44it/s]Extractor Predicting: 46it [00:30,  1.44it/s]Extractor Predicting: 47it [00:30,  1.49it/s]Extractor Predicting: 48it [00:31,  1.49it/s]Extractor Predicting: 49it [00:32,  1.53it/s]Extractor Predicting: 50it [00:32,  1.46it/s]Extractor Predicting: 51it [00:33,  1.50it/s]Extractor Predicting: 52it [00:34,  1.41it/s]Extractor Predicting: 53it [00:34,  1.43it/s]Extractor Predicting: 54it [00:35,  1.44it/s]Extractor Predicting: 55it [00:36,  1.44it/s]Extractor Predicting: 56it [00:36,  1.45it/s]Extractor Predicting: 57it [00:37,  1.50it/s]Extractor Predicting: 58it [00:38,  1.50it/s]Extractor Predicting: 59it [00:38,  1.47it/s]Extractor Predicting: 60it [00:39,  1.45it/s]Extractor Predicting: 61it [00:40,  1.45it/s]Extractor Predicting: 62it [00:41,  1.46it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:42,  1.47it/s]Extractor Predicting: 65it [00:43,  1.48it/s]Extractor Predicting: 66it [00:43,  1.50it/s]Extractor Predicting: 67it [00:44,  1.50it/s]Extractor Predicting: 68it [00:44,  1.52it/s]Extractor Predicting: 69it [00:45,  1.51it/s]Extractor Predicting: 70it [00:46,  1.48it/s]Extractor Predicting: 71it [00:47,  1.46it/s]Extractor Predicting: 72it [00:47,  1.49it/s]Extractor Predicting: 73it [00:48,  1.48it/s]Extractor Predicting: 74it [00:49,  1.51it/s]Extractor Predicting: 75it [00:49,  1.48it/s]Extractor Predicting: 76it [00:50,  1.50it/s]Extractor Predicting: 77it [00:51,  1.53it/s]Extractor Predicting: 78it [00:51,  1.50it/s]Extractor Predicting: 79it [00:52,  1.51it/s]Extractor Predicting: 80it [00:53,  1.48it/s]Extractor Predicting: 81it [00:53,  1.48it/s]Extractor Predicting: 82it [00:54,  1.48it/s]Extractor Predicting: 83it [00:55,  1.53it/s]Extractor Predicting: 84it [00:55,  1.50it/s]Extractor Predicting: 85it [00:56,  1.47it/s]Extractor Predicting: 86it [00:57,  1.49it/s]Extractor Predicting: 87it [00:57,  1.50it/s]Extractor Predicting: 88it [00:58,  1.53it/s]Extractor Predicting: 89it [00:59,  1.52it/s]Extractor Predicting: 90it [00:59,  1.53it/s]Extractor Predicting: 91it [01:00,  1.57it/s]Extractor Predicting: 92it [01:00,  1.58it/s]Extractor Predicting: 93it [01:01,  1.56it/s]Extractor Predicting: 94it [01:02,  1.58it/s]Extractor Predicting: 95it [01:02,  1.58it/s]Extractor Predicting: 96it [01:03,  1.54it/s]Extractor Predicting: 97it [01:04,  1.54it/s]Extractor Predicting: 98it [01:04,  1.52it/s]Extractor Predicting: 99it [01:05,  1.50it/s]Extractor Predicting: 100it [01:06,  1.49it/s]Extractor Predicting: 101it [01:06,  1.53it/s]Extractor Predicting: 102it [01:07,  1.58it/s]Extractor Predicting: 103it [01:08,  1.56it/s]Extractor Predicting: 104it [01:08,  1.58it/s]Extractor Predicting: 105it [01:09,  1.56it/s]Extractor Predicting: 106it [01:09,  1.57it/s]Extractor Predicting: 107it [01:10,  1.55it/s]Extractor Predicting: 108it [01:11,  1.55it/s]Extractor Predicting: 109it [01:11,  1.56it/s]Extractor Predicting: 110it [01:12,  1.55it/s]Extractor Predicting: 111it [01:13,  1.57it/s]Extractor Predicting: 112it [01:13,  1.58it/s]Extractor Predicting: 113it [01:14,  1.63it/s]Extractor Predicting: 114it [01:14,  1.62it/s]Extractor Predicting: 115it [01:15,  1.63it/s]Extractor Predicting: 116it [01:16,  1.59it/s]Extractor Predicting: 117it [01:16,  1.56it/s]Extractor Predicting: 118it [01:17,  1.42it/s]Extractor Predicting: 119it [01:18,  1.43it/s]Extractor Predicting: 120it [01:19,  1.44it/s]Extractor Predicting: 121it [01:19,  1.47it/s]Extractor Predicting: 122it [01:20,  1.48it/s]Extractor Predicting: 123it [01:21,  1.49it/s]Extractor Predicting: 124it [01:21,  1.48it/s]Extractor Predicting: 125it [01:22,  1.47it/s]Extractor Predicting: 126it [01:23,  1.44it/s]Extractor Predicting: 127it [01:23,  1.46it/s]Extractor Predicting: 128it [01:24,  1.52it/s]Extractor Predicting: 129it [01:25,  1.49it/s]Extractor Predicting: 130it [01:25,  1.52it/s]Extractor Predicting: 131it [01:26,  1.51it/s]Extractor Predicting: 132it [01:27,  1.51it/s]Extractor Predicting: 133it [01:27,  1.50it/s]Extractor Predicting: 134it [01:28,  1.48it/s]Extractor Predicting: 135it [01:29,  1.49it/s]Extractor Predicting: 136it [01:29,  1.49it/s]Extractor Predicting: 137it [01:30,  1.50it/s]Extractor Predicting: 138it [01:31,  1.51it/s]Extractor Predicting: 139it [01:31,  1.49it/s]Extractor Predicting: 140it [01:32,  1.52it/s]Extractor Predicting: 141it [01:33,  1.50it/s]Extractor Predicting: 142it [01:33,  1.47it/s]Extractor Predicting: 143it [01:34,  1.49it/s]Extractor Predicting: 144it [01:34,  1.80it/s]Extractor Predicting: 144it [01:34,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:02:49,333 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:02:49,441 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:02:49,441 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:02:49,441 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:02:49,441 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:02:50,506 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:02:50,507 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:02:51,300 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:02:52,466 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:02:52,515 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:02:55,850 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:02:55,930 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:02:55,930 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:02:55,931 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:02:55,931 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:02:57,157 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:02:57,158 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:02:57,872 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:02:58,156 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:02:58,157 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2718556119571348,
  "recall": 0.1381880733944954,
  "score": 0.18323512640182474,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19834
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19934, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:02,  1.45it/s]Extractor Predicting: 4it [00:02,  1.45it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:04,  1.46it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:06,  1.53it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:08,  1.49it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:10,  1.48it/s]Extractor Predicting: 16it [00:10,  1.48it/s]Extractor Predicting: 17it [00:11,  1.46it/s]Extractor Predicting: 18it [00:12,  1.47it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:14,  1.51it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:15,  1.55it/s]Extractor Predicting: 24it [00:16,  1.52it/s]Extractor Predicting: 25it [00:16,  1.48it/s]Extractor Predicting: 26it [00:17,  1.52it/s]Extractor Predicting: 27it [00:18,  1.50it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.50it/s]Extractor Predicting: 30it [00:20,  1.44it/s]Extractor Predicting: 31it [00:20,  1.52it/s]Extractor Predicting: 32it [00:21,  1.58it/s]Extractor Predicting: 33it [00:21,  1.62it/s]Extractor Predicting: 34it [00:22,  1.64it/s]Extractor Predicting: 35it [00:23,  1.64it/s]Extractor Predicting: 36it [00:23,  1.65it/s]Extractor Predicting: 37it [00:24,  1.67it/s]Extractor Predicting: 38it [00:24,  1.70it/s]Extractor Predicting: 39it [00:25,  1.74it/s]Extractor Predicting: 40it [00:25,  1.76it/s]Extractor Predicting: 41it [00:26,  1.73it/s]Extractor Predicting: 42it [00:27,  1.74it/s]Extractor Predicting: 43it [00:27,  1.72it/s]Extractor Predicting: 44it [00:28,  1.76it/s]Extractor Predicting: 45it [00:28,  1.74it/s]Extractor Predicting: 46it [00:29,  1.69it/s]Extractor Predicting: 47it [00:30,  1.70it/s]Extractor Predicting: 48it [00:30,  1.74it/s]Extractor Predicting: 49it [00:31,  1.76it/s]Extractor Predicting: 50it [00:31,  1.75it/s]Extractor Predicting: 51it [00:32,  1.77it/s]Extractor Predicting: 52it [00:32,  1.74it/s]Extractor Predicting: 53it [00:33,  1.70it/s]Extractor Predicting: 54it [00:34,  1.70it/s]Extractor Predicting: 55it [00:34,  1.67it/s]Extractor Predicting: 56it [00:35,  1.69it/s]Extractor Predicting: 57it [00:35,  1.67it/s]Extractor Predicting: 58it [00:36,  1.65it/s]Extractor Predicting: 59it [00:37,  1.59it/s]Extractor Predicting: 60it [00:37,  1.54it/s]Extractor Predicting: 61it [00:38,  1.49it/s]Extractor Predicting: 62it [00:39,  1.45it/s]Extractor Predicting: 63it [00:40,  1.46it/s]Extractor Predicting: 64it [00:40,  1.44it/s]Extractor Predicting: 65it [00:41,  1.47it/s]Extractor Predicting: 66it [00:42,  1.48it/s]Extractor Predicting: 67it [00:42,  1.48it/s]Extractor Predicting: 68it [00:43,  1.45it/s]Extractor Predicting: 69it [00:44,  1.42it/s]Extractor Predicting: 70it [00:44,  1.42it/s]Extractor Predicting: 71it [00:45,  1.39it/s]Extractor Predicting: 72it [00:46,  1.43it/s]Extractor Predicting: 73it [00:47,  1.43it/s]Extractor Predicting: 74it [00:47,  1.41it/s]Extractor Predicting: 75it [00:48,  1.44it/s]Extractor Predicting: 76it [00:49,  1.45it/s]Extractor Predicting: 77it [00:49,  1.45it/s]Extractor Predicting: 78it [00:50,  1.44it/s]Extractor Predicting: 79it [00:51,  1.39it/s]Extractor Predicting: 80it [00:51,  1.42it/s]Extractor Predicting: 81it [00:52,  1.42it/s]Extractor Predicting: 82it [00:53,  1.42it/s]Extractor Predicting: 83it [00:54,  1.43it/s]Extractor Predicting: 84it [00:54,  1.44it/s]Extractor Predicting: 85it [00:55,  1.44it/s]Extractor Predicting: 86it [00:56,  1.42it/s]Extractor Predicting: 87it [00:56,  1.43it/s]Extractor Predicting: 88it [00:57,  1.46it/s]Extractor Predicting: 89it [00:58,  1.42it/s]Extractor Predicting: 90it [00:58,  1.46it/s]Extractor Predicting: 91it [00:59,  1.44it/s]Extractor Predicting: 92it [01:00,  1.41it/s]Extractor Predicting: 93it [01:00,  1.46it/s]Extractor Predicting: 94it [01:01,  1.45it/s]Extractor Predicting: 95it [01:02,  1.48it/s]Extractor Predicting: 96it [01:02,  1.53it/s]Extractor Predicting: 97it [01:03,  1.53it/s]Extractor Predicting: 98it [01:04,  1.56it/s]Extractor Predicting: 99it [01:04,  1.52it/s]Extractor Predicting: 100it [01:05,  1.53it/s]Extractor Predicting: 101it [01:06,  1.58it/s]Extractor Predicting: 102it [01:06,  1.58it/s]Extractor Predicting: 103it [01:07,  1.54it/s]Extractor Predicting: 104it [01:08,  1.48it/s]Extractor Predicting: 105it [01:08,  1.50it/s]Extractor Predicting: 106it [01:09,  1.50it/s]Extractor Predicting: 107it [01:10,  1.51it/s]Extractor Predicting: 108it [01:10,  1.50it/s]Extractor Predicting: 109it [01:11,  1.35it/s]Extractor Predicting: 110it [01:12,  1.37it/s]Extractor Predicting: 111it [01:13,  1.42it/s]Extractor Predicting: 112it [01:13,  1.41it/s]Extractor Predicting: 113it [01:14,  1.46it/s]Extractor Predicting: 114it [01:15,  1.42it/s]Extractor Predicting: 115it [01:15,  1.44it/s]Extractor Predicting: 116it [01:16,  1.51it/s]Extractor Predicting: 117it [01:17,  1.50it/s]Extractor Predicting: 118it [01:17,  1.56it/s]Extractor Predicting: 119it [01:18,  1.57it/s]Extractor Predicting: 120it [01:18,  1.59it/s]Extractor Predicting: 121it [01:19,  1.62it/s]Extractor Predicting: 122it [01:20,  1.61it/s]Extractor Predicting: 123it [01:20,  1.63it/s]Extractor Predicting: 124it [01:21,  1.62it/s]Extractor Predicting: 125it [01:21,  1.64it/s]Extractor Predicting: 126it [01:22,  1.62it/s]Extractor Predicting: 127it [01:23,  1.62it/s]Extractor Predicting: 128it [01:23,  1.67it/s]Extractor Predicting: 129it [01:24,  1.64it/s]Extractor Predicting: 130it [01:24,  1.72it/s]Extractor Predicting: 131it [01:25,  1.75it/s]Extractor Predicting: 132it [01:26,  1.71it/s]Extractor Predicting: 133it [01:26,  1.67it/s]Extractor Predicting: 134it [01:27,  1.66it/s]Extractor Predicting: 135it [01:27,  1.66it/s]Extractor Predicting: 136it [01:28,  1.69it/s]Extractor Predicting: 137it [01:29,  1.69it/s]Extractor Predicting: 138it [01:29,  1.69it/s]Extractor Predicting: 139it [01:30,  1.66it/s]Extractor Predicting: 140it [01:30,  1.69it/s]Extractor Predicting: 141it [01:31,  1.67it/s]Extractor Predicting: 142it [01:32,  1.73it/s]Extractor Predicting: 143it [01:32,  1.72it/s]Extractor Predicting: 144it [01:33,  1.69it/s]Extractor Predicting: 145it [01:33,  1.68it/s]Extractor Predicting: 146it [01:34,  1.63it/s]Extractor Predicting: 147it [01:35,  1.59it/s]Extractor Predicting: 148it [01:35,  1.59it/s]Extractor Predicting: 149it [01:36,  1.57it/s]Extractor Predicting: 150it [01:37,  1.56it/s]Extractor Predicting: 151it [01:37,  1.56it/s]Extractor Predicting: 152it [01:38,  1.56it/s]Extractor Predicting: 153it [01:39,  1.51it/s]Extractor Predicting: 154it [01:39,  1.54it/s]Extractor Predicting: 155it [01:40,  1.57it/s]Extractor Predicting: 156it [01:40,  1.54it/s]Extractor Predicting: 157it [01:41,  1.54it/s]Extractor Predicting: 158it [01:42,  1.51it/s]Extractor Predicting: 159it [01:42,  1.51it/s]Extractor Predicting: 160it [01:43,  1.50it/s]Extractor Predicting: 161it [01:44,  1.50it/s]Extractor Predicting: 162it [01:44,  1.50it/s]Extractor Predicting: 163it [01:45,  1.52it/s]Extractor Predicting: 164it [01:46,  1.52it/s]Extractor Predicting: 165it [01:46,  1.50it/s]Extractor Predicting: 166it [01:47,  1.47it/s]Extractor Predicting: 167it [01:48,  1.50it/s]Extractor Predicting: 168it [01:49,  1.47it/s]Extractor Predicting: 169it [01:49,  1.51it/s]Extractor Predicting: 170it [01:50,  1.50it/s]Extractor Predicting: 171it [01:50,  1.51it/s]Extractor Predicting: 172it [01:51,  1.53it/s]Extractor Predicting: 173it [01:52,  1.45it/s]Extractor Predicting: 174it [01:53,  1.47it/s]Extractor Predicting: 175it [01:53,  1.50it/s]Extractor Predicting: 176it [01:54,  1.50it/s]Extractor Predicting: 177it [01:55,  1.49it/s]Extractor Predicting: 178it [01:55,  1.48it/s]Extractor Predicting: 179it [01:56,  1.49it/s]Extractor Predicting: 180it [01:57,  1.49it/s]Extractor Predicting: 181it [01:57,  1.50it/s]Extractor Predicting: 182it [01:58,  1.50it/s]Extractor Predicting: 183it [01:59,  1.47it/s]Extractor Predicting: 184it [01:59,  1.48it/s]Extractor Predicting: 185it [02:00,  1.50it/s]Extractor Predicting: 186it [02:01,  1.49it/s]Extractor Predicting: 187it [02:01,  1.51it/s]Extractor Predicting: 188it [02:02,  1.49it/s]Extractor Predicting: 189it [02:03,  1.52it/s]Extractor Predicting: 190it [02:03,  1.52it/s]Extractor Predicting: 191it [02:04,  1.55it/s]Extractor Predicting: 192it [02:04,  1.54it/s]Extractor Predicting: 193it [02:05,  1.56it/s]Extractor Predicting: 194it [02:06,  1.54it/s]Extractor Predicting: 195it [02:06,  1.54it/s]Extractor Predicting: 196it [02:07,  1.54it/s]Extractor Predicting: 197it [02:08,  1.53it/s]Extractor Predicting: 198it [02:08,  1.50it/s]Extractor Predicting: 199it [02:09,  1.49it/s]Extractor Predicting: 200it [02:10,  1.50it/s]Extractor Predicting: 201it [02:10,  1.53it/s]Extractor Predicting: 202it [02:11,  1.51it/s]Extractor Predicting: 203it [02:12,  1.54it/s]Extractor Predicting: 204it [02:12,  1.51it/s]Extractor Predicting: 205it [02:13,  1.51it/s]Extractor Predicting: 206it [02:14,  1.57it/s]Extractor Predicting: 207it [02:14,  1.56it/s]Extractor Predicting: 208it [02:15,  1.59it/s]Extractor Predicting: 209it [02:16,  1.54it/s]Extractor Predicting: 210it [02:16,  1.54it/s]Extractor Predicting: 211it [02:17,  1.52it/s]Extractor Predicting: 212it [02:18,  1.50it/s]Extractor Predicting: 213it [02:18,  1.51it/s]Extractor Predicting: 214it [02:19,  1.49it/s]Extractor Predicting: 215it [02:20,  1.48it/s]Extractor Predicting: 216it [02:20,  1.49it/s]Extractor Predicting: 217it [02:21,  1.46it/s]Extractor Predicting: 218it [02:22,  1.48it/s]Extractor Predicting: 219it [02:23,  1.32it/s]Extractor Predicting: 220it [02:23,  1.40it/s]Extractor Predicting: 221it [02:24,  1.42it/s]Extractor Predicting: 222it [02:25,  1.44it/s]Extractor Predicting: 223it [02:25,  1.48it/s]Extractor Predicting: 224it [02:26,  1.47it/s]Extractor Predicting: 225it [02:27,  1.46it/s]Extractor Predicting: 226it [02:27,  1.49it/s]Extractor Predicting: 227it [02:28,  1.48it/s]Extractor Predicting: 228it [02:29,  1.52it/s]Extractor Predicting: 229it [02:29,  1.48it/s]Extractor Predicting: 230it [02:30,  1.51it/s]Extractor Predicting: 231it [02:31,  1.48it/s]Extractor Predicting: 232it [02:31,  1.48it/s]Extractor Predicting: 233it [02:32,  1.48it/s]Extractor Predicting: 234it [02:33,  1.48it/s]Extractor Predicting: 235it [02:33,  1.50it/s]Extractor Predicting: 236it [02:34,  1.52it/s]Extractor Predicting: 237it [02:35,  1.45it/s]Extractor Predicting: 238it [02:35,  1.45it/s]Extractor Predicting: 239it [02:36,  1.43it/s]Extractor Predicting: 240it [02:37,  1.45it/s]Extractor Predicting: 241it [02:37,  1.45it/s]Extractor Predicting: 242it [02:38,  1.47it/s]Extractor Predicting: 243it [02:39,  1.47it/s]Extractor Predicting: 244it [02:39,  1.48it/s]Extractor Predicting: 245it [02:40,  1.47it/s]Extractor Predicting: 246it [02:41,  1.45it/s]Extractor Predicting: 247it [02:42,  1.40it/s]Extractor Predicting: 248it [02:42,  1.43it/s]Extractor Predicting: 249it [02:43,  1.48it/s]Extractor Predicting: 250it [02:44,  1.49it/s]Extractor Predicting: 251it [02:44,  1.48it/s]Extractor Predicting: 252it [02:45,  1.52it/s]Extractor Predicting: 253it [02:45,  1.52it/s]Extractor Predicting: 254it [02:46,  1.49it/s]Extractor Predicting: 255it [02:47,  1.48it/s]Extractor Predicting: 256it [02:48,  1.46it/s]Extractor Predicting: 257it [02:48,  1.47it/s]Extractor Predicting: 258it [02:49,  1.47it/s]Extractor Predicting: 259it [02:50,  1.50it/s]Extractor Predicting: 260it [02:50,  1.49it/s]Extractor Predicting: 261it [02:51,  1.52it/s]Extractor Predicting: 262it [02:52,  1.52it/s]Extractor Predicting: 263it [02:52,  1.51it/s]Extractor Predicting: 264it [02:53,  1.53it/s]Extractor Predicting: 265it [02:54,  1.51it/s]Extractor Predicting: 266it [02:54,  1.51it/s]Extractor Predicting: 267it [02:55,  1.55it/s]Extractor Predicting: 268it [02:55,  1.55it/s]Extractor Predicting: 269it [02:56,  1.54it/s]Extractor Predicting: 270it [02:57,  1.47it/s]Extractor Predicting: 271it [02:57,  1.53it/s]Extractor Predicting: 272it [02:58,  1.55it/s]Extractor Predicting: 273it [02:59,  1.58it/s]Extractor Predicting: 274it [02:59,  1.60it/s]Extractor Predicting: 275it [03:00,  1.51it/s]Extractor Predicting: 276it [03:01,  1.50it/s]Extractor Predicting: 277it [03:01,  1.52it/s]Extractor Predicting: 278it [03:02,  1.54it/s]Extractor Predicting: 279it [03:03,  1.54it/s]Extractor Predicting: 280it [03:03,  1.51it/s]Extractor Predicting: 281it [03:04,  1.55it/s]Extractor Predicting: 282it [03:05,  1.55it/s]Extractor Predicting: 283it [03:05,  1.51it/s]Extractor Predicting: 284it [03:06,  1.73it/s]Extractor Predicting: 284it [03:06,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:06:16,931 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:06:16,971 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:06:16,971 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:06:16,971 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:06:16,971 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:06:17,576 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:06:17,577 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:06:17,873 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:06:19,005 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:06:19,005 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:06:21,259 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:06:21,288 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:06:21,288 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:06:21,288 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:06:21,288 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:06:21,818 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:06:21,819 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:06:22,133 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:06:22,351 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:06:22,351 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.31769204031977755,
  "recall": 0.13437224345780652,
  "score": 0.18886248579398698,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1045
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1145, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:02,  1.36it/s]Extractor Predicting: 4it [00:02,  1.39it/s]Extractor Predicting: 5it [00:03,  1.87it/s]Extractor Predicting: 5it [00:03,  1.63it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.27586206896551724,
  "recall": 0.04040404040404041,
  "score": 0.07048458149779736,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_10_seed_3/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_10_seed_3', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_3/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_3/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:49, 16.38s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:33<03:38, 16.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:49<03:16, 16.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:03<02:51, 15.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:19<02:36, 15.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:35<02:21, 15.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:50<02:04, 15.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:07<01:51, 15.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:22<01:34, 15.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:37<01:17, 15.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:52<01:01, 15.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:08<00:46, 15.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:22<00:29, 14.91s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:38<00:15, 15.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:55<00:00, 15.98s/it]Generating: 100%|██████████| 15/15 [03:55<00:00, 15.73s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
['Relation : country . Context : The World Cup won the inaugural competition under the FIFA Confederation Cup 1994 , but at different times it had been contested in different competitions around the world . Head Entity : World Cup , Tail Entity : Brazil .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 573, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : country .', 'success_rate': 0.8165760869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 431, 'raw': 544}
{'target': 600, 'success': 454, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 503, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 580, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : place of death .', 'success_rate': 0.7838541666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 324, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 445, 'raw': 576}
{'target': 600, 'success': 470, 'raw': 608}
{'target': 600, 'success': 499, 'raw': 640}
{'target': 600, 'success': 526, 'raw': 672}
{'target': 600, 'success': 552, 'raw': 704}
{'target': 600, 'success': 583, 'raw': 736}
{'target': 600, 'success': 604, 'raw': 768}
{'prompt': 'Relation : production company .', 'success_rate': 0.7864583333333334, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8328804347826086, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : continent . Context : The Ciancian is a class of mountain range s that is located north of the Andes , between the Andes and the Pangaea , in what is now western Peru . Head Entity : The Andes , Tail Entity : Peru .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 402, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 455, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 614, 'raw': 800}
{'prompt': 'Relation : continent .', 'success_rate': 0.7675, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Royal Navy', 'continent', '', 'Sir John O. Kiefferford was the grandson of Major O. Kiefferford ( 1829 1882 ) , who later became Royal Navy captain .')"}}
['Relation : field of this occupation . Context : The song was nominated for the Grammy Award for Best New Artist at the 2004 MTV Video Music Awards , alongside artists such as Ariana Grande , Kacey Musgraves , and Katy Perry . Head Entity : Ariana Grande , Tail Entity : the song .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 487, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.8125, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 300, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 383, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 563, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7981770833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Royal Navy', 'field of work', '', 'He was commissioned to the Royal Navy in 1916 , where he was appointed Royal Pilot of the Royal Navy .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : founded by .', 'success_rate': 0.8165760869565217, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 604, 'raw': 736}
{'prompt': 'Relation : movement .', 'success_rate': 0.8206521739130435, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 565, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8383152173913043, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : performer .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('The Sun', 'performer', '', 'The night was also attended by performances by the BBC as well as The Edge , The Wire , The Voice , The Good Wife , and The Sun .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : producer .', 'success_rate': 0.9032738095238095, 'errors': {''}}
['Relation : record label . Context : Later in 2008 , he recorded his third studio album , The All Day , entitled All Day And I , released in September 2010 . Head Entity : The All Day , Tail Entity : Record label .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 321, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 397, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 446, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 487, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 604, 'raw': 800}
{'prompt': 'Relation : record label .', 'success_rate': 0.755, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('George Hormatsky', 'record label', '', 'A compilation album released in 2008 , it featured EPs All The Way Down ( with Keith Urban ) , I Wanna Leave You ( with Dave Chappelle ) and My Uncle , My Cousin Vinny ( with George Hormatsky ) .')"}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 597, 'raw': 768}
{'target': 600, 'success': 619, 'raw': 800}
{'prompt': 'Relation : replaces .', 'success_rate': 0.77375, 'errors': {'', 'too many values to unpack (expected 2)', "('Cimmerian', 'replaces', '', 'According to Greek law , Nefarius lived at the shrine of Bacchus ( The Temple of Bacchus ) for 100 years until he surrendered to the Phrygian Cimmerian , who led the Greeks to believe that the god Bacchus is nefarius .')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/0_ext.jsonl'}}
estimate vocab size: 14774
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14874, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:15, 15.83s/it]Extractor Estimating: 2it [00:18,  7.91s/it]Extractor Estimating: 3it [00:18,  4.57s/it]Extractor Estimating: 4it [00:19,  2.98s/it]Extractor Estimating: 5it [00:19,  2.13s/it]Extractor Estimating: 6it [00:20,  1.61s/it]Extractor Estimating: 7it [00:21,  1.49s/it]Extractor Estimating: 8it [00:22,  1.21s/it]Extractor Estimating: 9it [00:23,  1.02s/it]Extractor Estimating: 10it [00:23,  1.12it/s]Extractor Estimating: 11it [00:24,  1.01s/it]Extractor Estimating: 12it [00:25,  1.13it/s]Extractor Estimating: 13it [00:26,  1.26it/s]Extractor Estimating: 14it [00:26,  1.30it/s]Extractor Estimating: 15it [00:27,  1.38it/s]Extractor Estimating: 16it [00:28,  1.45it/s]Extractor Estimating: 17it [00:28,  1.49it/s]Extractor Estimating: 18it [00:29,  1.51it/s]Extractor Estimating: 19it [00:29,  1.51it/s]Extractor Estimating: 20it [00:30,  1.51it/s]Extractor Estimating: 21it [00:31,  1.56it/s]Extractor Estimating: 22it [00:31,  1.60it/s]Extractor Estimating: 23it [00:32,  1.57it/s]Extractor Estimating: 24it [00:33,  1.58it/s]Extractor Estimating: 25it [00:33,  1.62it/s]Extractor Estimating: 26it [00:34,  1.60it/s]Extractor Estimating: 27it [00:35,  1.55it/s]Extractor Estimating: 28it [00:35,  1.55it/s]Extractor Estimating: 29it [00:36,  1.52it/s]Extractor Estimating: 30it [00:37,  1.46it/s]Extractor Estimating: 31it [00:38,  1.03s/it]Extractor Estimating: 32it [00:39,  1.10it/s]Extractor Estimating: 33it [00:40,  1.19it/s]Extractor Estimating: 34it [00:40,  1.29it/s]Extractor Estimating: 35it [00:41,  1.32it/s]Extractor Estimating: 36it [00:42,  1.36it/s]Extractor Estimating: 37it [00:42,  1.39it/s]Extractor Estimating: 38it [00:43,  1.43it/s]Extractor Estimating: 39it [00:44,  1.47it/s]Extractor Estimating: 40it [00:45,  1.41it/s]Extractor Estimating: 41it [00:45,  1.42it/s]Extractor Estimating: 42it [00:46,  1.47it/s]Extractor Estimating: 43it [00:47,  1.41it/s]Extractor Estimating: 44it [00:47,  1.44it/s]Extractor Estimating: 45it [00:48,  1.47it/s]Extractor Estimating: 46it [00:49,  1.49it/s]Extractor Estimating: 47it [00:49,  1.49it/s]Extractor Estimating: 48it [00:50,  1.52it/s]Extractor Estimating: 49it [00:51,  1.50it/s]Extractor Estimating: 50it [00:51,  1.47it/s]Extractor Estimating: 51it [00:52,  1.51it/s]Extractor Estimating: 52it [00:52,  1.55it/s]Extractor Estimating: 53it [00:53,  1.57it/s]Extractor Estimating: 54it [00:54,  1.55it/s]Extractor Estimating: 55it [00:54,  1.51it/s]Extractor Estimating: 56it [00:55,  1.52it/s]Extractor Estimating: 57it [00:56,  1.54it/s]Extractor Estimating: 58it [00:56,  1.53it/s]Extractor Estimating: 59it [00:57,  1.53it/s]Extractor Estimating: 60it [00:58,  1.55it/s]Extractor Estimating: 61it [00:58,  1.61it/s]Extractor Estimating: 62it [00:59,  1.58it/s]Extractor Estimating: 63it [01:00,  1.59it/s]Extractor Estimating: 64it [01:00,  1.61it/s]Extractor Estimating: 65it [01:01,  1.55it/s]Extractor Estimating: 66it [01:01,  1.60it/s]Extractor Estimating: 67it [01:02,  1.60it/s]Extractor Estimating: 68it [01:03,  1.67it/s]Extractor Estimating: 69it [01:03,  1.64it/s]Extractor Estimating: 70it [01:04,  1.61it/s]Extractor Estimating: 71it [01:05,  1.59it/s]Extractor Estimating: 72it [01:05,  1.57it/s]Extractor Estimating: 73it [01:06,  1.51it/s]Extractor Estimating: 74it [01:06,  1.57it/s]Extractor Estimating: 75it [01:07,  1.52it/s]Extractor Estimating: 76it [01:08,  1.56it/s]Extractor Estimating: 77it [01:08,  1.59it/s]Extractor Estimating: 78it [01:09,  1.61it/s]Extractor Estimating: 79it [01:10,  1.57it/s]Extractor Estimating: 80it [01:10,  1.59it/s]Extractor Estimating: 81it [01:11,  1.58it/s]Extractor Estimating: 82it [01:12,  1.57it/s]Extractor Estimating: 83it [01:12,  1.58it/s]Extractor Estimating: 84it [01:13,  1.23it/s]Extractor Estimating: 85it [01:14,  1.32it/s]Extractor Estimating: 86it [01:15,  1.36it/s]Extractor Estimating: 87it [01:15,  1.42it/s]Extractor Estimating: 88it [01:16,  1.44it/s]Extractor Estimating: 89it [01:17,  1.49it/s]Extractor Estimating: 90it [01:17,  1.50it/s]Extractor Estimating: 91it [01:18,  1.53it/s]Extractor Estimating: 92it [01:18,  1.58it/s]Extractor Estimating: 93it [01:19,  1.58it/s]Extractor Estimating: 94it [01:20,  1.57it/s]Extractor Estimating: 95it [01:20,  1.59it/s]Extractor Estimating: 96it [01:21,  1.57it/s]Extractor Estimating: 97it [01:22,  1.61it/s]Extractor Estimating: 98it [01:22,  1.59it/s]Extractor Estimating: 99it [01:23,  1.59it/s]Extractor Estimating: 100it [01:24,  1.57it/s]Extractor Estimating: 101it [01:24,  1.60it/s]Extractor Estimating: 102it [01:25,  1.60it/s]Extractor Estimating: 103it [01:25,  1.59it/s]Extractor Estimating: 104it [01:26,  1.63it/s]Extractor Estimating: 105it [01:27,  1.66it/s]Extractor Estimating: 106it [01:27,  1.66it/s]Extractor Estimating: 107it [01:28,  1.66it/s]Extractor Estimating: 108it [01:28,  1.69it/s]Extractor Estimating: 109it [01:29,  1.64it/s]Extractor Estimating: 110it [01:30,  1.48it/s]Extractor Estimating: 111it [01:31,  1.47it/s]Extractor Estimating: 112it [01:31,  1.56it/s]Extractor Estimating: 113it [01:32,  1.60it/s]Extractor Estimating: 114it [01:32,  1.63it/s]Extractor Estimating: 115it [01:33,  1.60it/s]Extractor Estimating: 116it [01:34,  1.52it/s]Extractor Estimating: 117it [01:34,  1.59it/s]Extractor Estimating: 118it [01:35,  1.61it/s]Extractor Estimating: 119it [01:36,  1.52it/s]Extractor Estimating: 120it [01:36,  1.54it/s]Extractor Estimating: 121it [01:37,  1.57it/s]Extractor Estimating: 122it [01:37,  1.52it/s]Extractor Estimating: 123it [01:38,  1.53it/s]Extractor Estimating: 124it [01:39,  1.57it/s]Extractor Estimating: 125it [01:39,  1.53it/s]Extractor Estimating: 126it [01:40,  1.59it/s]Extractor Estimating: 127it [01:41,  1.58it/s]Extractor Estimating: 128it [01:41,  1.66it/s]Extractor Estimating: 129it [01:42,  1.69it/s]Extractor Estimating: 130it [01:42,  1.69it/s]Extractor Estimating: 131it [01:43,  1.69it/s]Extractor Estimating: 132it [01:44,  1.24it/s]Extractor Estimating: 133it [01:45,  1.30it/s]Extractor Estimating: 134it [01:46,  1.37it/s]Extractor Estimating: 135it [01:46,  1.45it/s]Extractor Estimating: 136it [01:47,  1.52it/s]Extractor Estimating: 137it [01:47,  1.60it/s]Extractor Estimating: 138it [01:48,  1.59it/s]Extractor Estimating: 139it [01:48,  1.62it/s]Extractor Estimating: 140it [01:49,  1.63it/s]Extractor Estimating: 141it [01:50,  1.67it/s]Extractor Estimating: 142it [01:50,  1.71it/s]Extractor Estimating: 143it [01:51,  1.59it/s]Extractor Estimating: 144it [01:52,  1.64it/s]Extractor Estimating: 145it [01:52,  1.65it/s]Extractor Estimating: 146it [01:53,  1.60it/s]Extractor Estimating: 147it [01:53,  1.65it/s]Extractor Estimating: 148it [01:54,  1.64it/s]Extractor Estimating: 149it [01:55,  1.64it/s]Extractor Estimating: 150it [01:55,  1.67it/s]Extractor Estimating: 151it [01:56,  1.66it/s]Extractor Estimating: 152it [01:56,  1.69it/s]Extractor Estimating: 153it [01:57,  1.67it/s]Extractor Estimating: 154it [01:58,  1.65it/s]Extractor Estimating: 155it [01:58,  1.67it/s]Extractor Estimating: 156it [01:59,  1.61it/s]Extractor Estimating: 157it [01:59,  1.60it/s]Extractor Estimating: 158it [02:00,  1.58it/s]Extractor Estimating: 159it [02:01,  1.54it/s]Extractor Estimating: 160it [02:01,  1.53it/s]Extractor Estimating: 161it [02:02,  1.52it/s]Extractor Estimating: 162it [02:03,  1.50it/s]Extractor Estimating: 163it [02:03,  1.54it/s]Extractor Estimating: 164it [02:04,  1.56it/s]Extractor Estimating: 165it [02:05,  1.58it/s]Extractor Estimating: 166it [02:05,  1.57it/s]Extractor Estimating: 167it [02:06,  1.60it/s]Extractor Estimating: 168it [02:06,  1.63it/s]Extractor Estimating: 169it [02:07,  1.59it/s]Extractor Estimating: 170it [02:08,  1.60it/s]Extractor Estimating: 171it [02:08,  1.64it/s]Extractor Estimating: 172it [02:09,  1.65it/s]Extractor Estimating: 173it [02:10,  1.64it/s]Extractor Estimating: 174it [02:10,  1.60it/s]Extractor Estimating: 175it [02:11,  1.57it/s]Extractor Estimating: 176it [02:12,  1.54it/s]Extractor Estimating: 177it [02:12,  1.54it/s]Extractor Estimating: 178it [02:13,  1.53it/s]Extractor Estimating: 179it [02:13,  1.56it/s]Extractor Estimating: 180it [02:14,  1.60it/s]Extractor Estimating: 181it [02:15,  1.49it/s]Extractor Estimating: 182it [02:15,  1.52it/s]Extractor Estimating: 183it [02:16,  1.54it/s]Extractor Estimating: 184it [02:17,  1.52it/s]Extractor Estimating: 185it [02:17,  1.54it/s]Extractor Estimating: 186it [02:18,  1.50it/s]Extractor Estimating: 187it [02:19,  1.56it/s]Extractor Estimating: 188it [02:19,  1.56it/s]Extractor Estimating: 189it [02:20,  1.60it/s]Extractor Estimating: 190it [02:21,  1.59it/s]Extractor Estimating: 191it [02:21,  1.63it/s]Extractor Estimating: 192it [02:22,  1.58it/s]Extractor Estimating: 193it [02:22,  1.56it/s]Extractor Estimating: 194it [02:23,  1.61it/s]Extractor Estimating: 195it [02:24,  1.56it/s]Extractor Estimating: 196it [02:24,  1.57it/s]Extractor Estimating: 197it [02:25,  1.57it/s]Extractor Estimating: 198it [02:26,  1.64it/s]Extractor Estimating: 199it [02:26,  1.65it/s]Extractor Estimating: 200it [02:27,  1.49it/s]Extractor Estimating: 201it [02:28,  1.53it/s]Extractor Estimating: 202it [02:28,  1.49it/s]Extractor Estimating: 203it [02:29,  1.48it/s]Extractor Estimating: 204it [02:30,  1.53it/s]Extractor Estimating: 205it [02:30,  1.38it/s]Extractor Estimating: 206it [02:31,  1.44it/s]Extractor Estimating: 207it [02:32,  1.48it/s]Extractor Estimating: 208it [02:32,  1.51it/s]Extractor Estimating: 209it [02:33,  1.54it/s]Extractor Estimating: 210it [02:34,  1.54it/s]Extractor Estimating: 211it [02:34,  1.54it/s]Extractor Estimating: 212it [02:35,  1.56it/s]Extractor Estimating: 213it [02:36,  1.57it/s]Extractor Estimating: 214it [02:36,  1.57it/s]Extractor Estimating: 215it [02:37,  1.56it/s]Extractor Estimating: 216it [02:37,  1.58it/s]Extractor Estimating: 217it [02:38,  1.61it/s]Extractor Estimating: 218it [02:39,  1.56it/s]Extractor Estimating: 219it [02:39,  1.58it/s]Extractor Estimating: 220it [02:40,  1.57it/s]Extractor Estimating: 221it [02:41,  1.56it/s]Extractor Estimating: 222it [02:41,  1.53it/s]Extractor Estimating: 223it [02:42,  1.58it/s]Extractor Estimating: 224it [02:43,  1.60it/s]Extractor Estimating: 225it [02:43,  1.57it/s]Extractor Estimating: 226it [02:44,  1.57it/s]Extractor Estimating: 227it [02:44,  1.57it/s]Extractor Estimating: 228it [02:45,  1.56it/s]Extractor Estimating: 229it [02:46,  1.57it/s]Extractor Estimating: 230it [02:46,  1.52it/s]Extractor Estimating: 231it [02:47,  1.47it/s]Extractor Estimating: 232it [02:48,  1.51it/s]Extractor Estimating: 233it [02:48,  1.53it/s]Extractor Estimating: 234it [02:49,  1.58it/s]Extractor Estimating: 235it [02:50,  1.57it/s]Extractor Estimating: 236it [02:50,  1.62it/s]Extractor Estimating: 237it [02:51,  1.59it/s]Extractor Estimating: 238it [02:51,  1.64it/s]Extractor Estimating: 239it [02:52,  1.67it/s]Extractor Estimating: 240it [02:53,  1.61it/s]Extractor Estimating: 241it [02:53,  1.61it/s]Extractor Estimating: 242it [02:54,  1.55it/s]Extractor Estimating: 243it [02:55,  1.54it/s]Extractor Estimating: 244it [02:55,  1.53it/s]Extractor Estimating: 245it [02:56,  1.54it/s]Extractor Estimating: 246it [02:57,  1.55it/s]Extractor Estimating: 247it [02:57,  1.53it/s]Extractor Estimating: 248it [02:58,  1.58it/s]Extractor Estimating: 249it [02:58,  1.58it/s]Extractor Estimating: 250it [02:59,  1.60it/s]Extractor Estimating: 251it [03:00,  1.56it/s]Extractor Estimating: 252it [03:00,  1.54it/s]Extractor Estimating: 253it [03:01,  1.58it/s]Extractor Estimating: 254it [03:02,  1.61it/s]Extractor Estimating: 255it [03:02,  1.67it/s]Extractor Estimating: 256it [03:03,  1.66it/s]Extractor Estimating: 257it [03:03,  1.65it/s]Extractor Estimating: 258it [03:04,  1.66it/s]Extractor Estimating: 259it [03:05,  1.67it/s]Extractor Estimating: 260it [03:05,  1.61it/s]Extractor Estimating: 261it [03:06,  1.60it/s]Extractor Estimating: 262it [03:06,  1.62it/s]Extractor Estimating: 263it [03:07,  1.63it/s]Extractor Estimating: 264it [03:08,  1.65it/s]Extractor Estimating: 265it [03:08,  1.64it/s]Extractor Estimating: 266it [03:09,  1.61it/s]Extractor Estimating: 267it [03:10,  1.62it/s]Extractor Estimating: 268it [03:10,  1.67it/s]Extractor Estimating: 269it [03:11,  1.52it/s]Extractor Estimating: 270it [03:11,  1.59it/s]Extractor Estimating: 271it [03:12,  1.59it/s]Extractor Estimating: 272it [03:13,  1.53it/s]Extractor Estimating: 273it [03:13,  1.61it/s]Extractor Estimating: 274it [03:14,  1.61it/s]Extractor Estimating: 275it [03:15,  1.62it/s]Extractor Estimating: 276it [03:15,  1.62it/s]Extractor Estimating: 277it [03:16,  1.54it/s]Extractor Estimating: 278it [03:17,  1.54it/s]Extractor Estimating: 279it [03:17,  1.53it/s]Extractor Estimating: 280it [03:18,  1.57it/s]Extractor Estimating: 281it [03:18,  1.57it/s]Extractor Estimating: 282it [03:19,  1.57it/s]Extractor Estimating: 283it [03:20,  1.59it/s]Extractor Estimating: 284it [03:20,  1.60it/s]Extractor Estimating: 285it [03:21,  1.60it/s]Extractor Estimating: 286it [03:22,  1.51it/s]Extractor Estimating: 287it [03:23,  1.38it/s]Extractor Estimating: 288it [03:23,  1.43it/s]Extractor Estimating: 289it [03:24,  1.45it/s]Extractor Estimating: 290it [03:24,  1.55it/s]Extractor Estimating: 291it [03:25,  1.55it/s]Extractor Estimating: 292it [03:26,  1.58it/s]Extractor Estimating: 293it [03:26,  1.62it/s]Extractor Estimating: 294it [03:27,  1.60it/s]Extractor Estimating: 295it [03:28,  1.62it/s]Extractor Estimating: 296it [03:28,  1.57it/s]Extractor Estimating: 297it [03:29,  1.59it/s]Extractor Estimating: 298it [03:29,  1.59it/s]Extractor Estimating: 299it [03:30,  1.61it/s]Extractor Estimating: 300it [03:31,  1.59it/s]Extractor Estimating: 301it [03:31,  1.57it/s]Extractor Estimating: 302it [03:32,  1.54it/s]Extractor Estimating: 303it [03:33,  1.56it/s]Extractor Estimating: 304it [03:33,  1.59it/s]Extractor Estimating: 305it [03:34,  1.58it/s]Extractor Estimating: 306it [03:35,  1.59it/s]Extractor Estimating: 307it [03:35,  1.48it/s]Extractor Estimating: 308it [03:36,  1.48it/s]Extractor Estimating: 309it [03:37,  1.56it/s]Extractor Estimating: 310it [03:37,  1.59it/s]Extractor Estimating: 311it [03:38,  1.60it/s]Extractor Estimating: 312it [03:38,  1.58it/s]Extractor Estimating: 313it [03:39,  1.57it/s]Extractor Estimating: 314it [03:40,  1.52it/s]Extractor Estimating: 315it [03:40,  1.61it/s]Extractor Estimating: 316it [03:41,  1.62it/s]Extractor Estimating: 317it [03:42,  1.54it/s]Extractor Estimating: 318it [03:42,  1.60it/s]Extractor Estimating: 319it [03:43,  1.59it/s]Extractor Estimating: 320it [03:44,  1.52it/s]Extractor Estimating: 321it [03:44,  1.52it/s]Extractor Estimating: 322it [03:45,  1.51it/s]Extractor Estimating: 323it [03:45,  1.55it/s]Extractor Estimating: 324it [03:46,  1.56it/s]Extractor Estimating: 325it [03:47,  1.56it/s]Extractor Estimating: 326it [03:47,  1.55it/s]Extractor Estimating: 327it [03:48,  1.51it/s]Extractor Estimating: 328it [03:49,  1.55it/s]Extractor Estimating: 329it [03:49,  1.53it/s]Extractor Estimating: 330it [03:50,  1.52it/s]Extractor Estimating: 331it [03:51,  1.51it/s]Extractor Estimating: 332it [03:51,  1.53it/s]Extractor Estimating: 333it [03:52,  1.57it/s]Extractor Estimating: 334it [03:53,  1.53it/s]Extractor Estimating: 335it [03:53,  1.54it/s]Extractor Estimating: 336it [03:54,  1.52it/s]Extractor Estimating: 337it [03:55,  1.51it/s]Extractor Estimating: 338it [03:55,  1.49it/s]Extractor Estimating: 339it [03:56,  1.45it/s]Extractor Estimating: 340it [03:57,  1.50it/s]Extractor Estimating: 341it [03:57,  1.52it/s]Extractor Estimating: 342it [03:58,  1.43it/s]Extractor Estimating: 343it [03:59,  1.49it/s]Extractor Estimating: 344it [03:59,  1.49it/s]Extractor Estimating: 345it [04:00,  1.44it/s]Extractor Estimating: 346it [04:01,  1.41it/s]Extractor Estimating: 347it [04:02,  1.40it/s]Extractor Estimating: 348it [04:02,  1.40it/s]Extractor Estimating: 349it [04:03,  1.41it/s]Extractor Estimating: 350it [04:04,  1.46it/s]Extractor Estimating: 351it [04:04,  1.51it/s]Extractor Estimating: 352it [04:05,  1.57it/s]Extractor Estimating: 353it [04:05,  1.55it/s]Extractor Estimating: 354it [04:06,  1.53it/s]Extractor Estimating: 355it [04:07,  1.56it/s]Extractor Estimating: 356it [04:07,  1.59it/s]Extractor Estimating: 357it [04:08,  1.43it/s]Extractor Estimating: 358it [04:09,  1.44it/s]Extractor Estimating: 359it [04:10,  1.44it/s]Extractor Estimating: 360it [04:10,  1.45it/s]Extractor Estimating: 361it [04:11,  1.46it/s]Extractor Estimating: 362it [04:12,  1.48it/s]Extractor Estimating: 363it [04:12,  1.53it/s]Extractor Estimating: 364it [04:13,  1.48it/s]Extractor Estimating: 365it [04:14,  1.49it/s]Extractor Estimating: 366it [04:14,  1.54it/s]Extractor Estimating: 367it [04:15,  1.51it/s]Extractor Estimating: 368it [04:16,  1.52it/s]Extractor Estimating: 369it [04:16,  1.52it/s]Extractor Estimating: 370it [04:17,  1.55it/s]Extractor Estimating: 371it [04:17,  1.56it/s]Extractor Estimating: 372it [04:18,  1.57it/s]Extractor Estimating: 373it [04:19,  1.58it/s]Extractor Estimating: 374it [04:19,  1.58it/s]Extractor Estimating: 375it [04:20,  1.66it/s]Extractor Estimating: 375it [04:20,  1.44it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7402 mean pseudo reward: 0.9454169431009412
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl'}
train vocab size: 32842
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 32942, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=32942, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.328, loss:2912.4693
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.001, loss:2146.9816
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.988, loss:1736.0617
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 91, avg_time 1.009, loss:1582.5099
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 191, avg_time 0.995, loss:1610.9433
>> valid entity prec:0.4840, rec:0.4892, f1:0.4866
>> valid relation prec:0.3908, rec:0.0188, f1:0.0359
>> valid relation with NER prec:0.3908, rec:0.0188, f1:0.0359
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 291, avg_time 3.665, loss:1536.6189
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 82, avg_time 0.989, loss:1443.4007
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 182, avg_time 1.004, loss:1407.2858
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 282, avg_time 1.012, loss:1383.1672
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 73, avg_time 0.994, loss:1314.3904
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5263, rec:0.3800, f1:0.4413
>> valid relation prec:0.3887, rec:0.0364, f1:0.0665
>> valid relation with NER prec:0.3887, rec:0.0364, f1:0.0665
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 173, avg_time 3.661, loss:1227.8021
g_step 1200, step 273, avg_time 0.987, loss:1207.4722
g_step 1300, step 64, avg_time 1.008, loss:1156.8813
g_step 1400, step 164, avg_time 0.994, loss:1133.4494
g_step 1500, step 264, avg_time 0.993, loss:1107.9411
>> valid entity prec:0.5403, rec:0.3724, f1:0.4409
>> valid relation prec:0.3675, rec:0.0281, f1:0.0521
>> valid relation with NER prec:0.3675, rec:0.0281, f1:0.0521
g_step 1600, step 55, avg_time 3.620, loss:1086.8260
g_step 1700, step 155, avg_time 1.000, loss:1058.5770
g_step 1800, step 255, avg_time 0.999, loss:1061.7708
g_step 1900, step 46, avg_time 1.001, loss:1017.2437
g_step 2000, step 146, avg_time 1.000, loss:1021.9362
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5323, rec:0.3978, f1:0.4553
>> valid relation prec:0.2472, rec:0.0184, f1:0.0342
>> valid relation with NER prec:0.2472, rec:0.0184, f1:0.0342
g_step 2100, step 246, avg_time 3.638, loss:984.1752
g_step 2200, step 37, avg_time 1.010, loss:1011.0361
g_step 2300, step 137, avg_time 1.019, loss:952.4347
g_step 2400, step 237, avg_time 1.007, loss:952.6488
g_step 2500, step 28, avg_time 0.992, loss:921.8396
>> valid entity prec:0.5250, rec:0.3601, f1:0.4272
>> valid relation prec:0.3228, rec:0.0250, f1:0.0464
>> valid relation with NER prec:0.3228, rec:0.0250, f1:0.0464
g_step 2600, step 128, avg_time 3.670, loss:920.9583
g_step 2700, step 228, avg_time 1.013, loss:898.5880
g_step 2800, step 19, avg_time 0.997, loss:933.1000
g_step 2900, step 119, avg_time 1.013, loss:854.4141
g_step 3000, step 219, avg_time 1.012, loss:906.3107
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5347, rec:0.3750, f1:0.4409
>> valid relation prec:0.2578, rec:0.0407, f1:0.0703
>> valid relation with NER prec:0.2578, rec:0.0407, f1:0.0703
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 10, avg_time 3.664, loss:901.2225
g_step 3200, step 110, avg_time 0.989, loss:818.1689
g_step 3300, step 210, avg_time 1.009, loss:848.6336
g_step 3400, step 1, avg_time 1.005, loss:839.7466
g_step 3500, step 101, avg_time 1.003, loss:810.7246
>> valid entity prec:0.5003, rec:0.4166, f1:0.4547
>> valid relation prec:0.2220, rec:0.0350, f1:0.0604
>> valid relation with NER prec:0.2220, rec:0.0350, f1:0.0604
g_step 3600, step 201, avg_time 3.656, loss:822.1929
g_step 3700, step 301, avg_time 0.997, loss:815.6691
g_step 3800, step 92, avg_time 0.985, loss:783.6586
g_step 3900, step 192, avg_time 0.999, loss:766.8102
g_step 4000, step 292, avg_time 1.001, loss:815.2173
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4955, rec:0.4197, f1:0.4545
>> valid relation prec:0.2170, rec:0.0283, f1:0.0501
>> valid relation with NER prec:0.2170, rec:0.0283, f1:0.0501
g_step 4100, step 83, avg_time 3.629, loss:723.1468
g_step 4200, step 183, avg_time 1.003, loss:750.1279
g_step 4300, step 283, avg_time 0.987, loss:781.9247
g_step 4400, step 74, avg_time 0.997, loss:710.5014
g_step 4500, step 174, avg_time 1.011, loss:721.2892
>> valid entity prec:0.4852, rec:0.4411, f1:0.4621
>> valid relation prec:0.2031, rec:0.0334, f1:0.0574
>> valid relation with NER prec:0.2031, rec:0.0334, f1:0.0574
g_step 4600, step 274, avg_time 3.613, loss:750.2405
g_step 4700, step 65, avg_time 0.990, loss:704.8137
g_step 4800, step 165, avg_time 0.993, loss:711.5935
g_step 4900, step 265, avg_time 0.984, loss:706.6793
g_step 5000, step 56, avg_time 0.999, loss:686.4701
learning rate was adjusted to 0.0008
>> valid entity prec:0.5038, rec:0.4515, f1:0.4762
>> valid relation prec:0.2017, rec:0.0378, f1:0.0636
>> valid relation with NER prec:0.2017, rec:0.0378, f1:0.0636
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 5100, step 156, avg_time 3.643, loss:669.2815
g_step 5200, step 256, avg_time 1.003, loss:697.7104
g_step 5300, step 47, avg_time 1.001, loss:674.5140
g_step 5400, step 147, avg_time 1.001, loss:656.1906
g_step 5500, step 247, avg_time 0.998, loss:645.0555
>> valid entity prec:0.4699, rec:0.4715, f1:0.4707
>> valid relation prec:0.1291, rec:0.0293, f1:0.0478
>> valid relation with NER prec:0.1291, rec:0.0293, f1:0.0478
g_step 5600, step 38, avg_time 3.623, loss:636.7706
g_step 5700, step 138, avg_time 1.000, loss:608.8139
g_step 5800, step 238, avg_time 0.990, loss:633.7019
g_step 5900, step 29, avg_time 0.992, loss:626.3997
g_step 6000, step 129, avg_time 0.999, loss:605.5567
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4855, rec:0.4310, f1:0.4566
>> valid relation prec:0.2167, rec:0.0406, f1:0.0684
>> valid relation with NER prec:0.2167, rec:0.0406, f1:0.0684
g_step 6100, step 229, avg_time 3.632, loss:627.0856
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 02:59:21 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 02:59:21 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_02-59-21_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 02:59:22 - WARNING - datasets.builder -   Using custom data configuration default-2155aa4404f37888
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-2155aa4404f37888/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  3.85 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 02:59:25,825 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:59:25,826 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 02:59:25,826 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:59:25,827 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 02:59:25,968 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:59:26,024 >> loading file outputs/wrapper/wiki/unseen_10_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:59:26,024 >> loading file outputs/wrapper/wiki/unseen_10_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:59:26,024 >> loading file outputs/wrapper/wiki/unseen_10_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:59:26,024 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:59:26,024 >> loading file outputs/wrapper/wiki/unseen_10_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:59:26,024 >> loading file outputs/wrapper/wiki/unseen_10_seed_3/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 02:59:26,499 >> loading weights file outputs/wrapper/wiki/unseen_10_seed_3/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 02:59:29,649 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 02:59:29,669 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_10_seed_3/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-2155aa4404f37888/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_10_seed_3/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/29/2023 02:59:29 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x150278e6fef0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:05,  1.19ba/s] 25%|██▌       | 2/8 [00:01<00:02,  2.11ba/s] 38%|███▊      | 3/8 [00:01<00:02,  2.38ba/s] 50%|█████     | 4/8 [00:01<00:01,  2.93ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.37ba/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.71ba/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.95ba/s]100%|██████████| 8/8 [00:02<00:00,  4.73ba/s]100%|██████████| 8/8 [00:02<00:00,  3.32ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:02,  3.13ba/s] 22%|██▏       | 2/9 [00:00<00:01,  3.84ba/s] 33%|███▎      | 3/9 [00:00<00:01,  3.86ba/s] 44%|████▍     | 4/9 [00:01<00:01,  4.14ba/s] 56%|█████▌    | 5/9 [00:01<00:00,  4.30ba/s] 67%|██████▋   | 6/9 [00:01<00:00,  4.42ba/s] 78%|███████▊  | 7/9 [00:01<00:00,  4.49ba/s] 89%|████████▉ | 8/9 [00:01<00:00,  4.52ba/s]100%|██████████| 9/9 [00:02<00:00,  5.20ba/s]100%|██████████| 9/9 [00:02<00:00,  4.49ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  5.35ba/s] 38%|███▊      | 3/8 [00:00<00:00,  8.71ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.79ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.30ba/s]100%|██████████| 8/8 [00:00<00:00, 10.31ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:01,  5.63ba/s] 33%|███▎      | 3/9 [00:00<00:00,  8.71ba/s] 44%|████▍     | 4/9 [00:00<00:00,  6.05ba/s] 67%|██████▋   | 6/9 [00:00<00:00,  7.76ba/s] 89%|████████▉ | 8/9 [00:00<00:00,  8.83ba/s]100%|██████████| 9/9 [00:01<00:00,  8.62ba/s]
[INFO|trainer.py:414] 2023-08-29 02:59:37,023 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 02:59:37,090 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 02:59:37,090 >>   Num examples = 7529
[INFO|trainer.py:1149] 2023-08-29 02:59:37,090 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 02:59:37,090 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 02:59:37,090 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 02:59:37,090 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 02:59:37,091 >>   Total optimization steps = 590
  0%|          | 0/590 [00:00<?, ?it/s]  0%|          | 1/590 [00:00<02:57,  3.31it/s]  0%|          | 2/590 [00:00<02:53,  3.40it/s]  1%|          | 3/590 [00:00<02:51,  3.41it/s]  1%|          | 4/590 [00:01<02:50,  3.43it/s]  1%|          | 5/590 [00:01<02:49,  3.44it/s]  1%|          | 6/590 [00:01<02:49,  3.45it/s]  1%|          | 7/590 [00:02<02:49,  3.44it/s]  1%|▏         | 8/590 [00:02<02:55,  3.32it/s]  2%|▏         | 9/590 [00:02<02:53,  3.35it/s]  2%|▏         | 10/590 [00:02<02:52,  3.36it/s]  2%|▏         | 11/590 [00:03<02:51,  3.37it/s]  2%|▏         | 12/590 [00:03<02:50,  3.39it/s]  2%|▏         | 13/590 [00:03<02:50,  3.39it/s]  2%|▏         | 14/590 [00:04<02:49,  3.40it/s]  3%|▎         | 15/590 [00:04<02:49,  3.40it/s]  3%|▎         | 16/590 [00:04<02:48,  3.40it/s]  3%|▎         | 17/590 [00:05<02:48,  3.40it/s]  3%|▎         | 18/590 [00:05<02:48,  3.40it/s]  3%|▎         | 19/590 [00:05<02:51,  3.33it/s]  3%|▎         | 20/590 [00:05<02:50,  3.35it/s]  4%|▎         | 21/590 [00:06<02:48,  3.37it/s]  4%|▎         | 22/590 [00:06<02:47,  3.38it/s]  4%|▍         | 23/590 [00:06<02:47,  3.39it/s]  4%|▍         | 24/590 [00:07<02:47,  3.39it/s]  4%|▍         | 25/590 [00:07<02:46,  3.39it/s]  4%|▍         | 26/590 [00:07<02:46,  3.39it/s]  5%|▍         | 27/590 [00:07<02:45,  3.40it/s]  5%|▍         | 28/590 [00:08<02:45,  3.40it/s]  5%|▍         | 29/590 [00:08<02:44,  3.40it/s]  5%|▌         | 30/590 [00:08<02:48,  3.32it/s]  5%|▌         | 31/590 [00:09<02:47,  3.34it/s]  5%|▌         | 32/590 [00:09<02:45,  3.36it/s]  6%|▌         | 33/590 [00:09<02:45,  3.38it/s]  6%|▌         | 34/590 [00:10<02:44,  3.38it/s]  6%|▌         | 35/590 [00:10<02:43,  3.39it/s]  6%|▌         | 36/590 [00:10<02:43,  3.39it/s]  6%|▋         | 37/590 [00:10<02:42,  3.40it/s]  6%|▋         | 38/590 [00:11<02:42,  3.40it/s]  7%|▋         | 39/590 [00:11<02:42,  3.40it/s]  7%|▋         | 40/590 [00:11<02:41,  3.40it/s]  7%|▋         | 41/590 [00:12<02:42,  3.37it/s]  7%|▋         | 42/590 [00:12<02:42,  3.38it/s]  7%|▋         | 43/590 [00:12<02:41,  3.38it/s]  7%|▋         | 44/590 [00:12<02:41,  3.39it/s]  8%|▊         | 45/590 [00:13<02:40,  3.39it/s]  8%|▊         | 46/590 [00:13<02:40,  3.39it/s]  8%|▊         | 47/590 [00:13<02:40,  3.39it/s]  8%|▊         | 48/590 [00:14<02:39,  3.40it/s]  8%|▊         | 49/590 [00:14<02:39,  3.40it/s]  8%|▊         | 50/590 [00:14<02:38,  3.40it/s]  9%|▊         | 51/590 [00:15<02:38,  3.40it/s]  9%|▉         | 52/590 [00:15<02:40,  3.36it/s]  9%|▉         | 53/590 [00:15<02:39,  3.37it/s]  9%|▉         | 54/590 [00:15<02:38,  3.38it/s]  9%|▉         | 55/590 [00:16<02:38,  3.38it/s]  9%|▉         | 56/590 [00:16<02:37,  3.39it/s] 10%|▉         | 57/590 [00:16<02:37,  3.39it/s] 10%|▉         | 58/590 [00:17<02:36,  3.39it/s] 10%|█         | 59/590 [00:17<02:36,  3.39it/s] 10%|█         | 60/590 [00:17<02:35,  3.40it/s] 10%|█         | 61/590 [00:18<02:35,  3.40it/s] 11%|█         | 62/590 [00:18<02:35,  3.40it/s] 11%|█         | 63/590 [00:18<02:36,  3.37it/s] 11%|█         | 64/590 [00:18<02:35,  3.38it/s] 11%|█         | 65/590 [00:19<02:34,  3.39it/s] 11%|█         | 66/590 [00:19<02:34,  3.40it/s] 11%|█▏        | 67/590 [00:19<02:33,  3.41it/s] 12%|█▏        | 68/590 [00:20<02:32,  3.42it/s] 12%|█▏        | 69/590 [00:20<02:32,  3.42it/s] 12%|█▏        | 70/590 [00:20<02:31,  3.44it/s] 12%|█▏        | 71/590 [00:20<02:30,  3.46it/s] 12%|█▏        | 72/590 [00:21<02:29,  3.47it/s] 12%|█▏        | 73/590 [00:21<02:28,  3.48it/s] 13%|█▎        | 74/590 [00:21<02:28,  3.47it/s] 13%|█▎        | 75/590 [00:22<02:35,  3.32it/s] 13%|█▎        | 76/590 [00:22<02:32,  3.37it/s] 13%|█▎        | 77/590 [00:22<02:30,  3.41it/s] 13%|█▎        | 78/590 [00:22<02:29,  3.43it/s] 13%|█▎        | 79/590 [00:23<02:28,  3.45it/s] 14%|█▎        | 80/590 [00:23<02:27,  3.46it/s] 14%|█▎        | 81/590 [00:23<02:26,  3.46it/s] 14%|█▍        | 82/590 [00:24<02:26,  3.47it/s] 14%|█▍        | 83/590 [00:24<02:25,  3.48it/s] 14%|█▍        | 84/590 [00:24<02:25,  3.47it/s] 14%|█▍        | 85/590 [00:24<02:25,  3.47it/s] 15%|█▍        | 86/590 [00:25<02:33,  3.28it/s] 15%|█▍        | 87/590 [00:25<02:30,  3.35it/s] 15%|█▍        | 88/590 [00:25<02:27,  3.39it/s] 15%|█▌        | 89/590 [00:26<02:26,  3.41it/s] 15%|█▌        | 90/590 [00:26<02:25,  3.43it/s] 15%|█▌        | 91/590 [00:26<02:25,  3.43it/s] 16%|█▌        | 92/590 [00:27<02:24,  3.44it/s] 16%|█▌        | 93/590 [00:27<02:24,  3.44it/s] 16%|█▌        | 94/590 [00:27<02:23,  3.44it/s] 16%|█▌        | 95/590 [00:27<02:23,  3.45it/s] 16%|█▋        | 96/590 [00:28<02:22,  3.46it/s] 16%|█▋        | 97/590 [00:28<02:27,  3.34it/s] 17%|█▋        | 98/590 [00:28<02:25,  3.37it/s] 17%|█▋        | 99/590 [00:29<02:24,  3.40it/s] 17%|█▋        | 100/590 [00:29<02:23,  3.42it/s] 17%|█▋        | 101/590 [00:29<02:22,  3.43it/s] 17%|█▋        | 102/590 [00:29<02:21,  3.44it/s] 17%|█▋        | 103/590 [00:30<02:21,  3.45it/s] 18%|█▊        | 104/590 [00:30<02:20,  3.45it/s] 18%|█▊        | 105/590 [00:30<02:20,  3.45it/s] 18%|█▊        | 106/590 [00:31<02:19,  3.46it/s] 18%|█▊        | 107/590 [00:31<02:19,  3.45it/s] 18%|█▊        | 108/590 [00:31<02:20,  3.43it/s] 18%|█▊        | 109/590 [00:32<02:20,  3.44it/s] 19%|█▊        | 110/590 [00:32<02:19,  3.44it/s] 19%|█▉        | 111/590 [00:32<02:19,  3.45it/s] 19%|█▉        | 112/590 [00:32<02:18,  3.45it/s] 19%|█▉        | 113/590 [00:33<02:18,  3.45it/s] 19%|█▉        | 114/590 [00:33<02:18,  3.45it/s] 19%|█▉        | 115/590 [00:33<02:17,  3.45it/s] 20%|█▉        | 116/590 [00:34<02:17,  3.45it/s] 20%|█▉        | 117/590 [00:34<02:17,  3.45it/s] 20%|██        | 118/590 [00:34<02:04,  3.80it/s][INFO|trainer.py:2140] 2023-08-29 03:00:11,646 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:00:11,646 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 03:00:11,646 >>   Batch size = 8

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:18, 56.82it/s][A
  1%|          | 12/1071 [00:00<00:21, 49.50it/s][A
  2%|▏         | 18/1071 [00:00<00:22, 47.24it/s][A
  2%|▏         | 23/1071 [00:00<00:22, 46.30it/s][A
  3%|▎         | 28/1071 [00:00<00:22, 45.79it/s][A
  3%|▎         | 33/1071 [00:00<00:22, 45.30it/s][A
  4%|▎         | 38/1071 [00:00<00:22, 45.13it/s][A
  4%|▍         | 43/1071 [00:00<00:22, 44.94it/s][A
  4%|▍         | 48/1071 [00:01<00:22, 45.00it/s][A
  5%|▍         | 53/1071 [00:01<00:22, 45.00it/s][A
  5%|▌         | 58/1071 [00:01<00:22, 45.14it/s][A
  6%|▌         | 63/1071 [00:01<00:22, 45.21it/s][A
  6%|▋         | 68/1071 [00:01<00:22, 44.92it/s][A
  7%|▋         | 73/1071 [00:01<00:22, 44.91it/s][A
  7%|▋         | 78/1071 [00:01<00:22, 44.85it/s][A
  8%|▊         | 83/1071 [00:01<00:22, 44.74it/s][A
  8%|▊         | 88/1071 [00:01<00:22, 44.67it/s][A
  9%|▊         | 93/1071 [00:02<00:21, 44.82it/s][A
  9%|▉         | 98/1071 [00:02<00:21, 44.99it/s][A
 10%|▉         | 103/1071 [00:02<00:21, 45.09it/s][A
 10%|█         | 108/1071 [00:02<00:21, 45.11it/s][A
 11%|█         | 113/1071 [00:02<00:21, 45.02it/s][A
 11%|█         | 118/1071 [00:02<00:21, 44.88it/s][A
 11%|█▏        | 123/1071 [00:02<00:21, 44.78it/s][A
 12%|█▏        | 128/1071 [00:02<00:21, 44.65it/s][A
 12%|█▏        | 133/1071 [00:02<00:21, 44.58it/s][A
 13%|█▎        | 138/1071 [00:03<00:20, 44.70it/s][A
 13%|█▎        | 143/1071 [00:03<00:20, 44.87it/s][A
 14%|█▍        | 148/1071 [00:03<00:20, 45.11it/s][A
 14%|█▍        | 153/1071 [00:03<00:20, 44.98it/s][A
 15%|█▍        | 158/1071 [00:03<00:20, 45.12it/s][A
 15%|█▌        | 163/1071 [00:03<00:20, 44.97it/s][A
 16%|█▌        | 168/1071 [00:03<00:20, 44.78it/s][A
 16%|█▌        | 173/1071 [00:03<00:20, 44.78it/s][A
 17%|█▋        | 178/1071 [00:03<00:19, 44.76it/s][A
 17%|█▋        | 183/1071 [00:04<00:19, 44.88it/s][A
 18%|█▊        | 188/1071 [00:04<00:19, 45.02it/s][A
 18%|█▊        | 193/1071 [00:04<00:19, 45.03it/s][A
 18%|█▊        | 198/1071 [00:04<00:19, 45.03it/s][A
 19%|█▉        | 203/1071 [00:04<00:19, 45.04it/s][A
 19%|█▉        | 208/1071 [00:04<00:19, 44.88it/s][A
 20%|█▉        | 213/1071 [00:04<00:19, 44.72it/s][A
 20%|██        | 218/1071 [00:04<00:19, 44.65it/s][A
 21%|██        | 223/1071 [00:04<00:19, 44.53it/s][A
 21%|██▏       | 228/1071 [00:05<00:18, 44.70it/s][A
 22%|██▏       | 233/1071 [00:05<00:18, 44.88it/s][A
 22%|██▏       | 238/1071 [00:05<00:18, 45.04it/s][A
 23%|██▎       | 243/1071 [00:05<00:18, 45.12it/s][A
 23%|██▎       | 248/1071 [00:05<00:18, 45.09it/s][A
 24%|██▎       | 253/1071 [00:05<00:18, 45.03it/s][A
 24%|██▍       | 258/1071 [00:05<00:18, 44.85it/s][A
 25%|██▍       | 263/1071 [00:05<00:18, 44.73it/s][A
 25%|██▌       | 268/1071 [00:05<00:17, 44.71it/s][A
 25%|██▌       | 273/1071 [00:06<00:17, 44.71it/s][A
 26%|██▌       | 278/1071 [00:06<00:17, 44.74it/s][A
 26%|██▋       | 283/1071 [00:06<00:17, 45.01it/s][A
 27%|██▋       | 288/1071 [00:06<00:17, 45.05it/s][A
 27%|██▋       | 293/1071 [00:06<00:17, 44.87it/s][A
 28%|██▊       | 298/1071 [00:06<00:17, 45.04it/s][A
 28%|██▊       | 303/1071 [00:06<00:17, 44.77it/s][A
 29%|██▉       | 308/1071 [00:06<00:17, 44.74it/s][A
 29%|██▉       | 313/1071 [00:06<00:16, 44.70it/s][A
 30%|██▉       | 318/1071 [00:07<00:16, 44.69it/s][A
 30%|███       | 323/1071 [00:07<00:16, 45.03it/s][A
 31%|███       | 328/1071 [00:07<00:16, 45.14it/s][A
 31%|███       | 333/1071 [00:07<00:16, 45.27it/s][A
 32%|███▏      | 338/1071 [00:07<00:16, 45.16it/s][A
 32%|███▏      | 343/1071 [00:07<00:16, 45.16it/s][A
 32%|███▏      | 348/1071 [00:07<00:16, 44.84it/s][A
 33%|███▎      | 353/1071 [00:07<00:16, 44.85it/s][A
 33%|███▎      | 358/1071 [00:07<00:15, 44.81it/s][A
 34%|███▍      | 363/1071 [00:08<00:15, 44.73it/s][A
 34%|███▍      | 368/1071 [00:08<00:15, 44.91it/s][A
 35%|███▍      | 373/1071 [00:08<00:15, 45.02it/s][A
 35%|███▌      | 378/1071 [00:08<00:15, 45.16it/s][A
 36%|███▌      | 383/1071 [00:08<00:15, 45.08it/s][A
 36%|███▌      | 388/1071 [00:08<00:15, 44.84it/s][A
 37%|███▋      | 393/1071 [00:08<00:15, 44.95it/s][A
 37%|███▋      | 398/1071 [00:08<00:14, 44.87it/s][A
 38%|███▊      | 403/1071 [00:08<00:14, 44.79it/s][A
 38%|███▊      | 408/1071 [00:09<00:14, 44.75it/s][A
 39%|███▊      | 413/1071 [00:09<00:14, 44.93it/s][A
 39%|███▉      | 418/1071 [00:09<00:14, 44.70it/s][A
 39%|███▉      | 423/1071 [00:09<00:14, 45.23it/s][A
 40%|███▉      | 428/1071 [00:09<00:14, 45.20it/s][A
 40%|████      | 433/1071 [00:09<00:14, 44.65it/s][A
 41%|████      | 438/1071 [00:09<00:14, 44.95it/s][A
 41%|████▏     | 443/1071 [00:09<00:13, 45.15it/s][A
 42%|████▏     | 448/1071 [00:09<00:13, 45.08it/s][A
 42%|████▏     | 453/1071 [00:10<00:13, 44.97it/s][A
 43%|████▎     | 458/1071 [00:10<00:13, 45.04it/s][A
 43%|████▎     | 463/1071 [00:10<00:13, 45.02it/s][A
 44%|████▎     | 468/1071 [00:10<00:13, 45.06it/s][A
 44%|████▍     | 473/1071 [00:10<00:13, 45.12it/s][A
 45%|████▍     | 478/1071 [00:10<00:13, 44.91it/s][A
 45%|████▌     | 483/1071 [00:10<00:13, 44.92it/s][A
 46%|████▌     | 488/1071 [00:10<00:12, 44.91it/s][A
 46%|████▌     | 493/1071 [00:10<00:12, 44.89it/s][A
 46%|████▋     | 498/1071 [00:11<00:12, 44.80it/s][A
 47%|████▋     | 503/1071 [00:11<00:12, 44.78it/s][A
 47%|████▋     | 508/1071 [00:11<00:12, 44.80it/s][A
 48%|████▊     | 513/1071 [00:11<00:12, 44.88it/s][A
 48%|████▊     | 518/1071 [00:11<00:12, 44.90it/s][A
 49%|████▉     | 523/1071 [00:11<00:12, 44.63it/s][A
 49%|████▉     | 528/1071 [00:11<00:12, 44.82it/s][A
 50%|████▉     | 533/1071 [00:11<00:12, 44.78it/s][A
 50%|█████     | 538/1071 [00:11<00:11, 44.86it/s][A
 51%|█████     | 543/1071 [00:12<00:11, 44.81it/s][A
 51%|█████     | 548/1071 [00:12<00:11, 44.51it/s][A
 52%|█████▏    | 553/1071 [00:12<00:11, 44.84it/s][A
 52%|█████▏    | 558/1071 [00:12<00:11, 44.82it/s][A
 53%|█████▎    | 563/1071 [00:12<00:11, 44.77it/s][A
 53%|█████▎    | 568/1071 [00:12<00:11, 44.71it/s][A
 54%|█████▎    | 573/1071 [00:12<00:11, 44.69it/s][A
 54%|█████▍    | 578/1071 [00:12<00:11, 44.69it/s][A
 54%|█████▍    | 583/1071 [00:12<00:10, 44.62it/s][A
 55%|█████▍    | 588/1071 [00:13<00:10, 44.77it/s][A
 55%|█████▌    | 593/1071 [00:13<00:10, 44.78it/s][A
 56%|█████▌    | 598/1071 [00:13<00:10, 44.76it/s][A
 56%|█████▋    | 603/1071 [00:13<00:10, 44.79it/s][A
 57%|█████▋    | 608/1071 [00:13<00:10, 44.52it/s][A
 57%|█████▋    | 613/1071 [00:13<00:10, 44.83it/s][A
 58%|█████▊    | 618/1071 [00:13<00:10, 44.80it/s][A
 58%|█████▊    | 623/1071 [00:13<00:10, 44.65it/s][A
 59%|█████▊    | 628/1071 [00:13<00:09, 44.79it/s][A
 59%|█████▉    | 633/1071 [00:14<00:09, 44.77it/s][A
 60%|█████▉    | 638/1071 [00:14<00:09, 44.80it/s][A
 60%|██████    | 643/1071 [00:14<00:09, 44.72it/s][A
 61%|██████    | 648/1071 [00:14<00:09, 44.81it/s][A
 61%|██████    | 653/1071 [00:14<00:09, 44.81it/s][A
 61%|██████▏   | 658/1071 [00:14<00:09, 44.76it/s][A
 62%|██████▏   | 663/1071 [00:14<00:09, 44.66it/s][A
 62%|██████▏   | 668/1071 [00:14<00:09, 44.60it/s][A
 63%|██████▎   | 673/1071 [00:14<00:08, 44.79it/s][A
 63%|██████▎   | 678/1071 [00:15<00:08, 44.77it/s][A
 64%|██████▍   | 683/1071 [00:15<00:08, 44.74it/s][A
 64%|██████▍   | 688/1071 [00:15<00:08, 44.81it/s][A
 65%|██████▍   | 693/1071 [00:15<00:08, 44.87it/s][A
 65%|██████▌   | 698/1071 [00:15<00:08, 44.85it/s][A
 66%|██████▌   | 703/1071 [00:15<00:08, 44.80it/s][A
 66%|██████▌   | 708/1071 [00:15<00:08, 44.75it/s][A
 67%|██████▋   | 713/1071 [00:15<00:07, 44.79it/s][A
 67%|██████▋   | 718/1071 [00:15<00:07, 44.66it/s][A
 68%|██████▊   | 723/1071 [00:16<00:07, 44.87it/s][A
 68%|██████▊   | 728/1071 [00:16<00:07, 44.83it/s][A
 68%|██████▊   | 733/1071 [00:16<00:07, 44.21it/s][A
 69%|██████▉   | 738/1071 [00:16<00:07, 44.68it/s][A
 69%|██████▉   | 743/1071 [00:16<00:07, 44.72it/s][A
 70%|██████▉   | 748/1071 [00:16<00:07, 44.73it/s][A
 70%|███████   | 753/1071 [00:16<00:07, 44.86it/s][A
 71%|███████   | 758/1071 [00:16<00:06, 44.73it/s][A
 71%|███████   | 763/1071 [00:16<00:06, 44.56it/s][A
 72%|███████▏  | 768/1071 [00:17<00:06, 44.65it/s][A
 72%|███████▏  | 773/1071 [00:17<00:06, 44.85it/s][A
 73%|███████▎  | 778/1071 [00:17<00:06, 44.66it/s][A
 73%|███████▎  | 783/1071 [00:17<00:06, 44.68it/s][A
 74%|███████▎  | 788/1071 [00:17<00:06, 44.82it/s][A
 74%|███████▍  | 793/1071 [00:17<00:06, 44.90it/s][A
 75%|███████▍  | 798/1071 [00:17<00:06, 44.74it/s][A
 75%|███████▍  | 803/1071 [00:17<00:05, 44.68it/s][A
 75%|███████▌  | 808/1071 [00:18<00:05, 44.81it/s][A
 76%|███████▌  | 813/1071 [00:18<00:05, 44.54it/s][A
 76%|███████▋  | 818/1071 [00:18<00:05, 44.59it/s][A
 77%|███████▋  | 823/1071 [00:18<00:05, 44.59it/s][A
 77%|███████▋  | 828/1071 [00:18<00:05, 44.62it/s][A
 78%|███████▊  | 833/1071 [00:18<00:05, 44.78it/s][A
 78%|███████▊  | 838/1071 [00:18<00:05, 44.75it/s][A
 79%|███████▊  | 843/1071 [00:18<00:05, 44.72it/s][A
 79%|███████▉  | 848/1071 [00:18<00:04, 44.76it/s][A
 80%|███████▉  | 853/1071 [00:18<00:04, 44.84it/s][A
 80%|████████  | 858/1071 [00:19<00:04, 44.85it/s][A
 81%|████████  | 863/1071 [00:19<00:04, 44.77it/s][A
 81%|████████  | 868/1071 [00:19<00:04, 44.81it/s][A
 82%|████████▏ | 873/1071 [00:19<00:04, 44.65it/s][A
 82%|████████▏ | 878/1071 [00:19<00:04, 44.53it/s][A
 82%|████████▏ | 883/1071 [00:19<00:04, 44.80it/s][A
 83%|████████▎ | 888/1071 [00:19<00:04, 44.58it/s][A
 83%|████████▎ | 893/1071 [00:19<00:03, 44.78it/s][A
 84%|████████▍ | 898/1071 [00:20<00:03, 44.80it/s][A
 84%|████████▍ | 903/1071 [00:20<00:03, 44.78it/s][A
 85%|████████▍ | 908/1071 [00:20<00:03, 44.84it/s][A
 85%|████████▌ | 913/1071 [00:20<00:03, 44.69it/s][A
 86%|████████▌ | 918/1071 [00:20<00:03, 44.57it/s][A
 86%|████████▌ | 923/1071 [00:20<00:03, 44.54it/s][A
 87%|████████▋ | 928/1071 [00:20<00:03, 44.49it/s][A
 87%|████████▋ | 933/1071 [00:20<00:03, 44.49it/s][A
 88%|████████▊ | 938/1071 [00:20<00:02, 44.65it/s][A
 88%|████████▊ | 943/1071 [00:21<00:02, 44.79it/s][A
 89%|████████▊ | 948/1071 [00:21<00:02, 43.67it/s][A
 89%|████████▉ | 953/1071 [00:21<00:02, 44.04it/s][A
 89%|████████▉ | 958/1071 [00:21<00:02, 44.14it/s][A
 90%|████████▉ | 963/1071 [00:21<00:02, 44.18it/s][A
 90%|█████████ | 968/1071 [00:21<00:02, 44.54it/s][A
 91%|█████████ | 973/1071 [00:21<00:02, 44.56it/s][A
 91%|█████████▏| 978/1071 [00:21<00:02, 44.57it/s][A
 92%|█████████▏| 983/1071 [00:21<00:01, 44.47it/s][A
 92%|█████████▏| 988/1071 [00:22<00:01, 44.85it/s][A
 93%|█████████▎| 993/1071 [00:22<00:01, 44.74it/s][A
 93%|█████████▎| 998/1071 [00:22<00:01, 44.85it/s][A
 94%|█████████▎| 1003/1071 [00:22<00:01, 44.85it/s][A
 94%|█████████▍| 1008/1071 [00:22<00:01, 44.78it/s][A
 95%|█████████▍| 1013/1071 [00:22<00:01, 44.76it/s][A
 95%|█████████▌| 1018/1071 [00:22<00:01, 44.79it/s][A
 96%|█████████▌| 1023/1071 [00:22<00:01, 44.75it/s][A
 96%|█████████▌| 1028/1071 [00:22<00:00, 44.79it/s][A
 96%|█████████▋| 1033/1071 [00:23<00:00, 44.78it/s][A
 97%|█████████▋| 1038/1071 [00:23<00:00, 44.87it/s][A
 97%|█████████▋| 1043/1071 [00:23<00:00, 44.80it/s][A
 98%|█████████▊| 1048/1071 [00:23<00:00, 44.75it/s][A
 98%|█████████▊| 1053/1071 [00:23<00:00, 44.68it/s][A
 99%|█████████▉| 1058/1071 [00:23<00:00, 44.49it/s][A
 99%|█████████▉| 1063/1071 [00:23<00:00, 44.47it/s][A
100%|█████████▉| 1068/1071 [00:23<00:00, 44.43it/s][A                                                 
                                                   [A 20%|██        | 118/590 [00:58<02:04,  3.80it/s]
100%|██████████| 1071/1071 [00:23<00:00, 44.43it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-29 03:00:35,872 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-118
[INFO|configuration_utils.py:351] 2023-08-29 03:00:36,077 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-118/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:00:39,113 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-118/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:00:39,284 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-118/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:00:39,346 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-118/special_tokens_map.json
 20%|██        | 119/590 [01:09<1:22:37, 10.53s/it] 20%|██        | 120/590 [01:09<58:29,  7.47s/it]   21%|██        | 121/590 [01:09<41:32,  5.31s/it] 21%|██        | 122/590 [01:09<29:42,  3.81s/it] 21%|██        | 123/590 [01:10<21:26,  2.75s/it] 21%|██        | 124/590 [01:10<15:39,  2.02s/it] 21%|██        | 125/590 [01:10<11:37,  1.50s/it] 21%|██▏       | 126/590 [01:11<08:48,  1.14s/it] 22%|██▏       | 127/590 [01:11<06:49,  1.13it/s] 22%|██▏       | 128/590 [01:11<05:26,  1.41it/s] 22%|██▏       | 129/590 [01:11<04:29,  1.71it/s] 22%|██▏       | 130/590 [01:12<03:48,  2.01it/s] 22%|██▏       | 131/590 [01:12<03:25,  2.24it/s] 22%|██▏       | 132/590 [01:12<03:03,  2.50it/s] 23%|██▎       | 133/590 [01:13<02:47,  2.72it/s] 23%|██▎       | 134/590 [01:13<02:36,  2.91it/s] 23%|██▎       | 135/590 [01:13<02:28,  3.05it/s] 23%|██▎       | 136/590 [01:14<02:23,  3.16it/s] 23%|██▎       | 137/590 [01:14<02:19,  3.25it/s] 23%|██▎       | 138/590 [01:14<02:16,  3.31it/s] 24%|██▎       | 139/590 [01:14<02:14,  3.35it/s] 24%|██▎       | 140/590 [01:15<02:13,  3.38it/s] 24%|██▍       | 141/590 [01:15<02:12,  3.40it/s] 24%|██▍       | 142/590 [01:15<02:14,  3.32it/s] 24%|██▍       | 143/590 [01:16<02:12,  3.36it/s] 24%|██▍       | 144/590 [01:16<02:11,  3.39it/s] 25%|██▍       | 145/590 [01:16<02:10,  3.41it/s] 25%|██▍       | 146/590 [01:16<02:09,  3.42it/s] 25%|██▍       | 147/590 [01:17<02:09,  3.43it/s] 25%|██▌       | 148/590 [01:17<02:08,  3.43it/s] 25%|██▌       | 149/590 [01:17<02:08,  3.44it/s] 25%|██▌       | 150/590 [01:18<02:07,  3.44it/s] 26%|██▌       | 151/590 [01:18<02:07,  3.45it/s] 26%|██▌       | 152/590 [01:18<02:07,  3.45it/s] 26%|██▌       | 153/590 [01:19<02:09,  3.38it/s] 26%|██▌       | 154/590 [01:19<02:08,  3.40it/s] 26%|██▋       | 155/590 [01:19<02:07,  3.41it/s] 26%|██▋       | 156/590 [01:19<02:06,  3.42it/s] 27%|██▋       | 157/590 [01:20<02:06,  3.44it/s] 27%|██▋       | 158/590 [01:20<02:10,  3.31it/s] 27%|██▋       | 159/590 [01:20<02:08,  3.35it/s] 27%|██▋       | 160/590 [01:21<02:06,  3.39it/s] 27%|██▋       | 161/590 [01:21<02:05,  3.41it/s] 27%|██▋       | 162/590 [01:21<02:04,  3.43it/s] 28%|██▊       | 163/590 [01:21<02:04,  3.44it/s] 28%|██▊       | 164/590 [01:22<02:03,  3.45it/s] 28%|██▊       | 165/590 [01:22<02:03,  3.45it/s] 28%|██▊       | 166/590 [01:22<02:02,  3.45it/s] 28%|██▊       | 167/590 [01:23<02:02,  3.45it/s] 28%|██▊       | 168/590 [01:23<02:02,  3.46it/s] 29%|██▊       | 169/590 [01:23<02:10,  3.22it/s] 29%|██▉       | 170/590 [01:24<02:07,  3.29it/s] 29%|██▉       | 171/590 [01:24<02:05,  3.34it/s] 29%|██▉       | 172/590 [01:24<02:03,  3.38it/s] 29%|██▉       | 173/590 [01:24<02:02,  3.40it/s] 29%|██▉       | 174/590 [01:25<02:01,  3.42it/s] 30%|██▉       | 175/590 [01:25<02:01,  3.43it/s] 30%|██▉       | 176/590 [01:25<02:00,  3.44it/s] 30%|███       | 177/590 [01:26<02:00,  3.44it/s] 30%|███       | 178/590 [01:26<01:59,  3.45it/s] 30%|███       | 179/590 [01:26<01:59,  3.45it/s] 31%|███       | 180/590 [01:27<02:05,  3.26it/s] 31%|███       | 181/590 [01:27<02:03,  3.32it/s] 31%|███       | 182/590 [01:27<02:01,  3.36it/s] 31%|███       | 183/590 [01:27<02:00,  3.39it/s] 31%|███       | 184/590 [01:28<01:58,  3.41it/s] 31%|███▏      | 185/590 [01:28<01:57,  3.43it/s] 32%|███▏      | 186/590 [01:28<01:57,  3.44it/s] 32%|███▏      | 187/590 [01:29<01:56,  3.45it/s] 32%|███▏      | 188/590 [01:29<01:56,  3.45it/s] 32%|███▏      | 189/590 [01:29<01:56,  3.45it/s] 32%|███▏      | 190/590 [01:29<01:55,  3.46it/s] 32%|███▏      | 191/590 [01:30<02:03,  3.22it/s] 33%|███▎      | 192/590 [01:30<02:00,  3.29it/s] 33%|███▎      | 193/590 [01:30<01:58,  3.34it/s] 33%|███▎      | 194/590 [01:31<01:56,  3.39it/s] 33%|███▎      | 195/590 [01:31<01:55,  3.41it/s] 33%|███▎      | 196/590 [01:31<01:54,  3.43it/s] 33%|███▎      | 197/590 [01:31<01:54,  3.44it/s] 34%|███▎      | 198/590 [01:32<01:53,  3.45it/s] 34%|███▎      | 199/590 [01:32<01:53,  3.45it/s] 34%|███▍      | 200/590 [01:32<01:52,  3.46it/s] 34%|███▍      | 201/590 [01:33<01:52,  3.47it/s] 34%|███▍      | 202/590 [01:33<02:00,  3.21it/s] 34%|███▍      | 203/590 [01:33<01:57,  3.29it/s] 35%|███▍      | 204/590 [01:34<01:55,  3.34it/s] 35%|███▍      | 205/590 [01:34<01:53,  3.39it/s] 35%|███▍      | 206/590 [01:34<01:52,  3.41it/s] 35%|███▌      | 207/590 [01:34<01:51,  3.42it/s] 35%|███▌      | 208/590 [01:35<01:51,  3.44it/s] 35%|███▌      | 209/590 [01:35<01:50,  3.45it/s] 36%|███▌      | 210/590 [01:35<01:50,  3.45it/s] 36%|███▌      | 211/590 [01:36<01:49,  3.45it/s] 36%|███▌      | 212/590 [01:36<01:49,  3.45it/s] 36%|███▌      | 213/590 [01:36<01:52,  3.36it/s] 36%|███▋      | 214/590 [01:36<01:50,  3.39it/s] 36%|███▋      | 215/590 [01:37<01:50,  3.40it/s] 37%|███▋      | 216/590 [01:37<01:49,  3.42it/s] 37%|███▋      | 217/590 [01:37<01:48,  3.42it/s] 37%|███▋      | 218/590 [01:38<01:48,  3.43it/s] 37%|███▋      | 219/590 [01:38<01:48,  3.43it/s] 37%|███▋      | 220/590 [01:38<01:47,  3.44it/s] 37%|███▋      | 221/590 [01:39<01:47,  3.44it/s] 38%|███▊      | 222/590 [01:39<01:46,  3.44it/s] 38%|███▊      | 223/590 [01:39<01:46,  3.44it/s] 38%|███▊      | 224/590 [01:39<01:49,  3.34it/s] 38%|███▊      | 225/590 [01:40<01:48,  3.37it/s] 38%|███▊      | 226/590 [01:40<01:47,  3.39it/s] 38%|███▊      | 227/590 [01:40<01:46,  3.41it/s] 39%|███▊      | 228/590 [01:41<01:45,  3.42it/s] 39%|███▉      | 229/590 [01:41<01:49,  3.29it/s] 39%|███▉      | 230/590 [01:41<01:48,  3.33it/s] 39%|███▉      | 231/590 [01:41<01:46,  3.37it/s] 39%|███▉      | 232/590 [01:42<01:45,  3.39it/s] 39%|███▉      | 233/590 [01:42<01:44,  3.41it/s] 40%|███▉      | 234/590 [01:42<01:44,  3.42it/s] 40%|███▉      | 235/590 [01:43<01:46,  3.35it/s] 40%|████      | 236/590 [01:43<01:35,  3.71it/s][INFO|trainer.py:2140] 2023-08-29 03:01:20,471 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:01:20,471 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 03:01:20,471 >>   Batch size = 8
{'eval_loss': 0.9176693558692932, 'eval_runtime': 23.9099, 'eval_samples_per_second': 358.345, 'eval_steps_per_second': 44.793, 'epoch': 1.0}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:18, 56.88it/s][A
  1%|          | 12/1071 [00:00<00:21, 49.12it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 47.75it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 46.28it/s][A
  3%|▎         | 27/1071 [00:00<00:22, 45.55it/s][A
  3%|▎         | 32/1071 [00:01<00:47, 21.65it/s][A
  3%|▎         | 37/1071 [00:01<00:39, 26.11it/s][A
  4%|▍         | 42/1071 [00:01<00:34, 30.11it/s][A
  4%|▍         | 47/1071 [00:01<00:30, 33.59it/s][A
  5%|▍         | 52/1071 [00:01<00:27, 36.40it/s][A
  5%|▌         | 57/1071 [00:01<00:26, 38.84it/s][A
  6%|▌         | 62/1071 [00:01<00:24, 40.64it/s][A
  6%|▋         | 67/1071 [00:01<00:24, 41.75it/s][A
  7%|▋         | 72/1071 [00:01<00:23, 42.34it/s][A
  7%|▋         | 77/1071 [00:02<00:23, 42.88it/s][A
  8%|▊         | 82/1071 [00:02<00:22, 43.34it/s][A
  8%|▊         | 87/1071 [00:02<00:22, 43.87it/s][A
  9%|▊         | 92/1071 [00:02<00:22, 44.41it/s][A
  9%|▉         | 97/1071 [00:02<00:21, 44.64it/s][A
 10%|▉         | 102/1071 [00:02<00:22, 42.18it/s][A
 10%|▉         | 107/1071 [00:02<00:22, 43.13it/s][A
 10%|█         | 112/1071 [00:02<00:21, 43.70it/s][A
 11%|█         | 117/1071 [00:02<00:21, 43.96it/s][A
 11%|█▏        | 122/1071 [00:03<00:21, 44.09it/s][A
 12%|█▏        | 127/1071 [00:03<00:21, 44.35it/s][A
 12%|█▏        | 132/1071 [00:03<00:21, 44.52it/s][A
 13%|█▎        | 137/1071 [00:03<00:21, 44.44it/s][A
 13%|█▎        | 142/1071 [00:03<00:20, 44.43it/s][A
 14%|█▎        | 147/1071 [00:03<00:20, 44.72it/s][A
 14%|█▍        | 152/1071 [00:03<00:20, 44.94it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 45.11it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 44.90it/s][A
 16%|█▌        | 167/1071 [00:04<00:20, 44.84it/s][A
 16%|█▌        | 172/1071 [00:04<00:20, 44.81it/s][A
 17%|█▋        | 177/1071 [00:04<00:20, 44.68it/s][A
 17%|█▋        | 182/1071 [00:04<00:19, 44.77it/s][A
 17%|█▋        | 187/1071 [00:04<00:19, 44.67it/s][A
 18%|█▊        | 192/1071 [00:04<00:19, 44.67it/s][A
 18%|█▊        | 197/1071 [00:04<00:19, 44.86it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 44.86it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 45.04it/s][A
 20%|█▉        | 212/1071 [00:05<00:19, 44.98it/s][A
 20%|██        | 217/1071 [00:05<00:19, 44.91it/s][A
 21%|██        | 222/1071 [00:05<00:18, 44.91it/s][A
 21%|██        | 227/1071 [00:05<00:18, 44.74it/s][A
 22%|██▏       | 232/1071 [00:05<00:18, 44.71it/s][A
 22%|██▏       | 237/1071 [00:05<00:19, 42.00it/s][A
 23%|██▎       | 242/1071 [00:05<00:19, 42.90it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 43.62it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 44.17it/s][A
 24%|██▍       | 257/1071 [00:06<00:18, 44.37it/s][A
 24%|██▍       | 262/1071 [00:06<00:18, 44.28it/s][A
 25%|██▍       | 267/1071 [00:06<00:18, 44.47it/s][A
 25%|██▌       | 272/1071 [00:06<00:18, 44.35it/s][A
 26%|██▌       | 277/1071 [00:06<00:17, 44.34it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 44.37it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 44.69it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 44.90it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 45.05it/s][A
 28%|██▊       | 302/1071 [00:07<00:17, 45.08it/s][A
 29%|██▊       | 307/1071 [00:07<00:16, 45.03it/s][A
 29%|██▉       | 312/1071 [00:07<00:16, 44.92it/s][A
 30%|██▉       | 317/1071 [00:07<00:16, 44.64it/s][A
 30%|███       | 322/1071 [00:07<00:16, 44.59it/s][A
 31%|███       | 327/1071 [00:07<00:17, 43.69it/s][A
 31%|███       | 332/1071 [00:07<00:16, 44.24it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 44.49it/s][A
 32%|███▏      | 342/1071 [00:08<00:16, 44.64it/s][A
 32%|███▏      | 347/1071 [00:08<00:16, 44.89it/s][A
 33%|███▎      | 352/1071 [00:08<00:16, 44.93it/s][A
 33%|███▎      | 357/1071 [00:08<00:15, 44.81it/s][A
 34%|███▍      | 362/1071 [00:08<00:15, 44.54it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.68it/s][A
 35%|███▍      | 372/1071 [00:08<00:15, 44.66it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 44.74it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 44.96it/s][A
 36%|███▌      | 387/1071 [00:09<00:15, 45.04it/s][A
 37%|███▋      | 392/1071 [00:09<00:15, 45.18it/s][A
 37%|███▋      | 397/1071 [00:09<00:15, 44.84it/s][A
 38%|███▊      | 402/1071 [00:09<00:14, 44.86it/s][A
 38%|███▊      | 407/1071 [00:09<00:14, 44.68it/s][A
 38%|███▊      | 412/1071 [00:09<00:14, 44.70it/s][A
 39%|███▉      | 417/1071 [00:09<00:14, 44.80it/s][A
 39%|███▉      | 422/1071 [00:09<00:14, 44.74it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 44.96it/s][A
 40%|████      | 432/1071 [00:10<00:14, 45.13it/s][A
 41%|████      | 437/1071 [00:10<00:14, 45.07it/s][A
 41%|████▏     | 442/1071 [00:10<00:14, 44.89it/s][A
 42%|████▏     | 447/1071 [00:10<00:13, 44.87it/s][A
 42%|████▏     | 452/1071 [00:10<00:13, 44.83it/s][A
 43%|████▎     | 457/1071 [00:10<00:13, 44.58it/s][A
 43%|████▎     | 462/1071 [00:10<00:14, 41.41it/s][A
 44%|████▎     | 467/1071 [00:10<00:14, 42.70it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 43.44it/s][A
 45%|████▍     | 477/1071 [00:11<00:13, 44.00it/s][A
 45%|████▌     | 482/1071 [00:11<00:13, 44.44it/s][A
 45%|████▌     | 487/1071 [00:11<00:13, 44.51it/s][A
 46%|████▌     | 492/1071 [00:11<00:12, 44.54it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.50it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.31it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 44.35it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 44.49it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 44.76it/s][A
 49%|████▊     | 522/1071 [00:12<00:12, 44.80it/s][A
 49%|████▉     | 527/1071 [00:12<00:12, 44.96it/s][A
 50%|████▉     | 532/1071 [00:12<00:11, 45.04it/s][A
 50%|█████     | 537/1071 [00:12<00:11, 45.08it/s][A
 51%|█████     | 542/1071 [00:12<00:11, 44.94it/s][A
 51%|█████     | 547/1071 [00:12<00:11, 44.76it/s][A
 52%|█████▏    | 552/1071 [00:12<00:11, 44.61it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 44.61it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 44.90it/s][A
 53%|█████▎    | 567/1071 [00:13<00:11, 44.82it/s][A
 53%|█████▎    | 572/1071 [00:13<00:11, 44.94it/s][A
 54%|█████▍    | 577/1071 [00:13<00:10, 45.08it/s][A
 54%|█████▍    | 582/1071 [00:13<00:10, 45.12it/s][A
 55%|█████▍    | 587/1071 [00:13<00:10, 44.85it/s][A
 55%|█████▌    | 592/1071 [00:13<00:10, 44.58it/s][A
 56%|█████▌    | 597/1071 [00:13<00:10, 44.11it/s][A
 56%|█████▌    | 602/1071 [00:13<00:10, 44.34it/s][A
 57%|█████▋    | 607/1071 [00:13<00:10, 44.55it/s][A
 57%|█████▋    | 612/1071 [00:14<00:10, 44.63it/s][A
 58%|█████▊    | 617/1071 [00:14<00:10, 44.71it/s][A
 58%|█████▊    | 622/1071 [00:14<00:10, 44.73it/s][A
 59%|█████▊    | 627/1071 [00:14<00:09, 44.87it/s][A
 59%|█████▉    | 632/1071 [00:14<00:09, 44.72it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.42it/s][A
 60%|█████▉    | 642/1071 [00:14<00:09, 44.48it/s][A
 60%|██████    | 647/1071 [00:14<00:09, 44.54it/s][A
 61%|██████    | 652/1071 [00:14<00:09, 44.51it/s][A
 61%|██████▏   | 657/1071 [00:15<00:09, 44.53it/s][A
 62%|██████▏   | 662/1071 [00:15<00:09, 44.71it/s][A
 62%|██████▏   | 667/1071 [00:15<00:09, 44.53it/s][A
 63%|██████▎   | 672/1071 [00:15<00:08, 44.70it/s][A
 63%|██████▎   | 677/1071 [00:15<00:08, 44.49it/s][A
 64%|██████▎   | 682/1071 [00:15<00:08, 44.22it/s][A
 64%|██████▍   | 687/1071 [00:15<00:08, 44.65it/s][A
 65%|██████▍   | 692/1071 [00:15<00:08, 44.65it/s][A
 65%|██████▌   | 697/1071 [00:15<00:08, 44.96it/s][A
 66%|██████▌   | 702/1071 [00:16<00:08, 44.90it/s][A
 66%|██████▌   | 707/1071 [00:16<00:08, 44.80it/s][A
 66%|██████▋   | 712/1071 [00:16<00:08, 44.83it/s][A
 67%|██████▋   | 717/1071 [00:16<00:07, 44.77it/s][A
 67%|██████▋   | 722/1071 [00:16<00:07, 44.64it/s][A
 68%|██████▊   | 727/1071 [00:16<00:07, 44.58it/s][A
 68%|██████▊   | 732/1071 [00:16<00:07, 43.09it/s][A
 69%|██████▉   | 737/1071 [00:16<00:07, 43.90it/s][A
 69%|██████▉   | 742/1071 [00:16<00:07, 44.13it/s][A
 70%|██████▉   | 747/1071 [00:17<00:07, 44.33it/s][A
 70%|███████   | 752/1071 [00:17<00:07, 44.74it/s][A
 71%|███████   | 757/1071 [00:17<00:07, 44.71it/s][A
 71%|███████   | 762/1071 [00:17<00:06, 44.68it/s][A
 72%|███████▏  | 767/1071 [00:17<00:06, 44.64it/s][A
 72%|███████▏  | 772/1071 [00:17<00:06, 44.41it/s][A
 73%|███████▎  | 777/1071 [00:17<00:06, 44.53it/s][A
 73%|███████▎  | 782/1071 [00:17<00:06, 44.70it/s][A
 73%|███████▎  | 787/1071 [00:17<00:06, 44.83it/s][A
 74%|███████▍  | 792/1071 [00:18<00:06, 45.01it/s][A
 74%|███████▍  | 797/1071 [00:18<00:06, 44.65it/s][A
 75%|███████▍  | 802/1071 [00:18<00:06, 44.61it/s][A
 75%|███████▌  | 807/1071 [00:18<00:05, 44.63it/s][A
 76%|███████▌  | 812/1071 [00:18<00:05, 44.48it/s][A
 76%|███████▋  | 817/1071 [00:18<00:05, 44.49it/s][A
 77%|███████▋  | 822/1071 [00:18<00:05, 44.40it/s][A
 77%|███████▋  | 827/1071 [00:18<00:05, 44.76it/s][A
 78%|███████▊  | 832/1071 [00:18<00:05, 44.81it/s][A
 78%|███████▊  | 837/1071 [00:19<00:05, 44.89it/s][A
 79%|███████▊  | 842/1071 [00:19<00:05, 44.68it/s][A
 79%|███████▉  | 847/1071 [00:19<00:04, 44.83it/s][A
 80%|███████▉  | 852/1071 [00:19<00:04, 44.99it/s][A
 80%|████████  | 857/1071 [00:19<00:04, 44.80it/s][A
 80%|████████  | 862/1071 [00:19<00:04, 44.80it/s][A
 81%|████████  | 867/1071 [00:19<00:04, 42.92it/s][A
 81%|████████▏ | 872/1071 [00:19<00:04, 43.85it/s][A
 82%|████████▏ | 877/1071 [00:20<00:04, 44.22it/s][A
 82%|████████▏ | 882/1071 [00:20<00:04, 44.56it/s][A
 83%|████████▎ | 887/1071 [00:20<00:04, 44.68it/s][A
 83%|████████▎ | 892/1071 [00:20<00:04, 44.68it/s][A
 84%|████████▍ | 897/1071 [00:20<00:03, 44.50it/s][A
 84%|████████▍ | 902/1071 [00:20<00:03, 44.47it/s][A
 85%|████████▍ | 907/1071 [00:20<00:03, 44.34it/s][A
 85%|████████▌ | 912/1071 [00:20<00:03, 44.30it/s][A
 86%|████████▌ | 917/1071 [00:20<00:03, 44.48it/s][A
 86%|████████▌ | 922/1071 [00:21<00:03, 44.81it/s][A
 87%|████████▋ | 927/1071 [00:21<00:03, 44.91it/s][A
 87%|████████▋ | 932/1071 [00:21<00:03, 44.94it/s][A
 87%|████████▋ | 937/1071 [00:21<00:02, 44.81it/s][A
 88%|████████▊ | 942/1071 [00:21<00:02, 44.83it/s][A
 88%|████████▊ | 947/1071 [00:21<00:02, 44.72it/s][A
 89%|████████▉ | 952/1071 [00:21<00:02, 44.52it/s][A
 89%|████████▉ | 957/1071 [00:21<00:02, 44.58it/s][A
 90%|████████▉ | 962/1071 [00:21<00:02, 44.73it/s][A
 90%|█████████ | 967/1071 [00:22<00:02, 44.86it/s][A
 91%|█████████ | 972/1071 [00:22<00:02, 44.98it/s][A
 91%|█████████ | 977/1071 [00:22<00:02, 45.26it/s][A
 92%|█████████▏| 982/1071 [00:22<00:01, 45.27it/s][A
 92%|█████████▏| 987/1071 [00:22<00:01, 45.16it/s][A
 93%|█████████▎| 992/1071 [00:22<00:01, 45.06it/s][A
 93%|█████████▎| 997/1071 [00:22<00:01, 44.83it/s][A
 94%|█████████▎| 1002/1071 [00:22<00:01, 35.39it/s][A
 94%|█████████▍| 1007/1071 [00:23<00:01, 37.70it/s][A
 94%|█████████▍| 1012/1071 [00:23<00:01, 39.71it/s][A
 95%|█████████▍| 1017/1071 [00:23<00:01, 41.26it/s][A
 95%|█████████▌| 1022/1071 [00:23<00:01, 42.28it/s][A
 96%|█████████▌| 1027/1071 [00:23<00:01, 43.23it/s][A
 96%|█████████▋| 1032/1071 [00:23<00:00, 43.57it/s][A
 97%|█████████▋| 1037/1071 [00:23<00:00, 44.00it/s][A
 97%|█████████▋| 1042/1071 [00:23<00:00, 43.85it/s][A
 98%|█████████▊| 1047/1071 [00:23<00:00, 44.07it/s][A
 98%|█████████▊| 1052/1071 [00:24<00:00, 44.14it/s][A
 99%|█████████▊| 1057/1071 [00:24<00:00, 44.46it/s][A
 99%|█████████▉| 1062/1071 [00:24<00:00, 44.72it/s][A
100%|█████████▉| 1067/1071 [00:24<00:00, 44.92it/s][A                                                 
                                                   [A 40%|████      | 236/590 [02:07<01:35,  3.71it/s]
100%|██████████| 1071/1071 [00:24<00:00, 44.92it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-29 03:01:45,220 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-236
[INFO|configuration_utils.py:351] 2023-08-29 03:01:45,429 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-236/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:01:48,652 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-236/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:01:48,813 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-236/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:01:48,890 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-236/special_tokens_map.json
 40%|████      | 237/590 [02:19<1:04:12, 10.91s/it] 40%|████      | 238/590 [02:19<45:24,  7.74s/it]   41%|████      | 239/590 [02:19<32:12,  5.51s/it] 41%|████      | 240/590 [02:20<22:59,  3.94s/it] 41%|████      | 241/590 [02:20<16:34,  2.85s/it] 41%|████      | 242/590 [02:20<12:04,  2.08s/it] 41%|████      | 243/590 [02:20<08:56,  1.55s/it] 41%|████▏     | 244/590 [02:21<06:44,  1.17s/it] 42%|████▏     | 245/590 [02:21<05:12,  1.10it/s] 42%|████▏     | 246/590 [02:21<04:08,  1.38it/s] 42%|████▏     | 247/590 [02:22<03:23,  1.68it/s] 42%|████▏     | 248/590 [02:22<02:52,  1.98it/s] 42%|████▏     | 249/590 [02:22<02:30,  2.27it/s] 42%|████▏     | 250/590 [02:22<02:14,  2.52it/s] 43%|████▎     | 251/590 [02:23<02:03,  2.74it/s] 43%|████▎     | 252/590 [02:23<01:56,  2.91it/s] 43%|████▎     | 253/590 [02:23<01:50,  3.05it/s] 43%|████▎     | 254/590 [02:24<01:46,  3.15it/s] 43%|████▎     | 255/590 [02:24<01:43,  3.22it/s] 43%|████▎     | 256/590 [02:24<01:42,  3.27it/s] 44%|████▎     | 257/590 [02:25<01:45,  3.16it/s] 44%|████▎     | 258/590 [02:25<01:42,  3.23it/s] 44%|████▍     | 259/590 [02:25<01:40,  3.28it/s] 44%|████▍     | 260/590 [02:25<01:39,  3.32it/s] 44%|████▍     | 261/590 [02:26<01:38,  3.34it/s] 44%|████▍     | 262/590 [02:26<01:37,  3.36it/s] 45%|████▍     | 263/590 [02:26<01:36,  3.38it/s] 45%|████▍     | 264/590 [02:27<01:36,  3.39it/s] 45%|████▍     | 265/590 [02:27<01:35,  3.40it/s] 45%|████▌     | 266/590 [02:27<01:34,  3.42it/s] 45%|████▌     | 267/590 [02:28<01:33,  3.44it/s] 45%|████▌     | 268/590 [02:28<01:40,  3.19it/s] 46%|████▌     | 269/590 [02:28<01:38,  3.27it/s] 46%|████▌     | 270/590 [02:28<01:36,  3.33it/s] 46%|████▌     | 271/590 [02:29<01:34,  3.37it/s] 46%|████▌     | 272/590 [02:29<01:33,  3.40it/s] 46%|████▋     | 273/590 [02:29<01:32,  3.43it/s] 46%|████▋     | 274/590 [02:30<01:31,  3.44it/s] 47%|████▋     | 275/590 [02:30<01:31,  3.45it/s] 47%|████▋     | 276/590 [02:30<01:30,  3.46it/s] 47%|████▋     | 277/590 [02:30<01:30,  3.47it/s] 47%|████▋     | 278/590 [02:31<01:29,  3.48it/s] 47%|████▋     | 279/590 [02:31<01:41,  3.06it/s] 47%|████▋     | 280/590 [02:31<01:37,  3.17it/s] 48%|████▊     | 281/590 [02:32<01:34,  3.25it/s] 48%|████▊     | 282/590 [02:32<01:32,  3.31it/s] 48%|████▊     | 283/590 [02:32<01:31,  3.36it/s] 48%|████▊     | 284/590 [02:33<01:30,  3.39it/s] 48%|████▊     | 285/590 [02:33<01:29,  3.42it/s] 48%|████▊     | 286/590 [02:33<01:28,  3.43it/s] 49%|████▊     | 287/590 [02:33<01:27,  3.45it/s] 49%|████▉     | 288/590 [02:34<01:27,  3.46it/s] 49%|████▉     | 289/590 [02:34<01:39,  3.03it/s] 49%|████▉     | 290/590 [02:34<01:35,  3.16it/s] 49%|████▉     | 291/590 [02:35<01:32,  3.25it/s] 49%|████▉     | 292/590 [02:35<01:29,  3.31it/s] 50%|████▉     | 293/590 [02:35<01:28,  3.36it/s] 50%|████▉     | 294/590 [02:36<01:27,  3.39it/s] 50%|█████     | 295/590 [02:36<01:26,  3.42it/s] 50%|█████     | 296/590 [02:36<01:25,  3.43it/s] 50%|█████     | 297/590 [02:36<01:25,  3.44it/s] 51%|█████     | 298/590 [02:37<01:24,  3.46it/s] 51%|█████     | 299/590 [02:37<01:30,  3.22it/s] 51%|█████     | 300/590 [02:37<01:28,  3.28it/s] 51%|█████     | 301/590 [02:38<01:26,  3.34it/s] 51%|█████     | 302/590 [02:38<01:25,  3.37it/s] 51%|█████▏    | 303/590 [02:38<01:24,  3.39it/s] 52%|█████▏    | 304/590 [02:39<01:23,  3.41it/s] 52%|█████▏    | 305/590 [02:39<01:23,  3.43it/s] 52%|█████▏    | 306/590 [02:39<01:22,  3.43it/s] 52%|█████▏    | 307/590 [02:39<01:22,  3.44it/s] 52%|█████▏    | 308/590 [02:40<01:21,  3.44it/s] 52%|█████▏    | 309/590 [02:40<01:21,  3.44it/s] 53%|█████▎    | 310/590 [02:40<01:23,  3.34it/s] 53%|█████▎    | 311/590 [02:41<01:22,  3.37it/s] 53%|█████▎    | 312/590 [02:41<01:23,  3.33it/s] 53%|█████▎    | 313/590 [02:41<01:22,  3.37it/s] 53%|█████▎    | 314/590 [02:42<01:21,  3.39it/s] 53%|█████▎    | 315/590 [02:42<01:20,  3.41it/s] 54%|█████▎    | 316/590 [02:42<01:20,  3.42it/s] 54%|█████▎    | 317/590 [02:42<01:19,  3.43it/s] 54%|█████▍    | 318/590 [02:43<01:19,  3.44it/s] 54%|█████▍    | 319/590 [02:43<01:18,  3.44it/s] 54%|█████▍    | 320/590 [02:43<01:18,  3.44it/s] 54%|█████▍    | 321/590 [02:44<01:24,  3.19it/s] 55%|█████▍    | 322/590 [02:44<01:30,  2.95it/s] 55%|█████▍    | 323/590 [02:44<01:26,  3.08it/s] 55%|█████▍    | 324/590 [02:45<01:23,  3.19it/s] 55%|█████▌    | 325/590 [02:45<01:21,  3.26it/s] 55%|█████▌    | 326/590 [02:45<01:19,  3.31it/s] 55%|█████▌    | 327/590 [02:45<01:18,  3.35it/s] 56%|█████▌    | 328/590 [02:46<01:17,  3.38it/s] 56%|█████▌    | 329/590 [02:46<01:16,  3.40it/s] 56%|█████▌    | 330/590 [02:46<01:16,  3.42it/s] 56%|█████▌    | 331/590 [02:47<01:18,  3.31it/s] 56%|█████▋    | 332/590 [02:47<01:17,  3.35it/s] 56%|█████▋    | 333/590 [02:47<01:16,  3.38it/s] 57%|█████▋    | 334/590 [02:48<01:15,  3.40it/s] 57%|█████▋    | 335/590 [02:48<01:14,  3.41it/s] 57%|█████▋    | 336/590 [02:48<01:14,  3.42it/s] 57%|█████▋    | 337/590 [02:48<01:13,  3.43it/s] 57%|█████▋    | 338/590 [02:49<01:13,  3.44it/s] 57%|█████▋    | 339/590 [02:49<01:13,  3.44it/s] 58%|█████▊    | 340/590 [02:49<01:12,  3.44it/s] 58%|█████▊    | 341/590 [02:50<01:12,  3.43it/s] 58%|█████▊    | 342/590 [02:50<01:13,  3.38it/s] 58%|█████▊    | 343/590 [02:50<01:12,  3.39it/s] 58%|█████▊    | 344/590 [02:50<01:12,  3.41it/s] 58%|█████▊    | 345/590 [02:51<01:11,  3.42it/s] 59%|█████▊    | 346/590 [02:51<01:11,  3.43it/s] 59%|█████▉    | 347/590 [02:51<01:10,  3.43it/s] 59%|█████▉    | 348/590 [02:52<01:10,  3.44it/s] 59%|█████▉    | 349/590 [02:52<01:10,  3.44it/s] 59%|█████▉    | 350/590 [02:52<01:09,  3.44it/s] 59%|█████▉    | 351/590 [02:52<01:09,  3.44it/s] 60%|█████▉    | 352/590 [02:53<01:09,  3.44it/s] 60%|█████▉    | 353/590 [02:53<01:08,  3.44it/s] 60%|██████    | 354/590 [02:53<01:02,  3.78it/s][INFO|trainer.py:2140] 2023-08-29 03:02:30,877 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:02:30,877 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 03:02:30,877 >>   Batch size = 8
{'eval_loss': 0.9230112433433533, 'eval_runtime': 24.4659, 'eval_samples_per_second': 350.202, 'eval_steps_per_second': 43.775, 'epoch': 2.0}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:18, 56.45it/s][A
  1%|          | 12/1071 [00:00<00:21, 48.98it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 46.83it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 46.03it/s][A
  3%|▎         | 27/1071 [00:00<00:22, 45.63it/s][A
  3%|▎         | 32/1071 [00:00<00:22, 45.39it/s][A
  3%|▎         | 37/1071 [00:00<00:22, 45.03it/s][A
  4%|▍         | 42/1071 [00:00<00:22, 44.85it/s][A
  4%|▍         | 47/1071 [00:01<00:22, 44.92it/s][A
  5%|▍         | 52/1071 [00:01<00:22, 45.03it/s][A
  5%|▌         | 57/1071 [00:01<00:22, 45.10it/s][A
  6%|▌         | 62/1071 [00:01<00:22, 44.91it/s][A
  6%|▋         | 67/1071 [00:01<00:22, 44.78it/s][A
  7%|▋         | 72/1071 [00:01<00:22, 44.90it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 44.28it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 44.29it/s][A
  8%|▊         | 87/1071 [00:01<00:22, 44.38it/s][A
  9%|▊         | 92/1071 [00:02<00:22, 44.37it/s][A
  9%|▉         | 97/1071 [00:02<00:21, 44.59it/s][A
 10%|▉         | 102/1071 [00:02<00:21, 44.82it/s][A
 10%|▉         | 107/1071 [00:02<00:21, 44.99it/s][A
 10%|█         | 112/1071 [00:02<00:21, 44.95it/s][A
 11%|█         | 117/1071 [00:02<00:21, 44.98it/s][A
 11%|█▏        | 122/1071 [00:02<00:21, 44.87it/s][A
 12%|█▏        | 127/1071 [00:02<00:21, 44.81it/s][A
 12%|█▏        | 132/1071 [00:02<00:21, 44.60it/s][A
 13%|█▎        | 137/1071 [00:03<00:20, 44.68it/s][A
 13%|█▎        | 142/1071 [00:03<00:20, 44.75it/s][A
 14%|█▎        | 147/1071 [00:03<00:20, 45.00it/s][A
 14%|█▍        | 152/1071 [00:03<00:20, 45.02it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 44.92it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 44.82it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 44.79it/s][A
 16%|█▌        | 172/1071 [00:03<00:20, 44.71it/s][A
 17%|█▋        | 177/1071 [00:03<00:20, 44.49it/s][A
 17%|█▋        | 182/1071 [00:04<00:19, 44.61it/s][A
 17%|█▋        | 187/1071 [00:04<00:19, 44.86it/s][A
 18%|█▊        | 192/1071 [00:04<00:19, 44.91it/s][A
 18%|█▊        | 197/1071 [00:04<00:19, 44.72it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 44.80it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 44.98it/s][A
 20%|█▉        | 212/1071 [00:04<00:20, 42.85it/s][A
 20%|██        | 217/1071 [00:04<00:19, 43.52it/s][A
 21%|██        | 222/1071 [00:04<00:19, 43.85it/s][A
 21%|██        | 227/1071 [00:05<00:19, 44.23it/s][A
 22%|██▏       | 232/1071 [00:05<00:18, 44.50it/s][A
 22%|██▏       | 237/1071 [00:05<00:18, 44.58it/s][A
 23%|██▎       | 242/1071 [00:05<00:18, 44.75it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 44.83it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 44.61it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 44.39it/s][A
 24%|██▍       | 262/1071 [00:05<00:18, 44.66it/s][A
 25%|██▍       | 267/1071 [00:05<00:18, 44.52it/s][A
 25%|██▌       | 272/1071 [00:06<00:17, 45.00it/s][A
 26%|██▌       | 277/1071 [00:06<00:17, 45.01it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 44.87it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 45.02it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 44.82it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 44.89it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 44.80it/s][A
 29%|██▊       | 307/1071 [00:06<00:17, 44.73it/s][A
 29%|██▉       | 312/1071 [00:06<00:16, 44.71it/s][A
 30%|██▉       | 317/1071 [00:07<00:16, 44.90it/s][A
 30%|███       | 322/1071 [00:07<00:16, 44.99it/s][A
 31%|███       | 327/1071 [00:07<00:16, 44.86it/s][A
 31%|███       | 332/1071 [00:07<00:16, 44.90it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 44.78it/s][A
 32%|███▏      | 342/1071 [00:07<00:16, 44.73it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 42.85it/s][A
 33%|███▎      | 352/1071 [00:07<00:16, 43.55it/s][A
 33%|███▎      | 357/1071 [00:07<00:16, 44.03it/s][A
 34%|███▍      | 362/1071 [00:08<00:16, 44.21it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.48it/s][A
 35%|███▍      | 372/1071 [00:08<00:15, 44.68it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 44.61it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 44.48it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 44.36it/s][A
 37%|███▋      | 392/1071 [00:08<00:15, 44.60it/s][A
 37%|███▋      | 397/1071 [00:08<00:15, 44.74it/s][A
 38%|███▊      | 402/1071 [00:08<00:14, 44.86it/s][A
 38%|███▊      | 407/1071 [00:09<00:14, 44.90it/s][A
 38%|███▊      | 412/1071 [00:09<00:14, 44.84it/s][A
 39%|███▉      | 417/1071 [00:09<00:14, 44.80it/s][A
 39%|███▉      | 422/1071 [00:09<00:14, 44.69it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 44.67it/s][A
 40%|████      | 432/1071 [00:09<00:14, 44.51it/s][A
 41%|████      | 437/1071 [00:09<00:14, 44.57it/s][A
 41%|████▏     | 442/1071 [00:09<00:14, 44.58it/s][A
 42%|████▏     | 447/1071 [00:09<00:13, 44.79it/s][A
 42%|████▏     | 452/1071 [00:10<00:13, 44.87it/s][A
 43%|████▎     | 457/1071 [00:10<00:13, 44.79it/s][A
 43%|████▎     | 462/1071 [00:10<00:13, 44.87it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 44.85it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 44.54it/s][A
 45%|████▍     | 477/1071 [00:10<00:13, 44.54it/s][A
 45%|████▌     | 482/1071 [00:10<00:13, 43.36it/s][A
 45%|████▌     | 487/1071 [00:10<00:13, 43.98it/s][A
 46%|████▌     | 492/1071 [00:11<00:13, 44.32it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.63it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.71it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 44.75it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 44.51it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 44.67it/s][A
 49%|████▊     | 522/1071 [00:11<00:12, 44.54it/s][A
 49%|████▉     | 527/1071 [00:11<00:12, 44.48it/s][A
 50%|████▉     | 532/1071 [00:11<00:12, 44.64it/s][A
 50%|█████     | 537/1071 [00:12<00:11, 44.81it/s][A
 51%|█████     | 542/1071 [00:12<00:11, 44.79it/s][A
 51%|█████     | 547/1071 [00:12<00:11, 44.83it/s][A
 52%|█████▏    | 552/1071 [00:12<00:11, 44.82it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 44.66it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 44.56it/s][A
 53%|█████▎    | 567/1071 [00:12<00:11, 44.55it/s][A
 53%|█████▎    | 572/1071 [00:12<00:11, 44.65it/s][A
 54%|█████▍    | 577/1071 [00:12<00:11, 44.67it/s][A
 54%|█████▍    | 582/1071 [00:13<00:10, 44.95it/s][A
 55%|█████▍    | 587/1071 [00:13<00:10, 44.86it/s][A
 55%|█████▌    | 592/1071 [00:13<00:10, 44.91it/s][A
 56%|█████▌    | 597/1071 [00:13<00:10, 44.69it/s][A
 56%|█████▌    | 602/1071 [00:13<00:10, 44.71it/s][A
 57%|█████▋    | 607/1071 [00:13<00:10, 44.48it/s][A
 57%|█████▋    | 612/1071 [00:13<00:10, 44.34it/s][A
 58%|█████▊    | 617/1071 [00:13<00:10, 43.30it/s][A
 58%|█████▊    | 622/1071 [00:13<00:10, 43.76it/s][A
 59%|█████▊    | 627/1071 [00:14<00:10, 44.23it/s][A
 59%|█████▉    | 632/1071 [00:14<00:09, 44.33it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.54it/s][A
 60%|█████▉    | 642/1071 [00:14<00:09, 44.57it/s][A
 60%|██████    | 647/1071 [00:14<00:09, 44.49it/s][A
 61%|██████    | 652/1071 [00:14<00:09, 44.57it/s][A
 61%|██████▏   | 657/1071 [00:14<00:09, 44.41it/s][A
 62%|██████▏   | 662/1071 [00:14<00:09, 44.43it/s][A
 62%|██████▏   | 667/1071 [00:14<00:09, 44.59it/s][A
 63%|██████▎   | 672/1071 [00:15<00:08, 44.88it/s][A
 63%|██████▎   | 677/1071 [00:15<00:08, 45.03it/s][A
 64%|██████▎   | 682/1071 [00:15<00:08, 45.03it/s][A
 64%|██████▍   | 687/1071 [00:15<00:08, 44.83it/s][A
 65%|██████▍   | 692/1071 [00:15<00:08, 44.80it/s][A
 65%|██████▌   | 697/1071 [00:15<00:08, 44.72it/s][A
 66%|██████▌   | 702/1071 [00:15<00:08, 44.66it/s][A
 66%|██████▌   | 707/1071 [00:15<00:08, 44.59it/s][A
 66%|██████▋   | 712/1071 [00:15<00:08, 44.71it/s][A
 67%|██████▋   | 717/1071 [00:16<00:07, 44.68it/s][A
 67%|██████▋   | 722/1071 [00:16<00:07, 44.85it/s][A
 68%|██████▊   | 727/1071 [00:16<00:07, 44.94it/s][A
 68%|██████▊   | 732/1071 [00:16<00:07, 44.96it/s][A
 69%|██████▉   | 737/1071 [00:16<00:07, 44.84it/s][A
 69%|██████▉   | 742/1071 [00:16<00:07, 44.74it/s][A
 70%|██████▉   | 747/1071 [00:16<00:07, 44.64it/s][A
 70%|███████   | 752/1071 [00:16<00:07, 43.07it/s][A
 71%|███████   | 757/1071 [00:16<00:07, 43.70it/s][A
 71%|███████   | 762/1071 [00:17<00:06, 44.30it/s][A
 72%|███████▏  | 767/1071 [00:17<00:06, 44.57it/s][A
 72%|███████▏  | 772/1071 [00:17<00:06, 44.71it/s][A
 73%|███████▎  | 777/1071 [00:17<00:06, 44.83it/s][A
 73%|███████▎  | 782/1071 [00:17<00:06, 44.76it/s][A
 73%|███████▎  | 787/1071 [00:17<00:06, 44.75it/s][A
 74%|███████▍  | 792/1071 [00:17<00:06, 44.34it/s][A
 74%|███████▍  | 797/1071 [00:17<00:06, 44.46it/s][A
 75%|███████▍  | 802/1071 [00:17<00:06, 44.62it/s][A
 75%|███████▌  | 807/1071 [00:18<00:05, 44.84it/s][A
 76%|███████▌  | 812/1071 [00:18<00:05, 45.16it/s][A
 76%|███████▋  | 817/1071 [00:18<00:05, 45.00it/s][A
 77%|███████▋  | 822/1071 [00:18<00:05, 45.08it/s][A
 77%|███████▋  | 827/1071 [00:18<00:05, 44.96it/s][A
 78%|███████▊  | 832/1071 [00:18<00:05, 44.74it/s][A
 78%|███████▊  | 837/1071 [00:18<00:05, 44.47it/s][A
 79%|███████▊  | 842/1071 [00:18<00:05, 44.45it/s][A
 79%|███████▉  | 847/1071 [00:18<00:05, 44.73it/s][A
 80%|███████▉  | 852/1071 [00:19<00:04, 44.86it/s][A
 80%|████████  | 857/1071 [00:19<00:04, 44.82it/s][A
 80%|████████  | 862/1071 [00:19<00:04, 45.00it/s][A
 81%|████████  | 867/1071 [00:19<00:04, 44.74it/s][A
 81%|████████▏ | 872/1071 [00:19<00:04, 44.87it/s][A
 82%|████████▏ | 877/1071 [00:19<00:04, 44.69it/s][A
 82%|████████▏ | 882/1071 [00:19<00:04, 44.51it/s][A
 83%|████████▎ | 887/1071 [00:19<00:04, 43.67it/s][A
 83%|████████▎ | 892/1071 [00:19<00:04, 44.12it/s][A
 84%|████████▍ | 897/1071 [00:20<00:03, 44.43it/s][A
 84%|████████▍ | 902/1071 [00:20<00:03, 44.64it/s][A
 85%|████████▍ | 907/1071 [00:20<00:03, 44.67it/s][A
 85%|████████▌ | 912/1071 [00:20<00:03, 44.59it/s][A
 86%|████████▌ | 917/1071 [00:20<00:03, 44.52it/s][A
 86%|████████▌ | 922/1071 [00:20<00:03, 44.40it/s][A
 87%|████████▋ | 927/1071 [00:20<00:03, 44.19it/s][A
 87%|████████▋ | 932/1071 [00:20<00:03, 44.35it/s][A
 87%|████████▋ | 937/1071 [00:20<00:03, 44.55it/s][A
 88%|████████▊ | 942/1071 [00:21<00:02, 44.67it/s][A
 88%|████████▊ | 947/1071 [00:21<00:02, 44.97it/s][A
 89%|████████▉ | 952/1071 [00:21<00:02, 44.92it/s][A
 89%|████████▉ | 957/1071 [00:21<00:02, 44.89it/s][A
 90%|████████▉ | 962/1071 [00:21<00:02, 44.97it/s][A
 90%|█████████ | 967/1071 [00:21<00:02, 44.56it/s][A
 91%|█████████ | 972/1071 [00:21<00:02, 44.19it/s][A
 91%|█████████ | 977/1071 [00:21<00:02, 44.36it/s][A
 92%|█████████▏| 982/1071 [00:21<00:02, 44.49it/s][A
 92%|█████████▏| 987/1071 [00:22<00:01, 44.58it/s][A
 93%|█████████▎| 992/1071 [00:22<00:01, 44.70it/s][A
 93%|█████████▎| 997/1071 [00:22<00:01, 44.83it/s][A
 94%|█████████▎| 1002/1071 [00:22<00:01, 44.87it/s][A
 94%|█████████▍| 1007/1071 [00:22<00:01, 44.62it/s][A
 94%|█████████▍| 1012/1071 [00:22<00:01, 44.55it/s][A
 95%|█████████▍| 1017/1071 [00:22<00:01, 44.40it/s][A
 95%|█████████▌| 1022/1071 [00:22<00:01, 43.37it/s][A
 96%|█████████▌| 1027/1071 [00:23<00:01, 43.97it/s][A
 96%|█████████▋| 1032/1071 [00:23<00:00, 44.44it/s][A
 97%|█████████▋| 1037/1071 [00:23<00:00, 44.48it/s][A
 97%|█████████▋| 1042/1071 [00:23<00:00, 44.70it/s][A
 98%|█████████▊| 1047/1071 [00:23<00:00, 44.68it/s][A
 98%|█████████▊| 1052/1071 [00:23<00:00, 44.36it/s][A
 99%|█████████▊| 1057/1071 [00:23<00:00, 44.22it/s][A
 99%|█████████▉| 1062/1071 [00:23<00:00, 44.33it/s][A
100%|█████████▉| 1067/1071 [00:23<00:00, 44.36it/s][A                                                 
                                                   [A 60%|██████    | 354/590 [03:17<01:02,  3.78it/s]
100%|██████████| 1071/1071 [00:24<00:00, 44.36it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-29 03:02:55,062 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-354
[INFO|configuration_utils.py:351] 2023-08-29 03:02:55,179 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-354/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:02:57,851 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-354/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:02:57,942 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-354/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:02:58,006 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-354/special_tokens_map.json
 60%|██████    | 355/590 [03:28<41:42, 10.65s/it] 60%|██████    | 356/590 [03:29<29:29,  7.56s/it] 61%|██████    | 357/590 [03:29<20:53,  5.38s/it] 61%|██████    | 358/590 [03:29<14:54,  3.86s/it] 61%|██████    | 359/590 [03:29<10:43,  2.79s/it] 61%|██████    | 360/590 [03:30<07:48,  2.04s/it] 61%|██████    | 361/590 [03:30<05:47,  1.52s/it] 61%|██████▏   | 362/590 [03:30<04:21,  1.15s/it] 62%|██████▏   | 363/590 [03:31<03:22,  1.12it/s] 62%|██████▏   | 364/590 [03:31<02:41,  1.40it/s] 62%|██████▏   | 365/590 [03:31<02:12,  1.70it/s] 62%|██████▏   | 366/590 [03:32<01:54,  1.95it/s] 62%|██████▏   | 367/590 [03:32<01:39,  2.24it/s] 62%|██████▏   | 368/590 [03:32<01:28,  2.50it/s] 63%|██████▎   | 369/590 [03:32<01:21,  2.71it/s] 63%|██████▎   | 370/590 [03:33<01:16,  2.89it/s] 63%|██████▎   | 371/590 [03:33<01:12,  3.03it/s] 63%|██████▎   | 372/590 [03:33<01:09,  3.14it/s] 63%|██████▎   | 373/590 [03:34<01:07,  3.22it/s] 63%|██████▎   | 374/590 [03:34<01:05,  3.28it/s] 64%|██████▎   | 375/590 [03:34<01:05,  3.31it/s] 64%|██████▎   | 376/590 [03:34<01:04,  3.33it/s] 64%|██████▍   | 377/590 [03:35<01:04,  3.30it/s] 64%|██████▍   | 378/590 [03:35<01:03,  3.33it/s] 64%|██████▍   | 379/590 [03:35<01:03,  3.35it/s] 64%|██████▍   | 380/590 [03:36<01:02,  3.36it/s] 65%|██████▍   | 381/590 [03:36<01:01,  3.37it/s] 65%|██████▍   | 382/590 [03:36<01:01,  3.38it/s] 65%|██████▍   | 383/590 [03:37<01:00,  3.40it/s] 65%|██████▌   | 384/590 [03:37<01:00,  3.41it/s] 65%|██████▌   | 385/590 [03:37<00:59,  3.42it/s] 65%|██████▌   | 386/590 [03:37<00:59,  3.43it/s] 66%|██████▌   | 387/590 [03:38<00:59,  3.44it/s] 66%|██████▌   | 388/590 [03:38<01:02,  3.26it/s] 66%|██████▌   | 389/590 [03:38<01:00,  3.32it/s] 66%|██████▌   | 390/590 [03:39<00:59,  3.36it/s] 66%|██████▋   | 391/590 [03:39<00:58,  3.38it/s] 66%|██████▋   | 392/590 [03:39<00:58,  3.40it/s] 67%|██████▋   | 393/590 [03:39<00:57,  3.41it/s] 67%|██████▋   | 394/590 [03:40<00:57,  3.43it/s] 67%|██████▋   | 395/590 [03:40<00:56,  3.43it/s] 67%|██████▋   | 396/590 [03:40<00:56,  3.43it/s] 67%|██████▋   | 397/590 [03:41<00:56,  3.44it/s] 67%|██████▋   | 398/590 [03:41<00:55,  3.44it/s] 68%|██████▊   | 399/590 [03:41<00:58,  3.25it/s] 68%|██████▊   | 400/590 [03:42<00:57,  3.31it/s] 68%|██████▊   | 401/590 [03:42<00:56,  3.35it/s] 68%|██████▊   | 402/590 [03:42<00:55,  3.38it/s] 68%|██████▊   | 403/590 [03:42<00:55,  3.40it/s] 68%|██████▊   | 404/590 [03:43<00:54,  3.41it/s] 69%|██████▊   | 405/590 [03:43<00:54,  3.42it/s] 69%|██████▉   | 406/590 [03:43<00:53,  3.43it/s] 69%|██████▉   | 407/590 [03:44<00:53,  3.41it/s] 69%|██████▉   | 408/590 [03:44<00:55,  3.31it/s] 69%|██████▉   | 409/590 [03:44<00:58,  3.07it/s] 69%|██████▉   | 410/590 [03:45<00:57,  3.12it/s] 70%|██████▉   | 411/590 [03:45<00:55,  3.21it/s] 70%|██████▉   | 412/590 [03:45<00:54,  3.28it/s] 70%|███████   | 413/590 [03:45<00:53,  3.33it/s] 70%|███████   | 414/590 [03:46<00:52,  3.36it/s] 70%|███████   | 415/590 [03:46<00:51,  3.39it/s] 71%|███████   | 416/590 [03:46<00:51,  3.41it/s] 71%|███████   | 417/590 [03:47<00:50,  3.42it/s] 71%|███████   | 418/590 [03:47<00:50,  3.43it/s] 71%|███████   | 419/590 [03:47<00:49,  3.44it/s] 71%|███████   | 420/590 [03:47<00:49,  3.44it/s] 71%|███████▏  | 421/590 [03:48<00:49,  3.38it/s] 72%|███████▏  | 422/590 [03:48<00:49,  3.40it/s] 72%|███████▏  | 423/590 [03:48<00:48,  3.41it/s] 72%|███████▏  | 424/590 [03:49<00:48,  3.42it/s] 72%|███████▏  | 425/590 [03:49<00:48,  3.43it/s] 72%|███████▏  | 426/590 [03:49<00:47,  3.43it/s] 72%|███████▏  | 427/590 [03:50<00:47,  3.44it/s] 73%|███████▎  | 428/590 [03:50<00:47,  3.44it/s] 73%|███████▎  | 429/590 [03:50<00:46,  3.44it/s] 73%|███████▎  | 430/590 [03:50<00:47,  3.38it/s] 73%|███████▎  | 431/590 [03:51<00:46,  3.40it/s] 73%|███████▎  | 432/590 [03:51<00:46,  3.36it/s] 73%|███████▎  | 433/590 [03:51<00:46,  3.39it/s] 74%|███████▎  | 434/590 [03:52<00:45,  3.40it/s] 74%|███████▎  | 435/590 [03:52<00:45,  3.42it/s] 74%|███████▍  | 436/590 [03:52<00:44,  3.43it/s] 74%|███████▍  | 437/590 [03:52<00:44,  3.43it/s] 74%|███████▍  | 438/590 [03:53<00:44,  3.43it/s] 74%|███████▍  | 439/590 [03:53<00:43,  3.44it/s] 75%|███████▍  | 440/590 [03:53<00:43,  3.44it/s] 75%|███████▍  | 441/590 [03:54<00:43,  3.44it/s] 75%|███████▍  | 442/590 [03:54<00:43,  3.44it/s] 75%|███████▌  | 443/590 [03:54<00:42,  3.44it/s] 75%|███████▌  | 444/590 [03:55<00:42,  3.44it/s] 75%|███████▌  | 445/590 [03:55<00:42,  3.44it/s] 76%|███████▌  | 446/590 [03:55<00:41,  3.44it/s] 76%|███████▌  | 447/590 [03:55<00:41,  3.44it/s] 76%|███████▌  | 448/590 [03:56<00:41,  3.44it/s] 76%|███████▌  | 449/590 [03:56<00:40,  3.44it/s] 76%|███████▋  | 450/590 [03:56<00:40,  3.44it/s] 76%|███████▋  | 451/590 [03:57<00:40,  3.44it/s] 77%|███████▋  | 452/590 [03:57<00:40,  3.41it/s] 77%|███████▋  | 453/590 [03:57<00:40,  3.42it/s] 77%|███████▋  | 454/590 [03:57<00:39,  3.42it/s] 77%|███████▋  | 455/590 [03:58<00:39,  3.43it/s] 77%|███████▋  | 456/590 [03:58<00:39,  3.43it/s] 77%|███████▋  | 457/590 [03:58<00:38,  3.43it/s] 78%|███████▊  | 458/590 [03:59<00:38,  3.43it/s] 78%|███████▊  | 459/590 [03:59<00:38,  3.44it/s] 78%|███████▊  | 460/590 [03:59<00:37,  3.44it/s] 78%|███████▊  | 461/590 [03:59<00:37,  3.44it/s] 78%|███████▊  | 462/590 [04:00<00:37,  3.44it/s] 78%|███████▊  | 463/590 [04:00<00:37,  3.41it/s] 79%|███████▊  | 464/590 [04:00<00:36,  3.42it/s] 79%|███████▉  | 465/590 [04:01<00:36,  3.43it/s] 79%|███████▉  | 466/590 [04:01<00:36,  3.43it/s] 79%|███████▉  | 467/590 [04:01<00:35,  3.43it/s] 79%|███████▉  | 468/590 [04:02<00:35,  3.44it/s] 79%|███████▉  | 469/590 [04:02<00:35,  3.44it/s] 80%|███████▉  | 470/590 [04:02<00:34,  3.44it/s] 80%|███████▉  | 471/590 [04:02<00:34,  3.44it/s] 80%|████████  | 472/590 [04:03<00:31,  3.78it/s][INFO|trainer.py:2140] 2023-08-29 03:03:40,176 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:03:40,176 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 03:03:40,176 >>   Batch size = 8
{'eval_loss': 0.930849015712738, 'eval_runtime': 24.014, 'eval_samples_per_second': 356.791, 'eval_steps_per_second': 44.599, 'epoch': 3.0}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:18, 56.69it/s][A
  1%|          | 12/1071 [00:00<00:21, 49.25it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 46.32it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 45.67it/s][A
  3%|▎         | 27/1071 [00:00<00:22, 45.42it/s][A
  3%|▎         | 32/1071 [00:00<00:23, 44.96it/s][A
  3%|▎         | 37/1071 [00:00<00:23, 44.78it/s][A
  4%|▍         | 42/1071 [00:00<00:23, 44.63it/s][A
  4%|▍         | 47/1071 [00:01<00:22, 44.73it/s][A
  5%|▍         | 52/1071 [00:01<00:22, 44.98it/s][A
  5%|▌         | 57/1071 [00:01<00:22, 45.23it/s][A
  6%|▌         | 62/1071 [00:01<00:22, 45.19it/s][A
  6%|▋         | 67/1071 [00:01<00:22, 44.98it/s][A
  7%|▋         | 72/1071 [00:01<00:22, 45.08it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 44.77it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 44.65it/s][A
  8%|▊         | 87/1071 [00:01<00:22, 44.46it/s][A
  9%|▊         | 92/1071 [00:02<00:22, 44.42it/s][A
  9%|▉         | 97/1071 [00:02<00:21, 44.55it/s][A
 10%|▉         | 102/1071 [00:02<00:21, 44.92it/s][A
 10%|▉         | 107/1071 [00:02<00:21, 44.85it/s][A
 10%|█         | 112/1071 [00:02<00:21, 44.70it/s][A
 11%|█         | 117/1071 [00:02<00:21, 44.62it/s][A
 11%|█▏        | 122/1071 [00:02<00:21, 44.68it/s][A
 12%|█▏        | 127/1071 [00:02<00:21, 44.52it/s][A
 12%|█▏        | 132/1071 [00:02<00:21, 44.44it/s][A
 13%|█▎        | 137/1071 [00:03<00:20, 44.59it/s][A
 13%|█▎        | 142/1071 [00:03<00:20, 44.76it/s][A
 14%|█▎        | 147/1071 [00:03<00:20, 44.99it/s][A
 14%|█▍        | 152/1071 [00:03<00:20, 44.20it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 44.32it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 44.49it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 44.54it/s][A
 16%|█▌        | 172/1071 [00:03<00:20, 44.55it/s][A
 17%|█▋        | 177/1071 [00:03<00:20, 44.54it/s][A
 17%|█▋        | 182/1071 [00:04<00:19, 44.45it/s][A
 17%|█▋        | 187/1071 [00:04<00:19, 44.69it/s][A
 18%|█▊        | 192/1071 [00:04<00:19, 44.67it/s][A
 18%|█▊        | 197/1071 [00:04<00:19, 44.76it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 44.98it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 44.83it/s][A
 20%|█▉        | 212/1071 [00:04<00:19, 44.92it/s][A
 20%|██        | 217/1071 [00:04<00:19, 44.56it/s][A
 21%|██        | 222/1071 [00:04<00:19, 44.58it/s][A
 21%|██        | 227/1071 [00:05<00:18, 44.71it/s][A
 22%|██▏       | 232/1071 [00:05<00:18, 44.72it/s][A
 22%|██▏       | 237/1071 [00:05<00:18, 44.52it/s][A
 23%|██▎       | 242/1071 [00:05<00:18, 44.74it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 44.66it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 44.72it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 44.66it/s][A
 24%|██▍       | 262/1071 [00:05<00:18, 44.64it/s][A
 25%|██▍       | 267/1071 [00:05<00:18, 44.62it/s][A
 25%|██▌       | 272/1071 [00:06<00:17, 44.69it/s][A
 26%|██▌       | 277/1071 [00:06<00:17, 44.65it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 44.65it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 43.88it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 44.44it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 44.48it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 44.60it/s][A
 29%|██▊       | 307/1071 [00:06<00:17, 44.60it/s][A
 29%|██▉       | 312/1071 [00:06<00:17, 44.56it/s][A
 30%|██▉       | 317/1071 [00:07<00:16, 44.61it/s][A
 30%|███       | 322/1071 [00:07<00:16, 44.55it/s][A
 31%|███       | 327/1071 [00:07<00:16, 44.74it/s][A
 31%|███       | 332/1071 [00:07<00:16, 44.90it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 45.08it/s][A
 32%|███▏      | 342/1071 [00:07<00:16, 44.97it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 44.81it/s][A
 33%|███▎      | 352/1071 [00:07<00:15, 44.98it/s][A
 33%|███▎      | 357/1071 [00:07<00:15, 44.83it/s][A
 34%|███▍      | 362/1071 [00:08<00:15, 44.69it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.76it/s][A
 35%|███▍      | 372/1071 [00:08<00:15, 44.93it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 44.91it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 44.93it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 44.80it/s][A
 37%|███▋      | 392/1071 [00:08<00:15, 44.76it/s][A
 37%|███▋      | 397/1071 [00:08<00:15, 44.69it/s][A
 38%|███▊      | 402/1071 [00:08<00:14, 44.70it/s][A
 38%|███▊      | 407/1071 [00:09<00:14, 44.53it/s][A
 38%|███▊      | 412/1071 [00:09<00:14, 44.65it/s][A
 39%|███▉      | 417/1071 [00:09<00:14, 44.73it/s][A
 39%|███▉      | 422/1071 [00:09<00:15, 42.36it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 43.32it/s][A
 40%|████      | 432/1071 [00:09<00:14, 43.80it/s][A
 41%|████      | 437/1071 [00:09<00:14, 44.21it/s][A
 41%|████▏     | 442/1071 [00:09<00:14, 44.31it/s][A
 42%|████▏     | 447/1071 [00:09<00:14, 44.17it/s][A
 42%|████▏     | 452/1071 [00:10<00:13, 44.45it/s][A
 43%|████▎     | 457/1071 [00:10<00:13, 44.57it/s][A
 43%|████▎     | 462/1071 [00:10<00:13, 44.40it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 44.41it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 44.72it/s][A
 45%|████▍     | 477/1071 [00:10<00:13, 44.79it/s][A
 45%|████▌     | 482/1071 [00:10<00:13, 44.96it/s][A
 45%|████▌     | 487/1071 [00:10<00:13, 44.82it/s][A
 46%|████▌     | 492/1071 [00:11<00:12, 44.76it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.84it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.60it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 44.43it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 44.48it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 44.55it/s][A
 49%|████▊     | 522/1071 [00:11<00:12, 44.85it/s][A
 49%|████▉     | 527/1071 [00:11<00:12, 44.85it/s][A
 50%|████▉     | 532/1071 [00:11<00:12, 44.74it/s][A
 50%|█████     | 537/1071 [00:12<00:11, 44.59it/s][A
 51%|█████     | 542/1071 [00:12<00:11, 44.76it/s][A
 51%|█████     | 547/1071 [00:12<00:11, 44.71it/s][A
 52%|█████▏    | 552/1071 [00:12<00:11, 44.65it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 43.18it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 43.66it/s][A
 53%|█████▎    | 567/1071 [00:12<00:11, 44.13it/s][A
 53%|█████▎    | 572/1071 [00:12<00:11, 44.37it/s][A
 54%|█████▍    | 577/1071 [00:12<00:11, 44.56it/s][A
 54%|█████▍    | 582/1071 [00:13<00:10, 44.53it/s][A
 55%|█████▍    | 587/1071 [00:13<00:10, 44.56it/s][A
 55%|█████▌    | 592/1071 [00:13<00:10, 44.59it/s][A
 56%|█████▌    | 597/1071 [00:13<00:10, 44.49it/s][A
 56%|█████▌    | 602/1071 [00:13<00:10, 44.42it/s][A
 57%|█████▋    | 607/1071 [00:13<00:10, 44.61it/s][A
 57%|█████▋    | 612/1071 [00:13<00:10, 44.71it/s][A
 58%|█████▊    | 617/1071 [00:13<00:10, 44.89it/s][A
 58%|█████▊    | 622/1071 [00:13<00:10, 44.81it/s][A
 59%|█████▊    | 627/1071 [00:14<00:09, 44.90it/s][A
 59%|█████▉    | 632/1071 [00:14<00:09, 44.77it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.61it/s][A
 60%|█████▉    | 642/1071 [00:14<00:09, 44.45it/s][A
 60%|██████    | 647/1071 [00:14<00:09, 44.60it/s][A
 61%|██████    | 652/1071 [00:14<00:09, 44.63it/s][A
 61%|██████▏   | 657/1071 [00:14<00:09, 44.86it/s][A
 62%|██████▏   | 662/1071 [00:14<00:09, 44.80it/s][A
 62%|██████▏   | 667/1071 [00:14<00:09, 44.83it/s][A
 63%|██████▎   | 672/1071 [00:15<00:08, 44.81it/s][A
 63%|██████▎   | 677/1071 [00:15<00:08, 44.69it/s][A
 64%|██████▎   | 682/1071 [00:15<00:08, 44.71it/s][A
 64%|██████▍   | 687/1071 [00:15<00:08, 44.73it/s][A
 65%|██████▍   | 692/1071 [00:15<00:08, 44.08it/s][A
 65%|██████▌   | 697/1071 [00:15<00:08, 44.35it/s][A
 66%|██████▌   | 702/1071 [00:15<00:08, 44.34it/s][A
 66%|██████▌   | 707/1071 [00:15<00:08, 44.52it/s][A
 66%|██████▋   | 712/1071 [00:15<00:08, 44.56it/s][A
 67%|██████▋   | 717/1071 [00:16<00:07, 44.71it/s][A
 67%|██████▋   | 722/1071 [00:16<00:07, 44.66it/s][A
 68%|██████▊   | 727/1071 [00:16<00:07, 44.54it/s][A
 68%|██████▊   | 732/1071 [00:16<00:07, 44.66it/s][A
 69%|██████▉   | 737/1071 [00:16<00:07, 44.46it/s][A
 69%|██████▉   | 742/1071 [00:16<00:07, 44.58it/s][A
 70%|██████▉   | 747/1071 [00:16<00:07, 44.52it/s][A
 70%|███████   | 752/1071 [00:16<00:07, 44.70it/s][A
 71%|███████   | 757/1071 [00:16<00:07, 44.67it/s][A
 71%|███████   | 762/1071 [00:17<00:06, 44.83it/s][A
 72%|███████▏  | 767/1071 [00:17<00:06, 44.82it/s][A
 72%|███████▏  | 772/1071 [00:17<00:06, 44.65it/s][A
 73%|███████▎  | 777/1071 [00:17<00:06, 44.59it/s][A
 73%|███████▎  | 782/1071 [00:17<00:06, 44.42it/s][A
 73%|███████▎  | 787/1071 [00:17<00:06, 44.46it/s][A
 74%|███████▍  | 792/1071 [00:17<00:06, 44.58it/s][A
 74%|███████▍  | 797/1071 [00:17<00:06, 44.65it/s][A
 75%|███████▍  | 802/1071 [00:17<00:06, 44.76it/s][A
 75%|███████▌  | 807/1071 [00:18<00:05, 44.58it/s][A
 76%|███████▌  | 812/1071 [00:18<00:05, 44.64it/s][A
 76%|███████▋  | 817/1071 [00:18<00:05, 44.50it/s][A
 77%|███████▋  | 822/1071 [00:18<00:05, 44.69it/s][A
 77%|███████▋  | 827/1071 [00:18<00:05, 42.10it/s][A
 78%|███████▊  | 832/1071 [00:18<00:05, 43.18it/s][A
 78%|███████▊  | 837/1071 [00:18<00:05, 43.70it/s][A
 79%|███████▊  | 842/1071 [00:18<00:05, 44.19it/s][A
 79%|███████▉  | 847/1071 [00:18<00:05, 44.36it/s][A
 80%|███████▉  | 852/1071 [00:19<00:04, 44.55it/s][A
 80%|████████  | 857/1071 [00:19<00:04, 44.68it/s][A
 80%|████████  | 862/1071 [00:19<00:04, 44.77it/s][A
 81%|████████  | 867/1071 [00:19<00:04, 44.42it/s][A
 81%|████████▏ | 872/1071 [00:19<00:04, 44.18it/s][A
 82%|████████▏ | 877/1071 [00:19<00:04, 44.51it/s][A
 82%|████████▏ | 882/1071 [00:19<00:04, 44.70it/s][A
 83%|████████▎ | 887/1071 [00:19<00:04, 44.86it/s][A
 83%|████████▎ | 892/1071 [00:19<00:03, 44.85it/s][A
 84%|████████▍ | 897/1071 [00:20<00:03, 44.86it/s][A
 84%|████████▍ | 902/1071 [00:20<00:03, 44.80it/s][A
 85%|████████▍ | 907/1071 [00:20<00:03, 44.75it/s][A
 85%|████████▌ | 912/1071 [00:20<00:03, 44.61it/s][A
 86%|████████▌ | 917/1071 [00:20<00:03, 44.60it/s][A
 86%|████████▌ | 922/1071 [00:20<00:03, 44.66it/s][A
 87%|████████▋ | 927/1071 [00:20<00:03, 44.74it/s][A
 87%|████████▋ | 932/1071 [00:20<00:03, 44.68it/s][A
 87%|████████▋ | 937/1071 [00:20<00:02, 44.99it/s][A
 88%|████████▊ | 942/1071 [00:21<00:02, 44.94it/s][A
 88%|████████▊ | 947/1071 [00:21<00:02, 45.04it/s][A
 89%|████████▉ | 952/1071 [00:21<00:02, 44.87it/s][A
 89%|████████▉ | 957/1071 [00:21<00:02, 44.73it/s][A
 90%|████████▉ | 962/1071 [00:21<00:02, 44.67it/s][A
 90%|█████████ | 967/1071 [00:21<00:02, 44.51it/s][A
 91%|█████████ | 972/1071 [00:21<00:02, 44.70it/s][A
 91%|█████████ | 977/1071 [00:21<00:02, 44.92it/s][A
 92%|█████████▏| 982/1071 [00:21<00:01, 44.91it/s][A
 92%|█████████▏| 987/1071 [00:22<00:01, 45.02it/s][A
 93%|█████████▎| 992/1071 [00:22<00:01, 44.85it/s][A
 93%|█████████▎| 997/1071 [00:22<00:01, 44.95it/s][A
 94%|█████████▎| 1002/1071 [00:22<00:01, 44.67it/s][A
 94%|█████████▍| 1007/1071 [00:22<00:01, 44.74it/s][A
 94%|█████████▍| 1012/1071 [00:22<00:01, 44.80it/s][A
 95%|█████████▍| 1017/1071 [00:22<00:01, 44.64it/s][A
 95%|█████████▌| 1022/1071 [00:22<00:01, 44.87it/s][A
 96%|█████████▌| 1027/1071 [00:23<00:00, 44.99it/s][A
 96%|█████████▋| 1032/1071 [00:23<00:00, 45.09it/s][A
 97%|█████████▋| 1037/1071 [00:23<00:00, 44.75it/s][A
 97%|█████████▋| 1042/1071 [00:23<00:00, 44.85it/s][A
 98%|█████████▊| 1047/1071 [00:23<00:00, 44.88it/s][A
 98%|█████████▊| 1052/1071 [00:23<00:00, 44.84it/s][A
 99%|█████████▊| 1057/1071 [00:23<00:00, 44.79it/s][A
 99%|█████████▉| 1062/1071 [00:23<00:00, 44.91it/s][A
100%|█████████▉| 1067/1071 [00:23<00:00, 44.95it/s][A                                                 
                                                   [A 80%|████████  | 472/590 [04:27<00:31,  3.78it/s]
100%|██████████| 1071/1071 [00:23<00:00, 44.95it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-29 03:04:04,692 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-472
[INFO|configuration_utils.py:351] 2023-08-29 03:04:05,123 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-472/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:04:09,383 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-472/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:04:09,743 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-472/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:04:09,868 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-472/special_tokens_map.json
 80%|████████  | 473/590 [04:42<23:34, 12.09s/it] 80%|████████  | 474/590 [04:43<16:32,  8.56s/it] 81%|████████  | 475/590 [04:43<11:38,  6.08s/it] 81%|████████  | 476/590 [04:43<08:15,  4.34s/it] 81%|████████  | 477/590 [04:43<05:53,  3.13s/it] 81%|████████  | 478/590 [04:44<04:15,  2.28s/it] 81%|████████  | 479/590 [04:44<03:08,  1.70s/it] 81%|████████▏ | 480/590 [04:45<02:26,  1.33s/it] 82%|████████▏ | 481/590 [04:45<01:51,  1.02s/it] 82%|████████▏ | 482/590 [04:45<01:26,  1.24it/s] 82%|████████▏ | 483/590 [04:45<01:09,  1.54it/s] 82%|████████▏ | 484/590 [04:46<00:58,  1.81it/s] 82%|████████▏ | 485/590 [04:46<00:49,  2.11it/s] 82%|████████▏ | 486/590 [04:46<00:43,  2.38it/s] 83%|████████▎ | 487/590 [04:47<00:39,  2.62it/s] 83%|████████▎ | 488/590 [04:47<00:36,  2.81it/s] 83%|████████▎ | 489/590 [04:47<00:34,  2.96it/s] 83%|████████▎ | 490/590 [04:48<00:32,  3.08it/s] 83%|████████▎ | 491/590 [04:48<00:31,  3.17it/s] 83%|████████▎ | 492/590 [04:48<00:30,  3.24it/s] 84%|████████▎ | 493/590 [04:48<00:29,  3.28it/s] 84%|████████▎ | 494/590 [04:49<00:28,  3.32it/s] 84%|████████▍ | 495/590 [04:49<00:29,  3.26it/s] 84%|████████▍ | 496/590 [04:49<00:28,  3.30it/s] 84%|████████▍ | 497/590 [04:50<00:27,  3.32it/s] 84%|████████▍ | 498/590 [04:50<00:27,  3.34it/s] 85%|████████▍ | 499/590 [04:50<00:27,  3.36it/s] 85%|████████▍ | 500/590 [04:51<00:26,  3.37it/s]                                                  85%|████████▍ | 500/590 [04:51<00:26,  3.37it/s] 85%|████████▍ | 501/590 [04:51<00:26,  3.38it/s] 85%|████████▌ | 502/590 [04:51<00:25,  3.39it/s] 85%|████████▌ | 503/590 [04:51<00:25,  3.39it/s] 85%|████████▌ | 504/590 [04:52<00:25,  3.40it/s] 86%|████████▌ | 505/590 [04:52<00:25,  3.40it/s] 86%|████████▌ | 506/590 [04:52<00:25,  3.31it/s] 86%|████████▌ | 507/590 [04:53<00:24,  3.34it/s] 86%|████████▌ | 508/590 [04:53<00:24,  3.35it/s] 86%|████████▋ | 509/590 [04:53<00:24,  3.36it/s] 86%|████████▋ | 510/590 [04:53<00:23,  3.38it/s] 87%|████████▋ | 511/590 [04:54<00:23,  3.38it/s] 87%|████████▋ | 512/590 [04:54<00:23,  3.38it/s] 87%|████████▋ | 513/590 [04:54<00:22,  3.39it/s] 87%|████████▋ | 514/590 [04:55<00:22,  3.41it/s] 87%|████████▋ | 515/590 [04:55<00:21,  3.42it/s] 87%|████████▋ | 516/590 [04:55<00:21,  3.43it/s] 88%|████████▊ | 517/590 [04:56<00:21,  3.43it/s] 88%|████████▊ | 518/590 [04:56<00:20,  3.44it/s] 88%|████████▊ | 519/590 [04:56<00:20,  3.44it/s] 88%|████████▊ | 520/590 [04:56<00:20,  3.45it/s] 88%|████████▊ | 521/590 [04:57<00:20,  3.45it/s] 88%|████████▊ | 522/590 [04:57<00:19,  3.45it/s] 89%|████████▊ | 523/590 [04:57<00:19,  3.45it/s] 89%|████████▉ | 524/590 [04:58<00:19,  3.45it/s] 89%|████████▉ | 525/590 [04:58<00:18,  3.44it/s] 89%|████████▉ | 526/590 [04:58<00:18,  3.37it/s] 89%|████████▉ | 527/590 [04:58<00:18,  3.40it/s] 89%|████████▉ | 528/590 [04:59<00:18,  3.42it/s] 90%|████████▉ | 529/590 [04:59<00:17,  3.43it/s] 90%|████████▉ | 530/590 [04:59<00:17,  3.44it/s] 90%|█████████ | 531/590 [05:00<00:17,  3.44it/s] 90%|█████████ | 532/590 [05:00<00:16,  3.44it/s] 90%|█████████ | 533/590 [05:00<00:16,  3.44it/s] 91%|█████████ | 534/590 [05:00<00:16,  3.45it/s] 91%|█████████ | 535/590 [05:01<00:15,  3.44it/s] 91%|█████████ | 536/590 [05:01<00:15,  3.45it/s] 91%|█████████ | 537/590 [05:01<00:15,  3.32it/s] 91%|█████████ | 538/590 [05:02<00:15,  3.36it/s] 91%|█████████▏| 539/590 [05:02<00:15,  3.39it/s] 92%|█████████▏| 540/590 [05:02<00:14,  3.41it/s] 92%|█████████▏| 541/590 [05:03<00:14,  3.42it/s] 92%|█████████▏| 542/590 [05:03<00:13,  3.43it/s] 92%|█████████▏| 543/590 [05:03<00:13,  3.44it/s] 92%|█████████▏| 544/590 [05:03<00:13,  3.44it/s] 92%|█████████▏| 545/590 [05:04<00:13,  3.44it/s] 93%|█████████▎| 546/590 [05:04<00:12,  3.45it/s] 93%|█████████▎| 547/590 [05:04<00:12,  3.45it/s] 93%|█████████▎| 548/590 [05:05<00:12,  3.35it/s] 93%|█████████▎| 549/590 [05:05<00:12,  3.38it/s] 93%|█████████▎| 550/590 [05:05<00:11,  3.40it/s] 93%|█████████▎| 551/590 [05:05<00:11,  3.41it/s] 94%|█████████▎| 552/590 [05:06<00:11,  3.42it/s] 94%|█████████▎| 553/590 [05:06<00:10,  3.43it/s] 94%|█████████▍| 554/590 [05:06<00:10,  3.44it/s] 94%|█████████▍| 555/590 [05:07<00:10,  3.44it/s] 94%|█████████▍| 556/590 [05:07<00:09,  3.45it/s] 94%|█████████▍| 557/590 [05:07<00:09,  3.45it/s] 95%|█████████▍| 558/590 [05:08<00:09,  3.45it/s] 95%|█████████▍| 559/590 [05:08<00:09,  3.38it/s] 95%|█████████▍| 560/590 [05:08<00:08,  3.41it/s] 95%|█████████▌| 561/590 [05:08<00:08,  3.42it/s] 95%|█████████▌| 562/590 [05:09<00:08,  3.43it/s] 95%|█████████▌| 563/590 [05:09<00:07,  3.43it/s] 96%|█████████▌| 564/590 [05:09<00:07,  3.44it/s] 96%|█████████▌| 565/590 [05:10<00:07,  3.44it/s] 96%|█████████▌| 566/590 [05:10<00:06,  3.45it/s] 96%|█████████▌| 567/590 [05:10<00:06,  3.44it/s] 96%|█████████▋| 568/590 [05:10<00:06,  3.45it/s] 96%|█████████▋| 569/590 [05:11<00:06,  3.44it/s] 97%|█████████▋| 570/590 [05:11<00:06,  3.30it/s] 97%|█████████▋| 571/590 [05:11<00:05,  3.35it/s] 97%|█████████▋| 572/590 [05:12<00:05,  3.38it/s] 97%|█████████▋| 573/590 [05:12<00:05,  3.40it/s] 97%|█████████▋| 574/590 [05:12<00:04,  3.42it/s] 97%|█████████▋| 575/590 [05:12<00:04,  3.43it/s] 98%|█████████▊| 576/590 [05:13<00:04,  3.44it/s] 98%|█████████▊| 577/590 [05:13<00:03,  3.44it/s] 98%|█████████▊| 578/590 [05:13<00:03,  3.44it/s] 98%|█████████▊| 579/590 [05:14<00:03,  3.44it/s] 98%|█████████▊| 580/590 [05:14<00:02,  3.45it/s] 98%|█████████▊| 581/590 [05:14<00:02,  3.31it/s] 99%|█████████▊| 582/590 [05:15<00:02,  3.35it/s] 99%|█████████▉| 583/590 [05:15<00:02,  3.38it/s] 99%|█████████▉| 584/590 [05:15<00:01,  3.40it/s] 99%|█████████▉| 585/590 [05:15<00:01,  3.41it/s] 99%|█████████▉| 586/590 [05:16<00:01,  3.43it/s] 99%|█████████▉| 587/590 [05:16<00:00,  3.43it/s]100%|█████████▉| 588/590 [05:16<00:00,  3.44it/s]100%|█████████▉| 589/590 [05:17<00:00,  3.44it/s]100%|██████████| 590/590 [05:17<00:00,  3.79it/s][INFO|trainer.py:2140] 2023-08-29 03:04:54,388 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:04:54,388 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 03:04:54,388 >>   Batch size = 8
{'eval_loss': 0.9311632513999939, 'eval_runtime': 24.0063, 'eval_samples_per_second': 356.906, 'eval_steps_per_second': 44.613, 'epoch': 4.0}
{'loss': 0.7443, 'learning_rate': 5.720338983050847e-06, 'epoch': 4.24}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:19, 55.85it/s][A
  1%|          | 12/1071 [00:00<00:21, 48.70it/s][A
  2%|▏         | 17/1071 [00:00<00:24, 43.68it/s][A
  2%|▏         | 22/1071 [00:00<00:23, 44.21it/s][A
  3%|▎         | 27/1071 [00:00<00:23, 44.38it/s][A
  3%|▎         | 32/1071 [00:00<00:23, 44.65it/s][A
  3%|▎         | 37/1071 [00:00<00:23, 44.47it/s][A
  4%|▍         | 42/1071 [00:00<00:23, 44.67it/s][A
  4%|▍         | 47/1071 [00:01<00:22, 44.71it/s][A
  5%|▍         | 52/1071 [00:01<00:22, 44.87it/s][A
  5%|▌         | 57/1071 [00:01<00:22, 44.57it/s][A
  6%|▌         | 62/1071 [00:01<00:22, 44.74it/s][A
  6%|▋         | 67/1071 [00:01<00:22, 44.91it/s][A
  7%|▋         | 72/1071 [00:01<00:22, 44.89it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 44.91it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 44.79it/s][A
  8%|▊         | 87/1071 [00:01<00:21, 44.76it/s][A
  9%|▊         | 92/1071 [00:02<00:21, 44.77it/s][A
  9%|▉         | 97/1071 [00:02<00:21, 44.78it/s][A
 10%|▉         | 102/1071 [00:02<00:21, 44.63it/s][A
 10%|▉         | 107/1071 [00:02<00:21, 44.60it/s][A
 10%|█         | 112/1071 [00:02<00:21, 44.75it/s][A
 11%|█         | 117/1071 [00:02<00:21, 44.85it/s][A
 11%|█▏        | 122/1071 [00:02<00:21, 44.78it/s][A
 12%|█▏        | 127/1071 [00:02<00:21, 44.62it/s][A
 12%|█▏        | 132/1071 [00:02<00:20, 44.79it/s][A
 13%|█▎        | 137/1071 [00:03<00:20, 44.76it/s][A
 13%|█▎        | 142/1071 [00:03<00:20, 44.69it/s][A
 14%|█▎        | 147/1071 [00:03<00:20, 44.60it/s][A
 14%|█▍        | 152/1071 [00:03<00:21, 43.60it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 43.89it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 44.23it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 44.32it/s][A
 16%|█▌        | 172/1071 [00:03<00:20, 44.54it/s][A
 17%|█▋        | 177/1071 [00:03<00:20, 44.57it/s][A
 17%|█▋        | 182/1071 [00:04<00:19, 44.70it/s][A
 17%|█▋        | 187/1071 [00:04<00:19, 44.59it/s][A
 18%|█▊        | 192/1071 [00:04<00:19, 44.44it/s][A
 18%|█▊        | 197/1071 [00:04<00:19, 44.49it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 44.61it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 44.83it/s][A
 20%|█▉        | 212/1071 [00:04<00:19, 44.93it/s][A
 20%|██        | 217/1071 [00:04<00:19, 44.83it/s][A
 21%|██        | 222/1071 [00:04<00:18, 44.93it/s][A
 21%|██        | 227/1071 [00:05<00:18, 44.93it/s][A
 22%|██▏       | 232/1071 [00:05<00:18, 44.78it/s][A
 22%|██▏       | 237/1071 [00:05<00:18, 44.59it/s][A
 23%|██▎       | 242/1071 [00:05<00:18, 44.67it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 44.79it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 44.79it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 44.87it/s][A
 24%|██▍       | 262/1071 [00:05<00:18, 44.69it/s][A
 25%|██▍       | 267/1071 [00:05<00:17, 44.70it/s][A
 25%|██▌       | 272/1071 [00:06<00:17, 44.65it/s][A
 26%|██▌       | 277/1071 [00:06<00:17, 44.54it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 44.53it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 43.67it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 44.16it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 44.52it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 44.67it/s][A
 29%|██▊       | 307/1071 [00:06<00:17, 44.65it/s][A
 29%|██▉       | 312/1071 [00:06<00:17, 44.64it/s][A
 30%|██▉       | 317/1071 [00:07<00:16, 44.80it/s][A
 30%|███       | 322/1071 [00:07<00:16, 44.72it/s][A
 31%|███       | 327/1071 [00:07<00:16, 44.42it/s][A
 31%|███       | 332/1071 [00:07<00:16, 44.45it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 44.68it/s][A
 32%|███▏      | 342/1071 [00:07<00:16, 44.80it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 44.83it/s][A
 33%|███▎      | 352/1071 [00:07<00:16, 44.86it/s][A
 33%|███▎      | 357/1071 [00:07<00:15, 44.90it/s][A
 34%|███▍      | 362/1071 [00:08<00:15, 44.83it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.69it/s][A
 35%|███▍      | 372/1071 [00:08<00:15, 44.51it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 44.55it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 44.70it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 44.83it/s][A
 37%|███▋      | 392/1071 [00:08<00:15, 44.55it/s][A
 37%|███▋      | 397/1071 [00:08<00:15, 44.71it/s][A
 38%|███▊      | 402/1071 [00:08<00:14, 44.81it/s][A
 38%|███▊      | 407/1071 [00:09<00:14, 44.87it/s][A
 38%|███▊      | 412/1071 [00:09<00:14, 44.63it/s][A
 39%|███▉      | 417/1071 [00:09<00:14, 44.36it/s][A
 39%|███▉      | 422/1071 [00:09<00:14, 44.55it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 44.57it/s][A
 40%|████      | 432/1071 [00:09<00:14, 44.49it/s][A
 41%|████      | 437/1071 [00:09<00:14, 44.75it/s][A
 41%|████▏     | 442/1071 [00:09<00:13, 45.01it/s][A
 42%|████▏     | 447/1071 [00:09<00:13, 44.98it/s][A
 42%|████▏     | 452/1071 [00:10<00:13, 44.80it/s][A
 43%|████▎     | 457/1071 [00:10<00:13, 44.65it/s][A
 43%|████▎     | 462/1071 [00:10<00:13, 44.44it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 44.66it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 44.69it/s][A
 45%|████▍     | 477/1071 [00:10<00:13, 44.79it/s][A
 45%|████▌     | 482/1071 [00:10<00:13, 45.11it/s][A
 45%|████▌     | 487/1071 [00:10<00:12, 45.03it/s][A
 46%|████▌     | 492/1071 [00:11<00:12, 44.98it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.82it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.63it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 44.38it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 44.61it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 44.92it/s][A
 49%|████▊     | 522/1071 [00:11<00:12, 44.95it/s][A
 49%|████▉     | 527/1071 [00:11<00:13, 40.92it/s][A
 50%|████▉     | 532/1071 [00:11<00:12, 42.16it/s][A
 50%|█████     | 537/1071 [00:12<00:12, 43.18it/s][A
 51%|█████     | 542/1071 [00:12<00:12, 43.69it/s][A
 51%|█████     | 547/1071 [00:12<00:11, 43.95it/s][A
 52%|█████▏    | 552/1071 [00:12<00:11, 44.03it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 44.28it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 44.51it/s][A
 53%|█████▎    | 567/1071 [00:12<00:11, 44.36it/s][A
 53%|█████▎    | 572/1071 [00:12<00:11, 44.41it/s][A
 54%|█████▍    | 577/1071 [00:12<00:11, 44.62it/s][A
 54%|█████▍    | 582/1071 [00:13<00:10, 44.79it/s][A
 55%|█████▍    | 587/1071 [00:13<00:10, 44.98it/s][A
 55%|█████▌    | 592/1071 [00:13<00:10, 44.97it/s][A
 56%|█████▌    | 597/1071 [00:13<00:10, 45.01it/s][A
 56%|█████▌    | 602/1071 [00:13<00:10, 44.92it/s][A
 57%|█████▋    | 607/1071 [00:13<00:10, 44.84it/s][A
 57%|█████▋    | 612/1071 [00:13<00:10, 44.73it/s][A
 58%|█████▊    | 617/1071 [00:13<00:10, 44.77it/s][A
 58%|█████▊    | 622/1071 [00:13<00:10, 44.88it/s][A
 59%|█████▊    | 627/1071 [00:14<00:09, 45.02it/s][A
 59%|█████▉    | 632/1071 [00:14<00:09, 45.06it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.85it/s][A
 60%|█████▉    | 642/1071 [00:14<00:09, 44.79it/s][A
 60%|██████    | 647/1071 [00:14<00:09, 44.91it/s][A
 61%|██████    | 652/1071 [00:14<00:09, 44.67it/s][A
 61%|██████▏   | 657/1071 [00:14<00:09, 44.68it/s][A
 62%|██████▏   | 662/1071 [00:14<00:09, 42.19it/s][A
 62%|██████▏   | 667/1071 [00:14<00:09, 43.08it/s][A
 63%|██████▎   | 672/1071 [00:15<00:09, 43.58it/s][A
 63%|██████▎   | 677/1071 [00:15<00:08, 44.19it/s][A
 64%|██████▎   | 682/1071 [00:15<00:08, 44.51it/s][A
 64%|██████▍   | 687/1071 [00:15<00:08, 44.51it/s][A
 65%|██████▍   | 692/1071 [00:15<00:08, 44.65it/s][A
 65%|██████▌   | 697/1071 [00:15<00:08, 44.55it/s][A
 66%|██████▌   | 702/1071 [00:15<00:08, 44.34it/s][A
 66%|██████▌   | 707/1071 [00:15<00:08, 44.47it/s][A
 66%|██████▋   | 712/1071 [00:15<00:08, 44.63it/s][A
 67%|██████▋   | 717/1071 [00:16<00:07, 44.60it/s][A
 67%|██████▋   | 722/1071 [00:16<00:07, 44.58it/s][A
 68%|██████▊   | 727/1071 [00:16<00:07, 44.78it/s][A
 68%|██████▊   | 732/1071 [00:16<00:07, 44.69it/s][A
 69%|██████▉   | 737/1071 [00:16<00:07, 44.07it/s][A
 69%|██████▉   | 742/1071 [00:16<00:07, 44.15it/s][A
 70%|██████▉   | 747/1071 [00:16<00:07, 44.86it/s][A
 70%|███████   | 752/1071 [00:16<00:07, 44.71it/s][A
 71%|███████   | 757/1071 [00:16<00:07, 44.75it/s][A
 71%|███████   | 762/1071 [00:17<00:06, 44.69it/s][A
 72%|███████▏  | 767/1071 [00:17<00:06, 44.88it/s][A
 72%|███████▏  | 772/1071 [00:17<00:06, 44.96it/s][A
 73%|███████▎  | 777/1071 [00:17<00:06, 45.06it/s][A
 73%|███████▎  | 782/1071 [00:17<00:06, 44.95it/s][A
 73%|███████▎  | 787/1071 [00:17<00:06, 44.92it/s][A
 74%|███████▍  | 792/1071 [00:17<00:06, 44.62it/s][A
 74%|███████▍  | 797/1071 [00:17<00:07, 37.84it/s][A
 75%|███████▍  | 802/1071 [00:18<00:06, 39.81it/s][A
 75%|███████▌  | 807/1071 [00:18<00:06, 41.35it/s][A
 76%|███████▌  | 812/1071 [00:18<00:06, 42.49it/s][A
 76%|███████▋  | 817/1071 [00:18<00:05, 43.18it/s][A
 77%|███████▋  | 822/1071 [00:18<00:05, 44.00it/s][A
 77%|███████▋  | 827/1071 [00:18<00:05, 44.46it/s][A
 78%|███████▊  | 832/1071 [00:18<00:05, 44.55it/s][A
 78%|███████▊  | 837/1071 [00:18<00:05, 44.30it/s][A
 79%|███████▊  | 842/1071 [00:18<00:05, 44.23it/s][A
 79%|███████▉  | 847/1071 [00:19<00:05, 44.34it/s][A
 80%|███████▉  | 852/1071 [00:19<00:04, 44.79it/s][A
 80%|████████  | 857/1071 [00:19<00:04, 44.83it/s][A
 80%|████████  | 862/1071 [00:19<00:04, 44.98it/s][A
 81%|████████  | 867/1071 [00:19<00:04, 45.19it/s][A
 81%|████████▏ | 872/1071 [00:19<00:04, 45.23it/s][A
 82%|████████▏ | 877/1071 [00:19<00:04, 45.17it/s][A
 82%|████████▏ | 882/1071 [00:19<00:04, 44.90it/s][A
 83%|████████▎ | 887/1071 [00:19<00:04, 44.85it/s][A
 83%|████████▎ | 892/1071 [00:20<00:03, 44.84it/s][A
 84%|████████▍ | 897/1071 [00:20<00:03, 44.87it/s][A
 84%|████████▍ | 902/1071 [00:20<00:03, 44.98it/s][A
 85%|████████▍ | 907/1071 [00:20<00:03, 45.14it/s][A
 85%|████████▌ | 912/1071 [00:20<00:03, 45.20it/s][A
 86%|████████▌ | 917/1071 [00:20<00:03, 45.24it/s][A
 86%|████████▌ | 922/1071 [00:20<00:03, 45.02it/s][A
 87%|████████▋ | 927/1071 [00:20<00:03, 44.71it/s][A
 87%|████████▋ | 932/1071 [00:21<00:03, 37.99it/s][A
 87%|████████▋ | 937/1071 [00:21<00:03, 40.00it/s][A
 88%|████████▊ | 942/1071 [00:21<00:03, 41.61it/s][A
 88%|████████▊ | 947/1071 [00:21<00:02, 42.71it/s][A
 89%|████████▉ | 952/1071 [00:21<00:02, 43.38it/s][A
 89%|████████▉ | 957/1071 [00:21<00:02, 44.08it/s][A
 90%|████████▉ | 962/1071 [00:21<00:02, 44.54it/s][A
 90%|█████████ | 967/1071 [00:21<00:02, 44.80it/s][A
 91%|█████████ | 972/1071 [00:21<00:02, 44.44it/s][A
 91%|█████████ | 977/1071 [00:22<00:02, 44.18it/s][A
 92%|█████████▏| 982/1071 [00:22<00:02, 44.43it/s][A
 92%|█████████▏| 987/1071 [00:22<00:01, 44.69it/s][A
 93%|█████████▎| 992/1071 [00:22<00:01, 44.93it/s][A
 93%|█████████▎| 997/1071 [00:22<00:01, 45.08it/s][A
 94%|█████████▎| 1002/1071 [00:22<00:01, 45.22it/s][A
 94%|█████████▍| 1007/1071 [00:22<00:01, 45.23it/s][A
 94%|█████████▍| 1012/1071 [00:22<00:01, 45.20it/s][A
 95%|█████████▍| 1017/1071 [00:22<00:01, 44.82it/s][A
 95%|█████████▌| 1022/1071 [00:23<00:01, 44.56it/s][A
 96%|█████████▌| 1027/1071 [00:23<00:00, 44.64it/s][A
 96%|█████████▋| 1032/1071 [00:23<00:00, 44.92it/s][A
 97%|█████████▋| 1037/1071 [00:23<00:00, 44.99it/s][A
 97%|█████████▋| 1042/1071 [00:23<00:00, 45.09it/s][A
 98%|█████████▊| 1047/1071 [00:23<00:00, 45.18it/s][A
 98%|█████████▊| 1052/1071 [00:23<00:00, 45.14it/s][A
 99%|█████████▊| 1057/1071 [00:23<00:00, 45.02it/s][A
 99%|█████████▉| 1062/1071 [00:23<00:00, 44.77it/s][A
100%|█████████▉| 1067/1071 [00:24<00:00, 41.80it/s][A                                                 
                                                   [A100%|██████████| 590/590 [05:41<00:00,  3.79it/s]
100%|██████████| 1071/1071 [00:24<00:00, 41.80it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-29 03:05:18,844 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-590
[INFO|configuration_utils.py:351] 2023-08-29 03:05:19,045 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-590/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:05:21,942 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-590/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:05:22,055 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-590/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:05:22,126 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-590/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 03:05:28,435 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 03:05:28,448 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-118 (score: 0.9176693558692932).
                                                 100%|██████████| 590/590 [05:58<00:00,  3.79it/s]100%|██████████| 590/590 [05:58<00:00,  1.65it/s]
[INFO|trainer.py:1894] 2023-08-29 03:05:35,600 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-29 03:05:35,725 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:05:38,749 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:05:38,883 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:05:38,959 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 03:05:39,620 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:05:39,620 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:05:39,620 >>   train_loss               =      0.739
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:05:39,620 >>   train_runtime            = 0:05:58.49
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:05:39,620 >>   train_samples            =       7529
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:05:39,620 >>   train_samples_per_second =    105.008
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:05:39,620 >>   train_steps_per_second   =      1.646
{'eval_loss': 0.9341690540313721, 'eval_runtime': 24.14, 'eval_samples_per_second': 354.93, 'eval_steps_per_second': 44.366, 'epoch': 5.0}
{'train_runtime': 358.4956, 'train_samples_per_second': 105.008, 'train_steps_per_second': 1.646, 'train_loss': 0.7389795271016784, 'epoch': 5.0}
08/29/2023 03:05:39 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 03:05:39,942 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:05:39,942 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 03:05:39,942 >>   Batch size = 8
  0%|          | 0/1071 [00:00<?, ?it/s]  1%|          | 6/1071 [00:00<00:19, 55.90it/s]  1%|          | 12/1071 [00:00<00:21, 49.44it/s]  2%|▏         | 17/1071 [00:00<00:21, 47.98it/s]  2%|▏         | 22/1071 [00:00<00:22, 47.00it/s]  3%|▎         | 27/1071 [00:00<00:22, 46.54it/s]  3%|▎         | 32/1071 [00:00<00:22, 46.29it/s]  3%|▎         | 37/1071 [00:00<00:22, 45.86it/s]  4%|▍         | 42/1071 [00:00<00:22, 45.49it/s]  4%|▍         | 47/1071 [00:01<00:22, 44.75it/s]  5%|▍         | 52/1071 [00:01<00:22, 44.59it/s]  5%|▌         | 57/1071 [00:01<00:22, 44.65it/s]  6%|▌         | 62/1071 [00:01<00:22, 45.08it/s]  6%|▋         | 67/1071 [00:01<00:22, 45.14it/s]  7%|▋         | 72/1071 [00:01<00:22, 45.38it/s]  7%|▋         | 77/1071 [00:01<00:21, 45.33it/s]  8%|▊         | 82/1071 [00:01<00:21, 45.35it/s]  8%|▊         | 87/1071 [00:01<00:22, 43.23it/s]  9%|▊         | 92/1071 [00:02<00:22, 43.62it/s]  9%|▉         | 97/1071 [00:02<00:22, 43.70it/s] 10%|▉         | 102/1071 [00:02<00:22, 43.93it/s] 10%|▉         | 107/1071 [00:02<00:21, 44.33it/s] 10%|█         | 112/1071 [00:02<00:21, 44.60it/s] 11%|█         | 117/1071 [00:02<00:21, 44.82it/s] 11%|█▏        | 122/1071 [00:02<00:21, 45.10it/s] 12%|█▏        | 127/1071 [00:02<00:20, 44.96it/s] 12%|█▏        | 132/1071 [00:02<00:20, 44.85it/s] 13%|█▎        | 137/1071 [00:03<00:20, 44.90it/s] 13%|█▎        | 142/1071 [00:03<00:20, 44.77it/s] 14%|█▎        | 147/1071 [00:03<00:20, 44.86it/s] 14%|█▍        | 152/1071 [00:03<00:20, 44.63it/s] 15%|█▍        | 157/1071 [00:03<00:20, 44.88it/s] 15%|█▌        | 162/1071 [00:03<00:20, 45.00it/s] 16%|█▌        | 167/1071 [00:03<00:20, 45.15it/s] 16%|█▌        | 172/1071 [00:03<00:19, 45.12it/s] 17%|█▋        | 177/1071 [00:03<00:19, 44.91it/s] 17%|█▋        | 182/1071 [00:04<00:19, 44.83it/s] 17%|█▋        | 187/1071 [00:04<00:19, 44.70it/s] 18%|█▊        | 192/1071 [00:04<00:19, 44.59it/s] 18%|█▊        | 197/1071 [00:04<00:19, 44.58it/s] 19%|█▉        | 202/1071 [00:04<00:19, 44.72it/s] 19%|█▉        | 207/1071 [00:04<00:19, 44.78it/s] 20%|█▉        | 212/1071 [00:04<00:19, 44.93it/s] 20%|██        | 217/1071 [00:04<00:19, 44.88it/s] 21%|██        | 222/1071 [00:04<00:19, 43.94it/s] 21%|██        | 227/1071 [00:05<00:19, 44.35it/s] 22%|██▏       | 232/1071 [00:05<00:18, 44.53it/s] 22%|██▏       | 237/1071 [00:05<00:18, 44.58it/s] 23%|██▎       | 242/1071 [00:05<00:18, 44.76it/s] 23%|██▎       | 247/1071 [00:05<00:18, 44.72it/s] 24%|██▎       | 252/1071 [00:05<00:18, 45.03it/s] 24%|██▍       | 257/1071 [00:05<00:18, 45.19it/s] 24%|██▍       | 262/1071 [00:05<00:17, 45.11it/s] 25%|██▍       | 267/1071 [00:05<00:17, 45.07it/s] 25%|██▌       | 272/1071 [00:06<00:17, 45.01it/s] 26%|██▌       | 277/1071 [00:06<00:17, 44.81it/s] 26%|██▋       | 282/1071 [00:06<00:17, 44.81it/s] 27%|██▋       | 287/1071 [00:06<00:17, 44.78it/s] 27%|██▋       | 292/1071 [00:06<00:17, 44.77it/s] 28%|██▊       | 297/1071 [00:06<00:17, 44.76it/s] 28%|██▊       | 302/1071 [00:06<00:17, 44.94it/s] 29%|██▊       | 307/1071 [00:06<00:16, 44.96it/s] 29%|██▉       | 312/1071 [00:06<00:16, 44.96it/s] 30%|██▉       | 317/1071 [00:07<00:16, 44.86it/s] 30%|███       | 322/1071 [00:07<00:16, 44.90it/s] 31%|███       | 327/1071 [00:07<00:16, 44.61it/s] 31%|███       | 332/1071 [00:07<00:16, 44.81it/s] 31%|███▏      | 337/1071 [00:07<00:16, 44.68it/s] 32%|███▏      | 342/1071 [00:07<00:16, 44.92it/s] 32%|███▏      | 347/1071 [00:07<00:16, 45.05it/s] 33%|███▎      | 352/1071 [00:07<00:16, 44.92it/s] 33%|███▎      | 357/1071 [00:07<00:15, 44.65it/s] 34%|███▍      | 362/1071 [00:08<00:15, 44.62it/s] 34%|███▍      | 367/1071 [00:08<00:15, 44.62it/s] 35%|███▍      | 372/1071 [00:08<00:15, 44.70it/s] 35%|███▌      | 377/1071 [00:08<00:15, 44.88it/s] 36%|███▌      | 382/1071 [00:08<00:15, 44.83it/s] 36%|███▌      | 387/1071 [00:08<00:15, 44.97it/s] 37%|███▋      | 392/1071 [00:08<00:15, 44.93it/s] 37%|███▋      | 397/1071 [00:08<00:14, 44.97it/s] 38%|███▊      | 402/1071 [00:08<00:14, 44.84it/s] 38%|███▊      | 407/1071 [00:09<00:14, 44.66it/s] 38%|███▊      | 412/1071 [00:09<00:14, 44.83it/s] 39%|███▉      | 417/1071 [00:09<00:14, 44.77it/s] 39%|███▉      | 422/1071 [00:09<00:14, 44.78it/s] 40%|███▉      | 427/1071 [00:09<00:14, 44.77it/s] 40%|████      | 432/1071 [00:09<00:14, 44.65it/s] 41%|████      | 437/1071 [00:09<00:14, 44.89it/s] 41%|████▏     | 442/1071 [00:09<00:14, 44.89it/s] 42%|████▏     | 447/1071 [00:09<00:13, 44.94it/s] 42%|████▏     | 452/1071 [00:10<00:13, 45.05it/s] 43%|████▎     | 457/1071 [00:10<00:13, 44.98it/s] 43%|████▎     | 462/1071 [00:10<00:13, 44.92it/s] 44%|████▎     | 467/1071 [00:10<00:13, 45.00it/s] 44%|████▍     | 472/1071 [00:10<00:13, 44.78it/s] 45%|████▍     | 477/1071 [00:10<00:13, 44.84it/s] 45%|████▌     | 482/1071 [00:10<00:13, 44.99it/s] 45%|████▌     | 487/1071 [00:10<00:12, 45.04it/s] 46%|████▌     | 492/1071 [00:10<00:13, 41.86it/s] 46%|████▋     | 497/1071 [00:11<00:13, 43.07it/s] 47%|████▋     | 502/1071 [00:11<00:13, 43.68it/s] 47%|████▋     | 507/1071 [00:11<00:12, 44.11it/s] 48%|████▊     | 512/1071 [00:11<00:12, 44.30it/s] 48%|████▊     | 517/1071 [00:11<00:12, 44.42it/s] 49%|████▊     | 522/1071 [00:11<00:12, 44.70it/s] 49%|████▉     | 527/1071 [00:11<00:12, 44.69it/s] 50%|████▉     | 532/1071 [00:11<00:12, 44.48it/s] 50%|█████     | 537/1071 [00:11<00:11, 44.55it/s] 51%|█████     | 542/1071 [00:12<00:11, 44.93it/s] 51%|█████     | 547/1071 [00:12<00:11, 45.07it/s] 52%|█████▏    | 552/1071 [00:12<00:11, 45.16it/s] 52%|█████▏    | 557/1071 [00:12<00:11, 45.06it/s] 52%|█████▏    | 562/1071 [00:12<00:11, 44.91it/s] 53%|█████▎    | 567/1071 [00:12<00:11, 45.00it/s] 53%|█████▎    | 572/1071 [00:12<00:11, 44.98it/s] 54%|█████▍    | 577/1071 [00:12<00:11, 44.73it/s] 54%|█████▍    | 582/1071 [00:12<00:10, 44.84it/s] 55%|█████▍    | 587/1071 [00:13<00:10, 44.90it/s] 55%|█████▌    | 592/1071 [00:13<00:10, 45.04it/s] 56%|█████▌    | 597/1071 [00:13<00:10, 45.01it/s] 56%|█████▌    | 602/1071 [00:13<00:10, 44.81it/s] 57%|█████▋    | 607/1071 [00:13<00:10, 44.75it/s] 57%|█████▋    | 612/1071 [00:13<00:10, 45.01it/s] 58%|█████▊    | 617/1071 [00:13<00:10, 44.84it/s] 58%|█████▊    | 622/1071 [00:13<00:10, 44.54it/s] 59%|█████▊    | 627/1071 [00:13<00:10, 43.59it/s] 59%|█████▉    | 632/1071 [00:14<00:09, 44.21it/s] 59%|█████▉    | 637/1071 [00:14<00:09, 44.50it/s] 60%|█████▉    | 642/1071 [00:14<00:09, 44.66it/s] 60%|██████    | 647/1071 [00:14<00:09, 44.64it/s] 61%|██████    | 652/1071 [00:14<00:09, 44.75it/s] 61%|██████▏   | 657/1071 [00:14<00:09, 44.79it/s] 62%|██████▏   | 662/1071 [00:14<00:09, 44.67it/s] 62%|██████▏   | 667/1071 [00:14<00:09, 44.58it/s] 63%|██████▎   | 672/1071 [00:14<00:08, 44.56it/s] 63%|██████▎   | 677/1071 [00:15<00:08, 44.90it/s] 64%|██████▎   | 682/1071 [00:15<00:08, 44.80it/s] 64%|██████▍   | 687/1071 [00:15<00:08, 44.92it/s] 65%|██████▍   | 692/1071 [00:15<00:08, 45.08it/s] 65%|██████▌   | 697/1071 [00:15<00:08, 45.00it/s] 66%|██████▌   | 702/1071 [00:15<00:08, 44.94it/s] 66%|██████▌   | 707/1071 [00:15<00:08, 44.96it/s] 66%|██████▋   | 712/1071 [00:15<00:08, 44.68it/s] 67%|██████▋   | 717/1071 [00:15<00:07, 44.75it/s] 67%|██████▋   | 722/1071 [00:16<00:07, 44.82it/s] 68%|██████▊   | 727/1071 [00:16<00:07, 44.88it/s] 68%|██████▊   | 732/1071 [00:16<00:07, 45.11it/s] 69%|██████▉   | 737/1071 [00:16<00:07, 45.11it/s] 69%|██████▉   | 742/1071 [00:16<00:07, 45.21it/s] 70%|██████▉   | 747/1071 [00:16<00:07, 44.93it/s] 70%|███████   | 752/1071 [00:16<00:07, 44.82it/s] 71%|███████   | 757/1071 [00:16<00:07, 44.70it/s] 71%|███████   | 762/1071 [00:17<00:07, 42.29it/s] 72%|███████▏  | 767/1071 [00:17<00:07, 43.15it/s] 72%|███████▏  | 772/1071 [00:17<00:06, 43.84it/s] 73%|███████▎  | 777/1071 [00:17<00:06, 44.22it/s] 73%|███████▎  | 782/1071 [00:17<00:06, 44.60it/s] 73%|███████▎  | 787/1071 [00:17<00:06, 44.71it/s] 74%|███████▍  | 792/1071 [00:17<00:06, 44.82it/s] 74%|███████▍  | 797/1071 [00:17<00:06, 44.78it/s] 75%|███████▍  | 802/1071 [00:17<00:06, 44.27it/s] 75%|███████▌  | 807/1071 [00:18<00:05, 44.32it/s] 76%|███████▌  | 812/1071 [00:18<00:05, 44.53it/s] 76%|███████▋  | 817/1071 [00:18<00:05, 44.71it/s] 77%|███████▋  | 822/1071 [00:18<00:05, 44.88it/s] 77%|███████▋  | 827/1071 [00:18<00:05, 45.09it/s] 78%|███████▊  | 832/1071 [00:18<00:05, 45.20it/s] 78%|███████▊  | 837/1071 [00:18<00:05, 44.94it/s] 79%|███████▊  | 842/1071 [00:18<00:05, 44.68it/s] 79%|███████▉  | 847/1071 [00:18<00:05, 44.40it/s] 80%|███████▉  | 852/1071 [00:19<00:04, 44.48it/s] 80%|████████  | 857/1071 [00:19<00:04, 44.69it/s] 80%|████████  | 862/1071 [00:19<00:04, 44.91it/s] 81%|████████  | 867/1071 [00:19<00:04, 44.96it/s] 81%|████████▏ | 872/1071 [00:19<00:04, 45.08it/s] 82%|████████▏ | 877/1071 [00:19<00:04, 45.05it/s] 82%|████████▏ | 882/1071 [00:19<00:04, 45.07it/s] 83%|████████▎ | 887/1071 [00:19<00:04, 44.78it/s] 83%|████████▎ | 892/1071 [00:19<00:04, 44.64it/s] 84%|████████▍ | 897/1071 [00:20<00:03, 44.27it/s] 84%|████████▍ | 902/1071 [00:20<00:03, 44.41it/s] 85%|████████▍ | 907/1071 [00:20<00:03, 44.49it/s] 85%|████████▌ | 912/1071 [00:20<00:03, 44.70it/s] 86%|████████▌ | 917/1071 [00:20<00:03, 45.01it/s] 86%|████████▌ | 922/1071 [00:20<00:03, 45.02it/s] 87%|████████▋ | 927/1071 [00:20<00:03, 44.73it/s] 87%|████████▋ | 932/1071 [00:20<00:03, 44.70it/s] 87%|████████▋ | 937/1071 [00:20<00:03, 44.63it/s] 88%|████████▊ | 942/1071 [00:21<00:02, 44.41it/s] 88%|████████▊ | 947/1071 [00:21<00:02, 44.62it/s] 89%|████████▉ | 952/1071 [00:21<00:02, 44.77it/s] 89%|████████▉ | 957/1071 [00:21<00:02, 45.05it/s] 90%|████████▉ | 962/1071 [00:21<00:02, 44.96it/s] 90%|█████████ | 967/1071 [00:21<00:02, 44.85it/s] 91%|█████████ | 972/1071 [00:21<00:02, 44.80it/s] 91%|█████████ | 977/1071 [00:21<00:02, 44.82it/s] 92%|█████████▏| 982/1071 [00:21<00:01, 44.55it/s] 92%|█████████▏| 987/1071 [00:22<00:01, 44.62it/s] 93%|█████████▎| 992/1071 [00:22<00:01, 44.53it/s] 93%|█████████▎| 997/1071 [00:22<00:01, 44.83it/s] 94%|█████████▎| 1002/1071 [00:22<00:01, 44.86it/s] 94%|█████████▍| 1007/1071 [00:22<00:01, 45.13it/s] 94%|█████████▍| 1012/1071 [00:22<00:01, 45.05it/s] 95%|█████████▍| 1017/1071 [00:22<00:01, 44.89it/s] 95%|█████████▌| 1022/1071 [00:22<00:01, 44.70it/s] 96%|█████████▌| 1027/1071 [00:22<00:00, 44.68it/s] 96%|█████████▋| 1032/1071 [00:23<00:00, 42.92it/s] 97%|█████████▋| 1037/1071 [00:23<00:00, 43.69it/s] 97%|█████████▋| 1042/1071 [00:23<00:00, 44.19it/s] 98%|█████████▊| 1047/1071 [00:23<00:00, 44.56it/s] 98%|█████████▊| 1052/1071 [00:23<00:00, 44.68it/s] 99%|█████████▊| 1057/1071 [00:23<00:00, 44.76it/s] 99%|█████████▉| 1062/1071 [00:23<00:00, 44.72it/s]100%|█████████▉| 1067/1071 [00:23<00:00, 44.58it/s]100%|██████████| 1071/1071 [00:23<00:00, 44.73it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 03:06:03,903 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:06:03,903 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:06:03,903 >>   eval_loss               =     0.9177
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:06:03,903 >>   eval_runtime            = 0:00:23.96
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:06:03,903 >>   eval_samples            =       8568
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:06:03,903 >>   eval_samples_per_second =     357.59
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:06:03,903 >>   eval_steps_per_second   =     44.699
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:06:03,903 >>   perplexity              =     2.5034
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:06:16,962 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:06:16,976 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:06:16,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:06:16,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:06:16,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:06:17,451 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:06:17,453 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:06:18,213 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:06:19,357 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:06:19,357 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:06:21,167 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:06:21,185 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:06:21,185 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:06:21,185 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:06:21,185 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:06:21,977 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:06:21,979 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:06:22,598 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:06:22,833 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:06:22,833 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-118
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-590
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-472
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-236
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/checkpoint-354
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl', 'labels': ['country', 'place of death', 'production company', 'screenwriter', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 21439
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21539, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.40it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:07,  1.47it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:09,  1.50it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.57it/s]Extractor Predicting: 18it [00:11,  1.61it/s]Extractor Predicting: 19it [00:12,  1.60it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:13,  1.66it/s]Extractor Predicting: 23it [00:14,  1.70it/s]Extractor Predicting: 24it [00:15,  1.63it/s]Extractor Predicting: 25it [00:15,  1.54it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.57it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:18,  1.63it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:20,  1.61it/s]Extractor Predicting: 33it [00:20,  1.63it/s]Extractor Predicting: 34it [00:21,  1.67it/s]Extractor Predicting: 35it [00:22,  1.70it/s]Extractor Predicting: 36it [00:22,  1.68it/s]Extractor Predicting: 37it [00:23,  1.74it/s]Extractor Predicting: 38it [00:23,  1.71it/s]Extractor Predicting: 39it [00:24,  1.70it/s]Extractor Predicting: 40it [00:24,  1.70it/s]Extractor Predicting: 41it [00:25,  1.69it/s]Extractor Predicting: 42it [00:26,  1.72it/s]Extractor Predicting: 43it [00:26,  1.72it/s]Extractor Predicting: 44it [00:27,  1.67it/s]Extractor Predicting: 45it [00:27,  1.71it/s]Extractor Predicting: 46it [00:28,  1.67it/s]Extractor Predicting: 47it [00:29,  1.64it/s]Extractor Predicting: 48it [00:29,  1.69it/s]Extractor Predicting: 49it [00:30,  1.71it/s]Extractor Predicting: 50it [00:30,  1.75it/s]Extractor Predicting: 51it [00:31,  1.76it/s]Extractor Predicting: 52it [00:31,  1.75it/s]Extractor Predicting: 53it [00:32,  1.75it/s]Extractor Predicting: 54it [00:33,  1.74it/s]Extractor Predicting: 55it [00:33,  1.75it/s]Extractor Predicting: 56it [00:34,  1.76it/s]Extractor Predicting: 57it [00:34,  1.79it/s]Extractor Predicting: 58it [00:35,  1.82it/s]Extractor Predicting: 59it [00:35,  1.70it/s]Extractor Predicting: 60it [00:36,  1.71it/s]Extractor Predicting: 61it [00:37,  1.70it/s]Extractor Predicting: 62it [00:37,  1.71it/s]Extractor Predicting: 63it [00:38,  1.69it/s]Extractor Predicting: 64it [00:38,  1.73it/s]Extractor Predicting: 65it [00:39,  1.78it/s]Extractor Predicting: 66it [00:39,  1.77it/s]Extractor Predicting: 67it [00:40,  1.76it/s]Extractor Predicting: 68it [00:41,  1.75it/s]Extractor Predicting: 69it [00:41,  1.77it/s]Extractor Predicting: 70it [00:42,  1.72it/s]Extractor Predicting: 71it [00:42,  1.71it/s]Extractor Predicting: 72it [00:43,  1.68it/s]Extractor Predicting: 73it [00:44,  1.67it/s]Extractor Predicting: 74it [00:44,  1.66it/s]Extractor Predicting: 75it [00:45,  1.67it/s]Extractor Predicting: 76it [00:45,  1.62it/s]Extractor Predicting: 77it [00:46,  1.59it/s]Extractor Predicting: 78it [00:47,  1.62it/s]Extractor Predicting: 79it [00:47,  1.58it/s]Extractor Predicting: 80it [00:48,  1.56it/s]Extractor Predicting: 81it [00:49,  1.49it/s]Extractor Predicting: 82it [00:49,  1.50it/s]Extractor Predicting: 83it [00:50,  1.52it/s]Extractor Predicting: 84it [00:51,  1.51it/s]Extractor Predicting: 85it [00:51,  1.52it/s]Extractor Predicting: 86it [00:52,  1.49it/s]Extractor Predicting: 87it [00:53,  1.50it/s]Extractor Predicting: 88it [00:53,  1.50it/s]Extractor Predicting: 89it [00:54,  1.52it/s]Extractor Predicting: 90it [00:55,  1.53it/s]Extractor Predicting: 91it [00:55,  1.49it/s]Extractor Predicting: 92it [00:56,  1.49it/s]Extractor Predicting: 93it [00:57,  1.48it/s]Extractor Predicting: 94it [00:57,  1.49it/s]Extractor Predicting: 95it [00:58,  1.50it/s]Extractor Predicting: 96it [00:59,  1.50it/s]Extractor Predicting: 97it [00:59,  1.53it/s]Extractor Predicting: 98it [01:00,  1.53it/s]Extractor Predicting: 99it [01:01,  1.54it/s]Extractor Predicting: 100it [01:01,  1.52it/s]Extractor Predicting: 101it [01:02,  1.52it/s]Extractor Predicting: 102it [01:03,  1.47it/s]Extractor Predicting: 103it [01:03,  1.50it/s]Extractor Predicting: 104it [01:04,  1.49it/s]Extractor Predicting: 105it [01:05,  1.50it/s]Extractor Predicting: 106it [01:05,  1.48it/s]Extractor Predicting: 107it [01:06,  1.49it/s]Extractor Predicting: 108it [01:07,  1.36it/s]Extractor Predicting: 109it [01:08,  1.40it/s]Extractor Predicting: 110it [01:08,  1.46it/s]Extractor Predicting: 111it [01:09,  1.47it/s]Extractor Predicting: 112it [01:10,  1.52it/s]Extractor Predicting: 113it [01:10,  1.53it/s]Extractor Predicting: 114it [01:11,  1.53it/s]Extractor Predicting: 115it [01:12,  1.48it/s]Extractor Predicting: 116it [01:12,  1.49it/s]Extractor Predicting: 117it [01:13,  1.50it/s]Extractor Predicting: 118it [01:14,  1.48it/s]Extractor Predicting: 119it [01:14,  1.48it/s]Extractor Predicting: 120it [01:15,  1.51it/s]Extractor Predicting: 121it [01:15,  1.53it/s]Extractor Predicting: 122it [01:16,  1.49it/s]Extractor Predicting: 123it [01:17,  1.50it/s]Extractor Predicting: 124it [01:18,  1.49it/s]Extractor Predicting: 125it [01:18,  1.51it/s]Extractor Predicting: 126it [01:19,  1.54it/s]Extractor Predicting: 127it [01:19,  1.60it/s]Extractor Predicting: 128it [01:20,  1.61it/s]Extractor Predicting: 129it [01:21,  1.62it/s]Extractor Predicting: 130it [01:21,  1.58it/s]Extractor Predicting: 131it [01:22,  1.60it/s]Extractor Predicting: 132it [01:23,  1.57it/s]Extractor Predicting: 133it [01:23,  1.55it/s]Extractor Predicting: 134it [01:24,  1.56it/s]Extractor Predicting: 135it [01:24,  1.58it/s]Extractor Predicting: 136it [01:25,  1.62it/s]Extractor Predicting: 137it [01:26,  1.63it/s]Extractor Predicting: 138it [01:26,  1.63it/s]Extractor Predicting: 139it [01:27,  1.64it/s]Extractor Predicting: 140it [01:27,  1.64it/s]Extractor Predicting: 141it [01:28,  1.66it/s]Extractor Predicting: 142it [01:29,  1.63it/s]Extractor Predicting: 143it [01:29,  1.58it/s]Extractor Predicting: 144it [01:30,  1.59it/s]Extractor Predicting: 145it [01:31,  1.64it/s]Extractor Predicting: 146it [01:31,  1.67it/s]Extractor Predicting: 147it [01:32,  1.67it/s]Extractor Predicting: 148it [01:32,  1.67it/s]Extractor Predicting: 149it [01:33,  1.64it/s]Extractor Predicting: 150it [01:34,  1.66it/s]Extractor Predicting: 151it [01:34,  1.62it/s]Extractor Predicting: 152it [01:35,  1.62it/s]Extractor Predicting: 153it [01:35,  1.61it/s]Extractor Predicting: 154it [01:36,  1.57it/s]Extractor Predicting: 155it [01:37,  1.57it/s]Extractor Predicting: 156it [01:37,  1.58it/s]Extractor Predicting: 157it [01:38,  1.59it/s]Extractor Predicting: 158it [01:39,  1.55it/s]Extractor Predicting: 159it [01:39,  1.56it/s]Extractor Predicting: 160it [01:40,  1.58it/s]Extractor Predicting: 161it [01:41,  1.59it/s]Extractor Predicting: 162it [01:41,  1.63it/s]Extractor Predicting: 163it [01:42,  1.78it/s]Extractor Predicting: 164it [01:42,  1.90it/s]Extractor Predicting: 165it [01:42,  1.91it/s]Extractor Predicting: 166it [01:43,  1.86it/s]Extractor Predicting: 167it [01:44,  1.74it/s]Extractor Predicting: 168it [01:44,  1.67it/s]Extractor Predicting: 169it [01:45,  1.62it/s]Extractor Predicting: 170it [01:46,  1.63it/s]Extractor Predicting: 171it [01:46,  1.64it/s]Extractor Predicting: 172it [01:47,  1.60it/s]Extractor Predicting: 173it [01:48,  1.60it/s]Extractor Predicting: 174it [01:48,  1.61it/s]Extractor Predicting: 175it [01:49,  1.59it/s]Extractor Predicting: 176it [01:49,  1.60it/s]Extractor Predicting: 177it [01:50,  1.58it/s]Extractor Predicting: 178it [01:51,  1.61it/s]Extractor Predicting: 179it [01:51,  1.56it/s]Extractor Predicting: 180it [01:52,  1.56it/s]Extractor Predicting: 181it [01:53,  1.59it/s]Extractor Predicting: 182it [01:53,  1.60it/s]Extractor Predicting: 183it [01:54,  1.57it/s]Extractor Predicting: 184it [01:55,  1.56it/s]Extractor Predicting: 185it [01:55,  1.55it/s]Extractor Predicting: 186it [01:56,  1.57it/s]Extractor Predicting: 187it [01:56,  1.58it/s]Extractor Predicting: 188it [01:57,  1.58it/s]Extractor Predicting: 189it [01:58,  1.58it/s]Extractor Predicting: 190it [01:58,  1.58it/s]Extractor Predicting: 191it [01:59,  1.60it/s]Extractor Predicting: 192it [02:00,  1.61it/s]Extractor Predicting: 193it [02:00,  1.61it/s]Extractor Predicting: 194it [02:01,  1.63it/s]Extractor Predicting: 195it [02:01,  1.58it/s]Extractor Predicting: 196it [02:02,  1.60it/s]Extractor Predicting: 197it [02:03,  1.57it/s]Extractor Predicting: 198it [02:03,  1.57it/s]Extractor Predicting: 199it [02:04,  1.56it/s]Extractor Predicting: 200it [02:05,  1.56it/s]Extractor Predicting: 201it [02:05,  1.53it/s]Extractor Predicting: 202it [02:06,  1.53it/s]Extractor Predicting: 203it [02:07,  1.53it/s]Extractor Predicting: 204it [02:07,  1.54it/s]Extractor Predicting: 205it [02:08,  1.55it/s]Extractor Predicting: 206it [02:09,  1.56it/s]Extractor Predicting: 207it [02:09,  1.59it/s]Extractor Predicting: 208it [02:10,  1.61it/s]Extractor Predicting: 209it [02:10,  1.59it/s]Extractor Predicting: 210it [02:11,  1.59it/s]Extractor Predicting: 211it [02:12,  1.59it/s]Extractor Predicting: 212it [02:12,  1.57it/s]Extractor Predicting: 213it [02:13,  1.39it/s]Extractor Predicting: 214it [02:14,  1.46it/s]Extractor Predicting: 215it [02:14,  1.50it/s]Extractor Predicting: 216it [02:15,  1.52it/s]Extractor Predicting: 217it [02:16,  1.52it/s]Extractor Predicting: 218it [02:16,  1.52it/s]Extractor Predicting: 219it [02:17,  1.54it/s]Extractor Predicting: 220it [02:18,  1.54it/s]Extractor Predicting: 221it [02:18,  1.58it/s]Extractor Predicting: 222it [02:19,  1.58it/s]Extractor Predicting: 223it [02:20,  1.59it/s]Extractor Predicting: 224it [02:20,  1.62it/s]Extractor Predicting: 225it [02:21,  1.64it/s]Extractor Predicting: 226it [02:21,  1.62it/s]Extractor Predicting: 227it [02:22,  1.60it/s]Extractor Predicting: 228it [02:23,  1.61it/s]Extractor Predicting: 229it [02:23,  1.53it/s]Extractor Predicting: 230it [02:24,  1.58it/s]Extractor Predicting: 231it [02:25,  1.58it/s]Extractor Predicting: 232it [02:25,  1.56it/s]Extractor Predicting: 233it [02:26,  1.55it/s]Extractor Predicting: 234it [02:27,  1.51it/s]Extractor Predicting: 235it [02:27,  1.52it/s]Extractor Predicting: 236it [02:28,  1.50it/s]Extractor Predicting: 237it [02:29,  1.47it/s]Extractor Predicting: 238it [02:29,  1.47it/s]Extractor Predicting: 239it [02:30,  1.44it/s]Extractor Predicting: 240it [02:31,  1.43it/s]Extractor Predicting: 241it [02:31,  1.48it/s]Extractor Predicting: 242it [02:32,  1.48it/s]Extractor Predicting: 243it [02:33,  1.51it/s]Extractor Predicting: 244it [02:33,  1.51it/s]Extractor Predicting: 245it [02:34,  1.55it/s]Extractor Predicting: 246it [02:35,  1.58it/s]Extractor Predicting: 247it [02:35,  1.60it/s]Extractor Predicting: 248it [02:36,  1.63it/s]Extractor Predicting: 249it [02:36,  1.67it/s]Extractor Predicting: 250it [02:37,  1.63it/s]Extractor Predicting: 251it [02:37,  1.66it/s]Extractor Predicting: 252it [02:38,  1.64it/s]Extractor Predicting: 253it [02:39,  1.62it/s]Extractor Predicting: 254it [02:39,  1.63it/s]Extractor Predicting: 255it [02:40,  1.63it/s]Extractor Predicting: 256it [02:41,  1.60it/s]Extractor Predicting: 257it [02:41,  1.62it/s]Extractor Predicting: 258it [02:42,  1.65it/s]Extractor Predicting: 259it [02:42,  1.62it/s]Extractor Predicting: 260it [02:43,  1.62it/s]Extractor Predicting: 261it [02:44,  1.61it/s]Extractor Predicting: 262it [02:44,  1.61it/s]Extractor Predicting: 263it [02:45,  1.60it/s]Extractor Predicting: 264it [02:46,  1.59it/s]Extractor Predicting: 265it [02:46,  1.60it/s]Extractor Predicting: 266it [02:47,  1.59it/s]Extractor Predicting: 267it [02:47,  1.59it/s]Extractor Predicting: 268it [02:48,  1.58it/s]Extractor Predicting: 269it [02:49,  1.53it/s]Extractor Predicting: 270it [02:49,  1.58it/s]Extractor Predicting: 271it [02:50,  1.60it/s]Extractor Predicting: 272it [02:51,  1.66it/s]Extractor Predicting: 273it [02:51,  1.68it/s]Extractor Predicting: 274it [02:52,  1.64it/s]Extractor Predicting: 275it [02:52,  1.62it/s]Extractor Predicting: 276it [02:53,  1.64it/s]Extractor Predicting: 277it [02:54,  1.66it/s]Extractor Predicting: 278it [02:54,  1.64it/s]Extractor Predicting: 279it [02:55,  1.58it/s]Extractor Predicting: 280it [02:55,  1.66it/s]Extractor Predicting: 281it [02:56,  1.67it/s]Extractor Predicting: 282it [02:57,  1.64it/s]Extractor Predicting: 283it [02:57,  1.60it/s]Extractor Predicting: 284it [02:58,  1.59it/s]Extractor Predicting: 285it [02:59,  1.55it/s]Extractor Predicting: 286it [02:59,  1.57it/s]Extractor Predicting: 287it [03:00,  1.55it/s]Extractor Predicting: 288it [03:01,  1.55it/s]Extractor Predicting: 289it [03:01,  1.53it/s]Extractor Predicting: 290it [03:02,  1.53it/s]Extractor Predicting: 291it [03:03,  1.56it/s]Extractor Predicting: 292it [03:03,  1.59it/s]Extractor Predicting: 293it [03:04,  1.58it/s]Extractor Predicting: 294it [03:04,  1.52it/s]Extractor Predicting: 295it [03:05,  1.53it/s]Extractor Predicting: 296it [03:06,  1.51it/s]Extractor Predicting: 297it [03:06,  1.48it/s]Extractor Predicting: 298it [03:07,  1.47it/s]Extractor Predicting: 299it [03:08,  1.45it/s]Extractor Predicting: 300it [03:09,  1.46it/s]Extractor Predicting: 301it [03:09,  1.44it/s]Extractor Predicting: 302it [03:10,  1.44it/s]Extractor Predicting: 303it [03:11,  1.45it/s]Extractor Predicting: 303it [03:11,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:46,752 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:46,784 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:46,785 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:46,785 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:46,785 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:09:47,753 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:09:47,754 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:09:48,386 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:09:49,571 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:09:49,571 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:52,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:52,553 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:52,553 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:52,553 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:09:52,553 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:09:53,274 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:09:53,275 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:09:53,860 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:09:54,064 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:09:54,064 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.40019102196752626,
  "recall": 0.048902894491129785,
  "score": 0.08715548621944878,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'labels': ['continent', 'field of this occupation', 'field of work', 'founded by', 'movement', 'owned by', 'performer', 'producer', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20815
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20915, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.66it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.66it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:04,  1.60it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:08,  1.64it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:11,  1.58it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:13,  1.55it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:14,  1.59it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:16,  1.62it/s]Extractor Predicting: 27it [00:16,  1.59it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.63it/s]Extractor Predicting: 32it [00:19,  1.58it/s]Extractor Predicting: 33it [00:20,  1.61it/s]Extractor Predicting: 34it [00:21,  1.59it/s]Extractor Predicting: 35it [00:21,  1.58it/s]Extractor Predicting: 36it [00:22,  1.62it/s]Extractor Predicting: 37it [00:23,  1.58it/s]Extractor Predicting: 38it [00:23,  1.60it/s]Extractor Predicting: 39it [00:24,  1.65it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:25,  1.63it/s]Extractor Predicting: 42it [00:26,  1.63it/s]Extractor Predicting: 43it [00:26,  1.60it/s]Extractor Predicting: 44it [00:27,  1.57it/s]Extractor Predicting: 45it [00:28,  1.55it/s]Extractor Predicting: 46it [00:28,  1.54it/s]Extractor Predicting: 47it [00:29,  1.53it/s]Extractor Predicting: 48it [00:30,  1.54it/s]Extractor Predicting: 49it [00:30,  1.56it/s]Extractor Predicting: 50it [00:31,  1.61it/s]Extractor Predicting: 51it [00:31,  1.62it/s]Extractor Predicting: 52it [00:32,  1.61it/s]Extractor Predicting: 53it [00:33,  1.62it/s]Extractor Predicting: 54it [00:33,  1.59it/s]Extractor Predicting: 55it [00:34,  1.62it/s]Extractor Predicting: 56it [00:35,  1.48it/s]Extractor Predicting: 57it [00:35,  1.52it/s]Extractor Predicting: 58it [00:36,  1.53it/s]Extractor Predicting: 59it [00:37,  1.55it/s]Extractor Predicting: 60it [00:37,  1.57it/s]Extractor Predicting: 61it [00:38,  1.57it/s]Extractor Predicting: 62it [00:38,  1.56it/s]Extractor Predicting: 63it [00:39,  1.58it/s]Extractor Predicting: 64it [00:40,  1.60it/s]Extractor Predicting: 65it [00:40,  1.52it/s]Extractor Predicting: 66it [00:41,  1.56it/s]Extractor Predicting: 67it [00:42,  1.58it/s]Extractor Predicting: 68it [00:42,  1.56it/s]Extractor Predicting: 69it [00:43,  1.57it/s]Extractor Predicting: 70it [00:44,  1.59it/s]Extractor Predicting: 71it [00:44,  1.59it/s]Extractor Predicting: 72it [00:45,  1.62it/s]Extractor Predicting: 73it [00:45,  1.62it/s]Extractor Predicting: 74it [00:46,  1.61it/s]Extractor Predicting: 75it [00:47,  1.58it/s]Extractor Predicting: 76it [00:47,  1.61it/s]Extractor Predicting: 77it [00:48,  1.61it/s]Extractor Predicting: 78it [00:48,  1.62it/s]Extractor Predicting: 79it [00:49,  1.61it/s]Extractor Predicting: 80it [00:50,  1.60it/s]Extractor Predicting: 81it [00:50,  1.61it/s]Extractor Predicting: 82it [00:51,  1.63it/s]Extractor Predicting: 83it [00:52,  1.62it/s]Extractor Predicting: 84it [00:52,  1.61it/s]Extractor Predicting: 85it [00:53,  1.60it/s]Extractor Predicting: 86it [00:53,  1.63it/s]Extractor Predicting: 87it [00:54,  1.59it/s]Extractor Predicting: 88it [00:55,  1.59it/s]Extractor Predicting: 89it [00:55,  1.59it/s]Extractor Predicting: 90it [00:56,  1.58it/s]Extractor Predicting: 91it [00:57,  1.59it/s]Extractor Predicting: 92it [00:57,  1.60it/s]Extractor Predicting: 93it [00:58,  1.59it/s]Extractor Predicting: 94it [00:58,  1.60it/s]Extractor Predicting: 95it [00:59,  1.59it/s]Extractor Predicting: 96it [01:00,  1.59it/s]Extractor Predicting: 97it [01:00,  1.57it/s]Extractor Predicting: 98it [01:01,  1.62it/s]Extractor Predicting: 99it [01:02,  1.61it/s]Extractor Predicting: 100it [01:02,  1.56it/s]Extractor Predicting: 101it [01:03,  1.59it/s]Extractor Predicting: 102it [01:03,  1.59it/s]Extractor Predicting: 103it [01:04,  1.59it/s]Extractor Predicting: 104it [01:05,  1.59it/s]Extractor Predicting: 105it [01:05,  1.58it/s]Extractor Predicting: 106it [01:06,  1.57it/s]Extractor Predicting: 107it [01:07,  1.60it/s]Extractor Predicting: 108it [01:07,  1.57it/s]Extractor Predicting: 109it [01:08,  1.57it/s]Extractor Predicting: 110it [01:09,  1.57it/s]Extractor Predicting: 111it [01:09,  1.61it/s]Extractor Predicting: 112it [01:10,  1.62it/s]Extractor Predicting: 113it [01:10,  1.64it/s]Extractor Predicting: 114it [01:11,  1.61it/s]Extractor Predicting: 115it [01:12,  1.60it/s]Extractor Predicting: 116it [01:12,  1.62it/s]Extractor Predicting: 117it [01:13,  1.60it/s]Extractor Predicting: 118it [01:14,  1.59it/s]Extractor Predicting: 119it [01:14,  1.59it/s]Extractor Predicting: 120it [01:15,  1.59it/s]Extractor Predicting: 121it [01:15,  1.61it/s]Extractor Predicting: 122it [01:16,  1.60it/s]Extractor Predicting: 123it [01:17,  1.62it/s]Extractor Predicting: 124it [01:17,  1.59it/s]Extractor Predicting: 125it [01:18,  1.61it/s]Extractor Predicting: 126it [01:19,  1.59it/s]Extractor Predicting: 127it [01:19,  1.59it/s]Extractor Predicting: 128it [01:20,  1.59it/s]Extractor Predicting: 129it [01:20,  1.62it/s]Extractor Predicting: 130it [01:21,  1.66it/s]Extractor Predicting: 131it [01:22,  1.65it/s]Extractor Predicting: 132it [01:22,  1.61it/s]Extractor Predicting: 133it [01:23,  1.56it/s]Extractor Predicting: 134it [01:24,  1.51it/s]Extractor Predicting: 135it [01:24,  1.55it/s]Extractor Predicting: 136it [01:25,  1.55it/s]Extractor Predicting: 137it [01:26,  1.55it/s]Extractor Predicting: 138it [01:26,  1.54it/s]Extractor Predicting: 139it [01:27,  1.53it/s]Extractor Predicting: 140it [01:27,  1.54it/s]Extractor Predicting: 141it [01:28,  1.38it/s]Extractor Predicting: 142it [01:29,  1.42it/s]Extractor Predicting: 143it [01:30,  1.45it/s]Extractor Predicting: 144it [01:30,  1.47it/s]Extractor Predicting: 145it [01:31,  1.49it/s]Extractor Predicting: 146it [01:32,  1.52it/s]Extractor Predicting: 147it [01:32,  1.50it/s]Extractor Predicting: 148it [01:33,  1.55it/s]Extractor Predicting: 149it [01:34,  1.53it/s]Extractor Predicting: 150it [01:34,  1.54it/s]Extractor Predicting: 151it [01:35,  1.54it/s]Extractor Predicting: 152it [01:35,  1.60it/s]Extractor Predicting: 153it [01:36,  1.62it/s]Extractor Predicting: 154it [01:37,  1.59it/s]Extractor Predicting: 155it [01:37,  1.59it/s]Extractor Predicting: 156it [01:38,  1.60it/s]Extractor Predicting: 157it [01:39,  1.62it/s]Extractor Predicting: 158it [01:39,  1.60it/s]Extractor Predicting: 159it [01:40,  1.50it/s]Extractor Predicting: 160it [01:41,  1.54it/s]Extractor Predicting: 161it [01:41,  1.56it/s]Extractor Predicting: 162it [01:42,  1.56it/s]Extractor Predicting: 163it [01:42,  1.58it/s]Extractor Predicting: 164it [01:43,  1.57it/s]Extractor Predicting: 165it [01:44,  1.61it/s]Extractor Predicting: 166it [01:44,  1.63it/s]Extractor Predicting: 167it [01:45,  1.59it/s]Extractor Predicting: 168it [01:46,  1.61it/s]Extractor Predicting: 169it [01:46,  1.62it/s]Extractor Predicting: 170it [01:47,  1.59it/s]Extractor Predicting: 171it [01:47,  1.57it/s]Extractor Predicting: 172it [01:48,  1.56it/s]Extractor Predicting: 173it [01:49,  1.57it/s]Extractor Predicting: 174it [01:49,  1.61it/s]Extractor Predicting: 175it [01:50,  1.63it/s]Extractor Predicting: 176it [01:51,  1.60it/s]Extractor Predicting: 177it [01:51,  1.59it/s]Extractor Predicting: 178it [01:52,  1.58it/s]Extractor Predicting: 179it [01:52,  1.57it/s]Extractor Predicting: 180it [01:53,  1.62it/s]Extractor Predicting: 181it [01:54,  1.63it/s]Extractor Predicting: 182it [01:54,  1.59it/s]Extractor Predicting: 183it [01:55,  1.62it/s]Extractor Predicting: 184it [01:55,  1.64it/s]Extractor Predicting: 185it [01:56,  1.63it/s]Extractor Predicting: 186it [01:57,  1.65it/s]Extractor Predicting: 187it [01:57,  1.63it/s]Extractor Predicting: 188it [01:58,  1.60it/s]Extractor Predicting: 189it [01:59,  1.57it/s]Extractor Predicting: 190it [01:59,  1.56it/s]Extractor Predicting: 191it [02:00,  1.58it/s]Extractor Predicting: 192it [02:01,  1.60it/s]Extractor Predicting: 193it [02:01,  1.60it/s]Extractor Predicting: 194it [02:02,  1.60it/s]Extractor Predicting: 195it [02:02,  1.62it/s]Extractor Predicting: 196it [02:03,  1.66it/s]Extractor Predicting: 197it [02:04,  1.62it/s]Extractor Predicting: 198it [02:04,  1.65it/s]Extractor Predicting: 199it [02:05,  1.64it/s]Extractor Predicting: 200it [02:05,  1.61it/s]Extractor Predicting: 201it [02:06,  1.62it/s]Extractor Predicting: 202it [02:07,  1.61it/s]Extractor Predicting: 203it [02:07,  1.62it/s]Extractor Predicting: 204it [02:08,  1.63it/s]Extractor Predicting: 205it [02:08,  1.66it/s]Extractor Predicting: 206it [02:09,  1.64it/s]Extractor Predicting: 207it [02:10,  1.63it/s]Extractor Predicting: 208it [02:10,  1.66it/s]Extractor Predicting: 209it [02:11,  1.66it/s]Extractor Predicting: 210it [02:12,  1.63it/s]Extractor Predicting: 211it [02:12,  1.64it/s]Extractor Predicting: 212it [02:13,  1.61it/s]Extractor Predicting: 213it [02:13,  1.60it/s]Extractor Predicting: 214it [02:14,  1.59it/s]Extractor Predicting: 215it [02:15,  1.59it/s]Extractor Predicting: 216it [02:15,  1.59it/s]Extractor Predicting: 217it [02:16,  1.57it/s]Extractor Predicting: 218it [02:17,  1.63it/s]Extractor Predicting: 219it [02:17,  1.64it/s]Extractor Predicting: 220it [02:18,  1.62it/s]Extractor Predicting: 221it [02:18,  1.59it/s]Extractor Predicting: 222it [02:19,  1.59it/s]Extractor Predicting: 223it [02:20,  1.61it/s]Extractor Predicting: 224it [02:20,  1.61it/s]Extractor Predicting: 225it [02:21,  1.65it/s]Extractor Predicting: 226it [02:21,  1.66it/s]Extractor Predicting: 227it [02:22,  1.64it/s]Extractor Predicting: 228it [02:23,  1.62it/s]Extractor Predicting: 229it [02:23,  1.64it/s]Extractor Predicting: 230it [02:24,  1.61it/s]Extractor Predicting: 231it [02:24,  1.66it/s]Extractor Predicting: 232it [02:25,  1.60it/s]Extractor Predicting: 233it [02:26,  1.61it/s]Extractor Predicting: 234it [02:26,  1.60it/s]Extractor Predicting: 235it [02:27,  1.61it/s]Extractor Predicting: 236it [02:28,  1.59it/s]Extractor Predicting: 237it [02:28,  1.60it/s]Extractor Predicting: 238it [02:29,  1.59it/s]Extractor Predicting: 239it [02:29,  1.64it/s]Extractor Predicting: 240it [02:30,  1.61it/s]Extractor Predicting: 241it [02:31,  1.59it/s]Extractor Predicting: 242it [02:31,  1.59it/s]Extractor Predicting: 243it [02:32,  1.63it/s]Extractor Predicting: 244it [02:33,  1.65it/s]Extractor Predicting: 245it [02:33,  1.67it/s]Extractor Predicting: 246it [02:34,  1.71it/s]Extractor Predicting: 247it [02:34,  1.68it/s]Extractor Predicting: 248it [02:35,  1.67it/s]Extractor Predicting: 249it [02:36,  1.67it/s]Extractor Predicting: 250it [02:36,  1.68it/s]Extractor Predicting: 251it [02:37,  1.67it/s]Extractor Predicting: 252it [02:37,  1.69it/s]Extractor Predicting: 253it [02:38,  1.51it/s]Extractor Predicting: 254it [02:39,  1.54it/s]Extractor Predicting: 255it [02:39,  1.61it/s]Extractor Predicting: 256it [02:40,  1.62it/s]Extractor Predicting: 257it [02:41,  1.63it/s]Extractor Predicting: 258it [02:41,  1.64it/s]Extractor Predicting: 259it [02:42,  1.64it/s]Extractor Predicting: 260it [02:42,  1.62it/s]Extractor Predicting: 261it [02:43,  1.65it/s]Extractor Predicting: 262it [02:44,  1.67it/s]Extractor Predicting: 263it [02:44,  1.71it/s]Extractor Predicting: 264it [02:45,  1.66it/s]Extractor Predicting: 265it [02:45,  1.62it/s]Extractor Predicting: 266it [02:46,  1.59it/s]Extractor Predicting: 267it [02:47,  1.58it/s]Extractor Predicting: 268it [02:47,  1.61it/s]Extractor Predicting: 269it [02:48,  1.64it/s]Extractor Predicting: 270it [02:48,  1.63it/s]Extractor Predicting: 271it [02:49,  1.63it/s]Extractor Predicting: 272it [02:50,  1.62it/s]Extractor Predicting: 273it [02:50,  1.65it/s]Extractor Predicting: 274it [02:51,  1.65it/s]Extractor Predicting: 275it [02:51,  1.69it/s]Extractor Predicting: 276it [02:52,  1.66it/s]Extractor Predicting: 277it [02:53,  1.59it/s]Extractor Predicting: 278it [02:53,  1.63it/s]Extractor Predicting: 279it [02:54,  1.67it/s]Extractor Predicting: 280it [02:55,  1.68it/s]Extractor Predicting: 281it [02:55,  1.71it/s]Extractor Predicting: 282it [02:56,  1.71it/s]Extractor Predicting: 283it [02:56,  1.71it/s]Extractor Predicting: 284it [02:57,  1.69it/s]Extractor Predicting: 285it [02:57,  1.65it/s]Extractor Predicting: 286it [02:58,  1.70it/s]Extractor Predicting: 287it [02:59,  1.68it/s]Extractor Predicting: 288it [02:59,  1.68it/s]Extractor Predicting: 289it [03:00,  1.66it/s]Extractor Predicting: 290it [03:00,  1.69it/s]Extractor Predicting: 291it [03:01,  1.69it/s]Extractor Predicting: 292it [03:02,  1.65it/s]Extractor Predicting: 293it [03:02,  1.64it/s]Extractor Predicting: 294it [03:03,  1.63it/s]Extractor Predicting: 295it [03:04,  1.64it/s]Extractor Predicting: 296it [03:04,  1.65it/s]Extractor Predicting: 297it [03:05,  1.63it/s]Extractor Predicting: 298it [03:05,  1.67it/s]Extractor Predicting: 299it [03:06,  1.60it/s]Extractor Predicting: 300it [03:07,  1.57it/s]Extractor Predicting: 301it [03:07,  1.52it/s]Extractor Predicting: 302it [03:08,  1.54it/s]Extractor Predicting: 303it [03:09,  1.53it/s]Extractor Predicting: 304it [03:09,  1.53it/s]Extractor Predicting: 305it [03:10,  1.51it/s]Extractor Predicting: 306it [03:11,  1.51it/s]Extractor Predicting: 306it [03:11,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:13:18,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:13:18,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:13:18,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:13:18,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:13:18,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:13:19,228 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:13:19,229 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:13:19,935 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:13:21,148 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:13:21,148 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:13:24,353 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:13:24,444 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:13:24,444 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:13:24,444 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:13:24,444 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:13:25,680 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:13:25,682 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:13:26,475 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:13:26,859 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:13:26,859 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3169291338582677,
  "recall": 0.04384531590413943,
  "score": 0.07703349282296651,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'labels': ['continent', 'field of this occupation', 'field of work', 'founded by', 'movement', 'owned by', 'performer', 'producer', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6322
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6422, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.59it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:08,  1.55it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:10,  1.51it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.60it/s]Extractor Predicting: 25it [00:16,  1.62it/s]Extractor Predicting: 26it [00:16,  1.63it/s]Extractor Predicting: 27it [00:17,  1.62it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:19,  1.62it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:20,  1.62it/s]Extractor Predicting: 33it [00:20,  1.61it/s]Extractor Predicting: 34it [00:21,  1.62it/s]Extractor Predicting: 35it [00:22,  1.62it/s]Extractor Predicting: 36it [00:22,  1.66it/s]Extractor Predicting: 37it [00:23,  1.66it/s]Extractor Predicting: 38it [00:23,  1.67it/s]Extractor Predicting: 39it [00:24,  1.65it/s]Extractor Predicting: 40it [00:25,  1.61it/s]Extractor Predicting: 41it [00:25,  1.61it/s]Extractor Predicting: 42it [00:26,  1.58it/s]Extractor Predicting: 43it [00:27,  1.55it/s]Extractor Predicting: 44it [00:27,  1.56it/s]Extractor Predicting: 45it [00:28,  1.55it/s]Extractor Predicting: 46it [00:29,  1.41it/s]Extractor Predicting: 47it [00:29,  1.46it/s]Extractor Predicting: 48it [00:30,  1.51it/s]Extractor Predicting: 49it [00:31,  1.54it/s]Extractor Predicting: 50it [00:31,  1.54it/s]Extractor Predicting: 51it [00:32,  1.56it/s]Extractor Predicting: 52it [00:33,  1.54it/s]Extractor Predicting: 53it [00:33,  1.56it/s]Extractor Predicting: 54it [00:34,  1.56it/s]Extractor Predicting: 55it [00:35,  1.56it/s]Extractor Predicting: 56it [00:35,  1.57it/s]Extractor Predicting: 57it [00:36,  1.53it/s]Extractor Predicting: 58it [00:37,  1.53it/s]Extractor Predicting: 59it [00:37,  1.53it/s]Extractor Predicting: 60it [00:38,  1.33it/s]Extractor Predicting: 60it [00:38,  1.55it/s]
[INFO|configuration_utils.py:515] 2023-08-29 03:14:10,620 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:14:10,621 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 03:14:10,697 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:14:10,698 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 03:14:10,730 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 03:14:21,047 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 03:14:21,090 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 03:14:21,443 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:14:21,444 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 03:14:21,614 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:14:21,705 >> loading file outputs/wrapper/wiki/unseen_10_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:14:21,706 >> loading file outputs/wrapper/wiki/unseen_10_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:14:21,706 >> loading file outputs/wrapper/wiki/unseen_10_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:14:21,706 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:14:21,706 >> loading file outputs/wrapper/wiki/unseen_10_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:14:21,706 >> loading file outputs/wrapper/wiki/unseen_10_seed_3/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6537102473498233,
  "recall": 0.05765035836709255,
  "score": 0.10595647193585336,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 03:14:22,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:22,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:23,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:23,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:24,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:25,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:25,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:26,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:27,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:27,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:28,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:29,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:29,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:30,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:31,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:31,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:32,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:33,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:33,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:34,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:35,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:35,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:36,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:27, 14.85s/it][WARNING|generation_utils.py:914] 2023-08-29 03:14:37,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:37,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:38,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:39,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:39,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:40,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:41,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:41,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:42,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:43,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:43,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:44,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:45,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:46,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:46,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:47,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:48,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:48,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:49,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:50,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:51,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:51,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:52,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:53,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:31<03:27, 16.00s/it][WARNING|generation_utils.py:914] 2023-08-29 03:14:53,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:54,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:55,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:55,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:56,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:57,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:57,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:58,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:59,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:14:59,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:00,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:00,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:01,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:02,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:02,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:03,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:04,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:04,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:05,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:05,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:06,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:07,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:07,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:46<03:04, 15.39s/it][WARNING|generation_utils.py:914] 2023-08-29 03:15:08,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:09,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:09,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:10,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:11,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:11,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:12,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:13,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:13,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:14,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:15,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:15,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:16,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:17,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:18,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:18,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:19,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:20,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:20,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:21,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:21,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:00<02:43, 14.88s/it][WARNING|generation_utils.py:914] 2023-08-29 03:15:22,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:23,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:23,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:24,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:25,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:25,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:26,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:26,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:27,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:28,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:28,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:29,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:30,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:30,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:31,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:32,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:32,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:33,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:33,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:34,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:35,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:36,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:36,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:15<02:28, 14.83s/it][WARNING|generation_utils.py:914] 2023-08-29 03:15:37,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:37,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:38,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:39,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:39,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:40,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:41,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:41,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:42,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:43,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:43,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:44,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:45,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:45,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:46,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:46,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:47,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:48,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:48,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:49,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:50,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:50,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:51,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:29<02:12, 14.75s/it][WARNING|generation_utils.py:914] 2023-08-29 03:15:51,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:52,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:53,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:53,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:54,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:54,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:55,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:56,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:56,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:57,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:58,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:58,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:59,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:15:59,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:00,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:01,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:01,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:02,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:03,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:03,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:04,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:05,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:43<01:56, 14.51s/it][WARNING|generation_utils.py:914] 2023-08-29 03:16:05,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:06,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:07,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:08,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:08,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:09,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:10,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:10,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:11,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:12,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:12,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:13,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:14,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:15,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:16,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:16,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:17,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:18,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:18,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:19,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:19,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:20,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:58<01:43, 14.73s/it][WARNING|generation_utils.py:914] 2023-08-29 03:16:21,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:21,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:22,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:23,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:23,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:24,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:25,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:25,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:26,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:27,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:27,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:28,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:29,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:29,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:30,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:31,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:31,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:32,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:32,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:33,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:34,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:35,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:13<01:27, 14.64s/it][WARNING|generation_utils.py:914] 2023-08-29 03:16:35,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:36,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:36,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:37,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:38,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:38,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:39,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:40,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:40,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:41,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:42,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:43,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:43,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:44,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:44,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:45,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:46,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:46,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:47,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:48,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:48,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:49,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:50,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:50,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:29<01:15, 15.04s/it][WARNING|generation_utils.py:914] 2023-08-29 03:16:51,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:52,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:52,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:53,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:54,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:54,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:55,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:56,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:56,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:57,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:57,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:58,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:16:59,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:00,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:00,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:01,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:02,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:02,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:03,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:03,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:04,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:05,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:43<00:59, 14.86s/it][WARNING|generation_utils.py:914] 2023-08-29 03:17:06,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:06,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:07,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:08,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:08,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:09,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:09,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:10,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:11,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:11,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:12,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:13,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:13,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:14,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:15,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:15,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:16,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:17,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:17,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:18,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:56<00:43, 14.34s/it][WARNING|generation_utils.py:914] 2023-08-29 03:17:19,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:19,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:20,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:21,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:21,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:22,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:23,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:23,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:24,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:24,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:25,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:26,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:26,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:27,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:28,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:28,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:29,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:29,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:30,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:31,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:31,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:10<00:28, 14.07s/it][WARNING|generation_utils.py:914] 2023-08-29 03:17:32,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:33,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:33,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:34,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:35,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:35,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:36,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:37,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:38,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:38,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:39,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:39,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:40,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:41,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:42,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:42,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:43,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:44,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:44,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:45,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:45,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:46,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:47,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:26<00:14, 14.54s/it][WARNING|generation_utils.py:914] 2023-08-29 03:17:48,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:48,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:49,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:50,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:51,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:51,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:52,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:53,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:54,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:54,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:55,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:56,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:57,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:57,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:58,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:17:59,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:18:00,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:18:00,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:18:01,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:18:02,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:18:02,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:18:03,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:18:04,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:42<00:00, 15.22s/it]Generating: 100%|██████████| 15/15 [03:42<00:00, 14.85s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:11,438 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:11,467 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:11,467 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:11,467 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:11,467 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:18:12,327 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:18:12,328 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:18:12,618 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:18:13,789 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:18:13,790 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:15,646 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:15,667 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:15,668 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:15,668 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:18:15,668 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:18:16,543 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:18:16,545 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:18:17,282 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:18:17,487 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:18:17,487 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : country .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 377, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 626, 'raw': 768}
{'prompt': 'Relation : place of death .', 'success_rate': 0.8151041666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : production company .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.90625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : subsidiary . Context : Later in the year , the band members , including bassist Andy Warhol , recorded a session that had George Jones on drums for the first time . Head Entity : George Jones , Tail Entity : Bassist .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8410326086956522, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : continent . Context : The Tintaro ( κ T ) River connects the cities of Liguria and Tuscany , across the Arian Sea , the Aegean Sea , the western Mediterranean Sea and finally the Amazonia River basin . Head Entity : Aegean Sea , Tail Entity : the Atlantic .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : continent .', 'success_rate': 0.8220108695652174, 'errors': {'', "('Australia', 'continent', '', 'Nettie is also an Australia n actress who plays the role of a woman in a fictional Aboriginal television show that aired on RTH from 2001 to 2005 .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8607954545454546, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : founded by .', 'success_rate': 0.8607954545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 575, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : movement .', 'success_rate': 0.8111979166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Heinrich Himmler', 'movement', '', 'On February 7 , 1914 , he joined the German General Staff under Heinrich Himmler during the attack on Pearl Harbor on April 20 , 1941 .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8536931818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.9453125, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : producer .', 'success_rate': 0.9270833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : record label . Context : Later in 2008 , he recorded his debut single , The Day All My Friends Turned Yellow , on the New York City Sound System label , alongside members of the label , David Bop on bass , and Chris Mathews . Head Entity : The Day All My Friends Turned Yellow , Tail Entity : NY Sound System .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 615, 'raw': 736}
{'prompt': 'Relation : record label .', 'success_rate': 0.8355978260869565, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 330, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 383, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : replaces .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/1_ext.jsonl'}}
estimate vocab size: 13458
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13558, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.62it/s]Extractor Estimating: 2it [00:01,  1.63it/s]Extractor Estimating: 3it [00:01,  1.57it/s]Extractor Estimating: 4it [00:02,  1.61it/s]Extractor Estimating: 5it [00:03,  1.62it/s]Extractor Estimating: 6it [00:03,  1.67it/s]Extractor Estimating: 7it [00:04,  1.62it/s]Extractor Estimating: 8it [00:04,  1.65it/s]Extractor Estimating: 9it [00:05,  1.64it/s]Extractor Estimating: 10it [00:06,  1.59it/s]Extractor Estimating: 11it [00:06,  1.58it/s]Extractor Estimating: 12it [00:07,  1.60it/s]Extractor Estimating: 13it [00:08,  1.52it/s]Extractor Estimating: 14it [00:08,  1.55it/s]Extractor Estimating: 15it [00:09,  1.57it/s]Extractor Estimating: 16it [00:09,  1.62it/s]Extractor Estimating: 17it [00:10,  1.65it/s]Extractor Estimating: 18it [00:11,  1.66it/s]Extractor Estimating: 19it [00:11,  1.64it/s]Extractor Estimating: 20it [00:12,  1.61it/s]Extractor Estimating: 21it [00:13,  1.64it/s]Extractor Estimating: 22it [00:13,  1.62it/s]Extractor Estimating: 23it [00:14,  1.65it/s]Extractor Estimating: 24it [00:14,  1.66it/s]Extractor Estimating: 25it [00:15,  1.65it/s]Extractor Estimating: 26it [00:16,  1.62it/s]Extractor Estimating: 27it [00:16,  1.60it/s]Extractor Estimating: 28it [00:17,  1.58it/s]Extractor Estimating: 29it [00:18,  1.54it/s]Extractor Estimating: 30it [00:18,  1.50it/s]Extractor Estimating: 31it [00:19,  1.53it/s]Extractor Estimating: 32it [00:20,  1.53it/s]Extractor Estimating: 33it [00:20,  1.50it/s]Extractor Estimating: 34it [00:21,  1.50it/s]Extractor Estimating: 35it [00:22,  1.53it/s]Extractor Estimating: 36it [00:22,  1.49it/s]Extractor Estimating: 37it [00:23,  1.54it/s]Extractor Estimating: 38it [00:23,  1.57it/s]Extractor Estimating: 39it [00:24,  1.51it/s]Extractor Estimating: 40it [00:25,  1.53it/s]Extractor Estimating: 41it [00:25,  1.53it/s]Extractor Estimating: 42it [00:26,  1.55it/s]Extractor Estimating: 43it [00:27,  1.52it/s]Extractor Estimating: 44it [00:27,  1.55it/s]Extractor Estimating: 45it [00:28,  1.56it/s]Extractor Estimating: 46it [00:29,  1.54it/s]Extractor Estimating: 47it [00:29,  1.52it/s]Extractor Estimating: 48it [00:30,  1.50it/s]Extractor Estimating: 49it [00:31,  1.49it/s]Extractor Estimating: 50it [00:31,  1.49it/s]Extractor Estimating: 51it [00:32,  1.54it/s]Extractor Estimating: 52it [00:33,  1.52it/s]Extractor Estimating: 53it [00:33,  1.55it/s]Extractor Estimating: 54it [00:34,  1.57it/s]Extractor Estimating: 55it [00:35,  1.59it/s]Extractor Estimating: 56it [00:35,  1.56it/s]Extractor Estimating: 57it [00:36,  1.53it/s]Extractor Estimating: 58it [00:37,  1.51it/s]Extractor Estimating: 59it [00:37,  1.54it/s]Extractor Estimating: 60it [00:38,  1.59it/s]Extractor Estimating: 61it [00:38,  1.62it/s]Extractor Estimating: 62it [00:39,  1.59it/s]Extractor Estimating: 63it [00:40,  1.57it/s]Extractor Estimating: 64it [00:40,  1.57it/s]Extractor Estimating: 65it [00:41,  1.60it/s]Extractor Estimating: 66it [00:42,  1.60it/s]Extractor Estimating: 67it [00:42,  1.57it/s]Extractor Estimating: 68it [00:43,  1.52it/s]Extractor Estimating: 69it [00:44,  1.52it/s]Extractor Estimating: 70it [00:44,  1.55it/s]Extractor Estimating: 71it [00:45,  1.51it/s]Extractor Estimating: 72it [00:46,  1.49it/s]Extractor Estimating: 73it [00:46,  1.49it/s]Extractor Estimating: 74it [00:47,  1.46it/s]Extractor Estimating: 75it [00:48,  1.49it/s]Extractor Estimating: 76it [00:48,  1.53it/s]Extractor Estimating: 77it [00:49,  1.51it/s]Extractor Estimating: 78it [00:50,  1.52it/s]Extractor Estimating: 79it [00:50,  1.56it/s]Extractor Estimating: 80it [00:51,  1.52it/s]Extractor Estimating: 81it [00:51,  1.54it/s]Extractor Estimating: 82it [00:52,  1.37it/s]Extractor Estimating: 83it [00:53,  1.41it/s]Extractor Estimating: 84it [00:54,  1.39it/s]Extractor Estimating: 85it [00:54,  1.47it/s]Extractor Estimating: 86it [00:55,  1.48it/s]Extractor Estimating: 87it [00:56,  1.40it/s]Extractor Estimating: 88it [00:56,  1.43it/s]Extractor Estimating: 89it [00:57,  1.45it/s]Extractor Estimating: 90it [00:58,  1.43it/s]Extractor Estimating: 91it [00:58,  1.50it/s]Extractor Estimating: 92it [00:59,  1.44it/s]Extractor Estimating: 93it [01:00,  1.45it/s]Extractor Estimating: 94it [01:01,  1.46it/s]Extractor Estimating: 95it [01:01,  1.51it/s]Extractor Estimating: 96it [01:02,  1.51it/s]Extractor Estimating: 97it [01:03,  1.51it/s]Extractor Estimating: 98it [01:03,  1.55it/s]Extractor Estimating: 99it [01:04,  1.53it/s]Extractor Estimating: 100it [01:04,  1.50it/s]Extractor Estimating: 101it [01:05,  1.55it/s]Extractor Estimating: 102it [01:06,  1.59it/s]Extractor Estimating: 103it [01:06,  1.54it/s]Extractor Estimating: 104it [01:07,  1.51it/s]Extractor Estimating: 105it [01:08,  1.49it/s]Extractor Estimating: 106it [01:08,  1.54it/s]Extractor Estimating: 107it [01:09,  1.55it/s]Extractor Estimating: 108it [01:10,  1.55it/s]Extractor Estimating: 109it [01:10,  1.56it/s]Extractor Estimating: 110it [01:11,  1.65it/s]Extractor Estimating: 111it [01:11,  1.66it/s]Extractor Estimating: 112it [01:12,  1.64it/s]Extractor Estimating: 113it [01:13,  1.56it/s]Extractor Estimating: 114it [01:13,  1.57it/s]Extractor Estimating: 115it [01:14,  1.60it/s]Extractor Estimating: 116it [01:15,  1.60it/s]Extractor Estimating: 117it [01:15,  1.55it/s]Extractor Estimating: 118it [01:16,  1.45it/s]Extractor Estimating: 119it [01:17,  1.50it/s]Extractor Estimating: 120it [01:17,  1.52it/s]Extractor Estimating: 121it [01:18,  1.54it/s]Extractor Estimating: 122it [01:19,  1.52it/s]Extractor Estimating: 123it [01:19,  1.52it/s]Extractor Estimating: 124it [01:20,  1.52it/s]Extractor Estimating: 125it [01:21,  1.52it/s]Extractor Estimating: 126it [01:21,  1.62it/s]Extractor Estimating: 127it [01:22,  1.59it/s]Extractor Estimating: 128it [01:22,  1.57it/s]Extractor Estimating: 129it [01:23,  1.60it/s]Extractor Estimating: 130it [01:24,  1.56it/s]Extractor Estimating: 131it [01:24,  1.52it/s]Extractor Estimating: 132it [01:25,  1.59it/s]Extractor Estimating: 133it [01:26,  1.60it/s]Extractor Estimating: 134it [01:26,  1.55it/s]Extractor Estimating: 135it [01:27,  1.58it/s]Extractor Estimating: 136it [01:28,  1.57it/s]Extractor Estimating: 137it [01:28,  1.62it/s]Extractor Estimating: 138it [01:29,  1.65it/s]Extractor Estimating: 139it [01:29,  1.58it/s]Extractor Estimating: 140it [01:30,  1.59it/s]Extractor Estimating: 141it [01:31,  1.66it/s]Extractor Estimating: 142it [01:31,  1.67it/s]Extractor Estimating: 143it [01:32,  1.66it/s]Extractor Estimating: 144it [01:32,  1.64it/s]Extractor Estimating: 145it [01:33,  1.61it/s]Extractor Estimating: 146it [01:34,  1.66it/s]Extractor Estimating: 147it [01:34,  1.62it/s]Extractor Estimating: 148it [01:35,  1.66it/s]Extractor Estimating: 149it [01:35,  1.67it/s]Extractor Estimating: 150it [01:36,  1.61it/s]Extractor Estimating: 151it [01:37,  1.65it/s]Extractor Estimating: 152it [01:37,  1.47it/s]Extractor Estimating: 153it [01:38,  1.53it/s]Extractor Estimating: 154it [01:39,  1.58it/s]Extractor Estimating: 155it [01:39,  1.59it/s]Extractor Estimating: 156it [01:40,  1.63it/s]Extractor Estimating: 157it [01:40,  1.62it/s]Extractor Estimating: 158it [01:41,  1.63it/s]Extractor Estimating: 159it [01:42,  1.63it/s]Extractor Estimating: 160it [01:42,  1.65it/s]Extractor Estimating: 161it [01:43,  1.64it/s]Extractor Estimating: 162it [01:44,  1.58it/s]Extractor Estimating: 163it [01:44,  1.63it/s]Extractor Estimating: 164it [01:45,  1.66it/s]Extractor Estimating: 165it [01:45,  1.64it/s]Extractor Estimating: 166it [01:46,  1.66it/s]Extractor Estimating: 167it [01:47,  1.63it/s]Extractor Estimating: 168it [01:47,  1.63it/s]Extractor Estimating: 169it [01:48,  1.63it/s]Extractor Estimating: 170it [01:48,  1.60it/s]Extractor Estimating: 171it [01:49,  1.59it/s]Extractor Estimating: 172it [01:50,  1.53it/s]Extractor Estimating: 173it [01:50,  1.58it/s]Extractor Estimating: 174it [01:51,  1.53it/s]Extractor Estimating: 175it [01:52,  1.54it/s]Extractor Estimating: 176it [01:52,  1.51it/s]Extractor Estimating: 177it [01:53,  1.47it/s]Extractor Estimating: 178it [01:54,  1.48it/s]Extractor Estimating: 179it [01:54,  1.50it/s]Extractor Estimating: 180it [01:55,  1.54it/s]Extractor Estimating: 181it [01:56,  1.57it/s]Extractor Estimating: 182it [01:56,  1.58it/s]Extractor Estimating: 183it [01:57,  1.59it/s]Extractor Estimating: 184it [01:58,  1.59it/s]Extractor Estimating: 185it [01:58,  1.51it/s]Extractor Estimating: 186it [01:59,  1.50it/s]Extractor Estimating: 187it [02:00,  1.51it/s]Extractor Estimating: 188it [02:00,  1.56it/s]Extractor Estimating: 189it [02:01,  1.51it/s]Extractor Estimating: 190it [02:02,  1.56it/s]Extractor Estimating: 191it [02:02,  1.53it/s]Extractor Estimating: 192it [02:03,  1.55it/s]Extractor Estimating: 193it [02:03,  1.54it/s]Extractor Estimating: 194it [02:04,  1.54it/s]Extractor Estimating: 195it [02:05,  1.62it/s]Extractor Estimating: 196it [02:05,  1.62it/s]Extractor Estimating: 197it [02:06,  1.61it/s]Extractor Estimating: 198it [02:07,  1.56it/s]Extractor Estimating: 199it [02:07,  1.60it/s]Extractor Estimating: 200it [02:08,  1.57it/s]Extractor Estimating: 201it [02:09,  1.56it/s]Extractor Estimating: 202it [02:09,  1.54it/s]Extractor Estimating: 203it [02:10,  1.54it/s]Extractor Estimating: 204it [02:10,  1.56it/s]Extractor Estimating: 205it [02:11,  1.56it/s]Extractor Estimating: 206it [02:12,  1.57it/s]Extractor Estimating: 207it [02:12,  1.57it/s]Extractor Estimating: 208it [02:13,  1.58it/s]Extractor Estimating: 209it [02:14,  1.56it/s]Extractor Estimating: 210it [02:14,  1.48it/s]Extractor Estimating: 211it [02:15,  1.54it/s]Extractor Estimating: 212it [02:16,  1.59it/s]Extractor Estimating: 213it [02:16,  1.57it/s]Extractor Estimating: 214it [02:17,  1.57it/s]Extractor Estimating: 215it [02:17,  1.56it/s]Extractor Estimating: 216it [02:18,  1.55it/s]Extractor Estimating: 217it [02:19,  1.52it/s]Extractor Estimating: 218it [02:19,  1.56it/s]Extractor Estimating: 219it [02:20,  1.58it/s]Extractor Estimating: 220it [02:21,  1.47it/s]Extractor Estimating: 221it [02:21,  1.52it/s]Extractor Estimating: 222it [02:22,  1.54it/s]Extractor Estimating: 223it [02:23,  1.56it/s]Extractor Estimating: 224it [02:23,  1.57it/s]Extractor Estimating: 225it [02:24,  1.57it/s]Extractor Estimating: 226it [02:25,  1.55it/s]Extractor Estimating: 227it [02:25,  1.54it/s]Extractor Estimating: 228it [02:26,  1.56it/s]Extractor Estimating: 229it [02:27,  1.60it/s]Extractor Estimating: 230it [02:27,  1.53it/s]Extractor Estimating: 231it [02:28,  1.56it/s]Extractor Estimating: 232it [02:29,  1.52it/s]Extractor Estimating: 233it [02:29,  1.58it/s]Extractor Estimating: 234it [02:30,  1.57it/s]Extractor Estimating: 235it [02:30,  1.57it/s]Extractor Estimating: 236it [02:31,  1.54it/s]Extractor Estimating: 237it [02:32,  1.55it/s]Extractor Estimating: 238it [02:32,  1.56it/s]Extractor Estimating: 239it [02:33,  1.56it/s]Extractor Estimating: 240it [02:34,  1.56it/s]Extractor Estimating: 241it [02:34,  1.60it/s]Extractor Estimating: 242it [02:35,  1.63it/s]Extractor Estimating: 243it [02:35,  1.62it/s]Extractor Estimating: 244it [02:36,  1.63it/s]Extractor Estimating: 245it [02:37,  1.58it/s]Extractor Estimating: 246it [02:37,  1.58it/s]Extractor Estimating: 247it [02:38,  1.59it/s]Extractor Estimating: 248it [02:39,  1.55it/s]Extractor Estimating: 249it [02:39,  1.55it/s]Extractor Estimating: 250it [02:40,  1.53it/s]Extractor Estimating: 251it [02:41,  1.53it/s]Extractor Estimating: 252it [02:41,  1.58it/s]Extractor Estimating: 253it [02:42,  1.56it/s]Extractor Estimating: 254it [02:42,  1.58it/s]Extractor Estimating: 255it [02:43,  1.57it/s]Extractor Estimating: 256it [02:44,  1.56it/s]Extractor Estimating: 257it [02:44,  1.57it/s]Extractor Estimating: 258it [02:45,  1.59it/s]Extractor Estimating: 259it [02:46,  1.55it/s]Extractor Estimating: 260it [02:46,  1.55it/s]Extractor Estimating: 261it [02:47,  1.63it/s]Extractor Estimating: 262it [02:47,  1.62it/s]Extractor Estimating: 263it [02:48,  1.58it/s]Extractor Estimating: 264it [02:49,  1.56it/s]Extractor Estimating: 265it [02:49,  1.53it/s]Extractor Estimating: 266it [02:50,  1.54it/s]Extractor Estimating: 267it [02:51,  1.59it/s]Extractor Estimating: 268it [02:51,  1.58it/s]Extractor Estimating: 269it [02:52,  1.58it/s]Extractor Estimating: 270it [02:53,  1.60it/s]Extractor Estimating: 271it [02:53,  1.58it/s]Extractor Estimating: 272it [02:54,  1.59it/s]Extractor Estimating: 273it [02:55,  1.55it/s]Extractor Estimating: 274it [02:55,  1.62it/s]Extractor Estimating: 275it [02:56,  1.63it/s]Extractor Estimating: 276it [02:56,  1.64it/s]Extractor Estimating: 277it [02:57,  1.61it/s]Extractor Estimating: 278it [02:58,  1.45it/s]Extractor Estimating: 279it [02:58,  1.48it/s]Extractor Estimating: 280it [02:59,  1.52it/s]Extractor Estimating: 281it [03:00,  1.53it/s]Extractor Estimating: 282it [03:00,  1.53it/s]Extractor Estimating: 283it [03:01,  1.52it/s]Extractor Estimating: 284it [03:02,  1.56it/s]Extractor Estimating: 285it [03:02,  1.59it/s]Extractor Estimating: 286it [03:03,  1.55it/s]Extractor Estimating: 287it [03:04,  1.57it/s]Extractor Estimating: 288it [03:04,  1.55it/s]Extractor Estimating: 289it [03:05,  1.50it/s]Extractor Estimating: 290it [03:06,  1.38it/s]Extractor Estimating: 291it [03:06,  1.48it/s]Extractor Estimating: 292it [03:07,  1.53it/s]Extractor Estimating: 293it [03:08,  1.52it/s]Extractor Estimating: 294it [03:08,  1.52it/s]Extractor Estimating: 295it [03:09,  1.56it/s]Extractor Estimating: 296it [03:10,  1.45it/s]Extractor Estimating: 297it [03:10,  1.45it/s]Extractor Estimating: 298it [03:11,  1.45it/s]Extractor Estimating: 299it [03:12,  1.53it/s]Extractor Estimating: 300it [03:12,  1.48it/s]Extractor Estimating: 301it [03:13,  1.43it/s]Extractor Estimating: 302it [03:14,  1.45it/s]Extractor Estimating: 303it [03:14,  1.47it/s]Extractor Estimating: 304it [03:15,  1.49it/s]Extractor Estimating: 305it [03:16,  1.52it/s]Extractor Estimating: 306it [03:16,  1.50it/s]Extractor Estimating: 307it [03:17,  1.54it/s]Extractor Estimating: 308it [03:18,  1.55it/s]Extractor Estimating: 309it [03:18,  1.52it/s]Extractor Estimating: 310it [03:19,  1.56it/s]Extractor Estimating: 311it [03:20,  1.55it/s]Extractor Estimating: 312it [03:20,  1.52it/s]Extractor Estimating: 313it [03:21,  1.51it/s]Extractor Estimating: 314it [03:22,  1.54it/s]Extractor Estimating: 315it [03:22,  1.56it/s]Extractor Estimating: 316it [03:23,  1.56it/s]Extractor Estimating: 317it [03:23,  1.58it/s]Extractor Estimating: 318it [03:24,  1.57it/s]Extractor Estimating: 319it [03:25,  1.58it/s]Extractor Estimating: 320it [03:25,  1.60it/s]Extractor Estimating: 321it [03:26,  1.57it/s]Extractor Estimating: 322it [03:27,  1.52it/s]Extractor Estimating: 323it [03:27,  1.53it/s]Extractor Estimating: 324it [03:28,  1.57it/s]Extractor Estimating: 325it [03:29,  1.49it/s]Extractor Estimating: 326it [03:29,  1.44it/s]Extractor Estimating: 327it [03:30,  1.45it/s]Extractor Estimating: 328it [03:31,  1.49it/s]Extractor Estimating: 329it [03:31,  1.50it/s]Extractor Estimating: 330it [03:32,  1.46it/s]Extractor Estimating: 331it [03:33,  1.42it/s]Extractor Estimating: 332it [03:33,  1.47it/s]Extractor Estimating: 333it [03:34,  1.41it/s]Extractor Estimating: 334it [03:35,  1.42it/s]Extractor Estimating: 335it [03:36,  1.44it/s]Extractor Estimating: 336it [03:36,  1.43it/s]Extractor Estimating: 337it [03:37,  1.43it/s]Extractor Estimating: 338it [03:38,  1.48it/s]Extractor Estimating: 339it [03:38,  1.46it/s]Extractor Estimating: 340it [03:39,  1.41it/s]Extractor Estimating: 341it [03:40,  1.40it/s]Extractor Estimating: 342it [03:40,  1.45it/s]Extractor Estimating: 343it [03:41,  1.43it/s]Extractor Estimating: 344it [03:42,  1.48it/s]Extractor Estimating: 345it [03:42,  1.55it/s]Extractor Estimating: 346it [03:43,  1.49it/s]Extractor Estimating: 347it [03:44,  1.47it/s]Extractor Estimating: 348it [03:45,  1.43it/s]Extractor Estimating: 349it [03:45,  1.38it/s]Extractor Estimating: 350it [03:46,  1.36it/s]Extractor Estimating: 351it [03:47,  1.40it/s]Extractor Estimating: 352it [03:47,  1.46it/s]Extractor Estimating: 353it [03:48,  1.49it/s]Extractor Estimating: 354it [03:49,  1.47it/s]Extractor Estimating: 355it [03:49,  1.46it/s]Extractor Estimating: 356it [03:50,  1.46it/s]Extractor Estimating: 357it [03:51,  1.46it/s]Extractor Estimating: 358it [03:52,  1.46it/s]Extractor Estimating: 359it [03:52,  1.44it/s]Extractor Estimating: 360it [03:53,  1.33it/s]Extractor Estimating: 361it [03:54,  1.37it/s]Extractor Estimating: 362it [03:54,  1.41it/s]Extractor Estimating: 363it [03:55,  1.45it/s]Extractor Estimating: 364it [03:56,  1.45it/s]Extractor Estimating: 365it [03:56,  1.48it/s]Extractor Estimating: 366it [03:57,  1.48it/s]Extractor Estimating: 367it [03:58,  1.52it/s]Extractor Estimating: 368it [03:58,  1.52it/s]Extractor Estimating: 369it [03:59,  1.53it/s]Extractor Estimating: 370it [04:00,  1.54it/s]Extractor Estimating: 371it [04:00,  1.48it/s]Extractor Estimating: 372it [04:01,  1.47it/s]Extractor Estimating: 373it [04:02,  1.50it/s]Extractor Estimating: 374it [04:02,  1.45it/s]Extractor Estimating: 375it [04:03,  1.81it/s]Extractor Estimating: 375it [04:03,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:22:45,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:22:45,109 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:22:45,109 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:22:45,109 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:22:45,109 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:22:45,593 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:22:45,594 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:22:45,912 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:22:47,088 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:22:47,088 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:22:48,984 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:22:49,006 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:22:49,006 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:22:49,007 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:22:49,007 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:22:49,860 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:22:49,862 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:22:50,160 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:22:50,381 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:22:50,381 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 06:01:12,853 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 06:01:13,287 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7427 mean pseudo reward: 0.9341980393682164
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl'}
train vocab size: 31177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=31277, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.024, loss:835.9284
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.022, loss:824.0124
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.021, loss:823.2511
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 90, avg_time 1.024, loss:748.9662
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 190, avg_time 1.020, loss:752.5507
>> valid entity prec:0.5114, rec:0.3740, f1:0.4321
>> valid relation prec:0.2191, rec:0.0443, f1:0.0737
>> valid relation with NER prec:0.2191, rec:0.0443, f1:0.0737
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 290, avg_time 3.672, loss:772.0951
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 80, avg_time 1.009, loss:719.4851
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 180, avg_time 1.024, loss:766.6794
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 280, avg_time 1.023, loss:780.0418
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 70, avg_time 1.014, loss:737.3878
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4868, rec:0.3994, f1:0.4388
>> valid relation prec:0.1574, rec:0.0260, f1:0.0446
>> valid relation with NER prec:0.1574, rec:0.0260, f1:0.0446
new max entity f1 on valid!
g_step 1100, step 170, avg_time 3.659, loss:741.8276
g_step 1200, step 270, avg_time 1.016, loss:783.6268
g_step 1300, step 60, avg_time 1.006, loss:730.4866
g_step 1400, step 160, avg_time 1.014, loss:734.2255
g_step 1500, step 260, avg_time 1.014, loss:746.2301
>> valid entity prec:0.5199, rec:0.4234, f1:0.4667
>> valid relation prec:0.1947, rec:0.0337, f1:0.0574
>> valid relation with NER prec:0.1947, rec:0.0337, f1:0.0574
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 50, avg_time 3.693, loss:688.1552
g_step 1700, step 150, avg_time 1.016, loss:689.9157
g_step 1800, step 250, avg_time 1.031, loss:727.3694
g_step 1900, step 40, avg_time 1.020, loss:688.4291
g_step 2000, step 140, avg_time 1.006, loss:655.2668
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4999, rec:0.4177, f1:0.4551
>> valid relation prec:0.1469, rec:0.0310, f1:0.0512
>> valid relation with NER prec:0.1469, rec:0.0310, f1:0.0512
g_step 2100, step 240, avg_time 3.645, loss:676.2029
g_step 2200, step 30, avg_time 1.017, loss:682.2663
g_step 2300, step 130, avg_time 1.017, loss:617.6746
g_step 2400, step 230, avg_time 1.016, loss:634.5524
g_step 2500, step 20, avg_time 1.008, loss:638.3841
>> valid entity prec:0.4905, rec:0.4790, f1:0.4847
>> valid relation prec:0.1417, rec:0.0240, f1:0.0410
>> valid relation with NER prec:0.1417, rec:0.0240, f1:0.0410
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 120, avg_time 3.685, loss:595.1745
g_step 2700, step 220, avg_time 1.022, loss:621.6102
g_step 2800, step 10, avg_time 1.003, loss:633.9044
g_step 2900, step 110, avg_time 1.021, loss:570.3157
g_step 3000, step 210, avg_time 1.029, loss:581.1628
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5223, rec:0.4273, f1:0.4701
>> valid relation prec:0.1473, rec:0.0295, f1:0.0491
>> valid relation with NER prec:0.1473, rec:0.0295, f1:0.0491
g_step 3100, step 310, avg_time 3.653, loss:623.1518
g_step 3200, step 100, avg_time 1.025, loss:548.5067
g_step 3300, step 200, avg_time 1.033, loss:556.3044
g_step 3400, step 300, avg_time 1.020, loss:606.9122
g_step 3500, step 90, avg_time 1.009, loss:505.4342
>> valid entity prec:0.5295, rec:0.4553, f1:0.4896
>> valid relation prec:0.1653, rec:0.0469, f1:0.0731
>> valid relation with NER prec:0.1653, rec:0.0469, f1:0.0731
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 190, avg_time 3.663, loss:566.2113
g_step 3700, step 290, avg_time 1.032, loss:560.8863
g_step 3800, step 80, avg_time 1.010, loss:519.2522
g_step 3900, step 180, avg_time 1.016, loss:526.8739
g_step 4000, step 280, avg_time 1.030, loss:565.5018
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5076, rec:0.4321, f1:0.4668
>> valid relation prec:0.1350, rec:0.0382, f1:0.0596
>> valid relation with NER prec:0.1350, rec:0.0382, f1:0.0596
g_step 4100, step 70, avg_time 3.663, loss:496.6515
g_step 4200, step 170, avg_time 1.029, loss:498.2739
g_step 4300, step 270, avg_time 1.016, loss:532.5846
g_step 4400, step 60, avg_time 1.013, loss:488.5845
g_step 4500, step 160, avg_time 1.016, loss:493.5848
>> valid entity prec:0.5059, rec:0.4208, f1:0.4595
>> valid relation prec:0.1421, rec:0.0405, f1:0.0630
>> valid relation with NER prec:0.1421, rec:0.0405, f1:0.0630
g_step 4600, step 260, avg_time 3.662, loss:501.9554
g_step 4700, step 50, avg_time 1.030, loss:477.2727
g_step 4800, step 150, avg_time 1.023, loss:467.3316
g_step 4900, step 250, avg_time 1.014, loss:520.8814
g_step 5000, step 40, avg_time 1.017, loss:466.1775
learning rate was adjusted to 0.0008
>> valid entity prec:0.4785, rec:0.4972, f1:0.4877
>> valid relation prec:0.1202, rec:0.0367, f1:0.0562
>> valid relation with NER prec:0.1202, rec:0.0367, f1:0.0562
g_step 5100, step 140, avg_time 3.670, loss:452.2895
g_step 5200, step 240, avg_time 1.019, loss:456.4082
g_step 5300, step 30, avg_time 1.033, loss:456.1066
g_step 5400, step 130, avg_time 1.026, loss:438.9652
g_step 5500, step 230, avg_time 1.024, loss:455.0307
>> valid entity prec:0.5228, rec:0.4085, f1:0.4587
>> valid relation prec:0.1385, rec:0.0410, f1:0.0633
>> valid relation with NER prec:0.1385, rec:0.0410, f1:0.0633
g_step 5600, step 20, avg_time 3.651, loss:453.8301
g_step 5700, step 120, avg_time 1.024, loss:409.9846
g_step 5800, step 220, avg_time 1.022, loss:434.4876
g_step 5900, step 10, avg_time 1.014, loss:437.2299
g_step 6000, step 110, avg_time 1.030, loss:403.7397
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4966, rec:0.4837, f1:0.4901
>> valid relation prec:0.1297, rec:0.0370, f1:0.0575
>> valid relation with NER prec:0.1297, rec:0.0370, f1:0.0575
new max entity f1 on valid!
g_step 6100, step 210, avg_time 3.648, loss:426.1283
g_step 6200, step 310, avg_time 1.020, loss:440.3494
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 06:01:13 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 06:01:13 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_06-01-12_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 06:01:14 - WARNING - datasets.builder -   Using custom data configuration default-e17fb8d3b1f59923
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-e17fb8d3b1f59923/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 06:01:17,812 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:01:17,813 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 06:01:17,814 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:01:17,815 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 06:01:17,961 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:01:18,020 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:01:18,020 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:01:18,020 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:01:18,020 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:01:18,020 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:01:18,020 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 06:01:18,363 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 06:01:21,466 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 06:01:21,476 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-e17fb8d3b1f59923/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.17ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.94ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.25ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.38ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.45ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.51ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.53ba/s]100%|██████████| 8/8 [00:01<00:00,  5.36ba/s]100%|██████████| 8/8 [00:01<00:00,  4.64ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:01,  4.15ba/s] 22%|██▏       | 2/9 [00:00<00:01,  4.42ba/s] 33%|███▎      | 3/9 [00:00<00:01,  4.54ba/s] 44%|████▍     | 4/9 [00:00<00:01,  4.60ba/s] 56%|█████▌    | 5/9 [00:01<00:00,  4.58ba/s] 67%|██████▋   | 6/9 [00:01<00:00,  3.72ba/s] 78%|███████▊  | 7/9 [00:01<00:00,  3.96ba/s] 89%|████████▉ | 8/9 [00:01<00:00,  4.13ba/s]100%|██████████| 9/9 [00:02<00:00,  4.85ba/s]100%|██████████| 9/9 [00:02<00:00,  4.42ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.26ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.47ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.72ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.77ba/s]100%|██████████| 8/8 [00:00<00:00, 11.36ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:00,  8.90ba/s] 33%|███▎      | 3/9 [00:00<00:00, 10.27ba/s] 56%|█████▌    | 5/9 [00:00<00:00, 10.54ba/s] 78%|███████▊  | 7/9 [00:00<00:00, 10.56ba/s]100%|██████████| 9/9 [00:00<00:00, 11.57ba/s]100%|██████████| 9/9 [00:00<00:00, 11.03ba/s]
[INFO|trainer.py:414] 2023-08-29 06:01:27,198 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 06:01:27,213 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 06:01:27,213 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 06:01:27,213 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 06:01:27,213 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 06:01:27,213 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 06:01:27,213 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 06:01:27,213 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:53,  3.36it/s]  0%|          | 2/585 [00:00<02:50,  3.42it/s]  1%|          | 3/585 [00:00<02:49,  3.43it/s]  1%|          | 4/585 [00:01<02:48,  3.45it/s]  1%|          | 5/585 [00:01<02:47,  3.46it/s]  1%|          | 6/585 [00:01<02:47,  3.46it/s]  1%|          | 7/585 [00:02<02:46,  3.47it/s]  1%|▏         | 8/585 [00:02<02:46,  3.46it/s]  2%|▏         | 9/585 [00:02<02:47,  3.45it/s]  2%|▏         | 10/585 [00:02<02:47,  3.43it/s]  2%|▏         | 11/585 [00:03<02:47,  3.43it/s]  2%|▏         | 12/585 [00:03<02:47,  3.42it/s]  2%|▏         | 13/585 [00:03<02:47,  3.42it/s]  2%|▏         | 14/585 [00:04<02:47,  3.41it/s]  3%|▎         | 15/585 [00:04<02:46,  3.42it/s]  3%|▎         | 16/585 [00:04<02:45,  3.43it/s]  3%|▎         | 17/585 [00:04<02:45,  3.44it/s]  3%|▎         | 18/585 [00:05<02:44,  3.45it/s]  3%|▎         | 19/585 [00:05<02:43,  3.45it/s]  3%|▎         | 20/585 [00:05<02:43,  3.46it/s]  4%|▎         | 21/585 [00:06<02:42,  3.46it/s]  4%|▍         | 22/585 [00:06<02:42,  3.47it/s]  4%|▍         | 23/585 [00:06<02:42,  3.47it/s]  4%|▍         | 24/585 [00:06<02:41,  3.47it/s]  4%|▍         | 25/585 [00:07<02:41,  3.47it/s]  4%|▍         | 26/585 [00:07<02:40,  3.48it/s]  5%|▍         | 27/585 [00:07<02:40,  3.48it/s]  5%|▍         | 28/585 [00:08<02:39,  3.48it/s]  5%|▍         | 29/585 [00:08<02:39,  3.48it/s]  5%|▌         | 30/585 [00:08<02:39,  3.48it/s]  5%|▌         | 31/585 [00:08<02:39,  3.47it/s]  5%|▌         | 32/585 [00:09<02:39,  3.48it/s]  6%|▌         | 33/585 [00:09<02:38,  3.48it/s]  6%|▌         | 34/585 [00:09<02:38,  3.47it/s]  6%|▌         | 35/585 [00:10<02:38,  3.48it/s]  6%|▌         | 36/585 [00:10<02:38,  3.45it/s]  6%|▋         | 37/585 [00:10<02:38,  3.45it/s]  6%|▋         | 38/585 [00:10<02:38,  3.46it/s]  7%|▋         | 39/585 [00:11<02:37,  3.46it/s]  7%|▋         | 40/585 [00:11<02:37,  3.45it/s]  7%|▋         | 41/585 [00:11<02:37,  3.45it/s]  7%|▋         | 42/585 [00:12<02:37,  3.45it/s]  7%|▋         | 43/585 [00:12<02:37,  3.45it/s]  8%|▊         | 44/585 [00:12<02:36,  3.45it/s]  8%|▊         | 45/585 [00:13<02:36,  3.45it/s]  8%|▊         | 46/585 [00:13<02:36,  3.45it/s]  8%|▊         | 47/585 [00:13<02:37,  3.42it/s]  8%|▊         | 48/585 [00:13<02:36,  3.43it/s]  8%|▊         | 49/585 [00:14<02:36,  3.43it/s]  9%|▊         | 50/585 [00:14<02:35,  3.44it/s]  9%|▊         | 51/585 [00:14<02:35,  3.44it/s]  9%|▉         | 52/585 [00:15<02:34,  3.44it/s]  9%|▉         | 53/585 [00:15<02:34,  3.44it/s]  9%|▉         | 54/585 [00:15<02:34,  3.45it/s]  9%|▉         | 55/585 [00:15<02:33,  3.45it/s] 10%|▉         | 56/585 [00:16<02:33,  3.45it/s] 10%|▉         | 57/585 [00:16<02:33,  3.45it/s] 10%|▉         | 58/585 [00:16<02:35,  3.39it/s] 10%|█         | 59/585 [00:17<02:34,  3.41it/s] 10%|█         | 60/585 [00:17<02:33,  3.42it/s] 10%|█         | 61/585 [00:17<02:32,  3.43it/s] 11%|█         | 62/585 [00:17<02:32,  3.44it/s] 11%|█         | 63/585 [00:18<02:31,  3.44it/s] 11%|█         | 64/585 [00:18<02:31,  3.44it/s] 11%|█         | 65/585 [00:18<02:31,  3.44it/s] 11%|█▏        | 66/585 [00:19<02:30,  3.45it/s] 11%|█▏        | 67/585 [00:19<02:30,  3.45it/s] 12%|█▏        | 68/585 [00:19<02:29,  3.45it/s] 12%|█▏        | 69/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 70/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 71/585 [00:20<02:28,  3.45it/s] 12%|█▏        | 72/585 [00:20<02:28,  3.45it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.45it/s] 13%|█▎        | 74/585 [00:21<02:28,  3.45it/s] 13%|█▎        | 75/585 [00:21<02:27,  3.45it/s] 13%|█▎        | 76/585 [00:22<02:29,  3.41it/s] 13%|█▎        | 77/585 [00:22<02:28,  3.42it/s] 13%|█▎        | 78/585 [00:22<02:27,  3.43it/s] 14%|█▎        | 79/585 [00:22<02:27,  3.43it/s] 14%|█▎        | 80/585 [00:23<02:26,  3.44it/s] 14%|█▍        | 81/585 [00:23<02:26,  3.44it/s] 14%|█▍        | 82/585 [00:23<02:26,  3.44it/s] 14%|█▍        | 83/585 [00:24<02:25,  3.44it/s] 14%|█▍        | 84/585 [00:24<02:25,  3.44it/s] 15%|█▍        | 85/585 [00:24<02:25,  3.45it/s] 15%|█▍        | 86/585 [00:24<02:24,  3.45it/s] 15%|█▍        | 87/585 [00:25<02:26,  3.40it/s] 15%|█▌        | 88/585 [00:25<02:25,  3.41it/s] 15%|█▌        | 89/585 [00:25<02:24,  3.42it/s] 15%|█▌        | 90/585 [00:26<02:24,  3.43it/s] 16%|█▌        | 91/585 [00:26<02:23,  3.44it/s] 16%|█▌        | 92/585 [00:26<02:23,  3.44it/s] 16%|█▌        | 93/585 [00:26<02:22,  3.44it/s] 16%|█▌        | 94/585 [00:27<02:22,  3.45it/s] 16%|█▌        | 95/585 [00:27<02:22,  3.44it/s] 16%|█▋        | 96/585 [00:27<02:21,  3.45it/s] 17%|█▋        | 97/585 [00:28<02:21,  3.44it/s] 17%|█▋        | 98/585 [00:28<02:26,  3.32it/s] 17%|█▋        | 99/585 [00:28<02:24,  3.36it/s] 17%|█▋        | 100/585 [00:29<02:23,  3.39it/s] 17%|█▋        | 101/585 [00:29<02:22,  3.41it/s] 17%|█▋        | 102/585 [00:29<02:21,  3.42it/s] 18%|█▊        | 103/585 [00:29<02:20,  3.43it/s] 18%|█▊        | 104/585 [00:30<02:20,  3.42it/s] 18%|█▊        | 105/585 [00:30<02:20,  3.41it/s] 18%|█▊        | 106/585 [00:30<02:20,  3.41it/s] 18%|█▊        | 107/585 [00:31<02:20,  3.40it/s] 18%|█▊        | 108/585 [00:31<02:20,  3.40it/s] 19%|█▊        | 109/585 [00:31<02:24,  3.30it/s] 19%|█▉        | 110/585 [00:32<02:22,  3.33it/s] 19%|█▉        | 111/585 [00:32<02:21,  3.35it/s] 19%|█▉        | 112/585 [00:32<02:20,  3.36it/s] 19%|█▉        | 113/585 [00:32<02:19,  3.38it/s] 19%|█▉        | 114/585 [00:33<02:19,  3.38it/s] 20%|█▉        | 115/585 [00:33<02:18,  3.38it/s] 20%|█▉        | 116/585 [00:33<02:18,  3.39it/s] 20%|██        | 117/585 [00:34<02:17,  3.39it/s][INFO|trainer.py:2140] 2023-08-29 06:02:01,341 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:02:01,342 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 06:02:01,342 >>   Batch size = 8

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:19, 55.62it/s][A
  1%|          | 12/1071 [00:00<00:21, 48.36it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 46.57it/s][A
  2%|▏         | 22/1071 [00:00<00:23, 45.02it/s][A
  3%|▎         | 27/1071 [00:00<00:23, 44.96it/s][A
  3%|▎         | 32/1071 [00:00<00:23, 44.73it/s][A
  3%|▎         | 37/1071 [00:00<00:23, 44.79it/s][A
  4%|▍         | 42/1071 [00:00<00:23, 44.47it/s][A
  4%|▍         | 47/1071 [00:01<00:22, 44.54it/s][A
  5%|▍         | 52/1071 [00:01<00:22, 44.74it/s][A
  5%|▌         | 57/1071 [00:01<00:22, 44.97it/s][A
  6%|▌         | 62/1071 [00:01<00:22, 44.83it/s][A
  6%|▋         | 67/1071 [00:01<00:22, 44.89it/s][A
  7%|▋         | 72/1071 [00:01<00:22, 44.77it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 44.70it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 44.63it/s][A
  8%|▊         | 87/1071 [00:01<00:22, 44.66it/s][A
  9%|▊         | 92/1071 [00:02<00:21, 44.67it/s][A
  9%|▉         | 97/1071 [00:02<00:21, 44.84it/s][A
 10%|▉         | 102/1071 [00:02<00:21, 44.92it/s][A
 10%|▉         | 107/1071 [00:02<00:21, 44.69it/s][A
 10%|█         | 112/1071 [00:02<00:21, 44.58it/s][A
 11%|█         | 117/1071 [00:02<00:21, 44.53it/s][A
 11%|█▏        | 122/1071 [00:02<00:21, 44.56it/s][A
 12%|█▏        | 127/1071 [00:02<00:21, 44.63it/s][A
 12%|█▏        | 132/1071 [00:02<00:21, 44.48it/s][A
 13%|█▎        | 137/1071 [00:03<00:20, 44.52it/s][A
 13%|█▎        | 142/1071 [00:03<00:20, 44.63it/s][A
 14%|█▎        | 147/1071 [00:03<00:20, 44.67it/s][A
 14%|█▍        | 152/1071 [00:03<00:20, 44.88it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 44.35it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 44.52it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 44.55it/s][A
 16%|█▌        | 172/1071 [00:03<00:20, 44.56it/s][A
 17%|█▋        | 177/1071 [00:03<00:20, 44.47it/s][A
 17%|█▋        | 182/1071 [00:04<00:20, 44.40it/s][A
 17%|█▋        | 187/1071 [00:04<00:19, 44.53it/s][A
 18%|█▊        | 192/1071 [00:04<00:19, 44.89it/s][A
 18%|█▊        | 197/1071 [00:04<00:19, 44.76it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 44.75it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 44.61it/s][A
 20%|█▉        | 212/1071 [00:04<00:19, 44.50it/s][A
 20%|██        | 217/1071 [00:04<00:19, 44.58it/s][A
 21%|██        | 222/1071 [00:04<00:19, 44.64it/s][A
 21%|██        | 227/1071 [00:05<00:18, 44.78it/s][A
 22%|██▏       | 232/1071 [00:05<00:18, 44.82it/s][A
 22%|██▏       | 237/1071 [00:05<00:18, 44.90it/s][A
 23%|██▎       | 242/1071 [00:05<00:18, 44.84it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 44.57it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 44.69it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 44.53it/s][A
 24%|██▍       | 262/1071 [00:05<00:18, 44.27it/s][A
 25%|██▍       | 267/1071 [00:05<00:18, 44.42it/s][A
 25%|██▌       | 272/1071 [00:06<00:17, 44.74it/s][A
 26%|██▌       | 277/1071 [00:06<00:17, 44.81it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 44.88it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 44.84it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 43.33it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 43.64it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 43.88it/s][A
 29%|██▊       | 307/1071 [00:06<00:17, 43.92it/s][A
 29%|██▉       | 312/1071 [00:06<00:17, 44.14it/s][A
 30%|██▉       | 317/1071 [00:07<00:16, 44.47it/s][A
 30%|███       | 322/1071 [00:07<00:16, 44.65it/s][A
 31%|███       | 327/1071 [00:07<00:16, 44.76it/s][A
 31%|███       | 332/1071 [00:07<00:16, 44.58it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 44.70it/s][A
 32%|███▏      | 342/1071 [00:07<00:16, 44.68it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 44.58it/s][A
 33%|███▎      | 352/1071 [00:07<00:16, 44.49it/s][A
 33%|███▎      | 357/1071 [00:07<00:16, 44.46it/s][A
 34%|███▍      | 362/1071 [00:08<00:15, 44.72it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.74it/s][A
 35%|███▍      | 372/1071 [00:08<00:15, 44.81it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 44.59it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 44.56it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 44.51it/s][A
 37%|███▋      | 392/1071 [00:08<00:15, 44.47it/s][A
 37%|███▋      | 397/1071 [00:08<00:15, 44.49it/s][A
 38%|███▊      | 402/1071 [00:08<00:15, 44.44it/s][A
 38%|███▊      | 407/1071 [00:09<00:14, 44.56it/s][A
 38%|███▊      | 412/1071 [00:09<00:15, 41.64it/s][A
 39%|███▉      | 417/1071 [00:09<00:15, 42.82it/s][A
 39%|███▉      | 422/1071 [00:09<00:14, 43.38it/s][A
 40%|███▉      | 427/1071 [00:09<00:15, 42.34it/s][A
 40%|████      | 432/1071 [00:09<00:14, 43.18it/s][A
 41%|████      | 437/1071 [00:09<00:14, 43.53it/s][A
 41%|████▏     | 442/1071 [00:09<00:14, 43.88it/s][A
 42%|████▏     | 447/1071 [00:10<00:14, 44.11it/s][A
 42%|████▏     | 452/1071 [00:10<00:14, 44.16it/s][A
 43%|████▎     | 457/1071 [00:10<00:13, 44.28it/s][A
 43%|████▎     | 462/1071 [00:10<00:13, 44.49it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 44.37it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 44.72it/s][A
 45%|████▍     | 477/1071 [00:10<00:13, 44.82it/s][A
 45%|████▌     | 482/1071 [00:10<00:13, 44.82it/s][A
 45%|████▌     | 487/1071 [00:10<00:13, 44.77it/s][A
 46%|████▌     | 492/1071 [00:11<00:12, 44.70it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.69it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.73it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 44.74it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 44.68it/s][A
 48%|████▊     | 517/1071 [00:12<00:31, 17.38it/s][A
 49%|████▊     | 522/1071 [00:12<00:25, 21.32it/s][A
 49%|████▉     | 527/1071 [00:12<00:21, 25.31it/s][A
 50%|████▉     | 532/1071 [00:12<00:18, 29.19it/s][A
 50%|█████     | 537/1071 [00:12<00:17, 30.88it/s][A
 51%|█████     | 542/1071 [00:12<00:15, 34.18it/s][A
 51%|█████     | 547/1071 [00:12<00:14, 36.95it/s][A
 52%|█████▏    | 552/1071 [00:12<00:13, 39.01it/s][A
 52%|█████▏    | 557/1071 [00:13<00:12, 40.29it/s][A
 52%|█████▏    | 562/1071 [00:13<00:12, 41.55it/s][A
 53%|█████▎    | 567/1071 [00:13<00:11, 42.50it/s][A
 53%|█████▎    | 572/1071 [00:13<00:11, 43.22it/s][A
 54%|█████▍    | 577/1071 [00:13<00:11, 43.47it/s][A
 54%|█████▍    | 582/1071 [00:13<00:11, 43.81it/s][A
 55%|█████▍    | 587/1071 [00:13<00:10, 44.14it/s][A
 55%|█████▌    | 592/1071 [00:13<00:10, 44.35it/s][A
 56%|█████▌    | 597/1071 [00:14<00:10, 44.56it/s][A
 56%|█████▌    | 602/1071 [00:14<00:10, 44.48it/s][A
 57%|█████▋    | 607/1071 [00:14<00:10, 44.55it/s][A
 57%|█████▋    | 612/1071 [00:14<00:10, 44.79it/s][A
 58%|█████▊    | 617/1071 [00:14<00:10, 44.75it/s][A
 58%|█████▊    | 622/1071 [00:14<00:10, 44.46it/s][A
 59%|█████▊    | 627/1071 [00:14<00:09, 44.69it/s][A
 59%|█████▉    | 632/1071 [00:14<00:09, 44.64it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.80it/s][A
 60%|█████▉    | 642/1071 [00:15<00:09, 44.80it/s][A
 60%|██████    | 647/1071 [00:15<00:09, 44.81it/s][A
 61%|██████    | 652/1071 [00:15<00:09, 44.65it/s][A
 61%|██████▏   | 657/1071 [00:15<00:09, 44.71it/s][A
 62%|██████▏   | 662/1071 [00:15<00:09, 44.72it/s][A
 62%|██████▏   | 667/1071 [00:15<00:09, 44.51it/s][A
 63%|██████▎   | 672/1071 [00:15<00:08, 44.58it/s][A
 63%|██████▎   | 677/1071 [00:15<00:08, 44.66it/s][A
 64%|██████▎   | 682/1071 [00:15<00:08, 44.83it/s][A
 64%|██████▍   | 687/1071 [00:16<00:08, 45.05it/s][A
 65%|██████▍   | 692/1071 [00:16<00:08, 44.79it/s][A
 65%|██████▌   | 697/1071 [00:16<00:08, 44.65it/s][A
 66%|██████▌   | 702/1071 [00:16<00:08, 44.72it/s][A
 66%|██████▌   | 707/1071 [00:16<00:08, 44.59it/s][A
 66%|██████▋   | 712/1071 [00:16<00:08, 44.51it/s][A
 67%|██████▋   | 717/1071 [00:16<00:07, 44.33it/s][A
 67%|██████▋   | 722/1071 [00:16<00:07, 44.58it/s][A
 68%|██████▊   | 727/1071 [00:16<00:07, 44.81it/s][A
 68%|██████▊   | 732/1071 [00:17<00:07, 44.90it/s][A
 69%|██████▉   | 737/1071 [00:17<00:07, 44.82it/s][A
 69%|██████▉   | 742/1071 [00:17<00:07, 44.71it/s][A
 70%|██████▉   | 747/1071 [00:17<00:07, 44.66it/s][A
 70%|███████   | 752/1071 [00:17<00:07, 44.44it/s][A
 71%|███████   | 757/1071 [00:17<00:07, 44.57it/s][A
 71%|███████   | 762/1071 [00:17<00:06, 44.73it/s][A
 72%|███████▏  | 767/1071 [00:17<00:06, 44.58it/s][A
 72%|███████▏  | 772/1071 [00:17<00:06, 44.68it/s][A
 73%|███████▎  | 777/1071 [00:18<00:06, 44.77it/s][A
 73%|███████▎  | 782/1071 [00:18<00:06, 44.79it/s][A
 73%|███████▎  | 787/1071 [00:18<00:06, 41.42it/s][A
 74%|███████▍  | 792/1071 [00:18<00:06, 42.56it/s][A
 74%|███████▍  | 797/1071 [00:18<00:06, 43.23it/s][A
 75%|███████▍  | 802/1071 [00:18<00:06, 43.62it/s][A
 75%|███████▌  | 807/1071 [00:18<00:06, 43.93it/s][A
 76%|███████▌  | 812/1071 [00:18<00:05, 44.33it/s][A
 76%|███████▋  | 817/1071 [00:18<00:05, 44.55it/s][A
 77%|███████▋  | 822/1071 [00:19<00:05, 44.54it/s][A
 77%|███████▋  | 827/1071 [00:19<00:05, 44.19it/s][A
 78%|███████▊  | 832/1071 [00:19<00:05, 44.36it/s][A
 78%|███████▊  | 837/1071 [00:19<00:05, 44.44it/s][A
 79%|███████▊  | 842/1071 [00:19<00:05, 44.49it/s][A
 79%|███████▉  | 847/1071 [00:19<00:05, 44.74it/s][A
 80%|███████▉  | 852/1071 [00:19<00:04, 44.94it/s][A
 80%|████████  | 857/1071 [00:19<00:04, 45.05it/s][A
 80%|████████  | 862/1071 [00:19<00:04, 44.93it/s][A
 81%|████████  | 867/1071 [00:20<00:04, 44.73it/s][A
 81%|████████▏ | 872/1071 [00:20<00:04, 44.68it/s][A
 82%|████████▏ | 877/1071 [00:20<00:04, 44.52it/s][A
 82%|████████▏ | 882/1071 [00:20<00:04, 44.52it/s][A
 83%|████████▎ | 887/1071 [00:20<00:04, 44.84it/s][A
 83%|████████▎ | 892/1071 [00:20<00:03, 44.89it/s][A
 84%|████████▍ | 897/1071 [00:20<00:03, 45.01it/s][A
 84%|████████▍ | 902/1071 [00:20<00:03, 44.98it/s][A
 85%|████████▍ | 907/1071 [00:20<00:03, 44.89it/s][A
 85%|████████▌ | 912/1071 [00:21<00:03, 44.91it/s][A
 86%|████████▌ | 917/1071 [00:21<00:03, 44.64it/s][A
 86%|████████▌ | 922/1071 [00:21<00:03, 40.86it/s][A
 87%|████████▋ | 927/1071 [00:21<00:03, 42.09it/s][A
 87%|████████▋ | 932/1071 [00:21<00:03, 43.14it/s][A
 87%|████████▋ | 937/1071 [00:21<00:03, 43.80it/s][A
 88%|████████▊ | 942/1071 [00:21<00:02, 44.16it/s][A
 88%|████████▊ | 947/1071 [00:21<00:02, 44.34it/s][A
 89%|████████▉ | 952/1071 [00:21<00:02, 44.32it/s][A
 89%|████████▉ | 957/1071 [00:22<00:02, 44.26it/s][A
 90%|████████▉ | 962/1071 [00:22<00:02, 44.14it/s][A
 90%|█████████ | 967/1071 [00:22<00:02, 44.29it/s][A
 91%|█████████ | 972/1071 [00:22<00:02, 44.40it/s][A
 91%|█████████ | 977/1071 [00:22<00:02, 44.62it/s][A
 92%|█████████▏| 982/1071 [00:22<00:01, 44.87it/s][A
 92%|█████████▏| 987/1071 [00:22<00:01, 44.95it/s][A
 93%|█████████▎| 992/1071 [00:22<00:01, 44.98it/s][A
 93%|█████████▎| 997/1071 [00:23<00:01, 44.85it/s][A
 94%|█████████▎| 1002/1071 [00:23<00:01, 44.50it/s][A
 94%|█████████▍| 1007/1071 [00:23<00:01, 44.38it/s][A
 94%|█████████▍| 1012/1071 [00:23<00:01, 44.34it/s][A
 95%|█████████▍| 1017/1071 [00:23<00:01, 44.47it/s][A
 95%|█████████▌| 1022/1071 [00:23<00:01, 44.54it/s][A
 96%|█████████▌| 1027/1071 [00:23<00:00, 44.75it/s][A
 96%|█████████▋| 1032/1071 [00:23<00:00, 44.87it/s][A
 97%|█████████▋| 1037/1071 [00:23<00:00, 44.66it/s][A
 97%|█████████▋| 1042/1071 [00:24<00:00, 44.57it/s][A
 98%|█████████▊| 1047/1071 [00:24<00:00, 44.42it/s][A
 98%|█████████▊| 1052/1071 [00:24<00:00, 44.43it/s][A
 99%|█████████▊| 1057/1071 [00:24<00:00, 43.28it/s][A
 99%|█████████▉| 1062/1071 [00:24<00:00, 43.82it/s][A
100%|█████████▉| 1067/1071 [00:24<00:00, 44.14it/s][A                                                 
                                                   [A 20%|██        | 117/585 [00:58<02:17,  3.39it/s]
100%|██████████| 1071/1071 [00:24<00:00, 44.14it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-29 06:02:26,187 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 06:02:26,342 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:02:28,745 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:02:28,822 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:02:28,861 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:07<1:18:52, 10.13s/it] 20%|██        | 119/585 [01:07<55:49,  7.19s/it]   21%|██        | 120/585 [01:07<39:40,  5.12s/it] 21%|██        | 121/585 [01:08<28:24,  3.67s/it] 21%|██        | 122/585 [01:08<20:30,  2.66s/it] 21%|██        | 123/585 [01:08<15:00,  1.95s/it] 21%|██        | 124/585 [01:08<11:09,  1.45s/it] 21%|██▏       | 125/585 [01:09<08:28,  1.11s/it] 22%|██▏       | 126/585 [01:09<06:35,  1.16it/s] 22%|██▏       | 127/585 [01:09<05:16,  1.45it/s] 22%|██▏       | 128/585 [01:10<04:22,  1.74it/s] 22%|██▏       | 129/585 [01:10<03:43,  2.04it/s] 22%|██▏       | 130/585 [01:10<03:18,  2.29it/s] 22%|██▏       | 131/585 [01:11<02:59,  2.54it/s] 23%|██▎       | 132/585 [01:11<02:44,  2.75it/s] 23%|██▎       | 133/585 [01:11<02:35,  2.91it/s] 23%|██▎       | 134/585 [01:11<02:28,  3.04it/s] 23%|██▎       | 135/585 [01:12<02:23,  3.14it/s] 23%|██▎       | 136/585 [01:12<02:19,  3.21it/s] 23%|██▎       | 137/585 [01:12<02:17,  3.27it/s] 24%|██▎       | 138/585 [01:13<02:15,  3.29it/s] 24%|██▍       | 139/585 [01:13<02:14,  3.32it/s] 24%|██▍       | 140/585 [01:13<02:13,  3.34it/s] 24%|██▍       | 141/585 [01:14<02:14,  3.29it/s] 24%|██▍       | 142/585 [01:14<02:13,  3.32it/s] 24%|██▍       | 143/585 [01:14<02:12,  3.34it/s] 25%|██▍       | 144/585 [01:14<02:11,  3.36it/s] 25%|██▍       | 145/585 [01:15<02:10,  3.37it/s] 25%|██▍       | 146/585 [01:15<02:10,  3.38it/s] 25%|██▌       | 147/585 [01:15<02:09,  3.38it/s] 25%|██▌       | 148/585 [01:16<02:09,  3.38it/s] 25%|██▌       | 149/585 [01:16<02:08,  3.39it/s] 26%|██▌       | 150/585 [01:16<02:08,  3.39it/s] 26%|██▌       | 151/585 [01:16<02:08,  3.39it/s] 26%|██▌       | 152/585 [01:17<02:11,  3.29it/s] 26%|██▌       | 153/585 [01:17<02:10,  3.32it/s] 26%|██▋       | 154/585 [01:17<02:08,  3.34it/s] 26%|██▋       | 155/585 [01:18<02:08,  3.36it/s] 27%|██▋       | 156/585 [01:18<02:07,  3.37it/s] 27%|██▋       | 157/585 [01:18<02:06,  3.37it/s] 27%|██▋       | 158/585 [01:19<02:06,  3.38it/s] 27%|██▋       | 159/585 [01:19<02:05,  3.38it/s] 27%|██▋       | 160/585 [01:19<02:05,  3.39it/s] 28%|██▊       | 161/585 [01:19<02:05,  3.39it/s] 28%|██▊       | 162/585 [01:20<02:04,  3.39it/s] 28%|██▊       | 163/585 [01:20<02:07,  3.32it/s] 28%|██▊       | 164/585 [01:20<02:05,  3.34it/s] 28%|██▊       | 165/585 [01:21<02:05,  3.36it/s] 28%|██▊       | 166/585 [01:21<02:04,  3.37it/s] 29%|██▊       | 167/585 [01:21<02:03,  3.38it/s] 29%|██▊       | 168/585 [01:22<02:03,  3.38it/s] 29%|██▉       | 169/585 [01:22<02:02,  3.38it/s] 29%|██▉       | 170/585 [01:22<02:02,  3.39it/s] 29%|██▉       | 171/585 [01:22<02:01,  3.40it/s] 29%|██▉       | 172/585 [01:23<02:01,  3.41it/s] 30%|██▉       | 173/585 [01:23<02:00,  3.42it/s] 30%|██▉       | 174/585 [01:23<02:01,  3.38it/s] 30%|██▉       | 175/585 [01:24<02:00,  3.40it/s] 30%|███       | 176/585 [01:24<01:59,  3.41it/s] 30%|███       | 177/585 [01:24<01:59,  3.42it/s] 30%|███       | 178/585 [01:24<01:58,  3.43it/s] 31%|███       | 179/585 [01:25<01:58,  3.43it/s] 31%|███       | 180/585 [01:25<01:57,  3.44it/s] 31%|███       | 181/585 [01:25<01:57,  3.44it/s] 31%|███       | 182/585 [01:26<01:57,  3.44it/s] 31%|███▏      | 183/585 [01:26<01:56,  3.44it/s] 31%|███▏      | 184/585 [01:26<01:56,  3.45it/s] 32%|███▏      | 185/585 [01:27<01:58,  3.37it/s] 32%|███▏      | 186/585 [01:27<01:57,  3.40it/s] 32%|███▏      | 187/585 [01:27<01:56,  3.41it/s] 32%|███▏      | 188/585 [01:27<01:56,  3.42it/s] 32%|███▏      | 189/585 [01:28<01:55,  3.43it/s] 32%|███▏      | 190/585 [01:28<01:55,  3.43it/s] 33%|███▎      | 191/585 [01:28<01:54,  3.44it/s] 33%|███▎      | 192/585 [01:29<01:54,  3.44it/s] 33%|███▎      | 193/585 [01:29<01:53,  3.44it/s] 33%|███▎      | 194/585 [01:29<01:53,  3.44it/s] 33%|███▎      | 195/585 [01:29<01:53,  3.44it/s] 34%|███▎      | 196/585 [01:30<01:55,  3.38it/s] 34%|███▎      | 197/585 [01:30<01:54,  3.40it/s] 34%|███▍      | 198/585 [01:30<01:53,  3.41it/s] 34%|███▍      | 199/585 [01:31<01:52,  3.42it/s] 34%|███▍      | 200/585 [01:31<01:52,  3.43it/s] 34%|███▍      | 201/585 [01:31<01:51,  3.44it/s] 35%|███▍      | 202/585 [01:31<01:51,  3.43it/s] 35%|███▍      | 203/585 [01:32<01:51,  3.44it/s] 35%|███▍      | 204/585 [01:32<01:51,  3.43it/s] 35%|███▌      | 205/585 [01:32<01:50,  3.44it/s] 35%|███▌      | 206/585 [01:33<01:50,  3.44it/s] 35%|███▌      | 207/585 [01:33<01:49,  3.44it/s] 36%|███▌      | 208/585 [01:33<01:52,  3.34it/s] 36%|███▌      | 209/585 [01:34<01:51,  3.37it/s] 36%|███▌      | 210/585 [01:34<01:50,  3.39it/s] 36%|███▌      | 211/585 [01:34<01:49,  3.41it/s] 36%|███▌      | 212/585 [01:34<01:49,  3.42it/s] 36%|███▋      | 213/585 [01:35<01:48,  3.43it/s] 37%|███▋      | 214/585 [01:35<01:48,  3.43it/s] 37%|███▋      | 215/585 [01:35<01:47,  3.44it/s] 37%|███▋      | 216/585 [01:36<01:47,  3.44it/s] 37%|███▋      | 217/585 [01:36<01:46,  3.44it/s] 37%|███▋      | 218/585 [01:36<01:46,  3.44it/s] 37%|███▋      | 219/585 [01:36<01:48,  3.36it/s] 38%|███▊      | 220/585 [01:37<01:47,  3.39it/s] 38%|███▊      | 221/585 [01:37<01:46,  3.40it/s] 38%|███▊      | 222/585 [01:37<01:46,  3.42it/s] 38%|███▊      | 223/585 [01:38<01:45,  3.42it/s] 38%|███▊      | 224/585 [01:38<01:45,  3.43it/s] 38%|███▊      | 225/585 [01:38<01:44,  3.44it/s] 39%|███▊      | 226/585 [01:38<01:44,  3.44it/s] 39%|███▉      | 227/585 [01:39<01:43,  3.44it/s] 39%|███▉      | 228/585 [01:39<01:43,  3.44it/s] 39%|███▉      | 229/585 [01:39<01:43,  3.44it/s] 39%|███▉      | 230/585 [01:40<01:44,  3.40it/s] 39%|███▉      | 231/585 [01:40<01:43,  3.41it/s] 40%|███▉      | 232/585 [01:40<01:43,  3.42it/s] 40%|███▉      | 233/585 [01:41<01:42,  3.43it/s] 40%|████      | 234/585 [01:41<01:42,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 06:03:08,565 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:03:08,565 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 06:03:08,565 >>   Batch size = 8
{'eval_loss': 0.9229092597961426, 'eval_runtime': 24.713, 'eval_samples_per_second': 346.7, 'eval_steps_per_second': 43.337, 'epoch': 1.0}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:19, 54.87it/s][A
  1%|          | 12/1071 [00:00<00:21, 48.35it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 46.63it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 45.92it/s][A
  3%|▎         | 27/1071 [00:00<00:23, 45.33it/s][A
  3%|▎         | 32/1071 [00:00<00:23, 44.86it/s][A
  3%|▎         | 37/1071 [00:00<00:23, 44.84it/s][A
  4%|▍         | 42/1071 [00:00<00:23, 44.63it/s][A
  4%|▍         | 47/1071 [00:01<00:22, 44.55it/s][A
  5%|▍         | 52/1071 [00:01<00:22, 44.59it/s][A
  5%|▌         | 57/1071 [00:01<00:22, 44.66it/s][A
  6%|▌         | 62/1071 [00:01<00:22, 44.74it/s][A
  6%|▋         | 67/1071 [00:01<00:22, 44.67it/s][A
  7%|▋         | 72/1071 [00:01<00:23, 42.45it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 43.23it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 43.64it/s][A
  8%|▊         | 87/1071 [00:01<00:23, 41.76it/s][A
  9%|▊         | 92/1071 [00:02<00:22, 43.87it/s][A
  9%|▉         | 97/1071 [00:02<00:22, 44.14it/s][A
 10%|▉         | 102/1071 [00:02<00:21, 44.54it/s][A
 10%|▉         | 107/1071 [00:02<00:21, 44.50it/s][A
 10%|█         | 112/1071 [00:02<00:21, 44.39it/s][A
 11%|█         | 117/1071 [00:02<00:21, 44.36it/s][A
 11%|█▏        | 122/1071 [00:02<00:21, 44.44it/s][A
 12%|█▏        | 127/1071 [00:02<00:21, 44.53it/s][A
 12%|█▏        | 132/1071 [00:02<00:21, 44.46it/s][A
 13%|█▎        | 137/1071 [00:03<00:20, 44.56it/s][A
 13%|█▎        | 142/1071 [00:03<00:20, 44.64it/s][A
 14%|█▎        | 147/1071 [00:03<00:20, 44.85it/s][A
 14%|█▍        | 152/1071 [00:03<00:20, 44.77it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 44.67it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 44.77it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 44.65it/s][A
 16%|█▌        | 172/1071 [00:03<00:20, 44.62it/s][A
 17%|█▋        | 177/1071 [00:03<00:20, 44.55it/s][A
 17%|█▋        | 182/1071 [00:04<00:19, 44.64it/s][A
 17%|█▋        | 187/1071 [00:04<00:19, 44.55it/s][A
 18%|█▊        | 192/1071 [00:04<00:19, 44.59it/s][A
 18%|█▊        | 197/1071 [00:04<00:21, 41.31it/s][A
 19%|█▉        | 202/1071 [00:04<00:21, 40.63it/s][A
 19%|█▉        | 207/1071 [00:04<00:22, 39.20it/s][A
 20%|█▉        | 212/1071 [00:04<00:21, 40.49it/s][A
 20%|██        | 217/1071 [00:04<00:20, 41.38it/s][A
 21%|██        | 222/1071 [00:05<00:20, 42.41it/s][A
 21%|██        | 227/1071 [00:05<00:19, 43.21it/s][A
 22%|██▏       | 232/1071 [00:05<00:19, 43.87it/s][A
 22%|██▏       | 237/1071 [00:05<00:18, 44.16it/s][A
 23%|██▎       | 242/1071 [00:05<00:18, 44.04it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 43.84it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 43.85it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 44.20it/s][A
 24%|██▍       | 262/1071 [00:05<00:18, 44.35it/s][A
 25%|██▍       | 267/1071 [00:06<00:18, 44.60it/s][A
 25%|██▌       | 272/1071 [00:06<00:17, 44.83it/s][A
 26%|██▌       | 277/1071 [00:06<00:17, 44.83it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 44.75it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 44.62it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 44.47it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 44.56it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 44.55it/s][A
 29%|██▊       | 307/1071 [00:06<00:17, 44.64it/s][A
 29%|██▉       | 312/1071 [00:07<00:16, 44.86it/s][A
 30%|██▉       | 317/1071 [00:07<00:16, 44.89it/s][A
 30%|███       | 322/1071 [00:07<00:16, 45.02it/s][A
 31%|███       | 327/1071 [00:07<00:16, 44.95it/s][A
 31%|███       | 332/1071 [00:07<00:16, 44.83it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 44.65it/s][A
 32%|███▏      | 342/1071 [00:07<00:17, 41.51it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 42.68it/s][A
 33%|███▎      | 352/1071 [00:07<00:16, 43.29it/s][A
 33%|███▎      | 357/1071 [00:08<00:16, 43.84it/s][A
 34%|███▍      | 362/1071 [00:08<00:16, 44.10it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.40it/s][A
 35%|███▍      | 372/1071 [00:08<00:15, 44.53it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 44.70it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 44.23it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 44.21it/s][A
 37%|███▋      | 392/1071 [00:08<00:15, 44.35it/s][A
 37%|███▋      | 397/1071 [00:08<00:15, 44.51it/s][A
 38%|███▊      | 402/1071 [00:09<00:15, 44.50it/s][A
 38%|███▊      | 407/1071 [00:09<00:14, 44.67it/s][A
 38%|███▊      | 412/1071 [00:09<00:14, 44.88it/s][A
 39%|███▉      | 417/1071 [00:09<00:14, 44.93it/s][A
 39%|███▉      | 422/1071 [00:09<00:14, 44.93it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 44.54it/s][A
 40%|████      | 432/1071 [00:09<00:14, 44.43it/s][A
 41%|████      | 437/1071 [00:09<00:14, 44.62it/s][A
 41%|████▏     | 442/1071 [00:10<00:14, 44.67it/s][A
 42%|████▏     | 447/1071 [00:10<00:13, 44.74it/s][A
 42%|████▏     | 452/1071 [00:10<00:13, 44.63it/s][A
 43%|████▎     | 457/1071 [00:10<00:13, 44.91it/s][A
 43%|████▎     | 462/1071 [00:10<00:13, 44.88it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 44.62it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 44.55it/s][A
 45%|████▍     | 477/1071 [00:10<00:14, 41.26it/s][A
 45%|████▌     | 482/1071 [00:10<00:13, 42.32it/s][A
 45%|████▌     | 487/1071 [00:11<00:13, 43.24it/s][A
 46%|████▌     | 492/1071 [00:11<00:13, 43.80it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.16it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.53it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 44.47it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 44.48it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 44.19it/s][A
 49%|████▊     | 522/1071 [00:11<00:12, 44.23it/s][A
 49%|████▉     | 527/1071 [00:11<00:12, 44.35it/s][A
 50%|████▉     | 532/1071 [00:12<00:12, 44.55it/s][A
 50%|█████     | 537/1071 [00:12<00:11, 44.65it/s][A
 51%|█████     | 542/1071 [00:12<00:11, 44.70it/s][A
 51%|█████     | 547/1071 [00:12<00:11, 44.88it/s][A
 52%|█████▏    | 552/1071 [00:12<00:11, 44.97it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 44.68it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 44.53it/s][A
 53%|█████▎    | 567/1071 [00:12<00:11, 44.44it/s][A
 53%|█████▎    | 572/1071 [00:12<00:11, 44.29it/s][A
 54%|█████▍    | 577/1071 [00:13<00:11, 44.60it/s][A
 54%|█████▍    | 582/1071 [00:13<00:10, 44.82it/s][A
 55%|█████▍    | 587/1071 [00:13<00:10, 44.98it/s][A
 55%|█████▌    | 592/1071 [00:13<00:10, 44.99it/s][A
 56%|█████▌    | 597/1071 [00:13<00:10, 44.97it/s][A
 56%|█████▌    | 602/1071 [00:13<00:10, 44.79it/s][A
 57%|█████▋    | 607/1071 [00:13<00:10, 44.64it/s][A
 57%|█████▋    | 612/1071 [00:13<00:10, 41.96it/s][A
 58%|█████▊    | 617/1071 [00:13<00:10, 43.09it/s][A
 58%|█████▊    | 622/1071 [00:14<00:10, 43.67it/s][A
 59%|█████▊    | 627/1071 [00:14<00:10, 43.93it/s][A
 59%|█████▉    | 632/1071 [00:14<00:09, 44.38it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.65it/s][A
 60%|█████▉    | 642/1071 [00:14<00:09, 44.52it/s][A
 60%|██████    | 647/1071 [00:14<00:09, 44.52it/s][A
 61%|██████    | 652/1071 [00:14<00:09, 44.02it/s][A
 61%|██████▏   | 657/1071 [00:14<00:09, 44.28it/s][A
 62%|██████▏   | 662/1071 [00:14<00:09, 44.45it/s][A
 62%|██████▏   | 667/1071 [00:15<00:09, 44.61it/s][A
 63%|██████▎   | 672/1071 [00:15<00:08, 44.82it/s][A
 63%|██████▎   | 677/1071 [00:15<00:08, 44.99it/s][A
 64%|██████▎   | 682/1071 [00:15<00:08, 44.91it/s][A
 64%|██████▍   | 687/1071 [00:15<00:08, 44.93it/s][A
 65%|██████▍   | 692/1071 [00:15<00:08, 44.71it/s][A
 65%|██████▌   | 697/1071 [00:15<00:08, 44.50it/s][A
 66%|██████▌   | 702/1071 [00:15<00:08, 44.57it/s][A
 66%|██████▌   | 707/1071 [00:15<00:08, 44.48it/s][A
 66%|██████▋   | 712/1071 [00:16<00:08, 44.73it/s][A
 67%|██████▋   | 717/1071 [00:16<00:07, 44.70it/s][A
 67%|██████▋   | 722/1071 [00:16<00:07, 44.90it/s][A
 68%|██████▊   | 727/1071 [00:16<00:07, 44.94it/s][A
 68%|██████▊   | 732/1071 [00:16<00:07, 44.84it/s][A
 69%|██████▉   | 737/1071 [00:16<00:07, 44.75it/s][A
 69%|██████▉   | 742/1071 [00:16<00:07, 44.55it/s][A
 70%|██████▉   | 747/1071 [00:16<00:07, 42.75it/s][A
 70%|███████   | 752/1071 [00:16<00:07, 43.47it/s][A
 71%|███████   | 757/1071 [00:17<00:07, 43.95it/s][A
 71%|███████   | 762/1071 [00:17<00:06, 44.14it/s][A
 72%|███████▏  | 767/1071 [00:17<00:06, 44.30it/s][A
 72%|███████▏  | 772/1071 [00:17<00:06, 44.51it/s][A
 73%|███████▎  | 777/1071 [00:17<00:06, 44.54it/s][A
 73%|███████▎  | 782/1071 [00:17<00:06, 44.46it/s][A
 73%|███████▎  | 787/1071 [00:17<00:06, 44.27it/s][A
 74%|███████▍  | 792/1071 [00:17<00:06, 44.37it/s][A
 74%|███████▍  | 797/1071 [00:18<00:06, 44.65it/s][A
 75%|███████▍  | 802/1071 [00:18<00:06, 44.66it/s][A
 75%|███████▌  | 807/1071 [00:18<00:05, 44.93it/s][A
 76%|███████▌  | 812/1071 [00:18<00:05, 44.82it/s][A
 76%|███████▋  | 817/1071 [00:18<00:05, 44.79it/s][A
 77%|███████▋  | 822/1071 [00:18<00:05, 44.84it/s][A
 77%|███████▋  | 827/1071 [00:18<00:05, 44.51it/s][A
 78%|███████▊  | 832/1071 [00:18<00:05, 44.23it/s][A
 78%|███████▊  | 837/1071 [00:18<00:05, 44.32it/s][A
 79%|███████▊  | 842/1071 [00:19<00:05, 44.50it/s][A
 79%|███████▉  | 847/1071 [00:19<00:05, 44.69it/s][A
 80%|███████▉  | 852/1071 [00:19<00:04, 44.75it/s][A
 80%|████████  | 857/1071 [00:19<00:04, 44.86it/s][A
 80%|████████  | 862/1071 [00:19<00:04, 44.89it/s][A
 81%|████████  | 867/1071 [00:19<00:04, 44.78it/s][A
 81%|████████▏ | 872/1071 [00:19<00:04, 44.50it/s][A
 82%|████████▏ | 877/1071 [00:19<00:04, 44.35it/s][A
 82%|████████▏ | 882/1071 [00:19<00:04, 44.41it/s][A
 83%|████████▎ | 887/1071 [00:20<00:04, 44.51it/s][A
 83%|████████▎ | 892/1071 [00:20<00:04, 44.53it/s][A
 84%|████████▍ | 897/1071 [00:20<00:03, 44.73it/s][A
 84%|████████▍ | 902/1071 [00:20<00:03, 44.65it/s][A
 85%|████████▍ | 907/1071 [00:20<00:03, 44.68it/s][A
 85%|████████▌ | 912/1071 [00:20<00:03, 44.44it/s][A
 86%|████████▌ | 917/1071 [00:20<00:03, 44.20it/s][A
 86%|████████▌ | 922/1071 [00:20<00:03, 44.33it/s][A
 87%|████████▋ | 927/1071 [00:20<00:03, 44.19it/s][A
 87%|████████▋ | 932/1071 [00:21<00:03, 44.53it/s][A
 87%|████████▋ | 937/1071 [00:21<00:03, 44.57it/s][A
 88%|████████▊ | 942/1071 [00:21<00:02, 44.72it/s][A
 88%|████████▊ | 947/1071 [00:21<00:02, 44.67it/s][A
 89%|████████▉ | 952/1071 [00:21<00:02, 44.81it/s][A
 89%|████████▉ | 957/1071 [00:21<00:02, 44.77it/s][A
 90%|████████▉ | 962/1071 [00:21<00:02, 44.69it/s][A
 90%|█████████ | 967/1071 [00:21<00:02, 44.54it/s][A
 91%|█████████ | 972/1071 [00:21<00:02, 44.44it/s][A
 91%|█████████ | 977/1071 [00:22<00:02, 44.64it/s][A
 92%|█████████▏| 982/1071 [00:22<00:01, 44.75it/s][A
 92%|█████████▏| 987/1071 [00:22<00:01, 44.60it/s][A
 93%|█████████▎| 992/1071 [00:22<00:01, 44.78it/s][A
 93%|█████████▎| 997/1071 [00:22<00:01, 44.79it/s][A
 94%|█████████▎| 1002/1071 [00:22<00:01, 44.56it/s][A
 94%|█████████▍| 1007/1071 [00:22<00:01, 43.49it/s][A
 94%|█████████▍| 1012/1071 [00:22<00:01, 43.78it/s][A
 95%|█████████▍| 1017/1071 [00:22<00:01, 44.14it/s][A
 95%|█████████▌| 1022/1071 [00:23<00:01, 44.34it/s][A
 96%|█████████▌| 1027/1071 [00:23<00:00, 44.57it/s][A
 96%|█████████▋| 1032/1071 [00:23<00:00, 44.68it/s][A
 97%|█████████▋| 1037/1071 [00:23<00:00, 44.68it/s][A
 97%|█████████▋| 1042/1071 [00:23<00:00, 44.63it/s][A
 98%|█████████▊| 1047/1071 [00:23<00:00, 44.41it/s][A
 98%|█████████▊| 1052/1071 [00:23<00:00, 43.73it/s][A
 99%|█████████▊| 1057/1071 [00:23<00:00, 43.87it/s][A
 99%|█████████▉| 1062/1071 [00:23<00:00, 44.19it/s][A
100%|█████████▉| 1067/1071 [00:24<00:00, 44.41it/s][A                                                 
                                                   [A 40%|████      | 234/585 [02:05<01:42,  3.43it/s]
100%|██████████| 1071/1071 [00:24<00:00, 44.41it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-29 06:03:32,995 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 06:03:33,152 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:03:36,357 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:03:36,509 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:03:36,574 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:15<1:01:53, 10.61s/it] 40%|████      | 236/585 [02:16<43:44,  7.52s/it]   41%|████      | 237/585 [02:16<31:02,  5.35s/it] 41%|████      | 238/585 [02:16<22:11,  3.84s/it] 41%|████      | 239/585 [02:17<15:59,  2.77s/it] 41%|████      | 240/585 [02:17<11:40,  2.03s/it] 41%|████      | 241/585 [02:17<08:39,  1.51s/it] 41%|████▏     | 242/585 [02:18<06:32,  1.14s/it] 42%|████▏     | 243/585 [02:18<05:04,  1.12it/s] 42%|████▏     | 244/585 [02:18<04:02,  1.41it/s] 42%|████▏     | 245/585 [02:18<03:19,  1.71it/s] 42%|████▏     | 246/585 [02:19<02:48,  2.01it/s] 42%|████▏     | 247/585 [02:19<02:30,  2.25it/s] 42%|████▏     | 248/585 [02:19<02:14,  2.50it/s] 43%|████▎     | 249/585 [02:20<02:03,  2.72it/s] 43%|████▎     | 250/585 [02:20<01:55,  2.89it/s] 43%|████▎     | 251/585 [02:20<01:50,  3.03it/s] 43%|████▎     | 252/585 [02:21<01:46,  3.13it/s] 43%|████▎     | 253/585 [02:21<01:43,  3.21it/s] 43%|████▎     | 254/585 [02:21<01:41,  3.26it/s] 44%|████▎     | 255/585 [02:21<01:39,  3.30it/s] 44%|████▍     | 256/585 [02:22<01:38,  3.33it/s] 44%|████▍     | 257/585 [02:22<01:38,  3.35it/s] 44%|████▍     | 258/585 [02:22<01:39,  3.30it/s] 44%|████▍     | 259/585 [02:23<01:38,  3.32it/s] 44%|████▍     | 260/585 [02:23<01:37,  3.34it/s] 45%|████▍     | 261/585 [02:23<01:36,  3.36it/s] 45%|████▍     | 262/585 [02:24<01:35,  3.37it/s] 45%|████▍     | 263/585 [02:24<01:35,  3.38it/s] 45%|████▌     | 264/585 [02:24<01:34,  3.38it/s] 45%|████▌     | 265/585 [02:24<01:34,  3.38it/s] 45%|████▌     | 266/585 [02:25<01:34,  3.39it/s] 46%|████▌     | 267/585 [02:25<01:33,  3.39it/s] 46%|████▌     | 268/585 [02:25<01:33,  3.39it/s] 46%|████▌     | 269/585 [02:26<01:34,  3.33it/s] 46%|████▌     | 270/585 [02:26<01:34,  3.35it/s] 46%|████▋     | 271/585 [02:26<01:33,  3.36it/s] 46%|████▋     | 272/585 [02:26<01:32,  3.37it/s] 47%|████▋     | 273/585 [02:27<01:32,  3.38it/s] 47%|████▋     | 274/585 [02:27<01:31,  3.39it/s] 47%|████▋     | 275/585 [02:27<01:31,  3.38it/s] 47%|████▋     | 276/585 [02:28<01:31,  3.38it/s] 47%|████▋     | 277/585 [02:28<01:30,  3.39it/s] 48%|████▊     | 278/585 [02:28<01:30,  3.39it/s] 48%|████▊     | 279/585 [02:29<01:30,  3.39it/s] 48%|████▊     | 280/585 [02:29<01:32,  3.28it/s] 48%|████▊     | 281/585 [02:29<01:31,  3.31it/s] 48%|████▊     | 282/585 [02:29<01:30,  3.33it/s] 48%|████▊     | 283/585 [02:30<01:30,  3.35it/s] 49%|████▊     | 284/585 [02:30<01:29,  3.37it/s] 49%|████▊     | 285/585 [02:30<01:28,  3.38it/s] 49%|████▉     | 286/585 [02:31<01:28,  3.38it/s] 49%|████▉     | 287/585 [02:31<01:28,  3.39it/s] 49%|████▉     | 288/585 [02:31<01:27,  3.39it/s] 49%|████▉     | 289/585 [02:32<01:30,  3.25it/s] 50%|████▉     | 290/585 [02:32<01:29,  3.29it/s] 50%|████▉     | 291/585 [02:32<01:28,  3.32it/s] 50%|████▉     | 292/585 [02:32<01:27,  3.34it/s] 50%|█████     | 293/585 [02:33<01:27,  3.36it/s] 50%|█████     | 294/585 [02:33<01:26,  3.37it/s] 50%|█████     | 295/585 [02:33<01:25,  3.38it/s] 51%|█████     | 296/585 [02:34<01:25,  3.38it/s] 51%|█████     | 297/585 [02:34<01:25,  3.38it/s] 51%|█████     | 298/585 [02:34<01:24,  3.39it/s] 51%|█████     | 299/585 [02:35<01:25,  3.33it/s] 51%|█████▏    | 300/585 [02:35<01:25,  3.35it/s] 51%|█████▏    | 301/585 [02:35<01:24,  3.36it/s] 52%|█████▏    | 302/585 [02:35<01:23,  3.37it/s] 52%|█████▏    | 303/585 [02:36<01:23,  3.38it/s] 52%|█████▏    | 304/585 [02:36<01:23,  3.38it/s] 52%|█████▏    | 305/585 [02:36<01:22,  3.38it/s] 52%|█████▏    | 306/585 [02:37<01:22,  3.39it/s] 52%|█████▏    | 307/585 [02:37<01:21,  3.39it/s] 53%|█████▎    | 308/585 [02:37<01:21,  3.39it/s] 53%|█████▎    | 309/585 [02:37<01:21,  3.39it/s] 53%|█████▎    | 310/585 [02:38<01:23,  3.30it/s] 53%|█████▎    | 311/585 [02:38<01:22,  3.33it/s] 53%|█████▎    | 312/585 [02:38<01:21,  3.35it/s] 54%|█████▎    | 313/585 [02:39<01:20,  3.36it/s] 54%|█████▎    | 314/585 [02:39<01:20,  3.37it/s] 54%|█████▍    | 315/585 [02:39<01:19,  3.38it/s] 54%|█████▍    | 316/585 [02:40<01:19,  3.39it/s] 54%|█████▍    | 317/585 [02:40<01:19,  3.39it/s] 54%|█████▍    | 318/585 [02:40<01:18,  3.39it/s] 55%|█████▍    | 319/585 [02:40<01:18,  3.40it/s] 55%|█████▍    | 320/585 [02:41<01:18,  3.39it/s] 55%|█████▍    | 321/585 [02:41<01:20,  3.29it/s] 55%|█████▌    | 322/585 [02:41<01:19,  3.32it/s] 55%|█████▌    | 323/585 [02:42<01:18,  3.34it/s] 55%|█████▌    | 324/585 [02:42<01:17,  3.36it/s] 56%|█████▌    | 325/585 [02:42<01:17,  3.37it/s] 56%|█████▌    | 326/585 [02:43<01:16,  3.38it/s] 56%|█████▌    | 327/585 [02:43<01:16,  3.39it/s] 56%|█████▌    | 328/585 [02:43<01:18,  3.29it/s] 56%|█████▌    | 329/585 [02:43<01:17,  3.32it/s] 56%|█████▋    | 330/585 [02:44<01:16,  3.34it/s] 57%|█████▋    | 331/585 [02:44<01:17,  3.27it/s] 57%|█████▋    | 332/585 [02:44<01:16,  3.31it/s] 57%|█████▋    | 333/585 [02:45<01:15,  3.33it/s] 57%|█████▋    | 334/585 [02:45<01:14,  3.35it/s] 57%|█████▋    | 335/585 [02:45<01:16,  3.25it/s] 57%|█████▋    | 336/585 [02:46<01:21,  3.07it/s] 58%|█████▊    | 337/585 [02:46<01:20,  3.06it/s] 58%|█████▊    | 338/585 [02:46<01:18,  3.16it/s] 58%|█████▊    | 339/585 [02:47<01:16,  3.23it/s] 58%|█████▊    | 340/585 [02:47<01:14,  3.27it/s] 58%|█████▊    | 341/585 [02:47<01:14,  3.25it/s] 58%|█████▊    | 342/585 [02:47<01:13,  3.30it/s] 59%|█████▊    | 343/585 [02:48<01:12,  3.32it/s] 59%|█████▉    | 344/585 [02:48<01:12,  3.35it/s] 59%|█████▉    | 345/585 [02:48<01:11,  3.37it/s] 59%|█████▉    | 346/585 [02:49<01:10,  3.38it/s] 59%|█████▉    | 347/585 [02:49<01:10,  3.39it/s] 59%|█████▉    | 348/585 [02:49<01:09,  3.39it/s] 60%|█████▉    | 349/585 [02:50<01:09,  3.40it/s] 60%|█████▉    | 350/585 [02:50<01:09,  3.40it/s] 60%|██████    | 351/585 [02:50<01:08,  3.40it/s][INFO|trainer.py:2140] 2023-08-29 06:04:17,897 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:04:17,897 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 06:04:17,897 >>   Batch size = 8
{'eval_loss': 0.9277217388153076, 'eval_runtime': 24.204, 'eval_samples_per_second': 353.991, 'eval_steps_per_second': 44.249, 'epoch': 2.0}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:18, 56.09it/s][A
  1%|          | 12/1071 [00:00<00:21, 48.61it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 47.10it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 46.56it/s][A
  3%|▎         | 27/1071 [00:00<00:22, 45.83it/s][A
  3%|▎         | 32/1071 [00:00<00:22, 45.52it/s][A
  3%|▎         | 37/1071 [00:00<00:22, 45.34it/s][A
  4%|▍         | 42/1071 [00:00<00:23, 44.68it/s][A
  4%|▍         | 47/1071 [00:01<00:22, 44.65it/s][A
  5%|▍         | 52/1071 [00:01<00:22, 44.67it/s][A
  5%|▌         | 57/1071 [00:01<00:22, 44.73it/s][A
  6%|▌         | 62/1071 [00:01<00:22, 44.82it/s][A
  6%|▋         | 67/1071 [00:01<00:22, 45.01it/s][A
  7%|▋         | 72/1071 [00:01<00:22, 44.97it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 44.92it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 44.54it/s][A
  8%|▊         | 87/1071 [00:01<00:22, 44.38it/s][A
  9%|▊         | 92/1071 [00:02<00:22, 44.32it/s][A
  9%|▉         | 97/1071 [00:02<00:21, 44.54it/s][A
 10%|▉         | 102/1071 [00:02<00:21, 44.56it/s][A
 10%|▉         | 107/1071 [00:02<00:21, 44.83it/s][A
 10%|█         | 112/1071 [00:02<00:21, 45.02it/s][A
 11%|█         | 117/1071 [00:02<00:21, 45.09it/s][A
 11%|█▏        | 122/1071 [00:02<00:21, 44.78it/s][A
 12%|█▏        | 127/1071 [00:02<00:21, 44.39it/s][A
 12%|█▏        | 132/1071 [00:02<00:21, 44.33it/s][A
 13%|█▎        | 137/1071 [00:03<00:22, 40.74it/s][A
 13%|█▎        | 142/1071 [00:03<00:22, 41.88it/s][A
 14%|█▎        | 147/1071 [00:03<00:21, 42.67it/s][A
 14%|█▍        | 152/1071 [00:03<00:21, 43.35it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 43.75it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 44.03it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 44.38it/s][A
 16%|█▌        | 172/1071 [00:03<00:20, 44.63it/s][A
 17%|█▋        | 177/1071 [00:03<00:20, 44.40it/s][A
 17%|█▋        | 182/1071 [00:04<00:19, 44.48it/s][A
 17%|█▋        | 187/1071 [00:04<00:19, 44.38it/s][A
 18%|█▊        | 192/1071 [00:04<00:19, 44.66it/s][A
 18%|█▊        | 197/1071 [00:04<00:19, 44.70it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 44.81it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 44.78it/s][A
 20%|█▉        | 212/1071 [00:04<00:19, 44.89it/s][A
 20%|██        | 217/1071 [00:04<00:19, 44.73it/s][A
 21%|██        | 222/1071 [00:04<00:19, 44.53it/s][A
 21%|██        | 227/1071 [00:05<00:18, 44.57it/s][A
 22%|██▏       | 232/1071 [00:05<00:18, 44.65it/s][A
 22%|██▏       | 237/1071 [00:05<00:18, 44.76it/s][A
 23%|██▎       | 242/1071 [00:05<00:18, 44.77it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 44.78it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 44.98it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 44.90it/s][A
 24%|██▍       | 262/1071 [00:05<00:18, 44.76it/s][A
 25%|██▍       | 267/1071 [00:06<00:17, 44.77it/s][A
 25%|██▌       | 272/1071 [00:06<00:19, 41.65it/s][A
 26%|██▌       | 277/1071 [00:06<00:18, 42.76it/s][A
 26%|██▋       | 282/1071 [00:06<00:18, 43.62it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 44.00it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 44.26it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 44.30it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 44.37it/s][A
 29%|██▊       | 307/1071 [00:06<00:17, 44.51it/s][A
 29%|██▉       | 312/1071 [00:07<00:17, 44.15it/s][A
 30%|██▉       | 317/1071 [00:07<00:16, 44.38it/s][A
 30%|███       | 322/1071 [00:07<00:16, 44.50it/s][A
 31%|███       | 327/1071 [00:07<00:16, 44.61it/s][A
 31%|███       | 332/1071 [00:07<00:16, 44.90it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 45.02it/s][A
 32%|███▏      | 342/1071 [00:07<00:16, 44.93it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 44.81it/s][A
 33%|███▎      | 352/1071 [00:07<00:16, 44.71it/s][A
 33%|███▎      | 357/1071 [00:08<00:16, 44.51it/s][A
 34%|███▍      | 362/1071 [00:08<00:15, 44.51it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.58it/s][A
 35%|███▍      | 372/1071 [00:08<00:15, 44.60it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 44.85it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 44.94it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 44.93it/s][A
 37%|███▋      | 392/1071 [00:08<00:15, 44.94it/s][A
 37%|███▋      | 397/1071 [00:08<00:15, 44.91it/s][A
 38%|███▊      | 402/1071 [00:09<00:15, 44.55it/s][A
 38%|███▊      | 407/1071 [00:09<00:17, 38.53it/s][A
 38%|███▊      | 412/1071 [00:09<00:16, 40.39it/s][A
 39%|███▉      | 417/1071 [00:09<00:15, 41.77it/s][A
 39%|███▉      | 422/1071 [00:09<00:15, 42.83it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 43.47it/s][A
 40%|████      | 432/1071 [00:09<00:14, 44.14it/s][A
 41%|████      | 437/1071 [00:09<00:14, 44.47it/s][A
 41%|████▏     | 442/1071 [00:09<00:14, 44.64it/s][A
 42%|████▏     | 447/1071 [00:10<00:14, 44.21it/s][A
 42%|████▏     | 452/1071 [00:10<00:14, 43.84it/s][A
 43%|████▎     | 457/1071 [00:10<00:13, 44.09it/s][A
 43%|████▎     | 462/1071 [00:10<00:13, 44.41it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 44.56it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 44.81it/s][A
 45%|████▍     | 477/1071 [00:10<00:13, 45.01it/s][A
 45%|████▌     | 482/1071 [00:10<00:13, 45.23it/s][A
 45%|████▌     | 487/1071 [00:10<00:12, 44.96it/s][A
 46%|████▌     | 492/1071 [00:11<00:12, 44.69it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.44it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.38it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 44.50it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 44.64it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 44.88it/s][A
 49%|████▊     | 522/1071 [00:11<00:12, 44.97it/s][A
 49%|████▉     | 527/1071 [00:11<00:16, 33.05it/s][A
 50%|████▉     | 532/1071 [00:12<00:14, 36.03it/s][A
 50%|█████     | 537/1071 [00:12<00:13, 38.36it/s][A
 51%|█████     | 542/1071 [00:12<00:13, 40.26it/s][A
 51%|█████     | 547/1071 [00:12<00:12, 41.68it/s][A
 52%|█████▏    | 552/1071 [00:12<00:12, 42.73it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 43.55it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 43.74it/s][A
 53%|█████▎    | 567/1071 [00:12<00:11, 43.61it/s][A
 53%|█████▎    | 572/1071 [00:13<00:11, 43.75it/s][A
 54%|█████▍    | 577/1071 [00:13<00:11, 43.78it/s][A
 54%|█████▍    | 582/1071 [00:13<00:11, 44.15it/s][A
 55%|█████▍    | 587/1071 [00:13<00:10, 44.52it/s][A
 55%|█████▌    | 592/1071 [00:13<00:10, 44.70it/s][A
 56%|█████▌    | 597/1071 [00:13<00:10, 44.96it/s][A
 56%|█████▌    | 602/1071 [00:13<00:10, 44.97it/s][A
 57%|█████▋    | 607/1071 [00:13<00:10, 44.88it/s][A
 57%|█████▋    | 612/1071 [00:13<00:10, 44.55it/s][A
 58%|█████▊    | 617/1071 [00:14<00:10, 44.10it/s][A
 58%|█████▊    | 622/1071 [00:14<00:10, 44.15it/s][A
 59%|█████▊    | 627/1071 [00:14<00:10, 44.30it/s][A
 59%|█████▉    | 632/1071 [00:14<00:10, 42.45it/s][A
 60%|█████▉    | 638/1071 [00:14<00:09, 44.64it/s][A
 60%|██████    | 643/1071 [00:14<00:09, 44.86it/s][A
 61%|██████    | 648/1071 [00:14<00:09, 44.98it/s][A
 61%|██████    | 653/1071 [00:14<00:09, 44.47it/s][A
 61%|██████▏   | 658/1071 [00:14<00:09, 44.08it/s][A
 62%|██████▏   | 663/1071 [00:15<00:09, 44.02it/s][A
 62%|██████▏   | 668/1071 [00:15<00:09, 44.01it/s][A
 63%|██████▎   | 673/1071 [00:15<00:08, 44.28it/s][A
 63%|██████▎   | 678/1071 [00:15<00:08, 44.52it/s][A
 64%|██████▍   | 683/1071 [00:15<00:08, 44.68it/s][A
 64%|██████▍   | 688/1071 [00:15<00:08, 44.97it/s][A
 65%|██████▍   | 693/1071 [00:15<00:08, 45.02it/s][A
 65%|██████▌   | 698/1071 [00:15<00:08, 44.63it/s][A
 66%|██████▌   | 703/1071 [00:15<00:08, 44.42it/s][A
 66%|██████▌   | 708/1071 [00:16<00:08, 44.30it/s][A
 67%|██████▋   | 713/1071 [00:16<00:08, 44.20it/s][A
 67%|██████▋   | 718/1071 [00:16<00:07, 44.40it/s][A
 68%|██████▊   | 723/1071 [00:16<00:07, 44.53it/s][A
 68%|██████▊   | 728/1071 [00:16<00:07, 44.85it/s][A
 68%|██████▊   | 733/1071 [00:16<00:07, 44.69it/s][A
 69%|██████▉   | 738/1071 [00:16<00:07, 44.59it/s][A
 69%|██████▉   | 743/1071 [00:16<00:07, 44.53it/s][A
 70%|██████▉   | 748/1071 [00:16<00:07, 44.44it/s][A
 70%|███████   | 753/1071 [00:17<00:07, 44.39it/s][A
 71%|███████   | 758/1071 [00:17<00:07, 44.22it/s][A
 71%|███████   | 763/1071 [00:17<00:06, 44.28it/s][A
 72%|███████▏  | 768/1071 [00:17<00:06, 44.66it/s][A
 72%|███████▏  | 773/1071 [00:17<00:06, 44.57it/s][A
 73%|███████▎  | 778/1071 [00:17<00:06, 44.69it/s][A
 73%|███████▎  | 783/1071 [00:17<00:06, 44.81it/s][A
 74%|███████▎  | 788/1071 [00:17<00:06, 43.96it/s][A
 74%|███████▍  | 793/1071 [00:17<00:06, 44.11it/s][A
 75%|███████▍  | 798/1071 [00:18<00:06, 43.99it/s][A
 75%|███████▍  | 803/1071 [00:18<00:06, 44.08it/s][A
 75%|███████▌  | 808/1071 [00:18<00:05, 44.25it/s][A
 76%|███████▌  | 813/1071 [00:18<00:05, 44.49it/s][A
 76%|███████▋  | 818/1071 [00:18<00:05, 44.61it/s][A
 77%|███████▋  | 823/1071 [00:18<00:05, 44.81it/s][A
 77%|███████▋  | 828/1071 [00:18<00:05, 44.67it/s][A
 78%|███████▊  | 833/1071 [00:18<00:05, 44.57it/s][A
 78%|███████▊  | 838/1071 [00:18<00:05, 44.55it/s][A
 79%|███████▊  | 843/1071 [00:19<00:05, 44.46it/s][A
 79%|███████▉  | 848/1071 [00:19<00:05, 44.54it/s][A
 80%|███████▉  | 853/1071 [00:19<00:04, 44.64it/s][A
 80%|████████  | 858/1071 [00:19<00:04, 44.72it/s][A
 81%|████████  | 863/1071 [00:19<00:04, 44.69it/s][A
 81%|████████  | 868/1071 [00:19<00:04, 44.56it/s][A
 82%|████████▏ | 873/1071 [00:19<00:04, 44.81it/s][A
 82%|████████▏ | 878/1071 [00:19<00:04, 44.83it/s][A
 82%|████████▏ | 883/1071 [00:19<00:04, 44.73it/s][A
 83%|████████▎ | 888/1071 [00:20<00:04, 44.73it/s][A
 83%|████████▎ | 893/1071 [00:20<00:04, 44.45it/s][A
 84%|████████▍ | 898/1071 [00:20<00:03, 44.52it/s][A
 84%|████████▍ | 903/1071 [00:20<00:03, 44.64it/s][A
 85%|████████▍ | 908/1071 [00:20<00:03, 44.76it/s][A
 85%|████████▌ | 913/1071 [00:20<00:03, 44.75it/s][A
 86%|████████▌ | 918/1071 [00:20<00:03, 44.56it/s][A
 86%|████████▌ | 923/1071 [00:20<00:03, 44.05it/s][A
 87%|████████▋ | 928/1071 [00:21<00:03, 44.31it/s][A
 87%|████████▋ | 933/1071 [00:21<00:03, 44.26it/s][A
 88%|████████▊ | 938/1071 [00:21<00:03, 44.19it/s][A
 88%|████████▊ | 943/1071 [00:21<00:02, 44.42it/s][A
 89%|████████▊ | 948/1071 [00:21<00:02, 44.64it/s][A
 89%|████████▉ | 953/1071 [00:21<00:02, 44.75it/s][A
 89%|████████▉ | 958/1071 [00:21<00:02, 44.68it/s][A
 90%|████████▉ | 963/1071 [00:21<00:02, 44.46it/s][A
 90%|█████████ | 968/1071 [00:21<00:02, 44.55it/s][A
 91%|█████████ | 973/1071 [00:22<00:02, 44.59it/s][A
 91%|█████████▏| 978/1071 [00:22<00:02, 44.43it/s][A
 92%|█████████▏| 983/1071 [00:22<00:01, 44.45it/s][A
 92%|█████████▏| 988/1071 [00:22<00:01, 44.45it/s][A
 93%|█████████▎| 993/1071 [00:22<00:01, 44.60it/s][A
 93%|█████████▎| 998/1071 [00:22<00:01, 44.79it/s][A
 94%|█████████▎| 1003/1071 [00:22<00:01, 44.74it/s][A
 94%|█████████▍| 1008/1071 [00:22<00:01, 44.59it/s][A
 95%|█████████▍| 1013/1071 [00:22<00:01, 44.66it/s][A
 95%|█████████▌| 1018/1071 [00:23<00:01, 44.67it/s][A
 96%|█████████▌| 1023/1071 [00:23<00:01, 44.50it/s][A
 96%|█████████▌| 1028/1071 [00:23<00:00, 44.55it/s][A
 96%|█████████▋| 1033/1071 [00:23<00:00, 44.55it/s][A
 97%|█████████▋| 1038/1071 [00:23<00:00, 44.60it/s][A
 97%|█████████▋| 1043/1071 [00:23<00:00, 44.45it/s][A
 98%|█████████▊| 1048/1071 [00:23<00:00, 44.92it/s][A
 98%|█████████▊| 1053/1071 [00:23<00:00, 44.79it/s][A
 99%|█████████▉| 1058/1071 [00:23<00:00, 42.30it/s][A
 99%|█████████▉| 1063/1071 [00:24<00:00, 42.95it/s][A
100%|█████████▉| 1068/1071 [00:24<00:00, 43.54it/s][A                                                 
                                                   [A 60%|██████    | 351/585 [03:14<01:08,  3.40it/s]
100%|██████████| 1071/1071 [00:24<00:00, 43.54it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-29 06:04:42,434 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 06:04:42,668 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:04:45,393 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:04:45,548 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:04:45,621 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:25<40:52, 10.53s/it] 60%|██████    | 353/585 [03:25<28:51,  7.46s/it] 61%|██████    | 354/585 [03:25<20:27,  5.31s/it] 61%|██████    | 355/585 [03:25<14:35,  3.81s/it] 61%|██████    | 356/585 [03:26<10:30,  2.75s/it] 61%|██████    | 357/585 [03:26<07:39,  2.02s/it] 61%|██████    | 358/585 [03:26<05:40,  1.50s/it] 61%|██████▏   | 359/585 [03:27<04:17,  1.14s/it] 62%|██████▏   | 360/585 [03:27<03:19,  1.13it/s] 62%|██████▏   | 361/585 [03:27<02:38,  1.41it/s] 62%|██████▏   | 362/585 [03:27<02:10,  1.71it/s] 62%|██████▏   | 363/585 [03:28<01:50,  2.01it/s] 62%|██████▏   | 364/585 [03:28<01:37,  2.26it/s] 62%|██████▏   | 365/585 [03:28<01:27,  2.51it/s] 63%|██████▎   | 366/585 [03:29<01:20,  2.72it/s] 63%|██████▎   | 367/585 [03:29<01:15,  2.89it/s] 63%|██████▎   | 368/585 [03:29<01:11,  3.03it/s] 63%|██████▎   | 369/585 [03:30<01:09,  3.13it/s] 63%|██████▎   | 370/585 [03:30<01:07,  3.20it/s] 63%|██████▎   | 371/585 [03:30<01:05,  3.26it/s] 64%|██████▎   | 372/585 [03:30<01:04,  3.30it/s] 64%|██████▍   | 373/585 [03:31<01:03,  3.33it/s] 64%|██████▍   | 374/585 [03:31<01:03,  3.35it/s] 64%|██████▍   | 375/585 [03:31<01:03,  3.32it/s] 64%|██████▍   | 376/585 [03:32<01:02,  3.34it/s] 64%|██████▍   | 377/585 [03:32<01:01,  3.36it/s] 65%|██████▍   | 378/585 [03:32<01:01,  3.37it/s] 65%|██████▍   | 379/585 [03:33<01:00,  3.38it/s] 65%|██████▍   | 380/585 [03:33<01:00,  3.38it/s] 65%|██████▌   | 381/585 [03:33<01:00,  3.38it/s] 65%|██████▌   | 382/585 [03:33<00:59,  3.39it/s] 65%|██████▌   | 383/585 [03:34<00:59,  3.40it/s] 66%|██████▌   | 384/585 [03:34<00:59,  3.40it/s] 66%|██████▌   | 385/585 [03:34<00:58,  3.40it/s] 66%|██████▌   | 386/585 [03:35<00:58,  3.40it/s] 66%|██████▌   | 387/585 [03:35<00:58,  3.40it/s] 66%|██████▋   | 388/585 [03:35<00:57,  3.40it/s] 66%|██████▋   | 389/585 [03:35<00:57,  3.40it/s] 67%|██████▋   | 390/585 [03:36<00:57,  3.40it/s] 67%|██████▋   | 391/585 [03:36<00:58,  3.32it/s] 67%|██████▋   | 392/585 [03:36<00:57,  3.34it/s] 67%|██████▋   | 393/585 [03:37<00:57,  3.36it/s] 67%|██████▋   | 394/585 [03:37<00:56,  3.37it/s] 68%|██████▊   | 395/585 [03:37<00:56,  3.38it/s] 68%|██████▊   | 396/585 [03:38<00:55,  3.38it/s] 68%|██████▊   | 397/585 [03:38<00:55,  3.39it/s] 68%|██████▊   | 398/585 [03:38<00:55,  3.39it/s] 68%|██████▊   | 399/585 [03:38<00:54,  3.39it/s] 68%|██████▊   | 400/585 [03:39<00:54,  3.39it/s] 69%|██████▊   | 401/585 [03:39<00:54,  3.39it/s] 69%|██████▊   | 402/585 [03:39<00:55,  3.32it/s] 69%|██████▉   | 403/585 [03:40<00:54,  3.34it/s] 69%|██████▉   | 404/585 [03:40<00:53,  3.36it/s] 69%|██████▉   | 405/585 [03:40<00:53,  3.37it/s] 69%|██████▉   | 406/585 [03:41<00:53,  3.38it/s] 70%|██████▉   | 407/585 [03:41<00:52,  3.38it/s] 70%|██████▉   | 408/585 [03:41<00:52,  3.39it/s] 70%|██████▉   | 409/585 [03:41<00:51,  3.39it/s] 70%|███████   | 410/585 [03:42<00:51,  3.39it/s] 70%|███████   | 411/585 [03:42<00:51,  3.39it/s] 70%|███████   | 412/585 [03:42<00:50,  3.39it/s] 71%|███████   | 413/585 [03:43<00:52,  3.30it/s] 71%|███████   | 414/585 [03:43<00:51,  3.33it/s] 71%|███████   | 415/585 [03:43<00:52,  3.26it/s] 71%|███████   | 416/585 [03:44<00:51,  3.30it/s] 71%|███████▏  | 417/585 [03:44<00:50,  3.32it/s] 71%|███████▏  | 418/585 [03:44<00:49,  3.35it/s] 72%|███████▏  | 419/585 [03:44<00:49,  3.36it/s] 72%|███████▏  | 420/585 [03:45<00:48,  3.37it/s] 72%|███████▏  | 421/585 [03:45<00:48,  3.38it/s] 72%|███████▏  | 422/585 [03:45<00:51,  3.17it/s] 72%|███████▏  | 423/585 [03:46<00:57,  2.82it/s] 72%|███████▏  | 424/585 [03:46<00:55,  2.89it/s] 73%|███████▎  | 425/585 [03:46<00:52,  3.04it/s] 73%|███████▎  | 426/585 [03:47<00:50,  3.15it/s] 73%|███████▎  | 427/585 [03:47<00:48,  3.23it/s] 73%|███████▎  | 428/585 [03:47<00:47,  3.29it/s] 73%|███████▎  | 429/585 [03:48<00:46,  3.34it/s] 74%|███████▎  | 430/585 [03:48<00:45,  3.37it/s] 74%|███████▎  | 431/585 [03:48<00:45,  3.39it/s] 74%|███████▍  | 432/585 [03:48<00:44,  3.41it/s] 74%|███████▍  | 433/585 [03:49<00:45,  3.34it/s] 74%|███████▍  | 434/585 [03:49<00:44,  3.37it/s] 74%|███████▍  | 435/585 [03:49<00:44,  3.40it/s] 75%|███████▍  | 436/585 [03:50<00:43,  3.41it/s] 75%|███████▍  | 437/585 [03:50<00:43,  3.42it/s] 75%|███████▍  | 438/585 [03:50<00:42,  3.43it/s] 75%|███████▌  | 439/585 [03:50<00:42,  3.44it/s] 75%|███████▌  | 440/585 [03:51<00:42,  3.44it/s] 75%|███████▌  | 441/585 [03:51<00:41,  3.45it/s] 76%|███████▌  | 442/585 [03:51<00:41,  3.45it/s] 76%|███████▌  | 443/585 [03:52<00:41,  3.45it/s] 76%|███████▌  | 444/585 [03:52<00:42,  3.29it/s] 76%|███████▌  | 445/585 [03:52<00:42,  3.33it/s] 76%|███████▌  | 446/585 [03:53<00:41,  3.37it/s] 76%|███████▋  | 447/585 [03:53<00:40,  3.40it/s] 77%|███████▋  | 448/585 [03:53<00:40,  3.41it/s] 77%|███████▋  | 449/585 [03:53<00:39,  3.43it/s] 77%|███████▋  | 450/585 [03:54<00:39,  3.44it/s] 77%|███████▋  | 451/585 [03:54<00:38,  3.44it/s] 77%|███████▋  | 452/585 [03:54<00:38,  3.45it/s] 77%|███████▋  | 453/585 [03:55<00:38,  3.45it/s] 78%|███████▊  | 454/585 [03:55<00:37,  3.45it/s] 78%|███████▊  | 455/585 [03:55<00:39,  3.31it/s] 78%|███████▊  | 456/585 [03:55<00:38,  3.35it/s] 78%|███████▊  | 457/585 [03:56<00:37,  3.38it/s] 78%|███████▊  | 458/585 [03:56<00:37,  3.40it/s] 78%|███████▊  | 459/585 [03:56<00:36,  3.42it/s] 79%|███████▊  | 460/585 [03:57<00:36,  3.43it/s] 79%|███████▉  | 461/585 [03:57<00:36,  3.44it/s] 79%|███████▉  | 462/585 [03:57<00:35,  3.44it/s] 79%|███████▉  | 463/585 [03:58<00:35,  3.45it/s] 79%|███████▉  | 464/585 [03:58<00:35,  3.45it/s] 79%|███████▉  | 465/585 [03:58<00:34,  3.45it/s] 80%|███████▉  | 466/585 [03:58<00:36,  3.29it/s] 80%|███████▉  | 467/585 [03:59<00:35,  3.34it/s] 80%|████████  | 468/585 [03:59<00:34,  3.38it/s][INFO|trainer.py:2140] 2023-08-29 06:05:26,762 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:05:26,762 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 06:05:26,762 >>   Batch size = 8
{'eval_loss': 0.9359139800071716, 'eval_runtime': 24.268, 'eval_samples_per_second': 353.058, 'eval_steps_per_second': 44.132, 'epoch': 3.0}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:18, 56.19it/s][A
  1%|          | 12/1071 [00:00<00:21, 48.91it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 47.02it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 45.83it/s][A
  3%|▎         | 27/1071 [00:00<00:23, 45.32it/s][A
  3%|▎         | 32/1071 [00:00<00:23, 45.03it/s][A
  3%|▎         | 37/1071 [00:00<00:23, 44.65it/s][A
  4%|▍         | 42/1071 [00:00<00:23, 44.60it/s][A
  4%|▍         | 47/1071 [00:01<00:22, 44.86it/s][A
  5%|▍         | 52/1071 [00:01<00:22, 45.04it/s][A
  5%|▌         | 57/1071 [00:01<00:22, 45.12it/s][A
  6%|▌         | 62/1071 [00:01<00:22, 44.97it/s][A
  6%|▋         | 67/1071 [00:01<00:22, 44.76it/s][A
  7%|▋         | 72/1071 [00:01<00:22, 44.76it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 44.46it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 44.51it/s][A
  8%|▊         | 87/1071 [00:01<00:22, 44.64it/s][A
  9%|▊         | 92/1071 [00:02<00:21, 44.50it/s][A
  9%|▉         | 97/1071 [00:02<00:22, 43.62it/s][A
 10%|▉         | 102/1071 [00:02<00:21, 44.16it/s][A
 10%|▉         | 107/1071 [00:02<00:21, 44.47it/s][A
 10%|█         | 112/1071 [00:02<00:21, 44.51it/s][A
 11%|█         | 117/1071 [00:02<00:21, 44.29it/s][A
 11%|█▏        | 122/1071 [00:02<00:21, 44.34it/s][A
 12%|█▏        | 127/1071 [00:02<00:21, 44.45it/s][A
 12%|█▏        | 132/1071 [00:02<00:21, 44.37it/s][A
 13%|█▎        | 137/1071 [00:03<00:21, 44.04it/s][A
 13%|█▎        | 142/1071 [00:03<00:20, 44.26it/s][A
 14%|█▎        | 147/1071 [00:03<00:20, 44.55it/s][A
 14%|█▍        | 152/1071 [00:03<00:20, 44.74it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 44.87it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 44.83it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 44.52it/s][A
 16%|█▌        | 172/1071 [00:03<00:20, 44.46it/s][A
 17%|█▋        | 177/1071 [00:03<00:20, 44.40it/s][A
 17%|█▋        | 182/1071 [00:04<00:20, 44.34it/s][A
 17%|█▋        | 187/1071 [00:04<00:19, 44.40it/s][A
 18%|█▊        | 192/1071 [00:04<00:19, 44.45it/s][A
 18%|█▊        | 197/1071 [00:04<00:20, 42.86it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 43.53it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 43.98it/s][A
 20%|█▉        | 212/1071 [00:04<00:19, 44.10it/s][A
 20%|██        | 217/1071 [00:04<00:19, 44.16it/s][A
 21%|██        | 222/1071 [00:04<00:19, 44.14it/s][A
 21%|██        | 227/1071 [00:05<00:19, 44.25it/s][A
 22%|██▏       | 232/1071 [00:05<00:18, 44.29it/s][A
 22%|██▏       | 237/1071 [00:05<00:18, 44.24it/s][A
 23%|██▎       | 242/1071 [00:05<00:18, 44.48it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 43.77it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 44.33it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 44.46it/s][A
 24%|██▍       | 262/1071 [00:05<00:18, 44.60it/s][A
 25%|██▍       | 267/1071 [00:05<00:18, 44.53it/s][A
 25%|██▌       | 272/1071 [00:06<00:17, 44.53it/s][A
 26%|██▌       | 277/1071 [00:06<00:17, 44.39it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 44.31it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 44.33it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 44.39it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 44.42it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 44.67it/s][A
 29%|██▊       | 307/1071 [00:06<00:17, 44.56it/s][A
 29%|██▉       | 312/1071 [00:06<00:16, 44.80it/s][A
 30%|██▉       | 317/1071 [00:07<00:16, 44.66it/s][A
 30%|███       | 322/1071 [00:07<00:16, 44.55it/s][A
 31%|███       | 327/1071 [00:07<00:16, 44.66it/s][A
 31%|███       | 332/1071 [00:07<00:17, 43.21it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 43.71it/s][A
 32%|███▏      | 342/1071 [00:07<00:16, 44.04it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 44.32it/s][A
 33%|███▎      | 352/1071 [00:07<00:16, 44.48it/s][A
 33%|███▎      | 357/1071 [00:08<00:16, 44.51it/s][A
 34%|███▍      | 362/1071 [00:08<00:15, 44.58it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.54it/s][A
 35%|███▍      | 372/1071 [00:08<00:15, 44.30it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 44.27it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 44.41it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 44.66it/s][A
 37%|███▋      | 392/1071 [00:08<00:15, 44.68it/s][A
 37%|███▋      | 397/1071 [00:08<00:15, 44.86it/s][A
 38%|███▊      | 402/1071 [00:09<00:14, 44.65it/s][A
 38%|███▊      | 407/1071 [00:09<00:14, 44.56it/s][A
 38%|███▊      | 412/1071 [00:09<00:14, 44.45it/s][A
 39%|███▉      | 417/1071 [00:09<00:14, 44.40it/s][A
 39%|███▉      | 422/1071 [00:09<00:14, 44.55it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 44.53it/s][A
 40%|████      | 432/1071 [00:09<00:14, 44.58it/s][A
 41%|████      | 437/1071 [00:09<00:14, 44.82it/s][A
 41%|████▏     | 442/1071 [00:09<00:14, 44.67it/s][A
 42%|████▏     | 447/1071 [00:10<00:13, 44.75it/s][A
 42%|████▏     | 452/1071 [00:10<00:13, 44.59it/s][A
 43%|████▎     | 457/1071 [00:10<00:13, 44.52it/s][A
 43%|████▎     | 462/1071 [00:10<00:13, 44.38it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 43.32it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 43.65it/s][A
 45%|████▍     | 477/1071 [00:10<00:13, 43.94it/s][A
 45%|████▌     | 482/1071 [00:10<00:13, 44.05it/s][A
 45%|████▌     | 487/1071 [00:10<00:13, 44.30it/s][A
 46%|████▌     | 492/1071 [00:11<00:13, 44.48it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.52it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.70it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 44.46it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 44.59it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 44.67it/s][A
 49%|████▊     | 522/1071 [00:11<00:12, 44.61it/s][A
 49%|████▉     | 527/1071 [00:11<00:12, 44.55it/s][A
 50%|████▉     | 532/1071 [00:11<00:12, 44.75it/s][A
 50%|█████     | 537/1071 [00:12<00:11, 44.59it/s][A
 51%|█████     | 542/1071 [00:12<00:11, 44.68it/s][A
 51%|█████     | 547/1071 [00:12<00:11, 44.68it/s][A
 52%|█████▏    | 552/1071 [00:12<00:11, 44.55it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 44.57it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 44.43it/s][A
 53%|█████▎    | 567/1071 [00:12<00:11, 44.43it/s][A
 53%|█████▎    | 572/1071 [00:12<00:11, 44.71it/s][A
 54%|█████▍    | 577/1071 [00:12<00:11, 44.63it/s][A
 54%|█████▍    | 582/1071 [00:13<00:10, 44.66it/s][A
 55%|█████▍    | 587/1071 [00:13<00:10, 44.42it/s][A
 55%|█████▌    | 592/1071 [00:13<00:10, 44.36it/s][A
 56%|█████▌    | 597/1071 [00:13<00:10, 44.51it/s][A
 56%|█████▌    | 602/1071 [00:13<00:10, 43.50it/s][A
 57%|█████▋    | 607/1071 [00:13<00:10, 43.96it/s][A
 57%|█████▋    | 612/1071 [00:13<00:10, 44.15it/s][A
 58%|█████▊    | 617/1071 [00:13<00:10, 44.45it/s][A
 58%|█████▊    | 622/1071 [00:13<00:10, 44.53it/s][A
 59%|█████▊    | 627/1071 [00:14<00:09, 44.58it/s][A
 59%|█████▉    | 632/1071 [00:14<00:09, 44.44it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.43it/s][A
 60%|█████▉    | 642/1071 [00:14<00:09, 44.39it/s][A
 60%|██████    | 647/1071 [00:14<00:09, 44.37it/s][A
 61%|██████    | 652/1071 [00:14<00:09, 44.61it/s][A
 61%|██████▏   | 657/1071 [00:14<00:09, 44.70it/s][A
 62%|██████▏   | 662/1071 [00:14<00:09, 44.85it/s][A
 62%|██████▏   | 667/1071 [00:14<00:09, 44.77it/s][A
 63%|██████▎   | 672/1071 [00:15<00:08, 44.67it/s][A
 63%|██████▎   | 677/1071 [00:15<00:08, 44.80it/s][A
 64%|██████▎   | 682/1071 [00:15<00:08, 44.77it/s][A
 64%|██████▍   | 687/1071 [00:15<00:08, 44.69it/s][A
 65%|██████▍   | 692/1071 [00:15<00:08, 44.65it/s][A
 65%|██████▌   | 697/1071 [00:15<00:08, 44.61it/s][A
 66%|██████▌   | 702/1071 [00:15<00:08, 44.69it/s][A
 66%|██████▌   | 707/1071 [00:15<00:08, 44.82it/s][A
 66%|██████▋   | 712/1071 [00:15<00:08, 44.84it/s][A
 67%|██████▋   | 717/1071 [00:16<00:07, 44.82it/s][A
 67%|██████▋   | 722/1071 [00:16<00:07, 44.80it/s][A
 68%|██████▊   | 727/1071 [00:16<00:07, 44.75it/s][A
 68%|██████▊   | 732/1071 [00:16<00:07, 44.53it/s][A
 69%|██████▉   | 737/1071 [00:16<00:07, 43.01it/s][A
 69%|██████▉   | 742/1071 [00:16<00:07, 43.58it/s][A
 70%|██████▉   | 747/1071 [00:16<00:07, 44.08it/s][A
 70%|███████   | 752/1071 [00:16<00:07, 44.26it/s][A
 71%|███████   | 757/1071 [00:17<00:07, 44.45it/s][A
 71%|███████   | 762/1071 [00:17<00:06, 44.24it/s][A
 72%|███████▏  | 767/1071 [00:17<00:06, 44.35it/s][A
 72%|███████▏  | 772/1071 [00:17<00:06, 44.40it/s][A
 73%|███████▎  | 777/1071 [00:17<00:06, 44.19it/s][A
 73%|███████▎  | 782/1071 [00:17<00:06, 44.24it/s][A
 73%|███████▎  | 787/1071 [00:17<00:06, 44.59it/s][A
 74%|███████▍  | 792/1071 [00:17<00:06, 44.74it/s][A
 74%|███████▍  | 797/1071 [00:17<00:06, 44.89it/s][A
 75%|███████▍  | 802/1071 [00:18<00:05, 44.84it/s][A
 75%|███████▌  | 807/1071 [00:18<00:05, 44.64it/s][A
 76%|███████▌  | 812/1071 [00:18<00:05, 44.67it/s][A
 76%|███████▋  | 817/1071 [00:18<00:05, 44.34it/s][A
 77%|███████▋  | 822/1071 [00:18<00:05, 44.28it/s][A
 77%|███████▋  | 827/1071 [00:18<00:05, 44.38it/s][A
 78%|███████▊  | 832/1071 [00:18<00:05, 44.52it/s][A
 78%|███████▊  | 837/1071 [00:18<00:05, 44.64it/s][A
 79%|███████▊  | 842/1071 [00:18<00:05, 44.82it/s][A
 79%|███████▉  | 847/1071 [00:19<00:04, 44.82it/s][A
 80%|███████▉  | 852/1071 [00:19<00:04, 44.68it/s][A
 80%|████████  | 857/1071 [00:19<00:04, 44.70it/s][A
 80%|████████  | 862/1071 [00:19<00:04, 44.38it/s][A
 81%|████████  | 867/1071 [00:19<00:04, 44.35it/s][A
 81%|████████▏ | 872/1071 [00:19<00:04, 44.12it/s][A
 82%|████████▏ | 877/1071 [00:19<00:04, 44.32it/s][A
 82%|████████▏ | 882/1071 [00:19<00:04, 44.37it/s][A
 83%|████████▎ | 887/1071 [00:19<00:04, 44.66it/s][A
 83%|████████▎ | 892/1071 [00:20<00:04, 44.58it/s][A
 84%|████████▍ | 897/1071 [00:20<00:03, 44.46it/s][A
 84%|████████▍ | 902/1071 [00:20<00:03, 44.34it/s][A
 85%|████████▍ | 907/1071 [00:20<00:03, 44.21it/s][A
 85%|████████▌ | 912/1071 [00:20<00:03, 44.29it/s][A
 86%|████████▌ | 917/1071 [00:20<00:03, 44.56it/s][A
 86%|████████▌ | 922/1071 [00:20<00:03, 44.74it/s][A
 87%|████████▋ | 927/1071 [00:20<00:03, 44.87it/s][A
 87%|████████▋ | 932/1071 [00:20<00:03, 44.92it/s][A
 87%|████████▋ | 937/1071 [00:21<00:02, 44.92it/s][A
 88%|████████▊ | 942/1071 [00:21<00:02, 44.67it/s][A
 88%|████████▊ | 947/1071 [00:21<00:02, 44.37it/s][A
 89%|████████▉ | 952/1071 [00:21<00:02, 44.12it/s][A
 89%|████████▉ | 957/1071 [00:21<00:02, 44.20it/s][A
 90%|████████▉ | 962/1071 [00:21<00:02, 44.37it/s][A
 90%|█████████ | 967/1071 [00:21<00:02, 44.66it/s][A
 91%|█████████ | 972/1071 [00:21<00:02, 44.84it/s][A
 91%|█████████ | 977/1071 [00:21<00:02, 44.86it/s][A
 92%|█████████▏| 982/1071 [00:22<00:01, 45.01it/s][A
 92%|█████████▏| 987/1071 [00:22<00:01, 44.75it/s][A
 93%|█████████▎| 992/1071 [00:22<00:01, 44.47it/s][A
 93%|█████████▎| 997/1071 [00:22<00:01, 44.05it/s][A
 94%|█████████▎| 1002/1071 [00:22<00:01, 43.96it/s][A
 94%|█████████▍| 1007/1071 [00:22<00:01, 43.49it/s][A
 94%|█████████▍| 1012/1071 [00:22<00:01, 44.07it/s][A
 95%|█████████▍| 1017/1071 [00:22<00:01, 44.23it/s][A
 95%|█████████▌| 1022/1071 [00:22<00:01, 44.43it/s][A
 96%|█████████▌| 1027/1071 [00:23<00:00, 44.55it/s][A
 96%|█████████▋| 1032/1071 [00:23<00:00, 44.61it/s][A
 97%|█████████▋| 1037/1071 [00:23<00:00, 44.52it/s][A
 97%|█████████▋| 1042/1071 [00:23<00:00, 44.42it/s][A
 98%|█████████▊| 1047/1071 [00:23<00:00, 44.27it/s][A
 98%|█████████▊| 1052/1071 [00:23<00:00, 44.41it/s][A
 99%|█████████▊| 1057/1071 [00:23<00:00, 44.41it/s][A
 99%|█████████▉| 1062/1071 [00:23<00:00, 44.58it/s][A
100%|█████████▉| 1067/1071 [00:23<00:00, 44.76it/s][A                                                 
                                                   [A 80%|████████  | 468/585 [04:23<00:34,  3.38it/s]
100%|██████████| 1071/1071 [00:24<00:00, 44.76it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-29 06:05:50,973 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 06:05:51,069 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:05:53,934 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:05:54,057 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:05:54,129 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [04:33<19:58, 10.33s/it] 80%|████████  | 470/585 [04:33<14:02,  7.32s/it] 81%|████████  | 471/585 [04:33<09:54,  5.22s/it] 81%|████████  | 472/585 [04:34<07:02,  3.74s/it] 81%|████████  | 473/585 [04:34<05:03,  2.71s/it] 81%|████████  | 474/585 [04:34<03:40,  1.98s/it] 81%|████████  | 475/585 [04:35<02:42,  1.48s/it] 81%|████████▏ | 476/585 [04:35<02:02,  1.12s/it] 82%|████████▏ | 477/585 [04:35<01:34,  1.14it/s] 82%|████████▏ | 478/585 [04:35<01:14,  1.43it/s] 82%|████████▏ | 479/585 [04:36<01:01,  1.73it/s] 82%|████████▏ | 480/585 [04:36<00:51,  2.03it/s] 82%|████████▏ | 481/585 [04:36<00:45,  2.27it/s] 82%|████████▏ | 482/585 [04:37<00:40,  2.52it/s] 83%|████████▎ | 483/585 [04:37<00:37,  2.73it/s] 83%|████████▎ | 484/585 [04:37<00:34,  2.90it/s] 83%|████████▎ | 485/585 [04:38<00:32,  3.03it/s] 83%|████████▎ | 486/585 [04:38<00:31,  3.13it/s] 83%|████████▎ | 487/585 [04:38<00:30,  3.21it/s] 83%|████████▎ | 488/585 [04:38<00:29,  3.26it/s] 84%|████████▎ | 489/585 [04:39<00:29,  3.30it/s] 84%|████████▍ | 490/585 [04:39<00:28,  3.33it/s] 84%|████████▍ | 491/585 [04:39<00:28,  3.35it/s] 84%|████████▍ | 492/585 [04:40<00:28,  3.26it/s] 84%|████████▍ | 493/585 [04:40<00:27,  3.30it/s] 84%|████████▍ | 494/585 [04:40<00:27,  3.33it/s] 85%|████████▍ | 495/585 [04:40<00:26,  3.35it/s] 85%|████████▍ | 496/585 [04:41<00:26,  3.37it/s] 85%|████████▍ | 497/585 [04:41<00:26,  3.37it/s] 85%|████████▌ | 498/585 [04:41<00:25,  3.38it/s] 85%|████████▌ | 499/585 [04:42<00:25,  3.39it/s] 85%|████████▌ | 500/585 [04:42<00:25,  3.39it/s]                                                  85%|████████▌ | 500/585 [04:42<00:25,  3.39it/s] 86%|████████▌ | 501/585 [04:42<00:24,  3.39it/s] 86%|████████▌ | 502/585 [04:43<00:24,  3.40it/s] 86%|████████▌ | 503/585 [04:43<00:24,  3.33it/s] 86%|████████▌ | 504/585 [04:43<00:24,  3.35it/s] 86%|████████▋ | 505/585 [04:43<00:24,  3.30it/s] 86%|████████▋ | 506/585 [04:44<00:23,  3.33it/s] 87%|████████▋ | 507/585 [04:44<00:23,  3.35it/s] 87%|████████▋ | 508/585 [04:44<00:22,  3.37it/s] 87%|████████▋ | 509/585 [04:45<00:22,  3.38it/s] 87%|████████▋ | 510/585 [04:45<00:22,  3.40it/s] 87%|████████▋ | 511/585 [04:45<00:21,  3.42it/s] 88%|████████▊ | 512/585 [04:46<00:22,  3.28it/s] 88%|████████▊ | 513/585 [04:46<00:25,  2.87it/s] 88%|████████▊ | 514/585 [04:46<00:24,  2.94it/s] 88%|████████▊ | 515/585 [04:47<00:22,  3.08it/s] 88%|████████▊ | 516/585 [04:47<00:21,  3.18it/s] 88%|████████▊ | 517/585 [04:47<00:20,  3.26it/s] 89%|████████▊ | 518/585 [04:47<00:20,  3.31it/s] 89%|████████▊ | 519/585 [04:48<00:19,  3.35it/s] 89%|████████▉ | 520/585 [04:48<00:19,  3.38it/s] 89%|████████▉ | 521/585 [04:48<00:18,  3.40it/s] 89%|████████▉ | 522/585 [04:49<00:18,  3.41it/s] 89%|████████▉ | 523/585 [04:49<00:18,  3.30it/s] 90%|████████▉ | 524/585 [04:49<00:18,  3.34it/s] 90%|████████▉ | 525/585 [04:50<00:17,  3.37it/s] 90%|████████▉ | 526/585 [04:50<00:17,  3.40it/s] 90%|█████████ | 527/585 [04:50<00:17,  3.41it/s] 90%|█████████ | 528/585 [04:50<00:16,  3.43it/s] 90%|█████████ | 529/585 [04:51<00:16,  3.44it/s] 91%|█████████ | 530/585 [04:51<00:15,  3.44it/s] 91%|█████████ | 531/585 [04:51<00:15,  3.45it/s] 91%|█████████ | 532/585 [04:52<00:15,  3.45it/s] 91%|█████████ | 533/585 [04:52<00:15,  3.46it/s] 91%|█████████▏| 534/585 [04:52<00:15,  3.27it/s] 91%|█████████▏| 535/585 [04:52<00:15,  3.33it/s] 92%|█████████▏| 536/585 [04:53<00:14,  3.37it/s] 92%|█████████▏| 537/585 [04:53<00:14,  3.39it/s] 92%|█████████▏| 538/585 [04:53<00:13,  3.41it/s] 92%|█████████▏| 539/585 [04:54<00:13,  3.42it/s] 92%|█████████▏| 540/585 [04:54<00:13,  3.43it/s] 92%|█████████▏| 541/585 [04:54<00:12,  3.44it/s] 93%|█████████▎| 542/585 [04:55<00:12,  3.44it/s] 93%|█████████▎| 543/585 [04:55<00:12,  3.44it/s] 93%|█████████▎| 544/585 [04:55<00:11,  3.45it/s] 93%|█████████▎| 545/585 [04:55<00:11,  3.44it/s] 93%|█████████▎| 546/585 [04:56<00:11,  3.45it/s] 94%|█████████▎| 547/585 [04:56<00:11,  3.45it/s] 94%|█████████▎| 548/585 [04:56<00:10,  3.46it/s] 94%|█████████▍| 549/585 [04:57<00:10,  3.45it/s] 94%|█████████▍| 550/585 [04:57<00:10,  3.45it/s] 94%|█████████▍| 551/585 [04:57<00:09,  3.45it/s] 94%|█████████▍| 552/585 [04:57<00:09,  3.45it/s] 95%|█████████▍| 553/585 [04:58<00:09,  3.45it/s] 95%|█████████▍| 554/585 [04:58<00:09,  3.34it/s] 95%|█████████▍| 555/585 [04:58<00:08,  3.37it/s] 95%|█████████▌| 556/585 [04:59<00:08,  3.40it/s] 95%|█████████▌| 557/585 [04:59<00:08,  3.41it/s] 95%|█████████▌| 558/585 [04:59<00:07,  3.43it/s] 96%|█████████▌| 559/585 [04:59<00:07,  3.45it/s] 96%|█████████▌| 560/585 [05:00<00:07,  3.45it/s] 96%|█████████▌| 561/585 [05:00<00:06,  3.46it/s] 96%|█████████▌| 562/585 [05:00<00:06,  3.47it/s] 96%|█████████▌| 563/585 [05:01<00:06,  3.47it/s] 96%|█████████▋| 564/585 [05:01<00:06,  3.47it/s] 97%|█████████▋| 565/585 [05:01<00:06,  3.29it/s] 97%|█████████▋| 566/585 [05:02<00:05,  3.34it/s] 97%|█████████▋| 567/585 [05:02<00:05,  3.37it/s] 97%|█████████▋| 568/585 [05:02<00:05,  3.39it/s] 97%|█████████▋| 569/585 [05:02<00:04,  3.41it/s] 97%|█████████▋| 570/585 [05:03<00:04,  3.42it/s] 98%|█████████▊| 571/585 [05:03<00:04,  3.42it/s] 98%|█████████▊| 572/585 [05:03<00:03,  3.43it/s] 98%|█████████▊| 573/585 [05:04<00:03,  3.43it/s] 98%|█████████▊| 574/585 [05:04<00:03,  3.44it/s] 98%|█████████▊| 575/585 [05:04<00:02,  3.44it/s] 98%|█████████▊| 576/585 [05:04<00:02,  3.37it/s] 99%|█████████▊| 577/585 [05:05<00:02,  3.39it/s] 99%|█████████▉| 578/585 [05:05<00:02,  3.41it/s] 99%|█████████▉| 579/585 [05:05<00:01,  3.42it/s] 99%|█████████▉| 580/585 [05:06<00:01,  3.43it/s] 99%|█████████▉| 581/585 [05:06<00:01,  3.44it/s] 99%|█████████▉| 582/585 [05:06<00:00,  3.44it/s]100%|█████████▉| 583/585 [05:06<00:00,  3.44it/s]100%|█████████▉| 584/585 [05:07<00:00,  3.44it/s]100%|██████████| 585/585 [05:07<00:00,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 06:06:34,797 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:06:34,797 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 06:06:34,797 >>   Batch size = 8
{'eval_loss': 0.9407033324241638, 'eval_runtime': 24.11, 'eval_samples_per_second': 355.372, 'eval_steps_per_second': 44.421, 'epoch': 4.0}
{'loss': 0.6917, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:19, 55.92it/s][A
  1%|          | 12/1071 [00:00<00:21, 48.23it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 46.74it/s][A
  2%|▏         | 22/1071 [00:00<00:23, 45.58it/s][A
  3%|▎         | 27/1071 [00:00<00:23, 45.18it/s][A
  3%|▎         | 32/1071 [00:00<00:23, 44.99it/s][A
  3%|▎         | 37/1071 [00:00<00:23, 44.74it/s][A
  4%|▍         | 42/1071 [00:00<00:23, 44.59it/s][A
  4%|▍         | 47/1071 [00:01<00:22, 44.75it/s][A
  5%|▍         | 52/1071 [00:01<00:22, 44.84it/s][A
  5%|▌         | 57/1071 [00:01<00:22, 45.10it/s][A
  6%|▌         | 62/1071 [00:01<00:22, 44.78it/s][A
  6%|▋         | 67/1071 [00:01<00:22, 44.61it/s][A
  7%|▋         | 72/1071 [00:01<00:22, 44.53it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 44.51it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 44.41it/s][A
  8%|▊         | 87/1071 [00:01<00:22, 44.50it/s][A
  9%|▊         | 92/1071 [00:02<00:21, 44.53it/s][A
  9%|▉         | 97/1071 [00:02<00:21, 44.61it/s][A
 10%|▉         | 102/1071 [00:02<00:21, 44.75it/s][A
 10%|▉         | 107/1071 [00:02<00:21, 44.76it/s][A
 10%|█         | 112/1071 [00:02<00:21, 44.62it/s][A
 11%|█         | 117/1071 [00:02<00:21, 44.68it/s][A
 11%|█▏        | 122/1071 [00:02<00:21, 44.75it/s][A
 12%|█▏        | 127/1071 [00:02<00:21, 44.63it/s][A
 12%|█▏        | 132/1071 [00:02<00:21, 44.54it/s][A
 13%|█▎        | 137/1071 [00:03<00:21, 44.41it/s][A
 13%|█▎        | 142/1071 [00:03<00:20, 44.59it/s][A
 14%|█▎        | 147/1071 [00:03<00:21, 43.04it/s][A
 14%|█▍        | 152/1071 [00:03<00:21, 43.70it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 43.93it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 44.20it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 44.30it/s][A
 16%|█▌        | 172/1071 [00:03<00:20, 44.32it/s][A
 17%|█▋        | 177/1071 [00:03<00:20, 44.30it/s][A
 17%|█▋        | 182/1071 [00:04<00:19, 44.52it/s][A
 17%|█▋        | 187/1071 [00:04<00:19, 44.42it/s][A
 18%|█▊        | 192/1071 [00:04<00:19, 44.64it/s][A
 18%|█▊        | 197/1071 [00:04<00:19, 44.60it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 44.73it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 44.85it/s][A
 20%|█▉        | 212/1071 [00:04<00:19, 44.81it/s][A
 20%|██        | 217/1071 [00:04<00:19, 44.76it/s][A
 21%|██        | 222/1071 [00:04<00:19, 44.57it/s][A
 21%|██        | 227/1071 [00:05<00:18, 44.62it/s][A
 22%|██▏       | 232/1071 [00:05<00:18, 44.48it/s][A
 22%|██▏       | 237/1071 [00:05<00:18, 44.67it/s][A
 23%|██▎       | 242/1071 [00:05<00:18, 44.64it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 44.59it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 44.83it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 44.74it/s][A
 24%|██▍       | 262/1071 [00:05<00:18, 44.52it/s][A
 25%|██▍       | 267/1071 [00:05<00:18, 44.44it/s][A
 25%|██▌       | 272/1071 [00:06<00:18, 44.36it/s][A
 26%|██▌       | 277/1071 [00:06<00:17, 44.44it/s][A
 26%|██▋       | 282/1071 [00:06<00:18, 42.21it/s][A
 27%|██▋       | 287/1071 [00:06<00:18, 42.92it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 43.50it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 44.10it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 44.10it/s][A
 29%|██▊       | 307/1071 [00:06<00:17, 44.11it/s][A
 29%|██▉       | 312/1071 [00:07<00:17, 44.05it/s][A
 30%|██▉       | 317/1071 [00:07<00:17, 44.17it/s][A
 30%|███       | 322/1071 [00:07<00:17, 44.04it/s][A
 31%|███       | 327/1071 [00:07<00:16, 44.36it/s][A
 31%|███       | 332/1071 [00:07<00:16, 44.67it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 44.77it/s][A
 32%|███▏      | 342/1071 [00:07<00:16, 44.97it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 44.75it/s][A
 33%|███▎      | 352/1071 [00:07<00:16, 44.40it/s][A
 33%|███▎      | 357/1071 [00:08<00:16, 44.26it/s][A
 34%|███▍      | 362/1071 [00:08<00:15, 44.38it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.16it/s][A
 35%|███▍      | 372/1071 [00:08<00:15, 44.54it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 44.70it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 44.80it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 44.84it/s][A
 37%|███▋      | 392/1071 [00:08<00:15, 44.70it/s][A
 37%|███▋      | 397/1071 [00:08<00:15, 44.44it/s][A
 38%|███▊      | 402/1071 [00:09<00:15, 44.25it/s][A
 38%|███▊      | 407/1071 [00:09<00:14, 44.44it/s][A
 38%|███▊      | 412/1071 [00:09<00:14, 44.48it/s][A
 39%|███▉      | 417/1071 [00:09<00:15, 43.53it/s][A
 39%|███▉      | 422/1071 [00:09<00:14, 44.01it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 44.31it/s][A
 40%|████      | 432/1071 [00:09<00:14, 44.67it/s][A
 41%|████      | 437/1071 [00:09<00:14, 44.60it/s][A
 41%|████▏     | 442/1071 [00:09<00:14, 44.61it/s][A
 42%|████▏     | 447/1071 [00:10<00:14, 44.45it/s][A
 42%|████▏     | 452/1071 [00:10<00:13, 44.29it/s][A
 43%|████▎     | 457/1071 [00:10<00:13, 44.37it/s][A
 43%|████▎     | 462/1071 [00:10<00:13, 44.44it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 44.74it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 44.85it/s][A
 45%|████▍     | 477/1071 [00:10<00:13, 45.05it/s][A
 45%|████▌     | 482/1071 [00:10<00:13, 44.80it/s][A
 45%|████▌     | 487/1071 [00:10<00:13, 44.88it/s][A
 46%|████▌     | 492/1071 [00:11<00:12, 44.69it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.61it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.44it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 44.40it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 44.66it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 44.58it/s][A
 49%|████▊     | 522/1071 [00:11<00:12, 44.60it/s][A
 49%|████▉     | 527/1071 [00:11<00:12, 44.76it/s][A
 50%|████▉     | 532/1071 [00:11<00:12, 44.70it/s][A
 50%|█████     | 537/1071 [00:12<00:11, 44.66it/s][A
 51%|█████     | 542/1071 [00:12<00:11, 44.57it/s][A
 51%|█████     | 547/1071 [00:12<00:11, 44.52it/s][A
 52%|█████▏    | 552/1071 [00:12<00:11, 43.46it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 43.85it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 44.39it/s][A
 53%|█████▎    | 567/1071 [00:12<00:11, 44.61it/s][A
 53%|█████▎    | 572/1071 [00:12<00:11, 44.68it/s][A
 54%|█████▍    | 577/1071 [00:12<00:11, 44.56it/s][A
 54%|█████▍    | 582/1071 [00:13<00:10, 44.46it/s][A
 55%|█████▍    | 587/1071 [00:13<00:10, 44.55it/s][A
 55%|█████▌    | 592/1071 [00:13<00:10, 44.41it/s][A
 56%|█████▌    | 597/1071 [00:13<00:10, 44.36it/s][A
 56%|█████▌    | 602/1071 [00:13<00:10, 44.46it/s][A
 57%|█████▋    | 607/1071 [00:13<00:10, 44.58it/s][A
 57%|█████▋    | 612/1071 [00:13<00:10, 44.41it/s][A
 58%|█████▊    | 617/1071 [00:13<00:10, 44.69it/s][A
 58%|█████▊    | 622/1071 [00:13<00:10, 44.84it/s][A
 59%|█████▊    | 627/1071 [00:14<00:09, 44.78it/s][A
 59%|█████▉    | 632/1071 [00:14<00:09, 44.53it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.38it/s][A
 60%|█████▉    | 642/1071 [00:14<00:09, 44.21it/s][A
 60%|██████    | 647/1071 [00:14<00:09, 44.40it/s][A
 61%|██████    | 652/1071 [00:14<00:09, 44.61it/s][A
 61%|██████▏   | 657/1071 [00:14<00:09, 44.75it/s][A
 62%|██████▏   | 662/1071 [00:14<00:09, 44.77it/s][A
 62%|██████▏   | 667/1071 [00:14<00:09, 44.73it/s][A
 63%|██████▎   | 672/1071 [00:15<00:08, 44.58it/s][A
 63%|██████▎   | 677/1071 [00:15<00:08, 44.52it/s][A
 64%|██████▎   | 682/1071 [00:15<00:08, 44.33it/s][A
 64%|██████▍   | 687/1071 [00:15<00:08, 42.83it/s][A
 65%|██████▍   | 692/1071 [00:15<00:08, 43.41it/s][A
 65%|██████▌   | 697/1071 [00:15<00:08, 43.72it/s][A
 66%|██████▌   | 702/1071 [00:15<00:08, 44.06it/s][A
 66%|██████▌   | 707/1071 [00:15<00:08, 44.28it/s][A
 66%|██████▋   | 712/1071 [00:16<00:08, 44.41it/s][A
 67%|██████▋   | 717/1071 [00:16<00:07, 44.46it/s][A
 67%|██████▋   | 722/1071 [00:16<00:07, 44.45it/s][A
 68%|██████▊   | 727/1071 [00:16<00:07, 44.18it/s][A
 68%|██████▊   | 732/1071 [00:16<00:07, 44.27it/s][A
 69%|██████▉   | 737/1071 [00:16<00:07, 44.36it/s][A
 69%|██████▉   | 742/1071 [00:16<00:07, 44.67it/s][A
 70%|██████▉   | 747/1071 [00:16<00:07, 44.71it/s][A
 70%|███████   | 752/1071 [00:16<00:07, 44.66it/s][A
 71%|███████   | 757/1071 [00:17<00:07, 44.72it/s][A
 71%|███████   | 762/1071 [00:17<00:06, 44.67it/s][A
 72%|███████▏  | 767/1071 [00:17<00:06, 44.54it/s][A
 72%|███████▏  | 772/1071 [00:17<00:06, 44.31it/s][A
 73%|███████▎  | 777/1071 [00:17<00:06, 44.29it/s][A
 73%|███████▎  | 782/1071 [00:17<00:06, 44.44it/s][A
 73%|███████▎  | 787/1071 [00:17<00:06, 44.35it/s][A
 74%|███████▍  | 792/1071 [00:17<00:06, 44.62it/s][A
 74%|███████▍  | 797/1071 [00:17<00:06, 44.81it/s][A
 75%|███████▍  | 802/1071 [00:18<00:05, 44.84it/s][A
 75%|███████▌  | 807/1071 [00:18<00:05, 44.63it/s][A
 76%|███████▌  | 812/1071 [00:18<00:05, 44.56it/s][A
 76%|███████▋  | 817/1071 [00:18<00:05, 44.40it/s][A
 77%|███████▋  | 822/1071 [00:18<00:05, 44.35it/s][A
 77%|███████▋  | 827/1071 [00:18<00:05, 44.56it/s][A
 78%|███████▊  | 832/1071 [00:18<00:05, 44.80it/s][A
 78%|███████▊  | 837/1071 [00:18<00:05, 44.74it/s][A
 79%|███████▊  | 842/1071 [00:18<00:05, 44.68it/s][A
 79%|███████▉  | 847/1071 [00:19<00:05, 44.57it/s][A
 80%|███████▉  | 852/1071 [00:19<00:04, 44.55it/s][A
 80%|████████  | 857/1071 [00:19<00:04, 44.39it/s][A
 80%|████████  | 862/1071 [00:19<00:04, 44.20it/s][A
 81%|████████  | 867/1071 [00:19<00:04, 44.27it/s][A
 81%|████████▏ | 872/1071 [00:19<00:04, 44.35it/s][A
 82%|████████▏ | 877/1071 [00:19<00:04, 44.61it/s][A
 82%|████████▏ | 882/1071 [00:19<00:04, 44.67it/s][A
 83%|████████▎ | 887/1071 [00:19<00:04, 44.87it/s][A
 83%|████████▎ | 892/1071 [00:20<00:03, 44.81it/s][A
 84%|████████▍ | 897/1071 [00:20<00:03, 44.63it/s][A
 84%|████████▍ | 902/1071 [00:20<00:03, 44.33it/s][A
 85%|████████▍ | 907/1071 [00:20<00:03, 44.24it/s][A
 85%|████████▌ | 912/1071 [00:20<00:03, 44.48it/s][A
 86%|████████▌ | 917/1071 [00:20<00:03, 44.60it/s][A
 86%|████████▌ | 922/1071 [00:20<00:03, 44.67it/s][A
 87%|████████▋ | 927/1071 [00:20<00:03, 44.73it/s][A
 87%|████████▋ | 932/1071 [00:20<00:03, 44.65it/s][A
 87%|████████▋ | 937/1071 [00:21<00:02, 44.86it/s][A
 88%|████████▊ | 942/1071 [00:21<00:02, 44.66it/s][A
 88%|████████▊ | 947/1071 [00:21<00:02, 44.47it/s][A
 89%|████████▉ | 952/1071 [00:21<00:02, 43.86it/s][A
 89%|████████▉ | 957/1071 [00:21<00:02, 44.24it/s][A
 90%|████████▉ | 962/1071 [00:21<00:02, 44.36it/s][A
 90%|█████████ | 967/1071 [00:21<00:02, 44.48it/s][A
 91%|█████████ | 972/1071 [00:21<00:02, 44.59it/s][A
 91%|█████████ | 977/1071 [00:21<00:02, 44.89it/s][A
 92%|█████████▏| 982/1071 [00:22<00:01, 44.87it/s][A
 92%|█████████▏| 987/1071 [00:22<00:01, 44.87it/s][A
 93%|█████████▎| 992/1071 [00:22<00:01, 44.61it/s][A
 93%|█████████▎| 997/1071 [00:22<00:01, 44.56it/s][A
 94%|█████████▎| 1002/1071 [00:22<00:01, 44.56it/s][A
 94%|█████████▍| 1007/1071 [00:22<00:01, 44.57it/s][A
 94%|█████████▍| 1012/1071 [00:22<00:01, 44.75it/s][A
 95%|█████████▍| 1017/1071 [00:22<00:01, 44.88it/s][A
 95%|█████████▌| 1022/1071 [00:22<00:01, 44.92it/s][A
 96%|█████████▌| 1027/1071 [00:23<00:00, 44.99it/s][A
 96%|█████████▋| 1032/1071 [00:23<00:00, 44.72it/s][A
 97%|█████████▋| 1037/1071 [00:23<00:00, 44.44it/s][A
 97%|█████████▋| 1042/1071 [00:23<00:00, 44.28it/s][A
 98%|█████████▊| 1047/1071 [00:23<00:00, 44.23it/s][A
 98%|█████████▊| 1052/1071 [00:23<00:00, 44.41it/s][A
 99%|█████████▊| 1057/1071 [00:23<00:00, 44.56it/s][A
 99%|█████████▉| 1062/1071 [00:23<00:00, 44.77it/s][A
100%|█████████▉| 1067/1071 [00:23<00:00, 44.94it/s][A                                                 
                                                   [A100%|██████████| 585/585 [05:31<00:00,  3.44it/s]
100%|██████████| 1071/1071 [00:24<00:00, 44.94it/s][A
                                                   [A[INFO|trainer.py:1894] 2023-08-29 06:06:59,087 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 06:06:59,266 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:07:02,196 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:07:02,311 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:07:02,368 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 06:07:08,689 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 06:07:08,720 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-117 (score: 0.9229092597961426).
                                                 100%|██████████| 585/585 [05:49<00:00,  3.44it/s]100%|██████████| 585/585 [05:49<00:00,  1.67it/s]
[INFO|trainer.py:1894] 2023-08-29 06:07:16,838 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 06:07:16,979 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:07:19,771 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:07:19,988 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:07:20,099 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 06:07:20,901 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:20,901 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:20,901 >>   train_loss               =     0.6868
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:20,901 >>   train_runtime            = 0:05:49.38
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:20,901 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:20,901 >>   train_samples_per_second =    107.331
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:20,901 >>   train_steps_per_second   =      1.674
{'eval_loss': 0.944893479347229, 'eval_runtime': 24.0805, 'eval_samples_per_second': 355.806, 'eval_steps_per_second': 44.476, 'epoch': 5.0}
{'train_runtime': 349.3866, 'train_samples_per_second': 107.331, 'train_steps_per_second': 1.674, 'train_loss': 0.6867945222773104, 'epoch': 5.0}
08/29/2023 06:07:21 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 06:07:21,340 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:07:21,340 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 06:07:21,340 >>   Batch size = 8
  0%|          | 0/1071 [00:00<?, ?it/s]  1%|          | 6/1071 [00:00<00:19, 55.42it/s]  1%|          | 12/1071 [00:00<00:21, 49.30it/s]  2%|▏         | 17/1071 [00:00<00:22, 47.84it/s]  2%|▏         | 22/1071 [00:00<00:22, 46.89it/s]  3%|▎         | 27/1071 [00:00<00:22, 46.41it/s]  3%|▎         | 32/1071 [00:00<00:22, 46.10it/s]  3%|▎         | 37/1071 [00:00<00:22, 46.10it/s]  4%|▍         | 42/1071 [00:00<00:22, 45.61it/s]  4%|▍         | 47/1071 [00:01<00:22, 45.07it/s]  5%|▍         | 52/1071 [00:01<00:22, 44.73it/s]  5%|▌         | 57/1071 [00:01<00:22, 44.74it/s]  6%|▌         | 62/1071 [00:01<00:22, 45.01it/s]  6%|▋         | 67/1071 [00:01<00:22, 45.22it/s]  7%|▋         | 72/1071 [00:01<00:23, 41.92it/s]  7%|▋         | 77/1071 [00:01<00:23, 43.01it/s]  8%|▊         | 82/1071 [00:01<00:22, 43.79it/s]  8%|▊         | 87/1071 [00:01<00:22, 44.15it/s]  9%|▊         | 92/1071 [00:02<00:22, 44.35it/s]  9%|▉         | 97/1071 [00:02<00:21, 44.47it/s] 10%|▉         | 102/1071 [00:02<00:21, 44.49it/s] 10%|▉         | 107/1071 [00:02<00:21, 44.64it/s] 10%|█         | 112/1071 [00:02<00:23, 40.34it/s] 11%|█         | 117/1071 [00:02<00:22, 41.88it/s] 11%|█▏        | 122/1071 [00:02<00:22, 42.87it/s] 12%|█▏        | 127/1071 [00:02<00:21, 43.72it/s] 12%|█▏        | 132/1071 [00:02<00:21, 44.25it/s] 13%|█▎        | 137/1071 [00:03<00:20, 44.68it/s] 13%|█▎        | 142/1071 [00:03<00:20, 44.81it/s] 14%|█▎        | 147/1071 [00:03<00:20, 44.98it/s] 14%|█▍        | 152/1071 [00:03<00:20, 44.45it/s] 15%|█▍        | 157/1071 [00:03<00:20, 44.14it/s] 15%|█▌        | 162/1071 [00:03<00:20, 44.32it/s] 16%|█▌        | 167/1071 [00:03<00:20, 44.68it/s] 16%|█▌        | 172/1071 [00:03<00:20, 44.95it/s] 17%|█▋        | 177/1071 [00:03<00:19, 45.28it/s] 17%|█▋        | 182/1071 [00:04<00:19, 45.41it/s] 17%|█▋        | 187/1071 [00:04<00:19, 45.28it/s] 18%|█▊        | 192/1071 [00:04<00:19, 45.15it/s] 18%|█▊        | 197/1071 [00:04<00:19, 44.81it/s] 19%|█▉        | 202/1071 [00:04<00:19, 44.69it/s] 19%|█▉        | 207/1071 [00:04<00:19, 44.74it/s] 20%|█▉        | 212/1071 [00:04<00:19, 44.82it/s] 20%|██        | 217/1071 [00:04<00:19, 44.94it/s] 21%|██        | 222/1071 [00:04<00:18, 45.09it/s] 21%|██        | 227/1071 [00:05<00:18, 45.19it/s] 22%|██▏       | 232/1071 [00:05<00:18, 45.14it/s] 22%|██▏       | 237/1071 [00:05<00:18, 45.07it/s] 23%|██▎       | 242/1071 [00:05<00:18, 44.81it/s] 23%|██▎       | 247/1071 [00:05<00:19, 42.32it/s] 24%|██▎       | 252/1071 [00:05<00:18, 43.23it/s] 24%|██▍       | 257/1071 [00:05<00:18, 43.93it/s] 24%|██▍       | 262/1071 [00:05<00:18, 44.42it/s] 25%|██▍       | 267/1071 [00:05<00:17, 44.69it/s] 25%|██▌       | 272/1071 [00:06<00:17, 44.95it/s] 26%|██▌       | 277/1071 [00:06<00:17, 44.91it/s] 26%|██▋       | 282/1071 [00:06<00:17, 44.98it/s] 27%|██▋       | 287/1071 [00:06<00:17, 44.30it/s] 27%|██▋       | 292/1071 [00:06<00:17, 44.37it/s] 28%|██▊       | 297/1071 [00:06<00:17, 44.62it/s] 28%|██▊       | 302/1071 [00:06<00:17, 44.91it/s] 29%|██▊       | 307/1071 [00:06<00:17, 44.91it/s] 29%|██▉       | 312/1071 [00:06<00:16, 45.25it/s] 30%|██▉       | 317/1071 [00:07<00:16, 45.31it/s] 30%|███       | 322/1071 [00:07<00:16, 45.28it/s] 31%|███       | 327/1071 [00:07<00:16, 45.07it/s] 31%|███       | 332/1071 [00:07<00:16, 44.89it/s] 31%|███▏      | 337/1071 [00:07<00:16, 44.77it/s] 32%|███▏      | 342/1071 [00:07<00:16, 44.83it/s] 32%|███▏      | 347/1071 [00:07<00:16, 44.92it/s] 33%|███▎      | 352/1071 [00:07<00:15, 45.09it/s] 33%|███▎      | 357/1071 [00:07<00:15, 45.36it/s] 34%|███▍      | 362/1071 [00:08<00:15, 45.31it/s] 34%|███▍      | 367/1071 [00:08<00:15, 45.30it/s] 35%|███▍      | 372/1071 [00:08<00:15, 44.94it/s] 35%|███▌      | 377/1071 [00:08<00:15, 44.93it/s] 36%|███▌      | 382/1071 [00:08<00:18, 36.91it/s] 36%|███▌      | 387/1071 [00:08<00:17, 39.22it/s] 37%|███▋      | 392/1071 [00:08<00:16, 40.95it/s] 37%|███▋      | 397/1071 [00:08<00:15, 42.14it/s] 38%|███▊      | 402/1071 [00:09<00:15, 43.05it/s] 38%|███▊      | 407/1071 [00:09<00:15, 43.80it/s] 38%|███▊      | 412/1071 [00:09<00:14, 44.29it/s] 39%|███▉      | 417/1071 [00:09<00:14, 44.71it/s] 39%|███▉      | 422/1071 [00:09<00:14, 44.33it/s] 40%|███▉      | 427/1071 [00:09<00:14, 44.08it/s] 40%|████      | 432/1071 [00:09<00:14, 44.32it/s] 41%|████      | 437/1071 [00:09<00:14, 44.55it/s] 41%|████▏     | 442/1071 [00:09<00:14, 44.54it/s] 42%|████▏     | 447/1071 [00:10<00:13, 44.66it/s] 42%|████▏     | 452/1071 [00:10<00:13, 44.91it/s] 43%|████▎     | 457/1071 [00:10<00:13, 45.16it/s] 43%|████▎     | 462/1071 [00:10<00:13, 45.20it/s] 44%|████▎     | 467/1071 [00:10<00:13, 45.14it/s] 44%|████▍     | 472/1071 [00:10<00:13, 44.96it/s] 45%|████▍     | 477/1071 [00:10<00:13, 44.80it/s] 45%|████▌     | 482/1071 [00:10<00:13, 44.85it/s] 45%|████▌     | 487/1071 [00:10<00:12, 44.93it/s] 46%|████▌     | 492/1071 [00:11<00:14, 40.90it/s] 46%|████▋     | 497/1071 [00:11<00:13, 42.35it/s] 47%|████▋     | 502/1071 [00:11<00:13, 43.16it/s] 47%|████▋     | 507/1071 [00:11<00:12, 43.92it/s] 48%|████▊     | 512/1071 [00:11<00:14, 39.71it/s] 48%|████▊     | 517/1071 [00:11<00:13, 41.39it/s] 49%|████▊     | 522/1071 [00:11<00:12, 42.49it/s] 49%|████▉     | 527/1071 [00:11<00:12, 43.36it/s] 50%|████▉     | 532/1071 [00:12<00:12, 43.58it/s] 50%|█████     | 537/1071 [00:12<00:12, 43.97it/s] 51%|█████     | 542/1071 [00:12<00:11, 44.35it/s] 51%|█████     | 547/1071 [00:12<00:11, 44.55it/s] 52%|█████▏    | 552/1071 [00:12<00:11, 44.23it/s] 52%|█████▏    | 557/1071 [00:12<00:11, 44.39it/s] 52%|█████▏    | 562/1071 [00:12<00:11, 44.59it/s] 53%|█████▎    | 567/1071 [00:12<00:11, 44.71it/s] 53%|█████▎    | 572/1071 [00:12<00:11, 44.89it/s] 54%|█████▍    | 577/1071 [00:13<00:11, 44.84it/s] 54%|█████▍    | 582/1071 [00:13<00:10, 44.88it/s] 55%|█████▍    | 587/1071 [00:13<00:10, 44.98it/s] 55%|█████▌    | 592/1071 [00:13<00:10, 44.71it/s] 56%|█████▌    | 597/1071 [00:13<00:10, 44.56it/s] 56%|█████▌    | 602/1071 [00:13<00:10, 44.50it/s] 57%|█████▋    | 607/1071 [00:13<00:10, 44.71it/s] 57%|█████▋    | 612/1071 [00:13<00:10, 44.71it/s] 58%|█████▊    | 617/1071 [00:13<00:10, 44.91it/s] 58%|█████▊    | 622/1071 [00:14<00:10, 44.89it/s] 59%|█████▊    | 627/1071 [00:14<00:09, 44.81it/s] 59%|█████▉    | 632/1071 [00:14<00:09, 44.92it/s] 59%|█████▉    | 637/1071 [00:14<00:09, 44.84it/s] 60%|█████▉    | 642/1071 [00:14<00:09, 44.75it/s] 60%|██████    | 647/1071 [00:14<00:09, 43.67it/s] 61%|██████    | 652/1071 [00:14<00:09, 44.17it/s] 61%|██████▏   | 657/1071 [00:14<00:09, 44.61it/s] 62%|██████▏   | 662/1071 [00:14<00:09, 44.59it/s] 62%|██████▏   | 667/1071 [00:15<00:09, 44.59it/s] 63%|██████▎   | 672/1071 [00:15<00:08, 44.51it/s] 63%|██████▎   | 677/1071 [00:15<00:08, 44.76it/s] 64%|██████▎   | 682/1071 [00:15<00:08, 44.64it/s] 64%|██████▍   | 687/1071 [00:15<00:08, 44.57it/s] 65%|██████▍   | 692/1071 [00:15<00:08, 44.48it/s] 65%|██████▌   | 697/1071 [00:15<00:08, 44.60it/s] 66%|██████▌   | 702/1071 [00:15<00:08, 44.89it/s] 66%|██████▌   | 707/1071 [00:15<00:08, 45.02it/s] 66%|██████▋   | 712/1071 [00:16<00:08, 44.87it/s] 67%|██████▋   | 717/1071 [00:16<00:07, 44.80it/s] 67%|██████▋   | 722/1071 [00:16<00:07, 44.65it/s] 68%|██████▊   | 727/1071 [00:16<00:07, 44.76it/s] 68%|██████▊   | 732/1071 [00:16<00:07, 44.61it/s] 69%|██████▉   | 737/1071 [00:16<00:07, 44.47it/s] 69%|██████▉   | 742/1071 [00:16<00:07, 44.65it/s] 70%|██████▉   | 747/1071 [00:16<00:07, 44.94it/s] 70%|███████   | 752/1071 [00:16<00:07, 44.82it/s] 71%|███████   | 757/1071 [00:17<00:06, 44.96it/s] 71%|███████   | 762/1071 [00:17<00:06, 44.96it/s] 72%|███████▏  | 767/1071 [00:17<00:06, 44.86it/s] 72%|███████▏  | 772/1071 [00:17<00:06, 44.87it/s] 73%|███████▎  | 777/1071 [00:17<00:06, 44.84it/s] 73%|███████▎  | 782/1071 [00:17<00:06, 44.06it/s] 73%|███████▎  | 787/1071 [00:17<00:06, 44.14it/s] 74%|███████▍  | 792/1071 [00:17<00:06, 44.43it/s] 74%|███████▍  | 797/1071 [00:17<00:06, 44.68it/s] 75%|███████▍  | 802/1071 [00:18<00:06, 44.74it/s] 75%|███████▌  | 807/1071 [00:18<00:05, 44.67it/s] 76%|███████▌  | 812/1071 [00:18<00:05, 44.49it/s] 76%|███████▋  | 817/1071 [00:18<00:05, 44.48it/s] 77%|███████▋  | 822/1071 [00:18<00:05, 44.60it/s] 77%|███████▋  | 827/1071 [00:18<00:05, 44.78it/s] 78%|███████▊  | 832/1071 [00:18<00:05, 44.84it/s] 78%|███████▊  | 837/1071 [00:18<00:05, 44.90it/s] 79%|███████▊  | 842/1071 [00:18<00:05, 45.07it/s] 79%|███████▉  | 847/1071 [00:19<00:04, 45.11it/s] 80%|███████▉  | 852/1071 [00:19<00:04, 44.98it/s] 80%|████████  | 857/1071 [00:19<00:04, 44.90it/s] 80%|████████  | 862/1071 [00:19<00:04, 44.86it/s] 81%|████████  | 867/1071 [00:19<00:04, 44.69it/s] 81%|████████▏ | 872/1071 [00:19<00:04, 44.66it/s] 82%|████████▏ | 877/1071 [00:19<00:04, 44.64it/s] 82%|████████▏ | 882/1071 [00:19<00:04, 44.79it/s] 83%|████████▎ | 887/1071 [00:19<00:04, 44.85it/s] 83%|████████▎ | 892/1071 [00:20<00:03, 44.83it/s] 84%|████████▍ | 897/1071 [00:20<00:03, 44.62it/s] 84%|████████▍ | 902/1071 [00:20<00:03, 44.78it/s] 85%|████████▍ | 907/1071 [00:20<00:03, 44.82it/s] 85%|████████▌ | 912/1071 [00:20<00:03, 44.64it/s] 86%|████████▌ | 917/1071 [00:20<00:03, 43.73it/s] 86%|████████▌ | 922/1071 [00:20<00:03, 44.15it/s] 87%|████████▋ | 927/1071 [00:20<00:03, 44.42it/s] 87%|████████▋ | 932/1071 [00:20<00:03, 44.70it/s] 87%|████████▋ | 937/1071 [00:21<00:02, 44.78it/s] 88%|████████▊ | 942/1071 [00:21<00:02, 44.69it/s] 88%|████████▊ | 947/1071 [00:21<00:02, 44.53it/s] 89%|████████▉ | 952/1071 [00:21<00:02, 44.47it/s] 89%|████████▉ | 957/1071 [00:21<00:02, 44.42it/s] 90%|████████▉ | 962/1071 [00:21<00:02, 44.54it/s] 90%|█████████ | 967/1071 [00:21<00:02, 44.74it/s] 91%|█████████ | 972/1071 [00:21<00:02, 44.83it/s] 91%|█████████ | 977/1071 [00:21<00:02, 44.84it/s] 92%|█████████▏| 982/1071 [00:22<00:01, 45.00it/s] 92%|█████████▏| 987/1071 [00:22<00:01, 44.81it/s] 93%|█████████▎| 992/1071 [00:22<00:01, 44.74it/s] 93%|█████████▎| 997/1071 [00:22<00:01, 44.66it/s] 94%|█████████▎| 1002/1071 [00:22<00:01, 44.81it/s] 94%|█████████▍| 1007/1071 [00:22<00:01, 44.52it/s] 94%|█████████▍| 1012/1071 [00:22<00:01, 44.69it/s] 95%|█████████▍| 1017/1071 [00:22<00:01, 44.89it/s] 95%|█████████▌| 1022/1071 [00:22<00:01, 44.98it/s] 96%|█████████▌| 1027/1071 [00:23<00:00, 44.87it/s] 96%|█████████▋| 1032/1071 [00:23<00:00, 44.86it/s] 97%|█████████▋| 1037/1071 [00:23<00:00, 44.69it/s] 97%|█████████▋| 1042/1071 [00:23<00:00, 44.56it/s] 98%|█████████▊| 1047/1071 [00:23<00:00, 44.71it/s] 98%|█████████▊| 1052/1071 [00:23<00:00, 43.28it/s] 99%|█████████▊| 1057/1071 [00:23<00:00, 43.87it/s] 99%|█████████▉| 1062/1071 [00:23<00:00, 44.28it/s]100%|█████████▉| 1067/1071 [00:24<00:00, 44.57it/s]100%|██████████| 1071/1071 [00:24<00:00, 44.43it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 06:07:45,462 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:45,462 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:45,462 >>   eval_loss               =     0.9229
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:45,462 >>   eval_runtime            = 0:00:24.12
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:45,462 >>   eval_samples            =       8568
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:45,462 >>   eval_samples_per_second =    355.197
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:45,462 >>   eval_steps_per_second   =       44.4
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:07:45,462 >>   perplexity              =     2.5166
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:07:56,212 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:07:56,215 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:07:56,215 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:07:56,215 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:07:56,215 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:07:56,941 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:07:56,942 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:07:57,561 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:07:58,739 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:07:58,739 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:08:01,650 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:08:01,660 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:08:01,661 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:08:01,661 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:08:01,661 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:08:02,389 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:08:02,390 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:08:02,986 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:08:03,213 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:08:03,213 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-585
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/checkpoint-117
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl', 'labels': ['country', 'place of death', 'production company', 'screenwriter', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 21439
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21539, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:06,  1.55it/s]Extractor Predicting: 12it [00:07,  1.51it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:11,  1.61it/s]Extractor Predicting: 20it [00:12,  1.64it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:14,  1.69it/s]Extractor Predicting: 24it [00:15,  1.63it/s]Extractor Predicting: 25it [00:15,  1.54it/s]Extractor Predicting: 26it [00:16,  1.52it/s]Extractor Predicting: 27it [00:17,  1.55it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:18,  1.61it/s]Extractor Predicting: 30it [00:18,  1.55it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:20,  1.61it/s]Extractor Predicting: 33it [00:20,  1.64it/s]Extractor Predicting: 34it [00:21,  1.67it/s]Extractor Predicting: 35it [00:21,  1.72it/s]Extractor Predicting: 36it [00:22,  1.68it/s]Extractor Predicting: 37it [00:22,  1.74it/s]Extractor Predicting: 38it [00:23,  1.74it/s]Extractor Predicting: 39it [00:24,  1.71it/s]Extractor Predicting: 40it [00:24,  1.72it/s]Extractor Predicting: 41it [00:25,  1.73it/s]Extractor Predicting: 42it [00:25,  1.72it/s]Extractor Predicting: 43it [00:26,  1.72it/s]Extractor Predicting: 44it [00:27,  1.67it/s]Extractor Predicting: 45it [00:27,  1.71it/s]Extractor Predicting: 46it [00:28,  1.67it/s]Extractor Predicting: 47it [00:28,  1.64it/s]Extractor Predicting: 48it [00:29,  1.67it/s]Extractor Predicting: 49it [00:30,  1.70it/s]Extractor Predicting: 50it [00:30,  1.73it/s]Extractor Predicting: 51it [00:31,  1.74it/s]Extractor Predicting: 52it [00:31,  1.74it/s]Extractor Predicting: 53it [00:32,  1.75it/s]Extractor Predicting: 54it [00:32,  1.75it/s]Extractor Predicting: 55it [00:33,  1.75it/s]Extractor Predicting: 56it [00:34,  1.76it/s]Extractor Predicting: 57it [00:34,  1.79it/s]Extractor Predicting: 58it [00:35,  1.82it/s]Extractor Predicting: 59it [00:35,  1.72it/s]Extractor Predicting: 60it [00:36,  1.72it/s]Extractor Predicting: 61it [00:36,  1.70it/s]Extractor Predicting: 62it [00:37,  1.70it/s]Extractor Predicting: 63it [00:38,  1.69it/s]Extractor Predicting: 64it [00:38,  1.73it/s]Extractor Predicting: 65it [00:39,  1.77it/s]Extractor Predicting: 66it [00:39,  1.76it/s]Extractor Predicting: 67it [00:40,  1.74it/s]Extractor Predicting: 68it [00:40,  1.74it/s]Extractor Predicting: 69it [00:41,  1.74it/s]Extractor Predicting: 70it [00:42,  1.69it/s]Extractor Predicting: 71it [00:42,  1.72it/s]Extractor Predicting: 72it [00:43,  1.69it/s]Extractor Predicting: 73it [00:43,  1.68it/s]Extractor Predicting: 74it [00:44,  1.65it/s]Extractor Predicting: 75it [00:45,  1.64it/s]Extractor Predicting: 76it [00:45,  1.61it/s]Extractor Predicting: 77it [00:46,  1.58it/s]Extractor Predicting: 78it [00:47,  1.61it/s]Extractor Predicting: 79it [00:47,  1.57it/s]Extractor Predicting: 80it [00:48,  1.52it/s]Extractor Predicting: 81it [00:49,  1.51it/s]Extractor Predicting: 82it [00:49,  1.49it/s]Extractor Predicting: 83it [00:50,  1.50it/s]Extractor Predicting: 84it [00:51,  1.48it/s]Extractor Predicting: 85it [00:51,  1.48it/s]Extractor Predicting: 86it [00:52,  1.49it/s]Extractor Predicting: 87it [00:53,  1.48it/s]Extractor Predicting: 88it [00:53,  1.49it/s]Extractor Predicting: 89it [00:54,  1.51it/s]Extractor Predicting: 90it [00:55,  1.52it/s]Extractor Predicting: 91it [00:55,  1.50it/s]Extractor Predicting: 92it [00:56,  1.49it/s]Extractor Predicting: 93it [00:57,  1.48it/s]Extractor Predicting: 94it [00:57,  1.48it/s]Extractor Predicting: 95it [00:58,  1.33it/s]Extractor Predicting: 96it [00:59,  1.39it/s]Extractor Predicting: 97it [01:00,  1.43it/s]Extractor Predicting: 98it [01:00,  1.46it/s]Extractor Predicting: 99it [01:01,  1.48it/s]Extractor Predicting: 100it [01:02,  1.48it/s]Extractor Predicting: 101it [01:02,  1.49it/s]Extractor Predicting: 102it [01:03,  1.45it/s]Extractor Predicting: 103it [01:04,  1.47it/s]Extractor Predicting: 104it [01:04,  1.47it/s]Extractor Predicting: 105it [01:05,  1.48it/s]Extractor Predicting: 106it [01:06,  1.48it/s]Extractor Predicting: 107it [01:06,  1.48it/s]Extractor Predicting: 108it [01:07,  1.50it/s]Extractor Predicting: 109it [01:08,  1.49it/s]Extractor Predicting: 110it [01:08,  1.53it/s]Extractor Predicting: 111it [01:09,  1.53it/s]Extractor Predicting: 112it [01:10,  1.54it/s]Extractor Predicting: 113it [01:10,  1.55it/s]Extractor Predicting: 114it [01:11,  1.54it/s]Extractor Predicting: 115it [01:12,  1.49it/s]Extractor Predicting: 116it [01:12,  1.50it/s]Extractor Predicting: 117it [01:13,  1.49it/s]Extractor Predicting: 118it [01:14,  1.49it/s]Extractor Predicting: 119it [01:14,  1.49it/s]Extractor Predicting: 120it [01:15,  1.51it/s]Extractor Predicting: 121it [01:16,  1.53it/s]Extractor Predicting: 122it [01:16,  1.46it/s]Extractor Predicting: 123it [01:17,  1.49it/s]Extractor Predicting: 124it [01:18,  1.49it/s]Extractor Predicting: 125it [01:18,  1.50it/s]Extractor Predicting: 126it [01:19,  1.53it/s]Extractor Predicting: 127it [01:19,  1.58it/s]Extractor Predicting: 128it [01:20,  1.60it/s]Extractor Predicting: 129it [01:21,  1.62it/s]Extractor Predicting: 130it [01:21,  1.59it/s]Extractor Predicting: 131it [01:22,  1.61it/s]Extractor Predicting: 132it [01:23,  1.58it/s]Extractor Predicting: 133it [01:23,  1.55it/s]Extractor Predicting: 134it [01:24,  1.56it/s]Extractor Predicting: 135it [01:25,  1.59it/s]Extractor Predicting: 136it [01:25,  1.64it/s]Extractor Predicting: 137it [01:26,  1.64it/s]Extractor Predicting: 138it [01:26,  1.60it/s]Extractor Predicting: 139it [01:27,  1.63it/s]Extractor Predicting: 140it [01:28,  1.64it/s]Extractor Predicting: 141it [01:28,  1.67it/s]Extractor Predicting: 142it [01:29,  1.64it/s]Extractor Predicting: 143it [01:29,  1.57it/s]Extractor Predicting: 144it [01:30,  1.58it/s]Extractor Predicting: 145it [01:31,  1.63it/s]Extractor Predicting: 146it [01:31,  1.66it/s]Extractor Predicting: 147it [01:32,  1.66it/s]Extractor Predicting: 148it [01:32,  1.65it/s]Extractor Predicting: 149it [01:33,  1.62it/s]Extractor Predicting: 150it [01:34,  1.64it/s]Extractor Predicting: 151it [01:34,  1.61it/s]Extractor Predicting: 152it [01:35,  1.61it/s]Extractor Predicting: 153it [01:36,  1.60it/s]Extractor Predicting: 154it [01:36,  1.55it/s]Extractor Predicting: 155it [01:37,  1.56it/s]Extractor Predicting: 156it [01:38,  1.57it/s]Extractor Predicting: 157it [01:38,  1.58it/s]Extractor Predicting: 158it [01:39,  1.55it/s]Extractor Predicting: 159it [01:39,  1.56it/s]Extractor Predicting: 160it [01:40,  1.58it/s]Extractor Predicting: 161it [01:41,  1.58it/s]Extractor Predicting: 162it [01:41,  1.63it/s]Extractor Predicting: 163it [01:42,  1.74it/s]Extractor Predicting: 164it [01:42,  1.86it/s]Extractor Predicting: 165it [01:43,  1.88it/s]Extractor Predicting: 166it [01:43,  1.83it/s]Extractor Predicting: 167it [01:44,  1.74it/s]Extractor Predicting: 168it [01:45,  1.67it/s]Extractor Predicting: 169it [01:45,  1.61it/s]Extractor Predicting: 170it [01:46,  1.63it/s]Extractor Predicting: 171it [01:46,  1.63it/s]Extractor Predicting: 172it [01:47,  1.62it/s]Extractor Predicting: 173it [01:48,  1.62it/s]Extractor Predicting: 174it [01:48,  1.60it/s]Extractor Predicting: 175it [01:49,  1.58it/s]Extractor Predicting: 176it [01:50,  1.58it/s]Extractor Predicting: 177it [01:50,  1.59it/s]Extractor Predicting: 178it [01:51,  1.61it/s]Extractor Predicting: 179it [01:52,  1.56it/s]Extractor Predicting: 180it [01:52,  1.58it/s]Extractor Predicting: 181it [01:53,  1.60it/s]Extractor Predicting: 182it [01:53,  1.62it/s]Extractor Predicting: 183it [01:54,  1.56it/s]Extractor Predicting: 184it [01:55,  1.56it/s]Extractor Predicting: 185it [01:55,  1.54it/s]Extractor Predicting: 186it [01:56,  1.57it/s]Extractor Predicting: 187it [01:57,  1.58it/s]Extractor Predicting: 188it [01:57,  1.57it/s]Extractor Predicting: 189it [01:58,  1.57it/s]Extractor Predicting: 190it [01:59,  1.41it/s]Extractor Predicting: 191it [01:59,  1.46it/s]Extractor Predicting: 192it [02:00,  1.52it/s]Extractor Predicting: 193it [02:01,  1.53it/s]Extractor Predicting: 194it [02:01,  1.57it/s]Extractor Predicting: 195it [02:02,  1.54it/s]Extractor Predicting: 196it [02:03,  1.56it/s]Extractor Predicting: 197it [02:03,  1.55it/s]Extractor Predicting: 198it [02:04,  1.54it/s]Extractor Predicting: 199it [02:05,  1.54it/s]Extractor Predicting: 200it [02:05,  1.54it/s]Extractor Predicting: 201it [02:06,  1.52it/s]Extractor Predicting: 202it [02:06,  1.54it/s]Extractor Predicting: 203it [02:07,  1.52it/s]Extractor Predicting: 204it [02:08,  1.52it/s]Extractor Predicting: 205it [02:08,  1.54it/s]Extractor Predicting: 206it [02:09,  1.55it/s]Extractor Predicting: 207it [02:10,  1.60it/s]Extractor Predicting: 208it [02:10,  1.60it/s]Extractor Predicting: 209it [02:11,  1.59it/s]Extractor Predicting: 210it [02:12,  1.59it/s]Extractor Predicting: 211it [02:12,  1.59it/s]Extractor Predicting: 212it [02:13,  1.56it/s]Extractor Predicting: 213it [02:14,  1.51it/s]Extractor Predicting: 214it [02:14,  1.56it/s]Extractor Predicting: 215it [02:15,  1.58it/s]Extractor Predicting: 216it [02:15,  1.58it/s]Extractor Predicting: 217it [02:16,  1.56it/s]Extractor Predicting: 218it [02:17,  1.54it/s]Extractor Predicting: 219it [02:17,  1.54it/s]Extractor Predicting: 220it [02:18,  1.56it/s]Extractor Predicting: 221it [02:19,  1.59it/s]Extractor Predicting: 222it [02:19,  1.59it/s]Extractor Predicting: 223it [02:20,  1.57it/s]Extractor Predicting: 224it [02:20,  1.61it/s]Extractor Predicting: 225it [02:21,  1.66it/s]Extractor Predicting: 226it [02:22,  1.63it/s]Extractor Predicting: 227it [02:22,  1.61it/s]Extractor Predicting: 228it [02:23,  1.62it/s]Extractor Predicting: 229it [02:24,  1.55it/s]Extractor Predicting: 230it [02:24,  1.61it/s]Extractor Predicting: 231it [02:25,  1.59it/s]Extractor Predicting: 232it [02:25,  1.58it/s]Extractor Predicting: 233it [02:26,  1.58it/s]Extractor Predicting: 234it [02:27,  1.55it/s]Extractor Predicting: 235it [02:27,  1.57it/s]Extractor Predicting: 236it [02:28,  1.49it/s]Extractor Predicting: 237it [02:29,  1.47it/s]Extractor Predicting: 238it [02:30,  1.46it/s]Extractor Predicting: 239it [02:30,  1.43it/s]Extractor Predicting: 240it [02:31,  1.44it/s]Extractor Predicting: 241it [02:32,  1.48it/s]Extractor Predicting: 242it [02:32,  1.47it/s]Extractor Predicting: 243it [02:33,  1.51it/s]Extractor Predicting: 244it [02:34,  1.51it/s]Extractor Predicting: 245it [02:34,  1.56it/s]Extractor Predicting: 246it [02:35,  1.57it/s]Extractor Predicting: 247it [02:35,  1.59it/s]Extractor Predicting: 248it [02:36,  1.62it/s]Extractor Predicting: 249it [02:37,  1.66it/s]Extractor Predicting: 250it [02:37,  1.62it/s]Extractor Predicting: 251it [02:38,  1.64it/s]Extractor Predicting: 252it [02:38,  1.62it/s]Extractor Predicting: 253it [02:39,  1.61it/s]Extractor Predicting: 254it [02:40,  1.62it/s]Extractor Predicting: 255it [02:40,  1.63it/s]Extractor Predicting: 256it [02:41,  1.60it/s]Extractor Predicting: 257it [02:42,  1.61it/s]Extractor Predicting: 258it [02:42,  1.62it/s]Extractor Predicting: 259it [02:43,  1.60it/s]Extractor Predicting: 260it [02:43,  1.60it/s]Extractor Predicting: 261it [02:44,  1.58it/s]Extractor Predicting: 262it [02:45,  1.58it/s]Extractor Predicting: 263it [02:45,  1.58it/s]Extractor Predicting: 264it [02:46,  1.59it/s]Extractor Predicting: 265it [02:47,  1.60it/s]Extractor Predicting: 266it [02:47,  1.57it/s]Extractor Predicting: 267it [02:48,  1.57it/s]Extractor Predicting: 268it [02:48,  1.57it/s]Extractor Predicting: 269it [02:49,  1.55it/s]Extractor Predicting: 270it [02:50,  1.59it/s]Extractor Predicting: 271it [02:51,  1.43it/s]Extractor Predicting: 272it [02:51,  1.52it/s]Extractor Predicting: 273it [02:52,  1.56it/s]Extractor Predicting: 274it [02:52,  1.58it/s]Extractor Predicting: 275it [02:53,  1.56it/s]Extractor Predicting: 276it [02:54,  1.57it/s]Extractor Predicting: 277it [02:54,  1.60it/s]Extractor Predicting: 278it [02:55,  1.57it/s]Extractor Predicting: 279it [02:56,  1.59it/s]Extractor Predicting: 280it [02:56,  1.65it/s]Extractor Predicting: 281it [02:57,  1.65it/s]Extractor Predicting: 282it [02:57,  1.61it/s]Extractor Predicting: 283it [02:58,  1.58it/s]Extractor Predicting: 284it [02:59,  1.56it/s]Extractor Predicting: 285it [02:59,  1.53it/s]Extractor Predicting: 286it [03:00,  1.55it/s]Extractor Predicting: 287it [03:01,  1.54it/s]Extractor Predicting: 288it [03:01,  1.53it/s]Extractor Predicting: 289it [03:02,  1.52it/s]Extractor Predicting: 290it [03:03,  1.52it/s]Extractor Predicting: 291it [03:03,  1.55it/s]Extractor Predicting: 292it [03:04,  1.58it/s]Extractor Predicting: 293it [03:05,  1.57it/s]Extractor Predicting: 294it [03:05,  1.51it/s]Extractor Predicting: 295it [03:06,  1.53it/s]Extractor Predicting: 296it [03:07,  1.50it/s]Extractor Predicting: 297it [03:07,  1.48it/s]Extractor Predicting: 298it [03:08,  1.46it/s]Extractor Predicting: 299it [03:09,  1.44it/s]Extractor Predicting: 300it [03:09,  1.45it/s]Extractor Predicting: 301it [03:10,  1.42it/s]Extractor Predicting: 302it [03:11,  1.43it/s]Extractor Predicting: 303it [03:11,  1.43it/s]Extractor Predicting: 303it [03:11,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:27,875 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:27,916 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:27,916 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:27,917 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:27,917 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:11:28,863 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:11:28,864 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:11:29,495 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:11:30,708 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:11:30,708 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:33,828 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:33,865 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:33,865 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:33,865 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:33,865 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:11:34,800 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:11:34,801 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:11:35,492 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:11:35,717 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:11:35,717 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.32463768115942027,
  "recall": 0.06535947712418301,
  "score": 0.1088118138540756,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'labels': ['continent', 'field of this occupation', 'field of work', 'founded by', 'movement', 'owned by', 'performer', 'producer', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20815
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20915, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.65it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:04,  1.59it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.60it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:11,  1.57it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:13,  1.54it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:15,  1.59it/s]Extractor Predicting: 26it [00:16,  1.57it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:18,  1.58it/s]Extractor Predicting: 31it [00:19,  1.59it/s]Extractor Predicting: 32it [00:20,  1.58it/s]Extractor Predicting: 33it [00:20,  1.59it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:22,  1.55it/s]Extractor Predicting: 36it [00:22,  1.59it/s]Extractor Predicting: 37it [00:23,  1.59it/s]Extractor Predicting: 38it [00:23,  1.58it/s]Extractor Predicting: 39it [00:24,  1.62it/s]Extractor Predicting: 40it [00:25,  1.60it/s]Extractor Predicting: 41it [00:25,  1.60it/s]Extractor Predicting: 42it [00:26,  1.62it/s]Extractor Predicting: 43it [00:27,  1.59it/s]Extractor Predicting: 44it [00:27,  1.56it/s]Extractor Predicting: 45it [00:28,  1.53it/s]Extractor Predicting: 46it [00:29,  1.52it/s]Extractor Predicting: 47it [00:29,  1.53it/s]Extractor Predicting: 48it [00:30,  1.55it/s]Extractor Predicting: 49it [00:31,  1.55it/s]Extractor Predicting: 50it [00:31,  1.59it/s]Extractor Predicting: 51it [00:32,  1.61it/s]Extractor Predicting: 52it [00:32,  1.61it/s]Extractor Predicting: 53it [00:33,  1.62it/s]Extractor Predicting: 54it [00:34,  1.58it/s]Extractor Predicting: 55it [00:34,  1.60it/s]Extractor Predicting: 56it [00:35,  1.60it/s]Extractor Predicting: 57it [00:35,  1.61it/s]Extractor Predicting: 58it [00:36,  1.58it/s]Extractor Predicting: 59it [00:37,  1.58it/s]Extractor Predicting: 60it [00:37,  1.58it/s]Extractor Predicting: 61it [00:38,  1.57it/s]Extractor Predicting: 62it [00:39,  1.59it/s]Extractor Predicting: 63it [00:39,  1.61it/s]Extractor Predicting: 64it [00:40,  1.62it/s]Extractor Predicting: 65it [00:41,  1.52it/s]Extractor Predicting: 66it [00:41,  1.56it/s]Extractor Predicting: 67it [00:42,  1.58it/s]Extractor Predicting: 68it [00:42,  1.56it/s]Extractor Predicting: 69it [00:43,  1.56it/s]Extractor Predicting: 70it [00:44,  1.56it/s]Extractor Predicting: 71it [00:44,  1.57it/s]Extractor Predicting: 72it [00:45,  1.60it/s]Extractor Predicting: 73it [00:46,  1.61it/s]Extractor Predicting: 74it [00:46,  1.60it/s]Extractor Predicting: 75it [00:47,  1.57it/s]Extractor Predicting: 76it [00:47,  1.61it/s]Extractor Predicting: 77it [00:48,  1.60it/s]Extractor Predicting: 78it [00:49,  1.46it/s]Extractor Predicting: 79it [00:50,  1.50it/s]Extractor Predicting: 80it [00:50,  1.51it/s]Extractor Predicting: 81it [00:51,  1.54it/s]Extractor Predicting: 82it [00:51,  1.58it/s]Extractor Predicting: 83it [00:52,  1.59it/s]Extractor Predicting: 84it [00:53,  1.59it/s]Extractor Predicting: 85it [00:53,  1.59it/s]Extractor Predicting: 86it [00:54,  1.62it/s]Extractor Predicting: 87it [00:55,  1.60it/s]Extractor Predicting: 88it [00:55,  1.56it/s]Extractor Predicting: 89it [00:56,  1.57it/s]Extractor Predicting: 90it [00:56,  1.59it/s]Extractor Predicting: 91it [00:57,  1.61it/s]Extractor Predicting: 92it [00:58,  1.62it/s]Extractor Predicting: 93it [00:58,  1.57it/s]Extractor Predicting: 94it [00:59,  1.58it/s]Extractor Predicting: 95it [01:00,  1.60it/s]Extractor Predicting: 96it [01:00,  1.60it/s]Extractor Predicting: 97it [01:01,  1.57it/s]Extractor Predicting: 98it [01:01,  1.59it/s]Extractor Predicting: 99it [01:02,  1.60it/s]Extractor Predicting: 100it [01:03,  1.56it/s]Extractor Predicting: 101it [01:03,  1.59it/s]Extractor Predicting: 102it [01:04,  1.58it/s]Extractor Predicting: 103it [01:05,  1.56it/s]Extractor Predicting: 104it [01:05,  1.57it/s]Extractor Predicting: 105it [01:06,  1.56it/s]Extractor Predicting: 106it [01:07,  1.56it/s]Extractor Predicting: 107it [01:07,  1.58it/s]Extractor Predicting: 108it [01:08,  1.55it/s]Extractor Predicting: 109it [01:09,  1.55it/s]Extractor Predicting: 110it [01:09,  1.57it/s]Extractor Predicting: 111it [01:10,  1.61it/s]Extractor Predicting: 112it [01:10,  1.62it/s]Extractor Predicting: 113it [01:11,  1.62it/s]Extractor Predicting: 114it [01:12,  1.58it/s]Extractor Predicting: 115it [01:12,  1.59it/s]Extractor Predicting: 116it [01:13,  1.61it/s]Extractor Predicting: 117it [01:13,  1.58it/s]Extractor Predicting: 118it [01:14,  1.57it/s]Extractor Predicting: 119it [01:15,  1.57it/s]Extractor Predicting: 120it [01:15,  1.57it/s]Extractor Predicting: 121it [01:16,  1.60it/s]Extractor Predicting: 122it [01:17,  1.59it/s]Extractor Predicting: 123it [01:17,  1.60it/s]Extractor Predicting: 124it [01:18,  1.60it/s]Extractor Predicting: 125it [01:18,  1.61it/s]Extractor Predicting: 126it [01:19,  1.59it/s]Extractor Predicting: 127it [01:20,  1.58it/s]Extractor Predicting: 128it [01:20,  1.57it/s]Extractor Predicting: 129it [01:21,  1.62it/s]Extractor Predicting: 130it [01:22,  1.65it/s]Extractor Predicting: 131it [01:22,  1.63it/s]Extractor Predicting: 132it [01:23,  1.60it/s]Extractor Predicting: 133it [01:24,  1.54it/s]Extractor Predicting: 134it [01:24,  1.54it/s]Extractor Predicting: 135it [01:25,  1.56it/s]Extractor Predicting: 136it [01:26,  1.55it/s]Extractor Predicting: 137it [01:26,  1.54it/s]Extractor Predicting: 138it [01:27,  1.53it/s]Extractor Predicting: 139it [01:27,  1.53it/s]Extractor Predicting: 140it [01:28,  1.54it/s]Extractor Predicting: 141it [01:29,  1.54it/s]Extractor Predicting: 142it [01:29,  1.52it/s]Extractor Predicting: 143it [01:30,  1.53it/s]Extractor Predicting: 144it [01:31,  1.55it/s]Extractor Predicting: 145it [01:31,  1.55it/s]Extractor Predicting: 146it [01:32,  1.55it/s]Extractor Predicting: 147it [01:33,  1.52it/s]Extractor Predicting: 148it [01:33,  1.57it/s]Extractor Predicting: 149it [01:34,  1.55it/s]Extractor Predicting: 150it [01:35,  1.56it/s]Extractor Predicting: 151it [01:35,  1.56it/s]Extractor Predicting: 152it [01:36,  1.60it/s]Extractor Predicting: 153it [01:36,  1.62it/s]Extractor Predicting: 154it [01:37,  1.61it/s]Extractor Predicting: 155it [01:38,  1.61it/s]Extractor Predicting: 156it [01:38,  1.60it/s]Extractor Predicting: 157it [01:39,  1.60it/s]Extractor Predicting: 158it [01:40,  1.58it/s]Extractor Predicting: 159it [01:40,  1.58it/s]Extractor Predicting: 160it [01:41,  1.60it/s]Extractor Predicting: 161it [01:41,  1.59it/s]Extractor Predicting: 162it [01:42,  1.57it/s]Extractor Predicting: 163it [01:43,  1.59it/s]Extractor Predicting: 164it [01:43,  1.59it/s]Extractor Predicting: 165it [01:44,  1.63it/s]Extractor Predicting: 166it [01:45,  1.64it/s]Extractor Predicting: 167it [01:45,  1.61it/s]Extractor Predicting: 168it [01:46,  1.62it/s]Extractor Predicting: 169it [01:46,  1.63it/s]Extractor Predicting: 170it [01:47,  1.60it/s]Extractor Predicting: 171it [01:48,  1.57it/s]Extractor Predicting: 172it [01:48,  1.56it/s]Extractor Predicting: 173it [01:49,  1.56it/s]Extractor Predicting: 174it [01:50,  1.61it/s]Extractor Predicting: 175it [01:50,  1.63it/s]Extractor Predicting: 176it [01:51,  1.59it/s]Extractor Predicting: 177it [01:51,  1.58it/s]Extractor Predicting: 178it [01:52,  1.58it/s]Extractor Predicting: 179it [01:53,  1.57it/s]Extractor Predicting: 180it [01:53,  1.63it/s]Extractor Predicting: 181it [01:54,  1.64it/s]Extractor Predicting: 182it [01:55,  1.62it/s]Extractor Predicting: 183it [01:55,  1.65it/s]Extractor Predicting: 184it [01:56,  1.68it/s]Extractor Predicting: 185it [01:56,  1.66it/s]Extractor Predicting: 186it [01:57,  1.64it/s]Extractor Predicting: 187it [01:58,  1.65it/s]Extractor Predicting: 188it [01:58,  1.63it/s]Extractor Predicting: 189it [01:59,  1.61it/s]Extractor Predicting: 190it [01:59,  1.59it/s]Extractor Predicting: 191it [02:00,  1.59it/s]Extractor Predicting: 192it [02:01,  1.63it/s]Extractor Predicting: 193it [02:01,  1.62it/s]Extractor Predicting: 194it [02:02,  1.63it/s]Extractor Predicting: 195it [02:03,  1.48it/s]Extractor Predicting: 196it [02:03,  1.51it/s]Extractor Predicting: 197it [02:04,  1.54it/s]Extractor Predicting: 198it [02:05,  1.59it/s]Extractor Predicting: 199it [02:05,  1.59it/s]Extractor Predicting: 200it [02:06,  1.57it/s]Extractor Predicting: 201it [02:06,  1.57it/s]Extractor Predicting: 202it [02:07,  1.58it/s]Extractor Predicting: 203it [02:08,  1.59it/s]Extractor Predicting: 204it [02:08,  1.61it/s]Extractor Predicting: 205it [02:09,  1.64it/s]Extractor Predicting: 206it [02:10,  1.62it/s]Extractor Predicting: 207it [02:10,  1.64it/s]Extractor Predicting: 208it [02:11,  1.67it/s]Extractor Predicting: 209it [02:11,  1.66it/s]Extractor Predicting: 210it [02:12,  1.63it/s]Extractor Predicting: 211it [02:13,  1.63it/s]Extractor Predicting: 212it [02:13,  1.62it/s]Extractor Predicting: 213it [02:14,  1.60it/s]Extractor Predicting: 214it [02:14,  1.60it/s]Extractor Predicting: 215it [02:15,  1.59it/s]Extractor Predicting: 216it [02:16,  1.58it/s]Extractor Predicting: 217it [02:16,  1.56it/s]Extractor Predicting: 218it [02:17,  1.62it/s]Extractor Predicting: 219it [02:18,  1.63it/s]Extractor Predicting: 220it [02:18,  1.61it/s]Extractor Predicting: 221it [02:19,  1.57it/s]Extractor Predicting: 222it [02:19,  1.59it/s]Extractor Predicting: 223it [02:20,  1.61it/s]Extractor Predicting: 224it [02:21,  1.60it/s]Extractor Predicting: 225it [02:21,  1.64it/s]Extractor Predicting: 226it [02:22,  1.62it/s]Extractor Predicting: 227it [02:23,  1.63it/s]Extractor Predicting: 228it [02:23,  1.61it/s]Extractor Predicting: 229it [02:24,  1.63it/s]Extractor Predicting: 230it [02:24,  1.59it/s]Extractor Predicting: 231it [02:25,  1.62it/s]Extractor Predicting: 232it [02:26,  1.60it/s]Extractor Predicting: 233it [02:26,  1.59it/s]Extractor Predicting: 234it [02:27,  1.58it/s]Extractor Predicting: 235it [02:28,  1.59it/s]Extractor Predicting: 236it [02:28,  1.58it/s]Extractor Predicting: 237it [02:29,  1.59it/s]Extractor Predicting: 238it [02:29,  1.58it/s]Extractor Predicting: 239it [02:30,  1.63it/s]Extractor Predicting: 240it [02:31,  1.60it/s]Extractor Predicting: 241it [02:31,  1.58it/s]Extractor Predicting: 242it [02:32,  1.59it/s]Extractor Predicting: 243it [02:33,  1.63it/s]Extractor Predicting: 244it [02:33,  1.65it/s]Extractor Predicting: 245it [02:34,  1.66it/s]Extractor Predicting: 246it [02:34,  1.70it/s]Extractor Predicting: 247it [02:35,  1.67it/s]Extractor Predicting: 248it [02:35,  1.69it/s]Extractor Predicting: 249it [02:36,  1.68it/s]Extractor Predicting: 250it [02:37,  1.68it/s]Extractor Predicting: 251it [02:37,  1.66it/s]Extractor Predicting: 252it [02:38,  1.67it/s]Extractor Predicting: 253it [02:38,  1.67it/s]Extractor Predicting: 254it [02:39,  1.67it/s]Extractor Predicting: 255it [02:40,  1.70it/s]Extractor Predicting: 256it [02:40,  1.67it/s]Extractor Predicting: 257it [02:41,  1.65it/s]Extractor Predicting: 258it [02:41,  1.65it/s]Extractor Predicting: 259it [02:42,  1.64it/s]Extractor Predicting: 260it [02:43,  1.64it/s]Extractor Predicting: 261it [02:43,  1.65it/s]Extractor Predicting: 262it [02:44,  1.67it/s]Extractor Predicting: 263it [02:44,  1.71it/s]Extractor Predicting: 264it [02:45,  1.65it/s]Extractor Predicting: 265it [02:46,  1.61it/s]Extractor Predicting: 266it [02:46,  1.61it/s]Extractor Predicting: 267it [02:47,  1.57it/s]Extractor Predicting: 268it [02:48,  1.60it/s]Extractor Predicting: 269it [02:48,  1.62it/s]Extractor Predicting: 270it [02:49,  1.62it/s]Extractor Predicting: 271it [02:49,  1.63it/s]Extractor Predicting: 272it [02:50,  1.61it/s]Extractor Predicting: 273it [02:51,  1.65it/s]Extractor Predicting: 274it [02:51,  1.65it/s]Extractor Predicting: 275it [02:52,  1.68it/s]Extractor Predicting: 276it [02:52,  1.66it/s]Extractor Predicting: 277it [02:53,  1.60it/s]Extractor Predicting: 278it [02:54,  1.61it/s]Extractor Predicting: 279it [02:54,  1.66it/s]Extractor Predicting: 280it [02:55,  1.68it/s]Extractor Predicting: 281it [02:55,  1.70it/s]Extractor Predicting: 282it [02:56,  1.71it/s]Extractor Predicting: 283it [02:57,  1.71it/s]Extractor Predicting: 284it [02:57,  1.70it/s]Extractor Predicting: 285it [02:58,  1.66it/s]Extractor Predicting: 286it [02:58,  1.68it/s]Extractor Predicting: 287it [02:59,  1.68it/s]Extractor Predicting: 288it [03:00,  1.68it/s]Extractor Predicting: 289it [03:00,  1.68it/s]Extractor Predicting: 290it [03:01,  1.71it/s]Extractor Predicting: 291it [03:01,  1.72it/s]Extractor Predicting: 292it [03:02,  1.63it/s]Extractor Predicting: 293it [03:03,  1.63it/s]Extractor Predicting: 294it [03:03,  1.64it/s]Extractor Predicting: 295it [03:04,  1.65it/s]Extractor Predicting: 296it [03:04,  1.65it/s]Extractor Predicting: 297it [03:05,  1.62it/s]Extractor Predicting: 298it [03:06,  1.66it/s]Extractor Predicting: 299it [03:06,  1.62it/s]Extractor Predicting: 300it [03:07,  1.58it/s]Extractor Predicting: 301it [03:08,  1.52it/s]Extractor Predicting: 302it [03:09,  1.37it/s]Extractor Predicting: 303it [03:09,  1.40it/s]Extractor Predicting: 304it [03:10,  1.44it/s]Extractor Predicting: 305it [03:11,  1.45it/s]Extractor Predicting: 306it [03:11,  1.45it/s]Extractor Predicting: 306it [03:11,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:14:58,882 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:14:58,902 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:14:58,903 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:14:58,903 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:14:58,903 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:14:59,668 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:14:59,670 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:15:00,276 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:15:01,400 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:15:01,400 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:15:04,299 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:15:04,318 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:15:04,319 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:15:04,319 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:15:04,319 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:15:05,069 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:15:05,071 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:15:05,664 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:15:05,859 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:15:05,859 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.32035685320356855,
  "recall": 0.05378540305010893,
  "score": 0.09210679724845518,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'labels': ['continent', 'field of this occupation', 'field of work', 'founded by', 'movement', 'owned by', 'performer', 'producer', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6322
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6422, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:16,  1.60it/s]Extractor Predicting: 26it [00:16,  1.61it/s]Extractor Predicting: 27it [00:17,  1.59it/s]Extractor Predicting: 28it [00:18,  1.60it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:19,  1.61it/s]Extractor Predicting: 31it [00:19,  1.60it/s]Extractor Predicting: 32it [00:20,  1.61it/s]Extractor Predicting: 33it [00:21,  1.60it/s]Extractor Predicting: 34it [00:21,  1.61it/s]Extractor Predicting: 35it [00:22,  1.62it/s]Extractor Predicting: 36it [00:22,  1.66it/s]Extractor Predicting: 37it [00:23,  1.65it/s]Extractor Predicting: 38it [00:24,  1.68it/s]Extractor Predicting: 39it [00:24,  1.66it/s]Extractor Predicting: 40it [00:25,  1.60it/s]Extractor Predicting: 41it [00:26,  1.61it/s]Extractor Predicting: 42it [00:26,  1.59it/s]Extractor Predicting: 43it [00:27,  1.57it/s]Extractor Predicting: 44it [00:27,  1.58it/s]Extractor Predicting: 45it [00:28,  1.56it/s]Extractor Predicting: 46it [00:29,  1.52it/s]Extractor Predicting: 47it [00:29,  1.55it/s]Extractor Predicting: 48it [00:30,  1.58it/s]Extractor Predicting: 49it [00:31,  1.59it/s]Extractor Predicting: 50it [00:31,  1.45it/s]Extractor Predicting: 51it [00:32,  1.50it/s]Extractor Predicting: 52it [00:33,  1.51it/s]Extractor Predicting: 53it [00:33,  1.55it/s]Extractor Predicting: 54it [00:34,  1.57it/s]Extractor Predicting: 55it [00:35,  1.53it/s]Extractor Predicting: 56it [00:35,  1.56it/s]Extractor Predicting: 57it [00:36,  1.52it/s]Extractor Predicting: 58it [00:37,  1.53it/s]Extractor Predicting: 59it [00:37,  1.50it/s]Extractor Predicting: 60it [00:38,  1.85it/s]Extractor Predicting: 60it [00:38,  1.58it/s]
[INFO|configuration_utils.py:515] 2023-08-29 06:15:45,862 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:15:45,864 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 06:15:45,917 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:15:45,918 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 06:15:45,936 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 06:15:54,446 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 06:15:54,465 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 06:15:54,683 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:15:54,684 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 06:15:54,761 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:15:54,797 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:15:54,797 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:15:54,797 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:15:54,797 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:15:54,797 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:15:54,797 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4956140350877193,
  "recall": 0.035213462137737614,
  "score": 0.06575501891184174,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 06:15:55,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:55,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:56,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:56,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:57,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:58,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:58,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:59,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:15:59,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:00,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:01,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:01,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:02,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:03,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:03,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:04,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:05,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:05,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:06,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:07,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:07,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:08,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:13<03:13, 13.85s/it][WARNING|generation_utils.py:914] 2023-08-29 06:16:08,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:09,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:10,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:10,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:11,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:12,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:12,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:13,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:14,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:14,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:15,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:16,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:17,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:17,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:18,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:19,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:19,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:20,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:21,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:21,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:22,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:23,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:23,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:29<03:11, 14.76s/it][WARNING|generation_utils.py:914] 2023-08-29 06:16:24,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:24,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:25,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:26,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:26,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:27,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:27,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:28,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:29,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:29,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:30,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:31,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:31,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:32,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:33,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:33,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:34,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:35,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:35,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:36,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:36,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:42<02:48, 14.04s/it][WARNING|generation_utils.py:914] 2023-08-29 06:16:37,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:38,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:38,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:39,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:39,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:40,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:41,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:41,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:42,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:42,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:43,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:44,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:44,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:45,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:46,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:46,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:47,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:48,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:48,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:49,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:50,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:55<02:30, 13.67s/it][WARNING|generation_utils.py:914] 2023-08-29 06:16:50,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:51,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:52,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:52,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:53,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:53,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:54,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:55,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:55,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:56,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:57,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:57,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:58,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:59,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:59,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:00,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:01,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:01,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:02,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:03,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:03,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:09<02:16, 13.69s/it][WARNING|generation_utils.py:914] 2023-08-29 06:17:04,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:05,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:05,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:06,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:07,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:07,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:08,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:08,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:09,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:10,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:10,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:11,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:12,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:12,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:13,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:14,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:14,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:15,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:15,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:16,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:17,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:17,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:23<02:04, 13.84s/it][WARNING|generation_utils.py:914] 2023-08-29 06:17:18,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:19,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:19,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:20,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:20,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:21,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:22,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:22,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:23,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:23,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:24,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:24,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:25,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:26,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:26,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:27,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:28,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:28,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:29,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:30,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:30,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:31,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:36<01:49, 13.69s/it][WARNING|generation_utils.py:914] 2023-08-29 06:17:31,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:32,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:33,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:33,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:34,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:35,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:35,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:36,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:37,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:38,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:38,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:39,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:40,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:40,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:41,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:42,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:42,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:43,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:44,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:44,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:45,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:45,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:51<01:37, 13.95s/it][WARNING|generation_utils.py:914] 2023-08-29 06:17:46,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:47,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:48,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:48,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:49,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:50,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:51,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:52,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:52,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:53,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:53,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:54,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:55,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:56,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:56,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:57,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:57,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:58,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:59,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:00,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:00,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:06<01:25, 14.32s/it][WARNING|generation_utils.py:914] 2023-08-29 06:18:01,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:02,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:02,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:03,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:03,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:04,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:05,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:06,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:06,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:07,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:07,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:08,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:09,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:10,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:10,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:11,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:12,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:12,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:13,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:14,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:14,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:15,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:16,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:21<01:13, 14.65s/it][WARNING|generation_utils.py:914] 2023-08-29 06:18:16,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:17,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:18,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:18,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:19,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:19,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:20,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:21,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:21,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:22,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:22,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:23,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:24,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:24,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:25,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:26,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:26,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:27,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:28,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:28,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:29,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:34<00:56, 14.20s/it][WARNING|generation_utils.py:914] 2023-08-29 06:18:30,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:30,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:31,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:31,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:32,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:33,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:34,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:34,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:35,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:35,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:36,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:37,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:37,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:38,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:39,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:40,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:40,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:41,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:41,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:42,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:47<00:41, 13.80s/it][WARNING|generation_utils.py:914] 2023-08-29 06:18:42,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:43,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:44,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:44,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:45,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:46,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:46,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:47,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:47,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:48,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:49,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:49,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:50,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:51,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:51,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:52,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:53,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:53,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:54,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:54,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:55,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:00<00:27, 13.58s/it][WARNING|generation_utils.py:914] 2023-08-29 06:18:56,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:56,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:57,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:57,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:58,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:59,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:59,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:00,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:01,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:01,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:02,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:03,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:03,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:04,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:05,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:05,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:06,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:07,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:07,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:08,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:09,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:09,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:15<00:13, 13.90s/it][WARNING|generation_utils.py:914] 2023-08-29 06:19:10,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:11,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:12,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:12,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:13,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:14,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:15,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:15,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:16,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:17,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:18,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:18,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:19,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:20,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:20,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:21,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:22,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:22,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:23,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:24,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:25,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:25,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:26,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:31<00:00, 14.64s/it]Generating: 100%|██████████| 15/15 [03:31<00:00, 14.13s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:19:32,492 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:19:32,495 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:19:32,495 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:19:32,495 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:19:32,495 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:19:32,899 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:19:32,900 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:19:33,189 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:19:34,340 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:19:34,357 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:19:36,195 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:19:36,198 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:19:36,198 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:19:36,198 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:19:36,198 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:19:36,640 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:19:36,641 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:19:37,381 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:19:37,682 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:19:37,683 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 607, 'raw': 704}
{'prompt': 'Relation : country .', 'success_rate': 0.8622159090909091, 'errors': {'', "('The film', 'country', '', 'The film was directed by Robert K. Lee and written by Stephen King , George Boggs , and John Cusack .')", "('American drama Seven Samurai', 'country', '', 'The first episode , titled Ojai Ueiru no Chitose ( A Nightmare Upon Ojai Island ) , debuted on December 2 , 2008 in the second season of the American drama Seven Samurai .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 560, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : place of death .', 'success_rate': 0.842391304347826, 'errors': {'', "('Henry James Harrison', 'place of death', '', 'Henry James Harrison ( born March 19 , 1846 ) is an American politician , and a Republican Member of the United States Congress .')", 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : production company .', 'success_rate': 0.8973214285714286, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.9226190476190477, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : continent .', 'success_rate': 0.8565340909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.8821022727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.890625, 'errors': {''}}
['Relation : founded by . Context : Later in the year , the University of New Brunswick purchased the land and began to develop a new campus , New Brunswick Museum , for the first time . Head Entity : University of New Brunswick , Tail Entity : NGC .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : founded by .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 615, 'raw': 736}
{'prompt': 'Relation : movement .', 'success_rate': 0.8355978260869565, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Bishop', 'movement', '', 'He was made a Bishop ( d.')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9032738095238095, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.9390625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : producer .', 'success_rate': 0.9345238095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : record label .', 'success_rate': 0.8835227272727273, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : replaces .', 'success_rate': 0.8505434782608695, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/2_ext.jsonl'}}
estimate vocab size: 12066
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12166, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.55it/s]Extractor Estimating: 2it [00:01,  1.49it/s]Extractor Estimating: 3it [00:01,  1.59it/s]Extractor Estimating: 4it [00:02,  1.58it/s]Extractor Estimating: 5it [00:03,  1.61it/s]Extractor Estimating: 6it [00:03,  1.60it/s]Extractor Estimating: 7it [00:04,  1.61it/s]Extractor Estimating: 8it [00:04,  1.65it/s]Extractor Estimating: 9it [00:05,  1.63it/s]Extractor Estimating: 10it [00:06,  1.53it/s]Extractor Estimating: 11it [00:06,  1.55it/s]Extractor Estimating: 12it [00:07,  1.59it/s]Extractor Estimating: 13it [00:08,  1.66it/s]Extractor Estimating: 14it [00:08,  1.67it/s]Extractor Estimating: 15it [00:09,  1.70it/s]Extractor Estimating: 16it [00:09,  1.72it/s]Extractor Estimating: 17it [00:10,  1.60it/s]Extractor Estimating: 18it [00:11,  1.59it/s]Extractor Estimating: 19it [00:11,  1.57it/s]Extractor Estimating: 20it [00:12,  1.60it/s]Extractor Estimating: 21it [00:13,  1.61it/s]Extractor Estimating: 22it [00:13,  1.60it/s]Extractor Estimating: 23it [00:14,  1.66it/s]Extractor Estimating: 24it [00:14,  1.61it/s]Extractor Estimating: 25it [00:15,  1.68it/s]Extractor Estimating: 26it [00:16,  1.70it/s]Extractor Estimating: 27it [00:16,  1.65it/s]Extractor Estimating: 28it [00:17,  1.61it/s]Extractor Estimating: 29it [00:17,  1.60it/s]Extractor Estimating: 30it [00:18,  1.48it/s]Extractor Estimating: 31it [00:19,  1.50it/s]Extractor Estimating: 32it [00:19,  1.55it/s]Extractor Estimating: 33it [00:20,  1.50it/s]Extractor Estimating: 34it [00:21,  1.49it/s]Extractor Estimating: 35it [00:22,  1.49it/s]Extractor Estimating: 36it [00:22,  1.49it/s]Extractor Estimating: 37it [00:23,  1.50it/s]Extractor Estimating: 38it [00:24,  1.52it/s]Extractor Estimating: 39it [00:24,  1.55it/s]Extractor Estimating: 40it [00:25,  1.57it/s]Extractor Estimating: 41it [00:25,  1.56it/s]Extractor Estimating: 42it [00:26,  1.57it/s]Extractor Estimating: 43it [00:27,  1.58it/s]Extractor Estimating: 44it [00:27,  1.53it/s]Extractor Estimating: 45it [00:28,  1.53it/s]Extractor Estimating: 46it [00:29,  1.54it/s]Extractor Estimating: 47it [00:29,  1.56it/s]Extractor Estimating: 48it [00:30,  1.55it/s]Extractor Estimating: 49it [00:31,  1.54it/s]Extractor Estimating: 50it [00:31,  1.60it/s]Extractor Estimating: 51it [00:32,  1.59it/s]Extractor Estimating: 52it [00:32,  1.61it/s]Extractor Estimating: 53it [00:33,  1.59it/s]Extractor Estimating: 54it [00:34,  1.60it/s]Extractor Estimating: 55it [00:34,  1.64it/s]Extractor Estimating: 56it [00:35,  1.64it/s]Extractor Estimating: 57it [00:35,  1.64it/s]Extractor Estimating: 58it [00:36,  1.62it/s]Extractor Estimating: 59it [00:37,  1.55it/s]Extractor Estimating: 60it [00:37,  1.54it/s]Extractor Estimating: 61it [00:38,  1.53it/s]Extractor Estimating: 62it [00:39,  1.54it/s]Extractor Estimating: 63it [00:39,  1.57it/s]Extractor Estimating: 64it [00:40,  1.56it/s]Extractor Estimating: 65it [00:41,  1.56it/s]Extractor Estimating: 66it [00:41,  1.53it/s]Extractor Estimating: 67it [00:42,  1.56it/s]Extractor Estimating: 68it [00:43,  1.61it/s]Extractor Estimating: 69it [00:43,  1.59it/s]Extractor Estimating: 70it [00:44,  1.57it/s]Extractor Estimating: 71it [00:44,  1.56it/s]Extractor Estimating: 72it [00:45,  1.60it/s]Extractor Estimating: 73it [00:46,  1.62it/s]Extractor Estimating: 74it [00:46,  1.59it/s]Extractor Estimating: 75it [00:47,  1.56it/s]Extractor Estimating: 76it [00:48,  1.54it/s]Extractor Estimating: 77it [00:48,  1.55it/s]Extractor Estimating: 78it [00:49,  1.57it/s]Extractor Estimating: 79it [00:50,  1.58it/s]Extractor Estimating: 80it [00:50,  1.61it/s]Extractor Estimating: 81it [00:51,  1.61it/s]Extractor Estimating: 82it [00:51,  1.62it/s]Extractor Estimating: 83it [00:52,  1.63it/s]Extractor Estimating: 84it [00:53,  1.61it/s]Extractor Estimating: 85it [00:53,  1.59it/s]Extractor Estimating: 86it [00:54,  1.54it/s]Extractor Estimating: 87it [00:55,  1.51it/s]Extractor Estimating: 88it [00:55,  1.56it/s]Extractor Estimating: 89it [00:56,  1.50it/s]Extractor Estimating: 90it [00:57,  1.50it/s]Extractor Estimating: 91it [00:57,  1.56it/s]Extractor Estimating: 92it [00:58,  1.54it/s]Extractor Estimating: 93it [00:58,  1.57it/s]Extractor Estimating: 94it [00:59,  1.57it/s]Extractor Estimating: 95it [01:00,  1.57it/s]Extractor Estimating: 96it [01:00,  1.59it/s]Extractor Estimating: 97it [01:01,  1.43it/s]Extractor Estimating: 98it [01:02,  1.49it/s]Extractor Estimating: 99it [01:02,  1.54it/s]Extractor Estimating: 100it [01:03,  1.53it/s]Extractor Estimating: 101it [01:04,  1.60it/s]Extractor Estimating: 102it [01:04,  1.55it/s]Extractor Estimating: 103it [01:05,  1.59it/s]Extractor Estimating: 104it [01:06,  1.60it/s]Extractor Estimating: 105it [01:06,  1.62it/s]Extractor Estimating: 106it [01:07,  1.61it/s]Extractor Estimating: 107it [01:07,  1.57it/s]Extractor Estimating: 108it [01:08,  1.61it/s]Extractor Estimating: 109it [01:09,  1.63it/s]Extractor Estimating: 110it [01:09,  1.64it/s]Extractor Estimating: 111it [01:10,  1.70it/s]Extractor Estimating: 112it [01:10,  1.69it/s]Extractor Estimating: 113it [01:11,  1.60it/s]Extractor Estimating: 114it [01:12,  1.63it/s]Extractor Estimating: 115it [01:12,  1.56it/s]Extractor Estimating: 116it [01:13,  1.61it/s]Extractor Estimating: 117it [01:14,  1.60it/s]Extractor Estimating: 118it [01:14,  1.55it/s]Extractor Estimating: 119it [01:15,  1.62it/s]Extractor Estimating: 120it [01:15,  1.59it/s]Extractor Estimating: 121it [01:16,  1.60it/s]Extractor Estimating: 122it [01:17,  1.60it/s]Extractor Estimating: 123it [01:17,  1.63it/s]Extractor Estimating: 124it [01:18,  1.61it/s]Extractor Estimating: 125it [01:19,  1.58it/s]Extractor Estimating: 126it [01:19,  1.62it/s]Extractor Estimating: 127it [01:20,  1.62it/s]Extractor Estimating: 128it [01:20,  1.62it/s]Extractor Estimating: 129it [01:21,  1.69it/s]Extractor Estimating: 130it [01:21,  1.73it/s]Extractor Estimating: 131it [01:22,  1.74it/s]Extractor Estimating: 132it [01:23,  1.78it/s]Extractor Estimating: 133it [01:23,  1.79it/s]Extractor Estimating: 134it [01:24,  1.65it/s]Extractor Estimating: 135it [01:24,  1.70it/s]Extractor Estimating: 136it [01:25,  1.72it/s]Extractor Estimating: 137it [01:26,  1.72it/s]Extractor Estimating: 138it [01:26,  1.66it/s]Extractor Estimating: 139it [01:27,  1.69it/s]Extractor Estimating: 140it [01:27,  1.71it/s]Extractor Estimating: 141it [01:28,  1.77it/s]Extractor Estimating: 142it [01:28,  1.77it/s]Extractor Estimating: 143it [01:29,  1.77it/s]Extractor Estimating: 144it [01:30,  1.80it/s]Extractor Estimating: 145it [01:30,  1.77it/s]Extractor Estimating: 146it [01:31,  1.72it/s]Extractor Estimating: 147it [01:31,  1.71it/s]Extractor Estimating: 148it [01:32,  1.70it/s]Extractor Estimating: 149it [01:32,  1.74it/s]Extractor Estimating: 150it [01:33,  1.71it/s]Extractor Estimating: 151it [01:34,  1.70it/s]Extractor Estimating: 152it [01:34,  1.68it/s]Extractor Estimating: 153it [01:35,  1.67it/s]Extractor Estimating: 154it [01:36,  1.65it/s]Extractor Estimating: 155it [01:36,  1.65it/s]Extractor Estimating: 156it [01:37,  1.61it/s]Extractor Estimating: 157it [01:37,  1.64it/s]Extractor Estimating: 158it [01:38,  1.68it/s]Extractor Estimating: 159it [01:39,  1.64it/s]Extractor Estimating: 160it [01:39,  1.69it/s]Extractor Estimating: 161it [01:40,  1.68it/s]Extractor Estimating: 162it [01:40,  1.69it/s]Extractor Estimating: 163it [01:41,  1.70it/s]Extractor Estimating: 164it [01:41,  1.68it/s]Extractor Estimating: 165it [01:42,  1.66it/s]Extractor Estimating: 166it [01:43,  1.61it/s]Extractor Estimating: 167it [01:44,  1.49it/s]Extractor Estimating: 168it [01:44,  1.53it/s]Extractor Estimating: 169it [01:45,  1.55it/s]Extractor Estimating: 170it [01:45,  1.55it/s]Extractor Estimating: 171it [01:46,  1.55it/s]Extractor Estimating: 172it [01:47,  1.57it/s]Extractor Estimating: 173it [01:47,  1.57it/s]Extractor Estimating: 174it [01:48,  1.57it/s]Extractor Estimating: 175it [01:49,  1.55it/s]Extractor Estimating: 176it [01:49,  1.57it/s]Extractor Estimating: 177it [01:50,  1.59it/s]Extractor Estimating: 178it [01:51,  1.55it/s]Extractor Estimating: 179it [01:51,  1.55it/s]Extractor Estimating: 180it [01:52,  1.57it/s]Extractor Estimating: 181it [01:52,  1.60it/s]Extractor Estimating: 182it [01:53,  1.65it/s]Extractor Estimating: 183it [01:54,  1.56it/s]Extractor Estimating: 184it [01:54,  1.58it/s]Extractor Estimating: 185it [01:55,  1.58it/s]Extractor Estimating: 186it [01:56,  1.61it/s]Extractor Estimating: 187it [01:56,  1.61it/s]Extractor Estimating: 188it [01:57,  1.60it/s]Extractor Estimating: 189it [01:57,  1.58it/s]Extractor Estimating: 190it [01:58,  1.60it/s]Extractor Estimating: 191it [01:59,  1.64it/s]Extractor Estimating: 192it [01:59,  1.55it/s]Extractor Estimating: 193it [02:00,  1.54it/s]Extractor Estimating: 194it [02:01,  1.63it/s]Extractor Estimating: 195it [02:01,  1.64it/s]Extractor Estimating: 196it [02:02,  1.58it/s]Extractor Estimating: 197it [02:02,  1.60it/s]Extractor Estimating: 198it [02:03,  1.61it/s]Extractor Estimating: 199it [02:04,  1.62it/s]Extractor Estimating: 200it [02:04,  1.65it/s]Extractor Estimating: 201it [02:05,  1.65it/s]Extractor Estimating: 202it [02:05,  1.64it/s]Extractor Estimating: 203it [02:06,  1.59it/s]Extractor Estimating: 204it [02:07,  1.58it/s]Extractor Estimating: 205it [02:07,  1.61it/s]Extractor Estimating: 206it [02:08,  1.63it/s]Extractor Estimating: 207it [02:09,  1.58it/s]Extractor Estimating: 208it [02:09,  1.52it/s]Extractor Estimating: 209it [02:10,  1.56it/s]Extractor Estimating: 210it [02:11,  1.61it/s]Extractor Estimating: 211it [02:11,  1.63it/s]Extractor Estimating: 212it [02:12,  1.60it/s]Extractor Estimating: 213it [02:13,  1.51it/s]Extractor Estimating: 214it [02:13,  1.53it/s]Extractor Estimating: 215it [02:14,  1.50it/s]Extractor Estimating: 216it [02:14,  1.58it/s]Extractor Estimating: 217it [02:15,  1.57it/s]Extractor Estimating: 218it [02:16,  1.58it/s]Extractor Estimating: 219it [02:16,  1.58it/s]Extractor Estimating: 220it [02:17,  1.60it/s]Extractor Estimating: 221it [02:18,  1.62it/s]Extractor Estimating: 222it [02:18,  1.58it/s]Extractor Estimating: 223it [02:19,  1.57it/s]Extractor Estimating: 224it [02:19,  1.57it/s]Extractor Estimating: 225it [02:20,  1.63it/s]Extractor Estimating: 226it [02:21,  1.64it/s]Extractor Estimating: 227it [02:21,  1.64it/s]Extractor Estimating: 228it [02:22,  1.66it/s]Extractor Estimating: 229it [02:22,  1.65it/s]Extractor Estimating: 230it [02:23,  1.64it/s]Extractor Estimating: 231it [02:24,  1.63it/s]Extractor Estimating: 232it [02:24,  1.62it/s]Extractor Estimating: 233it [02:25,  1.65it/s]Extractor Estimating: 234it [02:25,  1.66it/s]Extractor Estimating: 235it [02:26,  1.63it/s]Extractor Estimating: 236it [02:27,  1.59it/s]Extractor Estimating: 237it [02:27,  1.60it/s]Extractor Estimating: 238it [02:28,  1.60it/s]Extractor Estimating: 239it [02:29,  1.62it/s]Extractor Estimating: 240it [02:29,  1.60it/s]Extractor Estimating: 241it [02:30,  1.62it/s]Extractor Estimating: 242it [02:30,  1.63it/s]Extractor Estimating: 243it [02:31,  1.57it/s]Extractor Estimating: 244it [02:32,  1.58it/s]Extractor Estimating: 245it [02:32,  1.57it/s]Extractor Estimating: 246it [02:33,  1.54it/s]Extractor Estimating: 247it [02:34,  1.53it/s]Extractor Estimating: 248it [02:34,  1.55it/s]Extractor Estimating: 249it [02:35,  1.54it/s]Extractor Estimating: 250it [02:36,  1.59it/s]Extractor Estimating: 251it [02:36,  1.63it/s]Extractor Estimating: 252it [02:37,  1.62it/s]Extractor Estimating: 253it [02:37,  1.65it/s]Extractor Estimating: 254it [02:38,  1.75it/s]Extractor Estimating: 255it [02:39,  1.59it/s]Extractor Estimating: 256it [02:39,  1.61it/s]Extractor Estimating: 257it [02:40,  1.64it/s]Extractor Estimating: 258it [02:41,  1.62it/s]Extractor Estimating: 259it [02:41,  1.61it/s]Extractor Estimating: 260it [02:42,  1.67it/s]Extractor Estimating: 261it [02:42,  1.68it/s]Extractor Estimating: 262it [02:43,  1.70it/s]Extractor Estimating: 263it [02:43,  1.68it/s]Extractor Estimating: 264it [02:44,  1.69it/s]Extractor Estimating: 265it [02:45,  1.62it/s]Extractor Estimating: 266it [02:45,  1.68it/s]Extractor Estimating: 267it [02:46,  1.66it/s]Extractor Estimating: 268it [02:46,  1.65it/s]Extractor Estimating: 269it [02:47,  1.68it/s]Extractor Estimating: 270it [02:48,  1.74it/s]Extractor Estimating: 271it [02:48,  1.75it/s]Extractor Estimating: 272it [02:49,  1.79it/s]Extractor Estimating: 273it [02:49,  1.70it/s]Extractor Estimating: 274it [02:50,  1.68it/s]Extractor Estimating: 275it [02:51,  1.62it/s]Extractor Estimating: 276it [02:51,  1.63it/s]Extractor Estimating: 277it [02:52,  1.63it/s]Extractor Estimating: 278it [02:52,  1.62it/s]Extractor Estimating: 279it [02:53,  1.63it/s]Extractor Estimating: 280it [02:54,  1.61it/s]Extractor Estimating: 281it [02:54,  1.60it/s]Extractor Estimating: 282it [02:55,  1.61it/s]Extractor Estimating: 283it [02:56,  1.63it/s]Extractor Estimating: 284it [02:56,  1.63it/s]Extractor Estimating: 285it [02:57,  1.61it/s]Extractor Estimating: 286it [02:57,  1.58it/s]Extractor Estimating: 287it [02:58,  1.55it/s]Extractor Estimating: 288it [02:59,  1.52it/s]Extractor Estimating: 289it [02:59,  1.51it/s]Extractor Estimating: 290it [03:00,  1.51it/s]Extractor Estimating: 291it [03:01,  1.55it/s]Extractor Estimating: 292it [03:01,  1.56it/s]Extractor Estimating: 293it [03:02,  1.58it/s]Extractor Estimating: 294it [03:03,  1.56it/s]Extractor Estimating: 295it [03:03,  1.59it/s]Extractor Estimating: 296it [03:04,  1.60it/s]Extractor Estimating: 297it [03:05,  1.59it/s]Extractor Estimating: 298it [03:05,  1.58it/s]Extractor Estimating: 299it [03:06,  1.63it/s]Extractor Estimating: 300it [03:06,  1.64it/s]Extractor Estimating: 301it [03:07,  1.60it/s]Extractor Estimating: 302it [03:08,  1.57it/s]Extractor Estimating: 303it [03:08,  1.58it/s]Extractor Estimating: 304it [03:09,  1.61it/s]Extractor Estimating: 305it [03:09,  1.63it/s]Extractor Estimating: 306it [03:10,  1.68it/s]Extractor Estimating: 307it [03:11,  1.59it/s]Extractor Estimating: 308it [03:11,  1.57it/s]Extractor Estimating: 309it [03:12,  1.59it/s]Extractor Estimating: 310it [03:13,  1.57it/s]Extractor Estimating: 311it [03:13,  1.54it/s]Extractor Estimating: 312it [03:14,  1.52it/s]Extractor Estimating: 313it [03:15,  1.54it/s]Extractor Estimating: 314it [03:15,  1.52it/s]Extractor Estimating: 315it [03:16,  1.49it/s]Extractor Estimating: 316it [03:17,  1.51it/s]Extractor Estimating: 317it [03:17,  1.51it/s]Extractor Estimating: 318it [03:18,  1.54it/s]Extractor Estimating: 319it [03:19,  1.60it/s]Extractor Estimating: 320it [03:19,  1.59it/s]Extractor Estimating: 321it [03:20,  1.60it/s]Extractor Estimating: 322it [03:20,  1.58it/s]Extractor Estimating: 323it [03:21,  1.59it/s]Extractor Estimating: 324it [03:22,  1.61it/s]Extractor Estimating: 325it [03:22,  1.53it/s]Extractor Estimating: 326it [03:23,  1.50it/s]Extractor Estimating: 327it [03:24,  1.33it/s]Extractor Estimating: 328it [03:25,  1.36it/s]Extractor Estimating: 329it [03:25,  1.36it/s]Extractor Estimating: 330it [03:26,  1.37it/s]Extractor Estimating: 331it [03:27,  1.35it/s]Extractor Estimating: 332it [03:28,  1.40it/s]Extractor Estimating: 333it [03:28,  1.42it/s]Extractor Estimating: 334it [03:29,  1.42it/s]Extractor Estimating: 335it [03:30,  1.47it/s]Extractor Estimating: 336it [03:30,  1.43it/s]Extractor Estimating: 337it [03:31,  1.43it/s]Extractor Estimating: 338it [03:32,  1.44it/s]Extractor Estimating: 339it [03:32,  1.47it/s]Extractor Estimating: 340it [03:33,  1.45it/s]Extractor Estimating: 341it [03:34,  1.43it/s]Extractor Estimating: 342it [03:34,  1.48it/s]Extractor Estimating: 343it [03:35,  1.47it/s]Extractor Estimating: 344it [03:36,  1.47it/s]Extractor Estimating: 345it [03:37,  1.45it/s]Extractor Estimating: 346it [03:37,  1.46it/s]Extractor Estimating: 347it [03:38,  1.47it/s]Extractor Estimating: 348it [03:39,  1.45it/s]Extractor Estimating: 349it [03:39,  1.45it/s]Extractor Estimating: 350it [03:40,  1.44it/s]Extractor Estimating: 351it [03:41,  1.52it/s]Extractor Estimating: 352it [03:41,  1.51it/s]Extractor Estimating: 353it [03:42,  1.49it/s]Extractor Estimating: 354it [03:43,  1.52it/s]Extractor Estimating: 355it [03:43,  1.48it/s]Extractor Estimating: 356it [03:44,  1.47it/s]Extractor Estimating: 357it [03:45,  1.51it/s]Extractor Estimating: 358it [03:45,  1.53it/s]Extractor Estimating: 359it [03:46,  1.56it/s]Extractor Estimating: 360it [03:46,  1.60it/s]Extractor Estimating: 361it [03:47,  1.57it/s]Extractor Estimating: 362it [03:48,  1.56it/s]Extractor Estimating: 363it [03:48,  1.59it/s]Extractor Estimating: 364it [03:49,  1.56it/s]Extractor Estimating: 365it [03:50,  1.58it/s]Extractor Estimating: 366it [03:50,  1.49it/s]Extractor Estimating: 367it [03:51,  1.54it/s]Extractor Estimating: 368it [03:53,  1.05s/it]Extractor Estimating: 369it [03:54,  1.08it/s]Extractor Estimating: 370it [03:54,  1.19it/s]Extractor Estimating: 371it [03:55,  1.28it/s]Extractor Estimating: 372it [03:56,  1.34it/s]Extractor Estimating: 373it [03:56,  1.41it/s]Extractor Estimating: 374it [03:57,  1.44it/s]Extractor Estimating: 375it [03:57,  1.76it/s]Extractor Estimating: 375it [03:57,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:00,463 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:00,492 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:00,493 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:00,493 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:00,493 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:24:01,276 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:24:01,277 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:24:01,864 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:24:03,019 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:24:03,019 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:06,048 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:06,090 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:06,091 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:06,091 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:06,091 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:24:06,995 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:24:06,997 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:24:07,656 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:24:07,879 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:24:07,879 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 09:04:28,831 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 09:04:29,149 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7465 mean pseudo reward: 0.934588648968316
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl'}
train vocab size: 29198
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 29298, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=29298, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.042, loss:715.8413
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.019, loss:690.6275
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.024, loss:634.1809
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 1.035, loss:598.3402
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 1.042, loss:623.8321
>> valid entity prec:0.5143, rec:0.4494, f1:0.4797
>> valid relation prec:0.1390, rec:0.0382, f1:0.0600
>> valid relation with NER prec:0.1390, rec:0.0382, f1:0.0600
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 3.673, loss:637.1748
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.031, loss:605.4635
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.032, loss:610.6481
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 1.019, loss:635.1289
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 1.017, loss:585.2665
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5269, rec:0.4695, f1:0.4965
>> valid relation prec:0.1414, rec:0.0364, f1:0.0578
>> valid relation with NER prec:0.1414, rec:0.0364, f1:0.0578
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 164, avg_time 3.681, loss:610.4870
g_step 1200, step 264, avg_time 1.025, loss:657.7503
g_step 1300, step 52, avg_time 1.026, loss:618.8265
g_step 1400, step 152, avg_time 1.018, loss:576.7737
g_step 1500, step 252, avg_time 1.035, loss:591.3536
>> valid entity prec:0.5348, rec:0.3320, f1:0.4097
>> valid relation prec:0.0931, rec:0.0124, f1:0.0219
>> valid relation with NER prec:0.0931, rec:0.0124, f1:0.0219
g_step 1600, step 40, avg_time 3.649, loss:593.1036
g_step 1700, step 140, avg_time 1.027, loss:555.2905
g_step 1800, step 240, avg_time 1.016, loss:583.9033
g_step 1900, step 28, avg_time 1.014, loss:570.2615
g_step 2000, step 128, avg_time 1.032, loss:540.3643
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5044, rec:0.4624, f1:0.4825
>> valid relation prec:0.0965, rec:0.0295, f1:0.0451
>> valid relation with NER prec:0.0965, rec:0.0295, f1:0.0451
g_step 2100, step 228, avg_time 3.679, loss:550.1678
g_step 2200, step 16, avg_time 1.008, loss:549.4479
g_step 2300, step 116, avg_time 1.025, loss:511.9037
g_step 2400, step 216, avg_time 1.026, loss:526.3341
g_step 2500, step 4, avg_time 1.014, loss:547.4720
>> valid entity prec:0.4722, rec:0.4220, f1:0.4457
>> valid relation prec:0.1253, rec:0.0278, f1:0.0455
>> valid relation with NER prec:0.1253, rec:0.0278, f1:0.0455
g_step 2600, step 104, avg_time 3.647, loss:475.8326
g_step 2700, step 204, avg_time 1.035, loss:512.0448
g_step 2800, step 304, avg_time 1.032, loss:518.7617
g_step 2900, step 92, avg_time 1.026, loss:456.1048
g_step 3000, step 192, avg_time 1.040, loss:481.9851
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4468, rec:0.4591, f1:0.4529
>> valid relation prec:0.1436, rec:0.0431, f1:0.0664
>> valid relation with NER prec:0.1436, rec:0.0431, f1:0.0664
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 292, avg_time 3.664, loss:512.2800
g_step 3200, step 80, avg_time 1.020, loss:445.3395
g_step 3300, step 180, avg_time 1.031, loss:458.4584
g_step 3400, step 280, avg_time 1.040, loss:495.0855
g_step 3500, step 68, avg_time 1.036, loss:443.8808
>> valid entity prec:0.4793, rec:0.4580, f1:0.4684
>> valid relation prec:0.1101, rec:0.0302, f1:0.0474
>> valid relation with NER prec:0.1101, rec:0.0302, f1:0.0474
g_step 3600, step 168, avg_time 3.676, loss:465.2655
g_step 3700, step 268, avg_time 1.040, loss:458.4694
g_step 3800, step 56, avg_time 1.032, loss:434.8362
g_step 3900, step 156, avg_time 1.042, loss:432.5490
g_step 4000, step 256, avg_time 1.034, loss:453.7860
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4989, rec:0.4876, f1:0.4932
>> valid relation prec:0.1205, rec:0.0303, f1:0.0484
>> valid relation with NER prec:0.1205, rec:0.0303, f1:0.0484
g_step 4100, step 44, avg_time 3.732, loss:431.0825
g_step 4200, step 144, avg_time 1.054, loss:397.7093
g_step 4300, step 244, avg_time 1.039, loss:414.5051
g_step 4400, step 32, avg_time 1.019, loss:414.7228
g_step 4500, step 132, avg_time 1.033, loss:404.0077
>> valid entity prec:0.4931, rec:0.3800, f1:0.4292
>> valid relation prec:0.1197, rec:0.0275, f1:0.0447
>> valid relation with NER prec:0.1197, rec:0.0275, f1:0.0447
g_step 4600, step 232, avg_time 3.702, loss:412.6180
g_step 4700, step 20, avg_time 1.028, loss:415.4637
g_step 4800, step 120, avg_time 1.040, loss:375.3208
g_step 4900, step 220, avg_time 1.043, loss:406.4918
g_step 5000, step 8, avg_time 1.029, loss:404.7954
learning rate was adjusted to 0.0008
>> valid entity prec:0.4921, rec:0.4304, f1:0.4592
>> valid relation prec:0.1075, rec:0.0269, f1:0.0430
>> valid relation with NER prec:0.1075, rec:0.0269, f1:0.0430
g_step 5100, step 108, avg_time 3.711, loss:378.3476
g_step 5200, step 208, avg_time 1.043, loss:374.8519
g_step 5300, step 308, avg_time 1.029, loss:395.9080
g_step 5400, step 96, avg_time 1.050, loss:346.8256
g_step 5500, step 196, avg_time 1.033, loss:377.7386
>> valid entity prec:0.4747, rec:0.3774, f1:0.4205
>> valid relation prec:0.0883, rec:0.0230, f1:0.0365
>> valid relation with NER prec:0.0883, rec:0.0230, f1:0.0365
g_step 5600, step 296, avg_time 3.699, loss:389.2462
g_step 5700, step 84, avg_time 1.038, loss:343.2228
g_step 5800, step 184, avg_time 1.013, loss:362.7497
g_step 5900, step 284, avg_time 1.049, loss:383.8389
g_step 6000, step 72, avg_time 1.012, loss:340.1415
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5042, rec:0.4497, f1:0.4754
>> valid relation prec:0.0886, rec:0.0254, f1:0.0395
>> valid relation with NER prec:0.0886, rec:0.0254, f1:0.0395
g_step 6100, step 172, avg_time 3.693, loss:342.0320
g_step 6200, step 272, avg_time 1.050, loss:356.8607
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 09:04:29 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 09:04:29 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_09-04-28_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 09:04:30 - WARNING - datasets.builder -   Using custom data configuration default-a02d357b2b65afc2
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-a02d357b2b65afc2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 09:04:33,811 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:04:33,813 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 09:04:33,813 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:04:33,814 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 09:04:33,994 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:04:34,096 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:04:34,096 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:04:34,096 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:04:34,096 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:04:34,096 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:04:34,096 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 09:04:34,759 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 09:04:37,910 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 09:04:37,954 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-a02d357b2b65afc2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  2.23ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.23ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.74ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.05ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.24ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.39ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.48ba/s]100%|██████████| 8/8 [00:01<00:00,  5.35ba/s]100%|██████████| 8/8 [00:01<00:00,  4.33ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:04,  1.79ba/s] 22%|██▏       | 2/9 [00:00<00:02,  2.82ba/s] 33%|███▎      | 3/9 [00:00<00:01,  3.45ba/s] 44%|████▍     | 4/9 [00:01<00:01,  3.85ba/s] 56%|█████▌    | 5/9 [00:01<00:00,  4.11ba/s] 67%|██████▋   | 6/9 [00:01<00:00,  4.27ba/s] 78%|███████▊  | 7/9 [00:01<00:00,  4.38ba/s] 89%|████████▉ | 8/9 [00:02<00:00,  4.47ba/s]100%|██████████| 9/9 [00:02<00:00,  5.20ba/s]100%|██████████| 9/9 [00:02<00:00,  4.13ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  5.60ba/s] 38%|███▊      | 3/8 [00:00<00:00,  8.78ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.69ba/s] 75%|███████▌  | 6/8 [00:00<00:00,  9.41ba/s]100%|██████████| 8/8 [00:00<00:00, 11.00ba/s]100%|██████████| 8/8 [00:00<00:00,  9.95ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:01,  6.17ba/s] 33%|███▎      | 3/9 [00:00<00:00,  9.16ba/s] 56%|█████▌    | 5/9 [00:00<00:00, 10.03ba/s] 78%|███████▊  | 7/9 [00:00<00:00, 10.48ba/s]100%|██████████| 9/9 [00:00<00:00, 11.50ba/s]100%|██████████| 9/9 [00:00<00:00, 10.59ba/s]
[INFO|trainer.py:414] 2023-08-29 09:04:45,757 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 09:04:45,828 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 09:04:45,828 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 09:04:45,828 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 09:04:45,828 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 09:04:45,828 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 09:04:45,828 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 09:04:45,828 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:59,  3.26it/s]  0%|          | 2/585 [00:00<02:53,  3.36it/s]  1%|          | 3/585 [00:00<02:50,  3.40it/s]  1%|          | 4/585 [00:01<02:49,  3.42it/s]  1%|          | 5/585 [00:01<02:53,  3.34it/s]  1%|          | 6/585 [00:01<02:52,  3.36it/s]  1%|          | 7/585 [00:02<02:51,  3.37it/s]  1%|▏         | 8/585 [00:02<02:50,  3.38it/s]  2%|▏         | 9/585 [00:02<02:50,  3.39it/s]  2%|▏         | 10/585 [00:02<02:49,  3.39it/s]  2%|▏         | 11/585 [00:03<02:49,  3.39it/s]  2%|▏         | 12/585 [00:03<02:48,  3.41it/s]  2%|▏         | 13/585 [00:03<02:47,  3.42it/s]  2%|▏         | 14/585 [00:04<02:46,  3.43it/s]  3%|▎         | 15/585 [00:04<02:46,  3.43it/s]  3%|▎         | 16/585 [00:04<02:50,  3.33it/s]  3%|▎         | 17/585 [00:05<02:48,  3.37it/s]  3%|▎         | 18/585 [00:05<02:47,  3.39it/s]  3%|▎         | 19/585 [00:05<02:46,  3.41it/s]  3%|▎         | 20/585 [00:05<02:45,  3.42it/s]  4%|▎         | 21/585 [00:06<02:44,  3.43it/s]  4%|▍         | 22/585 [00:06<02:43,  3.43it/s]  4%|▍         | 23/585 [00:06<02:43,  3.44it/s]  4%|▍         | 24/585 [00:07<02:43,  3.44it/s]  4%|▍         | 25/585 [00:07<02:42,  3.44it/s]  4%|▍         | 26/585 [00:07<02:42,  3.45it/s]  5%|▍         | 27/585 [00:07<02:46,  3.36it/s]  5%|▍         | 28/585 [00:08<02:44,  3.39it/s]  5%|▍         | 29/585 [00:08<02:43,  3.40it/s]  5%|▌         | 30/585 [00:08<02:42,  3.42it/s]  5%|▌         | 31/585 [00:09<02:41,  3.42it/s]  5%|▌         | 32/585 [00:09<02:41,  3.43it/s]  6%|▌         | 33/585 [00:09<02:40,  3.43it/s]  6%|▌         | 34/585 [00:09<02:40,  3.44it/s]  6%|▌         | 35/585 [00:10<02:39,  3.44it/s]  6%|▌         | 36/585 [00:10<02:39,  3.45it/s]  6%|▋         | 37/585 [00:10<02:39,  3.45it/s]  6%|▋         | 38/585 [00:11<02:41,  3.39it/s]  7%|▋         | 39/585 [00:11<02:40,  3.40it/s]  7%|▋         | 40/585 [00:11<02:39,  3.42it/s]  7%|▋         | 41/585 [00:12<02:38,  3.42it/s]  7%|▋         | 42/585 [00:12<02:38,  3.43it/s]  7%|▋         | 43/585 [00:12<02:37,  3.43it/s]  8%|▊         | 44/585 [00:12<02:37,  3.43it/s]  8%|▊         | 45/585 [00:13<02:37,  3.44it/s]  8%|▊         | 46/585 [00:13<02:36,  3.44it/s]  8%|▊         | 47/585 [00:13<02:36,  3.44it/s]  8%|▊         | 48/585 [00:14<02:35,  3.44it/s]  8%|▊         | 49/585 [00:14<02:35,  3.44it/s]  9%|▊         | 50/585 [00:14<02:35,  3.45it/s]  9%|▊         | 51/585 [00:14<02:35,  3.44it/s]  9%|▉         | 52/585 [00:15<02:34,  3.44it/s]  9%|▉         | 53/585 [00:15<02:34,  3.44it/s]  9%|▉         | 54/585 [00:15<02:34,  3.44it/s]  9%|▉         | 55/585 [00:16<02:33,  3.44it/s] 10%|▉         | 56/585 [00:16<02:33,  3.44it/s] 10%|▉         | 57/585 [00:16<02:36,  3.37it/s] 10%|▉         | 58/585 [00:16<02:35,  3.39it/s] 10%|█         | 59/585 [00:17<02:34,  3.41it/s] 10%|█         | 60/585 [00:17<02:33,  3.42it/s] 10%|█         | 61/585 [00:17<02:32,  3.43it/s] 11%|█         | 62/585 [00:18<02:32,  3.43it/s] 11%|█         | 63/585 [00:18<02:31,  3.43it/s] 11%|█         | 64/585 [00:18<02:31,  3.44it/s] 11%|█         | 65/585 [00:19<02:31,  3.44it/s] 11%|█▏        | 66/585 [00:19<02:30,  3.44it/s] 11%|█▏        | 67/585 [00:19<02:30,  3.44it/s] 12%|█▏        | 68/585 [00:19<02:32,  3.39it/s] 12%|█▏        | 69/585 [00:20<02:31,  3.41it/s] 12%|█▏        | 70/585 [00:20<02:30,  3.42it/s] 12%|█▏        | 71/585 [00:20<02:30,  3.43it/s] 12%|█▏        | 72/585 [00:21<02:29,  3.43it/s] 12%|█▏        | 73/585 [00:21<02:29,  3.43it/s] 13%|█▎        | 74/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 75/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 76/585 [00:22<02:27,  3.44it/s] 13%|█▎        | 77/585 [00:22<02:27,  3.44it/s] 13%|█▎        | 78/585 [00:22<02:27,  3.44it/s] 14%|█▎        | 79/585 [00:23<02:29,  3.39it/s] 14%|█▎        | 80/585 [00:23<02:28,  3.41it/s] 14%|█▍        | 81/585 [00:23<02:27,  3.42it/s] 14%|█▍        | 82/585 [00:23<02:26,  3.42it/s] 14%|█▍        | 83/585 [00:24<02:26,  3.43it/s] 14%|█▍        | 84/585 [00:24<02:26,  3.43it/s] 15%|█▍        | 85/585 [00:24<02:25,  3.44it/s] 15%|█▍        | 86/585 [00:25<02:25,  3.44it/s] 15%|█▍        | 87/585 [00:25<02:24,  3.44it/s] 15%|█▌        | 88/585 [00:25<02:24,  3.44it/s] 15%|█▌        | 89/585 [00:26<02:24,  3.44it/s] 15%|█▌        | 90/585 [00:26<02:26,  3.39it/s] 16%|█▌        | 91/585 [00:26<02:25,  3.40it/s] 16%|█▌        | 92/585 [00:26<02:24,  3.41it/s] 16%|█▌        | 93/585 [00:27<02:23,  3.42it/s] 16%|█▌        | 94/585 [00:27<02:23,  3.43it/s] 16%|█▌        | 95/585 [00:27<02:22,  3.43it/s] 16%|█▋        | 96/585 [00:28<02:22,  3.44it/s] 17%|█▋        | 97/585 [00:28<02:22,  3.44it/s] 17%|█▋        | 98/585 [00:28<02:21,  3.44it/s] 17%|█▋        | 99/585 [00:28<02:21,  3.44it/s] 17%|█▋        | 100/585 [00:29<02:20,  3.44it/s] 17%|█▋        | 101/585 [00:29<02:24,  3.36it/s] 17%|█▋        | 102/585 [00:29<02:22,  3.38it/s] 18%|█▊        | 103/585 [00:30<02:21,  3.40it/s] 18%|█▊        | 104/585 [00:30<02:20,  3.41it/s] 18%|█▊        | 105/585 [00:30<02:20,  3.42it/s] 18%|█▊        | 106/585 [00:31<02:19,  3.43it/s] 18%|█▊        | 107/585 [00:31<02:19,  3.44it/s] 18%|█▊        | 108/585 [00:31<02:18,  3.44it/s] 19%|█▊        | 109/585 [00:31<02:18,  3.44it/s] 19%|█▉        | 110/585 [00:32<02:17,  3.44it/s] 19%|█▉        | 111/585 [00:32<02:17,  3.44it/s] 19%|█▉        | 112/585 [00:32<02:20,  3.36it/s] 19%|█▉        | 113/585 [00:33<02:19,  3.38it/s] 19%|█▉        | 114/585 [00:33<02:18,  3.39it/s] 20%|█▉        | 115/585 [00:33<02:17,  3.41it/s] 20%|█▉        | 116/585 [00:33<02:17,  3.42it/s] 20%|██        | 117/585 [00:34<02:16,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 09:05:20,095 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:05:20,095 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 09:05:20,095 >>   Batch size = 8

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:19, 55.19it/s][A
  1%|          | 12/1071 [00:00<00:21, 48.51it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 46.86it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 45.82it/s][A
  3%|▎         | 27/1071 [00:00<00:23, 45.19it/s][A
  3%|▎         | 32/1071 [00:00<00:23, 44.87it/s][A
  3%|▎         | 37/1071 [00:00<00:23, 44.62it/s][A
  4%|▍         | 42/1071 [00:00<00:23, 44.47it/s][A
  4%|▍         | 47/1071 [00:01<00:23, 44.44it/s][A
  5%|▍         | 52/1071 [00:01<00:22, 44.62it/s][A
  5%|▌         | 57/1071 [00:01<00:22, 44.78it/s][A
  6%|▌         | 62/1071 [00:01<00:22, 44.11it/s][A
  6%|▋         | 67/1071 [00:01<00:22, 44.36it/s][A
  7%|▋         | 72/1071 [00:01<00:22, 44.47it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 44.37it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 44.45it/s][A
  8%|▊         | 87/1071 [00:01<00:22, 44.41it/s][A
  9%|▊         | 92/1071 [00:02<00:21, 44.55it/s][A
  9%|▉         | 97/1071 [00:02<00:21, 44.52it/s][A
 10%|▉         | 102/1071 [00:02<00:21, 44.50it/s][A
 10%|▉         | 107/1071 [00:02<00:21, 44.69it/s][A
 10%|█         | 112/1071 [00:02<00:21, 44.73it/s][A
 11%|█         | 117/1071 [00:02<00:21, 44.79it/s][A
 11%|█▏        | 122/1071 [00:02<00:21, 44.73it/s][A
 12%|█▏        | 127/1071 [00:02<00:21, 44.60it/s][A
 12%|█▏        | 132/1071 [00:02<00:21, 44.61it/s][A
 13%|█▎        | 137/1071 [00:03<00:20, 44.71it/s][A
 13%|█▎        | 142/1071 [00:03<00:20, 44.69it/s][A
 14%|█▎        | 147/1071 [00:03<00:20, 44.82it/s][A
 14%|█▍        | 152/1071 [00:03<00:20, 44.74it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 44.73it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 44.78it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 44.75it/s][A
 16%|█▌        | 172/1071 [00:03<00:20, 44.53it/s][A
 17%|█▋        | 177/1071 [00:03<00:20, 44.55it/s][A
 17%|█▋        | 182/1071 [00:04<00:19, 44.58it/s][A
 17%|█▋        | 187/1071 [00:04<00:19, 44.60it/s][A
 18%|█▊        | 192/1071 [00:04<00:19, 44.72it/s][A
 18%|█▊        | 197/1071 [00:04<00:20, 43.39it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 43.87it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 44.20it/s][A
 20%|█▉        | 212/1071 [00:04<00:19, 44.28it/s][A
 20%|██        | 217/1071 [00:04<00:19, 44.28it/s][A
 21%|██        | 222/1071 [00:04<00:19, 44.42it/s][A
 21%|██        | 227/1071 [00:05<00:18, 44.45it/s][A
 22%|██▏       | 232/1071 [00:05<00:18, 44.54it/s][A
 22%|██▏       | 237/1071 [00:05<00:18, 44.45it/s][A
 23%|██▎       | 242/1071 [00:05<00:18, 44.57it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 44.63it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 44.76it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 44.59it/s][A
 24%|██▍       | 262/1071 [00:05<00:18, 44.54it/s][A
 25%|██▍       | 267/1071 [00:05<00:18, 44.47it/s][A
 25%|██▌       | 272/1071 [00:06<00:18, 44.38it/s][A
 26%|██▌       | 277/1071 [00:06<00:17, 44.26it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 44.45it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 44.51it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 44.76it/s][A
 28%|██▊       | 297/1071 [00:06<00:18, 42.12it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 43.69it/s][A
 29%|██▊       | 307/1071 [00:06<00:17, 44.00it/s][A
 29%|██▉       | 312/1071 [00:06<00:17, 44.21it/s][A
 30%|██▉       | 317/1071 [00:07<00:17, 44.01it/s][A
 30%|███       | 322/1071 [00:07<00:16, 44.16it/s][A
 31%|███       | 327/1071 [00:07<00:16, 44.13it/s][A
 31%|███       | 332/1071 [00:07<00:17, 42.79it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 43.49it/s][A
 32%|███▏      | 342/1071 [00:07<00:16, 43.76it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 44.16it/s][A
 33%|███▎      | 352/1071 [00:07<00:16, 44.44it/s][A
 33%|███▎      | 357/1071 [00:08<00:16, 44.49it/s][A
 34%|███▍      | 362/1071 [00:08<00:15, 44.48it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.51it/s][A
 35%|███▍      | 372/1071 [00:08<00:15, 44.37it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 44.51it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 44.44it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 44.71it/s][A
 37%|███▋      | 392/1071 [00:08<00:17, 38.37it/s][A
 37%|███▋      | 397/1071 [00:08<00:17, 39.36it/s][A
 38%|███▊      | 402/1071 [00:09<00:16, 41.20it/s][A
 38%|███▊      | 407/1071 [00:09<00:15, 42.17it/s][A
 38%|███▊      | 412/1071 [00:09<00:15, 42.96it/s][A
 39%|███▉      | 417/1071 [00:09<00:15, 43.55it/s][A
 39%|███▉      | 422/1071 [00:09<00:14, 44.04it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 44.36it/s][A
 40%|████      | 432/1071 [00:09<00:14, 44.12it/s][A
 41%|████      | 437/1071 [00:09<00:14, 44.06it/s][A
 41%|████▏     | 442/1071 [00:09<00:14, 44.06it/s][A
 42%|████▏     | 447/1071 [00:10<00:14, 44.15it/s][A
 42%|████▏     | 452/1071 [00:10<00:13, 44.49it/s][A
 43%|████▎     | 457/1071 [00:10<00:13, 44.72it/s][A
 43%|████▎     | 462/1071 [00:10<00:13, 44.97it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 45.09it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 45.07it/s][A
 45%|████▍     | 477/1071 [00:10<00:13, 44.80it/s][A
 45%|████▌     | 482/1071 [00:10<00:13, 44.48it/s][A
 45%|████▌     | 487/1071 [00:10<00:13, 44.44it/s][A
 46%|████▌     | 492/1071 [00:11<00:13, 44.23it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.56it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.71it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 44.79it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 44.90it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 44.93it/s][A
 49%|████▊     | 522/1071 [00:11<00:12, 44.78it/s][A
 49%|████▉     | 527/1071 [00:11<00:12, 44.94it/s][A
 50%|████▉     | 532/1071 [00:12<00:12, 44.78it/s][A
 50%|█████     | 537/1071 [00:12<00:11, 44.70it/s][A
 51%|█████     | 542/1071 [00:12<00:11, 44.66it/s][A
 51%|█████     | 547/1071 [00:12<00:11, 44.82it/s][A
 52%|█████▏    | 552/1071 [00:12<00:11, 44.93it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 44.87it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 44.79it/s][A
 53%|█████▎    | 567/1071 [00:12<00:11, 44.70it/s][A
 53%|█████▎    | 572/1071 [00:12<00:11, 44.72it/s][A
 54%|█████▍    | 577/1071 [00:13<00:11, 41.54it/s][A
 54%|█████▍    | 582/1071 [00:13<00:11, 42.58it/s][A
 55%|█████▍    | 587/1071 [00:13<00:11, 43.46it/s][A
 55%|█████▌    | 592/1071 [00:13<00:10, 43.87it/s][A
 56%|█████▌    | 597/1071 [00:13<00:10, 44.16it/s][A
 56%|█████▌    | 602/1071 [00:13<00:10, 44.23it/s][A
 57%|█████▋    | 607/1071 [00:13<00:10, 44.48it/s][A
 57%|█████▋    | 612/1071 [00:13<00:10, 44.50it/s][A
 58%|█████▊    | 617/1071 [00:13<00:10, 44.15it/s][A
 58%|█████▊    | 622/1071 [00:14<00:10, 44.36it/s][A
 59%|█████▊    | 627/1071 [00:14<00:09, 44.64it/s][A
 59%|█████▉    | 632/1071 [00:14<00:09, 44.64it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.85it/s][A
 60%|█████▉    | 642/1071 [00:14<00:09, 44.86it/s][A
 60%|██████    | 647/1071 [00:14<00:09, 44.71it/s][A
 61%|██████    | 652/1071 [00:14<00:09, 44.92it/s][A
 61%|██████▏   | 657/1071 [00:14<00:09, 44.63it/s][A
 62%|██████▏   | 662/1071 [00:14<00:09, 44.64it/s][A
 62%|██████▏   | 667/1071 [00:15<00:09, 44.52it/s][A
 63%|██████▎   | 672/1071 [00:15<00:08, 44.47it/s][A
 63%|██████▎   | 677/1071 [00:15<00:08, 44.52it/s][A
 64%|██████▎   | 682/1071 [00:15<00:08, 44.56it/s][A
 64%|██████▍   | 687/1071 [00:15<00:08, 44.89it/s][A
 65%|██████▍   | 692/1071 [00:15<00:08, 44.74it/s][A
 65%|██████▌   | 697/1071 [00:15<00:08, 44.85it/s][A
 66%|██████▌   | 702/1071 [00:15<00:08, 44.79it/s][A
 66%|██████▌   | 707/1071 [00:15<00:08, 44.48it/s][A
 66%|██████▋   | 712/1071 [00:16<00:08, 41.29it/s][A
 67%|██████▋   | 717/1071 [00:16<00:08, 42.41it/s][A
 67%|██████▋   | 722/1071 [00:16<00:08, 43.24it/s][A
 68%|██████▊   | 727/1071 [00:16<00:07, 43.94it/s][A
 68%|██████▊   | 732/1071 [00:16<00:08, 40.69it/s][A
 69%|██████▉   | 738/1071 [00:16<00:07, 43.49it/s][A
 69%|██████▉   | 743/1071 [00:16<00:07, 44.00it/s][A
 70%|██████▉   | 748/1071 [00:16<00:07, 44.12it/s][A
 70%|███████   | 753/1071 [00:17<00:07, 43.95it/s][A
 71%|███████   | 758/1071 [00:17<00:07, 44.09it/s][A
 71%|███████   | 763/1071 [00:17<00:06, 44.28it/s][A
 72%|███████▏  | 768/1071 [00:17<00:06, 44.62it/s][A
 72%|███████▏  | 773/1071 [00:17<00:06, 44.76it/s][A
 73%|███████▎  | 778/1071 [00:17<00:06, 44.70it/s][A
 73%|███████▎  | 783/1071 [00:17<00:06, 44.83it/s][A
 74%|███████▎  | 788/1071 [00:17<00:06, 45.04it/s][A
 74%|███████▍  | 793/1071 [00:17<00:06, 44.75it/s][A
 75%|███████▍  | 798/1071 [00:18<00:06, 44.61it/s][A
 75%|███████▍  | 803/1071 [00:18<00:06, 44.67it/s][A
 75%|███████▌  | 808/1071 [00:18<00:05, 44.41it/s][A
 76%|███████▌  | 813/1071 [00:18<00:05, 44.60it/s][A
 76%|███████▋  | 818/1071 [00:18<00:05, 44.75it/s][A
 77%|███████▋  | 823/1071 [00:18<00:05, 44.58it/s][A
 77%|███████▋  | 828/1071 [00:18<00:05, 44.82it/s][A
 78%|███████▊  | 833/1071 [00:18<00:05, 44.77it/s][A
 78%|███████▊  | 838/1071 [00:18<00:05, 44.66it/s][A
 79%|███████▊  | 843/1071 [00:19<00:05, 44.54it/s][A
 79%|███████▉  | 848/1071 [00:19<00:05, 40.77it/s][A
 80%|███████▉  | 853/1071 [00:19<00:05, 42.06it/s][A
 80%|████████  | 858/1071 [00:19<00:04, 42.85it/s][A
 81%|████████  | 863/1071 [00:19<00:04, 43.50it/s][A
 81%|████████  | 868/1071 [00:19<00:04, 44.03it/s][A
 82%|████████▏ | 873/1071 [00:19<00:04, 44.53it/s][A
 82%|████████▏ | 878/1071 [00:19<00:04, 44.56it/s][A
 82%|████████▏ | 883/1071 [00:19<00:04, 44.55it/s][A
 83%|████████▎ | 888/1071 [00:20<00:04, 44.13it/s][A
 83%|████████▎ | 893/1071 [00:20<00:04, 44.11it/s][A
 84%|████████▍ | 898/1071 [00:20<00:03, 44.39it/s][A
 84%|████████▍ | 903/1071 [00:20<00:03, 44.53it/s][A
 85%|████████▍ | 908/1071 [00:20<00:03, 44.71it/s][A
 85%|████████▌ | 913/1071 [00:20<00:03, 45.06it/s][A
 86%|████████▌ | 918/1071 [00:20<00:03, 44.95it/s][A
 86%|████████▌ | 923/1071 [00:20<00:03, 44.77it/s][A
 87%|████████▋ | 928/1071 [00:20<00:03, 44.30it/s][A
 87%|████████▋ | 933/1071 [00:21<00:03, 44.17it/s][A
 88%|████████▊ | 938/1071 [00:21<00:03, 44.30it/s][A
 88%|████████▊ | 943/1071 [00:21<00:02, 44.29it/s][A
 89%|████████▊ | 948/1071 [00:21<00:02, 44.47it/s][A
 89%|████████▉ | 953/1071 [00:21<00:02, 44.56it/s][A
 89%|████████▉ | 958/1071 [00:21<00:02, 44.83it/s][A
 90%|████████▉ | 963/1071 [00:21<00:02, 44.77it/s][A
 90%|█████████ | 968/1071 [00:21<00:02, 44.89it/s][A
 91%|█████████ | 973/1071 [00:21<00:02, 44.83it/s][A
 91%|█████████▏| 978/1071 [00:22<00:02, 44.61it/s][A
 92%|█████████▏| 983/1071 [00:22<00:02, 43.72it/s][A
 92%|█████████▏| 988/1071 [00:22<00:01, 43.92it/s][A
 93%|█████████▎| 993/1071 [00:22<00:01, 44.23it/s][A
 93%|█████████▎| 998/1071 [00:22<00:01, 44.50it/s][A
 94%|█████████▎| 1003/1071 [00:22<00:01, 44.65it/s][A
 94%|█████████▍| 1008/1071 [00:22<00:01, 44.75it/s][A
 95%|█████████▍| 1013/1071 [00:22<00:01, 44.70it/s][A
 95%|█████████▌| 1018/1071 [00:22<00:01, 44.62it/s][A
 96%|█████████▌| 1023/1071 [00:23<00:01, 44.37it/s][A
 96%|█████████▌| 1028/1071 [00:23<00:00, 44.26it/s][A
 96%|█████████▋| 1033/1071 [00:23<00:00, 44.10it/s][A
 97%|█████████▋| 1038/1071 [00:23<00:00, 44.19it/s][A
 97%|█████████▋| 1043/1071 [00:23<00:00, 44.48it/s][A
 98%|█████████▊| 1048/1071 [00:23<00:00, 44.66it/s][A
 98%|█████████▊| 1053/1071 [00:23<00:00, 44.84it/s][A
 99%|█████████▉| 1058/1071 [00:23<00:00, 44.72it/s][A
 99%|█████████▉| 1063/1071 [00:23<00:00, 44.68it/s][A
100%|█████████▉| 1068/1071 [00:24<00:00, 44.23it/s][A
                                                   [A                                                 
100%|██████████| 1071/1071 [00:24<00:00, 44.23it/s][A 20%|██        | 117/585 [00:58<02:16,  3.43it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-29 09:05:44,607 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 09:05:44,776 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:05:46,996 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:05:47,076 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:05:47,139 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:07<1:19:50, 10.26s/it] 20%|██        | 119/585 [01:08<56:29,  7.27s/it]   21%|██        | 120/585 [01:08<40:08,  5.18s/it] 21%|██        | 121/585 [01:08<28:43,  3.71s/it] 21%|██        | 122/585 [01:08<20:44,  2.69s/it] 21%|██        | 123/585 [01:09<15:10,  1.97s/it] 21%|██        | 124/585 [01:09<11:16,  1.47s/it] 21%|██▏       | 125/585 [01:09<08:33,  1.12s/it] 22%|██▏       | 126/585 [01:10<06:38,  1.15it/s] 22%|██▏       | 127/585 [01:10<05:19,  1.44it/s] 22%|██▏       | 128/585 [01:10<04:23,  1.74it/s] 22%|██▏       | 129/585 [01:10<03:44,  2.03it/s] 22%|██▏       | 130/585 [01:11<03:19,  2.28it/s] 22%|██▏       | 131/585 [01:11<02:59,  2.53it/s] 23%|██▎       | 132/585 [01:11<02:45,  2.74it/s] 23%|██▎       | 133/585 [01:12<02:35,  2.91it/s] 23%|██▎       | 134/585 [01:12<02:28,  3.04it/s] 23%|██▎       | 135/585 [01:12<02:23,  3.13it/s] 23%|██▎       | 136/585 [01:13<02:20,  3.21it/s] 23%|██▎       | 137/585 [01:13<02:17,  3.26it/s] 24%|██▎       | 138/585 [01:13<02:15,  3.30it/s] 24%|██▍       | 139/585 [01:13<02:14,  3.33it/s] 24%|██▍       | 140/585 [01:14<02:12,  3.35it/s] 24%|██▍       | 141/585 [01:14<02:14,  3.31it/s] 24%|██▍       | 142/585 [01:14<02:12,  3.33it/s] 24%|██▍       | 143/585 [01:15<02:11,  3.35it/s] 25%|██▍       | 144/585 [01:15<02:10,  3.37it/s] 25%|██▍       | 145/585 [01:15<02:10,  3.38it/s] 25%|██▍       | 146/585 [01:16<02:09,  3.38it/s] 25%|██▌       | 147/585 [01:16<02:09,  3.38it/s] 25%|██▌       | 148/585 [01:16<02:09,  3.38it/s] 25%|██▌       | 149/585 [01:16<02:08,  3.39it/s] 26%|██▌       | 150/585 [01:17<02:08,  3.39it/s] 26%|██▌       | 151/585 [01:17<02:07,  3.39it/s] 26%|██▌       | 152/585 [01:17<02:12,  3.28it/s] 26%|██▌       | 153/585 [01:18<02:10,  3.31it/s] 26%|██▋       | 154/585 [01:18<02:09,  3.34it/s] 26%|██▋       | 155/585 [01:18<02:08,  3.35it/s] 27%|██▋       | 156/585 [01:19<02:07,  3.36it/s] 27%|██▋       | 157/585 [01:19<02:06,  3.37it/s] 27%|██▋       | 158/585 [01:19<02:06,  3.38it/s] 27%|██▋       | 159/585 [01:19<02:05,  3.38it/s] 27%|██▋       | 160/585 [01:20<02:05,  3.39it/s] 28%|██▊       | 161/585 [01:20<02:05,  3.39it/s] 28%|██▊       | 162/585 [01:20<02:04,  3.39it/s] 28%|██▊       | 163/585 [01:21<02:07,  3.31it/s] 28%|██▊       | 164/585 [01:21<02:06,  3.34it/s] 28%|██▊       | 165/585 [01:21<02:05,  3.36it/s] 28%|██▊       | 166/585 [01:21<02:04,  3.37it/s] 29%|██▊       | 167/585 [01:22<02:03,  3.38it/s] 29%|██▊       | 168/585 [01:22<02:03,  3.38it/s] 29%|██▉       | 169/585 [01:22<02:02,  3.39it/s] 29%|██▉       | 170/585 [01:23<02:02,  3.39it/s] 29%|██▉       | 171/585 [01:23<02:02,  3.39it/s] 29%|██▉       | 172/585 [01:23<02:01,  3.39it/s] 30%|██▉       | 173/585 [01:24<02:01,  3.39it/s] 30%|██▉       | 174/585 [01:24<02:03,  3.32it/s] 30%|██▉       | 175/585 [01:24<02:02,  3.34it/s] 30%|███       | 176/585 [01:24<02:02,  3.35it/s] 30%|███       | 177/585 [01:25<02:01,  3.36it/s] 30%|███       | 178/585 [01:25<02:00,  3.37it/s] 31%|███       | 179/585 [01:25<02:00,  3.38it/s] 31%|███       | 180/585 [01:26<01:59,  3.38it/s] 31%|███       | 181/585 [01:26<01:59,  3.38it/s] 31%|███       | 182/585 [01:26<01:58,  3.39it/s] 31%|███▏      | 183/585 [01:27<01:58,  3.39it/s] 31%|███▏      | 184/585 [01:27<01:58,  3.39it/s] 32%|███▏      | 185/585 [01:27<02:00,  3.31it/s] 32%|███▏      | 186/585 [01:27<01:59,  3.34it/s] 32%|███▏      | 187/585 [01:28<01:58,  3.35it/s] 32%|███▏      | 188/585 [01:28<01:58,  3.36it/s] 32%|███▏      | 189/585 [01:28<01:57,  3.37it/s] 32%|███▏      | 190/585 [01:29<01:56,  3.38it/s] 33%|███▎      | 191/585 [01:29<01:56,  3.38it/s] 33%|███▎      | 192/585 [01:29<01:56,  3.39it/s] 33%|███▎      | 193/585 [01:29<01:55,  3.39it/s] 33%|███▎      | 194/585 [01:30<01:55,  3.39it/s] 33%|███▎      | 195/585 [01:30<01:54,  3.39it/s] 34%|███▎      | 196/585 [01:30<01:54,  3.39it/s] 34%|███▎      | 197/585 [01:31<01:54,  3.39it/s] 34%|███▍      | 198/585 [01:31<01:54,  3.39it/s] 34%|███▍      | 199/585 [01:31<01:53,  3.39it/s] 34%|███▍      | 200/585 [01:32<01:53,  3.39it/s] 34%|███▍      | 201/585 [01:32<01:53,  3.39it/s] 35%|███▍      | 202/585 [01:32<01:52,  3.39it/s] 35%|███▍      | 203/585 [01:32<01:52,  3.39it/s] 35%|███▍      | 204/585 [01:33<01:52,  3.39it/s] 35%|███▌      | 205/585 [01:33<01:54,  3.33it/s] 35%|███▌      | 206/585 [01:33<01:53,  3.35it/s] 35%|███▌      | 207/585 [01:34<01:52,  3.36it/s] 36%|███▌      | 208/585 [01:34<01:51,  3.37it/s] 36%|███▌      | 209/585 [01:34<01:51,  3.38it/s] 36%|███▌      | 210/585 [01:35<01:50,  3.38it/s] 36%|███▌      | 211/585 [01:35<01:50,  3.38it/s] 36%|███▌      | 212/585 [01:35<01:50,  3.38it/s] 36%|███▋      | 213/585 [01:35<01:49,  3.39it/s] 37%|███▋      | 214/585 [01:36<01:49,  3.39it/s] 37%|███▋      | 215/585 [01:36<01:49,  3.39it/s] 37%|███▋      | 216/585 [01:36<01:51,  3.31it/s] 37%|███▋      | 217/585 [01:37<01:50,  3.34it/s] 37%|███▋      | 218/585 [01:37<01:49,  3.35it/s] 37%|███▋      | 219/585 [01:37<01:48,  3.37it/s] 38%|███▊      | 220/585 [01:37<01:47,  3.38it/s] 38%|███▊      | 221/585 [01:38<01:47,  3.40it/s] 38%|███▊      | 222/585 [01:38<01:46,  3.41it/s] 38%|███▊      | 223/585 [01:38<01:45,  3.42it/s] 38%|███▊      | 224/585 [01:39<01:45,  3.42it/s] 38%|███▊      | 225/585 [01:39<01:45,  3.43it/s] 39%|███▊      | 226/585 [01:39<01:44,  3.43it/s] 39%|███▉      | 227/585 [01:40<01:47,  3.34it/s] 39%|███▉      | 228/585 [01:40<01:45,  3.37it/s] 39%|███▉      | 229/585 [01:40<01:44,  3.39it/s] 39%|███▉      | 230/585 [01:40<01:44,  3.41it/s] 39%|███▉      | 231/585 [01:41<01:45,  3.37it/s] 40%|███▉      | 232/585 [01:41<01:44,  3.39it/s] 40%|███▉      | 233/585 [01:41<01:43,  3.40it/s] 40%|████      | 234/585 [01:42<01:42,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 09:06:27,970 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:06:27,971 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 09:06:27,971 >>   Batch size = 8
{'eval_loss': 0.9300344586372375, 'eval_runtime': 24.219, 'eval_samples_per_second': 353.772, 'eval_steps_per_second': 44.222, 'epoch': 1.0}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:19, 55.87it/s][A
  1%|          | 12/1071 [00:00<00:21, 48.87it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 46.81it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 45.85it/s][A
  3%|▎         | 27/1071 [00:00<00:23, 45.32it/s][A
  3%|▎         | 32/1071 [00:00<00:23, 44.16it/s][A
  3%|▎         | 37/1071 [00:00<00:23, 44.41it/s][A
  4%|▍         | 42/1071 [00:01<00:31, 32.60it/s][A
  4%|▍         | 46/1071 [00:01<00:32, 31.52it/s][A
  5%|▍         | 51/1071 [00:01<00:29, 35.04it/s][A
  5%|▌         | 56/1071 [00:01<00:26, 37.75it/s][A
  6%|▌         | 61/1071 [00:01<00:25, 39.71it/s][A
  6%|▌         | 66/1071 [00:01<00:24, 41.11it/s][A
  7%|▋         | 71/1071 [00:01<00:23, 42.09it/s][A
  7%|▋         | 76/1071 [00:01<00:23, 43.00it/s][A
  8%|▊         | 81/1071 [00:01<00:22, 43.54it/s][A
  8%|▊         | 86/1071 [00:02<00:22, 43.51it/s][A
  8%|▊         | 91/1071 [00:02<00:22, 43.45it/s][A
  9%|▉         | 96/1071 [00:02<00:22, 43.46it/s][A
  9%|▉         | 101/1071 [00:02<00:22, 43.89it/s][A
 10%|▉         | 106/1071 [00:02<00:21, 44.19it/s][A
 10%|█         | 111/1071 [00:02<00:21, 44.30it/s][A
 11%|█         | 116/1071 [00:02<00:21, 44.76it/s][A
 11%|█▏        | 121/1071 [00:02<00:21, 44.83it/s][A
 12%|█▏        | 126/1071 [00:02<00:21, 44.88it/s][A
 12%|█▏        | 131/1071 [00:03<00:21, 44.50it/s][A
 13%|█▎        | 136/1071 [00:03<00:21, 44.31it/s][A
 13%|█▎        | 141/1071 [00:03<00:21, 44.11it/s][A
 14%|█▎        | 146/1071 [00:03<00:20, 44.27it/s][A
 14%|█▍        | 151/1071 [00:03<00:20, 44.58it/s][A
 15%|█▍        | 156/1071 [00:03<00:20, 44.82it/s][A
 15%|█▌        | 161/1071 [00:03<00:21, 42.51it/s][A
 15%|█▌        | 166/1071 [00:03<00:20, 43.29it/s][A
 16%|█▌        | 171/1071 [00:04<00:20, 43.84it/s][A
 16%|█▋        | 176/1071 [00:04<00:20, 44.14it/s][A
 17%|█▋        | 181/1071 [00:04<00:20, 44.18it/s][A
 17%|█▋        | 186/1071 [00:04<00:19, 44.30it/s][A
 18%|█▊        | 191/1071 [00:04<00:19, 44.36it/s][A
 18%|█▊        | 196/1071 [00:04<00:19, 44.65it/s][A
 19%|█▉        | 201/1071 [00:04<00:19, 44.64it/s][A
 19%|█▉        | 206/1071 [00:04<00:19, 44.48it/s][A
 20%|█▉        | 211/1071 [00:04<00:19, 44.77it/s][A
 20%|██        | 216/1071 [00:05<00:19, 44.74it/s][A
 21%|██        | 221/1071 [00:05<00:18, 44.75it/s][A
 21%|██        | 226/1071 [00:05<00:18, 44.70it/s][A
 22%|██▏       | 231/1071 [00:05<00:18, 44.72it/s][A
 22%|██▏       | 236/1071 [00:05<00:18, 44.68it/s][A
 23%|██▎       | 241/1071 [00:05<00:18, 44.50it/s][A
 23%|██▎       | 246/1071 [00:05<00:18, 44.47it/s][A
 23%|██▎       | 251/1071 [00:05<00:18, 44.57it/s][A
 24%|██▍       | 256/1071 [00:05<00:18, 44.67it/s][A
 24%|██▍       | 261/1071 [00:06<00:18, 44.76it/s][A
 25%|██▍       | 266/1071 [00:06<00:18, 44.69it/s][A
 25%|██▌       | 271/1071 [00:06<00:17, 44.45it/s][A
 26%|██▌       | 276/1071 [00:06<00:17, 44.65it/s][A
 26%|██▌       | 281/1071 [00:06<00:17, 44.78it/s][A
 27%|██▋       | 286/1071 [00:06<00:17, 44.68it/s][A
 27%|██▋       | 291/1071 [00:06<00:17, 44.65it/s][A
 28%|██▊       | 296/1071 [00:06<00:18, 41.16it/s][A
 28%|██▊       | 301/1071 [00:06<00:18, 42.30it/s][A
 29%|██▊       | 306/1071 [00:07<00:17, 43.18it/s][A
 29%|██▉       | 311/1071 [00:07<00:17, 43.75it/s][A
 30%|██▉       | 316/1071 [00:07<00:17, 44.01it/s][A
 30%|██▉       | 321/1071 [00:07<00:16, 44.30it/s][A
 30%|███       | 326/1071 [00:07<00:16, 44.45it/s][A
 31%|███       | 331/1071 [00:07<00:16, 44.44it/s][A
 31%|███▏      | 336/1071 [00:07<00:16, 44.21it/s][A
 32%|███▏      | 341/1071 [00:07<00:16, 44.33it/s][A
 32%|███▏      | 346/1071 [00:07<00:16, 44.50it/s][A
 33%|███▎      | 351/1071 [00:08<00:16, 44.69it/s][A
 33%|███▎      | 356/1071 [00:08<00:15, 44.88it/s][A
 34%|███▎      | 361/1071 [00:08<00:15, 44.87it/s][A
 34%|███▍      | 366/1071 [00:08<00:15, 44.81it/s][A
 35%|███▍      | 371/1071 [00:08<00:15, 44.71it/s][A
 35%|███▌      | 376/1071 [00:08<00:15, 44.37it/s][A
 36%|███▌      | 381/1071 [00:08<00:16, 41.26it/s][A
 36%|███▌      | 386/1071 [00:08<00:16, 42.56it/s][A
 37%|███▋      | 391/1071 [00:08<00:15, 43.34it/s][A
 37%|███▋      | 396/1071 [00:09<00:15, 43.98it/s][A
 37%|███▋      | 401/1071 [00:09<00:15, 44.35it/s][A
 38%|███▊      | 406/1071 [00:09<00:14, 44.35it/s][A
 38%|███▊      | 411/1071 [00:09<00:14, 44.43it/s][A
 39%|███▉      | 416/1071 [00:09<00:14, 44.52it/s][A
 39%|███▉      | 421/1071 [00:09<00:14, 44.31it/s][A
 40%|███▉      | 426/1071 [00:09<00:14, 44.13it/s][A
 40%|████      | 431/1071 [00:09<00:15, 41.65it/s][A
 41%|████      | 436/1071 [00:10<00:14, 42.63it/s][A
 41%|████      | 441/1071 [00:10<00:14, 43.36it/s][A
 42%|████▏     | 446/1071 [00:10<00:14, 43.77it/s][A
 42%|████▏     | 451/1071 [00:10<00:14, 44.26it/s][A
 43%|████▎     | 456/1071 [00:10<00:13, 44.45it/s][A
 43%|████▎     | 461/1071 [00:10<00:13, 44.49it/s][A
 44%|████▎     | 466/1071 [00:10<00:13, 44.68it/s][A
 44%|████▍     | 471/1071 [00:10<00:13, 44.36it/s][A
 44%|████▍     | 476/1071 [00:10<00:13, 44.40it/s][A
 45%|████▍     | 481/1071 [00:11<00:13, 44.65it/s][A
 45%|████▌     | 486/1071 [00:11<00:13, 44.62it/s][A
 46%|████▌     | 491/1071 [00:11<00:12, 44.68it/s][A
 46%|████▋     | 496/1071 [00:11<00:12, 44.93it/s][A
 47%|████▋     | 501/1071 [00:11<00:12, 44.88it/s][A
 47%|████▋     | 506/1071 [00:11<00:12, 44.92it/s][A
 48%|████▊     | 511/1071 [00:11<00:12, 44.80it/s][A
 48%|████▊     | 516/1071 [00:11<00:12, 44.48it/s][A
 49%|████▊     | 521/1071 [00:11<00:12, 44.68it/s][A
 49%|████▉     | 526/1071 [00:12<00:12, 44.52it/s][A
 50%|████▉     | 531/1071 [00:12<00:12, 44.69it/s][A
 50%|█████     | 536/1071 [00:12<00:11, 44.81it/s][A
 51%|█████     | 541/1071 [00:12<00:11, 44.74it/s][A
 51%|█████     | 546/1071 [00:12<00:11, 44.70it/s][A
 51%|█████▏    | 551/1071 [00:12<00:11, 44.83it/s][A
 52%|█████▏    | 556/1071 [00:12<00:11, 44.72it/s][A
 52%|█████▏    | 561/1071 [00:12<00:11, 44.62it/s][A
 53%|█████▎    | 566/1071 [00:12<00:12, 40.47it/s][A
 53%|█████▎    | 571/1071 [00:13<00:11, 41.72it/s][A
 54%|█████▍    | 576/1071 [00:13<00:11, 42.86it/s][A
 54%|█████▍    | 581/1071 [00:13<00:11, 43.64it/s][A
 55%|█████▍    | 586/1071 [00:13<00:10, 44.11it/s][A
 55%|█████▌    | 591/1071 [00:13<00:10, 44.36it/s][A
 56%|█████▌    | 596/1071 [00:13<00:10, 44.39it/s][A
 56%|█████▌    | 601/1071 [00:13<00:10, 44.16it/s][A
 57%|█████▋    | 606/1071 [00:13<00:10, 43.99it/s][A
 57%|█████▋    | 611/1071 [00:13<00:10, 43.88it/s][A
 58%|█████▊    | 616/1071 [00:14<00:10, 44.33it/s][A
 58%|█████▊    | 621/1071 [00:14<00:10, 44.62it/s][A
 58%|█████▊    | 626/1071 [00:14<00:09, 44.84it/s][A
 59%|█████▉    | 631/1071 [00:14<00:09, 45.00it/s][A
 59%|█████▉    | 636/1071 [00:14<00:09, 45.15it/s][A
 60%|█████▉    | 641/1071 [00:14<00:09, 45.04it/s][A
 60%|██████    | 646/1071 [00:14<00:09, 44.76it/s][A
 61%|██████    | 651/1071 [00:14<00:09, 44.40it/s][A
 61%|██████▏   | 656/1071 [00:14<00:09, 44.31it/s][A
 62%|██████▏   | 661/1071 [00:15<00:09, 44.46it/s][A
 62%|██████▏   | 666/1071 [00:15<00:09, 44.61it/s][A
 63%|██████▎   | 671/1071 [00:15<00:08, 44.86it/s][A
 63%|██████▎   | 676/1071 [00:15<00:08, 44.97it/s][A
 64%|██████▎   | 681/1071 [00:15<00:08, 45.14it/s][A
 64%|██████▍   | 686/1071 [00:15<00:08, 45.05it/s][A
 65%|██████▍   | 691/1071 [00:15<00:08, 44.78it/s][A
 65%|██████▍   | 696/1071 [00:15<00:08, 44.62it/s][A
 65%|██████▌   | 701/1071 [00:16<00:08, 41.49it/s][A
 66%|██████▌   | 706/1071 [00:16<00:08, 42.50it/s][A
 66%|██████▋   | 711/1071 [00:16<00:08, 43.22it/s][A
 67%|██████▋   | 716/1071 [00:16<00:08, 43.97it/s][A
 67%|██████▋   | 721/1071 [00:16<00:07, 44.36it/s][A
 68%|██████▊   | 726/1071 [00:16<00:07, 44.54it/s][A
 68%|██████▊   | 731/1071 [00:16<00:07, 44.67it/s][A
 69%|██████▊   | 736/1071 [00:16<00:07, 44.50it/s][A
 69%|██████▉   | 741/1071 [00:16<00:07, 44.11it/s][A
 70%|██████▉   | 746/1071 [00:17<00:07, 44.10it/s][A
 70%|███████   | 751/1071 [00:17<00:07, 44.24it/s][A
 71%|███████   | 756/1071 [00:17<00:07, 44.54it/s][A
 71%|███████   | 761/1071 [00:17<00:06, 44.71it/s][A
 72%|███████▏  | 766/1071 [00:17<00:06, 44.89it/s][A
 72%|███████▏  | 771/1071 [00:17<00:06, 44.84it/s][A
 72%|███████▏  | 776/1071 [00:17<00:06, 44.70it/s][A
 73%|███████▎  | 781/1071 [00:17<00:06, 44.54it/s][A
 73%|███████▎  | 786/1071 [00:17<00:06, 44.18it/s][A
 74%|███████▍  | 791/1071 [00:18<00:06, 44.29it/s][A
 74%|███████▍  | 796/1071 [00:18<00:06, 44.34it/s][A
 75%|███████▍  | 801/1071 [00:18<00:06, 44.56it/s][A
 75%|███████▌  | 806/1071 [00:18<00:05, 44.85it/s][A
 76%|███████▌  | 811/1071 [00:18<00:05, 44.82it/s][A
 76%|███████▌  | 816/1071 [00:18<00:05, 44.64it/s][A
 77%|███████▋  | 821/1071 [00:18<00:05, 44.81it/s][A
 77%|███████▋  | 826/1071 [00:18<00:05, 44.38it/s][A
 78%|███████▊  | 831/1071 [00:18<00:05, 44.21it/s][A
 78%|███████▊  | 836/1071 [00:19<00:05, 44.22it/s][A
 79%|███████▊  | 841/1071 [00:19<00:05, 44.36it/s][A
 79%|███████▉  | 846/1071 [00:19<00:05, 44.49it/s][A
 79%|███████▉  | 851/1071 [00:19<00:04, 44.52it/s][A
 80%|███████▉  | 856/1071 [00:19<00:04, 44.56it/s][A
 80%|████████  | 861/1071 [00:19<00:04, 44.92it/s][A
 81%|████████  | 866/1071 [00:19<00:04, 44.69it/s][A
 81%|████████▏ | 871/1071 [00:19<00:04, 44.63it/s][A
 82%|████████▏ | 876/1071 [00:19<00:04, 44.55it/s][A
 82%|████████▏ | 881/1071 [00:20<00:04, 44.47it/s][A
 83%|████████▎ | 886/1071 [00:20<00:04, 44.38it/s][A
 83%|████████▎ | 891/1071 [00:20<00:04, 44.47it/s][A
 84%|████████▎ | 896/1071 [00:20<00:03, 44.44it/s][A
 84%|████████▍ | 901/1071 [00:20<00:03, 44.49it/s][A
 85%|████████▍ | 906/1071 [00:20<00:03, 44.80it/s][A
 85%|████████▌ | 911/1071 [00:20<00:03, 44.54it/s][A
 86%|████████▌ | 916/1071 [00:20<00:03, 44.48it/s][A
 86%|████████▌ | 921/1071 [00:20<00:03, 44.48it/s][A
 86%|████████▋ | 926/1071 [00:21<00:03, 44.46it/s][A
 87%|████████▋ | 931/1071 [00:21<00:03, 44.48it/s][A
 87%|████████▋ | 936/1071 [00:21<00:03, 44.49it/s][A
 88%|████████▊ | 941/1071 [00:21<00:02, 44.51it/s][A
 88%|████████▊ | 946/1071 [00:21<00:02, 44.45it/s][A
 89%|████████▉ | 951/1071 [00:21<00:02, 44.65it/s][A
 89%|████████▉ | 956/1071 [00:21<00:02, 44.59it/s][A
 90%|████████▉ | 961/1071 [00:21<00:02, 44.28it/s][A
 90%|█████████ | 966/1071 [00:21<00:02, 44.32it/s][A
 91%|█████████ | 971/1071 [00:22<00:02, 44.31it/s][A
 91%|█████████ | 976/1071 [00:22<00:02, 44.42it/s][A
 92%|█████████▏| 981/1071 [00:22<00:02, 44.46it/s][A
 92%|█████████▏| 986/1071 [00:22<00:01, 44.58it/s][A
 93%|█████████▎| 991/1071 [00:22<00:01, 44.67it/s][A
 93%|█████████▎| 996/1071 [00:22<00:01, 44.68it/s][A
 93%|█████████▎| 1001/1071 [00:22<00:01, 44.56it/s][A
 94%|█████████▍| 1006/1071 [00:22<00:01, 44.35it/s][A
 94%|█████████▍| 1011/1071 [00:22<00:01, 44.46it/s][A
 95%|█████████▍| 1016/1071 [00:23<00:01, 44.58it/s][A
 95%|█████████▌| 1021/1071 [00:23<00:01, 44.59it/s][A
 96%|█████████▌| 1026/1071 [00:23<00:01, 44.64it/s][A
 96%|█████████▋| 1031/1071 [00:23<00:00, 44.59it/s][A
 97%|█████████▋| 1036/1071 [00:23<00:00, 44.75it/s][A
 97%|█████████▋| 1041/1071 [00:23<00:00, 44.79it/s][A
 98%|█████████▊| 1046/1071 [00:23<00:00, 44.73it/s][A
 98%|█████████▊| 1051/1071 [00:23<00:00, 44.49it/s][A
 99%|█████████▊| 1056/1071 [00:23<00:00, 44.56it/s][A
 99%|█████████▉| 1061/1071 [00:24<00:00, 44.61it/s][A
100%|█████████▉| 1066/1071 [00:24<00:00, 44.48it/s][A
100%|██████████| 1071/1071 [00:24<00:00, 44.35it/s][A
                                                   [A                                                 
100%|██████████| 1071/1071 [00:24<00:00, 44.35it/s][A 40%|████      | 234/585 [02:06<01:42,  3.41it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-29 09:06:52,525 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 09:06:52,674 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:06:55,546 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:06:55,643 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:06:55,701 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:15<59:55, 10.27s/it] 40%|████      | 236/585 [02:15<42:22,  7.29s/it] 41%|████      | 237/585 [02:16<30:05,  5.19s/it] 41%|████      | 238/585 [02:16<21:30,  3.72s/it] 41%|████      | 239/585 [02:16<15:31,  2.69s/it] 41%|████      | 240/585 [02:17<11:20,  1.97s/it] 41%|████      | 241/585 [02:17<08:25,  1.47s/it] 41%|████▏     | 242/585 [02:17<06:23,  1.12s/it] 42%|████▏     | 243/585 [02:18<04:57,  1.15it/s] 42%|████▏     | 244/585 [02:18<03:57,  1.44it/s] 42%|████▏     | 245/585 [02:18<03:15,  1.74it/s] 42%|████▏     | 246/585 [02:18<02:45,  2.04it/s] 42%|████▏     | 247/585 [02:19<02:27,  2.30it/s] 42%|████▏     | 248/585 [02:19<02:12,  2.55it/s] 43%|████▎     | 249/585 [02:19<02:01,  2.77it/s] 43%|████▎     | 250/585 [02:20<01:54,  2.94it/s] 43%|████▎     | 251/585 [02:20<01:48,  3.07it/s] 43%|████▎     | 252/585 [02:20<01:44,  3.18it/s] 43%|████▎     | 253/585 [02:20<01:42,  3.25it/s] 43%|████▎     | 254/585 [02:21<01:40,  3.31it/s] 44%|████▎     | 255/585 [02:21<01:38,  3.35it/s] 44%|████▍     | 256/585 [02:21<01:37,  3.37it/s] 44%|████▍     | 257/585 [02:22<01:36,  3.40it/s] 44%|████▍     | 258/585 [02:22<01:38,  3.32it/s] 44%|████▍     | 259/585 [02:22<01:37,  3.36it/s] 44%|████▍     | 260/585 [02:23<01:36,  3.38it/s] 45%|████▍     | 261/585 [02:23<01:35,  3.40it/s] 45%|████▍     | 262/585 [02:23<01:34,  3.41it/s] 45%|████▍     | 263/585 [02:23<01:34,  3.42it/s] 45%|████▌     | 264/585 [02:24<01:33,  3.43it/s] 45%|████▌     | 265/585 [02:24<01:33,  3.43it/s] 45%|████▌     | 266/585 [02:24<01:32,  3.44it/s] 46%|████▌     | 267/585 [02:25<01:32,  3.44it/s] 46%|████▌     | 268/585 [02:25<01:32,  3.44it/s] 46%|████▌     | 269/585 [02:25<01:34,  3.35it/s] 46%|████▌     | 270/585 [02:25<01:33,  3.38it/s] 46%|████▋     | 271/585 [02:26<01:32,  3.40it/s] 46%|████▋     | 272/585 [02:26<01:31,  3.41it/s] 47%|████▋     | 273/585 [02:26<01:31,  3.42it/s] 47%|████▋     | 274/585 [02:27<01:30,  3.43it/s] 47%|████▋     | 275/585 [02:27<01:30,  3.43it/s] 47%|████▋     | 276/585 [02:27<01:29,  3.44it/s] 47%|████▋     | 277/585 [02:27<01:29,  3.44it/s] 48%|████▊     | 278/585 [02:28<01:29,  3.44it/s] 48%|████▊     | 279/585 [02:28<01:28,  3.44it/s] 48%|████▊     | 280/585 [02:28<01:30,  3.39it/s] 48%|████▊     | 281/585 [02:29<01:29,  3.40it/s] 48%|████▊     | 282/585 [02:29<01:28,  3.41it/s] 48%|████▊     | 283/585 [02:29<01:28,  3.42it/s] 49%|████▊     | 284/585 [02:30<01:27,  3.43it/s] 49%|████▊     | 285/585 [02:30<01:27,  3.43it/s] 49%|████▉     | 286/585 [02:30<01:27,  3.43it/s] 49%|████▉     | 287/585 [02:30<01:26,  3.44it/s] 49%|████▉     | 288/585 [02:31<01:26,  3.44it/s] 49%|████▉     | 289/585 [02:31<01:25,  3.44it/s] 50%|████▉     | 290/585 [02:31<01:25,  3.44it/s] 50%|████▉     | 291/585 [02:32<01:25,  3.44it/s] 50%|████▉     | 292/585 [02:32<01:25,  3.44it/s] 50%|█████     | 293/585 [02:32<01:24,  3.44it/s] 50%|█████     | 294/585 [02:32<01:24,  3.44it/s] 50%|█████     | 295/585 [02:33<01:24,  3.44it/s] 51%|█████     | 296/585 [02:33<01:23,  3.44it/s] 51%|█████     | 297/585 [02:33<01:23,  3.44it/s] 51%|█████     | 298/585 [02:34<01:23,  3.44it/s] 51%|█████     | 299/585 [02:34<01:23,  3.44it/s] 51%|█████▏    | 300/585 [02:34<01:22,  3.44it/s] 51%|█████▏    | 301/585 [02:34<01:23,  3.40it/s] 52%|█████▏    | 302/585 [02:35<01:22,  3.41it/s] 52%|█████▏    | 303/585 [02:35<01:22,  3.42it/s] 52%|█████▏    | 304/585 [02:35<01:21,  3.43it/s] 52%|█████▏    | 305/585 [02:36<01:21,  3.43it/s] 52%|█████▏    | 306/585 [02:36<01:21,  3.44it/s] 52%|█████▏    | 307/585 [02:36<01:20,  3.44it/s] 53%|█████▎    | 308/585 [02:37<01:20,  3.44it/s] 53%|█████▎    | 309/585 [02:37<01:20,  3.44it/s] 53%|█████▎    | 310/585 [02:37<01:20,  3.43it/s] 53%|█████▎    | 311/585 [02:37<01:19,  3.43it/s] 53%|█████▎    | 312/585 [02:38<01:21,  3.36it/s] 54%|█████▎    | 313/585 [02:38<01:20,  3.38it/s] 54%|█████▎    | 314/585 [02:38<01:19,  3.40it/s] 54%|█████▍    | 315/585 [02:39<01:19,  3.41it/s] 54%|█████▍    | 316/585 [02:39<01:18,  3.42it/s] 54%|█████▍    | 317/585 [02:39<01:18,  3.43it/s] 54%|█████▍    | 318/585 [02:39<01:17,  3.43it/s] 55%|█████▍    | 319/585 [02:40<01:17,  3.43it/s] 55%|█████▍    | 320/585 [02:40<01:17,  3.44it/s] 55%|█████▍    | 321/585 [02:40<01:16,  3.44it/s] 55%|█████▌    | 322/585 [02:41<01:16,  3.44it/s] 55%|█████▌    | 323/585 [02:41<01:19,  3.28it/s] 55%|█████▌    | 324/585 [02:41<01:18,  3.33it/s] 56%|█████▌    | 325/585 [02:42<01:17,  3.36it/s] 56%|█████▌    | 326/585 [02:42<01:16,  3.39it/s] 56%|█████▌    | 327/585 [02:42<01:15,  3.40it/s] 56%|█████▌    | 328/585 [02:42<01:15,  3.41it/s] 56%|█████▌    | 329/585 [02:43<01:30,  2.82it/s] 56%|█████▋    | 330/585 [02:43<01:25,  2.98it/s] 57%|█████▋    | 331/585 [02:43<01:21,  3.10it/s] 57%|█████▋    | 332/585 [02:44<01:19,  3.20it/s] 57%|█████▋    | 333/585 [02:44<01:18,  3.22it/s] 57%|█████▋    | 334/585 [02:44<01:16,  3.28it/s] 57%|█████▋    | 335/585 [02:45<01:15,  3.33it/s] 57%|█████▋    | 336/585 [02:45<01:14,  3.36it/s] 58%|█████▊    | 337/585 [02:45<01:13,  3.38it/s] 58%|█████▊    | 338/585 [02:46<01:12,  3.40it/s] 58%|█████▊    | 339/585 [02:46<01:12,  3.41it/s] 58%|█████▊    | 340/585 [02:46<01:11,  3.42it/s] 58%|█████▊    | 341/585 [02:46<01:11,  3.43it/s] 58%|█████▊    | 342/585 [02:47<01:10,  3.43it/s] 59%|█████▊    | 343/585 [02:47<01:10,  3.43it/s] 59%|█████▉    | 344/585 [02:47<01:12,  3.32it/s] 59%|█████▉    | 345/585 [02:48<01:11,  3.36it/s] 59%|█████▉    | 346/585 [02:48<01:10,  3.38it/s] 59%|█████▉    | 347/585 [02:48<01:10,  3.40it/s] 59%|█████▉    | 348/585 [02:48<01:09,  3.41it/s] 60%|█████▉    | 349/585 [02:49<01:08,  3.42it/s] 60%|█████▉    | 350/585 [02:49<01:08,  3.43it/s] 60%|██████    | 351/585 [02:49<01:08,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 09:07:35,711 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:07:35,711 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 09:07:35,711 >>   Batch size = 8
{'eval_loss': 0.9405005574226379, 'eval_runtime': 24.3483, 'eval_samples_per_second': 351.893, 'eval_steps_per_second': 43.987, 'epoch': 2.0}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:19, 55.86it/s][A
  1%|          | 12/1071 [00:00<00:21, 48.80it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 46.90it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 46.03it/s][A
  3%|▎         | 27/1071 [00:00<00:22, 45.48it/s][A
  3%|▎         | 32/1071 [00:00<00:24, 42.07it/s][A
  3%|▎         | 37/1071 [00:00<00:23, 43.15it/s][A
  4%|▍         | 42/1071 [00:00<00:23, 43.48it/s][A
  4%|▍         | 47/1071 [00:01<00:24, 41.19it/s][A
  5%|▍         | 52/1071 [00:01<00:24, 42.37it/s][A
  5%|▌         | 57/1071 [00:01<00:23, 43.34it/s][A
  6%|▌         | 62/1071 [00:01<00:23, 43.82it/s][A
  6%|▋         | 67/1071 [00:01<00:22, 44.06it/s][A
  7%|▋         | 72/1071 [00:01<00:22, 44.06it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 44.27it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 44.25it/s][A
  8%|▊         | 87/1071 [00:01<00:22, 44.31it/s][A
  9%|▊         | 92/1071 [00:02<00:21, 44.52it/s][A
  9%|▉         | 97/1071 [00:02<00:21, 44.61it/s][A
 10%|▉         | 102/1071 [00:02<00:21, 44.71it/s][A
 10%|▉         | 107/1071 [00:02<00:21, 44.68it/s][A
 10%|█         | 112/1071 [00:02<00:21, 44.91it/s][A
 11%|█         | 117/1071 [00:02<00:21, 44.78it/s][A
 11%|█▏        | 122/1071 [00:02<00:21, 44.59it/s][A
 12%|█▏        | 127/1071 [00:02<00:21, 44.74it/s][A
 12%|█▏        | 132/1071 [00:02<00:21, 44.64it/s][A
 13%|█▎        | 137/1071 [00:03<00:20, 44.62it/s][A
 13%|█▎        | 142/1071 [00:03<00:20, 44.69it/s][A
 14%|█▎        | 147/1071 [00:03<00:20, 44.70it/s][A
 14%|█▍        | 152/1071 [00:03<00:20, 44.80it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 44.72it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 44.84it/s][A
 16%|█▌        | 167/1071 [00:03<00:22, 40.68it/s][A
 16%|█▌        | 172/1071 [00:03<00:21, 42.06it/s][A
 17%|█▋        | 177/1071 [00:04<00:20, 42.70it/s][A
 17%|█▋        | 182/1071 [00:04<00:20, 43.41it/s][A
 17%|█▋        | 187/1071 [00:04<00:20, 43.61it/s][A
 18%|█▊        | 192/1071 [00:04<00:20, 43.84it/s][A
 18%|█▊        | 197/1071 [00:04<00:19, 44.32it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 44.44it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 44.12it/s][A
 20%|█▉        | 212/1071 [00:04<00:19, 43.93it/s][A
 20%|██        | 217/1071 [00:04<00:19, 44.27it/s][A
 21%|██        | 222/1071 [00:05<00:19, 44.50it/s][A
 21%|██        | 227/1071 [00:05<00:18, 44.57it/s][A
 22%|██▏       | 232/1071 [00:05<00:18, 44.66it/s][A
 22%|██▏       | 237/1071 [00:05<00:18, 44.79it/s][A
 23%|██▎       | 242/1071 [00:05<00:18, 44.95it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 44.74it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 44.29it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 44.42it/s][A
 24%|██▍       | 262/1071 [00:05<00:18, 44.55it/s][A
 25%|██▍       | 267/1071 [00:06<00:17, 44.76it/s][A
 25%|██▌       | 272/1071 [00:06<00:17, 44.92it/s][A
 26%|██▌       | 277/1071 [00:06<00:17, 45.03it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 45.01it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 44.99it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 44.80it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 44.56it/s][A
 28%|██▊       | 302/1071 [00:06<00:19, 38.66it/s][A
 29%|██▊       | 307/1071 [00:06<00:18, 40.53it/s][A
 29%|██▉       | 312/1071 [00:07<00:18, 41.92it/s][A
 30%|██▉       | 317/1071 [00:07<00:17, 42.88it/s][A
 30%|███       | 322/1071 [00:07<00:17, 43.58it/s][A
 31%|███       | 327/1071 [00:07<00:16, 43.91it/s][A
 31%|███       | 332/1071 [00:07<00:16, 44.37it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 44.36it/s][A
 32%|███▏      | 342/1071 [00:07<00:16, 44.17it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 44.05it/s][A
 33%|███▎      | 352/1071 [00:07<00:16, 44.14it/s][A
 33%|███▎      | 357/1071 [00:08<00:16, 44.51it/s][A
 34%|███▍      | 362/1071 [00:08<00:15, 44.68it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.96it/s][A
 35%|███▍      | 372/1071 [00:08<00:15, 45.07it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 45.10it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 44.92it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 44.61it/s][A
 37%|███▋      | 392/1071 [00:08<00:15, 44.22it/s][A
 37%|███▋      | 397/1071 [00:08<00:15, 44.25it/s][A
 38%|███▊      | 402/1071 [00:09<00:15, 44.41it/s][A
 38%|███▊      | 407/1071 [00:09<00:14, 44.62it/s][A
 38%|███▊      | 412/1071 [00:09<00:14, 44.69it/s][A
 39%|███▉      | 417/1071 [00:09<00:14, 44.80it/s][A
 39%|███▉      | 422/1071 [00:09<00:14, 44.89it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 44.77it/s][A
 40%|████      | 432/1071 [00:09<00:14, 44.34it/s][A
 41%|████      | 437/1071 [00:09<00:14, 44.10it/s][A
 41%|████▏     | 442/1071 [00:10<00:14, 44.29it/s][A
 42%|████▏     | 447/1071 [00:10<00:14, 44.27it/s][A
 42%|████▏     | 452/1071 [00:10<00:13, 44.38it/s][A
 43%|████▎     | 457/1071 [00:10<00:13, 44.62it/s][A
 43%|████▎     | 462/1071 [00:10<00:13, 44.74it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 44.85it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 44.77it/s][A
 45%|████▍     | 477/1071 [00:10<00:13, 44.46it/s][A
 45%|████▌     | 482/1071 [00:10<00:13, 44.34it/s][A
 45%|████▌     | 487/1071 [00:11<00:13, 44.32it/s][A
 46%|████▌     | 492/1071 [00:11<00:13, 44.35it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.49it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.43it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 44.47it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 44.68it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 44.75it/s][A
 49%|████▊     | 522/1071 [00:11<00:12, 44.60it/s][A
 49%|████▉     | 527/1071 [00:11<00:12, 44.57it/s][A
 50%|████▉     | 532/1071 [00:12<00:12, 44.58it/s][A
 50%|█████     | 537/1071 [00:12<00:11, 44.59it/s][A
 51%|█████     | 542/1071 [00:12<00:11, 44.55it/s][A
 51%|█████     | 547/1071 [00:12<00:11, 44.61it/s][A
 52%|█████▏    | 552/1071 [00:12<00:11, 44.79it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 44.90it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 44.79it/s][A
 53%|█████▎    | 567/1071 [00:12<00:11, 44.80it/s][A
 53%|█████▎    | 572/1071 [00:12<00:11, 44.70it/s][A
 54%|█████▍    | 577/1071 [00:13<00:11, 44.63it/s][A
 54%|█████▍    | 582/1071 [00:13<00:10, 44.61it/s][A
 55%|█████▍    | 587/1071 [00:13<00:10, 44.53it/s][A
 55%|█████▌    | 592/1071 [00:13<00:10, 44.40it/s][A
 56%|█████▌    | 597/1071 [00:13<00:10, 44.69it/s][A
 56%|█████▌    | 602/1071 [00:13<00:10, 44.73it/s][A
 57%|█████▋    | 607/1071 [00:13<00:10, 44.78it/s][A
 57%|█████▋    | 612/1071 [00:13<00:10, 44.67it/s][A
 58%|█████▊    | 617/1071 [00:13<00:10, 44.63it/s][A
 58%|█████▊    | 622/1071 [00:14<00:10, 44.57it/s][A
 59%|█████▊    | 627/1071 [00:14<00:09, 44.59it/s][A
 59%|█████▉    | 632/1071 [00:14<00:09, 44.62it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.70it/s][A
 60%|█████▉    | 642/1071 [00:14<00:09, 44.56it/s][A
 60%|██████    | 647/1071 [00:14<00:09, 44.67it/s][A
 61%|██████    | 652/1071 [00:14<00:09, 44.61it/s][A
 61%|██████▏   | 657/1071 [00:14<00:09, 44.62it/s][A
 62%|██████▏   | 662/1071 [00:14<00:09, 44.51it/s][A
 62%|██████▏   | 667/1071 [00:15<00:09, 44.50it/s][A
 63%|██████▎   | 672/1071 [00:15<00:08, 44.50it/s][A
 63%|██████▎   | 677/1071 [00:15<00:08, 44.59it/s][A
 64%|██████▎   | 682/1071 [00:15<00:08, 44.56it/s][A
 64%|██████▍   | 687/1071 [00:15<00:08, 44.56it/s][A
 65%|██████▍   | 692/1071 [00:15<00:08, 44.29it/s][A
 65%|██████▌   | 697/1071 [00:15<00:08, 44.36it/s][A
 66%|██████▌   | 702/1071 [00:15<00:08, 44.48it/s][A
 66%|██████▌   | 707/1071 [00:15<00:08, 44.55it/s][A
 66%|██████▋   | 712/1071 [00:16<00:08, 44.52it/s][A
 67%|██████▋   | 717/1071 [00:16<00:07, 44.49it/s][A
 67%|██████▋   | 722/1071 [00:16<00:07, 44.63it/s][A
 68%|██████▊   | 727/1071 [00:16<00:07, 44.54it/s][A
 68%|██████▊   | 732/1071 [00:16<00:07, 44.53it/s][A
 69%|██████▉   | 737/1071 [00:16<00:07, 44.55it/s][A
 69%|██████▉   | 742/1071 [00:16<00:07, 44.73it/s][A
 70%|██████▉   | 747/1071 [00:16<00:07, 44.72it/s][A
 70%|███████   | 752/1071 [00:16<00:07, 44.69it/s][A
 71%|███████   | 757/1071 [00:17<00:07, 44.60it/s][A
 71%|███████   | 762/1071 [00:17<00:06, 44.66it/s][A
 72%|███████▏  | 767/1071 [00:17<00:06, 44.64it/s][A
 72%|███████▏  | 772/1071 [00:17<00:06, 44.72it/s][A
 73%|███████▎  | 777/1071 [00:17<00:06, 44.60it/s][A
 73%|███████▎  | 782/1071 [00:17<00:06, 44.67it/s][A
 73%|███████▎  | 787/1071 [00:17<00:06, 44.88it/s][A
 74%|███████▍  | 792/1071 [00:17<00:06, 44.77it/s][A
 74%|███████▍  | 797/1071 [00:17<00:06, 44.78it/s][A
 75%|███████▍  | 802/1071 [00:18<00:06, 44.67it/s][A
 75%|███████▌  | 807/1071 [00:18<00:05, 44.75it/s][A
 76%|███████▌  | 812/1071 [00:18<00:05, 44.66it/s][A
 76%|███████▋  | 817/1071 [00:18<00:05, 44.52it/s][A
 77%|███████▋  | 822/1071 [00:18<00:05, 44.63it/s][A
 77%|███████▋  | 827/1071 [00:18<00:05, 42.04it/s][A
 78%|███████▊  | 832/1071 [00:18<00:05, 42.96it/s][A
 78%|███████▊  | 837/1071 [00:18<00:05, 43.68it/s][A
 79%|███████▊  | 842/1071 [00:18<00:05, 44.00it/s][A
 79%|███████▉  | 847/1071 [00:19<00:05, 44.20it/s][A
 80%|███████▉  | 852/1071 [00:19<00:04, 44.38it/s][A
 80%|████████  | 857/1071 [00:19<00:04, 44.51it/s][A
 80%|████████  | 862/1071 [00:19<00:04, 44.34it/s][A
 81%|████████  | 867/1071 [00:19<00:04, 44.21it/s][A
 81%|████████▏ | 872/1071 [00:19<00:04, 44.27it/s][A
 82%|████████▏ | 877/1071 [00:19<00:04, 44.56it/s][A
 82%|████████▏ | 882/1071 [00:19<00:04, 44.78it/s][A
 83%|████████▎ | 887/1071 [00:19<00:04, 44.95it/s][A
 83%|████████▎ | 892/1071 [00:20<00:03, 44.95it/s][A
 84%|████████▍ | 897/1071 [00:20<00:03, 44.83it/s][A
 84%|████████▍ | 902/1071 [00:20<00:03, 44.82it/s][A
 85%|████████▍ | 907/1071 [00:20<00:03, 44.64it/s][A
 85%|████████▌ | 912/1071 [00:20<00:03, 44.39it/s][A
 86%|████████▌ | 917/1071 [00:20<00:03, 44.36it/s][A
 86%|████████▌ | 922/1071 [00:20<00:03, 44.48it/s][A
 87%|████████▋ | 927/1071 [00:20<00:03, 44.63it/s][A
 87%|████████▋ | 932/1071 [00:21<00:03, 44.91it/s][A
 87%|████████▋ | 937/1071 [00:21<00:02, 44.86it/s][A
 88%|████████▊ | 942/1071 [00:21<00:02, 44.79it/s][A
 88%|████████▊ | 947/1071 [00:21<00:02, 44.54it/s][A
 89%|████████▉ | 952/1071 [00:21<00:02, 44.44it/s][A
 89%|████████▉ | 957/1071 [00:21<00:02, 44.43it/s][A
 90%|████████▉ | 962/1071 [00:21<00:02, 42.97it/s][A
 90%|█████████ | 967/1071 [00:21<00:02, 43.61it/s][A
 91%|█████████ | 972/1071 [00:21<00:02, 44.11it/s][A
 91%|█████████ | 977/1071 [00:22<00:02, 44.31it/s][A
 92%|█████████▏| 982/1071 [00:22<00:01, 44.57it/s][A
 92%|█████████▏| 987/1071 [00:22<00:01, 44.37it/s][A
 93%|█████████▎| 992/1071 [00:22<00:01, 44.34it/s][A
 93%|█████████▎| 997/1071 [00:22<00:01, 44.34it/s][A
 94%|█████████▎| 1002/1071 [00:22<00:01, 44.25it/s][A
 94%|█████████▍| 1007/1071 [00:22<00:01, 44.24it/s][A
 94%|█████████▍| 1012/1071 [00:22<00:01, 44.45it/s][A
 95%|█████████▍| 1017/1071 [00:22<00:01, 44.55it/s][A
 95%|█████████▌| 1022/1071 [00:23<00:01, 44.85it/s][A
 96%|█████████▌| 1027/1071 [00:23<00:00, 44.77it/s][A
 96%|█████████▋| 1032/1071 [00:23<00:00, 44.76it/s][A
 97%|█████████▋| 1037/1071 [00:23<00:00, 44.56it/s][A
 97%|█████████▋| 1042/1071 [00:23<00:00, 44.30it/s][A
 98%|█████████▊| 1047/1071 [00:23<00:00, 44.24it/s][A
 98%|█████████▊| 1052/1071 [00:23<00:00, 43.98it/s][A
 99%|█████████▊| 1057/1071 [00:23<00:00, 44.21it/s][A
 99%|█████████▉| 1062/1071 [00:23<00:00, 44.54it/s][A
100%|█████████▉| 1067/1071 [00:24<00:00, 44.82it/s][A
                                                   [A                                                 
100%|██████████| 1071/1071 [00:24<00:00, 44.82it/s][A 60%|██████    | 351/585 [03:14<01:08,  3.43it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-29 09:08:00,052 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 09:08:00,145 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:08:03,034 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:08:03,143 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:08:03,207 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:24<41:44, 10.75s/it] 60%|██████    | 353/585 [03:25<29:26,  7.61s/it] 61%|██████    | 354/585 [03:25<20:51,  5.42s/it] 61%|██████    | 355/585 [03:25<14:52,  3.88s/it] 61%|██████    | 356/585 [03:26<10:41,  2.80s/it] 61%|██████    | 357/585 [03:26<07:47,  2.05s/it] 61%|██████    | 358/585 [03:26<05:45,  1.52s/it] 61%|██████▏   | 359/585 [03:27<04:20,  1.15s/it] 62%|██████▏   | 360/585 [03:27<03:20,  1.12it/s] 62%|██████▏   | 361/585 [03:27<02:39,  1.40it/s] 62%|██████▏   | 362/585 [03:27<02:10,  1.71it/s] 62%|██████▏   | 363/585 [03:28<01:50,  2.01it/s] 62%|██████▏   | 364/585 [03:28<01:37,  2.27it/s] 62%|██████▏   | 365/585 [03:28<01:27,  2.53it/s] 63%|██████▎   | 366/585 [03:29<01:19,  2.75it/s] 63%|██████▎   | 367/585 [03:29<01:14,  2.92it/s] 63%|██████▎   | 368/585 [03:29<01:10,  3.06it/s] 63%|██████▎   | 369/585 [03:29<01:08,  3.17it/s] 63%|██████▎   | 370/585 [03:30<01:06,  3.25it/s] 63%|██████▎   | 371/585 [03:30<01:04,  3.30it/s] 64%|██████▎   | 372/585 [03:30<01:03,  3.34it/s] 64%|██████▍   | 373/585 [03:31<01:02,  3.37it/s] 64%|██████▍   | 374/585 [03:31<01:02,  3.39it/s] 64%|██████▍   | 375/585 [03:31<01:03,  3.31it/s] 64%|██████▍   | 376/585 [03:32<01:02,  3.35it/s] 64%|██████▍   | 377/585 [03:32<01:01,  3.37it/s] 65%|██████▍   | 378/585 [03:32<01:00,  3.40it/s] 65%|██████▍   | 379/585 [03:32<01:00,  3.41it/s] 65%|██████▍   | 380/585 [03:33<00:59,  3.42it/s] 65%|██████▌   | 381/585 [03:33<00:59,  3.43it/s] 65%|██████▌   | 382/585 [03:33<00:59,  3.43it/s] 65%|██████▌   | 383/585 [03:34<00:58,  3.44it/s] 66%|██████▌   | 384/585 [03:34<00:58,  3.44it/s] 66%|██████▌   | 385/585 [03:34<00:58,  3.44it/s] 66%|██████▌   | 386/585 [03:34<00:57,  3.44it/s] 66%|██████▌   | 387/585 [03:35<00:57,  3.44it/s] 66%|██████▋   | 388/585 [03:35<00:57,  3.45it/s] 66%|██████▋   | 389/585 [03:35<00:56,  3.44it/s] 67%|██████▋   | 390/585 [03:36<00:56,  3.45it/s] 67%|██████▋   | 391/585 [03:36<00:57,  3.38it/s] 67%|██████▋   | 392/585 [03:36<00:56,  3.40it/s] 67%|██████▋   | 393/585 [03:36<00:56,  3.41it/s] 67%|██████▋   | 394/585 [03:37<00:55,  3.42it/s] 68%|██████▊   | 395/585 [03:37<00:55,  3.43it/s] 68%|██████▊   | 396/585 [03:37<00:55,  3.43it/s] 68%|██████▊   | 397/585 [03:38<00:54,  3.44it/s] 68%|██████▊   | 398/585 [03:38<00:54,  3.44it/s] 68%|██████▊   | 399/585 [03:38<00:54,  3.44it/s] 68%|██████▊   | 400/585 [03:39<00:53,  3.44it/s] 69%|██████▊   | 401/585 [03:39<00:53,  3.44it/s] 69%|██████▊   | 402/585 [03:39<00:53,  3.39it/s] 69%|██████▉   | 403/585 [03:39<00:53,  3.41it/s] 69%|██████▉   | 404/585 [03:40<00:52,  3.42it/s] 69%|██████▉   | 405/585 [03:40<00:52,  3.43it/s] 69%|██████▉   | 406/585 [03:40<00:52,  3.43it/s] 70%|██████▉   | 407/585 [03:41<00:51,  3.43it/s] 70%|██████▉   | 408/585 [03:41<00:51,  3.43it/s] 70%|██████▉   | 409/585 [03:41<00:51,  3.44it/s] 70%|███████   | 410/585 [03:41<00:50,  3.44it/s] 70%|███████   | 411/585 [03:42<00:50,  3.44it/s] 70%|███████   | 412/585 [03:42<00:50,  3.44it/s] 71%|███████   | 413/585 [03:42<00:50,  3.41it/s] 71%|███████   | 414/585 [03:43<00:49,  3.42it/s] 71%|███████   | 415/585 [03:43<00:50,  3.38it/s] 71%|███████   | 416/585 [03:43<00:49,  3.40it/s] 71%|███████▏  | 417/585 [03:43<00:49,  3.41it/s] 71%|███████▏  | 418/585 [03:44<00:48,  3.42it/s] 72%|███████▏  | 419/585 [03:44<00:48,  3.43it/s] 72%|███████▏  | 420/585 [03:44<00:48,  3.43it/s] 72%|███████▏  | 421/585 [03:45<00:47,  3.44it/s] 72%|███████▏  | 422/585 [03:45<00:47,  3.44it/s] 72%|███████▏  | 423/585 [03:45<00:47,  3.44it/s] 72%|███████▏  | 424/585 [03:46<00:47,  3.37it/s] 73%|███████▎  | 425/585 [03:46<00:47,  3.40it/s] 73%|███████▎  | 426/585 [03:46<00:46,  3.41it/s] 73%|███████▎  | 427/585 [03:46<00:46,  3.42it/s] 73%|███████▎  | 428/585 [03:47<00:45,  3.43it/s] 73%|███████▎  | 429/585 [03:47<00:45,  3.43it/s] 74%|███████▎  | 430/585 [03:47<00:45,  3.44it/s] 74%|███████▎  | 431/585 [03:48<00:44,  3.44it/s] 74%|███████▍  | 432/585 [03:48<00:44,  3.44it/s] 74%|███████▍  | 433/585 [03:48<00:44,  3.44it/s] 74%|███████▍  | 434/585 [03:48<00:43,  3.45it/s] 74%|███████▍  | 435/585 [03:49<00:45,  3.27it/s] 75%|███████▍  | 436/585 [03:49<00:44,  3.32it/s] 75%|███████▍  | 437/585 [03:49<00:44,  3.36it/s] 75%|███████▍  | 438/585 [03:50<00:43,  3.39it/s] 75%|███████▌  | 439/585 [03:50<00:42,  3.41it/s] 75%|███████▌  | 440/585 [03:50<00:42,  3.42it/s] 75%|███████▌  | 441/585 [03:51<00:41,  3.43it/s] 76%|███████▌  | 442/585 [03:51<00:41,  3.44it/s] 76%|███████▌  | 443/585 [03:51<00:41,  3.44it/s] 76%|███████▌  | 444/585 [03:51<00:40,  3.44it/s] 76%|███████▌  | 445/585 [03:52<00:40,  3.45it/s] 76%|███████▌  | 446/585 [03:52<00:43,  3.20it/s] 76%|███████▋  | 447/585 [03:52<00:42,  3.27it/s] 77%|███████▋  | 448/585 [03:53<00:41,  3.32it/s] 77%|███████▋  | 449/585 [03:53<00:40,  3.36it/s] 77%|███████▋  | 450/585 [03:53<00:39,  3.39it/s] 77%|███████▋  | 451/585 [03:53<00:39,  3.40it/s] 77%|███████▋  | 452/585 [03:54<00:38,  3.42it/s] 77%|███████▋  | 453/585 [03:54<00:38,  3.42it/s] 78%|███████▊  | 454/585 [03:54<00:38,  3.43it/s] 78%|███████▊  | 455/585 [03:55<00:37,  3.43it/s] 78%|███████▊  | 456/585 [03:55<00:37,  3.44it/s] 78%|███████▊  | 457/585 [03:55<00:39,  3.27it/s] 78%|███████▊  | 458/585 [03:56<00:38,  3.32it/s] 78%|███████▊  | 459/585 [03:56<00:37,  3.36it/s] 79%|███████▊  | 460/585 [03:56<00:36,  3.39it/s] 79%|███████▉  | 461/585 [03:56<00:36,  3.41it/s] 79%|███████▉  | 462/585 [03:57<00:35,  3.42it/s] 79%|███████▉  | 463/585 [03:57<00:35,  3.43it/s] 79%|███████▉  | 464/585 [03:57<00:35,  3.43it/s] 79%|███████▉  | 465/585 [03:58<00:34,  3.43it/s] 80%|███████▉  | 466/585 [03:58<00:34,  3.43it/s] 80%|███████▉  | 467/585 [03:58<00:34,  3.44it/s] 80%|████████  | 468/585 [03:59<00:36,  3.18it/s][INFO|trainer.py:2140] 2023-08-29 09:08:44,919 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:08:44,919 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 09:08:44,919 >>   Batch size = 8
{'eval_loss': 0.9534672498703003, 'eval_runtime': 24.1722, 'eval_samples_per_second': 354.457, 'eval_steps_per_second': 44.307, 'epoch': 3.0}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:18, 56.56it/s][A
  1%|          | 12/1071 [00:00<00:21, 49.33it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 47.76it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 46.80it/s][A
  3%|▎         | 27/1071 [00:00<00:22, 45.71it/s][A
  3%|▎         | 32/1071 [00:00<00:23, 44.88it/s][A
  3%|▎         | 37/1071 [00:00<00:23, 44.59it/s][A
  4%|▍         | 42/1071 [00:00<00:23, 44.46it/s][A
  4%|▍         | 47/1071 [00:01<00:22, 44.62it/s][A
  5%|▍         | 52/1071 [00:01<00:22, 44.82it/s][A
  5%|▌         | 57/1071 [00:01<00:22, 44.93it/s][A
  6%|▌         | 62/1071 [00:01<00:22, 45.09it/s][A
  6%|▋         | 67/1071 [00:01<00:22, 45.21it/s][A
  7%|▋         | 72/1071 [00:01<00:22, 44.99it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 44.76it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 44.52it/s][A
  8%|▊         | 87/1071 [00:01<00:22, 44.25it/s][A
  9%|▊         | 92/1071 [00:02<00:22, 44.43it/s][A
  9%|▉         | 97/1071 [00:02<00:21, 44.72it/s][A
 10%|▉         | 102/1071 [00:02<00:21, 44.89it/s][A
 10%|▉         | 107/1071 [00:02<00:21, 45.02it/s][A
 10%|█         | 112/1071 [00:02<00:21, 45.22it/s][A
 11%|█         | 117/1071 [00:02<00:21, 45.00it/s][A
 11%|█▏        | 122/1071 [00:02<00:24, 39.50it/s][A
 12%|█▏        | 127/1071 [00:02<00:23, 41.04it/s][A
 12%|█▏        | 132/1071 [00:02<00:22, 42.24it/s][A
 13%|█▎        | 137/1071 [00:03<00:21, 43.05it/s][A
 13%|█▎        | 142/1071 [00:03<00:21, 43.43it/s][A
 14%|█▎        | 147/1071 [00:03<00:21, 43.74it/s][A
 14%|█▍        | 152/1071 [00:03<00:20, 44.05it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 44.29it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 43.84it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 43.73it/s][A
 16%|█▌        | 172/1071 [00:03<00:20, 44.24it/s][A
 17%|█▋        | 177/1071 [00:03<00:20, 44.56it/s][A
 17%|█▋        | 182/1071 [00:04<00:19, 44.70it/s][A
 17%|█▋        | 187/1071 [00:04<00:19, 44.86it/s][A
 18%|█▊        | 192/1071 [00:04<00:19, 44.98it/s][A
 18%|█▊        | 197/1071 [00:04<00:19, 45.04it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 44.65it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 44.52it/s][A
 20%|█▉        | 212/1071 [00:04<00:24, 35.37it/s][A
 20%|██        | 217/1071 [00:04<00:22, 37.90it/s][A
 21%|██        | 222/1071 [00:05<00:21, 39.94it/s][A
 21%|██        | 227/1071 [00:05<00:20, 41.45it/s][A
 22%|██▏       | 232/1071 [00:05<00:19, 42.54it/s][A
 22%|██▏       | 237/1071 [00:05<00:19, 43.38it/s][A
 23%|██▎       | 242/1071 [00:05<00:18, 44.03it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 44.09it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 43.73it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 43.56it/s][A
 24%|██▍       | 262/1071 [00:05<00:18, 43.98it/s][A
 25%|██▍       | 267/1071 [00:06<00:18, 44.34it/s][A
 25%|██▌       | 272/1071 [00:06<00:17, 44.54it/s][A
 26%|██▌       | 277/1071 [00:06<00:17, 44.84it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 45.00it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 45.11it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 44.79it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 44.23it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 44.09it/s][A
 29%|██▊       | 307/1071 [00:06<00:17, 44.33it/s][A
 29%|██▉       | 312/1071 [00:07<00:17, 44.45it/s][A
 30%|██▉       | 317/1071 [00:07<00:16, 44.73it/s][A
 30%|███       | 322/1071 [00:07<00:16, 45.05it/s][A
 31%|███       | 327/1071 [00:07<00:16, 45.04it/s][A
 31%|███       | 332/1071 [00:07<00:16, 45.17it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 44.91it/s][A
 32%|███▏      | 342/1071 [00:07<00:16, 44.54it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 43.01it/s][A
 33%|███▎      | 352/1071 [00:07<00:16, 43.61it/s][A
 33%|███▎      | 357/1071 [00:08<00:16, 43.92it/s][A
 34%|███▍      | 362/1071 [00:08<00:15, 44.33it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.60it/s][A
 35%|███▍      | 372/1071 [00:08<00:15, 44.74it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 44.92it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 44.57it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 44.21it/s][A
 37%|███▋      | 392/1071 [00:08<00:15, 44.23it/s][A
 37%|███▋      | 397/1071 [00:09<00:15, 44.31it/s][A
 38%|███▊      | 402/1071 [00:09<00:15, 44.54it/s][A
 38%|███▊      | 407/1071 [00:09<00:14, 44.57it/s][A
 38%|███▊      | 412/1071 [00:09<00:14, 44.69it/s][A
 39%|███▉      | 417/1071 [00:09<00:14, 44.81it/s][A
 39%|███▉      | 422/1071 [00:09<00:14, 44.78it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 44.51it/s][A
 40%|████      | 432/1071 [00:09<00:14, 44.38it/s][A
 41%|████      | 437/1071 [00:09<00:14, 44.42it/s][A
 41%|████▏     | 442/1071 [00:10<00:14, 44.47it/s][A
 42%|████▏     | 447/1071 [00:10<00:14, 44.53it/s][A
 42%|████▏     | 452/1071 [00:10<00:13, 44.71it/s][A
 43%|████▎     | 457/1071 [00:10<00:13, 45.04it/s][A
 43%|████▎     | 462/1071 [00:10<00:13, 44.99it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 44.95it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 44.75it/s][A
 45%|████▍     | 477/1071 [00:10<00:13, 44.61it/s][A
 45%|████▌     | 482/1071 [00:10<00:13, 43.61it/s][A
 45%|████▌     | 487/1071 [00:11<00:13, 43.90it/s][A
 46%|████▌     | 492/1071 [00:11<00:13, 43.96it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.39it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.55it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 44.62it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 44.58it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 44.58it/s][A
 49%|████▊     | 522/1071 [00:11<00:12, 44.37it/s][A
 49%|████▉     | 527/1071 [00:11<00:12, 44.31it/s][A
 50%|████▉     | 532/1071 [00:12<00:12, 44.37it/s][A
 50%|█████     | 537/1071 [00:12<00:11, 44.56it/s][A
 51%|█████     | 542/1071 [00:12<00:11, 44.81it/s][A
 51%|█████     | 547/1071 [00:12<00:11, 44.98it/s][A
 52%|█████▏    | 552/1071 [00:12<00:11, 44.98it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 44.82it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 44.79it/s][A
 53%|█████▎    | 567/1071 [00:12<00:11, 44.57it/s][A
 53%|█████▎    | 572/1071 [00:12<00:11, 44.45it/s][A
 54%|█████▍    | 577/1071 [00:13<00:11, 44.51it/s][A
 54%|█████▍    | 582/1071 [00:13<00:11, 44.42it/s][A
 55%|█████▍    | 587/1071 [00:13<00:10, 44.64it/s][A
 55%|█████▌    | 592/1071 [00:13<00:10, 44.74it/s][A
 56%|█████▌    | 597/1071 [00:13<00:10, 44.90it/s][A
 56%|█████▌    | 602/1071 [00:13<00:10, 44.76it/s][A
 57%|█████▋    | 607/1071 [00:13<00:10, 44.44it/s][A
 57%|█████▋    | 612/1071 [00:13<00:10, 44.55it/s][A
 58%|█████▊    | 617/1071 [00:13<00:10, 43.37it/s][A
 58%|█████▊    | 622/1071 [00:14<00:10, 43.84it/s][A
 59%|█████▊    | 627/1071 [00:14<00:10, 43.95it/s][A
 59%|█████▉    | 632/1071 [00:14<00:09, 44.44it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.52it/s][A
 60%|█████▉    | 642/1071 [00:14<00:09, 44.65it/s][A
 60%|██████    | 647/1071 [00:14<00:09, 44.72it/s][A
 61%|██████    | 652/1071 [00:14<00:09, 44.66it/s][A
 61%|██████▏   | 657/1071 [00:14<00:09, 44.51it/s][A
 62%|██████▏   | 662/1071 [00:14<00:09, 44.43it/s][A
 62%|██████▏   | 667/1071 [00:15<00:09, 44.51it/s][A
 63%|██████▎   | 672/1071 [00:15<00:08, 44.69it/s][A
 63%|██████▎   | 677/1071 [00:15<00:08, 44.78it/s][A
 64%|██████▎   | 682/1071 [00:15<00:08, 44.82it/s][A
 64%|██████▍   | 687/1071 [00:15<00:08, 44.70it/s][A
 65%|██████▍   | 692/1071 [00:15<00:08, 44.75it/s][A
 65%|██████▌   | 697/1071 [00:15<00:08, 44.58it/s][A
 66%|██████▌   | 702/1071 [00:15<00:08, 44.56it/s][A
 66%|██████▌   | 707/1071 [00:15<00:08, 44.38it/s][A
 66%|██████▋   | 712/1071 [00:16<00:08, 44.51it/s][A
 67%|██████▋   | 717/1071 [00:16<00:07, 44.66it/s][A
 67%|██████▋   | 722/1071 [00:16<00:07, 44.54it/s][A
 68%|██████▊   | 727/1071 [00:16<00:07, 44.72it/s][A
 68%|██████▊   | 732/1071 [00:16<00:07, 44.84it/s][A
 69%|██████▉   | 737/1071 [00:16<00:07, 44.66it/s][A
 69%|██████▉   | 742/1071 [00:16<00:07, 44.62it/s][A
 70%|██████▉   | 747/1071 [00:16<00:07, 44.63it/s][A
 70%|███████   | 752/1071 [00:16<00:07, 42.59it/s][A
 71%|███████   | 757/1071 [00:17<00:07, 43.50it/s][A
 71%|███████   | 762/1071 [00:17<00:07, 43.74it/s][A
 72%|███████▏  | 767/1071 [00:17<00:06, 44.09it/s][A
 72%|███████▏  | 772/1071 [00:17<00:06, 44.43it/s][A
 73%|███████▎  | 777/1071 [00:17<00:06, 44.36it/s][A
 73%|███████▎  | 782/1071 [00:17<00:06, 44.45it/s][A
 73%|███████▎  | 787/1071 [00:17<00:06, 44.56it/s][A
 74%|███████▍  | 792/1071 [00:17<00:06, 44.41it/s][A
 74%|███████▍  | 797/1071 [00:17<00:06, 44.37it/s][A
 75%|███████▍  | 802/1071 [00:18<00:06, 44.57it/s][A
 75%|███████▌  | 807/1071 [00:18<00:05, 44.70it/s][A
 76%|███████▌  | 812/1071 [00:18<00:05, 44.59it/s][A
 76%|███████▋  | 817/1071 [00:18<00:05, 44.73it/s][A
 77%|███████▋  | 822/1071 [00:18<00:05, 44.57it/s][A
 77%|███████▋  | 827/1071 [00:18<00:05, 44.69it/s][A
 78%|███████▊  | 832/1071 [00:18<00:05, 44.58it/s][A
 78%|███████▊  | 837/1071 [00:18<00:05, 44.39it/s][A
 79%|███████▊  | 842/1071 [00:19<00:05, 44.43it/s][A
 79%|███████▉  | 847/1071 [00:19<00:05, 44.53it/s][A
 80%|███████▉  | 852/1071 [00:19<00:04, 44.51it/s][A
 80%|████████  | 857/1071 [00:19<00:04, 44.58it/s][A
 80%|████████  | 862/1071 [00:19<00:04, 44.48it/s][A
 81%|████████  | 867/1071 [00:19<00:04, 44.69it/s][A
 81%|████████▏ | 872/1071 [00:19<00:04, 44.59it/s][A
 82%|████████▏ | 877/1071 [00:19<00:04, 44.44it/s][A
 82%|████████▏ | 882/1071 [00:19<00:04, 44.50it/s][A
 83%|████████▎ | 887/1071 [00:20<00:04, 43.09it/s][A
 83%|████████▎ | 892/1071 [00:20<00:04, 43.79it/s][A
 84%|████████▍ | 897/1071 [00:20<00:03, 44.00it/s][A
 84%|████████▍ | 902/1071 [00:20<00:03, 44.22it/s][A
 85%|████████▍ | 907/1071 [00:20<00:03, 44.28it/s][A
 85%|████████▌ | 912/1071 [00:20<00:03, 44.45it/s][A
 86%|████████▌ | 917/1071 [00:20<00:03, 44.49it/s][A
 86%|████████▌ | 922/1071 [00:20<00:03, 44.40it/s][A
 87%|████████▋ | 927/1071 [00:20<00:03, 44.42it/s][A
 87%|████████▋ | 932/1071 [00:21<00:03, 44.56it/s][A
 87%|████████▋ | 937/1071 [00:21<00:03, 44.51it/s][A
 88%|████████▊ | 942/1071 [00:21<00:02, 44.59it/s][A
 88%|████████▊ | 947/1071 [00:21<00:02, 44.53it/s][A
 89%|████████▉ | 952/1071 [00:21<00:02, 44.52it/s][A
 89%|████████▉ | 957/1071 [00:21<00:02, 44.60it/s][A
 90%|████████▉ | 962/1071 [00:21<00:02, 44.43it/s][A
 90%|█████████ | 967/1071 [00:21<00:02, 44.61it/s][A
 91%|█████████ | 972/1071 [00:21<00:02, 44.56it/s][A
 91%|█████████ | 977/1071 [00:22<00:02, 44.50it/s][A
 92%|█████████▏| 982/1071 [00:22<00:01, 44.63it/s][A
 92%|█████████▏| 987/1071 [00:22<00:01, 44.81it/s][A
 93%|█████████▎| 992/1071 [00:22<00:01, 44.80it/s][A
 93%|█████████▎| 997/1071 [00:22<00:01, 44.61it/s][A
 94%|█████████▎| 1002/1071 [00:22<00:01, 44.43it/s][A
 94%|█████████▍| 1007/1071 [00:22<00:01, 44.55it/s][A
 94%|█████████▍| 1012/1071 [00:22<00:01, 44.37it/s][A
 95%|█████████▍| 1017/1071 [00:22<00:01, 44.47it/s][A
 95%|█████████▌| 1022/1071 [00:23<00:01, 44.00it/s][A
 96%|█████████▌| 1027/1071 [00:23<00:00, 44.22it/s][A
 96%|█████████▋| 1032/1071 [00:23<00:00, 44.36it/s][A
 97%|█████████▋| 1037/1071 [00:23<00:00, 44.52it/s][A
 97%|█████████▋| 1042/1071 [00:23<00:00, 44.51it/s][A
 98%|█████████▊| 1047/1071 [00:23<00:00, 44.46it/s][A
 98%|█████████▊| 1052/1071 [00:23<00:00, 44.36it/s][A
 99%|█████████▊| 1057/1071 [00:23<00:00, 44.29it/s][A
 99%|█████████▉| 1062/1071 [00:23<00:00, 44.31it/s][A
100%|█████████▉| 1067/1071 [00:24<00:00, 44.17it/s][A
                                                   [A                                                 
100%|██████████| 1071/1071 [00:24<00:00, 44.17it/s][A 80%|████████  | 468/585 [04:23<00:36,  3.18it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-29 09:09:09,241 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 09:09:09,362 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:09:12,226 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:09:12,372 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:09:12,430 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [04:34<20:56, 10.84s/it] 80%|████████  | 470/585 [04:34<14:48,  7.72s/it] 81%|████████  | 471/585 [04:35<10:26,  5.50s/it] 81%|████████  | 472/585 [04:35<07:24,  3.93s/it] 81%|████████  | 473/585 [04:35<05:18,  2.84s/it] 81%|████████  | 474/585 [04:36<03:50,  2.08s/it] 81%|████████  | 475/585 [04:36<02:49,  1.54s/it] 81%|████████▏ | 476/585 [04:36<02:07,  1.17s/it] 82%|████████▏ | 477/585 [04:36<01:37,  1.10it/s] 82%|████████▏ | 478/585 [04:37<01:17,  1.38it/s] 82%|████████▏ | 479/585 [04:37<01:02,  1.68it/s] 82%|████████▏ | 480/585 [04:37<00:54,  1.92it/s] 82%|████████▏ | 481/585 [04:38<00:47,  2.21it/s] 82%|████████▏ | 482/585 [04:38<00:41,  2.47it/s] 83%|████████▎ | 483/585 [04:38<00:37,  2.69it/s] 83%|████████▎ | 484/585 [04:39<00:35,  2.86it/s] 83%|████████▎ | 485/585 [04:39<00:33,  3.00it/s] 83%|████████▎ | 486/585 [04:39<00:31,  3.11it/s] 83%|████████▎ | 487/585 [04:39<00:30,  3.19it/s] 83%|████████▎ | 488/585 [04:40<00:29,  3.25it/s] 84%|████████▎ | 489/585 [04:40<00:29,  3.29it/s] 84%|████████▍ | 490/585 [04:40<00:29,  3.27it/s] 84%|████████▍ | 491/585 [04:41<00:28,  3.31it/s] 84%|████████▍ | 492/585 [04:41<00:27,  3.33it/s] 84%|████████▍ | 493/585 [04:41<00:27,  3.35it/s] 84%|████████▍ | 494/585 [04:42<00:27,  3.36it/s] 85%|████████▍ | 495/585 [04:42<00:26,  3.37it/s] 85%|████████▍ | 496/585 [04:42<00:26,  3.38it/s] 85%|████████▍ | 497/585 [04:42<00:25,  3.39it/s] 85%|████████▌ | 498/585 [04:43<00:34,  2.50it/s] 85%|████████▌ | 499/585 [04:43<00:32,  2.63it/s] 85%|████████▌ | 500/585 [04:44<00:30,  2.82it/s]                                                  85%|████████▌ | 500/585 [04:44<00:30,  2.82it/s] 86%|████████▌ | 501/585 [04:44<00:28,  2.97it/s] 86%|████████▌ | 502/585 [04:44<00:26,  3.08it/s] 86%|████████▌ | 503/585 [04:45<00:25,  3.17it/s] 86%|████████▌ | 504/585 [04:45<00:25,  3.24it/s] 86%|████████▋ | 505/585 [04:45<00:24,  3.28it/s] 86%|████████▋ | 506/585 [04:45<00:23,  3.32it/s] 87%|████████▋ | 507/585 [04:46<00:23,  3.34it/s] 87%|████████▋ | 508/585 [04:46<00:22,  3.36it/s] 87%|████████▋ | 509/585 [04:46<00:22,  3.37it/s] 87%|████████▋ | 510/585 [04:47<00:22,  3.32it/s] 87%|████████▋ | 511/585 [04:47<00:22,  3.35it/s] 88%|████████▊ | 512/585 [04:47<00:21,  3.38it/s] 88%|████████▊ | 513/585 [04:48<00:21,  3.40it/s] 88%|████████▊ | 514/585 [04:48<00:20,  3.41it/s] 88%|████████▊ | 515/585 [04:48<00:20,  3.42it/s] 88%|████████▊ | 516/585 [04:48<00:20,  3.43it/s] 88%|████████▊ | 517/585 [04:49<00:19,  3.43it/s] 89%|████████▊ | 518/585 [04:49<00:19,  3.44it/s] 89%|████████▊ | 519/585 [04:49<00:19,  3.44it/s] 89%|████████▉ | 520/585 [04:50<00:18,  3.44it/s] 89%|████████▉ | 521/585 [04:50<00:19,  3.32it/s] 89%|████████▉ | 522/585 [04:50<00:18,  3.36it/s] 89%|████████▉ | 523/585 [04:50<00:18,  3.39it/s] 90%|████████▉ | 524/585 [04:51<00:17,  3.41it/s] 90%|████████▉ | 525/585 [04:51<00:17,  3.42it/s] 90%|████████▉ | 526/585 [04:51<00:17,  3.43it/s] 90%|█████████ | 527/585 [04:52<00:16,  3.44it/s] 90%|█████████ | 528/585 [04:52<00:16,  3.44it/s] 90%|█████████ | 529/585 [04:52<00:16,  3.44it/s] 91%|█████████ | 530/585 [04:52<00:15,  3.44it/s] 91%|█████████ | 531/585 [04:53<00:15,  3.44it/s] 91%|█████████ | 532/585 [04:53<00:16,  3.31it/s] 91%|█████████ | 533/585 [04:53<00:15,  3.35it/s] 91%|█████████▏| 534/585 [04:54<00:15,  3.38it/s] 91%|█████████▏| 535/585 [04:54<00:14,  3.40it/s] 92%|█████████▏| 536/585 [04:54<00:14,  3.42it/s] 92%|█████████▏| 537/585 [04:55<00:14,  3.43it/s] 92%|█████████▏| 538/585 [04:55<00:13,  3.43it/s] 92%|█████████▏| 539/585 [04:55<00:13,  3.44it/s] 92%|█████████▏| 540/585 [04:55<00:13,  3.45it/s] 92%|█████████▏| 541/585 [04:56<00:12,  3.45it/s] 93%|█████████▎| 542/585 [04:56<00:12,  3.45it/s] 93%|█████████▎| 543/585 [04:56<00:12,  3.28it/s] 93%|█████████▎| 544/585 [04:57<00:12,  3.33it/s] 93%|█████████▎| 545/585 [04:57<00:11,  3.37it/s] 93%|█████████▎| 546/585 [04:57<00:11,  3.39it/s] 94%|█████████▎| 547/585 [04:58<00:11,  3.41it/s] 94%|█████████▎| 548/585 [04:58<00:10,  3.42it/s] 94%|█████████▍| 549/585 [04:58<00:10,  3.43it/s] 94%|█████████▍| 550/585 [04:58<00:10,  3.44it/s] 94%|█████████▍| 551/585 [04:59<00:09,  3.45it/s] 94%|█████████▍| 552/585 [04:59<00:09,  3.45it/s] 95%|█████████▍| 553/585 [04:59<00:09,  3.45it/s] 95%|█████████▍| 554/585 [05:00<00:09,  3.19it/s] 95%|█████████▍| 555/585 [05:00<00:09,  3.26it/s] 95%|█████████▌| 556/585 [05:00<00:08,  3.31it/s] 95%|█████████▌| 557/585 [05:00<00:08,  3.35it/s] 95%|█████████▌| 558/585 [05:01<00:07,  3.38it/s] 96%|█████████▌| 559/585 [05:01<00:07,  3.41it/s] 96%|█████████▌| 560/585 [05:01<00:07,  3.42it/s] 96%|█████████▌| 561/585 [05:02<00:06,  3.43it/s] 96%|█████████▌| 562/585 [05:02<00:06,  3.44it/s] 96%|█████████▌| 563/585 [05:02<00:06,  3.44it/s] 96%|█████████▋| 564/585 [05:03<00:06,  3.45it/s] 97%|█████████▋| 565/585 [05:03<00:06,  3.22it/s] 97%|█████████▋| 566/585 [05:03<00:05,  3.29it/s] 97%|█████████▋| 567/585 [05:03<00:05,  3.33it/s] 97%|█████████▋| 568/585 [05:04<00:05,  3.36it/s] 97%|█████████▋| 569/585 [05:04<00:04,  3.39it/s] 97%|█████████▋| 570/585 [05:04<00:04,  3.40it/s] 98%|█████████▊| 571/585 [05:05<00:04,  3.42it/s] 98%|█████████▊| 572/585 [05:05<00:03,  3.43it/s] 98%|█████████▊| 573/585 [05:05<00:03,  3.43it/s] 98%|█████████▊| 574/585 [05:05<00:03,  3.43it/s] 98%|█████████▊| 575/585 [05:06<00:02,  3.43it/s] 98%|█████████▊| 576/585 [05:06<00:02,  3.43it/s] 99%|█████████▊| 577/585 [05:06<00:02,  3.44it/s] 99%|█████████▉| 578/585 [05:07<00:02,  3.44it/s] 99%|█████████▉| 579/585 [05:07<00:01,  3.44it/s] 99%|█████████▉| 580/585 [05:07<00:01,  3.44it/s] 99%|█████████▉| 581/585 [05:08<00:01,  3.44it/s] 99%|█████████▉| 582/585 [05:08<00:00,  3.44it/s]100%|█████████▉| 583/585 [05:08<00:00,  3.32it/s]100%|█████████▉| 584/585 [05:08<00:00,  3.36it/s]100%|██████████| 585/585 [05:09<00:00,  3.38it/s][INFO|trainer.py:2140] 2023-08-29 09:09:55,044 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:09:55,044 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 09:09:55,044 >>   Batch size = 8
{'eval_loss': 0.9568675756454468, 'eval_runtime': 24.2026, 'eval_samples_per_second': 354.012, 'eval_steps_per_second': 44.251, 'epoch': 4.0}
{'loss': 0.6156, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:19, 55.52it/s][A
  1%|          | 12/1071 [00:00<00:21, 48.55it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 46.81it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 45.90it/s][A
  3%|▎         | 27/1071 [00:00<00:22, 45.52it/s][A
  3%|▎         | 32/1071 [00:00<00:23, 45.06it/s][A
  3%|▎         | 37/1071 [00:00<00:23, 44.85it/s][A
  4%|▍         | 42/1071 [00:00<00:23, 44.69it/s][A
  4%|▍         | 47/1071 [00:01<00:22, 44.84it/s][A
  5%|▍         | 52/1071 [00:01<00:22, 44.74it/s][A
  5%|▌         | 57/1071 [00:01<00:22, 44.82it/s][A
  6%|▌         | 62/1071 [00:01<00:22, 44.87it/s][A
  6%|▋         | 67/1071 [00:01<00:22, 44.79it/s][A
  7%|▋         | 72/1071 [00:01<00:22, 44.65it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 44.75it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 44.63it/s][A
  8%|▊         | 87/1071 [00:01<00:22, 44.65it/s][A
  9%|▊         | 92/1071 [00:02<00:21, 44.86it/s][A
  9%|▉         | 97/1071 [00:02<00:21, 44.82it/s][A
 10%|▉         | 102/1071 [00:02<00:23, 41.54it/s][A
 10%|▉         | 107/1071 [00:02<00:22, 42.70it/s][A
 10%|█         | 112/1071 [00:02<00:22, 43.27it/s][A
 11%|█         | 117/1071 [00:02<00:21, 43.55it/s][A
 11%|█▏        | 122/1071 [00:02<00:21, 43.92it/s][A
 12%|█▏        | 127/1071 [00:02<00:21, 44.10it/s][A
 12%|█▏        | 132/1071 [00:02<00:21, 44.24it/s][A
 13%|█▎        | 137/1071 [00:03<00:20, 44.49it/s][A
 13%|█▎        | 142/1071 [00:03<00:21, 44.23it/s][A
 14%|█▎        | 147/1071 [00:03<00:20, 44.25it/s][A
 14%|█▍        | 152/1071 [00:03<00:20, 44.52it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 44.73it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 44.81it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 44.96it/s][A
 16%|█▌        | 172/1071 [00:03<00:20, 44.81it/s][A
 17%|█▋        | 177/1071 [00:03<00:19, 44.89it/s][A
 17%|█▋        | 182/1071 [00:04<00:19, 44.70it/s][A
 17%|█▋        | 187/1071 [00:04<00:19, 44.47it/s][A
 18%|█▊        | 192/1071 [00:04<00:19, 44.38it/s][A
 18%|█▊        | 197/1071 [00:04<00:19, 44.61it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 44.71it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 44.79it/s][A
 20%|█▉        | 212/1071 [00:04<00:19, 44.75it/s][A
 20%|██        | 217/1071 [00:04<00:19, 44.82it/s][A
 21%|██        | 222/1071 [00:04<00:18, 44.86it/s][A
 21%|██        | 227/1071 [00:05<00:18, 44.61it/s][A
 22%|██▏       | 232/1071 [00:05<00:18, 44.74it/s][A
 22%|██▏       | 237/1071 [00:05<00:19, 42.60it/s][A
 23%|██▎       | 242/1071 [00:05<00:19, 43.36it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 43.92it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 44.12it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 44.28it/s][A
 24%|██▍       | 262/1071 [00:05<00:18, 44.41it/s][A
 25%|██▍       | 267/1071 [00:05<00:18, 44.50it/s][A
 25%|██▌       | 272/1071 [00:06<00:17, 44.46it/s][A
 26%|██▌       | 277/1071 [00:06<00:17, 44.20it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 44.39it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 44.45it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 44.42it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 44.58it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 44.61it/s][A
 29%|██▊       | 307/1071 [00:06<00:17, 44.68it/s][A
 29%|██▉       | 312/1071 [00:07<00:16, 44.73it/s][A
 30%|██▉       | 317/1071 [00:07<00:16, 44.63it/s][A
 30%|███       | 322/1071 [00:07<00:16, 44.50it/s][A
 31%|███       | 327/1071 [00:07<00:16, 44.58it/s][A
 31%|███       | 332/1071 [00:07<00:16, 44.41it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 44.65it/s][A
 32%|███▏      | 342/1071 [00:07<00:16, 44.83it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 44.86it/s][A
 33%|███▎      | 352/1071 [00:07<00:16, 44.86it/s][A
 33%|███▎      | 357/1071 [00:08<00:15, 44.67it/s][A
 34%|███▍      | 362/1071 [00:08<00:15, 44.47it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.51it/s][A
 35%|███▍      | 372/1071 [00:08<00:16, 43.47it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 43.98it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 44.37it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 44.48it/s][A
 37%|███▋      | 392/1071 [00:08<00:15, 44.51it/s][A
 37%|███▋      | 397/1071 [00:08<00:15, 44.55it/s][A
 38%|███▊      | 402/1071 [00:09<00:15, 44.32it/s][A
 38%|███▊      | 407/1071 [00:09<00:14, 44.40it/s][A
 38%|███▊      | 412/1071 [00:09<00:14, 44.22it/s][A
 39%|███▉      | 417/1071 [00:09<00:14, 44.39it/s][A
 39%|███▉      | 422/1071 [00:09<00:14, 44.72it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 44.83it/s][A
 40%|████      | 432/1071 [00:09<00:14, 44.97it/s][A
 41%|████      | 437/1071 [00:09<00:14, 44.79it/s][A
 41%|████▏     | 442/1071 [00:09<00:14, 44.76it/s][A
 42%|████▏     | 447/1071 [00:10<00:13, 44.64it/s][A
 42%|████▏     | 452/1071 [00:10<00:13, 44.38it/s][A
 43%|████▎     | 457/1071 [00:10<00:13, 44.33it/s][A
 43%|████▎     | 462/1071 [00:10<00:13, 44.54it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 44.56it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 44.66it/s][A
 45%|████▍     | 477/1071 [00:10<00:13, 44.64it/s][A
 45%|████▌     | 482/1071 [00:10<00:13, 44.82it/s][A
 45%|████▌     | 487/1071 [00:10<00:13, 44.54it/s][A
 46%|████▌     | 492/1071 [00:11<00:13, 44.47it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.46it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.54it/s][A
 47%|████▋     | 507/1071 [00:11<00:13, 43.28it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 43.85it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 44.21it/s][A
 49%|████▊     | 522/1071 [00:11<00:12, 44.43it/s][A
 49%|████▉     | 527/1071 [00:11<00:12, 44.59it/s][A
 50%|████▉     | 532/1071 [00:11<00:12, 44.58it/s][A
 50%|█████     | 537/1071 [00:12<00:11, 44.55it/s][A
 51%|█████     | 542/1071 [00:12<00:11, 44.54it/s][A
 51%|█████     | 547/1071 [00:12<00:11, 44.48it/s][A
 52%|█████▏    | 552/1071 [00:12<00:11, 44.50it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 44.61it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 44.62it/s][A
 53%|█████▎    | 567/1071 [00:12<00:11, 44.66it/s][A
 53%|█████▎    | 572/1071 [00:12<00:11, 44.69it/s][A
 54%|█████▍    | 577/1071 [00:12<00:11, 44.68it/s][A
 54%|█████▍    | 582/1071 [00:13<00:10, 44.63it/s][A
 55%|█████▍    | 587/1071 [00:13<00:10, 44.38it/s][A
 55%|█████▌    | 592/1071 [00:13<00:10, 44.42it/s][A
 56%|█████▌    | 597/1071 [00:13<00:10, 44.41it/s][A
 56%|█████▌    | 602/1071 [00:13<00:10, 44.59it/s][A
 57%|█████▋    | 607/1071 [00:13<00:10, 44.72it/s][A
 57%|█████▋    | 612/1071 [00:13<00:10, 44.70it/s][A
 58%|█████▊    | 617/1071 [00:13<00:10, 44.83it/s][A
 58%|█████▊    | 622/1071 [00:13<00:10, 44.65it/s][A
 59%|█████▊    | 627/1071 [00:14<00:09, 44.46it/s][A
 59%|█████▉    | 632/1071 [00:14<00:09, 44.58it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.41it/s][A
 60%|█████▉    | 642/1071 [00:14<00:09, 44.03it/s][A
 60%|██████    | 647/1071 [00:14<00:09, 44.29it/s][A
 61%|██████    | 652/1071 [00:14<00:09, 44.38it/s][A
 61%|██████▏   | 657/1071 [00:14<00:09, 44.54it/s][A
 62%|██████▏   | 662/1071 [00:14<00:09, 44.67it/s][A
 62%|██████▏   | 667/1071 [00:14<00:09, 44.57it/s][A
 63%|██████▎   | 672/1071 [00:15<00:08, 44.41it/s][A
 63%|██████▎   | 677/1071 [00:15<00:08, 44.61it/s][A
 64%|██████▎   | 682/1071 [00:15<00:08, 44.48it/s][A
 64%|██████▍   | 687/1071 [00:15<00:08, 44.46it/s][A
 65%|██████▍   | 692/1071 [00:15<00:08, 44.53it/s][A
 65%|██████▌   | 697/1071 [00:15<00:08, 44.65it/s][A
 66%|██████▌   | 702/1071 [00:15<00:08, 44.64it/s][A
 66%|██████▌   | 707/1071 [00:15<00:08, 44.77it/s][A
 66%|██████▋   | 712/1071 [00:15<00:08, 44.69it/s][A
 67%|██████▋   | 717/1071 [00:16<00:07, 44.77it/s][A
 67%|██████▋   | 722/1071 [00:16<00:07, 44.59it/s][A
 68%|██████▊   | 727/1071 [00:16<00:07, 44.59it/s][A
 68%|██████▊   | 732/1071 [00:16<00:07, 44.60it/s][A
 69%|██████▉   | 737/1071 [00:16<00:07, 44.64it/s][A
 69%|██████▉   | 742/1071 [00:16<00:07, 44.61it/s][A
 70%|██████▉   | 747/1071 [00:16<00:07, 44.71it/s][A
 70%|███████   | 752/1071 [00:16<00:07, 44.87it/s][A
 71%|███████   | 757/1071 [00:16<00:07, 44.68it/s][A
 71%|███████   | 762/1071 [00:17<00:06, 44.56it/s][A
 72%|███████▏  | 767/1071 [00:17<00:06, 44.68it/s][A
 72%|███████▏  | 772/1071 [00:17<00:06, 44.61it/s][A
 73%|███████▎  | 777/1071 [00:17<00:06, 42.42it/s][A
 73%|███████▎  | 782/1071 [00:17<00:06, 43.41it/s][A
 73%|███████▎  | 787/1071 [00:17<00:06, 43.84it/s][A
 74%|███████▍  | 792/1071 [00:17<00:06, 44.24it/s][A
 74%|███████▍  | 797/1071 [00:17<00:06, 44.33it/s][A
 75%|███████▍  | 802/1071 [00:18<00:06, 44.48it/s][A
 75%|███████▌  | 807/1071 [00:18<00:05, 44.52it/s][A
 76%|███████▌  | 812/1071 [00:18<00:05, 44.54it/s][A
 76%|███████▋  | 817/1071 [00:18<00:05, 44.31it/s][A
 77%|███████▋  | 822/1071 [00:18<00:05, 44.33it/s][A
 77%|███████▋  | 827/1071 [00:18<00:05, 44.42it/s][A
 78%|███████▊  | 832/1071 [00:18<00:05, 44.71it/s][A
 78%|███████▊  | 837/1071 [00:18<00:05, 44.65it/s][A
 79%|███████▊  | 842/1071 [00:18<00:05, 44.87it/s][A
 79%|███████▉  | 847/1071 [00:19<00:04, 44.89it/s][A
 80%|███████▉  | 852/1071 [00:19<00:04, 44.78it/s][A
 80%|████████  | 857/1071 [00:19<00:04, 44.55it/s][A
 80%|████████  | 862/1071 [00:19<00:04, 44.27it/s][A
 81%|████████  | 867/1071 [00:19<00:04, 44.35it/s][A
 81%|████████▏ | 872/1071 [00:19<00:04, 44.41it/s][A
 82%|████████▏ | 877/1071 [00:19<00:04, 44.58it/s][A
 82%|████████▏ | 882/1071 [00:19<00:04, 44.68it/s][A
 83%|████████▎ | 887/1071 [00:19<00:04, 44.86it/s][A
 83%|████████▎ | 892/1071 [00:20<00:03, 44.94it/s][A
 84%|████████▍ | 897/1071 [00:20<00:03, 44.78it/s][A
 84%|████████▍ | 902/1071 [00:20<00:03, 44.65it/s][A
 85%|████████▍ | 907/1071 [00:20<00:03, 44.48it/s][A
 85%|████████▌ | 912/1071 [00:20<00:03, 43.07it/s][A
 86%|████████▌ | 917/1071 [00:20<00:03, 43.70it/s][A
 86%|████████▌ | 922/1071 [00:20<00:03, 43.93it/s][A
 87%|████████▋ | 927/1071 [00:20<00:03, 44.35it/s][A
 87%|████████▋ | 932/1071 [00:20<00:03, 44.49it/s][A
 87%|████████▋ | 937/1071 [00:21<00:03, 44.55it/s][A
 88%|████████▊ | 942/1071 [00:21<00:02, 44.45it/s][A
 88%|████████▊ | 947/1071 [00:21<00:02, 44.30it/s][A
 89%|████████▉ | 952/1071 [00:21<00:02, 44.29it/s][A
 89%|████████▉ | 957/1071 [00:21<00:02, 44.31it/s][A
 90%|████████▉ | 962/1071 [00:21<00:02, 44.38it/s][A
 90%|█████████ | 967/1071 [00:21<00:02, 44.73it/s][A
 91%|█████████ | 972/1071 [00:21<00:02, 44.72it/s][A
 91%|█████████ | 977/1071 [00:21<00:02, 44.93it/s][A
 92%|█████████▏| 982/1071 [00:22<00:01, 44.67it/s][A
 92%|█████████▏| 987/1071 [00:22<00:01, 44.52it/s][A
 93%|█████████▎| 992/1071 [00:22<00:01, 44.43it/s][A
 93%|█████████▎| 997/1071 [00:22<00:01, 44.48it/s][A
 94%|█████████▎| 1002/1071 [00:22<00:01, 44.48it/s][A
 94%|█████████▍| 1007/1071 [00:22<00:01, 44.58it/s][A
 94%|█████████▍| 1012/1071 [00:22<00:01, 44.73it/s][A
 95%|█████████▍| 1017/1071 [00:22<00:01, 44.77it/s][A
 95%|█████████▌| 1022/1071 [00:22<00:01, 44.78it/s][A
 96%|█████████▌| 1027/1071 [00:23<00:00, 44.70it/s][A
 96%|█████████▋| 1032/1071 [00:23<00:00, 44.56it/s][A
 97%|█████████▋| 1037/1071 [00:23<00:00, 44.61it/s][A
 97%|█████████▋| 1042/1071 [00:23<00:00, 44.53it/s][A
 98%|█████████▊| 1047/1071 [00:23<00:00, 43.08it/s][A
 98%|█████████▊| 1052/1071 [00:23<00:00, 43.69it/s][A
 99%|█████████▊| 1057/1071 [00:23<00:00, 44.06it/s][A
 99%|█████████▉| 1062/1071 [00:23<00:00, 44.12it/s][A
100%|█████████▉| 1067/1071 [00:23<00:00, 44.19it/s][A
                                                   [A                                                 
100%|██████████| 1071/1071 [00:24<00:00, 44.19it/s][A100%|██████████| 585/585 [05:33<00:00,  3.38it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-29 09:10:19,345 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 09:10:19,544 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:10:23,244 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:10:23,354 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:10:23,430 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 09:10:35,208 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 09:10:35,255 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-117 (score: 0.9300344586372375).
                                                 100%|██████████| 585/585 [06:02<00:00,  3.38it/s]100%|██████████| 585/585 [06:02<00:00,  1.61it/s]
[INFO|trainer.py:1894] 2023-08-29 09:10:48,698 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 09:10:48,831 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:10:51,866 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:10:51,967 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:10:52,042 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 09:10:52,425 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:10:52,426 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:10:52,426 >>   train_loss               =     0.6113
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:10:52,426 >>   train_runtime            = 0:06:02.81
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:10:52,426 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:10:52,426 >>   train_samples_per_second =    103.359
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:10:52,426 >>   train_steps_per_second   =      1.612
{'eval_loss': 0.962845504283905, 'eval_runtime': 24.0982, 'eval_samples_per_second': 355.546, 'eval_steps_per_second': 44.443, 'epoch': 5.0}
{'train_runtime': 362.8143, 'train_samples_per_second': 103.359, 'train_steps_per_second': 1.612, 'train_loss': 0.6113257122854902, 'epoch': 5.0}
08/29/2023 09:10:52 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 09:10:52,602 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:10:52,602 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 09:10:52,602 >>   Batch size = 8
  0%|          | 0/1071 [00:00<?, ?it/s]  1%|          | 6/1071 [00:00<00:19, 55.21it/s]  1%|          | 12/1071 [00:00<00:21, 48.91it/s]  2%|▏         | 17/1071 [00:00<00:22, 47.57it/s]  2%|▏         | 22/1071 [00:00<00:22, 46.85it/s]  3%|▎         | 27/1071 [00:00<00:22, 46.44it/s]  3%|▎         | 32/1071 [00:00<00:22, 45.93it/s]  3%|▎         | 37/1071 [00:00<00:22, 45.73it/s]  4%|▍         | 42/1071 [00:00<00:22, 45.29it/s]  4%|▍         | 47/1071 [00:01<00:22, 44.76it/s]  5%|▍         | 52/1071 [00:01<00:22, 44.66it/s]  5%|▌         | 57/1071 [00:01<00:22, 44.89it/s]  6%|▌         | 62/1071 [00:01<00:22, 44.93it/s]  6%|▋         | 67/1071 [00:01<00:22, 45.08it/s]  7%|▋         | 72/1071 [00:01<00:22, 45.12it/s]  7%|▋         | 77/1071 [00:01<00:22, 45.03it/s]  8%|▊         | 82/1071 [00:01<00:21, 44.97it/s]  8%|▊         | 87/1071 [00:01<00:21, 44.86it/s]  9%|▊         | 92/1071 [00:02<00:22, 44.48it/s]  9%|▉         | 97/1071 [00:02<00:21, 44.41it/s] 10%|▉         | 102/1071 [00:02<00:21, 44.50it/s] 10%|▉         | 107/1071 [00:02<00:21, 44.76it/s] 10%|█         | 112/1071 [00:02<00:21, 45.00it/s] 11%|█         | 117/1071 [00:02<00:21, 45.23it/s] 11%|█▏        | 122/1071 [00:02<00:20, 45.25it/s] 12%|█▏        | 127/1071 [00:02<00:20, 45.24it/s] 12%|█▏        | 132/1071 [00:02<00:20, 45.02it/s] 13%|█▎        | 137/1071 [00:03<00:22, 40.93it/s] 13%|█▎        | 142/1071 [00:03<00:21, 42.34it/s] 14%|█▎        | 147/1071 [00:03<00:21, 43.12it/s] 14%|█▍        | 152/1071 [00:03<00:21, 43.73it/s] 15%|█▍        | 157/1071 [00:03<00:20, 44.22it/s] 15%|█▌        | 162/1071 [00:03<00:20, 44.70it/s] 16%|█▌        | 167/1071 [00:03<00:20, 44.81it/s] 16%|█▌        | 172/1071 [00:03<00:20, 44.79it/s] 17%|█▋        | 177/1071 [00:03<00:20, 44.30it/s] 17%|█▋        | 182/1071 [00:04<00:20, 44.32it/s] 17%|█▋        | 187/1071 [00:04<00:19, 44.61it/s] 18%|█▊        | 192/1071 [00:04<00:19, 44.82it/s] 18%|█▊        | 197/1071 [00:04<00:19, 44.69it/s] 19%|█▉        | 202/1071 [00:04<00:19, 44.76it/s] 19%|█▉        | 207/1071 [00:04<00:19, 45.06it/s] 20%|█▉        | 212/1071 [00:04<00:19, 45.05it/s] 20%|██        | 217/1071 [00:04<00:19, 44.89it/s] 21%|██        | 222/1071 [00:04<00:19, 44.46it/s] 21%|██        | 227/1071 [00:05<00:18, 44.52it/s] 22%|██▏       | 232/1071 [00:05<00:18, 44.45it/s] 22%|██▏       | 237/1071 [00:05<00:18, 44.52it/s] 23%|██▎       | 242/1071 [00:05<00:18, 44.59it/s] 23%|██▎       | 247/1071 [00:05<00:18, 44.68it/s] 24%|██▎       | 252/1071 [00:05<00:18, 44.80it/s] 24%|██▍       | 257/1071 [00:05<00:18, 44.78it/s] 24%|██▍       | 262/1071 [00:05<00:18, 44.93it/s] 25%|██▍       | 267/1071 [00:05<00:17, 44.87it/s] 25%|██▌       | 272/1071 [00:06<00:19, 41.03it/s] 26%|██▌       | 277/1071 [00:06<00:18, 42.27it/s] 26%|██▋       | 282/1071 [00:06<00:18, 43.05it/s] 27%|██▋       | 287/1071 [00:06<00:17, 43.89it/s] 27%|██▋       | 292/1071 [00:06<00:17, 44.34it/s] 28%|██▊       | 297/1071 [00:06<00:17, 44.67it/s] 28%|██▊       | 302/1071 [00:06<00:17, 44.69it/s] 29%|██▊       | 307/1071 [00:06<00:17, 44.74it/s] 29%|██▉       | 312/1071 [00:06<00:17, 44.22it/s] 30%|██▉       | 317/1071 [00:07<00:17, 44.25it/s] 30%|███       | 322/1071 [00:07<00:16, 44.56it/s] 31%|███       | 327/1071 [00:07<00:16, 44.83it/s] 31%|███       | 332/1071 [00:07<00:16, 45.01it/s] 31%|███▏      | 337/1071 [00:07<00:16, 45.08it/s] 32%|███▏      | 342/1071 [00:07<00:16, 45.09it/s] 32%|███▏      | 347/1071 [00:07<00:16, 45.16it/s] 33%|███▎      | 352/1071 [00:07<00:15, 44.98it/s] 33%|███▎      | 357/1071 [00:07<00:15, 44.72it/s] 34%|███▍      | 362/1071 [00:08<00:15, 44.61it/s] 34%|███▍      | 367/1071 [00:08<00:15, 44.71it/s] 35%|███▍      | 372/1071 [00:08<00:15, 44.92it/s] 35%|███▌      | 377/1071 [00:08<00:15, 45.07it/s] 36%|███▌      | 382/1071 [00:08<00:15, 45.13it/s] 36%|███▌      | 387/1071 [00:08<00:15, 45.16it/s] 37%|███▋      | 392/1071 [00:08<00:15, 45.09it/s] 37%|███▋      | 397/1071 [00:08<00:15, 44.79it/s] 38%|███▊      | 402/1071 [00:08<00:15, 44.46it/s] 38%|███▊      | 407/1071 [00:09<00:15, 41.65it/s] 38%|███▊      | 412/1071 [00:09<00:15, 42.57it/s] 39%|███▉      | 417/1071 [00:09<00:15, 43.46it/s] 39%|███▉      | 422/1071 [00:09<00:14, 44.01it/s] 40%|███▉      | 427/1071 [00:09<00:14, 44.35it/s] 40%|████      | 432/1071 [00:09<00:14, 44.68it/s] 41%|████      | 437/1071 [00:09<00:14, 44.79it/s] 41%|████▏     | 442/1071 [00:09<00:14, 44.57it/s] 42%|████▏     | 447/1071 [00:10<00:14, 44.41it/s] 42%|████▏     | 452/1071 [00:10<00:13, 44.27it/s] 43%|████▎     | 457/1071 [00:10<00:13, 44.50it/s] 43%|████▎     | 462/1071 [00:10<00:13, 44.58it/s] 44%|████▎     | 467/1071 [00:10<00:13, 44.70it/s] 44%|████▍     | 472/1071 [00:10<00:13, 44.87it/s] 45%|████▍     | 477/1071 [00:10<00:13, 44.99it/s] 45%|████▌     | 482/1071 [00:10<00:13, 45.01it/s] 45%|████▌     | 487/1071 [00:10<00:12, 45.03it/s] 46%|████▌     | 492/1071 [00:11<00:12, 44.70it/s] 46%|████▋     | 497/1071 [00:11<00:12, 44.52it/s] 47%|████▋     | 502/1071 [00:11<00:12, 44.48it/s] 47%|████▋     | 507/1071 [00:11<00:12, 44.60it/s] 48%|████▊     | 512/1071 [00:11<00:12, 44.85it/s] 48%|████▊     | 517/1071 [00:11<00:12, 45.01it/s] 49%|████▊     | 522/1071 [00:11<00:12, 45.16it/s] 49%|████▉     | 527/1071 [00:11<00:12, 45.27it/s] 50%|████▉     | 532/1071 [00:11<00:11, 45.06it/s] 50%|█████     | 537/1071 [00:12<00:11, 45.00it/s] 51%|█████     | 542/1071 [00:12<00:12, 41.30it/s] 51%|█████     | 547/1071 [00:12<00:12, 42.47it/s] 52%|█████▏    | 552/1071 [00:12<00:11, 43.35it/s] 52%|█████▏    | 557/1071 [00:12<00:11, 43.78it/s] 52%|█████▏    | 562/1071 [00:12<00:11, 44.29it/s] 53%|█████▎    | 567/1071 [00:12<00:11, 44.66it/s] 53%|█████▎    | 572/1071 [00:12<00:11, 44.87it/s] 54%|█████▍    | 577/1071 [00:12<00:11, 44.73it/s] 54%|█████▍    | 582/1071 [00:13<00:11, 44.28it/s] 55%|█████▍    | 587/1071 [00:13<00:10, 44.28it/s] 55%|█████▌    | 592/1071 [00:13<00:10, 44.34it/s] 56%|█████▌    | 597/1071 [00:13<00:10, 44.70it/s] 56%|█████▌    | 602/1071 [00:13<00:10, 45.01it/s] 57%|█████▋    | 607/1071 [00:13<00:10, 45.00it/s] 57%|█████▋    | 612/1071 [00:13<00:10, 45.00it/s] 58%|█████▊    | 617/1071 [00:13<00:10, 44.99it/s] 58%|█████▊    | 622/1071 [00:13<00:10, 44.81it/s] 59%|█████▊    | 627/1071 [00:14<00:09, 44.57it/s] 59%|█████▉    | 632/1071 [00:14<00:09, 44.56it/s] 59%|█████▉    | 637/1071 [00:14<00:09, 44.73it/s] 60%|█████▉    | 642/1071 [00:14<00:09, 44.64it/s] 60%|██████    | 647/1071 [00:14<00:09, 45.08it/s] 61%|██████    | 652/1071 [00:14<00:09, 45.15it/s] 61%|██████▏   | 657/1071 [00:14<00:09, 45.20it/s] 62%|██████▏   | 662/1071 [00:14<00:09, 45.16it/s] 62%|██████▏   | 667/1071 [00:14<00:09, 44.82it/s] 63%|██████▎   | 672/1071 [00:15<00:08, 44.70it/s] 63%|██████▎   | 677/1071 [00:15<00:09, 40.70it/s] 64%|██████▎   | 682/1071 [00:15<00:09, 42.08it/s] 64%|██████▍   | 687/1071 [00:15<00:08, 43.02it/s] 65%|██████▍   | 692/1071 [00:15<00:08, 43.77it/s] 65%|██████▌   | 697/1071 [00:15<00:08, 44.23it/s] 66%|██████▌   | 702/1071 [00:15<00:08, 44.57it/s] 66%|██████▌   | 707/1071 [00:15<00:08, 44.80it/s] 66%|██████▋   | 712/1071 [00:15<00:08, 44.64it/s] 67%|██████▋   | 717/1071 [00:16<00:08, 44.21it/s] 67%|██████▋   | 722/1071 [00:16<00:07, 44.26it/s] 68%|██████▊   | 727/1071 [00:16<00:07, 44.65it/s] 68%|██████▊   | 732/1071 [00:16<00:07, 44.79it/s] 69%|██████▉   | 737/1071 [00:16<00:07, 44.87it/s] 69%|██████▉   | 742/1071 [00:16<00:07, 44.95it/s] 70%|██████▉   | 747/1071 [00:16<00:07, 45.06it/s] 70%|███████   | 752/1071 [00:16<00:07, 45.12it/s] 71%|███████   | 757/1071 [00:16<00:07, 44.68it/s] 71%|███████   | 762/1071 [00:17<00:06, 44.38it/s] 72%|███████▏  | 767/1071 [00:17<00:06, 44.44it/s] 72%|███████▏  | 772/1071 [00:17<00:06, 44.46it/s] 73%|███████▎  | 777/1071 [00:17<00:06, 44.73it/s] 73%|███████▎  | 782/1071 [00:17<00:06, 44.87it/s] 73%|███████▎  | 787/1071 [00:17<00:06, 45.11it/s] 74%|███████▍  | 792/1071 [00:17<00:06, 45.08it/s] 74%|███████▍  | 797/1071 [00:17<00:06, 45.03it/s] 75%|███████▍  | 802/1071 [00:17<00:06, 44.80it/s] 75%|███████▌  | 807/1071 [00:18<00:05, 44.55it/s] 76%|███████▌  | 812/1071 [00:18<00:06, 41.57it/s] 76%|███████▋  | 817/1071 [00:18<00:05, 42.67it/s] 77%|███████▋  | 822/1071 [00:18<00:05, 43.33it/s] 77%|███████▋  | 827/1071 [00:18<00:05, 43.92it/s] 78%|███████▊  | 832/1071 [00:18<00:05, 44.11it/s] 78%|███████▊  | 837/1071 [00:18<00:05, 44.37it/s] 79%|███████▊  | 842/1071 [00:18<00:05, 44.56it/s] 79%|███████▉  | 847/1071 [00:19<00:05, 44.72it/s] 80%|███████▉  | 852/1071 [00:19<00:04, 44.34it/s] 80%|████████  | 857/1071 [00:19<00:04, 44.37it/s] 80%|████████  | 862/1071 [00:19<00:04, 44.58it/s] 81%|████████  | 867/1071 [00:19<00:04, 44.61it/s] 81%|████████▏ | 872/1071 [00:19<00:04, 44.99it/s] 82%|████████▏ | 877/1071 [00:19<00:04, 44.96it/s] 82%|████████▏ | 882/1071 [00:19<00:04, 45.03it/s] 83%|████████▎ | 887/1071 [00:19<00:04, 44.92it/s] 83%|████████▎ | 892/1071 [00:20<00:04, 44.73it/s] 84%|████████▍ | 897/1071 [00:20<00:03, 44.45it/s] 84%|████████▍ | 902/1071 [00:20<00:03, 44.50it/s] 85%|████████▍ | 907/1071 [00:20<00:03, 44.50it/s] 85%|████████▌ | 912/1071 [00:20<00:03, 44.69it/s] 86%|████████▌ | 917/1071 [00:20<00:03, 44.87it/s] 86%|████████▌ | 922/1071 [00:20<00:03, 44.83it/s] 87%|████████▋ | 927/1071 [00:20<00:03, 44.98it/s] 87%|████████▋ | 932/1071 [00:20<00:03, 44.83it/s] 87%|████████▋ | 937/1071 [00:21<00:02, 44.72it/s] 88%|████████▊ | 942/1071 [00:21<00:02, 44.57it/s] 88%|████████▊ | 947/1071 [00:21<00:02, 43.37it/s] 89%|████████▉ | 952/1071 [00:21<00:02, 43.96it/s] 89%|████████▉ | 957/1071 [00:21<00:02, 44.32it/s] 90%|████████▉ | 962/1071 [00:21<00:02, 44.58it/s] 90%|█████████ | 967/1071 [00:21<00:02, 44.50it/s] 91%|█████████ | 972/1071 [00:21<00:02, 44.59it/s] 91%|█████████ | 977/1071 [00:21<00:02, 44.71it/s] 92%|█████████▏| 982/1071 [00:22<00:01, 44.53it/s] 92%|█████████▏| 987/1071 [00:22<00:01, 44.46it/s] 93%|█████████▎| 992/1071 [00:22<00:01, 44.52it/s] 93%|█████████▎| 997/1071 [00:22<00:01, 44.59it/s] 94%|█████████▎| 1002/1071 [00:22<00:01, 44.78it/s] 94%|█████████▍| 1007/1071 [00:22<00:01, 44.99it/s] 94%|█████████▍| 1012/1071 [00:22<00:01, 44.84it/s] 95%|█████████▍| 1017/1071 [00:22<00:01, 44.75it/s] 95%|█████████▌| 1022/1071 [00:22<00:01, 44.77it/s] 96%|█████████▌| 1027/1071 [00:23<00:00, 44.72it/s] 96%|█████████▋| 1032/1071 [00:23<00:00, 44.74it/s] 97%|█████████▋| 1037/1071 [00:23<00:00, 44.67it/s] 97%|█████████▋| 1042/1071 [00:23<00:00, 44.72it/s] 98%|█████████▊| 1047/1071 [00:23<00:00, 44.85it/s] 98%|█████████▊| 1052/1071 [00:23<00:00, 44.95it/s] 99%|█████████▊| 1057/1071 [00:23<00:00, 45.12it/s] 99%|█████████▉| 1062/1071 [00:23<00:00, 45.08it/s]100%|█████████▉| 1067/1071 [00:23<00:00, 45.07it/s]100%|██████████| 1071/1071 [00:24<00:00, 44.53it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 09:11:16,676 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:11:16,676 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:11:16,676 >>   eval_loss               =       0.93
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:11:16,676 >>   eval_runtime            = 0:00:24.07
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:11:16,676 >>   eval_samples            =       8568
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:11:16,676 >>   eval_samples_per_second =    355.909
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:11:16,676 >>   eval_steps_per_second   =     44.489
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:11:16,676 >>   perplexity              =     2.5346
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:27,880 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:27,899 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:27,899 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:27,899 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:27,899 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:11:28,710 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:11:28,712 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:11:29,316 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:11:30,544 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:11:30,544 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:34,036 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:34,056 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:34,056 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:34,056 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:34,056 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:11:34,889 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:11:34,890 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:11:35,524 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:11:35,766 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:11:35,766 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-585
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/checkpoint-117
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl', 'labels': ['country', 'place of death', 'production company', 'screenwriter', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 21439
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21539, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.45it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.49it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:08,  1.56it/s]Extractor Predicting: 15it [00:09,  1.52it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.61it/s]Extractor Predicting: 19it [00:12,  1.61it/s]Extractor Predicting: 20it [00:12,  1.62it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:14,  1.68it/s]Extractor Predicting: 24it [00:15,  1.60it/s]Extractor Predicting: 25it [00:15,  1.52it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.55it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:18,  1.59it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:19,  1.55it/s]Extractor Predicting: 32it [00:20,  1.59it/s]Extractor Predicting: 33it [00:20,  1.61it/s]Extractor Predicting: 34it [00:21,  1.63it/s]Extractor Predicting: 35it [00:22,  1.69it/s]Extractor Predicting: 36it [00:22,  1.67it/s]Extractor Predicting: 37it [00:23,  1.73it/s]Extractor Predicting: 38it [00:23,  1.71it/s]Extractor Predicting: 39it [00:24,  1.69it/s]Extractor Predicting: 40it [00:25,  1.68it/s]Extractor Predicting: 41it [00:25,  1.69it/s]Extractor Predicting: 42it [00:26,  1.72it/s]Extractor Predicting: 43it [00:26,  1.71it/s]Extractor Predicting: 44it [00:27,  1.66it/s]Extractor Predicting: 45it [00:28,  1.69it/s]Extractor Predicting: 46it [00:28,  1.64it/s]Extractor Predicting: 47it [00:29,  1.63it/s]Extractor Predicting: 48it [00:29,  1.67it/s]Extractor Predicting: 49it [00:30,  1.68it/s]Extractor Predicting: 50it [00:30,  1.72it/s]Extractor Predicting: 51it [00:31,  1.73it/s]Extractor Predicting: 52it [00:32,  1.71it/s]Extractor Predicting: 53it [00:32,  1.57it/s]Extractor Predicting: 54it [00:33,  1.60it/s]Extractor Predicting: 55it [00:34,  1.64it/s]Extractor Predicting: 56it [00:34,  1.66it/s]Extractor Predicting: 57it [00:35,  1.69it/s]Extractor Predicting: 58it [00:35,  1.73it/s]Extractor Predicting: 59it [00:36,  1.65it/s]Extractor Predicting: 60it [00:37,  1.66it/s]Extractor Predicting: 61it [00:37,  1.65it/s]Extractor Predicting: 62it [00:38,  1.66it/s]Extractor Predicting: 63it [00:38,  1.64it/s]Extractor Predicting: 64it [00:39,  1.70it/s]Extractor Predicting: 65it [00:39,  1.74it/s]Extractor Predicting: 66it [00:40,  1.73it/s]Extractor Predicting: 67it [00:41,  1.72it/s]Extractor Predicting: 68it [00:41,  1.70it/s]Extractor Predicting: 69it [00:42,  1.70it/s]Extractor Predicting: 70it [00:42,  1.66it/s]Extractor Predicting: 71it [00:43,  1.70it/s]Extractor Predicting: 72it [00:44,  1.67it/s]Extractor Predicting: 73it [00:44,  1.65it/s]Extractor Predicting: 74it [00:45,  1.62it/s]Extractor Predicting: 75it [00:46,  1.61it/s]Extractor Predicting: 76it [00:46,  1.60it/s]Extractor Predicting: 77it [00:47,  1.57it/s]Extractor Predicting: 78it [00:47,  1.60it/s]Extractor Predicting: 79it [00:48,  1.55it/s]Extractor Predicting: 80it [00:49,  1.51it/s]Extractor Predicting: 81it [00:50,  1.49it/s]Extractor Predicting: 82it [00:50,  1.48it/s]Extractor Predicting: 83it [00:51,  1.48it/s]Extractor Predicting: 84it [00:52,  1.47it/s]Extractor Predicting: 85it [00:52,  1.46it/s]Extractor Predicting: 86it [00:53,  1.48it/s]Extractor Predicting: 87it [00:54,  1.48it/s]Extractor Predicting: 88it [00:54,  1.47it/s]Extractor Predicting: 89it [00:55,  1.49it/s]Extractor Predicting: 90it [00:56,  1.48it/s]Extractor Predicting: 91it [00:56,  1.47it/s]Extractor Predicting: 92it [00:57,  1.47it/s]Extractor Predicting: 93it [00:58,  1.47it/s]Extractor Predicting: 94it [00:58,  1.47it/s]Extractor Predicting: 95it [00:59,  1.46it/s]Extractor Predicting: 96it [01:00,  1.49it/s]Extractor Predicting: 97it [01:00,  1.51it/s]Extractor Predicting: 98it [01:01,  1.52it/s]Extractor Predicting: 99it [01:02,  1.53it/s]Extractor Predicting: 100it [01:02,  1.49it/s]Extractor Predicting: 101it [01:03,  1.51it/s]Extractor Predicting: 102it [01:04,  1.46it/s]Extractor Predicting: 103it [01:04,  1.49it/s]Extractor Predicting: 104it [01:05,  1.48it/s]Extractor Predicting: 105it [01:06,  1.46it/s]Extractor Predicting: 106it [01:06,  1.47it/s]Extractor Predicting: 107it [01:07,  1.48it/s]Extractor Predicting: 108it [01:08,  1.51it/s]Extractor Predicting: 109it [01:08,  1.50it/s]Extractor Predicting: 110it [01:09,  1.53it/s]Extractor Predicting: 111it [01:10,  1.53it/s]Extractor Predicting: 112it [01:10,  1.56it/s]Extractor Predicting: 113it [01:11,  1.55it/s]Extractor Predicting: 114it [01:12,  1.55it/s]Extractor Predicting: 115it [01:12,  1.48it/s]Extractor Predicting: 116it [01:13,  1.49it/s]Extractor Predicting: 117it [01:14,  1.50it/s]Extractor Predicting: 118it [01:14,  1.50it/s]Extractor Predicting: 119it [01:15,  1.49it/s]Extractor Predicting: 120it [01:16,  1.50it/s]Extractor Predicting: 121it [01:16,  1.52it/s]Extractor Predicting: 122it [01:17,  1.48it/s]Extractor Predicting: 123it [01:18,  1.51it/s]Extractor Predicting: 124it [01:18,  1.49it/s]Extractor Predicting: 125it [01:19,  1.50it/s]Extractor Predicting: 126it [01:20,  1.52it/s]Extractor Predicting: 127it [01:20,  1.58it/s]Extractor Predicting: 128it [01:21,  1.60it/s]Extractor Predicting: 129it [01:21,  1.62it/s]Extractor Predicting: 130it [01:22,  1.58it/s]Extractor Predicting: 131it [01:23,  1.58it/s]Extractor Predicting: 132it [01:23,  1.56it/s]Extractor Predicting: 133it [01:24,  1.56it/s]Extractor Predicting: 134it [01:25,  1.56it/s]Extractor Predicting: 135it [01:25,  1.59it/s]Extractor Predicting: 136it [01:26,  1.60it/s]Extractor Predicting: 137it [01:26,  1.62it/s]Extractor Predicting: 138it [01:27,  1.63it/s]Extractor Predicting: 139it [01:28,  1.64it/s]Extractor Predicting: 140it [01:28,  1.64it/s]Extractor Predicting: 141it [01:29,  1.63it/s]Extractor Predicting: 142it [01:30,  1.62it/s]Extractor Predicting: 143it [01:30,  1.58it/s]Extractor Predicting: 144it [01:31,  1.58it/s]Extractor Predicting: 145it [01:31,  1.63it/s]Extractor Predicting: 146it [01:32,  1.64it/s]Extractor Predicting: 147it [01:33,  1.46it/s]Extractor Predicting: 148it [01:33,  1.52it/s]Extractor Predicting: 149it [01:34,  1.53it/s]Extractor Predicting: 150it [01:35,  1.56it/s]Extractor Predicting: 151it [01:35,  1.54it/s]Extractor Predicting: 152it [01:36,  1.55it/s]Extractor Predicting: 153it [01:37,  1.56it/s]Extractor Predicting: 154it [01:37,  1.52it/s]Extractor Predicting: 155it [01:38,  1.53it/s]Extractor Predicting: 156it [01:39,  1.53it/s]Extractor Predicting: 157it [01:39,  1.55it/s]Extractor Predicting: 158it [01:40,  1.53it/s]Extractor Predicting: 159it [01:41,  1.56it/s]Extractor Predicting: 160it [01:41,  1.57it/s]Extractor Predicting: 161it [01:42,  1.55it/s]Extractor Predicting: 162it [01:42,  1.59it/s]Extractor Predicting: 163it [01:43,  1.73it/s]Extractor Predicting: 164it [01:43,  1.84it/s]Extractor Predicting: 165it [01:44,  1.86it/s]Extractor Predicting: 166it [01:44,  1.82it/s]Extractor Predicting: 167it [01:45,  1.72it/s]Extractor Predicting: 168it [01:46,  1.65it/s]Extractor Predicting: 169it [01:46,  1.61it/s]Extractor Predicting: 170it [01:47,  1.59it/s]Extractor Predicting: 171it [01:48,  1.60it/s]Extractor Predicting: 172it [01:48,  1.59it/s]Extractor Predicting: 173it [01:49,  1.59it/s]Extractor Predicting: 174it [01:50,  1.59it/s]Extractor Predicting: 175it [01:50,  1.56it/s]Extractor Predicting: 176it [01:51,  1.56it/s]Extractor Predicting: 177it [01:52,  1.57it/s]Extractor Predicting: 178it [01:52,  1.59it/s]Extractor Predicting: 179it [01:53,  1.55it/s]Extractor Predicting: 180it [01:53,  1.55it/s]Extractor Predicting: 181it [01:54,  1.58it/s]Extractor Predicting: 182it [01:55,  1.59it/s]Extractor Predicting: 183it [01:55,  1.56it/s]Extractor Predicting: 184it [01:56,  1.55it/s]Extractor Predicting: 185it [01:57,  1.52it/s]Extractor Predicting: 186it [01:57,  1.54it/s]Extractor Predicting: 187it [01:58,  1.56it/s]Extractor Predicting: 188it [01:59,  1.56it/s]Extractor Predicting: 189it [01:59,  1.56it/s]Extractor Predicting: 190it [02:00,  1.54it/s]Extractor Predicting: 191it [02:01,  1.57it/s]Extractor Predicting: 192it [02:01,  1.60it/s]Extractor Predicting: 193it [02:02,  1.60it/s]Extractor Predicting: 194it [02:02,  1.61it/s]Extractor Predicting: 195it [02:03,  1.55it/s]Extractor Predicting: 196it [02:04,  1.57it/s]Extractor Predicting: 197it [02:04,  1.57it/s]Extractor Predicting: 198it [02:05,  1.57it/s]Extractor Predicting: 199it [02:06,  1.56it/s]Extractor Predicting: 200it [02:06,  1.53it/s]Extractor Predicting: 201it [02:07,  1.51it/s]Extractor Predicting: 202it [02:08,  1.53it/s]Extractor Predicting: 203it [02:08,  1.54it/s]Extractor Predicting: 204it [02:09,  1.54it/s]Extractor Predicting: 205it [02:10,  1.53it/s]Extractor Predicting: 206it [02:10,  1.53it/s]Extractor Predicting: 207it [02:11,  1.58it/s]Extractor Predicting: 208it [02:11,  1.61it/s]Extractor Predicting: 209it [02:12,  1.58it/s]Extractor Predicting: 210it [02:13,  1.58it/s]Extractor Predicting: 211it [02:13,  1.58it/s]Extractor Predicting: 212it [02:14,  1.56it/s]Extractor Predicting: 213it [02:15,  1.53it/s]Extractor Predicting: 214it [02:15,  1.57it/s]Extractor Predicting: 215it [02:16,  1.59it/s]Extractor Predicting: 216it [02:17,  1.58it/s]Extractor Predicting: 217it [02:17,  1.56it/s]Extractor Predicting: 218it [02:18,  1.54it/s]Extractor Predicting: 219it [02:18,  1.56it/s]Extractor Predicting: 220it [02:19,  1.56it/s]Extractor Predicting: 221it [02:20,  1.59it/s]Extractor Predicting: 222it [02:20,  1.58it/s]Extractor Predicting: 223it [02:21,  1.58it/s]Extractor Predicting: 224it [02:22,  1.61it/s]Extractor Predicting: 225it [02:22,  1.65it/s]Extractor Predicting: 226it [02:23,  1.62it/s]Extractor Predicting: 227it [02:23,  1.61it/s]Extractor Predicting: 228it [02:24,  1.61it/s]Extractor Predicting: 229it [02:25,  1.53it/s]Extractor Predicting: 230it [02:25,  1.59it/s]Extractor Predicting: 231it [02:26,  1.58it/s]Extractor Predicting: 232it [02:27,  1.57it/s]Extractor Predicting: 233it [02:27,  1.55it/s]Extractor Predicting: 234it [02:28,  1.52it/s]Extractor Predicting: 235it [02:29,  1.53it/s]Extractor Predicting: 236it [02:29,  1.49it/s]Extractor Predicting: 237it [02:30,  1.46it/s]Extractor Predicting: 238it [02:31,  1.30it/s]Extractor Predicting: 239it [02:32,  1.31it/s]Extractor Predicting: 240it [02:32,  1.34it/s]Extractor Predicting: 241it [02:33,  1.40it/s]Extractor Predicting: 242it [02:34,  1.40it/s]Extractor Predicting: 243it [02:34,  1.45it/s]Extractor Predicting: 244it [02:35,  1.47it/s]Extractor Predicting: 245it [02:36,  1.52it/s]Extractor Predicting: 246it [02:36,  1.54it/s]Extractor Predicting: 247it [02:37,  1.56it/s]Extractor Predicting: 248it [02:38,  1.59it/s]Extractor Predicting: 249it [02:38,  1.62it/s]Extractor Predicting: 250it [02:39,  1.58it/s]Extractor Predicting: 251it [02:39,  1.62it/s]Extractor Predicting: 252it [02:40,  1.60it/s]Extractor Predicting: 253it [02:41,  1.60it/s]Extractor Predicting: 254it [02:41,  1.61it/s]Extractor Predicting: 255it [02:42,  1.60it/s]Extractor Predicting: 256it [02:43,  1.59it/s]Extractor Predicting: 257it [02:43,  1.60it/s]Extractor Predicting: 258it [02:44,  1.62it/s]Extractor Predicting: 259it [02:44,  1.60it/s]Extractor Predicting: 260it [02:45,  1.59it/s]Extractor Predicting: 261it [02:46,  1.58it/s]Extractor Predicting: 262it [02:46,  1.57it/s]Extractor Predicting: 263it [02:47,  1.58it/s]Extractor Predicting: 264it [02:48,  1.59it/s]Extractor Predicting: 265it [02:48,  1.58it/s]Extractor Predicting: 266it [02:49,  1.57it/s]Extractor Predicting: 267it [02:50,  1.57it/s]Extractor Predicting: 268it [02:50,  1.57it/s]Extractor Predicting: 269it [02:51,  1.55it/s]Extractor Predicting: 270it [02:51,  1.58it/s]Extractor Predicting: 271it [02:52,  1.60it/s]Extractor Predicting: 272it [02:53,  1.66it/s]Extractor Predicting: 273it [02:53,  1.64it/s]Extractor Predicting: 274it [02:54,  1.65it/s]Extractor Predicting: 275it [02:54,  1.61it/s]Extractor Predicting: 276it [02:55,  1.61it/s]Extractor Predicting: 277it [02:56,  1.62it/s]Extractor Predicting: 278it [02:56,  1.56it/s]Extractor Predicting: 279it [02:57,  1.59it/s]Extractor Predicting: 280it [02:58,  1.65it/s]Extractor Predicting: 281it [02:58,  1.64it/s]Extractor Predicting: 282it [02:59,  1.61it/s]Extractor Predicting: 283it [02:59,  1.57it/s]Extractor Predicting: 284it [03:00,  1.56it/s]Extractor Predicting: 285it [03:01,  1.53it/s]Extractor Predicting: 286it [03:01,  1.56it/s]Extractor Predicting: 287it [03:02,  1.55it/s]Extractor Predicting: 288it [03:03,  1.52it/s]Extractor Predicting: 289it [03:03,  1.52it/s]Extractor Predicting: 290it [03:04,  1.53it/s]Extractor Predicting: 291it [03:05,  1.56it/s]Extractor Predicting: 292it [03:05,  1.59it/s]Extractor Predicting: 293it [03:06,  1.55it/s]Extractor Predicting: 294it [03:07,  1.51it/s]Extractor Predicting: 295it [03:07,  1.52it/s]Extractor Predicting: 296it [03:08,  1.51it/s]Extractor Predicting: 297it [03:09,  1.48it/s]Extractor Predicting: 298it [03:09,  1.44it/s]Extractor Predicting: 299it [03:10,  1.44it/s]Extractor Predicting: 300it [03:11,  1.46it/s]Extractor Predicting: 301it [03:12,  1.44it/s]Extractor Predicting: 302it [03:12,  1.44it/s]Extractor Predicting: 303it [03:13,  1.40it/s]Extractor Predicting: 303it [03:13,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:05,849 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:05,852 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:05,852 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:05,852 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:05,852 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:15:06,458 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:15:06,480 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:15:07,083 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:15:08,149 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:15:08,149 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:11,146 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:11,167 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:11,167 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:11,167 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:11,167 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:15:11,817 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:15:11,818 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:15:12,397 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:15:12,553 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:15:12,553 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.29103535353535354,
  "recall": 0.053804855275443514,
  "score": 0.09081954294720254,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'labels': ['continent', 'field of this occupation', 'field of work', 'founded by', 'movement', 'owned by', 'performer', 'producer', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20815
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20915, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.58it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:10,  1.61it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:14,  1.54it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:17,  1.57it/s]Extractor Predicting: 29it [00:18,  1.54it/s]Extractor Predicting: 30it [00:19,  1.57it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:20,  1.57it/s]Extractor Predicting: 33it [00:21,  1.57it/s]Extractor Predicting: 34it [00:21,  1.56it/s]Extractor Predicting: 35it [00:22,  1.52it/s]Extractor Predicting: 36it [00:22,  1.57it/s]Extractor Predicting: 37it [00:23,  1.56it/s]Extractor Predicting: 38it [00:24,  1.57it/s]Extractor Predicting: 39it [00:24,  1.60it/s]Extractor Predicting: 40it [00:25,  1.58it/s]Extractor Predicting: 41it [00:26,  1.58it/s]Extractor Predicting: 42it [00:26,  1.60it/s]Extractor Predicting: 43it [00:27,  1.57it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:28,  1.52it/s]Extractor Predicting: 46it [00:29,  1.51it/s]Extractor Predicting: 47it [00:30,  1.53it/s]Extractor Predicting: 48it [00:30,  1.54it/s]Extractor Predicting: 49it [00:31,  1.55it/s]Extractor Predicting: 50it [00:31,  1.58it/s]Extractor Predicting: 51it [00:32,  1.60it/s]Extractor Predicting: 52it [00:33,  1.61it/s]Extractor Predicting: 53it [00:33,  1.62it/s]Extractor Predicting: 54it [00:34,  1.58it/s]Extractor Predicting: 55it [00:35,  1.60it/s]Extractor Predicting: 56it [00:35,  1.60it/s]Extractor Predicting: 57it [00:36,  1.62it/s]Extractor Predicting: 58it [00:36,  1.59it/s]Extractor Predicting: 59it [00:37,  1.60it/s]Extractor Predicting: 60it [00:38,  1.59it/s]Extractor Predicting: 61it [00:38,  1.59it/s]Extractor Predicting: 62it [00:39,  1.60it/s]Extractor Predicting: 63it [00:40,  1.62it/s]Extractor Predicting: 64it [00:40,  1.64it/s]Extractor Predicting: 65it [00:41,  1.50it/s]Extractor Predicting: 66it [00:42,  1.54it/s]Extractor Predicting: 67it [00:42,  1.58it/s]Extractor Predicting: 68it [00:43,  1.56it/s]Extractor Predicting: 69it [00:43,  1.57it/s]Extractor Predicting: 70it [00:44,  1.57it/s]Extractor Predicting: 71it [00:45,  1.56it/s]Extractor Predicting: 72it [00:45,  1.60it/s]Extractor Predicting: 73it [00:46,  1.60it/s]Extractor Predicting: 74it [00:47,  1.59it/s]Extractor Predicting: 75it [00:47,  1.56it/s]Extractor Predicting: 76it [00:48,  1.60it/s]Extractor Predicting: 77it [00:48,  1.60it/s]Extractor Predicting: 78it [00:49,  1.60it/s]Extractor Predicting: 79it [00:50,  1.60it/s]Extractor Predicting: 80it [00:50,  1.60it/s]Extractor Predicting: 81it [00:51,  1.61it/s]Extractor Predicting: 82it [00:52,  1.62it/s]Extractor Predicting: 83it [00:52,  1.61it/s]Extractor Predicting: 84it [00:53,  1.60it/s]Extractor Predicting: 85it [00:53,  1.59it/s]Extractor Predicting: 86it [00:54,  1.61it/s]Extractor Predicting: 87it [00:55,  1.58it/s]Extractor Predicting: 88it [00:55,  1.55it/s]Extractor Predicting: 89it [00:56,  1.56it/s]Extractor Predicting: 90it [00:57,  1.57it/s]Extractor Predicting: 91it [00:57,  1.58it/s]Extractor Predicting: 92it [00:58,  1.60it/s]Extractor Predicting: 93it [00:59,  1.57it/s]Extractor Predicting: 94it [00:59,  1.58it/s]Extractor Predicting: 95it [01:00,  1.43it/s]Extractor Predicting: 96it [01:01,  1.47it/s]Extractor Predicting: 97it [01:01,  1.47it/s]Extractor Predicting: 98it [01:02,  1.52it/s]Extractor Predicting: 99it [01:03,  1.53it/s]Extractor Predicting: 100it [01:03,  1.51it/s]Extractor Predicting: 101it [01:04,  1.54it/s]Extractor Predicting: 102it [01:05,  1.54it/s]Extractor Predicting: 103it [01:05,  1.52it/s]Extractor Predicting: 104it [01:06,  1.53it/s]Extractor Predicting: 105it [01:07,  1.53it/s]Extractor Predicting: 106it [01:07,  1.53it/s]Extractor Predicting: 107it [01:08,  1.56it/s]Extractor Predicting: 108it [01:08,  1.53it/s]Extractor Predicting: 109it [01:09,  1.52it/s]Extractor Predicting: 110it [01:10,  1.55it/s]Extractor Predicting: 111it [01:10,  1.59it/s]Extractor Predicting: 112it [01:11,  1.60it/s]Extractor Predicting: 113it [01:12,  1.60it/s]Extractor Predicting: 114it [01:12,  1.58it/s]Extractor Predicting: 115it [01:13,  1.59it/s]Extractor Predicting: 116it [01:13,  1.61it/s]Extractor Predicting: 117it [01:14,  1.59it/s]Extractor Predicting: 118it [01:15,  1.57it/s]Extractor Predicting: 119it [01:15,  1.56it/s]Extractor Predicting: 120it [01:16,  1.57it/s]Extractor Predicting: 121it [01:17,  1.59it/s]Extractor Predicting: 122it [01:17,  1.58it/s]Extractor Predicting: 123it [01:18,  1.59it/s]Extractor Predicting: 124it [01:19,  1.59it/s]Extractor Predicting: 125it [01:19,  1.61it/s]Extractor Predicting: 126it [01:20,  1.59it/s]Extractor Predicting: 127it [01:20,  1.58it/s]Extractor Predicting: 128it [01:21,  1.59it/s]Extractor Predicting: 129it [01:22,  1.64it/s]Extractor Predicting: 130it [01:22,  1.66it/s]Extractor Predicting: 131it [01:23,  1.64it/s]Extractor Predicting: 132it [01:24,  1.58it/s]Extractor Predicting: 133it [01:24,  1.53it/s]Extractor Predicting: 134it [01:25,  1.52it/s]Extractor Predicting: 135it [01:25,  1.55it/s]Extractor Predicting: 136it [01:26,  1.54it/s]Extractor Predicting: 137it [01:27,  1.51it/s]Extractor Predicting: 138it [01:27,  1.52it/s]Extractor Predicting: 139it [01:28,  1.52it/s]Extractor Predicting: 140it [01:29,  1.53it/s]Extractor Predicting: 141it [01:29,  1.52it/s]Extractor Predicting: 142it [01:30,  1.51it/s]Extractor Predicting: 143it [01:31,  1.52it/s]Extractor Predicting: 144it [01:31,  1.53it/s]Extractor Predicting: 145it [01:32,  1.54it/s]Extractor Predicting: 146it [01:33,  1.55it/s]Extractor Predicting: 147it [01:33,  1.52it/s]Extractor Predicting: 148it [01:34,  1.56it/s]Extractor Predicting: 149it [01:35,  1.56it/s]Extractor Predicting: 150it [01:35,  1.56it/s]Extractor Predicting: 151it [01:36,  1.55it/s]Extractor Predicting: 152it [01:37,  1.58it/s]Extractor Predicting: 153it [01:37,  1.61it/s]Extractor Predicting: 154it [01:38,  1.60it/s]Extractor Predicting: 155it [01:38,  1.60it/s]Extractor Predicting: 156it [01:39,  1.60it/s]Extractor Predicting: 157it [01:40,  1.60it/s]Extractor Predicting: 158it [01:40,  1.58it/s]Extractor Predicting: 159it [01:41,  1.58it/s]Extractor Predicting: 160it [01:42,  1.60it/s]Extractor Predicting: 161it [01:42,  1.59it/s]Extractor Predicting: 162it [01:43,  1.57it/s]Extractor Predicting: 163it [01:43,  1.59it/s]Extractor Predicting: 164it [01:44,  1.58it/s]Extractor Predicting: 165it [01:45,  1.62it/s]Extractor Predicting: 166it [01:45,  1.64it/s]Extractor Predicting: 167it [01:46,  1.60it/s]Extractor Predicting: 168it [01:47,  1.61it/s]Extractor Predicting: 169it [01:47,  1.62it/s]Extractor Predicting: 170it [01:48,  1.58it/s]Extractor Predicting: 171it [01:48,  1.56it/s]Extractor Predicting: 172it [01:49,  1.55it/s]Extractor Predicting: 173it [01:50,  1.57it/s]Extractor Predicting: 174it [01:50,  1.60it/s]Extractor Predicting: 175it [01:51,  1.63it/s]Extractor Predicting: 176it [01:52,  1.58it/s]Extractor Predicting: 177it [01:52,  1.60it/s]Extractor Predicting: 178it [01:53,  1.57it/s]Extractor Predicting: 179it [01:53,  1.56it/s]Extractor Predicting: 180it [01:54,  1.42it/s]Extractor Predicting: 181it [01:55,  1.49it/s]Extractor Predicting: 182it [01:56,  1.50it/s]Extractor Predicting: 183it [01:56,  1.55it/s]Extractor Predicting: 184it [01:57,  1.59it/s]Extractor Predicting: 185it [01:57,  1.58it/s]Extractor Predicting: 186it [01:58,  1.61it/s]Extractor Predicting: 187it [01:59,  1.61it/s]Extractor Predicting: 188it [01:59,  1.58it/s]Extractor Predicting: 189it [02:00,  1.55it/s]Extractor Predicting: 190it [02:01,  1.52it/s]Extractor Predicting: 191it [02:01,  1.55it/s]Extractor Predicting: 192it [02:02,  1.58it/s]Extractor Predicting: 193it [02:03,  1.57it/s]Extractor Predicting: 194it [02:03,  1.56it/s]Extractor Predicting: 195it [02:04,  1.58it/s]Extractor Predicting: 196it [02:04,  1.62it/s]Extractor Predicting: 197it [02:05,  1.60it/s]Extractor Predicting: 198it [02:06,  1.63it/s]Extractor Predicting: 199it [02:06,  1.62it/s]Extractor Predicting: 200it [02:07,  1.57it/s]Extractor Predicting: 201it [02:08,  1.58it/s]Extractor Predicting: 202it [02:08,  1.58it/s]Extractor Predicting: 203it [02:09,  1.59it/s]Extractor Predicting: 204it [02:09,  1.61it/s]Extractor Predicting: 205it [02:10,  1.62it/s]Extractor Predicting: 206it [02:11,  1.62it/s]Extractor Predicting: 207it [02:11,  1.63it/s]Extractor Predicting: 208it [02:12,  1.66it/s]Extractor Predicting: 209it [02:12,  1.66it/s]Extractor Predicting: 210it [02:13,  1.62it/s]Extractor Predicting: 211it [02:14,  1.62it/s]Extractor Predicting: 212it [02:14,  1.61it/s]Extractor Predicting: 213it [02:15,  1.60it/s]Extractor Predicting: 214it [02:16,  1.60it/s]Extractor Predicting: 215it [02:16,  1.57it/s]Extractor Predicting: 216it [02:17,  1.57it/s]Extractor Predicting: 217it [02:18,  1.56it/s]Extractor Predicting: 218it [02:18,  1.62it/s]Extractor Predicting: 219it [02:19,  1.63it/s]Extractor Predicting: 220it [02:19,  1.59it/s]Extractor Predicting: 221it [02:20,  1.56it/s]Extractor Predicting: 222it [02:21,  1.59it/s]Extractor Predicting: 223it [02:21,  1.60it/s]Extractor Predicting: 224it [02:22,  1.60it/s]Extractor Predicting: 225it [02:22,  1.63it/s]Extractor Predicting: 226it [02:23,  1.63it/s]Extractor Predicting: 227it [02:24,  1.63it/s]Extractor Predicting: 228it [02:24,  1.61it/s]Extractor Predicting: 229it [02:25,  1.61it/s]Extractor Predicting: 230it [02:26,  1.59it/s]Extractor Predicting: 231it [02:26,  1.62it/s]Extractor Predicting: 232it [02:27,  1.60it/s]Extractor Predicting: 233it [02:27,  1.60it/s]Extractor Predicting: 234it [02:28,  1.57it/s]Extractor Predicting: 235it [02:29,  1.59it/s]Extractor Predicting: 236it [02:29,  1.57it/s]Extractor Predicting: 237it [02:30,  1.59it/s]Extractor Predicting: 238it [02:31,  1.58it/s]Extractor Predicting: 239it [02:31,  1.62it/s]Extractor Predicting: 240it [02:32,  1.61it/s]Extractor Predicting: 241it [02:32,  1.59it/s]Extractor Predicting: 242it [02:33,  1.60it/s]Extractor Predicting: 243it [02:34,  1.64it/s]Extractor Predicting: 244it [02:34,  1.63it/s]Extractor Predicting: 245it [02:35,  1.66it/s]Extractor Predicting: 246it [02:35,  1.71it/s]Extractor Predicting: 247it [02:36,  1.68it/s]Extractor Predicting: 248it [02:37,  1.69it/s]Extractor Predicting: 249it [02:37,  1.68it/s]Extractor Predicting: 250it [02:38,  1.67it/s]Extractor Predicting: 251it [02:38,  1.67it/s]Extractor Predicting: 252it [02:39,  1.69it/s]Extractor Predicting: 253it [02:40,  1.69it/s]Extractor Predicting: 254it [02:40,  1.70it/s]Extractor Predicting: 255it [02:41,  1.73it/s]Extractor Predicting: 256it [02:41,  1.67it/s]Extractor Predicting: 257it [02:42,  1.68it/s]Extractor Predicting: 258it [02:43,  1.68it/s]Extractor Predicting: 259it [02:43,  1.68it/s]Extractor Predicting: 260it [02:44,  1.67it/s]Extractor Predicting: 261it [02:44,  1.64it/s]Extractor Predicting: 262it [02:45,  1.67it/s]Extractor Predicting: 263it [02:46,  1.72it/s]Extractor Predicting: 264it [02:46,  1.65it/s]Extractor Predicting: 265it [02:47,  1.62it/s]Extractor Predicting: 266it [02:47,  1.59it/s]Extractor Predicting: 267it [02:48,  1.59it/s]Extractor Predicting: 268it [02:49,  1.61it/s]Extractor Predicting: 269it [02:49,  1.64it/s]Extractor Predicting: 270it [02:50,  1.64it/s]Extractor Predicting: 271it [02:51,  1.62it/s]Extractor Predicting: 272it [02:51,  1.62it/s]Extractor Predicting: 273it [02:52,  1.66it/s]Extractor Predicting: 274it [02:52,  1.66it/s]Extractor Predicting: 275it [02:53,  1.69it/s]Extractor Predicting: 276it [02:54,  1.66it/s]Extractor Predicting: 277it [02:54,  1.60it/s]Extractor Predicting: 278it [02:55,  1.63it/s]Extractor Predicting: 279it [02:55,  1.66it/s]Extractor Predicting: 280it [02:56,  1.69it/s]Extractor Predicting: 281it [02:57,  1.51it/s]Extractor Predicting: 282it [02:57,  1.57it/s]Extractor Predicting: 283it [02:58,  1.60it/s]Extractor Predicting: 284it [02:59,  1.60it/s]Extractor Predicting: 285it [02:59,  1.57it/s]Extractor Predicting: 286it [03:00,  1.64it/s]Extractor Predicting: 287it [03:00,  1.63it/s]Extractor Predicting: 288it [03:01,  1.63it/s]Extractor Predicting: 289it [03:02,  1.62it/s]Extractor Predicting: 290it [03:02,  1.64it/s]Extractor Predicting: 291it [03:03,  1.64it/s]Extractor Predicting: 292it [03:03,  1.60it/s]Extractor Predicting: 293it [03:04,  1.60it/s]Extractor Predicting: 294it [03:05,  1.60it/s]Extractor Predicting: 295it [03:05,  1.60it/s]Extractor Predicting: 296it [03:06,  1.63it/s]Extractor Predicting: 297it [03:07,  1.60it/s]Extractor Predicting: 298it [03:07,  1.65it/s]Extractor Predicting: 299it [03:08,  1.58it/s]Extractor Predicting: 300it [03:09,  1.56it/s]Extractor Predicting: 301it [03:09,  1.50it/s]Extractor Predicting: 302it [03:10,  1.53it/s]Extractor Predicting: 303it [03:11,  1.52it/s]Extractor Predicting: 304it [03:11,  1.51it/s]Extractor Predicting: 305it [03:12,  1.50it/s]Extractor Predicting: 306it [03:13,  1.49it/s]Extractor Predicting: 306it [03:13,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:34,954 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:34,973 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:34,973 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:34,973 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:34,973 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:18:35,699 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:18:35,700 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:18:36,368 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:18:37,431 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:18:37,431 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:39,233 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:39,235 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:39,236 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:39,236 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:39,236 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:18:39,997 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:18:39,998 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:18:40,267 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:18:40,438 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:18:40,438 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.32704918032786884,
  "recall": 0.054330065359477125,
  "score": 0.09318075665576833,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'labels': ['continent', 'field of this occupation', 'field of work', 'founded by', 'movement', 'owned by', 'performer', 'producer', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6322
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6422, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.47it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.52it/s]Extractor Predicting: 13it [00:08,  1.54it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.49it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:11,  1.50it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:13,  1.52it/s]Extractor Predicting: 21it [00:13,  1.49it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:15,  1.56it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:16,  1.60it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:17,  1.60it/s]Extractor Predicting: 28it [00:18,  1.62it/s]Extractor Predicting: 29it [00:18,  1.59it/s]Extractor Predicting: 30it [00:19,  1.61it/s]Extractor Predicting: 31it [00:20,  1.59it/s]Extractor Predicting: 32it [00:20,  1.61it/s]Extractor Predicting: 33it [00:21,  1.60it/s]Extractor Predicting: 34it [00:21,  1.62it/s]Extractor Predicting: 35it [00:22,  1.62it/s]Extractor Predicting: 36it [00:23,  1.64it/s]Extractor Predicting: 37it [00:23,  1.64it/s]Extractor Predicting: 38it [00:24,  1.66it/s]Extractor Predicting: 39it [00:24,  1.63it/s]Extractor Predicting: 40it [00:25,  1.61it/s]Extractor Predicting: 41it [00:26,  1.59it/s]Extractor Predicting: 42it [00:26,  1.58it/s]Extractor Predicting: 43it [00:27,  1.55it/s]Extractor Predicting: 44it [00:28,  1.56it/s]Extractor Predicting: 45it [00:28,  1.57it/s]Extractor Predicting: 46it [00:29,  1.52it/s]Extractor Predicting: 47it [00:30,  1.54it/s]Extractor Predicting: 48it [00:30,  1.56it/s]Extractor Predicting: 49it [00:31,  1.56it/s]Extractor Predicting: 50it [00:32,  1.45it/s]Extractor Predicting: 51it [00:32,  1.49it/s]Extractor Predicting: 52it [00:33,  1.49it/s]Extractor Predicting: 53it [00:34,  1.52it/s]Extractor Predicting: 54it [00:34,  1.51it/s]Extractor Predicting: 55it [00:35,  1.52it/s]Extractor Predicting: 56it [00:36,  1.53it/s]Extractor Predicting: 57it [00:36,  1.50it/s]Extractor Predicting: 58it [00:37,  1.50it/s]Extractor Predicting: 59it [00:38,  1.48it/s]Extractor Predicting: 60it [00:38,  1.83it/s]Extractor Predicting: 60it [00:38,  1.56it/s]
[INFO|configuration_utils.py:515] 2023-08-29 09:19:20,444 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:19:20,445 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 09:19:20,481 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:19:20,482 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 09:19:20,502 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 09:19:28,752 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 09:19:28,764 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 09:19:28,832 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:19:28,833 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 09:19:28,894 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:19:28,930 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:19:28,930 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:19:28,930 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:19:28,930 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:19:28,930 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:19:28,930 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.45410628019323673,
  "recall": 0.029292614521657837,
  "score": 0.05503512880562061,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 09:19:29,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:29,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:30,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:31,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:31,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:32,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:33,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:33,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:34,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:35,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:35,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:36,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:37,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:37,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:38,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:39,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:39,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:40,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:41,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:41,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:42,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:43,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:24, 14.64s/it][WARNING|generation_utils.py:914] 2023-08-29 09:19:43,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:44,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:45,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:45,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:46,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:47,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:47,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:48,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:49,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:49,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:50,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:51,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:51,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:52,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:53,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:54,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:54,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:55,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:56,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:56,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:57,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:28<03:07, 14.45s/it][WARNING|generation_utils.py:914] 2023-08-29 09:19:58,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:58,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:59,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:19:59,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:00,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:01,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:01,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:02,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:02,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:03,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:04,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:05,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:05,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:06,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:07,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:07,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:08,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:09,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:09,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:10,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:11,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:42<02:48, 14.05s/it][WARNING|generation_utils.py:914] 2023-08-29 09:20:11,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:12,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:12,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:13,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:14,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:14,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:15,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:16,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:16,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:17,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:18,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:18,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:19,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:20,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:20,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:21,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:21,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:22,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:23,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:23,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:24,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:55<02:31, 13.82s/it][WARNING|generation_utils.py:914] 2023-08-29 09:20:25,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:25,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:26,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:27,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:27,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:28,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:29,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:29,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:30,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:31,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:31,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:32,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:32,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:33,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:34,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:35,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:35,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:36,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:36,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:37,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:38,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:09<02:17, 13.74s/it][WARNING|generation_utils.py:914] 2023-08-29 09:20:38,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:39,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:40,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:40,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:41,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:42,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:42,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:43,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:44,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:44,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:45,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:45,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:46,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:47,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:47,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:48,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:49,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:49,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:50,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:50,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:51,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:52,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:52,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:24<02:06, 14.03s/it][WARNING|generation_utils.py:914] 2023-08-29 09:20:53,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:53,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:54,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:55,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:55,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:56,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:57,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:57,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:58,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:59,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:59,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:00,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:00,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:01,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:02,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:02,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:03,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:04,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:04,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:05,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:36<01:48, 13.62s/it][WARNING|generation_utils.py:914] 2023-08-29 09:21:06,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:06,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:07,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:08,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:09,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:09,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:10,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:11,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:11,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:12,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:13,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:13,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:14,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:15,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:16,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:16,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:17,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:17,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:18,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:19,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:20,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:51<01:37, 13.98s/it][WARNING|generation_utils.py:914] 2023-08-29 09:21:20,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:21,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:22,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:22,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:23,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:24,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:25,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:25,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:26,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:26,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:27,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:28,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:28,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:29,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:30,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:30,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:31,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:31,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:32,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:33,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:34,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:05<01:23, 13.93s/it][WARNING|generation_utils.py:914] 2023-08-29 09:21:34,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:35,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:36,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:36,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:37,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:38,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:39,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:39,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:40,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:41,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:41,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:42,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:42,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:43,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:44,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:44,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:45,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:46,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:46,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:47,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:48,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:49,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:20<01:11, 14.35s/it][WARNING|generation_utils.py:914] 2023-08-29 09:21:50,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:50,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:51,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:51,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:52,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:53,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:53,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:54,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:54,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:55,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:56,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:56,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:57,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:57,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:58,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:59,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:59,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:00,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:01,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:01,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:02,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:33<00:55, 13.89s/it][WARNING|generation_utils.py:914] 2023-08-29 09:22:02,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:03,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:04,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:04,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:05,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:05,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:06,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:07,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:07,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:08,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:09,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:09,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:10,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:11,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:11,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:12,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:13,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:13,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:14,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:15,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:46<00:40, 13.58s/it][WARNING|generation_utils.py:914] 2023-08-29 09:22:15,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:16,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:17,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:17,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:18,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:19,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:19,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:20,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:21,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:21,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:22,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:23,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:23,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:24,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:25,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:25,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:26,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:26,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:27,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:28,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:59<00:26, 13.49s/it][WARNING|generation_utils.py:914] 2023-08-29 09:22:29,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:29,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:30,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:30,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:31,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:32,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:32,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:33,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:34,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:35,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:35,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:36,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:37,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:38,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:38,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:39,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:39,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:40,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:41,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:42,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:42,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:14<00:13, 13.76s/it][WARNING|generation_utils.py:914] 2023-08-29 09:22:43,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:44,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:45,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:45,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:46,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:47,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:47,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:48,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:49,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:49,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:50,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:51,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:51,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:52,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:53,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:53,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:54,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:55,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:56,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:56,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:57,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:58,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:29<00:00, 14.24s/it]Generating: 100%|██████████| 15/15 [03:29<00:00, 13.97s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:23:09,029 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:23:09,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:23:09,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:23:09,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:23:09,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:23:09,776 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:23:09,777 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:23:10,384 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:23:11,515 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:23:11,515 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:23:14,534 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:23:14,557 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:23:14,557 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:23:14,557 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:23:14,557 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:23:15,502 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:23:15,503 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:23:16,210 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:23:16,433 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:23:16,433 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : country .', 'success_rate': 0.8892045454545454, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : place of death .', 'success_rate': 0.9166666666666666, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : production company .', 'success_rate': 0.9285714285714286, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.9166666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9136904761904762, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : continent . Context : The Cretaceous ( κ Cr ) , a marine age of the Jurassic at Spitsbergen , northern Germany , is now estimated to be about 0 . 14 million years old . Head Entity : The Jurassic , Tail Entity : Cretaceous .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : continent .', 'success_rate': 0.8410326086956522, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.9375, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9345238095238095, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : founded by .', 'success_rate': 0.9181547619047619, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : movement .', 'success_rate': 0.8806818181818182, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9300595238095238, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.965625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : producer .', 'success_rate': 0.9484375, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 631, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.9389880952380952, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : replaces .', 'success_rate': 0.890625, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/3_ext.jsonl'}}
estimate vocab size: 10766
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10866, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.59it/s]Extractor Estimating: 2it [00:01,  1.55it/s]Extractor Estimating: 3it [00:01,  1.54it/s]Extractor Estimating: 4it [00:02,  1.49it/s]Extractor Estimating: 5it [00:03,  1.52it/s]Extractor Estimating: 6it [00:03,  1.52it/s]Extractor Estimating: 7it [00:04,  1.50it/s]Extractor Estimating: 8it [00:05,  1.56it/s]Extractor Estimating: 9it [00:05,  1.59it/s]Extractor Estimating: 10it [00:06,  1.58it/s]Extractor Estimating: 11it [00:07,  1.47it/s]Extractor Estimating: 12it [00:07,  1.55it/s]Extractor Estimating: 13it [00:08,  1.54it/s]Extractor Estimating: 14it [00:09,  1.54it/s]Extractor Estimating: 15it [00:09,  1.47it/s]Extractor Estimating: 16it [00:10,  1.51it/s]Extractor Estimating: 17it [00:11,  1.59it/s]Extractor Estimating: 18it [00:11,  1.63it/s]Extractor Estimating: 19it [00:12,  1.56it/s]Extractor Estimating: 20it [00:12,  1.55it/s]Extractor Estimating: 21it [00:13,  1.54it/s]Extractor Estimating: 22it [00:14,  1.52it/s]Extractor Estimating: 23it [00:14,  1.57it/s]Extractor Estimating: 24it [00:15,  1.60it/s]Extractor Estimating: 25it [00:16,  1.63it/s]Extractor Estimating: 26it [00:16,  1.60it/s]Extractor Estimating: 27it [00:17,  1.56it/s]Extractor Estimating: 28it [00:18,  1.57it/s]Extractor Estimating: 29it [00:18,  1.59it/s]Extractor Estimating: 30it [00:19,  1.55it/s]Extractor Estimating: 31it [00:20,  1.48it/s]Extractor Estimating: 32it [00:20,  1.51it/s]Extractor Estimating: 33it [00:21,  1.52it/s]Extractor Estimating: 34it [00:21,  1.55it/s]Extractor Estimating: 35it [00:22,  1.53it/s]Extractor Estimating: 36it [00:23,  1.49it/s]Extractor Estimating: 37it [00:23,  1.51it/s]Extractor Estimating: 38it [00:24,  1.53it/s]Extractor Estimating: 39it [00:25,  1.53it/s]Extractor Estimating: 40it [00:26,  1.48it/s]Extractor Estimating: 41it [00:26,  1.43it/s]Extractor Estimating: 42it [00:27,  1.42it/s]Extractor Estimating: 43it [00:28,  1.45it/s]Extractor Estimating: 44it [00:28,  1.48it/s]Extractor Estimating: 45it [00:29,  1.49it/s]Extractor Estimating: 46it [00:30,  1.50it/s]Extractor Estimating: 47it [00:30,  1.42it/s]Extractor Estimating: 48it [00:31,  1.45it/s]Extractor Estimating: 49it [00:32,  1.43it/s]Extractor Estimating: 50it [00:32,  1.44it/s]Extractor Estimating: 51it [00:33,  1.49it/s]Extractor Estimating: 52it [00:34,  1.46it/s]Extractor Estimating: 53it [00:34,  1.52it/s]Extractor Estimating: 54it [00:35,  1.55it/s]Extractor Estimating: 55it [00:36,  1.54it/s]Extractor Estimating: 56it [00:36,  1.50it/s]Extractor Estimating: 57it [00:37,  1.51it/s]Extractor Estimating: 58it [00:38,  1.53it/s]Extractor Estimating: 59it [00:38,  1.55it/s]Extractor Estimating: 60it [00:39,  1.52it/s]Extractor Estimating: 61it [00:40,  1.56it/s]Extractor Estimating: 62it [00:40,  1.51it/s]Extractor Estimating: 63it [00:41,  1.54it/s]Extractor Estimating: 64it [00:42,  1.49it/s]Extractor Estimating: 65it [00:42,  1.52it/s]Extractor Estimating: 66it [00:43,  1.52it/s]Extractor Estimating: 67it [00:44,  1.49it/s]Extractor Estimating: 68it [00:44,  1.50it/s]Extractor Estimating: 69it [00:45,  1.50it/s]Extractor Estimating: 70it [00:46,  1.54it/s]Extractor Estimating: 71it [00:46,  1.54it/s]Extractor Estimating: 72it [00:47,  1.51it/s]Extractor Estimating: 73it [00:47,  1.54it/s]Extractor Estimating: 74it [00:48,  1.59it/s]Extractor Estimating: 75it [00:49,  1.58it/s]Extractor Estimating: 76it [00:49,  1.59it/s]Extractor Estimating: 77it [00:50,  1.59it/s]Extractor Estimating: 78it [00:51,  1.55it/s]Extractor Estimating: 79it [00:51,  1.56it/s]Extractor Estimating: 80it [00:52,  1.56it/s]Extractor Estimating: 81it [00:53,  1.54it/s]Extractor Estimating: 82it [00:53,  1.54it/s]Extractor Estimating: 83it [00:54,  1.54it/s]Extractor Estimating: 84it [00:55,  1.53it/s]Extractor Estimating: 85it [00:55,  1.52it/s]Extractor Estimating: 86it [00:56,  1.44it/s]Extractor Estimating: 87it [00:57,  1.48it/s]Extractor Estimating: 88it [00:57,  1.49it/s]Extractor Estimating: 89it [00:58,  1.51it/s]Extractor Estimating: 90it [00:59,  1.47it/s]Extractor Estimating: 91it [00:59,  1.45it/s]Extractor Estimating: 92it [01:00,  1.50it/s]Extractor Estimating: 93it [01:01,  1.53it/s]Extractor Estimating: 94it [01:01,  1.53it/s]Extractor Estimating: 95it [01:02,  1.49it/s]Extractor Estimating: 96it [01:03,  1.52it/s]Extractor Estimating: 97it [01:03,  1.53it/s]Extractor Estimating: 98it [01:04,  1.52it/s]Extractor Estimating: 99it [01:05,  1.56it/s]Extractor Estimating: 100it [01:05,  1.53it/s]Extractor Estimating: 101it [01:06,  1.57it/s]Extractor Estimating: 102it [01:06,  1.63it/s]Extractor Estimating: 103it [01:07,  1.56it/s]Extractor Estimating: 104it [01:08,  1.49it/s]Extractor Estimating: 105it [01:08,  1.54it/s]Extractor Estimating: 106it [01:09,  1.56it/s]Extractor Estimating: 107it [01:10,  1.59it/s]Extractor Estimating: 108it [01:10,  1.58it/s]Extractor Estimating: 109it [01:11,  1.57it/s]Extractor Estimating: 110it [01:11,  1.60it/s]Extractor Estimating: 111it [01:12,  1.58it/s]Extractor Estimating: 112it [01:13,  1.60it/s]Extractor Estimating: 113it [01:13,  1.58it/s]Extractor Estimating: 114it [01:14,  1.63it/s]Extractor Estimating: 115it [01:15,  1.63it/s]Extractor Estimating: 116it [01:15,  1.61it/s]Extractor Estimating: 117it [01:16,  1.61it/s]Extractor Estimating: 118it [01:17,  1.52it/s]Extractor Estimating: 119it [01:17,  1.55it/s]Extractor Estimating: 120it [01:18,  1.58it/s]Extractor Estimating: 121it [01:18,  1.62it/s]Extractor Estimating: 122it [01:19,  1.64it/s]Extractor Estimating: 123it [01:20,  1.62it/s]Extractor Estimating: 124it [01:20,  1.63it/s]Extractor Estimating: 125it [01:21,  1.64it/s]Extractor Estimating: 126it [01:22,  1.55it/s]Extractor Estimating: 127it [01:22,  1.66it/s]Extractor Estimating: 128it [01:23,  1.67it/s]Extractor Estimating: 129it [01:23,  1.67it/s]Extractor Estimating: 130it [01:24,  1.69it/s]Extractor Estimating: 131it [01:24,  1.74it/s]Extractor Estimating: 132it [01:25,  1.73it/s]Extractor Estimating: 133it [01:25,  1.76it/s]Extractor Estimating: 134it [01:26,  1.77it/s]Extractor Estimating: 135it [01:27,  1.74it/s]Extractor Estimating: 136it [01:27,  1.78it/s]Extractor Estimating: 137it [01:28,  1.84it/s]Extractor Estimating: 138it [01:28,  1.78it/s]Extractor Estimating: 139it [01:29,  1.77it/s]Extractor Estimating: 140it [01:29,  1.77it/s]Extractor Estimating: 141it [01:30,  1.81it/s]Extractor Estimating: 142it [01:31,  1.74it/s]Extractor Estimating: 143it [01:31,  1.74it/s]Extractor Estimating: 144it [01:32,  1.75it/s]Extractor Estimating: 145it [01:32,  1.78it/s]Extractor Estimating: 146it [01:33,  1.82it/s]Extractor Estimating: 147it [01:33,  1.76it/s]Extractor Estimating: 148it [01:34,  1.72it/s]Extractor Estimating: 149it [01:35,  1.73it/s]Extractor Estimating: 150it [01:35,  1.77it/s]Extractor Estimating: 151it [01:36,  1.69it/s]Extractor Estimating: 152it [01:36,  1.64it/s]Extractor Estimating: 153it [01:37,  1.59it/s]Extractor Estimating: 154it [01:38,  1.52it/s]Extractor Estimating: 155it [01:38,  1.54it/s]Extractor Estimating: 156it [01:39,  1.58it/s]Extractor Estimating: 157it [01:40,  1.58it/s]Extractor Estimating: 158it [01:40,  1.60it/s]Extractor Estimating: 159it [01:41,  1.53it/s]Extractor Estimating: 160it [01:42,  1.42it/s]Extractor Estimating: 161it [01:42,  1.47it/s]Extractor Estimating: 162it [01:43,  1.50it/s]Extractor Estimating: 163it [01:44,  1.52it/s]Extractor Estimating: 164it [01:44,  1.51it/s]Extractor Estimating: 165it [01:45,  1.49it/s]Extractor Estimating: 166it [01:46,  1.55it/s]Extractor Estimating: 167it [01:46,  1.57it/s]Extractor Estimating: 168it [01:47,  1.55it/s]Extractor Estimating: 169it [01:48,  1.56it/s]Extractor Estimating: 170it [01:48,  1.57it/s]Extractor Estimating: 171it [01:49,  1.57it/s]Extractor Estimating: 172it [01:49,  1.58it/s]Extractor Estimating: 173it [01:50,  1.61it/s]Extractor Estimating: 174it [01:51,  1.58it/s]Extractor Estimating: 175it [01:51,  1.56it/s]Extractor Estimating: 176it [01:52,  1.54it/s]Extractor Estimating: 177it [01:53,  1.50it/s]Extractor Estimating: 178it [01:53,  1.45it/s]Extractor Estimating: 179it [01:54,  1.47it/s]Extractor Estimating: 180it [01:55,  1.49it/s]Extractor Estimating: 181it [01:55,  1.52it/s]Extractor Estimating: 182it [01:56,  1.52it/s]Extractor Estimating: 183it [01:57,  1.44it/s]Extractor Estimating: 184it [01:58,  1.44it/s]Extractor Estimating: 185it [01:58,  1.46it/s]Extractor Estimating: 186it [01:59,  1.46it/s]Extractor Estimating: 187it [02:00,  1.51it/s]Extractor Estimating: 188it [02:00,  1.54it/s]Extractor Estimating: 189it [02:01,  1.57it/s]Extractor Estimating: 190it [02:01,  1.56it/s]Extractor Estimating: 191it [02:02,  1.49it/s]Extractor Estimating: 192it [02:03,  1.51it/s]Extractor Estimating: 193it [02:04,  1.45it/s]Extractor Estimating: 194it [02:04,  1.45it/s]Extractor Estimating: 195it [02:05,  1.47it/s]Extractor Estimating: 196it [02:06,  1.47it/s]Extractor Estimating: 197it [02:06,  1.47it/s]Extractor Estimating: 198it [02:07,  1.48it/s]Extractor Estimating: 199it [02:08,  1.51it/s]Extractor Estimating: 200it [02:08,  1.52it/s]Extractor Estimating: 201it [02:09,  1.52it/s]Extractor Estimating: 202it [02:09,  1.53it/s]Extractor Estimating: 203it [02:10,  1.55it/s]Extractor Estimating: 204it [02:11,  1.59it/s]Extractor Estimating: 205it [02:11,  1.59it/s]Extractor Estimating: 206it [02:12,  1.57it/s]Extractor Estimating: 207it [02:13,  1.63it/s]Extractor Estimating: 208it [02:13,  1.67it/s]Extractor Estimating: 209it [02:14,  1.71it/s]Extractor Estimating: 210it [02:14,  1.70it/s]Extractor Estimating: 211it [02:15,  1.71it/s]Extractor Estimating: 212it [02:15,  1.72it/s]Extractor Estimating: 213it [02:16,  1.65it/s]Extractor Estimating: 214it [02:17,  1.66it/s]Extractor Estimating: 215it [02:17,  1.67it/s]Extractor Estimating: 216it [02:18,  1.62it/s]Extractor Estimating: 217it [02:19,  1.61it/s]Extractor Estimating: 218it [02:19,  1.66it/s]Extractor Estimating: 219it [02:20,  1.67it/s]Extractor Estimating: 220it [02:20,  1.68it/s]Extractor Estimating: 221it [02:21,  1.65it/s]Extractor Estimating: 222it [02:21,  1.67it/s]Extractor Estimating: 223it [02:22,  1.65it/s]Extractor Estimating: 224it [02:23,  1.66it/s]Extractor Estimating: 225it [02:23,  1.67it/s]Extractor Estimating: 226it [02:24,  1.66it/s]Extractor Estimating: 227it [02:25,  1.56it/s]Extractor Estimating: 228it [02:25,  1.53it/s]Extractor Estimating: 229it [02:26,  1.58it/s]Extractor Estimating: 230it [02:27,  1.56it/s]Extractor Estimating: 231it [02:27,  1.50it/s]Extractor Estimating: 232it [02:28,  1.53it/s]Extractor Estimating: 233it [02:29,  1.49it/s]Extractor Estimating: 234it [02:29,  1.57it/s]Extractor Estimating: 235it [02:30,  1.58it/s]Extractor Estimating: 236it [02:30,  1.61it/s]Extractor Estimating: 237it [02:31,  1.66it/s]Extractor Estimating: 238it [02:32,  1.65it/s]Extractor Estimating: 239it [02:32,  1.63it/s]Extractor Estimating: 240it [02:33,  1.62it/s]Extractor Estimating: 241it [02:33,  1.63it/s]Extractor Estimating: 242it [02:34,  1.62it/s]Extractor Estimating: 243it [02:35,  1.67it/s]Extractor Estimating: 244it [02:35,  1.67it/s]Extractor Estimating: 245it [02:36,  1.63it/s]Extractor Estimating: 246it [02:37,  1.45it/s]Extractor Estimating: 247it [02:37,  1.52it/s]Extractor Estimating: 248it [02:38,  1.52it/s]Extractor Estimating: 249it [02:39,  1.52it/s]Extractor Estimating: 250it [02:39,  1.46it/s]Extractor Estimating: 251it [02:40,  1.54it/s]Extractor Estimating: 252it [02:41,  1.59it/s]Extractor Estimating: 253it [02:41,  1.63it/s]Extractor Estimating: 254it [02:42,  1.65it/s]Extractor Estimating: 255it [02:42,  1.58it/s]Extractor Estimating: 256it [02:43,  1.57it/s]Extractor Estimating: 257it [02:44,  1.58it/s]Extractor Estimating: 258it [02:44,  1.60it/s]Extractor Estimating: 259it [02:45,  1.65it/s]Extractor Estimating: 260it [02:45,  1.60it/s]Extractor Estimating: 261it [02:46,  1.63it/s]Extractor Estimating: 262it [02:47,  1.63it/s]Extractor Estimating: 263it [02:47,  1.64it/s]Extractor Estimating: 264it [02:48,  1.65it/s]Extractor Estimating: 265it [02:48,  1.66it/s]Extractor Estimating: 266it [02:49,  1.63it/s]Extractor Estimating: 267it [02:50,  1.68it/s]Extractor Estimating: 268it [02:50,  1.64it/s]Extractor Estimating: 269it [02:51,  1.71it/s]Extractor Estimating: 270it [02:51,  1.75it/s]Extractor Estimating: 271it [02:52,  1.68it/s]Extractor Estimating: 272it [02:53,  1.72it/s]Extractor Estimating: 273it [02:53,  1.75it/s]Extractor Estimating: 274it [02:54,  1.73it/s]Extractor Estimating: 275it [02:54,  1.74it/s]Extractor Estimating: 276it [02:55,  1.65it/s]Extractor Estimating: 277it [02:56,  1.62it/s]Extractor Estimating: 278it [02:56,  1.59it/s]Extractor Estimating: 279it [02:57,  1.63it/s]Extractor Estimating: 280it [02:57,  1.61it/s]Extractor Estimating: 281it [02:58,  1.58it/s]Extractor Estimating: 282it [02:59,  1.59it/s]Extractor Estimating: 283it [02:59,  1.61it/s]Extractor Estimating: 284it [03:00,  1.57it/s]Extractor Estimating: 285it [03:01,  1.54it/s]Extractor Estimating: 286it [03:01,  1.57it/s]Extractor Estimating: 287it [03:02,  1.54it/s]Extractor Estimating: 288it [03:03,  1.52it/s]Extractor Estimating: 289it [03:03,  1.52it/s]Extractor Estimating: 290it [03:04,  1.54it/s]Extractor Estimating: 291it [03:05,  1.56it/s]Extractor Estimating: 292it [03:05,  1.57it/s]Extractor Estimating: 293it [03:06,  1.55it/s]Extractor Estimating: 294it [03:07,  1.55it/s]Extractor Estimating: 295it [03:07,  1.55it/s]Extractor Estimating: 296it [03:08,  1.52it/s]Extractor Estimating: 297it [03:09,  1.53it/s]Extractor Estimating: 298it [03:09,  1.57it/s]Extractor Estimating: 299it [03:10,  1.56it/s]Extractor Estimating: 300it [03:10,  1.53it/s]Extractor Estimating: 301it [03:11,  1.48it/s]Extractor Estimating: 302it [03:12,  1.47it/s]Extractor Estimating: 303it [03:13,  1.49it/s]Extractor Estimating: 304it [03:13,  1.51it/s]Extractor Estimating: 305it [03:14,  1.55it/s]Extractor Estimating: 306it [03:14,  1.53it/s]Extractor Estimating: 307it [03:15,  1.49it/s]Extractor Estimating: 308it [03:16,  1.47it/s]Extractor Estimating: 309it [03:17,  1.46it/s]Extractor Estimating: 310it [03:17,  1.46it/s]Extractor Estimating: 311it [03:18,  1.49it/s]Extractor Estimating: 312it [03:18,  1.54it/s]Extractor Estimating: 313it [03:19,  1.55it/s]Extractor Estimating: 314it [03:20,  1.55it/s]Extractor Estimating: 315it [03:20,  1.59it/s]Extractor Estimating: 316it [03:21,  1.54it/s]Extractor Estimating: 317it [03:22,  1.41it/s]Extractor Estimating: 318it [03:23,  1.46it/s]Extractor Estimating: 319it [03:23,  1.48it/s]Extractor Estimating: 320it [03:24,  1.50it/s]Extractor Estimating: 321it [03:25,  1.48it/s]Extractor Estimating: 322it [03:25,  1.52it/s]Extractor Estimating: 323it [03:26,  1.54it/s]Extractor Estimating: 324it [03:26,  1.51it/s]Extractor Estimating: 325it [03:27,  1.46it/s]Extractor Estimating: 326it [03:28,  1.45it/s]Extractor Estimating: 327it [03:29,  1.45it/s]Extractor Estimating: 328it [03:29,  1.46it/s]Extractor Estimating: 329it [03:30,  1.40it/s]Extractor Estimating: 330it [03:31,  1.41it/s]Extractor Estimating: 331it [03:31,  1.41it/s]Extractor Estimating: 332it [03:32,  1.39it/s]Extractor Estimating: 333it [03:33,  1.43it/s]Extractor Estimating: 334it [03:34,  1.43it/s]Extractor Estimating: 335it [03:34,  1.41it/s]Extractor Estimating: 336it [03:35,  1.40it/s]Extractor Estimating: 337it [03:36,  1.41it/s]Extractor Estimating: 338it [03:36,  1.40it/s]Extractor Estimating: 339it [03:37,  1.42it/s]Extractor Estimating: 340it [03:38,  1.40it/s]Extractor Estimating: 341it [03:38,  1.46it/s]Extractor Estimating: 342it [03:39,  1.42it/s]Extractor Estimating: 343it [03:40,  1.42it/s]Extractor Estimating: 344it [03:41,  1.43it/s]Extractor Estimating: 345it [03:41,  1.44it/s]Extractor Estimating: 346it [03:42,  1.42it/s]Extractor Estimating: 347it [03:43,  1.41it/s]Extractor Estimating: 348it [03:43,  1.44it/s]Extractor Estimating: 349it [03:44,  1.46it/s]Extractor Estimating: 350it [03:45,  1.46it/s]Extractor Estimating: 351it [03:45,  1.48it/s]Extractor Estimating: 352it [03:46,  1.49it/s]Extractor Estimating: 353it [03:47,  1.54it/s]Extractor Estimating: 354it [03:47,  1.55it/s]Extractor Estimating: 355it [03:48,  1.59it/s]Extractor Estimating: 356it [03:48,  1.59it/s]Extractor Estimating: 357it [03:49,  1.62it/s]Extractor Estimating: 358it [03:50,  1.59it/s]Extractor Estimating: 359it [03:50,  1.63it/s]Extractor Estimating: 360it [03:51,  1.65it/s]Extractor Estimating: 361it [03:52,  1.63it/s]Extractor Estimating: 362it [03:52,  1.62it/s]Extractor Estimating: 363it [03:53,  1.67it/s]Extractor Estimating: 364it [03:53,  1.67it/s]Extractor Estimating: 365it [03:54,  1.63it/s]Extractor Estimating: 366it [03:55,  1.66it/s]Extractor Estimating: 367it [03:55,  1.62it/s]Extractor Estimating: 368it [03:56,  1.62it/s]Extractor Estimating: 369it [03:56,  1.61it/s]Extractor Estimating: 370it [03:57,  1.62it/s]Extractor Estimating: 371it [03:58,  1.66it/s]Extractor Estimating: 372it [03:58,  1.57it/s]Extractor Estimating: 373it [03:59,  1.58it/s]Extractor Estimating: 374it [04:00,  1.57it/s]Extractor Estimating: 375it [04:00,  2.09it/s]Extractor Estimating: 375it [04:00,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:27:35,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:27:35,249 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:27:35,249 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:27:35,250 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:27:35,250 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:27:35,917 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:27:35,918 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:27:36,498 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:27:37,592 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:27:37,592 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:27:40,557 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:27:40,571 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:27:40,572 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:27:40,572 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:27:40,572 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:27:41,269 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:27:41,270 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:27:41,866 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:27:42,066 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:27:42,067 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 12:10:27,013 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 12:10:27,540 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7475 mean pseudo reward: 0.931859159403379
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl'}
train vocab size: 27084
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27184, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27184, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.065, loss:666.3690
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.080, loss:651.3535
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.118, loss:664.4408
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 1.075, loss:587.7226
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 1.064, loss:627.4785
>> valid entity prec:0.5368, rec:0.4120, f1:0.4662
>> valid relation prec:0.1410, rec:0.0304, f1:0.0500
>> valid relation with NER prec:0.1410, rec:0.0304, f1:0.0500
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 3.758, loss:639.2228
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.068, loss:598.5664
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.080, loss:602.6171
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 1.075, loss:629.6273
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 1.063, loss:609.0296
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4895, rec:0.4580, f1:0.4732
>> valid relation prec:0.1111, rec:0.0275, f1:0.0441
>> valid relation with NER prec:0.1111, rec:0.0275, f1:0.0441
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 164, avg_time 3.739, loss:625.6671
g_step 1200, step 264, avg_time 1.080, loss:623.9858
g_step 1300, step 52, avg_time 1.056, loss:575.9013
g_step 1400, step 152, avg_time 1.063, loss:577.5113
g_step 1500, step 252, avg_time 1.074, loss:592.3777
>> valid entity prec:0.5079, rec:0.3776, f1:0.4332
>> valid relation prec:0.0950, rec:0.0137, f1:0.0239
>> valid relation with NER prec:0.0950, rec:0.0137, f1:0.0239
g_step 1600, step 40, avg_time 3.711, loss:569.9525
g_step 1700, step 140, avg_time 1.075, loss:547.6564
g_step 1800, step 240, avg_time 1.054, loss:565.3643
g_step 1900, step 28, avg_time 1.051, loss:554.9040
g_step 2000, step 128, avg_time 1.067, loss:519.3554
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5212, rec:0.4445, f1:0.4798
>> valid relation prec:0.1328, rec:0.0237, f1:0.0403
>> valid relation with NER prec:0.1328, rec:0.0237, f1:0.0403
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 228, avg_time 3.723, loss:528.7065
g_step 2200, step 16, avg_time 1.051, loss:532.1280
g_step 2300, step 116, avg_time 1.049, loss:480.7055
g_step 2400, step 216, avg_time 1.054, loss:512.2977
g_step 2500, step 4, avg_time 1.063, loss:513.4314
>> valid entity prec:0.5265, rec:0.4328, f1:0.4751
>> valid relation prec:0.2025, rec:0.0375, f1:0.0633
>> valid relation with NER prec:0.2025, rec:0.0375, f1:0.0633
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 104, avg_time 3.703, loss:470.2470
g_step 2700, step 204, avg_time 1.063, loss:480.5251
g_step 2800, step 304, avg_time 1.054, loss:519.7935
g_step 2900, step 92, avg_time 1.044, loss:452.2333
g_step 3000, step 192, avg_time 1.057, loss:465.0782
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4971, rec:0.4639, f1:0.4799
>> valid relation prec:0.1019, rec:0.0312, f1:0.0478
>> valid relation with NER prec:0.1019, rec:0.0312, f1:0.0478
new max entity f1 on valid!
g_step 3100, step 292, avg_time 3.734, loss:487.8067
g_step 3200, step 80, avg_time 1.058, loss:453.4479
g_step 3300, step 180, avg_time 1.047, loss:459.7271
g_step 3400, step 280, avg_time 1.054, loss:445.2725
g_step 3500, step 68, avg_time 1.037, loss:414.9110
>> valid entity prec:0.5139, rec:0.4858, f1:0.4995
>> valid relation prec:0.1414, rec:0.0330, f1:0.0535
>> valid relation with NER prec:0.1414, rec:0.0330, f1:0.0535
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 168, avg_time 3.686, loss:437.7585
g_step 3700, step 268, avg_time 1.043, loss:453.5160
g_step 3800, step 56, avg_time 1.053, loss:417.7743
g_step 3900, step 156, avg_time 1.047, loss:417.8412
g_step 4000, step 256, avg_time 1.059, loss:437.6952
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4926, rec:0.4284, f1:0.4583
>> valid relation prec:0.1039, rec:0.0217, f1:0.0360
>> valid relation with NER prec:0.1039, rec:0.0217, f1:0.0360
g_step 4100, step 44, avg_time 3.681, loss:410.8819
g_step 4200, step 144, avg_time 1.061, loss:409.4781
g_step 4300, step 244, avg_time 1.036, loss:418.2756
g_step 4400, step 32, avg_time 1.055, loss:402.7532
g_step 4500, step 132, avg_time 1.050, loss:381.8886
>> valid entity prec:0.4875, rec:0.4279, f1:0.4558
>> valid relation prec:0.0804, rec:0.0198, f1:0.0317
>> valid relation with NER prec:0.0804, rec:0.0198, f1:0.0317
g_step 4600, step 232, avg_time 3.815, loss:390.5389
g_step 4700, step 20, avg_time 1.035, loss:386.3145
g_step 4800, step 120, avg_time 1.048, loss:377.5324
g_step 4900, step 220, avg_time 1.044, loss:387.6201
g_step 5000, step 8, avg_time 1.047, loss:394.7169
learning rate was adjusted to 0.0008
>> valid entity prec:0.5017, rec:0.3971, f1:0.4433
>> valid relation prec:0.0905, rec:0.0168, f1:0.0284
>> valid relation with NER prec:0.0905, rec:0.0168, f1:0.0284
g_step 5100, step 108, avg_time 3.635, loss:348.1472
g_step 5200, step 208, avg_time 1.039, loss:366.8373
g_step 5300, step 308, avg_time 1.038, loss:370.9727
g_step 5400, step 96, avg_time 1.030, loss:336.4596
g_step 5500, step 196, avg_time 1.036, loss:342.1344
>> valid entity prec:0.4855, rec:0.3973, f1:0.4370
>> valid relation prec:0.1042, rec:0.0282, f1:0.0444
>> valid relation with NER prec:0.1042, rec:0.0282, f1:0.0444
g_step 5600, step 296, avg_time 3.632, loss:358.1719
g_step 5700, step 84, avg_time 1.034, loss:332.4737
g_step 5800, step 184, avg_time 1.030, loss:339.1437
g_step 5900, step 284, avg_time 1.035, loss:345.8508
g_step 6000, step 72, avg_time 1.032, loss:317.5526
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4784, rec:0.4732, f1:0.4758
>> valid relation prec:0.0880, rec:0.0281, f1:0.0426
>> valid relation with NER prec:0.0880, rec:0.0281, f1:0.0426
g_step 6100, step 172, avg_time 3.620, loss:335.9092
g_step 6200, step 272, avg_time 1.040, loss:338.1693
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 12:10:27 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 12:10:27 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_12-10-27_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 12:10:28 - WARNING - datasets.builder -   Using custom data configuration default-e3d0599c88049206
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-e3d0599c88049206/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 12:10:31,498 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:10:31,499 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 12:10:31,500 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:10:31,501 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 12:10:31,615 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:10:31,671 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:10:31,672 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:10:31,672 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:10:31,672 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:10:31,672 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:10:31,672 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 12:10:32,159 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 12:10:35,271 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 12:10:35,327 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-e3d0599c88049206/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.57ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.56ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.04ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.33ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.48ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.57ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.63ba/s]100%|██████████| 8/8 [00:01<00:00,  5.53ba/s]100%|██████████| 8/8 [00:01<00:00,  4.59ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:02,  2.94ba/s] 22%|██▏       | 2/9 [00:00<00:01,  3.78ba/s] 33%|███▎      | 3/9 [00:00<00:01,  3.25ba/s] 44%|████▍     | 4/9 [00:01<00:01,  3.46ba/s] 56%|█████▌    | 5/9 [00:01<00:01,  3.83ba/s] 67%|██████▋   | 6/9 [00:01<00:00,  4.08ba/s] 78%|███████▊  | 7/9 [00:01<00:00,  4.25ba/s] 89%|████████▉ | 8/9 [00:02<00:00,  4.38ba/s]100%|██████████| 9/9 [00:02<00:00,  5.12ba/s]100%|██████████| 9/9 [00:02<00:00,  4.19ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  5.64ba/s] 38%|███▊      | 3/8 [00:00<00:00,  8.90ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.01ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.52ba/s]100%|██████████| 8/8 [00:00<00:00, 10.60ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:01,  5.91ba/s] 33%|███▎      | 3/9 [00:00<00:00,  9.08ba/s] 56%|█████▌    | 5/9 [00:00<00:00, 10.10ba/s] 78%|███████▊  | 7/9 [00:00<00:00,  9.74ba/s]100%|██████████| 9/9 [00:00<00:00, 10.97ba/s]100%|██████████| 9/9 [00:00<00:00, 10.17ba/s]
[INFO|trainer.py:414] 2023-08-29 12:10:42,315 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 12:10:42,446 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 12:10:42,446 >>   Num examples = 7499
[INFO|trainer.py:1149] 2023-08-29 12:10:42,446 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 12:10:42,446 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 12:10:42,446 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 12:10:42,446 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 12:10:42,446 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:55,  3.33it/s]  0%|          | 2/585 [00:00<02:50,  3.42it/s]  1%|          | 3/585 [00:00<02:48,  3.45it/s]  1%|          | 4/585 [00:01<02:47,  3.46it/s]  1%|          | 5/585 [00:01<02:47,  3.47it/s]  1%|          | 6/585 [00:01<02:47,  3.46it/s]  1%|          | 7/585 [00:02<02:47,  3.45it/s]  1%|▏         | 8/585 [00:02<02:58,  3.23it/s]  2%|▏         | 9/585 [00:02<02:55,  3.29it/s]  2%|▏         | 10/585 [00:02<02:52,  3.33it/s]  2%|▏         | 11/585 [00:03<02:51,  3.35it/s]  2%|▏         | 12/585 [00:03<02:49,  3.38it/s]  2%|▏         | 13/585 [00:03<02:48,  3.39it/s]  2%|▏         | 14/585 [00:04<02:48,  3.40it/s]  3%|▎         | 15/585 [00:04<02:47,  3.41it/s]  3%|▎         | 16/585 [00:04<02:47,  3.41it/s]  3%|▎         | 17/585 [00:05<02:46,  3.41it/s]  3%|▎         | 18/585 [00:05<02:45,  3.42it/s]  3%|▎         | 19/585 [00:05<02:52,  3.29it/s]  3%|▎         | 20/585 [00:05<02:49,  3.34it/s]  4%|▎         | 21/585 [00:06<02:47,  3.37it/s]  4%|▍         | 22/585 [00:06<02:45,  3.40it/s]  4%|▍         | 23/585 [00:06<02:44,  3.42it/s]  4%|▍         | 24/585 [00:07<02:43,  3.43it/s]  4%|▍         | 25/585 [00:07<02:42,  3.44it/s]  4%|▍         | 26/585 [00:07<02:41,  3.45it/s]  5%|▍         | 27/585 [00:07<02:41,  3.46it/s]  5%|▍         | 28/585 [00:08<02:41,  3.45it/s]  5%|▍         | 29/585 [00:08<02:40,  3.46it/s]  5%|▌         | 30/585 [00:08<02:51,  3.24it/s]  5%|▌         | 31/585 [00:09<02:47,  3.31it/s]  5%|▌         | 32/585 [00:09<02:44,  3.36it/s]  6%|▌         | 33/585 [00:09<02:42,  3.39it/s]  6%|▌         | 34/585 [00:10<02:41,  3.41it/s]  6%|▌         | 35/585 [00:10<02:40,  3.43it/s]  6%|▌         | 36/585 [00:10<02:39,  3.44it/s]  6%|▋         | 37/585 [00:10<02:38,  3.45it/s]  6%|▋         | 38/585 [00:11<02:38,  3.46it/s]  7%|▋         | 39/585 [00:11<02:37,  3.46it/s]  7%|▋         | 40/585 [00:11<02:37,  3.46it/s]  7%|▋         | 41/585 [00:12<02:50,  3.18it/s]  7%|▋         | 42/585 [00:12<02:46,  3.26it/s]  7%|▋         | 43/585 [00:12<02:43,  3.32it/s]  8%|▊         | 44/585 [00:12<02:40,  3.37it/s]  8%|▊         | 45/585 [00:13<02:38,  3.40it/s]  8%|▊         | 46/585 [00:13<02:37,  3.42it/s]  8%|▊         | 47/585 [00:13<02:36,  3.44it/s]  8%|▊         | 48/585 [00:14<02:35,  3.45it/s]  8%|▊         | 49/585 [00:14<02:35,  3.46it/s]  9%|▊         | 50/585 [00:14<02:34,  3.46it/s]  9%|▊         | 51/585 [00:15<02:34,  3.47it/s]  9%|▉         | 52/585 [00:15<02:33,  3.47it/s]  9%|▉         | 53/585 [00:15<02:33,  3.47it/s]  9%|▉         | 54/585 [00:15<02:33,  3.47it/s]  9%|▉         | 55/585 [00:16<02:32,  3.47it/s] 10%|▉         | 56/585 [00:16<02:32,  3.47it/s] 10%|▉         | 57/585 [00:16<02:32,  3.47it/s] 10%|▉         | 58/585 [00:17<02:31,  3.47it/s] 10%|█         | 59/585 [00:17<02:31,  3.47it/s] 10%|█         | 60/585 [00:17<02:31,  3.47it/s] 10%|█         | 61/585 [00:17<02:30,  3.47it/s] 11%|█         | 62/585 [00:18<02:43,  3.21it/s] 11%|█         | 63/585 [00:18<02:39,  3.28it/s] 11%|█         | 64/585 [00:18<02:36,  3.34it/s] 11%|█         | 65/585 [00:19<02:34,  3.38it/s] 11%|█▏        | 66/585 [00:19<02:32,  3.40it/s] 11%|█▏        | 67/585 [00:19<02:31,  3.42it/s] 12%|█▏        | 68/585 [00:19<02:30,  3.43it/s] 12%|█▏        | 69/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 70/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 71/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 72/585 [00:21<02:28,  3.46it/s] 12%|█▏        | 73/585 [00:21<02:38,  3.22it/s] 13%|█▎        | 74/585 [00:21<02:35,  3.29it/s] 13%|█▎        | 75/585 [00:22<02:32,  3.35it/s] 13%|█▎        | 76/585 [00:22<02:30,  3.38it/s] 13%|█▎        | 77/585 [00:22<02:29,  3.41it/s] 13%|█▎        | 78/585 [00:22<02:28,  3.42it/s] 14%|█▎        | 79/585 [00:23<02:27,  3.43it/s] 14%|█▎        | 80/585 [00:23<02:26,  3.44it/s] 14%|█▍        | 81/585 [00:23<02:26,  3.45it/s] 14%|█▍        | 82/585 [00:24<02:25,  3.45it/s] 14%|█▍        | 83/585 [00:24<02:25,  3.46it/s] 14%|█▍        | 84/585 [00:24<02:33,  3.26it/s] 15%|█▍        | 85/585 [00:25<02:30,  3.31it/s] 15%|█▍        | 86/585 [00:25<02:28,  3.36it/s] 15%|█▍        | 87/585 [00:25<02:26,  3.39it/s] 15%|█▌        | 88/585 [00:25<02:25,  3.41it/s] 15%|█▌        | 89/585 [00:26<02:24,  3.42it/s] 15%|█▌        | 90/585 [00:26<02:24,  3.43it/s] 16%|█▌        | 91/585 [00:26<02:23,  3.44it/s] 16%|█▌        | 92/585 [00:27<02:22,  3.45it/s] 16%|█▌        | 93/585 [00:27<02:22,  3.45it/s] 16%|█▌        | 94/585 [00:27<02:22,  3.45it/s] 16%|█▌        | 95/585 [00:27<02:30,  3.27it/s] 16%|█▋        | 96/585 [00:28<02:27,  3.32it/s] 17%|█▋        | 97/585 [00:28<02:25,  3.36it/s] 17%|█▋        | 98/585 [00:28<02:23,  3.39it/s] 17%|█▋        | 99/585 [00:29<02:22,  3.41it/s] 17%|█▋        | 100/585 [00:29<02:21,  3.43it/s] 17%|█▋        | 101/585 [00:29<02:21,  3.43it/s] 17%|█▋        | 102/585 [00:29<02:20,  3.43it/s] 18%|█▊        | 103/585 [00:30<02:20,  3.44it/s] 18%|█▊        | 104/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 105/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 106/585 [00:31<02:22,  3.36it/s] 18%|█▊        | 107/585 [00:31<02:21,  3.38it/s] 18%|█▊        | 108/585 [00:31<02:20,  3.40it/s] 19%|█▊        | 109/585 [00:32<02:19,  3.42it/s] 19%|█▉        | 110/585 [00:32<02:18,  3.43it/s] 19%|█▉        | 111/585 [00:32<02:18,  3.43it/s] 19%|█▉        | 112/585 [00:32<02:17,  3.44it/s] 19%|█▉        | 113/585 [00:33<02:16,  3.45it/s] 19%|█▉        | 114/585 [00:33<02:16,  3.46it/s] 20%|█▉        | 115/585 [00:33<02:15,  3.46it/s] 20%|█▉        | 116/585 [00:34<02:15,  3.46it/s] 20%|██        | 117/585 [00:34<02:20,  3.33it/s][INFO|trainer.py:2140] 2023-08-29 12:11:16,940 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:11:16,940 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 12:11:16,940 >>   Batch size = 8

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:18, 56.47it/s][A
  1%|          | 12/1071 [00:00<00:21, 49.06it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 47.50it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 46.47it/s][A
  3%|▎         | 27/1071 [00:00<00:22, 45.99it/s][A
  3%|▎         | 32/1071 [00:00<00:22, 45.81it/s][A
  3%|▎         | 37/1071 [00:00<00:22, 45.66it/s][A
  4%|▍         | 42/1071 [00:00<00:22, 45.04it/s][A
  4%|▍         | 47/1071 [00:01<00:22, 44.75it/s][A
  5%|▍         | 52/1071 [00:01<00:22, 44.85it/s][A
  5%|▌         | 57/1071 [00:01<00:22, 44.90it/s][A
  6%|▌         | 62/1071 [00:01<00:22, 45.00it/s][A
  6%|▋         | 67/1071 [00:01<00:22, 44.99it/s][A
  7%|▋         | 72/1071 [00:01<00:22, 45.08it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 45.10it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 44.87it/s][A
  8%|▊         | 87/1071 [00:01<00:22, 44.41it/s][A
  9%|▊         | 92/1071 [00:02<00:22, 44.39it/s][A
  9%|▉         | 97/1071 [00:02<00:21, 44.55it/s][A
 10%|▉         | 102/1071 [00:02<00:21, 44.80it/s][A
 10%|▉         | 107/1071 [00:02<00:21, 44.74it/s][A
 10%|█         | 112/1071 [00:02<00:21, 44.87it/s][A
 11%|█         | 117/1071 [00:02<00:21, 44.89it/s][A
 11%|█▏        | 122/1071 [00:02<00:22, 42.90it/s][A
 12%|█▏        | 127/1071 [00:02<00:21, 43.34it/s][A
 12%|█▏        | 132/1071 [00:02<00:21, 43.73it/s][A
 13%|█▎        | 137/1071 [00:03<00:21, 44.04it/s][A
 13%|█▎        | 142/1071 [00:03<00:21, 44.17it/s][A
 14%|█▎        | 147/1071 [00:03<00:20, 44.48it/s][A
 14%|█▍        | 152/1071 [00:03<00:20, 44.68it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 44.83it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 44.63it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 44.79it/s][A
 16%|█▌        | 172/1071 [00:03<00:20, 44.83it/s][A
 17%|█▋        | 177/1071 [00:03<00:19, 44.80it/s][A
 17%|█▋        | 182/1071 [00:04<00:19, 44.70it/s][A
 17%|█▋        | 187/1071 [00:04<00:19, 44.65it/s][A
 18%|█▊        | 192/1071 [00:04<00:19, 44.96it/s][A
 18%|█▊        | 197/1071 [00:04<00:19, 45.03it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 44.84it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 44.82it/s][A
 20%|█▉        | 212/1071 [00:04<00:19, 44.89it/s][A
 20%|██        | 217/1071 [00:04<00:19, 44.81it/s][A
 21%|██        | 222/1071 [00:04<00:18, 44.77it/s][A
 21%|██        | 227/1071 [00:05<00:18, 44.51it/s][A
 22%|██▏       | 232/1071 [00:05<00:18, 44.60it/s][A
 22%|██▏       | 237/1071 [00:05<00:18, 44.88it/s][A
 23%|██▎       | 242/1071 [00:05<00:18, 44.90it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 44.88it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 44.89it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 43.66it/s][A
 24%|██▍       | 262/1071 [00:05<00:18, 44.13it/s][A
 25%|██▍       | 267/1071 [00:05<00:18, 44.22it/s][A
 25%|██▌       | 272/1071 [00:06<00:18, 44.38it/s][A
 26%|██▌       | 277/1071 [00:06<00:17, 44.32it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 44.63it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 44.69it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 44.80it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 44.63it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 44.76it/s][A
 29%|██▊       | 307/1071 [00:06<00:17, 44.85it/s][A
 29%|██▉       | 312/1071 [00:06<00:16, 44.86it/s][A
 30%|██▉       | 317/1071 [00:07<00:16, 44.76it/s][A
 30%|███       | 322/1071 [00:07<00:16, 44.73it/s][A
 31%|███       | 327/1071 [00:07<00:19, 39.13it/s][A
 31%|███       | 333/1071 [00:07<00:17, 42.26it/s][A
 32%|███▏      | 338/1071 [00:07<00:16, 43.16it/s][A
 32%|███▏      | 343/1071 [00:07<00:16, 43.84it/s][A
 32%|███▏      | 348/1071 [00:07<00:16, 44.17it/s][A
 33%|███▎      | 353/1071 [00:07<00:16, 44.42it/s][A
 33%|███▎      | 358/1071 [00:08<00:15, 44.58it/s][A
 34%|███▍      | 363/1071 [00:08<00:15, 44.59it/s][A
 34%|███▍      | 368/1071 [00:08<00:15, 44.59it/s][A
 35%|███▍      | 373/1071 [00:08<00:15, 44.33it/s][A
 35%|███▌      | 378/1071 [00:08<00:15, 44.38it/s][A
 36%|███▌      | 383/1071 [00:08<00:15, 44.68it/s][A
 36%|███▌      | 388/1071 [00:08<00:15, 44.69it/s][A
 37%|███▋      | 393/1071 [00:08<00:17, 39.24it/s][A
 37%|███▋      | 398/1071 [00:08<00:16, 40.94it/s][A
 38%|███▊      | 403/1071 [00:09<00:15, 42.15it/s][A
 38%|███▊      | 408/1071 [00:09<00:15, 43.13it/s][A
 39%|███▊      | 413/1071 [00:09<00:15, 43.73it/s][A
 39%|███▉      | 418/1071 [00:09<00:14, 44.15it/s][A
 39%|███▉      | 423/1071 [00:09<00:14, 44.44it/s][A
 40%|███▉      | 428/1071 [00:09<00:14, 44.52it/s][A
 40%|████      | 433/1071 [00:09<00:14, 44.05it/s][A
 41%|████      | 438/1071 [00:09<00:14, 44.00it/s][A
 41%|████▏     | 443/1071 [00:09<00:14, 44.26it/s][A
 42%|████▏     | 448/1071 [00:10<00:13, 44.56it/s][A
 42%|████▏     | 453/1071 [00:10<00:15, 40.10it/s][A
 43%|████▎     | 458/1071 [00:10<00:23, 25.56it/s][A
 43%|████▎     | 463/1071 [00:10<00:20, 29.47it/s][A
 44%|████▎     | 468/1071 [00:10<00:18, 32.90it/s][A
 44%|████▍     | 473/1071 [00:10<00:16, 35.82it/s][A
 45%|████▍     | 478/1071 [00:11<00:15, 38.13it/s][A
 45%|████▌     | 483/1071 [00:11<00:14, 40.03it/s][A
 46%|████▌     | 488/1071 [00:11<00:14, 41.42it/s][A
 46%|████▌     | 493/1071 [00:11<00:13, 42.46it/s][A
 46%|████▋     | 498/1071 [00:11<00:13, 42.95it/s][A
 47%|████▋     | 503/1071 [00:11<00:13, 43.27it/s][A
 47%|████▋     | 508/1071 [00:11<00:12, 43.88it/s][A
 48%|████▊     | 513/1071 [00:11<00:12, 44.22it/s][A
 48%|████▊     | 518/1071 [00:11<00:12, 44.30it/s][A
 49%|████▉     | 523/1071 [00:12<00:12, 44.43it/s][A
 49%|████▉     | 528/1071 [00:12<00:12, 44.35it/s][A
 50%|████▉     | 533/1071 [00:12<00:12, 44.61it/s][A
 50%|█████     | 538/1071 [00:12<00:11, 44.47it/s][A
 51%|█████     | 543/1071 [00:12<00:11, 44.30it/s][A
 51%|█████     | 548/1071 [00:12<00:11, 44.49it/s][A
 52%|█████▏    | 553/1071 [00:12<00:11, 44.58it/s][A
 52%|█████▏    | 558/1071 [00:12<00:11, 44.74it/s][A
 53%|█████▎    | 563/1071 [00:12<00:11, 44.90it/s][A
 53%|█████▎    | 568/1071 [00:13<00:11, 45.00it/s][A
 54%|█████▎    | 573/1071 [00:13<00:11, 44.97it/s][A
 54%|█████▍    | 578/1071 [00:13<00:10, 44.87it/s][A
 54%|█████▍    | 583/1071 [00:13<00:10, 44.70it/s][A
 55%|█████▍    | 588/1071 [00:13<00:10, 44.61it/s][A
 55%|█████▌    | 593/1071 [00:13<00:10, 44.50it/s][A
 56%|█████▌    | 598/1071 [00:13<00:10, 44.64it/s][A
 56%|█████▋    | 603/1071 [00:13<00:10, 44.78it/s][A
 57%|█████▋    | 608/1071 [00:13<00:10, 44.89it/s][A
 57%|█████▋    | 613/1071 [00:14<00:10, 45.04it/s][A
 58%|█████▊    | 618/1071 [00:14<00:10, 41.79it/s][A
 58%|█████▊    | 623/1071 [00:14<00:10, 42.77it/s][A
 59%|█████▊    | 628/1071 [00:14<00:10, 43.51it/s][A
 59%|█████▉    | 633/1071 [00:14<00:09, 43.80it/s][A
 60%|█████▉    | 638/1071 [00:14<00:09, 44.08it/s][A
 60%|██████    | 643/1071 [00:14<00:09, 44.33it/s][A
 61%|██████    | 648/1071 [00:14<00:09, 44.56it/s][A
 61%|██████    | 653/1071 [00:14<00:09, 44.73it/s][A
 61%|██████▏   | 658/1071 [00:15<00:09, 44.46it/s][A
 62%|██████▏   | 663/1071 [00:15<00:09, 44.34it/s][A
 62%|██████▏   | 668/1071 [00:15<00:09, 44.50it/s][A
 63%|██████▎   | 673/1071 [00:15<00:08, 44.68it/s][A
 63%|██████▎   | 678/1071 [00:15<00:08, 44.74it/s][A
 64%|██████▍   | 683/1071 [00:15<00:08, 44.86it/s][A
 64%|██████▍   | 688/1071 [00:15<00:08, 44.78it/s][A
 65%|██████▍   | 693/1071 [00:15<00:08, 44.70it/s][A
 65%|██████▌   | 698/1071 [00:15<00:08, 44.85it/s][A
 66%|██████▌   | 703/1071 [00:16<00:08, 44.67it/s][A
 66%|██████▌   | 708/1071 [00:16<00:08, 44.41it/s][A
 67%|██████▋   | 713/1071 [00:16<00:08, 44.51it/s][A
 67%|██████▋   | 718/1071 [00:16<00:07, 44.68it/s][A
 68%|██████▊   | 723/1071 [00:16<00:07, 44.71it/s][A
 68%|██████▊   | 728/1071 [00:16<00:07, 44.86it/s][A
 68%|██████▊   | 733/1071 [00:16<00:07, 44.91it/s][A
 69%|██████▉   | 738/1071 [00:16<00:07, 44.95it/s][A
 69%|██████▉   | 743/1071 [00:16<00:07, 45.02it/s][A
 70%|██████▉   | 748/1071 [00:17<00:07, 44.71it/s][A
 70%|███████   | 753/1071 [00:17<00:08, 39.38it/s][A
 71%|███████   | 758/1071 [00:17<00:07, 40.98it/s][A
 71%|███████   | 763/1071 [00:17<00:07, 42.24it/s][A
 72%|███████▏  | 768/1071 [00:17<00:07, 43.21it/s][A
 72%|███████▏  | 773/1071 [00:17<00:06, 43.74it/s][A
 73%|███████▎  | 778/1071 [00:17<00:06, 44.26it/s][A
 73%|███████▎  | 783/1071 [00:17<00:06, 44.61it/s][A
 74%|███████▎  | 788/1071 [00:18<00:06, 44.63it/s][A
 74%|███████▍  | 793/1071 [00:18<00:06, 44.37it/s][A
 75%|███████▍  | 798/1071 [00:18<00:06, 44.15it/s][A
 75%|███████▍  | 803/1071 [00:18<00:06, 44.22it/s][A
 75%|███████▌  | 808/1071 [00:18<00:05, 44.57it/s][A
 76%|███████▌  | 813/1071 [00:18<00:05, 44.79it/s][A
 76%|███████▋  | 818/1071 [00:18<00:05, 44.92it/s][A
 77%|███████▋  | 823/1071 [00:18<00:05, 44.73it/s][A
 77%|███████▋  | 828/1071 [00:18<00:05, 45.07it/s][A
 78%|███████▊  | 833/1071 [00:19<00:05, 44.99it/s][A
 78%|███████▊  | 838/1071 [00:19<00:05, 44.67it/s][A
 79%|███████▊  | 843/1071 [00:19<00:05, 44.51it/s][A
 79%|███████▉  | 848/1071 [00:19<00:05, 44.48it/s][A
 80%|███████▉  | 853/1071 [00:19<00:04, 44.63it/s][A
 80%|████████  | 858/1071 [00:19<00:04, 44.84it/s][A
 81%|████████  | 863/1071 [00:19<00:04, 44.93it/s][A
 81%|████████  | 868/1071 [00:19<00:04, 45.15it/s][A
 82%|████████▏ | 873/1071 [00:19<00:04, 44.99it/s][A
 82%|████████▏ | 878/1071 [00:20<00:04, 44.93it/s][A
 82%|████████▏ | 883/1071 [00:20<00:04, 44.75it/s][A
 83%|████████▎ | 888/1071 [00:20<00:04, 39.32it/s][A
 83%|████████▎ | 893/1071 [00:20<00:04, 40.97it/s][A
 84%|████████▍ | 898/1071 [00:20<00:04, 42.23it/s][A
 84%|████████▍ | 903/1071 [00:20<00:03, 43.16it/s][A
 85%|████████▍ | 908/1071 [00:20<00:03, 43.86it/s][A
 85%|████████▌ | 913/1071 [00:20<00:03, 44.25it/s][A
 86%|████████▌ | 918/1071 [00:20<00:03, 44.61it/s][A
 86%|████████▌ | 923/1071 [00:21<00:03, 44.54it/s][A
 87%|████████▋ | 928/1071 [00:21<00:03, 44.30it/s][A
 87%|████████▋ | 933/1071 [00:21<00:03, 44.13it/s][A
 88%|████████▊ | 938/1071 [00:21<00:03, 44.08it/s][A
 88%|████████▊ | 943/1071 [00:21<00:02, 44.43it/s][A
 89%|████████▊ | 948/1071 [00:21<00:02, 44.77it/s][A
 89%|████████▉ | 953/1071 [00:21<00:02, 44.97it/s][A
 89%|████████▉ | 958/1071 [00:21<00:02, 45.14it/s][A
 90%|████████▉ | 963/1071 [00:21<00:02, 45.09it/s][A
 90%|█████████ | 968/1071 [00:22<00:02, 44.89it/s][A
 91%|█████████ | 973/1071 [00:22<00:02, 44.44it/s][A
 91%|█████████▏| 978/1071 [00:22<00:02, 44.40it/s][A
 92%|█████████▏| 983/1071 [00:22<00:01, 44.38it/s][A
 92%|█████████▏| 988/1071 [00:22<00:01, 44.51it/s][A
 93%|█████████▎| 993/1071 [00:22<00:01, 44.69it/s][A
 93%|█████████▎| 998/1071 [00:22<00:01, 44.83it/s][A
 94%|█████████▎| 1003/1071 [00:22<00:01, 45.09it/s][A
 94%|█████████▍| 1008/1071 [00:22<00:01, 45.03it/s][A
 95%|█████████▍| 1013/1071 [00:23<00:01, 44.93it/s][A
 95%|█████████▌| 1018/1071 [00:23<00:01, 44.75it/s][A
 96%|█████████▌| 1023/1071 [00:23<00:01, 40.73it/s][A
 96%|█████████▌| 1028/1071 [00:23<00:01, 42.07it/s][A
 96%|█████████▋| 1033/1071 [00:23<00:00, 42.99it/s][A
 97%|█████████▋| 1038/1071 [00:23<00:00, 43.65it/s][A
 97%|█████████▋| 1043/1071 [00:23<00:00, 44.18it/s][A
 98%|█████████▊| 1048/1071 [00:23<00:00, 44.43it/s][A
 98%|█████████▊| 1053/1071 [00:24<00:00, 44.59it/s][A
 99%|█████████▉| 1058/1071 [00:24<00:00, 44.58it/s][A
 99%|█████████▉| 1063/1071 [00:24<00:00, 44.25it/s][A
100%|█████████▉| 1068/1071 [00:24<00:00, 44.28it/s][A
                                                   [A                                                 
100%|██████████| 1071/1071 [00:24<00:00, 44.28it/s][A 20%|██        | 117/585 [00:58<02:20,  3.33it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-29 12:11:41,781 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 12:11:42,026 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:11:45,687 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:11:45,881 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:11:45,980 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:11<1:28:13, 11.34s/it] 20%|██        | 119/585 [01:11<1:02:23,  8.03s/it] 21%|██        | 120/585 [01:12<44:15,  5.71s/it]   21%|██        | 121/585 [01:12<31:35,  4.09s/it] 21%|██        | 122/585 [01:12<22:44,  2.95s/it] 21%|██        | 123/585 [01:12<16:33,  2.15s/it] 21%|██        | 124/585 [01:13<12:14,  1.59s/it] 21%|██▏       | 125/585 [01:13<09:12,  1.20s/it] 22%|██▏       | 126/585 [01:13<07:05,  1.08it/s] 22%|██▏       | 127/585 [01:14<05:36,  1.36it/s] 22%|██▏       | 128/585 [01:14<04:34,  1.66it/s] 22%|██▏       | 129/585 [01:14<03:51,  1.97it/s] 22%|██▏       | 130/585 [01:15<03:27,  2.19it/s] 22%|██▏       | 131/585 [01:15<03:04,  2.47it/s] 23%|██▎       | 132/585 [01:15<02:47,  2.70it/s] 23%|██▎       | 133/585 [01:15<02:36,  2.89it/s] 23%|██▎       | 134/585 [01:16<02:28,  3.05it/s] 23%|██▎       | 135/585 [01:16<02:22,  3.16it/s] 23%|██▎       | 136/585 [01:16<02:18,  3.25it/s] 23%|██▎       | 137/585 [01:17<02:15,  3.31it/s] 24%|██▎       | 138/585 [01:17<02:13,  3.35it/s] 24%|██▍       | 139/585 [01:17<02:11,  3.39it/s] 24%|██▍       | 140/585 [01:17<02:10,  3.41it/s] 24%|██▍       | 141/585 [01:18<02:09,  3.43it/s] 24%|██▍       | 142/585 [01:18<02:08,  3.44it/s] 24%|██▍       | 143/585 [01:18<02:08,  3.45it/s] 25%|██▍       | 144/585 [01:19<02:07,  3.46it/s] 25%|██▍       | 145/585 [01:19<02:07,  3.46it/s] 25%|██▍       | 146/585 [01:19<02:10,  3.36it/s] 25%|██▌       | 147/585 [01:19<02:09,  3.39it/s] 25%|██▌       | 148/585 [01:20<02:08,  3.41it/s] 25%|██▌       | 149/585 [01:20<02:07,  3.43it/s] 26%|██▌       | 150/585 [01:20<02:06,  3.43it/s] 26%|██▌       | 151/585 [01:21<02:06,  3.44it/s] 26%|██▌       | 152/585 [01:21<02:05,  3.45it/s] 26%|██▌       | 153/585 [01:21<02:05,  3.45it/s] 26%|██▋       | 154/585 [01:22<02:04,  3.45it/s] 26%|██▋       | 155/585 [01:22<02:04,  3.45it/s] 27%|██▋       | 156/585 [01:22<02:04,  3.45it/s] 27%|██▋       | 157/585 [01:22<02:06,  3.37it/s] 27%|██▋       | 158/585 [01:23<02:05,  3.40it/s] 27%|██▋       | 159/585 [01:23<02:04,  3.41it/s] 27%|██▋       | 160/585 [01:23<02:04,  3.42it/s] 28%|██▊       | 161/585 [01:24<02:03,  3.43it/s] 28%|██▊       | 162/585 [01:24<02:02,  3.44it/s] 28%|██▊       | 163/585 [01:24<02:02,  3.45it/s] 28%|██▊       | 164/585 [01:24<02:01,  3.45it/s] 28%|██▊       | 165/585 [01:25<02:01,  3.45it/s] 28%|██▊       | 166/585 [01:25<02:01,  3.46it/s] 29%|██▊       | 167/585 [01:25<02:00,  3.46it/s] 29%|██▊       | 168/585 [01:26<02:03,  3.37it/s] 29%|██▉       | 169/585 [01:26<02:02,  3.39it/s] 29%|██▉       | 170/585 [01:26<02:01,  3.42it/s] 29%|██▉       | 171/585 [01:26<02:00,  3.43it/s] 29%|██▉       | 172/585 [01:27<02:00,  3.44it/s] 30%|██▉       | 173/585 [01:27<01:59,  3.44it/s] 30%|██▉       | 174/585 [01:27<01:59,  3.45it/s] 30%|██▉       | 175/585 [01:28<01:58,  3.45it/s] 30%|███       | 176/585 [01:28<01:58,  3.45it/s] 30%|███       | 177/585 [01:28<01:57,  3.46it/s] 30%|███       | 178/585 [01:28<01:57,  3.46it/s] 31%|███       | 179/585 [01:29<02:00,  3.37it/s] 31%|███       | 180/585 [01:29<01:59,  3.40it/s] 31%|███       | 181/585 [01:29<01:58,  3.42it/s] 31%|███       | 182/585 [01:30<01:57,  3.43it/s] 31%|███▏      | 183/585 [01:30<01:57,  3.43it/s] 31%|███▏      | 184/585 [01:30<01:56,  3.43it/s] 32%|███▏      | 185/585 [01:31<01:56,  3.44it/s] 32%|███▏      | 186/585 [01:31<01:55,  3.44it/s] 32%|███▏      | 187/585 [01:31<01:55,  3.44it/s] 32%|███▏      | 188/585 [01:31<01:55,  3.45it/s] 32%|███▏      | 189/585 [01:32<01:54,  3.45it/s] 32%|███▏      | 190/585 [01:32<01:57,  3.37it/s] 33%|███▎      | 191/585 [01:32<01:56,  3.39it/s] 33%|███▎      | 192/585 [01:33<01:55,  3.41it/s] 33%|███▎      | 193/585 [01:33<01:54,  3.42it/s] 33%|███▎      | 194/585 [01:33<01:53,  3.43it/s] 33%|███▎      | 195/585 [01:33<01:53,  3.44it/s] 34%|███▎      | 196/585 [01:34<01:53,  3.44it/s] 34%|███▎      | 197/585 [01:34<01:52,  3.45it/s] 34%|███▍      | 198/585 [01:34<01:52,  3.45it/s] 34%|███▍      | 199/585 [01:35<01:51,  3.45it/s] 34%|███▍      | 200/585 [01:35<01:51,  3.45it/s] 34%|███▍      | 201/585 [01:35<01:54,  3.35it/s] 35%|███▍      | 202/585 [01:36<01:53,  3.38it/s] 35%|███▍      | 203/585 [01:36<01:52,  3.40it/s] 35%|███▍      | 204/585 [01:36<01:51,  3.42it/s] 35%|███▌      | 205/585 [01:36<01:50,  3.43it/s] 35%|███▌      | 206/585 [01:37<01:50,  3.43it/s] 35%|███▌      | 207/585 [01:37<01:50,  3.44it/s] 36%|███▌      | 208/585 [01:37<01:49,  3.44it/s] 36%|███▌      | 209/585 [01:38<01:49,  3.44it/s] 36%|███▌      | 210/585 [01:38<01:48,  3.44it/s] 36%|███▌      | 211/585 [01:38<01:48,  3.45it/s] 36%|███▌      | 212/585 [01:38<01:51,  3.34it/s] 36%|███▋      | 213/585 [01:39<01:50,  3.37it/s] 37%|███▋      | 214/585 [01:39<01:49,  3.40it/s] 37%|███▋      | 215/585 [01:39<01:48,  3.41it/s] 37%|███▋      | 216/585 [01:40<01:47,  3.42it/s] 37%|███▋      | 217/585 [01:40<01:47,  3.43it/s] 37%|███▋      | 218/585 [01:40<01:46,  3.44it/s] 37%|███▋      | 219/585 [01:40<01:46,  3.44it/s] 38%|███▊      | 220/585 [01:41<01:45,  3.45it/s] 38%|███▊      | 221/585 [01:41<01:45,  3.45it/s] 38%|███▊      | 222/585 [01:41<01:47,  3.38it/s] 38%|███▊      | 223/585 [01:42<01:49,  3.31it/s] 38%|███▊      | 224/585 [01:42<01:47,  3.35it/s] 38%|███▊      | 225/585 [01:42<01:46,  3.38it/s] 39%|███▊      | 226/585 [01:43<01:45,  3.40it/s] 39%|███▉      | 227/585 [01:43<01:44,  3.41it/s] 39%|███▉      | 228/585 [01:43<01:44,  3.42it/s] 39%|███▉      | 229/585 [01:43<01:43,  3.43it/s] 39%|███▉      | 230/585 [01:44<01:43,  3.44it/s] 39%|███▉      | 231/585 [01:44<01:42,  3.44it/s] 40%|███▉      | 232/585 [01:44<01:54,  3.08it/s] 40%|███▉      | 233/585 [01:45<02:04,  2.83it/s] 40%|████      | 234/585 [01:45<01:57,  2.99it/s][INFO|trainer.py:2140] 2023-08-29 12:12:28,109 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:12:28,109 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 12:12:28,109 >>   Batch size = 8
{'eval_loss': 0.9560420513153076, 'eval_runtime': 24.4763, 'eval_samples_per_second': 350.054, 'eval_steps_per_second': 43.757, 'epoch': 1.0}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:19, 55.64it/s][A
  1%|          | 12/1071 [00:00<00:21, 49.08it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 47.31it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 46.44it/s][A
  3%|▎         | 27/1071 [00:00<00:22, 45.68it/s][A
  3%|▎         | 32/1071 [00:00<00:22, 45.24it/s][A
  3%|▎         | 37/1071 [00:00<00:22, 45.04it/s][A
  4%|▍         | 42/1071 [00:00<00:22, 44.79it/s][A
  4%|▍         | 47/1071 [00:01<00:22, 44.81it/s][A
  5%|▍         | 52/1071 [00:01<00:22, 44.83it/s][A
  5%|▌         | 57/1071 [00:01<00:22, 44.94it/s][A
  6%|▌         | 62/1071 [00:01<00:25, 39.81it/s][A
  6%|▋         | 67/1071 [00:01<00:24, 41.37it/s][A
  7%|▋         | 72/1071 [00:01<00:23, 42.51it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 43.41it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 43.82it/s][A
  8%|▊         | 87/1071 [00:01<00:22, 44.16it/s][A
  9%|▊         | 92/1071 [00:02<00:22, 44.35it/s][A
  9%|▉         | 97/1071 [00:02<00:21, 44.37it/s][A
 10%|▉         | 102/1071 [00:02<00:21, 44.12it/s][A
 10%|▉         | 107/1071 [00:02<00:21, 44.14it/s][A
 10%|█         | 112/1071 [00:02<00:21, 44.42it/s][A
 11%|█         | 117/1071 [00:02<00:21, 44.53it/s][A
 11%|█▏        | 122/1071 [00:02<00:21, 44.77it/s][A
 12%|█▏        | 127/1071 [00:02<00:21, 44.83it/s][A
 12%|█▏        | 132/1071 [00:02<00:20, 44.93it/s][A
 13%|█▎        | 137/1071 [00:03<00:20, 44.91it/s][A
 13%|█▎        | 142/1071 [00:03<00:20, 44.91it/s][A
 14%|█▎        | 147/1071 [00:03<00:20, 44.86it/s][A
 14%|█▍        | 152/1071 [00:03<00:20, 44.76it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 44.89it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 44.79it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 44.83it/s][A
 16%|█▌        | 172/1071 [00:03<00:19, 44.99it/s][A
 17%|█▋        | 177/1071 [00:03<00:19, 44.93it/s][A
 17%|█▋        | 182/1071 [00:04<00:19, 44.91it/s][A
 17%|█▋        | 187/1071 [00:04<00:19, 44.83it/s][A
 18%|█▊        | 192/1071 [00:04<00:19, 44.84it/s][A
 18%|█▊        | 197/1071 [00:04<00:20, 42.80it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 43.58it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 43.86it/s][A
 20%|█▉        | 212/1071 [00:04<00:19, 44.21it/s][A
 20%|██        | 217/1071 [00:04<00:19, 44.47it/s][A
 21%|██        | 222/1071 [00:04<00:19, 44.53it/s][A
 21%|██        | 227/1071 [00:05<00:18, 44.65it/s][A
 22%|██▏       | 232/1071 [00:05<00:18, 44.66it/s][A
 22%|██▏       | 237/1071 [00:05<00:18, 44.48it/s][A
 23%|██▎       | 242/1071 [00:05<00:18, 44.63it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 44.73it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 44.75it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 44.89it/s][A
 24%|██▍       | 262/1071 [00:05<00:18, 44.74it/s][A
 25%|██▍       | 267/1071 [00:05<00:17, 44.78it/s][A
 25%|██▌       | 272/1071 [00:06<00:17, 44.78it/s][A
 26%|██▌       | 277/1071 [00:06<00:17, 44.65it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 44.52it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 44.65it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 44.75it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 44.91it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 44.98it/s][A
 29%|██▊       | 307/1071 [00:06<00:16, 45.00it/s][A
 29%|██▉       | 312/1071 [00:06<00:16, 44.97it/s][A
 30%|██▉       | 317/1071 [00:07<00:16, 44.84it/s][A
 30%|███       | 322/1071 [00:07<00:16, 44.77it/s][A
 31%|███       | 327/1071 [00:07<00:16, 44.53it/s][A
 31%|███       | 332/1071 [00:07<00:16, 44.50it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 44.69it/s][A
 32%|███▏      | 342/1071 [00:07<00:16, 44.94it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 45.05it/s][A
 33%|███▎      | 352/1071 [00:07<00:16, 44.88it/s][A
 33%|███▎      | 357/1071 [00:08<00:15, 44.95it/s][A
 34%|███▍      | 362/1071 [00:08<00:15, 44.80it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.75it/s][A
 35%|███▍      | 372/1071 [00:08<00:15, 44.66it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 44.64it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 44.70it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 44.75it/s][A
 37%|███▋      | 392/1071 [00:08<00:15, 44.89it/s][A
 37%|███▋      | 397/1071 [00:08<00:15, 44.87it/s][A
 38%|███▊      | 402/1071 [00:09<00:14, 44.76it/s][A
 38%|███▊      | 407/1071 [00:09<00:14, 44.79it/s][A
 38%|███▊      | 412/1071 [00:09<00:14, 44.72it/s][A
 39%|███▉      | 417/1071 [00:09<00:14, 44.68it/s][A
 39%|███▉      | 422/1071 [00:09<00:14, 44.67it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 44.78it/s][A
 40%|████      | 432/1071 [00:09<00:14, 44.90it/s][A
 41%|████      | 437/1071 [00:09<00:14, 44.93it/s][A
 41%|████▏     | 442/1071 [00:09<00:14, 44.91it/s][A
 42%|████▏     | 447/1071 [00:10<00:13, 44.74it/s][A
 42%|████▏     | 452/1071 [00:10<00:13, 44.76it/s][A
 43%|████▎     | 457/1071 [00:10<00:13, 44.68it/s][A
 43%|████▎     | 462/1071 [00:10<00:13, 44.61it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 44.40it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 44.63it/s][A
 45%|████▍     | 477/1071 [00:10<00:13, 44.75it/s][A
 45%|████▌     | 482/1071 [00:10<00:13, 44.80it/s][A
 45%|████▌     | 487/1071 [00:10<00:13, 44.83it/s][A
 46%|████▌     | 492/1071 [00:11<00:12, 44.93it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.54it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.64it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 44.64it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 44.59it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 44.71it/s][A
 49%|████▊     | 522/1071 [00:11<00:12, 44.82it/s][A
 49%|████▉     | 527/1071 [00:11<00:12, 44.93it/s][A
 50%|████▉     | 532/1071 [00:11<00:12, 44.84it/s][A
 50%|█████     | 537/1071 [00:12<00:11, 44.82it/s][A
 51%|█████     | 542/1071 [00:12<00:11, 44.83it/s][A
 51%|█████     | 547/1071 [00:12<00:11, 44.63it/s][A
 52%|█████▏    | 552/1071 [00:12<00:11, 44.56it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 44.71it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 44.76it/s][A
 53%|█████▎    | 567/1071 [00:12<00:11, 44.84it/s][A
 53%|█████▎    | 572/1071 [00:12<00:11, 44.80it/s][A
 54%|█████▍    | 577/1071 [00:12<00:11, 44.74it/s][A
 54%|█████▍    | 582/1071 [00:13<00:10, 44.73it/s][A
 55%|█████▍    | 587/1071 [00:13<00:10, 44.61it/s][A
 55%|█████▌    | 592/1071 [00:13<00:10, 44.64it/s][A
 56%|█████▌    | 597/1071 [00:13<00:10, 44.54it/s][A
 56%|█████▌    | 602/1071 [00:13<00:10, 43.24it/s][A
 57%|█████▋    | 607/1071 [00:13<00:10, 43.87it/s][A
 57%|█████▋    | 612/1071 [00:13<00:10, 44.32it/s][A
 58%|█████▊    | 617/1071 [00:13<00:10, 44.51it/s][A
 58%|█████▊    | 622/1071 [00:13<00:10, 44.56it/s][A
 59%|█████▊    | 627/1071 [00:14<00:09, 44.47it/s][A
 59%|█████▉    | 632/1071 [00:14<00:09, 44.63it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.49it/s][A
 60%|█████▉    | 642/1071 [00:14<00:09, 43.45it/s][A
 60%|██████    | 647/1071 [00:14<00:09, 44.03it/s][A
 61%|██████    | 652/1071 [00:14<00:09, 44.43it/s][A
 61%|██████▏   | 657/1071 [00:14<00:09, 44.72it/s][A
 62%|██████▏   | 662/1071 [00:14<00:09, 44.80it/s][A
 62%|██████▏   | 667/1071 [00:14<00:09, 44.74it/s][A
 63%|██████▎   | 672/1071 [00:15<00:08, 44.69it/s][A
 63%|██████▎   | 677/1071 [00:15<00:08, 44.70it/s][A
 64%|██████▎   | 682/1071 [00:15<00:08, 44.46it/s][A
 64%|██████▍   | 687/1071 [00:15<00:08, 44.39it/s][A
 65%|██████▍   | 692/1071 [00:15<00:08, 44.57it/s][A
 65%|██████▌   | 697/1071 [00:15<00:08, 44.77it/s][A
 66%|██████▌   | 702/1071 [00:15<00:08, 44.83it/s][A
 66%|██████▌   | 707/1071 [00:15<00:08, 44.88it/s][A
 66%|██████▋   | 712/1071 [00:15<00:07, 44.91it/s][A
 67%|██████▋   | 717/1071 [00:16<00:07, 44.84it/s][A
 67%|██████▋   | 722/1071 [00:16<00:07, 44.73it/s][A
 68%|██████▊   | 727/1071 [00:16<00:07, 44.58it/s][A
 68%|██████▊   | 732/1071 [00:16<00:07, 44.49it/s][A
 69%|██████▉   | 737/1071 [00:16<00:07, 42.55it/s][A
 69%|██████▉   | 742/1071 [00:16<00:07, 43.38it/s][A
 70%|██████▉   | 747/1071 [00:16<00:07, 43.95it/s][A
 70%|███████   | 752/1071 [00:16<00:07, 44.36it/s][A
 71%|███████   | 757/1071 [00:16<00:07, 44.48it/s][A
 71%|███████   | 762/1071 [00:17<00:06, 44.46it/s][A
 72%|███████▏  | 767/1071 [00:17<00:06, 44.47it/s][A
 72%|███████▏  | 772/1071 [00:17<00:06, 44.55it/s][A
 73%|███████▎  | 777/1071 [00:17<00:06, 44.22it/s][A
 73%|███████▎  | 782/1071 [00:17<00:06, 44.36it/s][A
 73%|███████▎  | 787/1071 [00:17<00:06, 44.65it/s][A
 74%|███████▍  | 792/1071 [00:17<00:06, 44.86it/s][A
 74%|███████▍  | 797/1071 [00:17<00:06, 44.97it/s][A
 75%|███████▍  | 802/1071 [00:17<00:05, 45.09it/s][A
 75%|███████▌  | 807/1071 [00:18<00:05, 44.95it/s][A
 76%|███████▌  | 812/1071 [00:18<00:05, 44.82it/s][A
 76%|███████▋  | 817/1071 [00:18<00:05, 44.70it/s][A
 77%|███████▋  | 822/1071 [00:18<00:05, 44.47it/s][A
 77%|███████▋  | 827/1071 [00:18<00:05, 44.43it/s][A
 78%|███████▊  | 832/1071 [00:18<00:05, 44.55it/s][A
 78%|███████▊  | 837/1071 [00:18<00:05, 44.76it/s][A
 79%|███████▊  | 842/1071 [00:18<00:05, 44.73it/s][A
 79%|███████▉  | 847/1071 [00:18<00:04, 44.86it/s][A
 80%|███████▉  | 852/1071 [00:19<00:04, 44.83it/s][A
 80%|████████  | 857/1071 [00:19<00:04, 44.84it/s][A
 80%|████████  | 862/1071 [00:19<00:04, 44.58it/s][A
 81%|████████  | 867/1071 [00:19<00:04, 44.48it/s][A
 81%|████████▏ | 872/1071 [00:19<00:04, 41.30it/s][A
 82%|████████▏ | 877/1071 [00:19<00:04, 42.47it/s][A
 82%|████████▏ | 882/1071 [00:19<00:04, 43.37it/s][A
 83%|████████▎ | 887/1071 [00:19<00:04, 43.98it/s][A
 83%|████████▎ | 892/1071 [00:20<00:04, 44.36it/s][A
 84%|████████▍ | 897/1071 [00:20<00:03, 44.54it/s][A
 84%|████████▍ | 902/1071 [00:20<00:03, 44.63it/s][A
 85%|████████▍ | 907/1071 [00:20<00:03, 44.46it/s][A
 85%|████████▌ | 912/1071 [00:20<00:03, 44.24it/s][A
 86%|████████▌ | 917/1071 [00:20<00:03, 44.17it/s][A
 86%|████████▌ | 922/1071 [00:20<00:03, 44.51it/s][A
 87%|████████▋ | 927/1071 [00:20<00:03, 44.75it/s][A
 87%|████████▋ | 932/1071 [00:20<00:03, 44.95it/s][A
 87%|████████▋ | 937/1071 [00:21<00:02, 45.00it/s][A
 88%|████████▊ | 942/1071 [00:21<00:02, 45.08it/s][A
 88%|████████▊ | 947/1071 [00:21<00:02, 44.82it/s][A
 89%|████████▉ | 952/1071 [00:21<00:02, 44.65it/s][A
 89%|████████▉ | 957/1071 [00:21<00:02, 44.44it/s][A
 90%|████████▉ | 962/1071 [00:21<00:02, 44.34it/s][A
 90%|█████████ | 967/1071 [00:21<00:02, 44.60it/s][A
 91%|█████████ | 972/1071 [00:21<00:02, 44.90it/s][A
 91%|█████████ | 977/1071 [00:21<00:02, 45.02it/s][A
 92%|█████████▏| 982/1071 [00:22<00:01, 45.12it/s][A
 92%|█████████▏| 987/1071 [00:22<00:01, 45.02it/s][A
 93%|█████████▎| 992/1071 [00:22<00:01, 44.88it/s][A
 93%|█████████▎| 997/1071 [00:22<00:01, 44.66it/s][A
 94%|█████████▎| 1002/1071 [00:22<00:01, 44.36it/s][A
 94%|█████████▍| 1007/1071 [00:22<00:01, 42.68it/s][A
 94%|█████████▍| 1012/1071 [00:22<00:01, 43.48it/s][A
 95%|█████████▍| 1017/1071 [00:22<00:01, 44.10it/s][A
 95%|█████████▌| 1022/1071 [00:22<00:01, 44.49it/s][A
 96%|█████████▌| 1027/1071 [00:23<00:00, 44.68it/s][A
 96%|█████████▋| 1032/1071 [00:23<00:00, 44.75it/s][A
 97%|█████████▋| 1037/1071 [00:23<00:00, 44.55it/s][A
 97%|█████████▋| 1042/1071 [00:23<00:00, 44.26it/s][A
 98%|█████████▊| 1047/1071 [00:23<00:00, 44.17it/s][A
 98%|█████████▊| 1052/1071 [00:23<00:00, 44.20it/s][A
 99%|█████████▊| 1057/1071 [00:23<00:00, 44.47it/s][A
 99%|█████████▉| 1062/1071 [00:23<00:00, 44.67it/s][A
100%|█████████▉| 1067/1071 [00:23<00:00, 44.79it/s][A
                                                   [A                                                 
100%|██████████| 1071/1071 [00:24<00:00, 44.79it/s][A 40%|████      | 234/585 [02:09<01:57,  2.99it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-29 12:12:52,450 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 12:12:52,599 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:12:57,139 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:12:57,406 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:12:57,578 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:24<1:09:55, 11.99s/it] 40%|████      | 236/585 [02:25<49:22,  8.49s/it]   41%|████      | 237/585 [02:25<34:58,  6.03s/it] 41%|████      | 238/585 [02:25<24:55,  4.31s/it] 41%|████      | 239/585 [02:25<17:53,  3.10s/it] 41%|████      | 240/585 [02:26<12:59,  2.26s/it] 41%|████      | 241/585 [02:26<09:34,  1.67s/it] 41%|████▏     | 242/585 [02:26<07:11,  1.26s/it] 42%|████▏     | 243/585 [02:27<05:30,  1.03it/s] 42%|████▏     | 244/585 [02:27<04:20,  1.31it/s] 42%|████▏     | 245/585 [02:27<03:32,  1.60it/s] 42%|████▏     | 246/585 [02:28<02:57,  1.91it/s] 42%|████▏     | 247/585 [02:28<02:37,  2.15it/s] 42%|████▏     | 248/585 [02:28<02:19,  2.42it/s] 43%|████▎     | 249/585 [02:28<02:06,  2.65it/s] 43%|████▎     | 250/585 [02:29<01:57,  2.84it/s] 43%|████▎     | 251/585 [02:29<01:51,  2.99it/s] 43%|████▎     | 252/585 [02:29<01:47,  3.11it/s] 43%|████▎     | 253/585 [02:30<01:43,  3.19it/s] 43%|████▎     | 254/585 [02:30<01:41,  3.26it/s] 44%|████▎     | 255/585 [02:30<01:39,  3.30it/s] 44%|████▍     | 256/585 [02:31<01:38,  3.33it/s] 44%|████▍     | 257/585 [02:31<01:37,  3.36it/s] 44%|████▍     | 258/585 [02:31<01:41,  3.23it/s] 44%|████▍     | 259/585 [02:31<01:39,  3.28it/s] 44%|████▍     | 260/585 [02:32<01:37,  3.32it/s] 45%|████▍     | 261/585 [02:32<01:36,  3.34it/s] 45%|████▍     | 262/585 [02:32<01:36,  3.36it/s] 45%|████▍     | 263/585 [02:33<01:35,  3.37it/s] 45%|████▌     | 264/585 [02:33<01:34,  3.39it/s] 45%|████▌     | 265/585 [02:33<01:34,  3.39it/s] 45%|████▌     | 266/585 [02:33<01:33,  3.40it/s] 46%|████▌     | 267/585 [02:34<01:33,  3.40it/s] 46%|████▌     | 268/585 [02:34<01:33,  3.40it/s] 46%|████▌     | 269/585 [02:34<01:36,  3.28it/s] 46%|████▌     | 270/585 [02:35<01:34,  3.32it/s] 46%|████▋     | 271/585 [02:35<01:33,  3.35it/s] 46%|████▋     | 272/585 [02:35<01:32,  3.37it/s] 47%|████▋     | 273/585 [02:36<01:32,  3.38it/s] 47%|████▋     | 274/585 [02:36<01:31,  3.39it/s] 47%|████▋     | 275/585 [02:36<01:31,  3.40it/s] 47%|████▋     | 276/585 [02:36<01:30,  3.41it/s] 47%|████▋     | 277/585 [02:37<01:30,  3.41it/s] 48%|████▊     | 278/585 [02:37<01:30,  3.41it/s] 48%|████▊     | 279/585 [02:37<01:29,  3.41it/s] 48%|████▊     | 280/585 [02:38<01:33,  3.25it/s] 48%|████▊     | 281/585 [02:38<01:32,  3.30it/s] 48%|████▊     | 282/585 [02:38<01:30,  3.33it/s] 48%|████▊     | 283/585 [02:39<01:29,  3.36it/s] 49%|████▊     | 284/585 [02:39<01:29,  3.37it/s] 49%|████▊     | 285/585 [02:39<01:28,  3.38it/s] 49%|████▉     | 286/585 [02:39<01:28,  3.39it/s] 49%|████▉     | 287/585 [02:40<01:27,  3.40it/s] 49%|████▉     | 288/585 [02:40<01:27,  3.41it/s] 49%|████▉     | 289/585 [02:40<01:26,  3.40it/s] 50%|████▉     | 290/585 [02:41<01:26,  3.41it/s] 50%|████▉     | 291/585 [02:41<01:28,  3.31it/s] 50%|████▉     | 292/585 [02:41<01:27,  3.34it/s] 50%|█████     | 293/585 [02:42<01:29,  3.27it/s] 50%|█████     | 294/585 [02:42<01:27,  3.31it/s] 50%|█████     | 295/585 [02:42<01:26,  3.34it/s] 51%|█████     | 296/585 [02:42<01:26,  3.36it/s] 51%|█████     | 297/585 [02:43<01:25,  3.37it/s] 51%|█████     | 298/585 [02:43<01:24,  3.38it/s] 51%|█████     | 299/585 [02:43<01:24,  3.39it/s] 51%|█████▏    | 300/585 [02:44<01:23,  3.39it/s] 51%|█████▏    | 301/585 [02:44<01:23,  3.40it/s] 52%|█████▏    | 302/585 [02:44<01:25,  3.30it/s] 52%|█████▏    | 303/585 [02:45<01:33,  3.00it/s] 52%|█████▏    | 304/585 [02:45<01:42,  2.73it/s] 52%|█████▏    | 305/585 [02:45<01:36,  2.90it/s] 52%|█████▏    | 306/585 [02:46<01:31,  3.04it/s] 52%|█████▏    | 307/585 [02:46<01:28,  3.14it/s] 53%|█████▎    | 308/585 [02:46<01:26,  3.21it/s] 53%|█████▎    | 309/585 [02:47<01:24,  3.27it/s] 53%|█████▎    | 310/585 [02:47<01:23,  3.31it/s] 53%|█████▎    | 311/585 [02:47<01:22,  3.34it/s] 53%|█████▎    | 312/585 [02:47<01:23,  3.26it/s] 54%|█████▎    | 313/585 [02:48<01:22,  3.30it/s] 54%|█████▎    | 314/585 [02:48<01:21,  3.33it/s] 54%|█████▍    | 315/585 [02:48<01:20,  3.36it/s] 54%|█████▍    | 316/585 [02:49<01:19,  3.37it/s] 54%|█████▍    | 317/585 [02:49<01:19,  3.38it/s] 54%|█████▍    | 318/585 [02:49<01:18,  3.39it/s] 55%|█████▍    | 319/585 [02:49<01:18,  3.40it/s] 55%|█████▍    | 320/585 [02:50<01:18,  3.40it/s] 55%|█████▍    | 321/585 [02:50<01:17,  3.40it/s] 55%|█████▌    | 322/585 [02:50<01:17,  3.40it/s] 55%|█████▌    | 323/585 [02:51<01:17,  3.40it/s] 55%|█████▌    | 324/585 [02:51<01:16,  3.40it/s] 56%|█████▌    | 325/585 [02:51<01:17,  3.35it/s] 56%|█████▌    | 326/585 [02:52<01:17,  3.36it/s] 56%|█████▌    | 327/585 [02:52<01:16,  3.38it/s] 56%|█████▌    | 328/585 [02:52<01:15,  3.39it/s] 56%|█████▌    | 329/585 [02:52<01:15,  3.39it/s] 56%|█████▋    | 330/585 [02:53<01:15,  3.39it/s] 57%|█████▋    | 331/585 [02:53<01:14,  3.40it/s] 57%|█████▋    | 332/585 [02:53<01:14,  3.40it/s] 57%|█████▋    | 333/585 [02:54<01:14,  3.40it/s] 57%|█████▋    | 334/585 [02:54<01:13,  3.40it/s] 57%|█████▋    | 335/585 [02:54<01:13,  3.40it/s] 57%|█████▋    | 336/585 [02:55<01:15,  3.29it/s] 58%|█████▊    | 337/585 [02:55<01:14,  3.33it/s] 58%|█████▊    | 338/585 [02:55<01:13,  3.35it/s] 58%|█████▊    | 339/585 [02:55<01:13,  3.36it/s] 58%|█████▊    | 340/585 [02:56<01:12,  3.38it/s] 58%|█████▊    | 341/585 [02:56<01:12,  3.38it/s] 58%|█████▊    | 342/585 [02:56<01:11,  3.39it/s] 59%|█████▊    | 343/585 [02:57<01:11,  3.40it/s] 59%|█████▉    | 344/585 [02:57<01:10,  3.40it/s] 59%|█████▉    | 345/585 [02:57<01:10,  3.41it/s] 59%|█████▉    | 346/585 [02:57<01:10,  3.41it/s] 59%|█████▉    | 347/585 [02:58<01:13,  3.22it/s] 59%|█████▉    | 348/585 [02:58<01:12,  3.28it/s] 60%|█████▉    | 349/585 [02:58<01:11,  3.32it/s] 60%|█████▉    | 350/585 [02:59<01:10,  3.34it/s] 60%|██████    | 351/585 [02:59<01:09,  3.37it/s][INFO|trainer.py:2140] 2023-08-29 12:13:41,987 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:13:41,987 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 12:13:41,987 >>   Batch size = 8
{'eval_loss': 0.9596056938171387, 'eval_runtime': 24.0753, 'eval_samples_per_second': 355.884, 'eval_steps_per_second': 44.485, 'epoch': 2.0}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:18, 56.05it/s][A
  1%|          | 12/1071 [00:00<00:21, 48.80it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 47.25it/s][A
  2%|▏         | 22/1071 [00:00<00:23, 44.30it/s][A
  3%|▎         | 27/1071 [00:00<00:23, 44.97it/s][A
  3%|▎         | 32/1071 [00:00<00:23, 44.96it/s][A
  3%|▎         | 37/1071 [00:00<00:23, 44.78it/s][A
  4%|▍         | 42/1071 [00:00<00:22, 44.76it/s][A
  4%|▍         | 47/1071 [00:01<00:22, 44.83it/s][A
  5%|▍         | 52/1071 [00:01<00:22, 44.90it/s][A
  5%|▌         | 57/1071 [00:01<00:22, 45.03it/s][A
  6%|▌         | 62/1071 [00:01<00:22, 45.05it/s][A
  6%|▋         | 67/1071 [00:01<00:22, 44.80it/s][A
  7%|▋         | 72/1071 [00:01<00:23, 42.16it/s][A
  7%|▋         | 77/1071 [00:01<00:23, 43.09it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 43.42it/s][A
  8%|▊         | 87/1071 [00:01<00:22, 43.89it/s][A
  9%|▊         | 92/1071 [00:02<00:22, 44.23it/s][A
  9%|▉         | 97/1071 [00:02<00:21, 44.54it/s][A
 10%|▉         | 102/1071 [00:02<00:21, 44.71it/s][A
 10%|▉         | 107/1071 [00:02<00:21, 44.73it/s][A
 10%|█         | 112/1071 [00:02<00:21, 44.50it/s][A
 11%|█         | 117/1071 [00:02<00:21, 44.47it/s][A
 11%|█▏        | 122/1071 [00:02<00:21, 44.63it/s][A
 12%|█▏        | 127/1071 [00:02<00:21, 44.71it/s][A
 12%|█▏        | 132/1071 [00:02<00:20, 44.78it/s][A
 13%|█▎        | 137/1071 [00:03<00:20, 44.80it/s][A
 13%|█▎        | 142/1071 [00:03<00:20, 44.86it/s][A
 14%|█▎        | 147/1071 [00:03<00:20, 45.01it/s][A
 14%|█▍        | 152/1071 [00:03<00:20, 44.88it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 44.70it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 44.65it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 44.63it/s][A
 16%|█▌        | 172/1071 [00:03<00:20, 44.90it/s][A
 17%|█▋        | 177/1071 [00:03<00:19, 44.82it/s][A
 17%|█▋        | 182/1071 [00:04<00:19, 44.95it/s][A
 17%|█▋        | 187/1071 [00:04<00:19, 44.99it/s][A
 18%|█▊        | 192/1071 [00:04<00:19, 45.09it/s][A
 18%|█▊        | 197/1071 [00:04<00:19, 44.92it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 44.79it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 43.29it/s][A
 20%|█▉        | 212/1071 [00:04<00:19, 43.80it/s][A
 20%|██        | 217/1071 [00:04<00:19, 43.94it/s][A
 21%|██        | 222/1071 [00:04<00:19, 44.37it/s][A
 21%|██        | 227/1071 [00:05<00:18, 44.65it/s][A
 22%|██▏       | 232/1071 [00:05<00:18, 44.80it/s][A
 22%|██▏       | 237/1071 [00:05<00:18, 44.86it/s][A
 23%|██▎       | 242/1071 [00:05<00:18, 44.73it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 44.38it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 44.50it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 44.58it/s][A
 24%|██▍       | 262/1071 [00:05<00:18, 44.72it/s][A
 25%|██▍       | 267/1071 [00:05<00:17, 44.69it/s][A
 25%|██▌       | 272/1071 [00:06<00:17, 44.88it/s][A
 26%|██▌       | 277/1071 [00:06<00:17, 44.82it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 44.75it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 44.58it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 44.57it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 44.68it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 44.72it/s][A
 29%|██▊       | 307/1071 [00:06<00:17, 44.90it/s][A
 29%|██▉       | 312/1071 [00:06<00:16, 44.96it/s][A
 30%|██▉       | 317/1071 [00:07<00:16, 45.00it/s][A
 30%|███       | 322/1071 [00:07<00:16, 45.00it/s][A
 31%|███       | 327/1071 [00:07<00:16, 44.68it/s][A
 31%|███       | 332/1071 [00:07<00:16, 44.63it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 44.68it/s][A
 32%|███▏      | 342/1071 [00:07<00:16, 43.16it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 43.86it/s][A
 33%|███▎      | 352/1071 [00:07<00:16, 44.27it/s][A
 33%|███▎      | 357/1071 [00:07<00:16, 44.49it/s][A
 34%|███▍      | 362/1071 [00:08<00:15, 44.72it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.74it/s][A
 35%|███▍      | 372/1071 [00:08<00:15, 44.62it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 44.55it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 44.33it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 44.42it/s][A
 37%|███▋      | 392/1071 [00:08<00:15, 44.72it/s][A
 37%|███▋      | 397/1071 [00:08<00:15, 44.85it/s][A
 38%|███▊      | 402/1071 [00:08<00:14, 44.91it/s][A
 38%|███▊      | 407/1071 [00:09<00:14, 44.75it/s][A
 38%|███▊      | 412/1071 [00:09<00:14, 44.84it/s][A
 39%|███▉      | 417/1071 [00:09<00:14, 44.79it/s][A
 39%|███▉      | 422/1071 [00:09<00:14, 44.59it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 44.42it/s][A
 40%|████      | 432/1071 [00:09<00:14, 44.50it/s][A
 41%|████      | 437/1071 [00:09<00:14, 44.60it/s][A
 41%|████▏     | 442/1071 [00:09<00:14, 44.84it/s][A
 42%|████▏     | 447/1071 [00:10<00:13, 45.00it/s][A
 42%|████▏     | 452/1071 [00:10<00:13, 45.11it/s][A
 43%|████▎     | 457/1071 [00:10<00:13, 45.00it/s][A
 43%|████▎     | 462/1071 [00:10<00:13, 44.97it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 44.71it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 44.39it/s][A
 45%|████▍     | 477/1071 [00:10<00:13, 43.04it/s][A
 45%|████▌     | 482/1071 [00:10<00:13, 43.76it/s][A
 45%|████▌     | 487/1071 [00:10<00:13, 44.18it/s][A
 46%|████▌     | 492/1071 [00:11<00:12, 44.63it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.74it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.78it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 44.61it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 44.50it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 44.37it/s][A
 49%|████▊     | 522/1071 [00:11<00:12, 44.49it/s][A
 49%|████▉     | 527/1071 [00:11<00:12, 44.72it/s][A
 50%|████▉     | 532/1071 [00:11<00:12, 44.85it/s][A
 50%|█████     | 537/1071 [00:12<00:11, 44.99it/s][A
 51%|█████     | 542/1071 [00:12<00:11, 44.90it/s][A
 51%|█████     | 547/1071 [00:12<00:11, 44.90it/s][A
 52%|█████▏    | 552/1071 [00:12<00:11, 44.77it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 44.55it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 44.56it/s][A
 53%|█████▎    | 567/1071 [00:12<00:11, 44.40it/s][A
 53%|█████▎    | 572/1071 [00:12<00:11, 44.69it/s][A
 54%|█████▍    | 577/1071 [00:12<00:11, 44.80it/s][A
 54%|█████▍    | 582/1071 [00:13<00:10, 45.02it/s][A
 55%|█████▍    | 587/1071 [00:13<00:10, 44.99it/s][A
 55%|█████▌    | 592/1071 [00:13<00:10, 44.97it/s][A
 56%|█████▌    | 597/1071 [00:13<00:10, 44.81it/s][A
 56%|█████▌    | 602/1071 [00:13<00:10, 44.65it/s][A
 57%|█████▋    | 607/1071 [00:13<00:10, 44.70it/s][A
 57%|█████▋    | 612/1071 [00:13<00:11, 40.77it/s][A
 58%|█████▊    | 617/1071 [00:13<00:10, 42.10it/s][A
 58%|█████▊    | 622/1071 [00:13<00:10, 43.08it/s][A
 59%|█████▊    | 627/1071 [00:14<00:10, 43.70it/s][A
 59%|█████▉    | 632/1071 [00:14<00:09, 44.24it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.34it/s][A
 60%|█████▉    | 642/1071 [00:14<00:09, 44.25it/s][A
 60%|██████    | 647/1071 [00:14<00:09, 44.30it/s][A
 61%|██████    | 652/1071 [00:14<00:09, 44.13it/s][A
 61%|██████▏   | 657/1071 [00:14<00:09, 44.26it/s][A
 62%|██████▏   | 662/1071 [00:14<00:09, 44.54it/s][A
 62%|██████▏   | 667/1071 [00:14<00:09, 44.74it/s][A
 63%|██████▎   | 672/1071 [00:15<00:08, 44.93it/s][A
 63%|██████▎   | 677/1071 [00:15<00:08, 45.03it/s][A
 64%|██████▎   | 682/1071 [00:15<00:08, 45.05it/s][A
 64%|██████▍   | 687/1071 [00:15<00:08, 44.75it/s][A
 65%|██████▍   | 692/1071 [00:15<00:08, 44.43it/s][A
 65%|██████▌   | 697/1071 [00:15<00:08, 44.28it/s][A
 66%|██████▌   | 702/1071 [00:15<00:08, 44.40it/s][A
 66%|██████▌   | 707/1071 [00:15<00:08, 44.72it/s][A
 66%|██████▋   | 712/1071 [00:15<00:08, 44.82it/s][A
 67%|██████▋   | 717/1071 [00:16<00:07, 45.01it/s][A
 67%|██████▋   | 722/1071 [00:16<00:07, 45.16it/s][A
 68%|██████▊   | 727/1071 [00:16<00:07, 44.92it/s][A
 68%|██████▊   | 732/1071 [00:16<00:07, 44.70it/s][A
 69%|██████▉   | 737/1071 [00:16<00:07, 44.48it/s][A
 69%|██████▉   | 742/1071 [00:16<00:07, 44.39it/s][A
 70%|██████▉   | 747/1071 [00:16<00:07, 41.82it/s][A
 70%|███████   | 752/1071 [00:16<00:07, 42.91it/s][A
 71%|███████   | 757/1071 [00:17<00:07, 43.58it/s][A
 71%|███████   | 762/1071 [00:17<00:07, 44.13it/s][A
 72%|███████▏  | 767/1071 [00:17<00:06, 44.58it/s][A
 72%|███████▏  | 772/1071 [00:17<00:06, 44.70it/s][A
 73%|███████▎  | 777/1071 [00:17<00:06, 44.65it/s][A
 73%|███████▎  | 782/1071 [00:17<00:06, 44.53it/s][A
 73%|███████▎  | 787/1071 [00:17<00:06, 44.21it/s][A
 74%|███████▍  | 792/1071 [00:17<00:06, 44.31it/s][A
 74%|███████▍  | 797/1071 [00:17<00:06, 44.51it/s][A
 75%|███████▍  | 802/1071 [00:18<00:06, 44.66it/s][A
 75%|███████▌  | 807/1071 [00:18<00:05, 44.84it/s][A
 76%|███████▌  | 812/1071 [00:18<00:05, 44.92it/s][A
 76%|███████▋  | 817/1071 [00:18<00:05, 44.97it/s][A
 77%|███████▋  | 822/1071 [00:18<00:05, 44.97it/s][A
 77%|███████▋  | 827/1071 [00:18<00:05, 44.73it/s][A
 78%|███████▊  | 832/1071 [00:18<00:05, 44.62it/s][A
 78%|███████▊  | 837/1071 [00:18<00:05, 44.54it/s][A
 79%|███████▊  | 842/1071 [00:18<00:05, 44.71it/s][A
 79%|███████▉  | 847/1071 [00:19<00:05, 44.78it/s][A
 80%|███████▉  | 852/1071 [00:19<00:04, 44.98it/s][A
 80%|████████  | 857/1071 [00:19<00:04, 44.94it/s][A
 80%|████████  | 862/1071 [00:19<00:04, 45.00it/s][A
 81%|████████  | 867/1071 [00:19<00:04, 44.90it/s][A
 81%|████████▏ | 872/1071 [00:19<00:04, 44.64it/s][A
 82%|████████▏ | 877/1071 [00:19<00:04, 39.45it/s][A
 82%|████████▏ | 882/1071 [00:19<00:04, 41.05it/s][A
 83%|████████▎ | 887/1071 [00:19<00:04, 42.29it/s][A
 83%|████████▎ | 892/1071 [00:20<00:04, 43.21it/s][A
 84%|████████▍ | 897/1071 [00:20<00:03, 43.87it/s][A
 84%|████████▍ | 902/1071 [00:20<00:03, 44.32it/s][A
 85%|████████▍ | 907/1071 [00:20<00:03, 44.53it/s][A
 85%|████████▌ | 912/1071 [00:20<00:03, 44.56it/s][A
 86%|████████▌ | 917/1071 [00:20<00:03, 44.18it/s][A
 86%|████████▌ | 922/1071 [00:20<00:03, 44.03it/s][A
 87%|████████▋ | 927/1071 [00:20<00:03, 44.20it/s][A
 87%|████████▋ | 932/1071 [00:20<00:03, 44.45it/s][A
 87%|████████▋ | 937/1071 [00:21<00:02, 44.68it/s][A
 88%|████████▊ | 942/1071 [00:21<00:02, 44.71it/s][A
 88%|████████▊ | 947/1071 [00:21<00:02, 45.06it/s][A
 89%|████████▉ | 952/1071 [00:21<00:02, 45.11it/s][A
 89%|████████▉ | 957/1071 [00:21<00:02, 44.89it/s][A
 90%|████████▉ | 962/1071 [00:21<00:02, 44.52it/s][A
 90%|█████████ | 967/1071 [00:21<00:02, 44.55it/s][A
 91%|█████████ | 972/1071 [00:21<00:02, 44.44it/s][A
 91%|█████████ | 977/1071 [00:21<00:02, 44.67it/s][A
 92%|█████████▏| 982/1071 [00:22<00:01, 44.81it/s][A
 92%|█████████▏| 987/1071 [00:22<00:01, 44.47it/s][A
 93%|█████████▎| 992/1071 [00:22<00:01, 44.74it/s][A
 93%|█████████▎| 997/1071 [00:22<00:01, 44.94it/s][A
 94%|█████████▎| 1002/1071 [00:22<00:01, 44.84it/s][A
 94%|█████████▍| 1007/1071 [00:22<00:01, 44.61it/s][A
 94%|█████████▍| 1012/1071 [00:22<00:01, 40.36it/s][A
 95%|█████████▍| 1017/1071 [00:22<00:01, 41.78it/s][A
 95%|█████████▌| 1022/1071 [00:23<00:01, 42.76it/s][A
 96%|█████████▌| 1027/1071 [00:23<00:01, 43.54it/s][A
 96%|█████████▋| 1032/1071 [00:23<00:00, 44.15it/s][A
 97%|█████████▋| 1037/1071 [00:23<00:00, 44.46it/s][A
 97%|█████████▋| 1042/1071 [00:23<00:00, 44.61it/s][A
 98%|█████████▊| 1047/1071 [00:23<00:00, 44.56it/s][A
 98%|█████████▊| 1052/1071 [00:23<00:00, 44.18it/s][A
 99%|█████████▊| 1057/1071 [00:23<00:00, 44.10it/s][A
 99%|█████████▉| 1062/1071 [00:23<00:00, 44.32it/s][A
100%|█████████▉| 1067/1071 [00:24<00:00, 44.60it/s][A
                                                   [A                                                 
100%|██████████| 1071/1071 [00:24<00:00, 44.60it/s][A 60%|██████    | 351/585 [03:23<01:09,  3.37it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-29 12:14:06,637 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 12:14:07,052 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:14:10,572 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:14:10,789 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:14:10,891 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:35<42:49, 11.03s/it] 60%|██████    | 353/585 [03:35<30:13,  7.82s/it] 61%|██████    | 354/585 [03:36<21:24,  5.56s/it] 61%|██████    | 355/585 [03:36<15:15,  3.98s/it] 61%|██████    | 356/585 [03:36<10:57,  2.87s/it] 61%|██████    | 357/585 [03:37<07:58,  2.10s/it] 61%|██████    | 358/585 [03:37<05:53,  1.56s/it] 61%|██████▏   | 359/585 [03:37<04:26,  1.18s/it] 62%|██████▏   | 360/585 [03:37<03:25,  1.10it/s] 62%|██████▏   | 361/585 [03:38<02:42,  1.38it/s] 62%|██████▏   | 362/585 [03:38<02:12,  1.68it/s] 62%|██████▏   | 363/585 [03:38<01:52,  1.98it/s] 62%|██████▏   | 364/585 [03:39<01:38,  2.24it/s] 62%|██████▏   | 365/585 [03:39<01:28,  2.50it/s] 63%|██████▎   | 366/585 [03:39<01:20,  2.72it/s] 63%|██████▎   | 367/585 [03:39<01:15,  2.90it/s] 63%|██████▎   | 368/585 [03:40<01:11,  3.03it/s] 63%|██████▎   | 369/585 [03:40<01:08,  3.14it/s] 63%|██████▎   | 370/585 [03:40<01:06,  3.22it/s] 63%|██████▎   | 371/585 [03:41<01:05,  3.27it/s] 64%|██████▎   | 372/585 [03:41<01:04,  3.32it/s] 64%|██████▍   | 373/585 [03:41<01:03,  3.34it/s] 64%|██████▍   | 374/585 [03:42<01:04,  3.28it/s] 64%|██████▍   | 375/585 [03:42<01:04,  3.24it/s] 64%|██████▍   | 376/585 [03:42<01:03,  3.29it/s] 64%|██████▍   | 377/585 [03:42<01:02,  3.32it/s] 65%|██████▍   | 378/585 [03:43<01:01,  3.35it/s] 65%|██████▍   | 379/585 [03:43<01:01,  3.37it/s] 65%|██████▍   | 380/585 [03:43<01:00,  3.38it/s] 65%|██████▌   | 381/585 [03:44<01:00,  3.39it/s] 65%|██████▌   | 382/585 [03:44<00:59,  3.39it/s] 65%|██████▌   | 383/585 [03:44<00:59,  3.39it/s] 66%|██████▌   | 384/585 [03:45<01:05,  3.07it/s] 66%|██████▌   | 385/585 [03:45<01:05,  3.05it/s] 66%|██████▌   | 386/585 [03:45<01:09,  2.87it/s] 66%|██████▌   | 387/585 [03:46<01:05,  3.01it/s] 66%|██████▋   | 388/585 [03:46<01:03,  3.12it/s] 66%|██████▋   | 389/585 [03:46<01:01,  3.20it/s] 67%|██████▋   | 390/585 [03:47<00:59,  3.26it/s] 67%|██████▋   | 391/585 [03:47<00:58,  3.30it/s] 67%|██████▋   | 392/585 [03:47<00:57,  3.33it/s] 67%|██████▋   | 393/585 [03:47<00:57,  3.35it/s] 67%|██████▋   | 394/585 [03:48<00:56,  3.37it/s] 68%|██████▊   | 395/585 [03:48<00:57,  3.30it/s] 68%|██████▊   | 396/585 [03:48<00:56,  3.35it/s] 68%|██████▊   | 397/585 [03:49<00:55,  3.38it/s] 68%|██████▊   | 398/585 [03:49<00:54,  3.40it/s] 68%|██████▊   | 399/585 [03:49<00:54,  3.42it/s] 68%|██████▊   | 400/585 [03:49<00:53,  3.43it/s] 69%|██████▊   | 401/585 [03:50<00:53,  3.43it/s] 69%|██████▊   | 402/585 [03:50<00:53,  3.44it/s] 69%|██████▉   | 403/585 [03:50<00:52,  3.45it/s] 69%|██████▉   | 404/585 [03:51<00:52,  3.45it/s] 69%|██████▉   | 405/585 [03:51<00:52,  3.45it/s] 69%|██████▉   | 406/585 [03:51<00:51,  3.45it/s] 70%|██████▉   | 407/585 [03:52<00:51,  3.45it/s] 70%|██████▉   | 408/585 [03:52<00:51,  3.45it/s] 70%|██████▉   | 409/585 [03:52<00:50,  3.45it/s] 70%|███████   | 410/585 [03:52<00:50,  3.45it/s] 70%|███████   | 411/585 [03:53<00:53,  3.28it/s] 70%|███████   | 412/585 [03:53<00:51,  3.33it/s] 71%|███████   | 413/585 [03:53<00:51,  3.37it/s] 71%|███████   | 414/585 [03:54<00:50,  3.39it/s] 71%|███████   | 415/585 [03:54<00:49,  3.41it/s] 71%|███████   | 416/585 [03:54<00:49,  3.42it/s] 71%|███████▏  | 417/585 [03:54<00:48,  3.43it/s] 71%|███████▏  | 418/585 [03:55<00:48,  3.43it/s] 72%|███████▏  | 419/585 [03:55<00:48,  3.44it/s] 72%|███████▏  | 420/585 [03:55<00:47,  3.44it/s] 72%|███████▏  | 421/585 [03:56<00:47,  3.44it/s] 72%|███████▏  | 422/585 [03:56<00:48,  3.37it/s] 72%|███████▏  | 423/585 [03:56<00:47,  3.40it/s] 72%|███████▏  | 424/585 [03:57<00:47,  3.41it/s] 73%|███████▎  | 425/585 [03:57<00:46,  3.43it/s] 73%|███████▎  | 426/585 [03:57<00:46,  3.43it/s] 73%|███████▎  | 427/585 [03:57<00:45,  3.44it/s] 73%|███████▎  | 428/585 [03:58<00:45,  3.44it/s] 73%|███████▎  | 429/585 [03:58<00:45,  3.45it/s] 74%|███████▎  | 430/585 [03:58<00:44,  3.45it/s] 74%|███████▎  | 431/585 [03:59<00:44,  3.45it/s] 74%|███████▍  | 432/585 [03:59<00:44,  3.45it/s] 74%|███████▍  | 433/585 [03:59<00:44,  3.40it/s] 74%|███████▍  | 434/585 [03:59<00:44,  3.41it/s] 74%|███████▍  | 435/585 [04:00<00:43,  3.42it/s] 75%|███████▍  | 436/585 [04:00<00:43,  3.43it/s] 75%|███████▍  | 437/585 [04:00<00:43,  3.44it/s] 75%|███████▍  | 438/585 [04:01<00:42,  3.44it/s] 75%|███████▌  | 439/585 [04:01<00:42,  3.44it/s] 75%|███████▌  | 440/585 [04:01<00:42,  3.44it/s] 75%|███████▌  | 441/585 [04:01<00:41,  3.44it/s] 76%|███████▌  | 442/585 [04:02<00:41,  3.45it/s] 76%|███████▌  | 443/585 [04:02<00:41,  3.45it/s] 76%|███████▌  | 444/585 [04:02<00:41,  3.37it/s] 76%|███████▌  | 445/585 [04:03<00:41,  3.39it/s] 76%|███████▌  | 446/585 [04:03<00:40,  3.41it/s] 76%|███████▋  | 447/585 [04:03<00:40,  3.42it/s] 77%|███████▋  | 448/585 [04:04<00:40,  3.42it/s] 77%|███████▋  | 449/585 [04:04<00:39,  3.43it/s] 77%|███████▋  | 450/585 [04:04<00:39,  3.44it/s] 77%|███████▋  | 451/585 [04:04<00:38,  3.44it/s] 77%|███████▋  | 452/585 [04:05<00:38,  3.44it/s] 77%|███████▋  | 453/585 [04:05<00:38,  3.45it/s] 78%|███████▊  | 454/585 [04:05<00:38,  3.45it/s] 78%|███████▊  | 455/585 [04:06<00:38,  3.38it/s] 78%|███████▊  | 456/585 [04:06<00:37,  3.40it/s] 78%|███████▊  | 457/585 [04:06<00:37,  3.41it/s] 78%|███████▊  | 458/585 [04:06<00:37,  3.42it/s] 78%|███████▊  | 459/585 [04:07<00:36,  3.43it/s] 79%|███████▊  | 460/585 [04:07<00:36,  3.44it/s] 79%|███████▉  | 461/585 [04:07<00:36,  3.44it/s] 79%|███████▉  | 462/585 [04:08<00:35,  3.44it/s] 79%|███████▉  | 463/585 [04:08<00:35,  3.45it/s] 79%|███████▉  | 464/585 [04:08<00:35,  3.45it/s] 79%|███████▉  | 465/585 [04:08<00:34,  3.45it/s] 80%|███████▉  | 466/585 [04:09<00:36,  3.27it/s] 80%|███████▉  | 467/585 [04:09<00:35,  3.32it/s] 80%|████████  | 468/585 [04:09<00:34,  3.36it/s][INFO|trainer.py:2140] 2023-08-29 12:14:52,359 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:14:52,359 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 12:14:52,359 >>   Batch size = 8
{'eval_loss': 0.9704146385192871, 'eval_runtime': 24.1318, 'eval_samples_per_second': 355.05, 'eval_steps_per_second': 44.381, 'epoch': 3.0}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:19, 56.03it/s][A
  1%|          | 12/1071 [00:00<00:21, 48.99it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 47.34it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 46.29it/s][A
  3%|▎         | 27/1071 [00:00<00:22, 45.47it/s][A
  3%|▎         | 32/1071 [00:00<00:23, 45.06it/s][A
  3%|▎         | 37/1071 [00:00<00:23, 44.87it/s][A
  4%|▍         | 42/1071 [00:00<00:23, 44.64it/s][A
  4%|▍         | 47/1071 [00:01<00:22, 44.68it/s][A
  5%|▍         | 52/1071 [00:01<00:22, 44.93it/s][A
  5%|▌         | 57/1071 [00:01<00:22, 45.02it/s][A
  6%|▌         | 62/1071 [00:01<00:22, 45.04it/s][A
  6%|▋         | 67/1071 [00:01<00:22, 45.03it/s][A
  7%|▋         | 72/1071 [00:01<00:22, 44.83it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 44.85it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 44.59it/s][A
  8%|▊         | 87/1071 [00:01<00:22, 44.35it/s][A
  9%|▊         | 92/1071 [00:02<00:21, 44.59it/s][A
  9%|▉         | 97/1071 [00:02<00:23, 41.99it/s][A
 10%|▉         | 102/1071 [00:02<00:22, 43.06it/s][A
 10%|▉         | 107/1071 [00:02<00:22, 43.60it/s][A
 10%|█         | 112/1071 [00:02<00:21, 44.10it/s][A
 11%|█         | 117/1071 [00:02<00:21, 44.33it/s][A
 11%|█▏        | 122/1071 [00:02<00:21, 44.47it/s][A
 12%|█▏        | 127/1071 [00:02<00:21, 44.41it/s][A
 12%|█▏        | 132/1071 [00:02<00:21, 44.38it/s][A
 13%|█▎        | 137/1071 [00:03<00:21, 44.26it/s][A
 13%|█▎        | 142/1071 [00:03<00:20, 44.48it/s][A
 14%|█▎        | 147/1071 [00:03<00:20, 44.60it/s][A
 14%|█▍        | 152/1071 [00:03<00:20, 44.83it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 44.95it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 45.10it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 44.98it/s][A
 16%|█▌        | 172/1071 [00:03<00:20, 44.80it/s][A
 17%|█▋        | 177/1071 [00:03<00:20, 44.63it/s][A
 17%|█▋        | 182/1071 [00:04<00:19, 44.51it/s][A
 17%|█▋        | 187/1071 [00:04<00:19, 44.51it/s][A
 18%|█▊        | 192/1071 [00:04<00:19, 44.82it/s][A
 18%|█▊        | 197/1071 [00:04<00:19, 44.90it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 45.06it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 45.10it/s][A
 20%|█▉        | 212/1071 [00:04<00:19, 45.07it/s][A
 20%|██        | 217/1071 [00:04<00:19, 44.88it/s][A
 21%|██        | 222/1071 [00:04<00:19, 44.55it/s][A
 21%|██        | 227/1071 [00:05<00:18, 44.57it/s][A
 22%|██▏       | 232/1071 [00:05<00:21, 39.26it/s][A
 22%|██▏       | 237/1071 [00:05<00:20, 40.90it/s][A
 23%|██▎       | 242/1071 [00:05<00:19, 42.21it/s][A
 23%|██▎       | 247/1071 [00:05<00:19, 43.13it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 43.75it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 43.98it/s][A
 24%|██▍       | 262/1071 [00:05<00:18, 44.64it/s][A
 25%|██▍       | 267/1071 [00:06<00:18, 44.52it/s][A
 25%|██▌       | 272/1071 [00:06<00:18, 44.28it/s][A
 26%|██▌       | 277/1071 [00:06<00:18, 44.10it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 44.32it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 44.50it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 44.78it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 44.94it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 45.06it/s][A
 29%|██▊       | 307/1071 [00:06<00:16, 45.16it/s][A
 29%|██▉       | 312/1071 [00:07<00:16, 44.98it/s][A
 30%|██▉       | 317/1071 [00:07<00:16, 44.61it/s][A
 30%|███       | 322/1071 [00:07<00:16, 44.41it/s][A
 31%|███       | 327/1071 [00:07<00:16, 44.54it/s][A
 31%|███       | 332/1071 [00:07<00:16, 44.58it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 44.87it/s][A
 32%|███▏      | 342/1071 [00:07<00:16, 45.04it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 45.17it/s][A
 33%|███▎      | 352/1071 [00:07<00:15, 45.11it/s][A
 33%|███▎      | 357/1071 [00:08<00:15, 44.82it/s][A
 34%|███▍      | 362/1071 [00:08<00:15, 44.63it/s][A
 34%|███▍      | 367/1071 [00:08<00:16, 41.59it/s][A
 35%|███▍      | 372/1071 [00:08<00:16, 42.60it/s][A
 35%|███▌      | 377/1071 [00:08<00:16, 43.33it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 43.78it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 44.23it/s][A
 37%|███▋      | 392/1071 [00:08<00:15, 44.50it/s][A
 37%|███▋      | 397/1071 [00:08<00:15, 44.68it/s][A
 38%|███▊      | 402/1071 [00:09<00:14, 44.84it/s][A
 38%|███▊      | 407/1071 [00:09<00:14, 44.40it/s][A
 38%|███▊      | 412/1071 [00:09<00:14, 44.54it/s][A
 39%|███▉      | 417/1071 [00:09<00:14, 44.63it/s][A
 39%|███▉      | 422/1071 [00:09<00:14, 44.81it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 44.79it/s][A
 40%|████      | 432/1071 [00:09<00:14, 44.87it/s][A
 41%|████      | 437/1071 [00:09<00:14, 45.00it/s][A
 41%|████▏     | 442/1071 [00:09<00:13, 44.96it/s][A
 42%|████▏     | 447/1071 [00:10<00:13, 44.81it/s][A
 42%|████▏     | 452/1071 [00:10<00:13, 44.57it/s][A
 43%|████▎     | 457/1071 [00:10<00:13, 44.49it/s][A
 43%|████▎     | 462/1071 [00:10<00:13, 44.66it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 44.76it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 44.89it/s][A
 45%|████▍     | 477/1071 [00:10<00:15, 39.51it/s][A
 45%|████▌     | 482/1071 [00:10<00:14, 41.09it/s][A
 45%|████▌     | 487/1071 [00:10<00:13, 42.30it/s][A
 46%|████▌     | 492/1071 [00:11<00:13, 43.28it/s][A
 46%|████▋     | 497/1071 [00:11<00:13, 43.82it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.24it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 44.31it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 44.50it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 44.16it/s][A
 49%|████▊     | 522/1071 [00:11<00:12, 44.13it/s][A
 49%|████▉     | 527/1071 [00:11<00:12, 44.32it/s][A
 50%|████▉     | 532/1071 [00:11<00:12, 44.49it/s][A
 50%|█████     | 537/1071 [00:12<00:11, 44.74it/s][A
 51%|█████     | 542/1071 [00:12<00:11, 44.90it/s][A
 51%|█████     | 547/1071 [00:12<00:11, 45.12it/s][A
 52%|█████▏    | 552/1071 [00:12<00:11, 45.11it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 44.89it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 44.61it/s][A
 53%|█████▎    | 567/1071 [00:12<00:11, 44.41it/s][A
 53%|█████▎    | 572/1071 [00:12<00:11, 44.57it/s][A
 54%|█████▍    | 577/1071 [00:12<00:11, 44.77it/s][A
 54%|█████▍    | 582/1071 [00:13<00:10, 44.88it/s][A
 55%|█████▍    | 587/1071 [00:13<00:10, 44.86it/s][A
 55%|█████▌    | 592/1071 [00:13<00:10, 45.06it/s][A
 56%|█████▌    | 597/1071 [00:13<00:10, 45.09it/s][A
 56%|█████▌    | 602/1071 [00:13<00:10, 44.83it/s][A
 57%|█████▋    | 607/1071 [00:13<00:10, 44.71it/s][A
 57%|█████▋    | 612/1071 [00:13<00:11, 39.15it/s][A
 58%|█████▊    | 617/1071 [00:13<00:11, 40.88it/s][A
 58%|█████▊    | 622/1071 [00:14<00:10, 42.17it/s][A
 59%|█████▊    | 627/1071 [00:14<00:10, 43.06it/s][A
 59%|█████▉    | 632/1071 [00:14<00:10, 43.86it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.28it/s][A
 60%|█████▉    | 642/1071 [00:14<00:09, 44.61it/s][A
 60%|██████    | 647/1071 [00:14<00:09, 44.70it/s][A
 61%|██████    | 652/1071 [00:14<00:09, 44.40it/s][A
 61%|██████▏   | 657/1071 [00:14<00:09, 44.04it/s][A
 62%|██████▏   | 662/1071 [00:14<00:09, 44.22it/s][A
 62%|██████▏   | 667/1071 [00:15<00:09, 44.48it/s][A
 63%|██████▎   | 672/1071 [00:15<00:08, 44.72it/s][A
 63%|██████▎   | 677/1071 [00:15<00:08, 44.88it/s][A
 64%|██████▎   | 682/1071 [00:15<00:08, 45.01it/s][A
 64%|██████▍   | 687/1071 [00:15<00:08, 45.17it/s][A
 65%|██████▍   | 692/1071 [00:15<00:08, 44.94it/s][A
 65%|██████▌   | 697/1071 [00:15<00:08, 44.64it/s][A
 66%|██████▌   | 702/1071 [00:15<00:08, 44.44it/s][A
 66%|██████▌   | 707/1071 [00:15<00:08, 44.57it/s][A
 66%|██████▋   | 712/1071 [00:16<00:08, 44.48it/s][A
 67%|██████▋   | 717/1071 [00:16<00:07, 44.63it/s][A
 67%|██████▋   | 722/1071 [00:16<00:07, 44.92it/s][A
 68%|██████▊   | 727/1071 [00:16<00:07, 45.10it/s][A
 68%|██████▊   | 732/1071 [00:16<00:07, 45.02it/s][A
 69%|██████▉   | 737/1071 [00:16<00:07, 44.90it/s][A
 69%|██████▉   | 742/1071 [00:16<00:07, 44.50it/s][A
 70%|██████▉   | 747/1071 [00:16<00:07, 40.57it/s][A
 70%|███████   | 752/1071 [00:16<00:07, 41.93it/s][A
 71%|███████   | 757/1071 [00:17<00:07, 42.90it/s][A
 71%|███████   | 762/1071 [00:17<00:07, 43.63it/s][A
 72%|███████▏  | 767/1071 [00:17<00:06, 43.98it/s][A
 72%|███████▏  | 772/1071 [00:17<00:06, 44.46it/s][A
 73%|███████▎  | 777/1071 [00:17<00:06, 44.63it/s][A
 73%|███████▎  | 782/1071 [00:17<00:06, 44.53it/s][A
 73%|███████▎  | 787/1071 [00:17<00:06, 44.29it/s][A
 74%|███████▍  | 792/1071 [00:17<00:06, 44.17it/s][A
 74%|███████▍  | 797/1071 [00:17<00:06, 44.10it/s][A
 75%|███████▍  | 802/1071 [00:18<00:06, 44.52it/s][A
 75%|███████▌  | 807/1071 [00:18<00:05, 44.75it/s][A
 76%|███████▌  | 812/1071 [00:18<00:05, 45.07it/s][A
 76%|███████▋  | 817/1071 [00:18<00:05, 45.11it/s][A
 77%|███████▋  | 822/1071 [00:18<00:05, 45.07it/s][A
 77%|███████▋  | 827/1071 [00:18<00:05, 44.76it/s][A
 78%|███████▊  | 832/1071 [00:18<00:05, 44.51it/s][A
 78%|███████▊  | 837/1071 [00:18<00:05, 44.41it/s][A
 79%|███████▊  | 842/1071 [00:19<00:05, 44.41it/s][A
 79%|███████▉  | 847/1071 [00:19<00:05, 44.57it/s][A
 80%|███████▉  | 852/1071 [00:19<00:04, 44.83it/s][A
 80%|████████  | 857/1071 [00:19<00:04, 45.05it/s][A
 80%|████████  | 862/1071 [00:19<00:04, 45.06it/s][A
 81%|████████  | 867/1071 [00:19<00:04, 45.00it/s][A
 81%|████████▏ | 872/1071 [00:19<00:04, 44.87it/s][A
 82%|████████▏ | 877/1071 [00:19<00:04, 44.66it/s][A
 82%|████████▏ | 882/1071 [00:19<00:04, 41.23it/s][A
 83%|████████▎ | 887/1071 [00:20<00:04, 42.46it/s][A
 83%|████████▎ | 892/1071 [00:20<00:04, 43.18it/s][A
 84%|████████▍ | 897/1071 [00:20<00:03, 43.86it/s][A
 84%|████████▍ | 902/1071 [00:20<00:03, 44.34it/s][A
 85%|████████▍ | 907/1071 [00:20<00:03, 44.50it/s][A
 85%|████████▌ | 912/1071 [00:20<00:03, 44.62it/s][A
 86%|████████▌ | 917/1071 [00:20<00:03, 44.52it/s][A
 86%|████████▌ | 922/1071 [00:20<00:03, 44.17it/s][A
 87%|████████▋ | 927/1071 [00:20<00:03, 44.35it/s][A
 87%|████████▋ | 932/1071 [00:21<00:03, 44.39it/s][A
 87%|████████▋ | 937/1071 [00:21<00:03, 44.62it/s][A
 88%|████████▊ | 942/1071 [00:21<00:02, 44.84it/s][A
 88%|████████▊ | 947/1071 [00:21<00:02, 44.94it/s][A
 89%|████████▉ | 952/1071 [00:21<00:02, 45.15it/s][A
 89%|████████▉ | 957/1071 [00:21<00:02, 45.03it/s][A
 90%|████████▉ | 962/1071 [00:21<00:02, 44.70it/s][A
 90%|█████████ | 967/1071 [00:21<00:02, 44.53it/s][A
 91%|█████████ | 972/1071 [00:21<00:02, 44.49it/s][A
 91%|█████████ | 977/1071 [00:22<00:02, 44.66it/s][A
 92%|█████████▏| 982/1071 [00:22<00:01, 44.71it/s][A
 92%|█████████▏| 987/1071 [00:22<00:01, 44.99it/s][A
 93%|█████████▎| 992/1071 [00:22<00:01, 44.98it/s][A
 93%|█████████▎| 997/1071 [00:22<00:01, 44.97it/s][A
 94%|█████████▎| 1002/1071 [00:22<00:01, 44.93it/s][A
 94%|█████████▍| 1007/1071 [00:22<00:01, 44.74it/s][A
 94%|█████████▍| 1012/1071 [00:22<00:01, 44.51it/s][A
 95%|█████████▍| 1017/1071 [00:22<00:01, 41.87it/s][A
 95%|█████████▌| 1022/1071 [00:23<00:01, 42.84it/s][A
 96%|█████████▌| 1027/1071 [00:23<00:01, 43.69it/s][A
 96%|█████████▋| 1032/1071 [00:23<00:00, 44.19it/s][A
 97%|█████████▋| 1037/1071 [00:23<00:00, 44.52it/s][A
 97%|█████████▋| 1042/1071 [00:23<00:00, 44.59it/s][A
 98%|█████████▊| 1047/1071 [00:23<00:00, 44.75it/s][A
 98%|█████████▊| 1052/1071 [00:23<00:00, 44.49it/s][A
 99%|█████████▊| 1057/1071 [00:23<00:00, 44.12it/s][A
 99%|█████████▉| 1062/1071 [00:23<00:00, 44.10it/s][A
100%|█████████▉| 1067/1071 [00:24<00:00, 44.48it/s][A
                                                   [A                                                 
100%|██████████| 1071/1071 [00:24<00:00, 44.48it/s][A 80%|████████  | 468/585 [04:34<00:34,  3.36it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-29 12:15:16,842 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 12:15:17,019 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:15:20,595 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:15:20,748 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:15:20,842 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [04:46<21:51, 11.30s/it] 80%|████████  | 470/585 [04:47<15:21,  8.01s/it] 81%|████████  | 471/585 [04:47<10:49,  5.70s/it] 81%|████████  | 472/585 [04:47<07:40,  4.08s/it] 81%|████████  | 473/585 [04:48<05:29,  2.94s/it] 81%|████████  | 474/585 [04:48<03:58,  2.15s/it] 81%|████████  | 475/585 [04:48<02:55,  1.59s/it] 81%|████████▏ | 476/585 [04:48<02:10,  1.20s/it] 82%|████████▏ | 477/585 [04:49<01:40,  1.08it/s] 82%|████████▏ | 478/585 [04:49<01:18,  1.36it/s] 82%|████████▏ | 479/585 [04:49<01:03,  1.66it/s] 82%|████████▏ | 480/585 [04:50<00:53,  1.97it/s] 82%|████████▏ | 481/585 [04:50<00:47,  2.19it/s] 82%|████████▏ | 482/585 [04:50<00:41,  2.46it/s] 83%|████████▎ | 483/585 [04:51<00:37,  2.70it/s] 83%|████████▎ | 484/585 [04:51<00:34,  2.89it/s] 83%|████████▎ | 485/585 [04:51<00:32,  3.04it/s] 83%|████████▎ | 486/585 [04:51<00:31,  3.15it/s] 83%|████████▎ | 487/585 [04:52<00:30,  3.24it/s] 83%|████████▎ | 488/585 [04:52<00:29,  3.30it/s] 84%|████████▎ | 489/585 [04:52<00:28,  3.35it/s] 84%|████████▍ | 490/585 [04:53<00:28,  3.38it/s] 84%|████████▍ | 491/585 [04:53<00:27,  3.40it/s] 84%|████████▍ | 492/585 [04:53<00:27,  3.42it/s] 84%|████████▍ | 493/585 [04:53<00:26,  3.43it/s] 84%|████████▍ | 494/585 [04:54<00:26,  3.44it/s] 85%|████████▍ | 495/585 [04:54<00:26,  3.45it/s] 85%|████████▍ | 496/585 [04:54<00:26,  3.31it/s] 85%|████████▍ | 497/585 [04:55<00:26,  3.35it/s] 85%|████████▌ | 498/585 [04:55<00:25,  3.38it/s] 85%|████████▌ | 499/585 [04:55<00:25,  3.40it/s] 85%|████████▌ | 500/585 [04:55<00:24,  3.42it/s]                                                  85%|████████▌ | 500/585 [04:55<00:24,  3.42it/s] 86%|████████▌ | 501/585 [04:56<00:24,  3.43it/s] 86%|████████▌ | 502/585 [04:56<00:24,  3.44it/s] 86%|████████▌ | 503/585 [04:56<00:23,  3.45it/s] 86%|████████▌ | 504/585 [04:57<00:23,  3.45it/s] 86%|████████▋ | 505/585 [04:57<00:23,  3.45it/s] 86%|████████▋ | 506/585 [04:57<00:22,  3.45it/s] 87%|████████▋ | 507/585 [04:58<00:22,  3.39it/s] 87%|████████▋ | 508/585 [04:58<00:22,  3.41it/s] 87%|████████▋ | 509/585 [04:58<00:22,  3.43it/s] 87%|████████▋ | 510/585 [04:58<00:21,  3.43it/s] 87%|████████▋ | 511/585 [04:59<00:21,  3.44it/s] 88%|████████▊ | 512/585 [04:59<00:21,  3.44it/s] 88%|████████▊ | 513/585 [04:59<00:20,  3.45it/s] 88%|████████▊ | 514/585 [05:00<00:20,  3.45it/s] 88%|████████▊ | 515/585 [05:00<00:20,  3.45it/s] 88%|████████▊ | 516/585 [05:00<00:19,  3.45it/s] 88%|████████▊ | 517/585 [05:00<00:19,  3.45it/s] 89%|████████▊ | 518/585 [05:01<00:20,  3.32it/s] 89%|████████▊ | 519/585 [05:01<00:19,  3.36it/s] 89%|████████▉ | 520/585 [05:01<00:19,  3.39it/s] 89%|████████▉ | 521/585 [05:02<00:18,  3.41it/s] 89%|████████▉ | 522/585 [05:02<00:18,  3.42it/s] 89%|████████▉ | 523/585 [05:02<00:18,  3.43it/s] 90%|████████▉ | 524/585 [05:02<00:17,  3.44it/s] 90%|████████▉ | 525/585 [05:03<00:17,  3.45it/s] 90%|████████▉ | 526/585 [05:03<00:17,  3.45it/s] 90%|█████████ | 527/585 [05:03<00:16,  3.45it/s] 90%|█████████ | 528/585 [05:04<00:16,  3.45it/s] 90%|█████████ | 529/585 [05:04<00:16,  3.32it/s] 91%|█████████ | 530/585 [05:04<00:16,  3.36it/s] 91%|█████████ | 531/585 [05:05<00:15,  3.39it/s] 91%|█████████ | 532/585 [05:05<00:15,  3.41it/s] 91%|█████████ | 533/585 [05:05<00:15,  3.43it/s] 91%|█████████▏| 534/585 [05:05<00:14,  3.44it/s] 91%|█████████▏| 535/585 [05:06<00:14,  3.44it/s] 92%|█████████▏| 536/585 [05:06<00:14,  3.44it/s] 92%|█████████▏| 537/585 [05:06<00:13,  3.45it/s] 92%|█████████▏| 538/585 [05:07<00:13,  3.45it/s] 92%|█████████▏| 539/585 [05:07<00:13,  3.45it/s] 92%|█████████▏| 540/585 [05:07<00:13,  3.36it/s] 92%|█████████▏| 541/585 [05:07<00:13,  3.38it/s] 93%|█████████▎| 542/585 [05:08<00:12,  3.41it/s] 93%|█████████▎| 543/585 [05:08<00:12,  3.42it/s] 93%|█████████▎| 544/585 [05:08<00:11,  3.43it/s] 93%|█████████▎| 545/585 [05:09<00:11,  3.44it/s] 93%|█████████▎| 546/585 [05:09<00:11,  3.44it/s] 94%|█████████▎| 547/585 [05:09<00:11,  3.45it/s] 94%|█████████▎| 548/585 [05:09<00:10,  3.45it/s] 94%|█████████▍| 549/585 [05:10<00:10,  3.45it/s] 94%|█████████▍| 550/585 [05:10<00:10,  3.45it/s] 94%|█████████▍| 551/585 [05:10<00:10,  3.36it/s] 94%|█████████▍| 552/585 [05:11<00:09,  3.38it/s] 95%|█████████▍| 553/585 [05:11<00:09,  3.40it/s] 95%|█████████▍| 554/585 [05:11<00:09,  3.42it/s] 95%|█████████▍| 555/585 [05:12<00:08,  3.43it/s] 95%|█████████▌| 556/585 [05:12<00:08,  3.44it/s] 95%|█████████▌| 557/585 [05:12<00:08,  3.44it/s] 95%|█████████▌| 558/585 [05:12<00:07,  3.45it/s] 96%|█████████▌| 559/585 [05:13<00:07,  3.45it/s] 96%|█████████▌| 560/585 [05:13<00:07,  3.45it/s] 96%|█████████▌| 561/585 [05:13<00:06,  3.45it/s] 96%|█████████▌| 562/585 [05:14<00:06,  3.34it/s] 96%|█████████▌| 563/585 [05:14<00:06,  3.37it/s] 96%|█████████▋| 564/585 [05:14<00:06,  3.40it/s] 97%|█████████▋| 565/585 [05:14<00:05,  3.41it/s] 97%|█████████▋| 566/585 [05:15<00:05,  3.42it/s] 97%|█████████▋| 567/585 [05:15<00:05,  3.43it/s] 97%|█████████▋| 568/585 [05:15<00:04,  3.44it/s] 97%|█████████▋| 569/585 [05:16<00:04,  3.45it/s] 97%|█████████▋| 570/585 [05:16<00:04,  3.45it/s] 98%|█████████▊| 571/585 [05:16<00:04,  3.46it/s] 98%|█████████▊| 572/585 [05:16<00:03,  3.46it/s] 98%|█████████▊| 573/585 [05:17<00:03,  3.26it/s] 98%|█████████▊| 574/585 [05:17<00:03,  3.32it/s] 98%|█████████▊| 575/585 [05:17<00:02,  3.36it/s] 98%|█████████▊| 576/585 [05:18<00:02,  3.39it/s] 99%|█████████▊| 577/585 [05:18<00:02,  3.41it/s] 99%|█████████▉| 578/585 [05:18<00:02,  3.43it/s] 99%|█████████▉| 579/585 [05:19<00:01,  3.44it/s] 99%|█████████▉| 580/585 [05:19<00:01,  3.45it/s] 99%|█████████▉| 581/585 [05:19<00:01,  3.45it/s] 99%|█████████▉| 582/585 [05:19<00:00,  3.45it/s]100%|█████████▉| 583/585 [05:20<00:00,  3.46it/s]100%|█████████▉| 584/585 [05:20<00:00,  3.26it/s]100%|██████████| 585/585 [05:20<00:00,  3.32it/s][INFO|trainer.py:2140] 2023-08-29 12:16:03,320 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:16:03,320 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 12:16:03,320 >>   Batch size = 8
{'eval_loss': 0.9820075631141663, 'eval_runtime': 24.2017, 'eval_samples_per_second': 354.025, 'eval_steps_per_second': 44.253, 'epoch': 4.0}
{'loss': 0.5336, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:18, 56.59it/s][A
  1%|          | 12/1071 [00:00<00:21, 49.20it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 47.20it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 46.12it/s][A
  3%|▎         | 27/1071 [00:00<00:22, 45.65it/s][A
  3%|▎         | 32/1071 [00:00<00:22, 45.26it/s][A
  3%|▎         | 37/1071 [00:00<00:22, 45.09it/s][A
  4%|▍         | 42/1071 [00:00<00:22, 44.96it/s][A
  4%|▍         | 47/1071 [00:01<00:22, 44.96it/s][A
  5%|▍         | 52/1071 [00:01<00:22, 45.05it/s][A
  5%|▌         | 57/1071 [00:01<00:24, 41.66it/s][A
  6%|▌         | 62/1071 [00:01<00:23, 42.76it/s][A
  6%|▋         | 67/1071 [00:01<00:23, 43.32it/s][A
  7%|▋         | 72/1071 [00:01<00:22, 43.70it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 44.11it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 44.16it/s][A
  8%|▊         | 87/1071 [00:01<00:22, 44.30it/s][A
  9%|▊         | 92/1071 [00:02<00:22, 44.38it/s][A
  9%|▉         | 97/1071 [00:02<00:22, 44.27it/s][A
 10%|▉         | 102/1071 [00:02<00:21, 44.53it/s][A
 10%|▉         | 107/1071 [00:02<00:21, 44.65it/s][A
 10%|█         | 112/1071 [00:02<00:21, 44.91it/s][A
 11%|█         | 117/1071 [00:02<00:21, 44.98it/s][A
 11%|█▏        | 122/1071 [00:02<00:21, 44.92it/s][A
 12%|█▏        | 127/1071 [00:02<00:21, 44.78it/s][A
 12%|█▏        | 132/1071 [00:02<00:20, 44.81it/s][A
 13%|█▎        | 137/1071 [00:03<00:20, 44.65it/s][A
 13%|█▎        | 142/1071 [00:03<00:20, 44.62it/s][A
 14%|█▎        | 147/1071 [00:03<00:20, 44.70it/s][A
 14%|█▍        | 152/1071 [00:03<00:20, 44.73it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 44.93it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 44.93it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 44.93it/s][A
 16%|█▌        | 172/1071 [00:03<00:19, 44.97it/s][A
 17%|█▋        | 177/1071 [00:03<00:19, 44.98it/s][A
 17%|█▋        | 182/1071 [00:04<00:19, 44.95it/s][A
 17%|█▋        | 187/1071 [00:04<00:19, 44.89it/s][A
 18%|█▊        | 192/1071 [00:04<00:23, 38.07it/s][A
 18%|█▊        | 197/1071 [00:04<00:21, 40.03it/s][A
 19%|█▉        | 202/1071 [00:04<00:20, 41.59it/s][A
 19%|█▉        | 207/1071 [00:04<00:20, 42.65it/s][A
 20%|█▉        | 212/1071 [00:04<00:19, 43.43it/s][A
 20%|██        | 217/1071 [00:04<00:19, 44.00it/s][A
 21%|██        | 222/1071 [00:05<00:19, 44.53it/s][A
 21%|██        | 227/1071 [00:05<00:18, 44.50it/s][A
 22%|██▏       | 232/1071 [00:05<00:18, 44.16it/s][A
 22%|██▏       | 237/1071 [00:05<00:18, 44.10it/s][A
 23%|██▎       | 242/1071 [00:05<00:18, 44.16it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 44.59it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 44.79it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 44.98it/s][A
 24%|██▍       | 262/1071 [00:05<00:17, 45.18it/s][A
 25%|██▍       | 267/1071 [00:06<00:17, 45.08it/s][A
 25%|██▌       | 272/1071 [00:06<00:17, 44.79it/s][A
 26%|██▌       | 277/1071 [00:06<00:17, 44.46it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 44.19it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 44.30it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 44.61it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 44.88it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 44.94it/s][A
 29%|██▊       | 307/1071 [00:06<00:16, 45.20it/s][A
 29%|██▉       | 312/1071 [00:07<00:16, 45.11it/s][A
 30%|██▉       | 317/1071 [00:07<00:16, 44.84it/s][A
 30%|███       | 322/1071 [00:07<00:16, 44.43it/s][A
 31%|███       | 327/1071 [00:07<00:19, 38.54it/s][A
 31%|███       | 332/1071 [00:07<00:18, 40.40it/s][A
 31%|███▏      | 337/1071 [00:07<00:17, 41.84it/s][A
 32%|███▏      | 342/1071 [00:07<00:17, 42.81it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 43.63it/s][A
 33%|███▎      | 352/1071 [00:07<00:16, 44.02it/s][A
 33%|███▎      | 357/1071 [00:08<00:16, 44.48it/s][A
 34%|███▍      | 362/1071 [00:08<00:15, 44.71it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.18it/s][A
 35%|███▍      | 372/1071 [00:08<00:15, 44.10it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 44.19it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 44.26it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 44.57it/s][A
 37%|███▋      | 392/1071 [00:08<00:15, 44.74it/s][A
 37%|███▋      | 397/1071 [00:08<00:14, 45.05it/s][A
 38%|███▊      | 402/1071 [00:09<00:14, 45.15it/s][A
 38%|███▊      | 407/1071 [00:09<00:14, 45.09it/s][A
 38%|███▊      | 412/1071 [00:09<00:14, 44.71it/s][A
 39%|███▉      | 417/1071 [00:09<00:14, 44.44it/s][A
 39%|███▉      | 422/1071 [00:09<00:14, 44.30it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 44.48it/s][A
 40%|████      | 432/1071 [00:09<00:14, 44.68it/s][A
 41%|████      | 437/1071 [00:09<00:14, 44.91it/s][A
 41%|████▏     | 442/1071 [00:09<00:13, 45.12it/s][A
 42%|████▏     | 447/1071 [00:10<00:13, 45.16it/s][A
 42%|████▏     | 452/1071 [00:10<00:13, 45.00it/s][A
 43%|████▎     | 457/1071 [00:10<00:13, 44.79it/s][A
 43%|████▎     | 462/1071 [00:10<00:14, 42.65it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 43.32it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 43.73it/s][A
 45%|████▍     | 477/1071 [00:10<00:13, 44.23it/s][A
 45%|████▌     | 482/1071 [00:10<00:13, 44.43it/s][A
 45%|████▌     | 487/1071 [00:11<00:13, 44.78it/s][A
 46%|████▌     | 492/1071 [00:11<00:12, 44.98it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.78it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.50it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 44.39it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 44.50it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 44.42it/s][A
 49%|████▊     | 522/1071 [00:11<00:12, 44.65it/s][A
 49%|████▉     | 527/1071 [00:11<00:12, 44.61it/s][A
 50%|████▉     | 532/1071 [00:12<00:11, 45.04it/s][A
 50%|█████     | 537/1071 [00:12<00:11, 45.13it/s][A
 51%|█████     | 542/1071 [00:12<00:11, 44.96it/s][A
 51%|█████     | 547/1071 [00:12<00:11, 44.68it/s][A
 52%|█████▏    | 552/1071 [00:12<00:11, 44.55it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 44.53it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 44.58it/s][A
 53%|█████▎    | 567/1071 [00:12<00:11, 44.70it/s][A
 53%|█████▎    | 572/1071 [00:12<00:11, 44.87it/s][A
 54%|█████▍    | 577/1071 [00:13<00:10, 44.95it/s][A
 54%|█████▍    | 582/1071 [00:13<00:10, 45.03it/s][A
 55%|█████▍    | 587/1071 [00:13<00:10, 44.76it/s][A
 55%|█████▌    | 592/1071 [00:13<00:10, 44.73it/s][A
 56%|█████▌    | 597/1071 [00:13<00:10, 43.49it/s][A
 56%|█████▌    | 602/1071 [00:13<00:10, 43.84it/s][A
 57%|█████▋    | 607/1071 [00:13<00:10, 44.16it/s][A
 57%|█████▋    | 612/1071 [00:13<00:10, 44.43it/s][A
 58%|█████▊    | 617/1071 [00:13<00:10, 44.64it/s][A
 58%|█████▊    | 622/1071 [00:14<00:10, 44.68it/s][A
 59%|█████▊    | 627/1071 [00:14<00:09, 44.81it/s][A
 59%|█████▉    | 632/1071 [00:14<00:09, 44.58it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.26it/s][A
 60%|█████▉    | 642/1071 [00:14<00:09, 44.30it/s][A
 60%|██████    | 647/1071 [00:14<00:09, 44.44it/s][A
 61%|██████    | 652/1071 [00:14<00:09, 44.63it/s][A
 61%|██████▏   | 657/1071 [00:14<00:09, 44.78it/s][A
 62%|██████▏   | 662/1071 [00:14<00:09, 44.86it/s][A
 62%|██████▏   | 667/1071 [00:15<00:08, 45.03it/s][A
 63%|██████▎   | 672/1071 [00:15<00:08, 44.98it/s][A
 63%|██████▎   | 677/1071 [00:15<00:08, 44.89it/s][A
 64%|██████▎   | 682/1071 [00:15<00:08, 44.56it/s][A
 64%|██████▍   | 687/1071 [00:15<00:08, 44.49it/s][A
 65%|██████▍   | 692/1071 [00:15<00:08, 44.54it/s][A
 65%|██████▌   | 697/1071 [00:15<00:08, 44.65it/s][A
 66%|██████▌   | 702/1071 [00:15<00:08, 44.86it/s][A
 66%|██████▌   | 707/1071 [00:15<00:08, 44.76it/s][A
 66%|██████▋   | 712/1071 [00:16<00:07, 44.93it/s][A
 67%|██████▋   | 717/1071 [00:16<00:07, 44.94it/s][A
 67%|██████▋   | 722/1071 [00:16<00:07, 44.81it/s][A
 68%|██████▊   | 727/1071 [00:16<00:07, 44.62it/s][A
 68%|██████▊   | 732/1071 [00:16<00:08, 42.31it/s][A
 69%|██████▉   | 737/1071 [00:16<00:07, 43.08it/s][A
 69%|██████▉   | 742/1071 [00:16<00:07, 43.76it/s][A
 70%|██████▉   | 747/1071 [00:16<00:07, 44.22it/s][A
 70%|███████   | 752/1071 [00:16<00:07, 44.60it/s][A
 71%|███████   | 757/1071 [00:17<00:07, 44.63it/s][A
 71%|███████   | 762/1071 [00:17<00:06, 44.70it/s][A
 72%|███████▏  | 767/1071 [00:17<00:06, 44.59it/s][A
 72%|███████▏  | 772/1071 [00:17<00:06, 44.31it/s][A
 73%|███████▎  | 777/1071 [00:17<00:06, 44.38it/s][A
 73%|███████▎  | 782/1071 [00:17<00:06, 44.44it/s][A
 73%|███████▎  | 787/1071 [00:17<00:06, 44.58it/s][A
 74%|███████▍  | 792/1071 [00:17<00:06, 44.81it/s][A
 74%|███████▍  | 797/1071 [00:17<00:06, 44.87it/s][A
 75%|███████▍  | 802/1071 [00:18<00:05, 45.07it/s][A
 75%|███████▌  | 807/1071 [00:18<00:05, 44.91it/s][A
 76%|███████▌  | 812/1071 [00:18<00:05, 44.70it/s][A
 76%|███████▋  | 817/1071 [00:18<00:05, 44.60it/s][A
 77%|███████▋  | 822/1071 [00:18<00:05, 44.55it/s][A
 77%|███████▋  | 827/1071 [00:18<00:05, 44.55it/s][A
 78%|███████▊  | 832/1071 [00:18<00:05, 44.80it/s][A
 78%|███████▊  | 837/1071 [00:18<00:05, 44.87it/s][A
 79%|███████▊  | 842/1071 [00:18<00:05, 45.00it/s][A
 79%|███████▉  | 847/1071 [00:19<00:04, 45.03it/s][A
 80%|███████▉  | 852/1071 [00:19<00:04, 44.89it/s][A
 80%|████████  | 857/1071 [00:19<00:04, 44.72it/s][A
 80%|████████  | 862/1071 [00:19<00:04, 44.64it/s][A
 81%|████████  | 867/1071 [00:19<00:04, 43.85it/s][A
 81%|████████▏ | 872/1071 [00:19<00:04, 44.18it/s][A
 82%|████████▏ | 877/1071 [00:19<00:04, 44.36it/s][A
 82%|████████▏ | 882/1071 [00:19<00:04, 44.62it/s][A
 83%|████████▎ | 887/1071 [00:19<00:04, 44.83it/s][A
 83%|████████▎ | 892/1071 [00:20<00:03, 44.81it/s][A
 84%|████████▍ | 897/1071 [00:20<00:03, 44.82it/s][A
 84%|████████▍ | 902/1071 [00:20<00:03, 44.65it/s][A
 85%|████████▍ | 907/1071 [00:20<00:03, 44.31it/s][A
 85%|████████▌ | 912/1071 [00:20<00:03, 44.54it/s][A
 86%|████████▌ | 917/1071 [00:20<00:03, 44.61it/s][A
 86%|████████▌ | 922/1071 [00:20<00:03, 44.89it/s][A
 87%|████████▋ | 927/1071 [00:20<00:03, 44.70it/s][A
 87%|████████▋ | 932/1071 [00:20<00:03, 44.82it/s][A
 87%|████████▋ | 937/1071 [00:21<00:02, 44.79it/s][A
 88%|████████▊ | 942/1071 [00:21<00:02, 44.77it/s][A
 88%|████████▊ | 947/1071 [00:21<00:02, 44.63it/s][A
 89%|████████▉ | 952/1071 [00:21<00:02, 44.66it/s][A
 89%|████████▉ | 957/1071 [00:21<00:02, 44.51it/s][A
 90%|████████▉ | 962/1071 [00:21<00:02, 44.64it/s][A
 90%|█████████ | 967/1071 [00:21<00:02, 44.63it/s][A
 91%|█████████ | 972/1071 [00:21<00:02, 44.82it/s][A
 91%|█████████ | 977/1071 [00:21<00:02, 44.90it/s][A
 92%|█████████▏| 982/1071 [00:22<00:01, 44.89it/s][A
 92%|█████████▏| 987/1071 [00:22<00:01, 44.75it/s][A
 93%|█████████▎| 992/1071 [00:22<00:01, 44.56it/s][A
 93%|█████████▎| 997/1071 [00:22<00:01, 44.61it/s][A
 94%|█████████▎| 1002/1071 [00:22<00:01, 40.95it/s][A
 94%|█████████▍| 1007/1071 [00:22<00:01, 42.15it/s][A
 94%|█████████▍| 1012/1071 [00:22<00:01, 42.91it/s][A
 95%|█████████▍| 1017/1071 [00:22<00:01, 43.60it/s][A
 95%|█████████▌| 1022/1071 [00:23<00:01, 44.03it/s][A
 96%|█████████▌| 1027/1071 [00:23<00:00, 44.35it/s][A
 96%|█████████▋| 1032/1071 [00:23<00:00, 44.53it/s][A
 97%|█████████▋| 1037/1071 [00:23<00:00, 44.60it/s][A
 97%|█████████▋| 1042/1071 [00:23<00:00, 44.34it/s][A
 98%|█████████▊| 1047/1071 [00:23<00:00, 44.38it/s][A
 98%|█████████▊| 1052/1071 [00:23<00:00, 44.62it/s][A
 99%|█████████▊| 1057/1071 [00:23<00:00, 44.75it/s][A
 99%|█████████▉| 1062/1071 [00:23<00:00, 44.80it/s][A
100%|█████████▉| 1067/1071 [00:24<00:00, 44.94it/s][A
                                                   [A                                                 
100%|██████████| 1071/1071 [00:24<00:00, 44.94it/s][A100%|██████████| 585/585 [05:45<00:00,  3.32it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-29 12:16:27,700 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 12:16:27,853 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:16:31,095 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:16:31,254 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:16:31,325 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 12:16:38,356 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 12:16:38,388 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117 (score: 0.9560420513153076).
                                                 100%|██████████| 585/585 [06:05<00:00,  3.32it/s]100%|██████████| 585/585 [06:05<00:00,  1.60it/s]
[INFO|trainer.py:1894] 2023-08-29 12:16:47,512 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 12:16:47,725 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:16:51,235 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:16:51,440 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:16:51,556 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 12:16:52,165 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:16:52,166 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:16:52,166 >>   train_loss               =     0.5298
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:16:52,166 >>   train_runtime            = 0:06:04.98
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:16:52,166 >>   train_samples            =       7499
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:16:52,166 >>   train_samples_per_second =    102.729
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:16:52,166 >>   train_steps_per_second   =      1.603
{'eval_loss': 0.9866148829460144, 'eval_runtime': 24.144, 'eval_samples_per_second': 354.871, 'eval_steps_per_second': 44.359, 'epoch': 5.0}
{'train_runtime': 364.9889, 'train_samples_per_second': 102.729, 'train_steps_per_second': 1.603, 'train_loss': 0.5298408508300781, 'epoch': 5.0}
08/29/2023 12:16:52 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 12:16:52,509 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:16:52,509 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 12:16:52,509 >>   Batch size = 8
  0%|          | 0/1071 [00:00<?, ?it/s]  1%|          | 6/1071 [00:00<00:18, 56.12it/s]  1%|          | 12/1071 [00:00<00:21, 49.63it/s]  2%|▏         | 18/1071 [00:00<00:22, 47.50it/s]  2%|▏         | 23/1071 [00:00<00:22, 46.90it/s]  3%|▎         | 28/1071 [00:00<00:22, 46.39it/s]  3%|▎         | 33/1071 [00:00<00:22, 46.22it/s]  4%|▎         | 38/1071 [00:00<00:22, 46.09it/s]  4%|▍         | 43/1071 [00:00<00:22, 45.51it/s]  4%|▍         | 48/1071 [00:01<00:22, 45.06it/s]  5%|▍         | 53/1071 [00:01<00:22, 44.81it/s]  5%|▌         | 58/1071 [00:01<00:22, 44.85it/s]  6%|▌         | 63/1071 [00:01<00:22, 45.08it/s]  6%|▋         | 68/1071 [00:01<00:22, 45.24it/s]  7%|▋         | 73/1071 [00:01<00:21, 45.39it/s]  7%|▋         | 78/1071 [00:01<00:21, 45.43it/s]  8%|▊         | 83/1071 [00:01<00:21, 45.52it/s]  8%|▊         | 88/1071 [00:01<00:22, 43.87it/s]  9%|▊         | 93/1071 [00:02<00:22, 43.92it/s]  9%|▉         | 98/1071 [00:02<00:22, 43.95it/s] 10%|▉         | 103/1071 [00:02<00:21, 44.35it/s] 10%|█         | 108/1071 [00:02<00:21, 44.73it/s] 11%|█         | 113/1071 [00:02<00:21, 44.90it/s] 11%|█         | 118/1071 [00:02<00:21, 45.00it/s] 11%|█▏        | 123/1071 [00:02<00:21, 45.05it/s] 12%|█▏        | 128/1071 [00:02<00:20, 45.02it/s] 12%|█▏        | 133/1071 [00:02<00:20, 45.01it/s] 13%|█▎        | 138/1071 [00:03<00:20, 44.82it/s] 13%|█▎        | 143/1071 [00:03<00:20, 44.62it/s] 14%|█▍        | 148/1071 [00:03<00:20, 44.80it/s] 14%|█▍        | 153/1071 [00:03<00:20, 44.94it/s] 15%|█▍        | 158/1071 [00:03<00:20, 45.14it/s] 15%|█▌        | 163/1071 [00:03<00:20, 45.22it/s] 16%|█▌        | 168/1071 [00:03<00:19, 45.43it/s] 16%|█▌        | 173/1071 [00:03<00:19, 45.33it/s] 17%|█▋        | 178/1071 [00:03<00:19, 45.08it/s] 17%|█▋        | 183/1071 [00:04<00:19, 44.86it/s] 18%|█▊        | 188/1071 [00:04<00:19, 44.76it/s] 18%|█▊        | 193/1071 [00:04<00:19, 44.89it/s] 18%|█▊        | 198/1071 [00:04<00:19, 45.00it/s] 19%|█▉        | 203/1071 [00:04<00:19, 45.11it/s] 19%|█▉        | 208/1071 [00:04<00:19, 45.35it/s] 20%|█▉        | 213/1071 [00:04<00:18, 45.39it/s] 20%|██        | 218/1071 [00:04<00:18, 45.22it/s] 21%|██        | 223/1071 [00:04<00:19, 44.04it/s] 21%|██▏       | 228/1071 [00:05<00:19, 44.10it/s] 22%|██▏       | 233/1071 [00:05<00:18, 44.25it/s] 22%|██▏       | 238/1071 [00:05<00:18, 44.47it/s] 23%|██▎       | 243/1071 [00:05<00:18, 44.79it/s] 23%|██▎       | 248/1071 [00:05<00:18, 44.99it/s] 24%|██▎       | 253/1071 [00:05<00:18, 45.18it/s] 24%|██▍       | 258/1071 [00:05<00:17, 45.22it/s] 25%|██▍       | 263/1071 [00:05<00:17, 45.05it/s] 25%|██▌       | 268/1071 [00:05<00:17, 44.88it/s] 25%|██▌       | 273/1071 [00:06<00:17, 44.91it/s] 26%|██▌       | 278/1071 [00:06<00:17, 44.81it/s] 26%|██▋       | 283/1071 [00:06<00:17, 44.91it/s] 27%|██▋       | 288/1071 [00:06<00:17, 44.98it/s] 27%|██▋       | 293/1071 [00:06<00:17, 45.02it/s] 28%|██▊       | 298/1071 [00:06<00:17, 45.22it/s] 28%|██▊       | 303/1071 [00:06<00:17, 45.14it/s] 29%|██▉       | 308/1071 [00:06<00:16, 45.12it/s] 29%|██▉       | 313/1071 [00:06<00:16, 45.01it/s] 30%|██▉       | 318/1071 [00:07<00:16, 44.92it/s] 30%|███       | 323/1071 [00:07<00:16, 44.91it/s] 31%|███       | 328/1071 [00:07<00:16, 44.92it/s] 31%|███       | 333/1071 [00:07<00:16, 45.04it/s] 32%|███▏      | 338/1071 [00:07<00:16, 45.15it/s] 32%|███▏      | 343/1071 [00:07<00:16, 45.33it/s] 32%|███▏      | 348/1071 [00:07<00:16, 45.03it/s] 33%|███▎      | 353/1071 [00:07<00:15, 45.06it/s] 33%|███▎      | 358/1071 [00:07<00:16, 42.08it/s] 34%|███▍      | 363/1071 [00:08<00:16, 42.97it/s] 34%|███▍      | 368/1071 [00:08<00:16, 43.63it/s] 35%|███▍      | 373/1071 [00:08<00:15, 44.01it/s] 35%|███▌      | 378/1071 [00:08<00:15, 44.52it/s] 36%|███▌      | 383/1071 [00:08<00:15, 44.68it/s] 36%|███▌      | 388/1071 [00:08<00:15, 44.95it/s] 37%|███▋      | 393/1071 [00:08<00:15, 44.90it/s] 37%|███▋      | 398/1071 [00:08<00:15, 44.63it/s] 38%|███▊      | 403/1071 [00:08<00:14, 44.71it/s] 38%|███▊      | 408/1071 [00:09<00:14, 44.92it/s] 39%|███▊      | 413/1071 [00:09<00:14, 45.00it/s] 39%|███▉      | 418/1071 [00:09<00:14, 44.95it/s] 39%|███▉      | 423/1071 [00:09<00:14, 44.91it/s] 40%|███▉      | 428/1071 [00:09<00:14, 45.17it/s] 40%|████      | 433/1071 [00:09<00:14, 45.30it/s] 41%|████      | 438/1071 [00:09<00:14, 45.05it/s] 41%|████▏     | 443/1071 [00:09<00:13, 44.97it/s] 42%|████▏     | 448/1071 [00:09<00:13, 44.88it/s] 42%|████▏     | 453/1071 [00:10<00:13, 44.97it/s] 43%|████▎     | 458/1071 [00:10<00:13, 45.05it/s] 43%|████▎     | 463/1071 [00:10<00:13, 45.14it/s] 44%|████▎     | 468/1071 [00:10<00:13, 44.87it/s] 44%|████▍     | 473/1071 [00:10<00:13, 45.06it/s] 45%|████▍     | 478/1071 [00:10<00:13, 45.11it/s] 45%|████▌     | 483/1071 [00:10<00:13, 44.90it/s] 46%|████▌     | 488/1071 [00:10<00:13, 44.81it/s] 46%|████▌     | 493/1071 [00:10<00:13, 41.91it/s] 46%|████▋     | 498/1071 [00:11<00:13, 42.87it/s] 47%|████▋     | 503/1071 [00:11<00:13, 43.68it/s] 47%|████▋     | 508/1071 [00:11<00:12, 44.25it/s] 48%|████▊     | 513/1071 [00:11<00:12, 44.62it/s] 48%|████▊     | 518/1071 [00:11<00:12, 44.68it/s] 49%|████▉     | 523/1071 [00:11<00:12, 44.84it/s] 49%|████▉     | 528/1071 [00:11<00:12, 44.73it/s] 50%|████▉     | 533/1071 [00:11<00:12, 44.51it/s] 50%|█████     | 538/1071 [00:11<00:11, 44.52it/s] 51%|█████     | 543/1071 [00:12<00:11, 44.54it/s] 51%|█████     | 548/1071 [00:12<00:11, 44.88it/s] 52%|█████▏    | 553/1071 [00:12<00:11, 44.97it/s] 52%|█████▏    | 558/1071 [00:12<00:11, 45.14it/s] 53%|█████▎    | 563/1071 [00:12<00:11, 45.18it/s] 53%|█████▎    | 568/1071 [00:12<00:11, 45.26it/s] 54%|█████▎    | 573/1071 [00:12<00:11, 45.15it/s] 54%|█████▍    | 578/1071 [00:12<00:10, 44.97it/s] 54%|█████▍    | 583/1071 [00:12<00:10, 44.90it/s] 55%|█████▍    | 588/1071 [00:13<00:10, 44.97it/s] 55%|█████▌    | 593/1071 [00:13<00:10, 45.03it/s] 56%|█████▌    | 598/1071 [00:13<00:10, 45.07it/s] 56%|█████▋    | 603/1071 [00:13<00:10, 45.02it/s] 57%|█████▋    | 608/1071 [00:13<00:11, 40.87it/s] 57%|█████▋    | 613/1071 [00:13<00:10, 42.21it/s] 58%|█████▊    | 618/1071 [00:13<00:10, 43.16it/s] 58%|█████▊    | 623/1071 [00:13<00:10, 43.78it/s] 59%|█████▊    | 628/1071 [00:14<00:10, 44.13it/s] 59%|█████▉    | 633/1071 [00:14<00:09, 44.49it/s] 60%|█████▉    | 638/1071 [00:14<00:09, 44.66it/s] 60%|██████    | 643/1071 [00:14<00:09, 44.67it/s] 61%|██████    | 648/1071 [00:14<00:09, 44.46it/s] 61%|██████    | 653/1071 [00:14<00:09, 44.44it/s] 61%|██████▏   | 658/1071 [00:14<00:09, 44.58it/s] 62%|██████▏   | 663/1071 [00:14<00:09, 44.84it/s] 62%|██████▏   | 668/1071 [00:14<00:08, 45.08it/s] 63%|██████▎   | 673/1071 [00:15<00:08, 45.15it/s] 63%|██████▎   | 678/1071 [00:15<00:08, 45.21it/s] 64%|██████▍   | 683/1071 [00:15<00:08, 45.29it/s] 64%|██████▍   | 688/1071 [00:15<00:08, 45.07it/s] 65%|██████▍   | 693/1071 [00:15<00:08, 44.81it/s] 65%|██████▌   | 698/1071 [00:15<00:08, 44.62it/s] 66%|██████▌   | 703/1071 [00:15<00:08, 44.60it/s] 66%|██████▌   | 708/1071 [00:15<00:08, 44.87it/s] 67%|██████▋   | 713/1071 [00:15<00:07, 45.03it/s] 67%|██████▋   | 718/1071 [00:16<00:07, 45.20it/s] 68%|██████▊   | 723/1071 [00:16<00:07, 45.36it/s] 68%|██████▊   | 728/1071 [00:16<00:07, 45.29it/s] 68%|██████▊   | 733/1071 [00:16<00:07, 45.00it/s] 69%|██████▉   | 738/1071 [00:16<00:08, 40.84it/s] 69%|██████▉   | 743/1071 [00:16<00:07, 42.16it/s] 70%|██████▉   | 748/1071 [00:16<00:07, 43.03it/s] 70%|███████   | 753/1071 [00:16<00:07, 43.54it/s] 71%|███████   | 758/1071 [00:16<00:07, 44.25it/s] 71%|███████   | 763/1071 [00:17<00:06, 44.51it/s] 72%|███████▏  | 768/1071 [00:17<00:06, 44.76it/s] 72%|███████▏  | 773/1071 [00:17<00:06, 44.61it/s] 73%|███████▎  | 778/1071 [00:17<00:06, 44.40it/s] 73%|███████▎  | 783/1071 [00:17<00:06, 44.40it/s] 74%|███████▎  | 788/1071 [00:17<00:06, 44.56it/s] 74%|███████▍  | 793/1071 [00:17<00:06, 44.71it/s] 75%|███████▍  | 798/1071 [00:17<00:06, 44.98it/s] 75%|███████▍  | 803/1071 [00:17<00:05, 45.14it/s] 75%|███████▌  | 808/1071 [00:18<00:05, 45.22it/s] 76%|███████▌  | 813/1071 [00:18<00:05, 45.18it/s] 76%|███████▋  | 818/1071 [00:18<00:05, 44.75it/s] 77%|███████▋  | 823/1071 [00:18<00:05, 44.60it/s] 77%|███████▋  | 828/1071 [00:18<00:05, 44.60it/s] 78%|███████▊  | 833/1071 [00:18<00:05, 44.65it/s] 78%|███████▊  | 838/1071 [00:18<00:05, 44.90it/s] 79%|███████▊  | 843/1071 [00:18<00:05, 45.10it/s] 79%|███████▉  | 848/1071 [00:18<00:04, 45.28it/s] 80%|███████▉  | 853/1071 [00:19<00:04, 45.30it/s] 80%|████████  | 858/1071 [00:19<00:04, 45.34it/s] 81%|████████  | 863/1071 [00:19<00:04, 45.07it/s] 81%|████████  | 868/1071 [00:19<00:04, 44.72it/s] 82%|████████▏ | 873/1071 [00:19<00:04, 43.27it/s] 82%|████████▏ | 878/1071 [00:19<00:04, 43.81it/s] 82%|████████▏ | 883/1071 [00:19<00:04, 44.26it/s] 83%|████████▎ | 888/1071 [00:19<00:04, 44.61it/s] 83%|████████▎ | 893/1071 [00:19<00:03, 44.78it/s] 84%|████████▍ | 898/1071 [00:20<00:03, 44.98it/s] 84%|████████▍ | 903/1071 [00:20<00:03, 44.93it/s] 85%|████████▍ | 908/1071 [00:20<00:03, 44.65it/s] 85%|████████▌ | 913/1071 [00:20<00:03, 44.46it/s] 86%|████████▌ | 918/1071 [00:20<00:03, 44.63it/s] 86%|████████▌ | 923/1071 [00:20<00:03, 44.80it/s] 87%|████████▋ | 928/1071 [00:20<00:03, 44.88it/s] 87%|████████▋ | 933/1071 [00:20<00:03, 45.10it/s] 88%|████████▊ | 938/1071 [00:20<00:02, 45.21it/s] 88%|████████▊ | 943/1071 [00:21<00:02, 45.20it/s] 89%|████████▊ | 948/1071 [00:21<00:02, 45.11it/s] 89%|████████▉ | 953/1071 [00:21<00:02, 44.77it/s] 89%|████████▉ | 958/1071 [00:21<00:02, 44.61it/s] 90%|████████▉ | 963/1071 [00:21<00:02, 44.62it/s] 90%|█████████ | 968/1071 [00:21<00:02, 44.57it/s] 91%|█████████ | 973/1071 [00:21<00:02, 44.91it/s] 91%|█████████▏| 978/1071 [00:21<00:02, 44.99it/s] 92%|█████████▏| 983/1071 [00:21<00:01, 45.18it/s] 92%|█████████▏| 988/1071 [00:22<00:01, 45.27it/s] 93%|█████████▎| 993/1071 [00:22<00:01, 45.15it/s] 93%|█████████▎| 998/1071 [00:22<00:01, 44.97it/s] 94%|█████████▎| 1003/1071 [00:22<00:01, 44.84it/s] 94%|█████████▍| 1008/1071 [00:22<00:01, 42.64it/s] 95%|█████████▍| 1013/1071 [00:22<00:01, 43.49it/s] 95%|█████████▌| 1018/1071 [00:22<00:01, 44.02it/s] 96%|█████████▌| 1023/1071 [00:22<00:01, 44.38it/s] 96%|█████████▌| 1028/1071 [00:22<00:00, 44.73it/s] 96%|█████████▋| 1033/1071 [00:23<00:00, 44.88it/s] 97%|█████████▋| 1038/1071 [00:23<00:00, 44.86it/s] 97%|█████████▋| 1043/1071 [00:23<00:00, 44.75it/s] 98%|█████████▊| 1048/1071 [00:23<00:00, 44.57it/s] 98%|█████████▊| 1053/1071 [00:23<00:00, 44.52it/s] 99%|█████████▉| 1058/1071 [00:23<00:00, 44.66it/s] 99%|█████████▉| 1063/1071 [00:23<00:00, 44.92it/s]100%|█████████▉| 1068/1071 [00:23<00:00, 45.07it/s]100%|██████████| 1071/1071 [00:23<00:00, 44.72it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 12:17:16,475 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:17:16,475 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:17:16,475 >>   eval_loss               =      0.956
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:17:16,475 >>   eval_runtime            = 0:00:23.96
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:17:16,475 >>   eval_samples            =       8568
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:17:16,475 >>   eval_samples_per_second =    357.508
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:17:16,475 >>   eval_steps_per_second   =     44.689
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:17:16,475 >>   perplexity              =     2.6014
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:17:27,586 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:17:27,604 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:17:27,604 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:17:27,604 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:17:27,604 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:17:28,094 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:17:28,095 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:17:28,396 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:17:29,556 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:17:29,556 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:17:31,462 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:17:31,485 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:17:31,485 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:17:31,485 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:17:31,485 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:17:32,037 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:17:32,038 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:17:32,543 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:17:32,793 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:17:32,793 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-585
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl', 'labels': ['country', 'place of death', 'production company', 'screenwriter', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 21439
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21539, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.47it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.62it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:06,  1.56it/s]Extractor Predicting: 12it [00:07,  1.51it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:08,  1.56it/s]Extractor Predicting: 15it [00:09,  1.52it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:12,  1.61it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:13,  1.62it/s]Extractor Predicting: 22it [00:13,  1.66it/s]Extractor Predicting: 23it [00:14,  1.70it/s]Extractor Predicting: 24it [00:15,  1.64it/s]Extractor Predicting: 25it [00:15,  1.53it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.57it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:18,  1.63it/s]Extractor Predicting: 30it [00:18,  1.54it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:20,  1.61it/s]Extractor Predicting: 33it [00:20,  1.64it/s]Extractor Predicting: 34it [00:21,  1.68it/s]Extractor Predicting: 35it [00:21,  1.72it/s]Extractor Predicting: 36it [00:22,  1.69it/s]Extractor Predicting: 37it [00:23,  1.75it/s]Extractor Predicting: 38it [00:23,  1.74it/s]Extractor Predicting: 39it [00:24,  1.72it/s]Extractor Predicting: 40it [00:24,  1.72it/s]Extractor Predicting: 41it [00:25,  1.69it/s]Extractor Predicting: 42it [00:25,  1.73it/s]Extractor Predicting: 43it [00:26,  1.73it/s]Extractor Predicting: 44it [00:27,  1.69it/s]Extractor Predicting: 45it [00:27,  1.73it/s]Extractor Predicting: 46it [00:28,  1.69it/s]Extractor Predicting: 47it [00:28,  1.65it/s]Extractor Predicting: 48it [00:29,  1.70it/s]Extractor Predicting: 49it [00:30,  1.72it/s]Extractor Predicting: 50it [00:30,  1.76it/s]Extractor Predicting: 51it [00:31,  1.77it/s]Extractor Predicting: 52it [00:31,  1.77it/s]Extractor Predicting: 53it [00:32,  1.79it/s]Extractor Predicting: 54it [00:32,  1.79it/s]Extractor Predicting: 55it [00:33,  1.75it/s]Extractor Predicting: 56it [00:33,  1.75it/s]Extractor Predicting: 57it [00:34,  1.79it/s]Extractor Predicting: 58it [00:35,  1.83it/s]Extractor Predicting: 59it [00:35,  1.73it/s]Extractor Predicting: 60it [00:36,  1.73it/s]Extractor Predicting: 61it [00:37,  1.53it/s]Extractor Predicting: 62it [00:37,  1.58it/s]Extractor Predicting: 63it [00:38,  1.60it/s]Extractor Predicting: 64it [00:38,  1.68it/s]Extractor Predicting: 65it [00:39,  1.73it/s]Extractor Predicting: 66it [00:39,  1.71it/s]Extractor Predicting: 67it [00:40,  1.71it/s]Extractor Predicting: 68it [00:41,  1.71it/s]Extractor Predicting: 69it [00:41,  1.74it/s]Extractor Predicting: 70it [00:42,  1.69it/s]Extractor Predicting: 71it [00:42,  1.73it/s]Extractor Predicting: 72it [00:43,  1.67it/s]Extractor Predicting: 73it [00:44,  1.66it/s]Extractor Predicting: 74it [00:44,  1.64it/s]Extractor Predicting: 75it [00:45,  1.65it/s]Extractor Predicting: 76it [00:45,  1.62it/s]Extractor Predicting: 77it [00:46,  1.58it/s]Extractor Predicting: 78it [00:47,  1.61it/s]Extractor Predicting: 79it [00:47,  1.57it/s]Extractor Predicting: 80it [00:48,  1.54it/s]Extractor Predicting: 81it [00:49,  1.51it/s]Extractor Predicting: 82it [00:49,  1.48it/s]Extractor Predicting: 83it [00:50,  1.49it/s]Extractor Predicting: 84it [00:51,  1.48it/s]Extractor Predicting: 85it [00:51,  1.48it/s]Extractor Predicting: 86it [00:52,  1.50it/s]Extractor Predicting: 87it [00:53,  1.49it/s]Extractor Predicting: 88it [00:54,  1.49it/s]Extractor Predicting: 89it [00:54,  1.51it/s]Extractor Predicting: 90it [00:55,  1.51it/s]Extractor Predicting: 91it [00:55,  1.49it/s]Extractor Predicting: 92it [00:56,  1.49it/s]Extractor Predicting: 93it [00:57,  1.48it/s]Extractor Predicting: 94it [00:58,  1.49it/s]Extractor Predicting: 95it [00:58,  1.50it/s]Extractor Predicting: 96it [00:59,  1.51it/s]Extractor Predicting: 97it [00:59,  1.52it/s]Extractor Predicting: 98it [01:00,  1.53it/s]Extractor Predicting: 99it [01:01,  1.54it/s]Extractor Predicting: 100it [01:01,  1.53it/s]Extractor Predicting: 101it [01:02,  1.54it/s]Extractor Predicting: 102it [01:03,  1.49it/s]Extractor Predicting: 103it [01:03,  1.50it/s]Extractor Predicting: 104it [01:04,  1.49it/s]Extractor Predicting: 105it [01:05,  1.50it/s]Extractor Predicting: 106it [01:05,  1.50it/s]Extractor Predicting: 107it [01:06,  1.50it/s]Extractor Predicting: 108it [01:07,  1.51it/s]Extractor Predicting: 109it [01:07,  1.50it/s]Extractor Predicting: 110it [01:08,  1.54it/s]Extractor Predicting: 111it [01:09,  1.54it/s]Extractor Predicting: 112it [01:09,  1.56it/s]Extractor Predicting: 113it [01:10,  1.55it/s]Extractor Predicting: 114it [01:11,  1.55it/s]Extractor Predicting: 115it [01:11,  1.49it/s]Extractor Predicting: 116it [01:12,  1.50it/s]Extractor Predicting: 117it [01:13,  1.51it/s]Extractor Predicting: 118it [01:13,  1.49it/s]Extractor Predicting: 119it [01:14,  1.49it/s]Extractor Predicting: 120it [01:15,  1.51it/s]Extractor Predicting: 121it [01:15,  1.54it/s]Extractor Predicting: 122it [01:16,  1.49it/s]Extractor Predicting: 123it [01:17,  1.50it/s]Extractor Predicting: 124it [01:17,  1.49it/s]Extractor Predicting: 125it [01:18,  1.51it/s]Extractor Predicting: 126it [01:19,  1.53it/s]Extractor Predicting: 127it [01:19,  1.59it/s]Extractor Predicting: 128it [01:20,  1.60it/s]Extractor Predicting: 129it [01:20,  1.62it/s]Extractor Predicting: 130it [01:21,  1.59it/s]Extractor Predicting: 131it [01:22,  1.61it/s]Extractor Predicting: 132it [01:22,  1.58it/s]Extractor Predicting: 133it [01:23,  1.57it/s]Extractor Predicting: 134it [01:24,  1.57it/s]Extractor Predicting: 135it [01:24,  1.60it/s]Extractor Predicting: 136it [01:25,  1.64it/s]Extractor Predicting: 137it [01:25,  1.64it/s]Extractor Predicting: 138it [01:26,  1.63it/s]Extractor Predicting: 139it [01:27,  1.64it/s]Extractor Predicting: 140it [01:27,  1.66it/s]Extractor Predicting: 141it [01:28,  1.67it/s]Extractor Predicting: 142it [01:28,  1.65it/s]Extractor Predicting: 143it [01:29,  1.58it/s]Extractor Predicting: 144it [01:30,  1.59it/s]Extractor Predicting: 145it [01:30,  1.64it/s]Extractor Predicting: 146it [01:31,  1.67it/s]Extractor Predicting: 147it [01:31,  1.67it/s]Extractor Predicting: 148it [01:32,  1.68it/s]Extractor Predicting: 149it [01:33,  1.65it/s]Extractor Predicting: 150it [01:33,  1.67it/s]Extractor Predicting: 151it [01:34,  1.63it/s]Extractor Predicting: 152it [01:35,  1.59it/s]Extractor Predicting: 153it [01:35,  1.60it/s]Extractor Predicting: 154it [01:36,  1.56it/s]Extractor Predicting: 155it [01:37,  1.56it/s]Extractor Predicting: 156it [01:37,  1.57it/s]Extractor Predicting: 157it [01:38,  1.42it/s]Extractor Predicting: 158it [01:39,  1.45it/s]Extractor Predicting: 159it [01:39,  1.48it/s]Extractor Predicting: 160it [01:40,  1.52it/s]Extractor Predicting: 161it [01:41,  1.53it/s]Extractor Predicting: 162it [01:41,  1.57it/s]Extractor Predicting: 163it [01:42,  1.71it/s]Extractor Predicting: 164it [01:42,  1.83it/s]Extractor Predicting: 165it [01:43,  1.86it/s]Extractor Predicting: 166it [01:43,  1.82it/s]Extractor Predicting: 167it [01:44,  1.74it/s]Extractor Predicting: 168it [01:44,  1.65it/s]Extractor Predicting: 169it [01:45,  1.60it/s]Extractor Predicting: 170it [01:46,  1.62it/s]Extractor Predicting: 171it [01:46,  1.63it/s]Extractor Predicting: 172it [01:47,  1.60it/s]Extractor Predicting: 173it [01:48,  1.60it/s]Extractor Predicting: 174it [01:48,  1.60it/s]Extractor Predicting: 175it [01:49,  1.57it/s]Extractor Predicting: 176it [01:50,  1.57it/s]Extractor Predicting: 177it [01:50,  1.58it/s]Extractor Predicting: 178it [01:51,  1.58it/s]Extractor Predicting: 179it [01:51,  1.55it/s]Extractor Predicting: 180it [01:52,  1.56it/s]Extractor Predicting: 181it [01:53,  1.59it/s]Extractor Predicting: 182it [01:53,  1.61it/s]Extractor Predicting: 183it [01:54,  1.56it/s]Extractor Predicting: 184it [01:55,  1.56it/s]Extractor Predicting: 185it [01:55,  1.55it/s]Extractor Predicting: 186it [01:56,  1.57it/s]Extractor Predicting: 187it [01:57,  1.58it/s]Extractor Predicting: 188it [01:57,  1.57it/s]Extractor Predicting: 189it [01:58,  1.58it/s]Extractor Predicting: 190it [01:58,  1.57it/s]Extractor Predicting: 191it [01:59,  1.59it/s]Extractor Predicting: 192it [02:00,  1.61it/s]Extractor Predicting: 193it [02:00,  1.60it/s]Extractor Predicting: 194it [02:01,  1.62it/s]Extractor Predicting: 195it [02:02,  1.58it/s]Extractor Predicting: 196it [02:02,  1.60it/s]Extractor Predicting: 197it [02:03,  1.59it/s]Extractor Predicting: 198it [02:03,  1.58it/s]Extractor Predicting: 199it [02:04,  1.58it/s]Extractor Predicting: 200it [02:05,  1.58it/s]Extractor Predicting: 201it [02:05,  1.54it/s]Extractor Predicting: 202it [02:06,  1.55it/s]Extractor Predicting: 203it [02:07,  1.55it/s]Extractor Predicting: 204it [02:07,  1.55it/s]Extractor Predicting: 205it [02:08,  1.56it/s]Extractor Predicting: 206it [02:09,  1.54it/s]Extractor Predicting: 207it [02:09,  1.59it/s]Extractor Predicting: 208it [02:10,  1.62it/s]Extractor Predicting: 209it [02:10,  1.59it/s]Extractor Predicting: 210it [02:11,  1.60it/s]Extractor Predicting: 211it [02:12,  1.58it/s]Extractor Predicting: 212it [02:12,  1.56it/s]Extractor Predicting: 213it [02:13,  1.53it/s]Extractor Predicting: 214it [02:14,  1.58it/s]Extractor Predicting: 215it [02:14,  1.60it/s]Extractor Predicting: 216it [02:15,  1.57it/s]Extractor Predicting: 217it [02:16,  1.57it/s]Extractor Predicting: 218it [02:16,  1.56it/s]Extractor Predicting: 219it [02:17,  1.56it/s]Extractor Predicting: 220it [02:17,  1.57it/s]Extractor Predicting: 221it [02:18,  1.59it/s]Extractor Predicting: 222it [02:19,  1.59it/s]Extractor Predicting: 223it [02:19,  1.59it/s]Extractor Predicting: 224it [02:20,  1.63it/s]Extractor Predicting: 225it [02:21,  1.67it/s]Extractor Predicting: 226it [02:21,  1.64it/s]Extractor Predicting: 227it [02:22,  1.62it/s]Extractor Predicting: 228it [02:22,  1.63it/s]Extractor Predicting: 229it [02:23,  1.55it/s]Extractor Predicting: 230it [02:24,  1.61it/s]Extractor Predicting: 231it [02:24,  1.60it/s]Extractor Predicting: 232it [02:25,  1.58it/s]Extractor Predicting: 233it [02:26,  1.57it/s]Extractor Predicting: 234it [02:26,  1.54it/s]Extractor Predicting: 235it [02:27,  1.55it/s]Extractor Predicting: 236it [02:28,  1.50it/s]Extractor Predicting: 237it [02:28,  1.48it/s]Extractor Predicting: 238it [02:29,  1.47it/s]Extractor Predicting: 239it [02:30,  1.45it/s]Extractor Predicting: 240it [02:30,  1.46it/s]Extractor Predicting: 241it [02:31,  1.48it/s]Extractor Predicting: 242it [02:32,  1.48it/s]Extractor Predicting: 243it [02:32,  1.53it/s]Extractor Predicting: 244it [02:33,  1.54it/s]Extractor Predicting: 245it [02:34,  1.59it/s]Extractor Predicting: 246it [02:34,  1.61it/s]Extractor Predicting: 247it [02:35,  1.64it/s]Extractor Predicting: 248it [02:35,  1.66it/s]Extractor Predicting: 249it [02:36,  1.66it/s]Extractor Predicting: 250it [02:37,  1.63it/s]Extractor Predicting: 251it [02:37,  1.68it/s]Extractor Predicting: 252it [02:38,  1.66it/s]Extractor Predicting: 253it [02:39,  1.48it/s]Extractor Predicting: 254it [02:39,  1.52it/s]Extractor Predicting: 255it [02:40,  1.56it/s]Extractor Predicting: 256it [02:40,  1.56it/s]Extractor Predicting: 257it [02:41,  1.59it/s]Extractor Predicting: 258it [02:42,  1.62it/s]Extractor Predicting: 259it [02:42,  1.58it/s]Extractor Predicting: 260it [02:43,  1.58it/s]Extractor Predicting: 261it [02:44,  1.57it/s]Extractor Predicting: 262it [02:44,  1.58it/s]Extractor Predicting: 263it [02:45,  1.58it/s]Extractor Predicting: 264it [02:45,  1.58it/s]Extractor Predicting: 265it [02:46,  1.59it/s]Extractor Predicting: 266it [02:47,  1.58it/s]Extractor Predicting: 267it [02:47,  1.58it/s]Extractor Predicting: 268it [02:48,  1.58it/s]Extractor Predicting: 269it [02:49,  1.54it/s]Extractor Predicting: 270it [02:49,  1.56it/s]Extractor Predicting: 271it [02:50,  1.58it/s]Extractor Predicting: 272it [02:50,  1.65it/s]Extractor Predicting: 273it [02:51,  1.65it/s]Extractor Predicting: 274it [02:52,  1.64it/s]Extractor Predicting: 275it [02:52,  1.61it/s]Extractor Predicting: 276it [02:53,  1.62it/s]Extractor Predicting: 277it [02:54,  1.63it/s]Extractor Predicting: 278it [02:54,  1.60it/s]Extractor Predicting: 279it [02:55,  1.61it/s]Extractor Predicting: 280it [02:55,  1.67it/s]Extractor Predicting: 281it [02:56,  1.66it/s]Extractor Predicting: 282it [02:57,  1.63it/s]Extractor Predicting: 283it [02:57,  1.59it/s]Extractor Predicting: 284it [02:58,  1.57it/s]Extractor Predicting: 285it [02:59,  1.53it/s]Extractor Predicting: 286it [02:59,  1.56it/s]Extractor Predicting: 287it [03:00,  1.55it/s]Extractor Predicting: 288it [03:01,  1.54it/s]Extractor Predicting: 289it [03:01,  1.51it/s]Extractor Predicting: 290it [03:02,  1.51it/s]Extractor Predicting: 291it [03:03,  1.55it/s]Extractor Predicting: 292it [03:03,  1.57it/s]Extractor Predicting: 293it [03:04,  1.57it/s]Extractor Predicting: 294it [03:04,  1.52it/s]Extractor Predicting: 295it [03:05,  1.54it/s]Extractor Predicting: 296it [03:06,  1.52it/s]Extractor Predicting: 297it [03:07,  1.47it/s]Extractor Predicting: 298it [03:07,  1.47it/s]Extractor Predicting: 299it [03:08,  1.46it/s]Extractor Predicting: 300it [03:09,  1.47it/s]Extractor Predicting: 301it [03:09,  1.44it/s]Extractor Predicting: 302it [03:10,  1.43it/s]Extractor Predicting: 303it [03:11,  1.43it/s]Extractor Predicting: 303it [03:11,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:20:59,194 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:20:59,214 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:20:59,214 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:20:59,214 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:20:59,214 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:21:00,021 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:21:00,022 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:21:00,608 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:21:01,744 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:21:01,744 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:21:04,774 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:21:04,797 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:21:04,797 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:21:04,797 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:21:04,797 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:21:05,657 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:21:05,658 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:21:06,309 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:21:06,572 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:21:06,572 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.324468085106383,
  "recall": 0.049836601307189546,
  "score": 0.08640226628895184,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'labels': ['continent', 'field of this occupation', 'field of work', 'founded by', 'movement', 'owned by', 'performer', 'producer', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20815
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20915, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.76it/s]Extractor Predicting: 2it [00:01,  1.69it/s]Extractor Predicting: 3it [00:01,  1.68it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:04,  1.61it/s]Extractor Predicting: 9it [00:05,  1.50it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:06,  1.56it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:11,  1.58it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:13,  1.54it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:15,  1.58it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:16,  1.58it/s]Extractor Predicting: 28it [00:17,  1.59it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:18,  1.59it/s]Extractor Predicting: 31it [00:19,  1.60it/s]Extractor Predicting: 32it [00:20,  1.58it/s]Extractor Predicting: 33it [00:20,  1.60it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:22,  1.54it/s]Extractor Predicting: 36it [00:22,  1.58it/s]Extractor Predicting: 37it [00:23,  1.58it/s]Extractor Predicting: 38it [00:23,  1.60it/s]Extractor Predicting: 39it [00:24,  1.64it/s]Extractor Predicting: 40it [00:25,  1.62it/s]Extractor Predicting: 41it [00:25,  1.62it/s]Extractor Predicting: 42it [00:26,  1.63it/s]Extractor Predicting: 43it [00:26,  1.61it/s]Extractor Predicting: 44it [00:27,  1.57it/s]Extractor Predicting: 45it [00:28,  1.54it/s]Extractor Predicting: 46it [00:29,  1.53it/s]Extractor Predicting: 47it [00:29,  1.55it/s]Extractor Predicting: 48it [00:30,  1.56it/s]Extractor Predicting: 49it [00:30,  1.56it/s]Extractor Predicting: 50it [00:31,  1.61it/s]Extractor Predicting: 51it [00:32,  1.63it/s]Extractor Predicting: 52it [00:32,  1.63it/s]Extractor Predicting: 53it [00:33,  1.64it/s]Extractor Predicting: 54it [00:33,  1.59it/s]Extractor Predicting: 55it [00:34,  1.63it/s]Extractor Predicting: 56it [00:35,  1.61it/s]Extractor Predicting: 57it [00:35,  1.63it/s]Extractor Predicting: 58it [00:36,  1.60it/s]Extractor Predicting: 59it [00:37,  1.60it/s]Extractor Predicting: 60it [00:37,  1.60it/s]Extractor Predicting: 61it [00:38,  1.60it/s]Extractor Predicting: 62it [00:38,  1.61it/s]Extractor Predicting: 63it [00:39,  1.64it/s]Extractor Predicting: 64it [00:40,  1.64it/s]Extractor Predicting: 65it [00:40,  1.54it/s]Extractor Predicting: 66it [00:41,  1.58it/s]Extractor Predicting: 67it [00:42,  1.62it/s]Extractor Predicting: 68it [00:42,  1.59it/s]Extractor Predicting: 69it [00:43,  1.57it/s]Extractor Predicting: 70it [00:43,  1.59it/s]Extractor Predicting: 71it [00:44,  1.60it/s]Extractor Predicting: 72it [00:45,  1.63it/s]Extractor Predicting: 73it [00:45,  1.64it/s]Extractor Predicting: 74it [00:46,  1.61it/s]Extractor Predicting: 75it [00:47,  1.60it/s]Extractor Predicting: 76it [00:47,  1.63it/s]Extractor Predicting: 77it [00:48,  1.62it/s]Extractor Predicting: 78it [00:48,  1.63it/s]Extractor Predicting: 79it [00:49,  1.62it/s]Extractor Predicting: 80it [00:50,  1.62it/s]Extractor Predicting: 81it [00:50,  1.63it/s]Extractor Predicting: 82it [00:51,  1.65it/s]Extractor Predicting: 83it [00:51,  1.63it/s]Extractor Predicting: 84it [00:52,  1.61it/s]Extractor Predicting: 85it [00:53,  1.61it/s]Extractor Predicting: 86it [00:54,  1.46it/s]Extractor Predicting: 87it [00:54,  1.48it/s]Extractor Predicting: 88it [00:55,  1.50it/s]Extractor Predicting: 89it [00:55,  1.50it/s]Extractor Predicting: 90it [00:56,  1.52it/s]Extractor Predicting: 91it [00:57,  1.55it/s]Extractor Predicting: 92it [00:57,  1.57it/s]Extractor Predicting: 93it [00:58,  1.57it/s]Extractor Predicting: 94it [00:59,  1.55it/s]Extractor Predicting: 95it [00:59,  1.59it/s]Extractor Predicting: 96it [01:00,  1.59it/s]Extractor Predicting: 97it [01:01,  1.57it/s]Extractor Predicting: 98it [01:01,  1.61it/s]Extractor Predicting: 99it [01:02,  1.61it/s]Extractor Predicting: 100it [01:02,  1.58it/s]Extractor Predicting: 101it [01:03,  1.61it/s]Extractor Predicting: 102it [01:04,  1.60it/s]Extractor Predicting: 103it [01:04,  1.55it/s]Extractor Predicting: 104it [01:05,  1.57it/s]Extractor Predicting: 105it [01:06,  1.57it/s]Extractor Predicting: 106it [01:06,  1.58it/s]Extractor Predicting: 107it [01:07,  1.61it/s]Extractor Predicting: 108it [01:07,  1.55it/s]Extractor Predicting: 109it [01:08,  1.56it/s]Extractor Predicting: 110it [01:09,  1.58it/s]Extractor Predicting: 111it [01:09,  1.63it/s]Extractor Predicting: 112it [01:10,  1.64it/s]Extractor Predicting: 113it [01:10,  1.64it/s]Extractor Predicting: 114it [01:11,  1.60it/s]Extractor Predicting: 115it [01:12,  1.61it/s]Extractor Predicting: 116it [01:12,  1.62it/s]Extractor Predicting: 117it [01:13,  1.59it/s]Extractor Predicting: 118it [01:14,  1.57it/s]Extractor Predicting: 119it [01:14,  1.58it/s]Extractor Predicting: 120it [01:15,  1.57it/s]Extractor Predicting: 121it [01:16,  1.60it/s]Extractor Predicting: 122it [01:16,  1.59it/s]Extractor Predicting: 123it [01:17,  1.60it/s]Extractor Predicting: 124it [01:17,  1.60it/s]Extractor Predicting: 125it [01:18,  1.62it/s]Extractor Predicting: 126it [01:19,  1.60it/s]Extractor Predicting: 127it [01:19,  1.59it/s]Extractor Predicting: 128it [01:20,  1.58it/s]Extractor Predicting: 129it [01:21,  1.64it/s]Extractor Predicting: 130it [01:21,  1.67it/s]Extractor Predicting: 131it [01:22,  1.65it/s]Extractor Predicting: 132it [01:22,  1.61it/s]Extractor Predicting: 133it [01:23,  1.55it/s]Extractor Predicting: 134it [01:24,  1.55it/s]Extractor Predicting: 135it [01:24,  1.58it/s]Extractor Predicting: 136it [01:25,  1.57it/s]Extractor Predicting: 137it [01:26,  1.56it/s]Extractor Predicting: 138it [01:26,  1.54it/s]Extractor Predicting: 139it [01:27,  1.54it/s]Extractor Predicting: 140it [01:28,  1.55it/s]Extractor Predicting: 141it [01:28,  1.54it/s]Extractor Predicting: 142it [01:29,  1.53it/s]Extractor Predicting: 143it [01:30,  1.53it/s]Extractor Predicting: 144it [01:30,  1.54it/s]Extractor Predicting: 145it [01:31,  1.55it/s]Extractor Predicting: 146it [01:31,  1.54it/s]Extractor Predicting: 147it [01:32,  1.53it/s]Extractor Predicting: 148it [01:33,  1.57it/s]Extractor Predicting: 149it [01:33,  1.56it/s]Extractor Predicting: 150it [01:34,  1.57it/s]Extractor Predicting: 151it [01:35,  1.56it/s]Extractor Predicting: 152it [01:35,  1.61it/s]Extractor Predicting: 153it [01:36,  1.62it/s]Extractor Predicting: 154it [01:36,  1.61it/s]Extractor Predicting: 155it [01:37,  1.61it/s]Extractor Predicting: 156it [01:38,  1.60it/s]Extractor Predicting: 157it [01:38,  1.62it/s]Extractor Predicting: 158it [01:39,  1.60it/s]Extractor Predicting: 159it [01:40,  1.59it/s]Extractor Predicting: 160it [01:40,  1.61it/s]Extractor Predicting: 161it [01:41,  1.60it/s]Extractor Predicting: 162it [01:41,  1.59it/s]Extractor Predicting: 163it [01:42,  1.61it/s]Extractor Predicting: 164it [01:43,  1.61it/s]Extractor Predicting: 165it [01:43,  1.65it/s]Extractor Predicting: 166it [01:44,  1.65it/s]Extractor Predicting: 167it [01:45,  1.63it/s]Extractor Predicting: 168it [01:45,  1.64it/s]Extractor Predicting: 169it [01:46,  1.65it/s]Extractor Predicting: 170it [01:46,  1.62it/s]Extractor Predicting: 171it [01:47,  1.58it/s]Extractor Predicting: 172it [01:48,  1.59it/s]Extractor Predicting: 173it [01:48,  1.59it/s]Extractor Predicting: 174it [01:49,  1.63it/s]Extractor Predicting: 175it [01:49,  1.66it/s]Extractor Predicting: 176it [01:50,  1.59it/s]Extractor Predicting: 177it [01:51,  1.61it/s]Extractor Predicting: 178it [01:51,  1.60it/s]Extractor Predicting: 179it [01:52,  1.58it/s]Extractor Predicting: 180it [01:53,  1.64it/s]Extractor Predicting: 181it [01:53,  1.64it/s]Extractor Predicting: 182it [01:54,  1.63it/s]Extractor Predicting: 183it [01:54,  1.65it/s]Extractor Predicting: 184it [01:55,  1.67it/s]Extractor Predicting: 185it [01:56,  1.66it/s]Extractor Predicting: 186it [01:56,  1.64it/s]Extractor Predicting: 187it [01:57,  1.65it/s]Extractor Predicting: 188it [01:57,  1.63it/s]Extractor Predicting: 189it [01:58,  1.60it/s]Extractor Predicting: 190it [01:59,  1.59it/s]Extractor Predicting: 191it [01:59,  1.56it/s]Extractor Predicting: 192it [02:00,  1.61it/s]Extractor Predicting: 193it [02:01,  1.60it/s]Extractor Predicting: 194it [02:01,  1.60it/s]Extractor Predicting: 195it [02:02,  1.63it/s]Extractor Predicting: 196it [02:02,  1.67it/s]Extractor Predicting: 197it [02:03,  1.66it/s]Extractor Predicting: 198it [02:04,  1.68it/s]Extractor Predicting: 199it [02:04,  1.68it/s]Extractor Predicting: 200it [02:05,  1.64it/s]Extractor Predicting: 201it [02:05,  1.60it/s]Extractor Predicting: 202it [02:06,  1.45it/s]Extractor Predicting: 203it [02:07,  1.51it/s]Extractor Predicting: 204it [02:07,  1.56it/s]Extractor Predicting: 205it [02:08,  1.62it/s]Extractor Predicting: 206it [02:09,  1.57it/s]Extractor Predicting: 207it [02:09,  1.60it/s]Extractor Predicting: 208it [02:10,  1.65it/s]Extractor Predicting: 209it [02:11,  1.65it/s]Extractor Predicting: 210it [02:11,  1.63it/s]Extractor Predicting: 211it [02:12,  1.62it/s]Extractor Predicting: 212it [02:12,  1.61it/s]Extractor Predicting: 213it [02:13,  1.59it/s]Extractor Predicting: 214it [02:14,  1.59it/s]Extractor Predicting: 215it [02:14,  1.59it/s]Extractor Predicting: 216it [02:15,  1.59it/s]Extractor Predicting: 217it [02:16,  1.57it/s]Extractor Predicting: 218it [02:16,  1.64it/s]Extractor Predicting: 219it [02:17,  1.65it/s]Extractor Predicting: 220it [02:17,  1.62it/s]Extractor Predicting: 221it [02:18,  1.58it/s]Extractor Predicting: 222it [02:19,  1.60it/s]Extractor Predicting: 223it [02:19,  1.62it/s]Extractor Predicting: 224it [02:20,  1.62it/s]Extractor Predicting: 225it [02:20,  1.65it/s]Extractor Predicting: 226it [02:21,  1.63it/s]Extractor Predicting: 227it [02:22,  1.64it/s]Extractor Predicting: 228it [02:22,  1.62it/s]Extractor Predicting: 229it [02:23,  1.64it/s]Extractor Predicting: 230it [02:24,  1.60it/s]Extractor Predicting: 231it [02:24,  1.63it/s]Extractor Predicting: 232it [02:25,  1.61it/s]Extractor Predicting: 233it [02:25,  1.61it/s]Extractor Predicting: 234it [02:26,  1.60it/s]Extractor Predicting: 235it [02:27,  1.62it/s]Extractor Predicting: 236it [02:27,  1.58it/s]Extractor Predicting: 237it [02:28,  1.61it/s]Extractor Predicting: 238it [02:29,  1.60it/s]Extractor Predicting: 239it [02:29,  1.66it/s]Extractor Predicting: 240it [02:30,  1.63it/s]Extractor Predicting: 241it [02:30,  1.60it/s]Extractor Predicting: 242it [02:31,  1.62it/s]Extractor Predicting: 243it [02:32,  1.64it/s]Extractor Predicting: 244it [02:32,  1.67it/s]Extractor Predicting: 245it [02:33,  1.70it/s]Extractor Predicting: 246it [02:33,  1.74it/s]Extractor Predicting: 247it [02:34,  1.71it/s]Extractor Predicting: 248it [02:34,  1.71it/s]Extractor Predicting: 249it [02:35,  1.70it/s]Extractor Predicting: 250it [02:36,  1.71it/s]Extractor Predicting: 251it [02:36,  1.68it/s]Extractor Predicting: 252it [02:37,  1.69it/s]Extractor Predicting: 253it [02:37,  1.69it/s]Extractor Predicting: 254it [02:38,  1.70it/s]Extractor Predicting: 255it [02:39,  1.73it/s]Extractor Predicting: 256it [02:39,  1.71it/s]Extractor Predicting: 257it [02:40,  1.69it/s]Extractor Predicting: 258it [02:40,  1.68it/s]Extractor Predicting: 259it [02:41,  1.67it/s]Extractor Predicting: 260it [02:42,  1.68it/s]Extractor Predicting: 261it [02:42,  1.69it/s]Extractor Predicting: 262it [02:43,  1.71it/s]Extractor Predicting: 263it [02:43,  1.72it/s]Extractor Predicting: 264it [02:44,  1.67it/s]Extractor Predicting: 265it [02:45,  1.63it/s]Extractor Predicting: 266it [02:45,  1.63it/s]Extractor Predicting: 267it [02:46,  1.60it/s]Extractor Predicting: 268it [02:46,  1.62it/s]Extractor Predicting: 269it [02:47,  1.64it/s]Extractor Predicting: 270it [02:48,  1.65it/s]Extractor Predicting: 271it [02:48,  1.65it/s]Extractor Predicting: 272it [02:49,  1.65it/s]Extractor Predicting: 273it [02:49,  1.66it/s]Extractor Predicting: 274it [02:50,  1.66it/s]Extractor Predicting: 275it [02:51,  1.69it/s]Extractor Predicting: 276it [02:51,  1.67it/s]Extractor Predicting: 277it [02:52,  1.61it/s]Extractor Predicting: 278it [02:52,  1.63it/s]Extractor Predicting: 279it [02:53,  1.67it/s]Extractor Predicting: 280it [02:54,  1.70it/s]Extractor Predicting: 281it [02:54,  1.72it/s]Extractor Predicting: 282it [02:55,  1.73it/s]Extractor Predicting: 283it [02:55,  1.73it/s]Extractor Predicting: 284it [02:56,  1.71it/s]Extractor Predicting: 285it [02:57,  1.67it/s]Extractor Predicting: 286it [02:57,  1.72it/s]Extractor Predicting: 287it [02:58,  1.70it/s]Extractor Predicting: 288it [02:58,  1.68it/s]Extractor Predicting: 289it [02:59,  1.69it/s]Extractor Predicting: 290it [02:59,  1.68it/s]Extractor Predicting: 291it [03:00,  1.70it/s]Extractor Predicting: 292it [03:01,  1.65it/s]Extractor Predicting: 293it [03:01,  1.65it/s]Extractor Predicting: 294it [03:02,  1.66it/s]Extractor Predicting: 295it [03:03,  1.63it/s]Extractor Predicting: 296it [03:03,  1.65it/s]Extractor Predicting: 297it [03:04,  1.63it/s]Extractor Predicting: 298it [03:04,  1.68it/s]Extractor Predicting: 299it [03:05,  1.64it/s]Extractor Predicting: 300it [03:06,  1.60it/s]Extractor Predicting: 301it [03:06,  1.55it/s]Extractor Predicting: 302it [03:07,  1.54it/s]Extractor Predicting: 303it [03:08,  1.54it/s]Extractor Predicting: 304it [03:08,  1.56it/s]Extractor Predicting: 305it [03:09,  1.54it/s]Extractor Predicting: 306it [03:10,  1.53it/s]Extractor Predicting: 306it [03:10,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:24:31,928 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:24:31,955 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:24:31,955 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:24:31,956 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:24:31,956 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:24:32,867 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:24:32,868 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:24:33,501 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:24:34,639 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:24:34,639 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:24:37,870 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:24:37,894 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:24:37,895 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:24:37,895 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:24:37,895 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:24:38,692 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:24:38,693 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:24:39,301 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:24:39,519 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:24:39,519 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.26537997587454765,
  "recall": 0.05991285403050109,
  "score": 0.09775605421017552,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'labels': ['continent', 'field of this occupation', 'field of work', 'founded by', 'movement', 'owned by', 'performer', 'producer', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6322
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6422, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.56it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:12,  1.52it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.60it/s]Extractor Predicting: 25it [00:16,  1.59it/s]Extractor Predicting: 26it [00:16,  1.61it/s]Extractor Predicting: 27it [00:17,  1.61it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:18,  1.60it/s]Extractor Predicting: 30it [00:19,  1.61it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:20,  1.62it/s]Extractor Predicting: 33it [00:21,  1.61it/s]Extractor Predicting: 34it [00:21,  1.63it/s]Extractor Predicting: 35it [00:22,  1.63it/s]Extractor Predicting: 36it [00:22,  1.66it/s]Extractor Predicting: 37it [00:23,  1.66it/s]Extractor Predicting: 38it [00:24,  1.68it/s]Extractor Predicting: 39it [00:24,  1.66it/s]Extractor Predicting: 40it [00:25,  1.50it/s]Extractor Predicting: 41it [00:26,  1.52it/s]Extractor Predicting: 42it [00:26,  1.52it/s]Extractor Predicting: 43it [00:27,  1.52it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:28,  1.54it/s]Extractor Predicting: 46it [00:29,  1.50it/s]Extractor Predicting: 47it [00:30,  1.53it/s]Extractor Predicting: 48it [00:30,  1.56it/s]Extractor Predicting: 49it [00:31,  1.57it/s]Extractor Predicting: 50it [00:31,  1.54it/s]Extractor Predicting: 51it [00:32,  1.57it/s]Extractor Predicting: 52it [00:33,  1.56it/s]Extractor Predicting: 53it [00:33,  1.58it/s]Extractor Predicting: 54it [00:34,  1.58it/s]Extractor Predicting: 55it [00:35,  1.56it/s]Extractor Predicting: 56it [00:35,  1.57it/s]Extractor Predicting: 57it [00:36,  1.54it/s]Extractor Predicting: 58it [00:37,  1.54it/s]Extractor Predicting: 59it [00:37,  1.53it/s]Extractor Predicting: 60it [00:37,  1.88it/s]Extractor Predicting: 60it [00:37,  1.58it/s]
[INFO|configuration_utils.py:515] 2023-08-29 12:25:20,837 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:25:20,913 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 12:25:21,053 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:25:21,054 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 12:25:21,114 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 12:25:32,468 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 12:25:32,485 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 12:25:32,649 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:25:32,650 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 12:25:32,706 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:25:32,742 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:25:32,742 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:25:32,742 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:25:32,742 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:25:32,742 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:25:32,742 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.45409429280397023,
  "recall": 0.05702711124961047,
  "score": 0.10132890365448503,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 12:25:33,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:33,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:34,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:34,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:35,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:35,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:36,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:37,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:38,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:38,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:39,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:39,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:40,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:41,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:41,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:42,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:43,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:43,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:44,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:45,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:45,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:46,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:17, 14.13s/it][WARNING|generation_utils.py:914] 2023-08-29 12:25:47,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:47,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:48,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:48,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:49,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:50,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:50,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:51,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:52,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:52,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:53,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:53,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:54,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:55,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:55,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:56,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:57,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:57,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:58,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:59,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:25:59,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:27<02:55, 13.54s/it][WARNING|generation_utils.py:914] 2023-08-29 12:26:00,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:00,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:01,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:02,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:02,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:03,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:03,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:04,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:05,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:05,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:06,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:06,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:07,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:08,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:08,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:09,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:09,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:10,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:11,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:11,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:39<02:34, 12.89s/it][WARNING|generation_utils.py:914] 2023-08-29 12:26:12,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:13,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:13,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:14,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:14,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:15,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:15,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:16,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:17,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:17,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:18,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:19,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:19,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:20,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:20,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:21,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:22,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:22,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:23,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:24,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:51<02:18, 12.63s/it][WARNING|generation_utils.py:914] 2023-08-29 12:26:24,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:25,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:25,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:26,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:27,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:27,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:28,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:29,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:29,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:30,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:30,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:31,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:32,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:32,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:33,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:33,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:34,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:35,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:35,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:36,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:36,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:04<02:07, 12.72s/it][WARNING|generation_utils.py:914] 2023-08-29 12:26:37,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:38,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:38,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:39,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:39,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:40,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:40,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:41,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:41,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:42,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:42,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:43,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:43,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:44,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:45,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:45,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:46,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:46,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:47,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:47,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:48,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:15<01:50, 12.27s/it][WARNING|generation_utils.py:914] 2023-08-29 12:26:48,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:49,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:50,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:50,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:51,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:51,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:52,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:53,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:53,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:54,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:55,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:55,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:56,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:57,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:57,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:58,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:59,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:26:59,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:00,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:00,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:28<01:39, 12.38s/it][WARNING|generation_utils.py:914] 2023-08-29 12:27:01,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:02,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:02,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:03,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:04,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:04,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:05,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:06,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:06,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:07,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:08,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:08,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:09,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:10,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:10,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:11,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:12,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:12,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:13,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:14,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:41<01:28, 12.62s/it][WARNING|generation_utils.py:914] 2023-08-29 12:27:14,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:15,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:15,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:16,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:17,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:17,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:18,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:18,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:19,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:19,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:20,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:21,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:21,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:22,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:23,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:23,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:24,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:24,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:25,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:26,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:26,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:54<01:16, 12.67s/it][WARNING|generation_utils.py:914] 2023-08-29 12:27:27,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:28,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:28,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:29,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:29,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:30,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:30,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:31,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:32,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:33,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:33,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:34,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:34,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:35,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:35,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:36,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:36,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:37,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:37,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:38,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:39,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:06<01:02, 12.56s/it][WARNING|generation_utils.py:914] 2023-08-29 12:27:39,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:40,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:41,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:41,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:42,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:43,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:43,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:44,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:44,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:45,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:45,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:46,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:46,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:47,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:48,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:48,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:49,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:49,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:50,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:51,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:51,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:19<00:50, 12.51s/it][WARNING|generation_utils.py:914] 2023-08-29 12:27:52,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:52,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:53,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:54,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:54,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:55,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:55,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:56,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:57,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:57,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:58,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:59,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:27:59,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:00,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:01,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:01,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:02,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:02,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:03,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:04,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:31<00:37, 12.55s/it][WARNING|generation_utils.py:914] 2023-08-29 12:28:04,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:05,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:06,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:06,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:07,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:07,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:08,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:09,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:09,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:10,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:11,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:11,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:12,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:12,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:13,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:14,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:14,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:15,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:16,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:16,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:44<00:25, 12.55s/it][WARNING|generation_utils.py:914] 2023-08-29 12:28:17,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:17,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:18,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:19,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:19,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:20,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:21,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:21,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:22,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:23,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:23,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:24,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:24,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:25,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:26,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:26,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:27,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:28,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:28,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:29,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [02:56<00:12, 12.57s/it][WARNING|generation_utils.py:914] 2023-08-29 12:28:29,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:30,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:31,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:31,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:32,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:32,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:33,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:34,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:34,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:35,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:36,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:36,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:37,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:37,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:38,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:38,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:39,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:40,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:40,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:41,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:28:42,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:09<00:00, 12.60s/it]Generating: 100%|██████████| 15/15 [03:09<00:00, 12.64s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:28:50,342 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:28:50,370 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:28:50,370 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:28:50,370 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:28:50,370 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:28:50,867 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:28:50,868 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:28:51,188 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:28:52,341 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:28:52,341 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:28:55,571 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:28:55,588 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:28:55,588 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:28:55,588 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:28:55,588 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:28:56,346 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:28:56,347 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:28:56,954 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:28:57,179 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:28:57,179 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : country .', 'success_rate': 0.890625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : place of death .', 'success_rate': 0.9181547619047619, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 616, 'raw': 640}
{'prompt': 'Relation : production company .', 'success_rate': 0.9625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : continent .', 'success_rate': 0.9092261904761905, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 611, 'raw': 640}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.9546875, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 468, 'raw': 480}
{'target': 600, 'success': 498, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 620, 'raw': 640}
{'prompt': 'Relation : field of work .', 'success_rate': 0.96875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : founded by .', 'success_rate': 0.9285714285714286, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : movement .', 'success_rate': 0.9315476190476191, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9181547619047619, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 313, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 406, 'raw': 416}
{'target': 600, 'success': 437, 'raw': 448}
{'target': 600, 'success': 467, 'raw': 480}
{'target': 600, 'success': 499, 'raw': 512}
{'target': 600, 'success': 531, 'raw': 544}
{'target': 600, 'success': 562, 'raw': 576}
{'target': 600, 'success': 593, 'raw': 608}
{'target': 600, 'success': 624, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.975, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : producer .', 'success_rate': 0.9609375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.95625, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : replaces .', 'success_rate': 0.9107142857142857, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/4_ext.jsonl'}}
estimate vocab size: 8840
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8940, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.54it/s]Extractor Estimating: 2it [00:01,  1.50it/s]Extractor Estimating: 3it [00:01,  1.59it/s]Extractor Estimating: 4it [00:02,  1.62it/s]Extractor Estimating: 5it [00:03,  1.60it/s]Extractor Estimating: 6it [00:03,  1.57it/s]Extractor Estimating: 7it [00:04,  1.60it/s]Extractor Estimating: 8it [00:05,  1.63it/s]Extractor Estimating: 9it [00:05,  1.61it/s]Extractor Estimating: 10it [00:06,  1.52it/s]Extractor Estimating: 11it [00:07,  1.40it/s]Extractor Estimating: 12it [00:07,  1.48it/s]Extractor Estimating: 13it [00:08,  1.52it/s]Extractor Estimating: 14it [00:09,  1.53it/s]Extractor Estimating: 15it [00:09,  1.61it/s]Extractor Estimating: 16it [00:10,  1.60it/s]Extractor Estimating: 17it [00:10,  1.65it/s]Extractor Estimating: 18it [00:11,  1.63it/s]Extractor Estimating: 19it [00:12,  1.63it/s]Extractor Estimating: 20it [00:12,  1.60it/s]Extractor Estimating: 21it [00:13,  1.57it/s]Extractor Estimating: 22it [00:14,  1.55it/s]Extractor Estimating: 23it [00:14,  1.56it/s]Extractor Estimating: 24it [00:15,  1.61it/s]Extractor Estimating: 25it [00:15,  1.67it/s]Extractor Estimating: 26it [00:16,  1.60it/s]Extractor Estimating: 27it [00:17,  1.62it/s]Extractor Estimating: 28it [00:17,  1.62it/s]Extractor Estimating: 29it [00:18,  1.58it/s]Extractor Estimating: 30it [00:18,  1.59it/s]Extractor Estimating: 31it [00:19,  1.50it/s]Extractor Estimating: 32it [00:20,  1.49it/s]Extractor Estimating: 33it [00:21,  1.55it/s]Extractor Estimating: 34it [00:21,  1.53it/s]Extractor Estimating: 35it [00:22,  1.52it/s]Extractor Estimating: 36it [00:22,  1.55it/s]Extractor Estimating: 37it [00:23,  1.58it/s]Extractor Estimating: 38it [00:24,  1.57it/s]Extractor Estimating: 39it [00:24,  1.54it/s]Extractor Estimating: 40it [00:25,  1.58it/s]Extractor Estimating: 41it [00:26,  1.58it/s]Extractor Estimating: 42it [00:26,  1.57it/s]Extractor Estimating: 43it [00:27,  1.51it/s]Extractor Estimating: 44it [00:28,  1.50it/s]Extractor Estimating: 45it [00:28,  1.50it/s]Extractor Estimating: 46it [00:29,  1.52it/s]Extractor Estimating: 47it [00:30,  1.55it/s]Extractor Estimating: 48it [00:30,  1.51it/s]Extractor Estimating: 49it [00:31,  1.48it/s]Extractor Estimating: 50it [00:32,  1.47it/s]Extractor Estimating: 51it [00:32,  1.48it/s]Extractor Estimating: 52it [00:33,  1.50it/s]Extractor Estimating: 53it [00:34,  1.50it/s]Extractor Estimating: 54it [00:34,  1.54it/s]Extractor Estimating: 55it [00:35,  1.55it/s]Extractor Estimating: 56it [00:36,  1.55it/s]Extractor Estimating: 57it [00:36,  1.55it/s]Extractor Estimating: 58it [00:37,  1.55it/s]Extractor Estimating: 59it [00:38,  1.53it/s]Extractor Estimating: 60it [00:38,  1.53it/s]Extractor Estimating: 61it [00:39,  1.55it/s]Extractor Estimating: 62it [00:39,  1.57it/s]Extractor Estimating: 63it [00:40,  1.53it/s]Extractor Estimating: 64it [00:41,  1.57it/s]Extractor Estimating: 65it [00:41,  1.59it/s]Extractor Estimating: 66it [00:42,  1.56it/s]Extractor Estimating: 67it [00:43,  1.55it/s]Extractor Estimating: 68it [00:43,  1.54it/s]Extractor Estimating: 69it [00:44,  1.56it/s]Extractor Estimating: 70it [00:45,  1.55it/s]Extractor Estimating: 71it [00:45,  1.52it/s]Extractor Estimating: 72it [00:46,  1.52it/s]Extractor Estimating: 73it [00:47,  1.50it/s]Extractor Estimating: 74it [00:47,  1.54it/s]Extractor Estimating: 75it [00:48,  1.55it/s]Extractor Estimating: 76it [00:48,  1.56it/s]Extractor Estimating: 77it [00:49,  1.55it/s]Extractor Estimating: 78it [00:50,  1.54it/s]Extractor Estimating: 79it [00:50,  1.52it/s]Extractor Estimating: 80it [00:51,  1.55it/s]Extractor Estimating: 81it [00:52,  1.45it/s]Extractor Estimating: 82it [00:53,  1.47it/s]Extractor Estimating: 83it [00:53,  1.46it/s]Extractor Estimating: 84it [00:54,  1.53it/s]Extractor Estimating: 85it [00:54,  1.54it/s]Extractor Estimating: 86it [00:55,  1.43it/s]Extractor Estimating: 87it [00:56,  1.47it/s]Extractor Estimating: 88it [00:57,  1.52it/s]Extractor Estimating: 89it [00:57,  1.50it/s]Extractor Estimating: 90it [00:58,  1.52it/s]Extractor Estimating: 91it [00:58,  1.52it/s]Extractor Estimating: 92it [00:59,  1.54it/s]Extractor Estimating: 93it [01:00,  1.51it/s]Extractor Estimating: 94it [01:00,  1.53it/s]Extractor Estimating: 95it [01:01,  1.54it/s]Extractor Estimating: 96it [01:02,  1.50it/s]Extractor Estimating: 97it [01:02,  1.48it/s]Extractor Estimating: 98it [01:03,  1.48it/s]Extractor Estimating: 99it [01:04,  1.51it/s]Extractor Estimating: 100it [01:04,  1.50it/s]Extractor Estimating: 101it [01:05,  1.53it/s]Extractor Estimating: 102it [01:06,  1.58it/s]Extractor Estimating: 103it [01:06,  1.62it/s]Extractor Estimating: 104it [01:07,  1.64it/s]Extractor Estimating: 105it [01:07,  1.69it/s]Extractor Estimating: 106it [01:08,  1.77it/s]Extractor Estimating: 107it [01:09,  1.73it/s]Extractor Estimating: 108it [01:09,  1.71it/s]Extractor Estimating: 109it [01:10,  1.71it/s]Extractor Estimating: 110it [01:10,  1.75it/s]Extractor Estimating: 111it [01:11,  1.70it/s]Extractor Estimating: 112it [01:11,  1.77it/s]Extractor Estimating: 113it [01:12,  1.80it/s]Extractor Estimating: 114it [01:13,  1.73it/s]Extractor Estimating: 115it [01:13,  1.72it/s]Extractor Estimating: 116it [01:14,  1.73it/s]Extractor Estimating: 117it [01:14,  1.71it/s]Extractor Estimating: 118it [01:15,  1.70it/s]Extractor Estimating: 119it [01:15,  1.71it/s]Extractor Estimating: 120it [01:16,  1.71it/s]Extractor Estimating: 121it [01:17,  1.71it/s]Extractor Estimating: 122it [01:17,  1.71it/s]Extractor Estimating: 123it [01:18,  1.71it/s]Extractor Estimating: 124it [01:18,  1.71it/s]Extractor Estimating: 125it [01:19,  1.70it/s]Extractor Estimating: 126it [01:20,  1.78it/s]Extractor Estimating: 127it [01:20,  1.81it/s]Extractor Estimating: 128it [01:21,  1.87it/s]Extractor Estimating: 129it [01:21,  1.81it/s]Extractor Estimating: 130it [01:22,  1.87it/s]Extractor Estimating: 131it [01:22,  1.89it/s]Extractor Estimating: 132it [01:23,  1.96it/s]Extractor Estimating: 133it [01:23,  1.94it/s]Extractor Estimating: 134it [01:24,  1.95it/s]Extractor Estimating: 135it [01:24,  1.90it/s]Extractor Estimating: 136it [01:25,  1.92it/s]Extractor Estimating: 137it [01:25,  1.92it/s]Extractor Estimating: 138it [01:26,  1.99it/s]Extractor Estimating: 139it [01:26,  1.91it/s]Extractor Estimating: 140it [01:27,  1.92it/s]Extractor Estimating: 141it [01:27,  1.90it/s]Extractor Estimating: 142it [01:28,  1.89it/s]Extractor Estimating: 143it [01:28,  1.81it/s]Extractor Estimating: 144it [01:29,  1.88it/s]Extractor Estimating: 145it [01:29,  1.89it/s]Extractor Estimating: 146it [01:30,  1.92it/s]Extractor Estimating: 147it [01:31,  1.84it/s]Extractor Estimating: 148it [01:31,  1.85it/s]Extractor Estimating: 149it [01:32,  1.76it/s]Extractor Estimating: 150it [01:32,  1.80it/s]Extractor Estimating: 151it [01:33,  1.76it/s]Extractor Estimating: 152it [01:33,  1.71it/s]Extractor Estimating: 153it [01:34,  1.68it/s]Extractor Estimating: 154it [01:35,  1.62it/s]Extractor Estimating: 155it [01:35,  1.59it/s]Extractor Estimating: 156it [01:36,  1.65it/s]Extractor Estimating: 157it [01:37,  1.63it/s]Extractor Estimating: 158it [01:37,  1.62it/s]Extractor Estimating: 159it [01:38,  1.61it/s]Extractor Estimating: 160it [01:38,  1.59it/s]Extractor Estimating: 161it [01:39,  1.58it/s]Extractor Estimating: 162it [01:40,  1.54it/s]Extractor Estimating: 163it [01:41,  1.42it/s]Extractor Estimating: 164it [01:41,  1.44it/s]Extractor Estimating: 165it [01:42,  1.42it/s]Extractor Estimating: 166it [01:43,  1.46it/s]Extractor Estimating: 167it [01:43,  1.47it/s]Extractor Estimating: 168it [01:44,  1.49it/s]Extractor Estimating: 169it [01:45,  1.48it/s]Extractor Estimating: 170it [01:45,  1.48it/s]Extractor Estimating: 171it [01:46,  1.51it/s]Extractor Estimating: 172it [01:47,  1.55it/s]Extractor Estimating: 173it [01:47,  1.53it/s]Extractor Estimating: 174it [01:48,  1.54it/s]Extractor Estimating: 175it [01:49,  1.55it/s]Extractor Estimating: 176it [01:49,  1.55it/s]Extractor Estimating: 177it [01:50,  1.57it/s]Extractor Estimating: 178it [01:50,  1.58it/s]Extractor Estimating: 179it [01:51,  1.55it/s]Extractor Estimating: 180it [01:52,  1.56it/s]Extractor Estimating: 181it [01:52,  1.51it/s]Extractor Estimating: 182it [01:53,  1.53it/s]Extractor Estimating: 183it [01:54,  1.53it/s]Extractor Estimating: 184it [01:54,  1.58it/s]Extractor Estimating: 185it [01:55,  1.58it/s]Extractor Estimating: 186it [01:56,  1.56it/s]Extractor Estimating: 187it [01:56,  1.56it/s]Extractor Estimating: 188it [01:57,  1.54it/s]Extractor Estimating: 189it [01:58,  1.57it/s]Extractor Estimating: 190it [01:58,  1.57it/s]Extractor Estimating: 191it [01:59,  1.54it/s]Extractor Estimating: 192it [02:00,  1.50it/s]Extractor Estimating: 193it [02:00,  1.53it/s]Extractor Estimating: 194it [02:01,  1.58it/s]Extractor Estimating: 195it [02:01,  1.54it/s]Extractor Estimating: 196it [02:02,  1.56it/s]Extractor Estimating: 197it [02:03,  1.54it/s]Extractor Estimating: 198it [02:03,  1.54it/s]Extractor Estimating: 199it [02:04,  1.54it/s]Extractor Estimating: 200it [02:05,  1.56it/s]Extractor Estimating: 201it [02:05,  1.62it/s]Extractor Estimating: 202it [02:06,  1.63it/s]Extractor Estimating: 203it [02:06,  1.64it/s]Extractor Estimating: 204it [02:07,  1.70it/s]Extractor Estimating: 205it [02:08,  1.71it/s]Extractor Estimating: 206it [02:08,  1.73it/s]Extractor Estimating: 207it [02:09,  1.65it/s]Extractor Estimating: 208it [02:09,  1.67it/s]Extractor Estimating: 209it [02:10,  1.69it/s]Extractor Estimating: 210it [02:10,  1.75it/s]Extractor Estimating: 211it [02:11,  1.70it/s]Extractor Estimating: 212it [02:12,  1.75it/s]Extractor Estimating: 213it [02:12,  1.67it/s]Extractor Estimating: 214it [02:13,  1.67it/s]Extractor Estimating: 215it [02:13,  1.71it/s]Extractor Estimating: 216it [02:14,  1.71it/s]Extractor Estimating: 217it [02:15,  1.69it/s]Extractor Estimating: 218it [02:15,  1.77it/s]Extractor Estimating: 219it [02:16,  1.81it/s]Extractor Estimating: 220it [02:16,  1.74it/s]Extractor Estimating: 221it [02:17,  1.70it/s]Extractor Estimating: 222it [02:18,  1.65it/s]Extractor Estimating: 223it [02:18,  1.70it/s]Extractor Estimating: 224it [02:19,  1.76it/s]Extractor Estimating: 225it [02:19,  1.73it/s]Extractor Estimating: 226it [02:20,  1.72it/s]Extractor Estimating: 227it [02:20,  1.71it/s]Extractor Estimating: 228it [02:21,  1.69it/s]Extractor Estimating: 229it [02:22,  1.71it/s]Extractor Estimating: 230it [02:22,  1.70it/s]Extractor Estimating: 231it [02:23,  1.71it/s]Extractor Estimating: 232it [02:23,  1.66it/s]Extractor Estimating: 233it [02:24,  1.68it/s]Extractor Estimating: 234it [02:25,  1.52it/s]Extractor Estimating: 235it [02:25,  1.53it/s]Extractor Estimating: 236it [02:26,  1.61it/s]Extractor Estimating: 237it [02:27,  1.53it/s]Extractor Estimating: 238it [02:27,  1.61it/s]Extractor Estimating: 239it [02:28,  1.66it/s]Extractor Estimating: 240it [02:28,  1.71it/s]Extractor Estimating: 241it [02:29,  1.70it/s]Extractor Estimating: 242it [02:30,  1.69it/s]Extractor Estimating: 243it [02:30,  1.65it/s]Extractor Estimating: 244it [02:31,  1.68it/s]Extractor Estimating: 245it [02:31,  1.72it/s]Extractor Estimating: 246it [02:32,  1.75it/s]Extractor Estimating: 247it [02:32,  1.77it/s]Extractor Estimating: 248it [02:33,  1.68it/s]Extractor Estimating: 249it [02:34,  1.64it/s]Extractor Estimating: 250it [02:34,  1.69it/s]Extractor Estimating: 251it [02:35,  1.81it/s]Extractor Estimating: 252it [02:35,  1.71it/s]Extractor Estimating: 253it [02:36,  1.72it/s]Extractor Estimating: 254it [02:37,  1.68it/s]Extractor Estimating: 255it [02:37,  1.63it/s]Extractor Estimating: 256it [02:38,  1.70it/s]Extractor Estimating: 257it [02:38,  1.74it/s]Extractor Estimating: 258it [02:39,  1.74it/s]Extractor Estimating: 259it [02:39,  1.75it/s]Extractor Estimating: 260it [02:40,  1.78it/s]Extractor Estimating: 261it [02:41,  1.72it/s]Extractor Estimating: 262it [02:41,  1.72it/s]Extractor Estimating: 263it [02:42,  1.74it/s]Extractor Estimating: 264it [02:42,  1.78it/s]Extractor Estimating: 265it [02:43,  1.79it/s]Extractor Estimating: 266it [02:43,  1.77it/s]Extractor Estimating: 267it [02:44,  1.76it/s]Extractor Estimating: 268it [02:45,  1.82it/s]Extractor Estimating: 269it [02:45,  1.83it/s]Extractor Estimating: 270it [02:46,  1.81it/s]Extractor Estimating: 271it [02:46,  1.75it/s]Extractor Estimating: 272it [02:47,  1.82it/s]Extractor Estimating: 273it [02:47,  1.84it/s]Extractor Estimating: 274it [02:48,  1.88it/s]Extractor Estimating: 275it [02:48,  1.88it/s]Extractor Estimating: 276it [02:49,  1.74it/s]Extractor Estimating: 277it [02:50,  1.68it/s]Extractor Estimating: 278it [02:50,  1.63it/s]Extractor Estimating: 279it [02:51,  1.63it/s]Extractor Estimating: 280it [02:51,  1.69it/s]Extractor Estimating: 281it [02:52,  1.66it/s]Extractor Estimating: 282it [02:53,  1.67it/s]Extractor Estimating: 283it [02:53,  1.62it/s]Extractor Estimating: 284it [02:54,  1.50it/s]Extractor Estimating: 285it [02:55,  1.54it/s]Extractor Estimating: 286it [02:55,  1.54it/s]Extractor Estimating: 287it [02:56,  1.56it/s]Extractor Estimating: 288it [02:57,  1.58it/s]Extractor Estimating: 289it [02:57,  1.56it/s]Extractor Estimating: 290it [02:58,  1.58it/s]Extractor Estimating: 291it [02:59,  1.52it/s]Extractor Estimating: 292it [02:59,  1.55it/s]Extractor Estimating: 293it [03:00,  1.54it/s]Extractor Estimating: 294it [03:01,  1.52it/s]Extractor Estimating: 295it [03:01,  1.60it/s]Extractor Estimating: 296it [03:02,  1.59it/s]Extractor Estimating: 297it [03:02,  1.59it/s]Extractor Estimating: 298it [03:03,  1.56it/s]Extractor Estimating: 299it [03:04,  1.57it/s]Extractor Estimating: 300it [03:04,  1.55it/s]Extractor Estimating: 301it [03:05,  1.55it/s]Extractor Estimating: 302it [03:06,  1.57it/s]Extractor Estimating: 303it [03:06,  1.58it/s]Extractor Estimating: 304it [03:07,  1.53it/s]Extractor Estimating: 305it [03:07,  1.57it/s]Extractor Estimating: 306it [03:08,  1.58it/s]Extractor Estimating: 307it [03:09,  1.58it/s]Extractor Estimating: 308it [03:09,  1.59it/s]Extractor Estimating: 309it [03:10,  1.53it/s]Extractor Estimating: 310it [03:11,  1.40it/s]Extractor Estimating: 311it [03:12,  1.43it/s]Extractor Estimating: 312it [03:12,  1.45it/s]Extractor Estimating: 313it [03:13,  1.48it/s]Extractor Estimating: 314it [03:14,  1.51it/s]Extractor Estimating: 315it [03:14,  1.56it/s]Extractor Estimating: 316it [03:15,  1.64it/s]Extractor Estimating: 317it [03:15,  1.65it/s]Extractor Estimating: 318it [03:16,  1.62it/s]Extractor Estimating: 319it [03:17,  1.62it/s]Extractor Estimating: 320it [03:17,  1.65it/s]Extractor Estimating: 321it [03:18,  1.67it/s]Extractor Estimating: 322it [03:18,  1.61it/s]Extractor Estimating: 323it [03:19,  1.57it/s]Extractor Estimating: 324it [03:20,  1.55it/s]Extractor Estimating: 325it [03:20,  1.49it/s]Extractor Estimating: 326it [03:21,  1.48it/s]Extractor Estimating: 327it [03:22,  1.46it/s]Extractor Estimating: 328it [03:23,  1.45it/s]Extractor Estimating: 329it [03:23,  1.43it/s]Extractor Estimating: 330it [03:24,  1.41it/s]Extractor Estimating: 331it [03:25,  1.42it/s]Extractor Estimating: 332it [03:25,  1.44it/s]Extractor Estimating: 333it [03:26,  1.42it/s]Extractor Estimating: 334it [03:27,  1.37it/s]Extractor Estimating: 335it [03:28,  1.42it/s]Extractor Estimating: 336it [03:28,  1.37it/s]Extractor Estimating: 337it [03:29,  1.38it/s]Extractor Estimating: 338it [03:30,  1.41it/s]Extractor Estimating: 339it [03:30,  1.42it/s]Extractor Estimating: 340it [03:31,  1.47it/s]Extractor Estimating: 341it [03:32,  1.46it/s]Extractor Estimating: 342it [03:32,  1.45it/s]Extractor Estimating: 343it [03:33,  1.44it/s]Extractor Estimating: 344it [03:34,  1.51it/s]Extractor Estimating: 345it [03:35,  1.42it/s]Extractor Estimating: 346it [03:35,  1.35it/s]Extractor Estimating: 347it [03:36,  1.39it/s]Extractor Estimating: 348it [03:37,  1.42it/s]Extractor Estimating: 349it [03:37,  1.42it/s]Extractor Estimating: 350it [03:38,  1.49it/s]Extractor Estimating: 351it [03:39,  1.52it/s]Extractor Estimating: 352it [03:39,  1.55it/s]Extractor Estimating: 353it [03:40,  1.56it/s]Extractor Estimating: 354it [03:40,  1.63it/s]Extractor Estimating: 355it [03:41,  1.62it/s]Extractor Estimating: 356it [03:42,  1.64it/s]Extractor Estimating: 357it [03:42,  1.65it/s]Extractor Estimating: 358it [03:43,  1.69it/s]Extractor Estimating: 359it [03:43,  1.69it/s]Extractor Estimating: 360it [03:44,  1.70it/s]Extractor Estimating: 361it [03:45,  1.72it/s]Extractor Estimating: 362it [03:45,  1.68it/s]Extractor Estimating: 363it [03:46,  1.67it/s]Extractor Estimating: 364it [03:46,  1.75it/s]Extractor Estimating: 365it [03:47,  1.73it/s]Extractor Estimating: 366it [03:47,  1.75it/s]Extractor Estimating: 367it [03:48,  1.76it/s]Extractor Estimating: 368it [03:49,  1.75it/s]Extractor Estimating: 369it [03:49,  1.71it/s]Extractor Estimating: 370it [03:50,  1.77it/s]Extractor Estimating: 371it [03:50,  1.77it/s]Extractor Estimating: 372it [03:51,  1.77it/s]Extractor Estimating: 373it [03:51,  1.75it/s]Extractor Estimating: 374it [03:52,  1.64it/s]Extractor Estimating: 374it [03:52,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:33:10,477 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:33:10,497 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:33:10,498 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:33:10,498 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:33:10,498 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:33:11,149 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:33:11,150 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:33:11,744 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:33:12,832 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:33:12,832 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:33:15,761 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:33:15,778 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:33:15,778 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:33:15,778 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:33:15,778 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:33:16,421 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:33:16,421 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:33:17,015 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:33:17,187 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:33:17,187 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 15:11:42,809 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 15:11:43,253 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7478 mean pseudo reward: 0.9367824456149254
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl'}
train vocab size: 24180
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24280, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24280, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.060, loss:723.5754
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.059, loss:672.7796
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.110, loss:632.9387
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 1.061, loss:588.1213
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 1.054, loss:611.9564
>> valid entity prec:0.5339, rec:0.4565, f1:0.4922
>> valid relation prec:0.1719, rec:0.0382, f1:0.0626
>> valid relation with NER prec:0.1719, rec:0.0382, f1:0.0626
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 3.649, loss:609.6099
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.038, loss:575.1367
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.036, loss:587.6030
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 1.044, loss:597.2260
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 1.043, loss:572.2153
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5209, rec:0.4623, f1:0.4899
>> valid relation prec:0.1654, rec:0.0381, f1:0.0620
>> valid relation with NER prec:0.1654, rec:0.0381, f1:0.0620
g_step 1100, step 164, avg_time 3.614, loss:569.7414
g_step 1200, step 264, avg_time 1.039, loss:577.7315
g_step 1300, step 52, avg_time 1.042, loss:537.6960
g_step 1400, step 152, avg_time 1.045, loss:513.8335
g_step 1500, step 252, avg_time 1.041, loss:550.1568
>> valid entity prec:0.5212, rec:0.5209, f1:0.5210
>> valid relation prec:0.1384, rec:0.0423, f1:0.0648
>> valid relation with NER prec:0.1384, rec:0.0423, f1:0.0648
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 40, avg_time 3.668, loss:516.1144
g_step 1700, step 140, avg_time 1.048, loss:506.0448
g_step 1800, step 240, avg_time 1.036, loss:530.4383
g_step 1900, step 28, avg_time 1.102, loss:520.4369
g_step 2000, step 128, avg_time 1.030, loss:480.8530
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5344, rec:0.4980, f1:0.5156
>> valid relation prec:0.1449, rec:0.0513, f1:0.0758
>> valid relation with NER prec:0.1449, rec:0.0513, f1:0.0758
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 228, avg_time 3.741, loss:488.6076
g_step 2200, step 16, avg_time 1.036, loss:470.7721
g_step 2300, step 116, avg_time 1.021, loss:461.0609
g_step 2400, step 216, avg_time 1.039, loss:479.6029
g_step 2500, step 4, avg_time 1.027, loss:476.1740
>> valid entity prec:0.5085, rec:0.5044, f1:0.5064
>> valid relation prec:0.1518, rec:0.0551, f1:0.0808
>> valid relation with NER prec:0.1518, rec:0.0551, f1:0.0808
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 104, avg_time 3.590, loss:446.2793
g_step 2700, step 204, avg_time 1.013, loss:440.7517
g_step 2800, step 304, avg_time 1.007, loss:480.8035
g_step 2900, step 92, avg_time 1.011, loss:426.2480
g_step 3000, step 192, avg_time 1.021, loss:439.2472
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5023, rec:0.4555, f1:0.4778
>> valid relation prec:0.1059, rec:0.0375, f1:0.0554
>> valid relation with NER prec:0.1059, rec:0.0375, f1:0.0554
g_step 3100, step 292, avg_time 3.546, loss:439.7869
g_step 3200, step 80, avg_time 0.995, loss:409.7924
g_step 3300, step 180, avg_time 1.022, loss:404.5436
g_step 3400, step 280, avg_time 1.014, loss:430.9384
g_step 3500, step 68, avg_time 1.006, loss:393.0075
>> valid entity prec:0.5165, rec:0.4689, f1:0.4916
>> valid relation prec:0.1380, rec:0.0516, f1:0.0751
>> valid relation with NER prec:0.1380, rec:0.0516, f1:0.0751
g_step 3600, step 168, avg_time 3.552, loss:390.7250
g_step 3700, step 268, avg_time 1.000, loss:412.5939
g_step 3800, step 56, avg_time 1.000, loss:394.5920
g_step 3900, step 156, avg_time 0.986, loss:383.2176
g_step 4000, step 256, avg_time 1.009, loss:405.2751
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5291, rec:0.4689, f1:0.4972
>> valid relation prec:0.1156, rec:0.0427, f1:0.0623
>> valid relation with NER prec:0.1156, rec:0.0427, f1:0.0623
g_step 4100, step 44, avg_time 3.523, loss:390.1045
g_step 4200, step 144, avg_time 1.008, loss:393.5767
g_step 4300, step 244, avg_time 1.008, loss:382.1637
g_step 4400, step 32, avg_time 0.989, loss:391.5065
g_step 4500, step 132, avg_time 1.026, loss:352.2241
>> valid entity prec:0.5313, rec:0.3881, f1:0.4486
>> valid relation prec:0.1241, rec:0.0373, f1:0.0574
>> valid relation with NER prec:0.1241, rec:0.0373, f1:0.0574
g_step 4600, step 232, avg_time 3.595, loss:374.1508
g_step 4700, step 20, avg_time 1.033, loss:398.0820
g_step 4800, step 120, avg_time 1.032, loss:346.5819
g_step 4900, step 220, avg_time 1.040, loss:360.3488
g_step 5000, step 8, avg_time 1.030, loss:374.6311
learning rate was adjusted to 0.0008
>> valid entity prec:0.5426, rec:0.4147, f1:0.4701
>> valid relation prec:0.1248, rec:0.0463, f1:0.0675
>> valid relation with NER prec:0.1248, rec:0.0463, f1:0.0675
g_step 5100, step 108, avg_time 3.600, loss:329.1435
g_step 5200, step 208, avg_time 1.018, loss:346.6057
g_step 5300, step 308, avg_time 1.000, loss:342.1904
g_step 5400, step 96, avg_time 1.016, loss:317.8027
g_step 5500, step 196, avg_time 1.011, loss:347.7507
>> valid entity prec:0.5173, rec:0.3857, f1:0.4419
>> valid relation prec:0.1397, rec:0.0490, f1:0.0725
>> valid relation with NER prec:0.1397, rec:0.0490, f1:0.0725
g_step 5600, step 296, avg_time 3.568, loss:337.1428
g_step 5700, step 84, avg_time 1.019, loss:312.3104
g_step 5800, step 184, avg_time 1.009, loss:329.9473
g_step 5900, step 284, avg_time 1.007, loss:343.2947
g_step 6000, step 72, avg_time 1.019, loss:313.4573
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5183, rec:0.4825, f1:0.4997
>> valid relation prec:0.1587, rec:0.0637, f1:0.0909
>> valid relation with NER prec:0.1587, rec:0.0637, f1:0.0909
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 6100, step 172, avg_time 3.576, loss:330.6106
g_step 6200, step 272, avg_time 1.017, loss:322.8557
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 15:11:43 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 15:11:43 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_15-11-42_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 15:11:45 - WARNING - datasets.builder -   Using custom data configuration default-54c9ac5231dd52e4
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-54c9ac5231dd52e4/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:02,  2.95s/ tables]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 15:11:52,830 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 15:11:52,832 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 15:11:52,832 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 15:11:52,833 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 15:11:53,004 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 15:11:53,075 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 15:11:53,075 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 15:11:53,075 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 15:11:53,075 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 15:11:53,075 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 15:11:53,075 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 15:11:54,600 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 15:11:57,820 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 15:11:57,876 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-54c9ac5231dd52e4/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:01<00:07,  1.04s/ba] 25%|██▌       | 2/8 [00:01<00:03,  1.82ba/s] 38%|███▊      | 3/8 [00:01<00:01,  2.56ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.17ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.65ba/s] 75%|███████▌  | 6/8 [00:02<00:00,  4.03ba/s] 88%|████████▊ | 7/8 [00:02<00:00,  4.30ba/s]100%|██████████| 8/8 [00:02<00:00,  5.23ba/s]100%|██████████| 8/8 [00:02<00:00,  3.40ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:02,  3.04ba/s] 22%|██▏       | 2/9 [00:00<00:01,  3.54ba/s] 33%|███▎      | 3/9 [00:00<00:01,  4.01ba/s] 44%|████▍     | 4/9 [00:00<00:01,  4.31ba/s] 56%|█████▌    | 5/9 [00:01<00:00,  4.48ba/s] 67%|██████▋   | 6/9 [00:01<00:00,  4.64ba/s] 78%|███████▊  | 7/9 [00:01<00:00,  4.72ba/s] 89%|████████▉ | 8/9 [00:01<00:00,  4.74ba/s]100%|██████████| 9/9 [00:01<00:00,  5.49ba/s]100%|██████████| 9/9 [00:01<00:00,  4.65ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  2.13ba/s] 38%|███▊      | 3/8 [00:00<00:00,  5.37ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  7.48ba/s] 75%|███████▌  | 6/8 [00:00<00:00,  6.78ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  5.93ba/s]100%|██████████| 8/8 [00:01<00:00,  6.33ba/s]
  0%|          | 0/9 [00:00<?, ?ba/s] 11%|█         | 1/9 [00:00<00:03,  2.20ba/s] 22%|██▏       | 2/9 [00:00<00:01,  3.95ba/s] 44%|████▍     | 4/9 [00:00<00:00,  6.65ba/s] 67%|██████▋   | 6/9 [00:00<00:00,  8.33ba/s] 78%|███████▊  | 7/9 [00:01<00:00,  7.51ba/s]100%|██████████| 9/9 [00:01<00:00,  9.41ba/s]100%|██████████| 9/9 [00:01<00:00,  7.33ba/s]
[INFO|trainer.py:414] 2023-08-29 15:12:08,486 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 15:12:09,018 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 15:12:09,018 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 15:12:09,018 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 15:12:09,018 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 15:12:09,019 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 15:12:09,019 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 15:12:09,019 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:05<55:30,  5.70s/it]  0%|          | 2/585 [00:06<26:51,  2.76s/it]  1%|          | 3/585 [00:07<17:22,  1.79s/it]  1%|          | 4/585 [00:07<12:00,  1.24s/it]  1%|          | 5/585 [00:07<08:53,  1.09it/s]  1%|          | 6/585 [00:08<06:55,  1.39it/s]  1%|          | 7/585 [00:08<05:34,  1.73it/s]  1%|▏         | 8/585 [00:08<04:46,  2.01it/s]  2%|▏         | 9/585 [00:09<04:19,  2.22it/s]  2%|▏         | 10/585 [00:09<03:50,  2.49it/s]  2%|▏         | 11/585 [00:09<03:30,  2.72it/s]  2%|▏         | 12/585 [00:09<03:16,  2.91it/s]  2%|▏         | 13/585 [00:10<03:07,  3.05it/s]  2%|▏         | 14/585 [00:10<03:00,  3.16it/s]  3%|▎         | 15/585 [00:10<02:56,  3.24it/s]  3%|▎         | 16/585 [00:11<02:52,  3.29it/s]  3%|▎         | 17/585 [00:11<02:50,  3.34it/s]  3%|▎         | 18/585 [00:11<02:48,  3.37it/s]  3%|▎         | 19/585 [00:11<02:46,  3.39it/s]  3%|▎         | 20/585 [00:12<02:55,  3.22it/s]  4%|▎         | 21/585 [00:12<02:51,  3.28it/s]  4%|▍         | 22/585 [00:12<02:49,  3.32it/s]  4%|▍         | 23/585 [00:13<02:48,  3.34it/s]  4%|▍         | 24/585 [00:13<02:46,  3.36it/s]  4%|▍         | 25/585 [00:13<02:45,  3.38it/s]  4%|▍         | 26/585 [00:14<02:44,  3.40it/s]  5%|▍         | 27/585 [00:14<02:43,  3.41it/s]  5%|▍         | 28/585 [00:14<02:43,  3.42it/s]  5%|▍         | 29/585 [00:14<02:42,  3.41it/s]  5%|▌         | 30/585 [00:15<02:42,  3.41it/s]  5%|▌         | 31/585 [00:15<02:51,  3.23it/s]  5%|▌         | 32/585 [00:15<02:48,  3.28it/s]  6%|▌         | 33/585 [00:16<02:46,  3.32it/s]  6%|▌         | 34/585 [00:16<02:44,  3.35it/s]  6%|▌         | 35/585 [00:16<02:43,  3.37it/s]  6%|▌         | 36/585 [00:17<02:42,  3.38it/s]  6%|▋         | 37/585 [00:17<02:41,  3.39it/s]  6%|▋         | 38/585 [00:17<02:40,  3.41it/s]  7%|▋         | 39/585 [00:17<02:39,  3.42it/s]  7%|▋         | 40/585 [00:18<02:39,  3.42it/s]  7%|▋         | 41/585 [00:18<02:38,  3.43it/s]  7%|▋         | 42/585 [00:18<02:46,  3.26it/s]  7%|▋         | 43/585 [00:19<02:43,  3.31it/s]  8%|▊         | 44/585 [00:19<02:41,  3.34it/s]  8%|▊         | 45/585 [00:19<02:40,  3.36it/s]  8%|▊         | 46/585 [00:20<02:39,  3.38it/s]  8%|▊         | 47/585 [00:20<02:38,  3.39it/s]  8%|▊         | 48/585 [00:20<02:38,  3.40it/s]  8%|▊         | 49/585 [00:20<02:37,  3.40it/s]  9%|▊         | 50/585 [00:21<02:37,  3.41it/s]  9%|▊         | 51/585 [00:21<02:36,  3.41it/s]  9%|▉         | 52/585 [00:21<02:36,  3.41it/s]  9%|▉         | 53/585 [00:22<02:41,  3.29it/s]  9%|▉         | 54/585 [00:22<02:39,  3.33it/s]  9%|▉         | 55/585 [00:22<02:38,  3.35it/s] 10%|▉         | 56/585 [00:23<02:36,  3.37it/s] 10%|▉         | 57/585 [00:23<02:35,  3.39it/s] 10%|▉         | 58/585 [00:23<02:34,  3.41it/s] 10%|█         | 59/585 [00:23<02:34,  3.41it/s] 10%|█         | 60/585 [00:24<02:33,  3.42it/s] 10%|█         | 61/585 [00:24<02:32,  3.43it/s] 11%|█         | 62/585 [00:24<02:32,  3.44it/s] 11%|█         | 63/585 [00:25<02:31,  3.44it/s] 11%|█         | 64/585 [00:25<02:37,  3.30it/s] 11%|█         | 65/585 [00:25<02:35,  3.34it/s] 11%|█▏        | 66/585 [00:25<02:33,  3.39it/s] 11%|█▏        | 67/585 [00:26<02:31,  3.42it/s] 12%|█▏        | 68/585 [00:26<02:30,  3.44it/s] 12%|█▏        | 69/585 [00:26<02:29,  3.46it/s] 12%|█▏        | 70/585 [00:27<02:28,  3.46it/s] 12%|█▏        | 71/585 [00:27<02:28,  3.46it/s] 12%|█▏        | 72/585 [00:27<02:27,  3.47it/s] 12%|█▏        | 73/585 [00:27<02:27,  3.48it/s] 13%|█▎        | 74/585 [00:28<02:26,  3.48it/s] 13%|█▎        | 75/585 [00:28<02:33,  3.32it/s] 13%|█▎        | 76/585 [00:28<02:31,  3.36it/s] 13%|█▎        | 77/585 [00:29<02:29,  3.39it/s] 13%|█▎        | 78/585 [00:29<02:28,  3.42it/s] 14%|█▎        | 79/585 [00:29<02:27,  3.44it/s] 14%|█▎        | 80/585 [00:30<02:26,  3.45it/s] 14%|█▍        | 81/585 [00:30<02:26,  3.45it/s] 14%|█▍        | 82/585 [00:30<02:31,  3.33it/s] 14%|█▍        | 83/585 [00:30<02:29,  3.36it/s] 14%|█▍        | 84/585 [00:31<02:27,  3.40it/s] 15%|█▍        | 85/585 [00:31<02:25,  3.43it/s] 15%|█▍        | 86/585 [00:31<02:31,  3.29it/s] 15%|█▍        | 87/585 [00:32<02:28,  3.34it/s] 15%|█▌        | 88/585 [00:32<02:27,  3.38it/s] 15%|█▌        | 89/585 [00:32<02:25,  3.41it/s] 15%|█▌        | 90/585 [00:32<02:24,  3.43it/s] 16%|█▌        | 91/585 [00:33<02:23,  3.44it/s] 16%|█▌        | 92/585 [00:33<02:22,  3.45it/s] 16%|█▌        | 93/585 [00:33<02:22,  3.46it/s] 16%|█▌        | 94/585 [00:34<03:17,  2.48it/s] 16%|█▌        | 95/585 [00:34<03:07,  2.61it/s] 16%|█▋        | 96/585 [00:35<02:53,  2.82it/s] 17%|█▋        | 97/585 [00:35<02:43,  2.98it/s] 17%|█▋        | 98/585 [00:35<02:36,  3.11it/s] 17%|█▋        | 99/585 [00:35<02:31,  3.21it/s] 17%|█▋        | 100/585 [00:36<02:27,  3.29it/s] 17%|█▋        | 101/585 [00:36<02:24,  3.34it/s] 17%|█▋        | 102/585 [00:36<02:23,  3.37it/s] 18%|█▊        | 103/585 [00:37<02:21,  3.40it/s] 18%|█▊        | 104/585 [00:37<02:20,  3.42it/s] 18%|█▊        | 105/585 [00:37<02:19,  3.43it/s] 18%|█▊        | 106/585 [00:38<02:26,  3.28it/s] 18%|█▊        | 107/585 [00:38<02:23,  3.34it/s] 18%|█▊        | 108/585 [00:38<02:21,  3.38it/s] 19%|█▊        | 109/585 [00:38<02:19,  3.41it/s] 19%|█▉        | 110/585 [00:39<02:18,  3.42it/s] 19%|█▉        | 111/585 [00:39<02:17,  3.44it/s] 19%|█▉        | 112/585 [00:39<02:17,  3.45it/s] 19%|█▉        | 113/585 [00:40<02:16,  3.46it/s] 19%|█▉        | 114/585 [00:40<02:16,  3.46it/s] 20%|█▉        | 115/585 [00:40<02:15,  3.46it/s] 20%|█▉        | 116/585 [00:40<02:15,  3.46it/s] 20%|██        | 117/585 [00:41<02:15,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 15:12:50,292 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 15:12:50,293 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 15:12:50,293 >>   Batch size = 8

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:18, 56.48it/s][A
  1%|          | 12/1071 [00:00<00:21, 49.16it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 47.24it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 46.33it/s][A
  3%|▎         | 27/1071 [00:00<00:22, 45.78it/s][A
  3%|▎         | 32/1071 [00:00<00:22, 45.40it/s][A
  3%|▎         | 37/1071 [00:00<00:22, 45.16it/s][A
  4%|▍         | 42/1071 [00:00<00:22, 44.81it/s][A
  4%|▍         | 47/1071 [00:01<00:22, 44.95it/s][A
  5%|▍         | 52/1071 [00:01<00:22, 45.12it/s][A
  5%|▌         | 57/1071 [00:01<00:23, 42.42it/s][A
  6%|▌         | 62/1071 [00:01<00:23, 43.39it/s][A
  6%|▋         | 67/1071 [00:01<00:22, 44.01it/s][A
  7%|▋         | 72/1071 [00:01<00:22, 44.41it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 44.53it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 44.53it/s][A
  8%|▊         | 87/1071 [00:01<00:22, 44.64it/s][A
  9%|▊         | 92/1071 [00:02<00:21, 44.61it/s][A
  9%|▉         | 97/1071 [00:02<00:21, 44.61it/s][A
 10%|▉         | 102/1071 [00:02<00:21, 44.64it/s][A
 10%|▉         | 107/1071 [00:02<00:21, 44.82it/s][A
 10%|█         | 112/1071 [00:02<00:21, 44.99it/s][A
 11%|█         | 117/1071 [00:02<00:21, 45.13it/s][A
 11%|█▏        | 122/1071 [00:02<00:21, 45.16it/s][A
 12%|█▏        | 127/1071 [00:02<00:20, 45.02it/s][A
 12%|█▏        | 132/1071 [00:02<00:20, 45.05it/s][A
 13%|█▎        | 137/1071 [00:03<00:20, 44.87it/s][A
 13%|█▎        | 142/1071 [00:03<00:20, 44.80it/s][A
 14%|█▎        | 147/1071 [00:03<00:20, 44.71it/s][A
 14%|█▍        | 152/1071 [00:03<00:20, 44.90it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 45.00it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 45.18it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 44.99it/s][A
 16%|█▌        | 172/1071 [00:03<00:19, 44.98it/s][A
 17%|█▋        | 177/1071 [00:03<00:19, 44.97it/s][A
 17%|█▋        | 182/1071 [00:04<00:19, 44.83it/s][A
 17%|█▋        | 187/1071 [00:04<00:19, 44.76it/s][A
 18%|█▊        | 192/1071 [00:04<00:20, 43.51it/s][A
 18%|█▊        | 197/1071 [00:04<00:19, 43.89it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 44.43it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 44.80it/s][A
 20%|█▉        | 212/1071 [00:04<00:19, 44.83it/s][A
 20%|██        | 217/1071 [00:04<00:19, 44.88it/s][A
 21%|██        | 222/1071 [00:04<00:18, 44.88it/s][A
 21%|██        | 227/1071 [00:05<00:18, 44.84it/s][A
 22%|██▏       | 232/1071 [00:05<00:18, 44.64it/s][A
 22%|██▏       | 237/1071 [00:05<00:18, 44.66it/s][A
 23%|██▎       | 242/1071 [00:05<00:18, 44.89it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 45.08it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 45.09it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 45.07it/s][A
 24%|██▍       | 262/1071 [00:05<00:17, 45.09it/s][A
 25%|██▍       | 267/1071 [00:05<00:17, 44.95it/s][A
 25%|██▌       | 272/1071 [00:06<00:17, 44.85it/s][A
 26%|██▌       | 277/1071 [00:06<00:17, 44.79it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 44.66it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 44.84it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 45.03it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 45.19it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 45.13it/s][A
 29%|██▊       | 307/1071 [00:06<00:16, 45.08it/s][A
 29%|██▉       | 312/1071 [00:06<00:16, 45.02it/s][A
 30%|██▉       | 317/1071 [00:07<00:16, 44.88it/s][A
 30%|███       | 322/1071 [00:07<00:16, 44.86it/s][A
 31%|███       | 327/1071 [00:07<00:17, 42.44it/s][A
 31%|███       | 332/1071 [00:07<00:17, 43.38it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 44.00it/s][A
 32%|███▏      | 342/1071 [00:07<00:16, 44.30it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 44.61it/s][A
 33%|███▎      | 352/1071 [00:07<00:16, 44.79it/s][A
 33%|███▎      | 357/1071 [00:07<00:15, 44.78it/s][A
 34%|███▍      | 362/1071 [00:08<00:15, 44.62it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.38it/s][A
 35%|███▍      | 372/1071 [00:08<00:15, 44.40it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 44.70it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 44.87it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 45.03it/s][A
 37%|███▋      | 392/1071 [00:08<00:15, 45.18it/s][A
 37%|███▋      | 397/1071 [00:08<00:14, 45.12it/s][A
 38%|███▊      | 402/1071 [00:08<00:14, 45.10it/s][A
 38%|███▊      | 407/1071 [00:09<00:14, 44.90it/s][A
 38%|███▊      | 412/1071 [00:09<00:14, 44.65it/s][A
 39%|███▉      | 417/1071 [00:09<00:14, 44.64it/s][A
 39%|███▉      | 422/1071 [00:09<00:14, 44.78it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 44.94it/s][A
 40%|████      | 432/1071 [00:09<00:14, 45.10it/s][A
 41%|████      | 437/1071 [00:09<00:14, 45.23it/s][A
 41%|████▏     | 442/1071 [00:09<00:13, 45.11it/s][A
 42%|████▏     | 447/1071 [00:09<00:13, 45.14it/s][A
 42%|████▏     | 452/1071 [00:10<00:13, 44.83it/s][A
 43%|████▎     | 457/1071 [00:10<00:13, 44.59it/s][A
 43%|████▎     | 462/1071 [00:10<00:14, 42.52it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 43.40it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 43.99it/s][A
 45%|████▍     | 477/1071 [00:10<00:13, 44.49it/s][A
 45%|████▌     | 482/1071 [00:10<00:13, 44.64it/s][A
 45%|████▌     | 487/1071 [00:10<00:13, 44.73it/s][A
 46%|████▌     | 492/1071 [00:10<00:12, 44.77it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.66it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.36it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 44.45it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 44.68it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 44.97it/s][A
 49%|████▊     | 522/1071 [00:11<00:12, 45.06it/s][A
 49%|████▉     | 527/1071 [00:11<00:12, 45.15it/s][A
 50%|████▉     | 532/1071 [00:11<00:11, 45.27it/s][A
 50%|█████     | 537/1071 [00:11<00:11, 45.11it/s][A
 51%|█████     | 542/1071 [00:12<00:11, 44.81it/s][A
 51%|█████     | 547/1071 [00:12<00:11, 44.56it/s][A
 52%|█████▏    | 552/1071 [00:12<00:11, 44.62it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 44.76it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 44.97it/s][A
 53%|█████▎    | 567/1071 [00:12<00:11, 45.18it/s][A
 53%|█████▎    | 572/1071 [00:12<00:11, 45.29it/s][A
 54%|█████▍    | 577/1071 [00:12<00:10, 45.21it/s][A
 54%|█████▍    | 582/1071 [00:12<00:10, 45.13it/s][A
 55%|█████▍    | 587/1071 [00:13<00:10, 44.83it/s][A
 55%|█████▌    | 592/1071 [00:13<00:10, 44.68it/s][A
 56%|█████▌    | 597/1071 [00:13<00:12, 38.78it/s][A
 56%|█████▌    | 602/1071 [00:13<00:11, 40.54it/s][A
 57%|█████▋    | 607/1071 [00:13<00:11, 41.95it/s][A
 57%|█████▋    | 612/1071 [00:13<00:10, 42.84it/s][A
 58%|█████▊    | 617/1071 [00:13<00:10, 43.68it/s][A
 58%|█████▊    | 622/1071 [00:13<00:10, 44.17it/s][A
 59%|█████▊    | 627/1071 [00:14<00:09, 44.58it/s][A
 59%|█████▉    | 632/1071 [00:14<00:09, 44.71it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.41it/s][A
 60%|█████▉    | 642/1071 [00:14<00:09, 44.30it/s][A
 60%|██████    | 647/1071 [00:14<00:09, 44.27it/s][A
 61%|██████    | 652/1071 [00:14<00:09, 44.63it/s][A
 61%|██████▏   | 657/1071 [00:14<00:09, 44.82it/s][A
 62%|██████▏   | 662/1071 [00:14<00:09, 44.99it/s][A
 62%|██████▏   | 667/1071 [00:14<00:08, 45.15it/s][A
 63%|██████▎   | 672/1071 [00:15<00:08, 45.23it/s][A
 63%|██████▎   | 677/1071 [00:15<00:08, 45.18it/s][A
 64%|██████▎   | 682/1071 [00:15<00:08, 44.89it/s][A
 64%|██████▍   | 687/1071 [00:15<00:08, 44.58it/s][A
 65%|██████▍   | 692/1071 [00:15<00:08, 44.48it/s][A
 65%|██████▌   | 697/1071 [00:15<00:08, 44.71it/s][A
 66%|██████▌   | 702/1071 [00:15<00:08, 44.81it/s][A
 66%|██████▌   | 707/1071 [00:15<00:08, 45.06it/s][A
 66%|██████▋   | 712/1071 [00:15<00:07, 45.20it/s][A
 67%|██████▋   | 717/1071 [00:16<00:07, 45.30it/s][A
 67%|██████▋   | 722/1071 [00:16<00:07, 45.14it/s][A
 68%|██████▊   | 727/1071 [00:16<00:07, 44.92it/s][A
 68%|██████▊   | 732/1071 [00:16<00:10, 33.83it/s][A
 69%|██████▉   | 737/1071 [00:16<00:09, 36.64it/s][A
 69%|██████▉   | 742/1071 [00:16<00:08, 38.95it/s][A
 70%|██████▉   | 747/1071 [00:16<00:07, 40.71it/s][A
 70%|███████   | 752/1071 [00:16<00:07, 42.04it/s][A
 71%|███████   | 757/1071 [00:17<00:07, 43.01it/s][A
 71%|███████   | 762/1071 [00:17<00:07, 43.76it/s][A
 72%|███████▏  | 767/1071 [00:17<00:06, 44.13it/s][A
 72%|███████▏  | 772/1071 [00:17<00:06, 44.00it/s][A
 73%|███████▎  | 777/1071 [00:17<00:06, 43.97it/s][A
 73%|███████▎  | 782/1071 [00:17<00:06, 44.17it/s][A
 73%|███████▎  | 787/1071 [00:17<00:06, 44.43it/s][A
 74%|███████▍  | 792/1071 [00:17<00:06, 44.68it/s][A
 74%|███████▍  | 797/1071 [00:17<00:06, 44.95it/s][A
 75%|███████▍  | 802/1071 [00:18<00:05, 45.15it/s][A
 75%|███████▌  | 807/1071 [00:18<00:05, 45.23it/s][A
 76%|███████▌  | 812/1071 [00:18<00:05, 45.10it/s][A
 76%|███████▋  | 817/1071 [00:18<00:05, 44.76it/s][A
 77%|███████▋  | 822/1071 [00:18<00:05, 44.58it/s][A
 77%|███████▋  | 827/1071 [00:18<00:05, 44.56it/s][A
 78%|███████▊  | 832/1071 [00:18<00:05, 44.79it/s][A
 78%|███████▊  | 837/1071 [00:18<00:05, 44.93it/s][A
 79%|███████▊  | 842/1071 [00:18<00:05, 45.08it/s][A
 79%|███████▉  | 847/1071 [00:19<00:04, 45.25it/s][A
 80%|███████▉  | 852/1071 [00:19<00:04, 45.25it/s][A
 80%|████████  | 857/1071 [00:19<00:04, 45.08it/s][A
 80%|████████  | 862/1071 [00:19<00:05, 36.46it/s][A
 81%|████████  | 867/1071 [00:19<00:05, 38.78it/s][A
 81%|████████▏ | 872/1071 [00:19<00:04, 40.60it/s][A
 82%|████████▏ | 877/1071 [00:19<00:04, 41.95it/s][A
 82%|████████▏ | 882/1071 [00:19<00:04, 42.97it/s][A
 83%|████████▎ | 887/1071 [00:20<00:04, 43.67it/s][A
 83%|████████▎ | 892/1071 [00:20<00:04, 44.21it/s][A
 84%|████████▍ | 897/1071 [00:20<00:03, 44.45it/s][A
 84%|████████▍ | 902/1071 [00:20<00:03, 44.24it/s][A
 85%|████████▍ | 907/1071 [00:20<00:03, 44.06it/s][A
 85%|████████▌ | 912/1071 [00:20<00:03, 44.27it/s][A
 86%|████████▌ | 917/1071 [00:20<00:03, 44.48it/s][A
 86%|████████▌ | 922/1071 [00:20<00:03, 44.76it/s][A
 87%|████████▋ | 927/1071 [00:20<00:03, 44.89it/s][A
 87%|████████▋ | 932/1071 [00:21<00:03, 45.01it/s][A
 87%|████████▋ | 937/1071 [00:21<00:02, 44.99it/s][A
 88%|████████▊ | 942/1071 [00:21<00:02, 44.97it/s][A
 88%|████████▊ | 947/1071 [00:21<00:02, 44.77it/s][A
 89%|████████▉ | 952/1071 [00:21<00:02, 44.58it/s][A
 89%|████████▉ | 957/1071 [00:21<00:02, 44.50it/s][A
 90%|████████▉ | 962/1071 [00:21<00:02, 44.71it/s][A
 90%|█████████ | 967/1071 [00:21<00:02, 44.87it/s][A
 91%|█████████ | 972/1071 [00:21<00:02, 45.03it/s][A
 91%|█████████ | 977/1071 [00:22<00:02, 45.14it/s][A
 92%|█████████▏| 982/1071 [00:22<00:01, 45.23it/s][A
 92%|█████████▏| 987/1071 [00:22<00:01, 45.05it/s][A
 93%|█████████▎| 992/1071 [00:22<00:01, 44.66it/s][A
 93%|█████████▎| 997/1071 [00:22<00:01, 37.05it/s][A
 94%|█████████▎| 1002/1071 [00:22<00:01, 39.25it/s][A
 94%|█████████▍| 1007/1071 [00:22<00:01, 40.94it/s][A
 94%|█████████▍| 1012/1071 [00:22<00:01, 42.23it/s][A
 95%|█████████▍| 1017/1071 [00:22<00:01, 43.15it/s][A
 95%|█████████▌| 1022/1071 [00:23<00:01, 43.77it/s][A
 96%|█████████▌| 1027/1071 [00:23<00:00, 44.31it/s][A
 96%|█████████▋| 1032/1071 [00:23<00:00, 44.48it/s][A
 97%|█████████▋| 1037/1071 [00:23<00:00, 44.28it/s][A
 97%|█████████▋| 1042/1071 [00:23<00:00, 44.05it/s][A
 98%|█████████▊| 1047/1071 [00:23<00:00, 44.22it/s][A
 98%|█████████▊| 1052/1071 [00:23<00:00, 44.37it/s][A
 99%|█████████▊| 1057/1071 [00:23<00:00, 44.70it/s][A
 99%|█████████▉| 1062/1071 [00:24<00:00, 44.93it/s][A
100%|█████████▉| 1067/1071 [00:24<00:00, 45.09it/s][A
                                                   [A                                                 
100%|██████████| 1071/1071 [00:24<00:00, 45.09it/s][A 20%|██        | 117/585 [01:05<02:15,  3.46it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-29 15:13:14,918 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 15:13:15,141 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 15:13:19,208 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 15:13:19,406 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 15:13:19,525 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:21<1:34:33, 12.15s/it] 20%|██        | 119/585 [01:21<1:06:49,  8.60s/it] 21%|██        | 120/585 [01:21<47:21,  6.11s/it]   21%|██        | 121/585 [01:21<33:45,  4.36s/it] 21%|██        | 122/585 [01:22<24:15,  3.14s/it] 21%|██        | 123/585 [01:22<17:37,  2.29s/it] 21%|██        | 124/585 [01:22<12:58,  1.69s/it] 21%|██▏       | 125/585 [01:23<09:44,  1.27s/it] 22%|██▏       | 126/585 [01:23<07:28,  1.02it/s] 22%|██▏       | 127/585 [01:23<05:53,  1.29it/s] 22%|██▏       | 128/585 [01:24<04:47,  1.59it/s] 22%|██▏       | 129/585 [01:24<04:01,  1.89it/s] 22%|██▏       | 130/585 [01:24<03:33,  2.13it/s] 22%|██▏       | 131/585 [01:24<03:08,  2.40it/s] 23%|██▎       | 132/585 [01:25<02:51,  2.64it/s] 23%|██▎       | 133/585 [01:25<02:39,  2.84it/s] 23%|██▎       | 134/585 [01:25<02:31,  2.98it/s] 23%|██▎       | 135/585 [01:26<02:25,  3.10it/s] 23%|██▎       | 136/585 [01:26<02:20,  3.19it/s] 23%|██▎       | 137/585 [01:26<02:17,  3.25it/s] 24%|██▎       | 138/585 [01:26<02:15,  3.30it/s] 24%|██▍       | 139/585 [01:27<02:13,  3.33it/s] 24%|██▍       | 140/585 [01:27<02:12,  3.36it/s] 24%|██▍       | 141/585 [01:27<02:22,  3.13it/s] 24%|██▍       | 142/585 [01:28<02:18,  3.20it/s] 24%|██▍       | 143/585 [01:28<02:15,  3.27it/s] 25%|██▍       | 144/585 [01:28<02:13,  3.31it/s] 25%|██▍       | 145/585 [01:29<02:11,  3.34it/s] 25%|██▍       | 146/585 [01:29<02:10,  3.37it/s] 25%|██▌       | 147/585 [01:29<02:09,  3.38it/s] 25%|██▌       | 148/585 [01:29<02:08,  3.39it/s] 25%|██▌       | 149/585 [01:30<02:08,  3.40it/s] 26%|██▌       | 150/585 [01:30<02:07,  3.40it/s] 26%|██▌       | 151/585 [01:30<02:22,  3.05it/s] 26%|██▌       | 152/585 [01:31<02:17,  3.15it/s] 26%|██▌       | 153/585 [01:31<02:13,  3.24it/s] 26%|██▋       | 154/585 [01:31<02:10,  3.31it/s] 26%|██▋       | 155/585 [01:32<02:07,  3.36it/s] 27%|██▋       | 156/585 [01:32<02:06,  3.39it/s] 27%|██▋       | 157/585 [01:32<02:05,  3.42it/s] 27%|██▋       | 158/585 [01:33<02:04,  3.43it/s] 27%|██▋       | 159/585 [01:33<02:03,  3.44it/s] 27%|██▋       | 160/585 [01:33<02:02,  3.46it/s] 28%|██▊       | 161/585 [01:33<02:09,  3.28it/s] 28%|██▊       | 162/585 [01:34<03:21,  2.10it/s] 28%|██▊       | 163/585 [01:35<02:57,  2.38it/s] 28%|██▊       | 164/585 [01:35<02:40,  2.62it/s] 28%|██▊       | 165/585 [01:35<02:28,  2.83it/s] 28%|██▊       | 166/585 [01:35<02:19,  3.00it/s] 29%|██▊       | 167/585 [01:36<02:13,  3.13it/s] 29%|██▊       | 168/585 [01:36<02:09,  3.23it/s] 29%|██▉       | 169/585 [01:36<02:06,  3.30it/s] 29%|██▉       | 170/585 [01:37<02:14,  3.08it/s] 29%|██▉       | 171/585 [01:37<02:09,  3.19it/s] 29%|██▉       | 172/585 [01:37<02:06,  3.27it/s] 30%|██▉       | 173/585 [01:38<02:03,  3.33it/s] 30%|██▉       | 174/585 [01:38<02:01,  3.38it/s] 30%|██▉       | 175/585 [01:38<02:00,  3.41it/s] 30%|███       | 176/585 [01:38<01:59,  3.43it/s] 30%|███       | 177/585 [01:39<01:58,  3.44it/s] 30%|███       | 178/585 [01:39<01:57,  3.45it/s] 31%|███       | 179/585 [01:39<01:57,  3.46it/s] 31%|███       | 180/585 [01:40<01:56,  3.46it/s] 31%|███       | 181/585 [01:40<02:17,  2.94it/s] 31%|███       | 182/585 [01:40<02:10,  3.08it/s] 31%|███▏      | 183/585 [01:41<02:05,  3.19it/s] 31%|███▏      | 184/585 [01:41<02:02,  3.27it/s] 32%|███▏      | 185/585 [01:41<02:00,  3.33it/s] 32%|███▏      | 186/585 [01:41<01:58,  3.38it/s] 32%|███▏      | 187/585 [01:42<01:56,  3.41it/s] 32%|███▏      | 188/585 [01:42<01:55,  3.43it/s] 32%|███▏      | 189/585 [01:42<01:55,  3.44it/s] 32%|███▏      | 190/585 [01:43<01:54,  3.45it/s] 33%|███▎      | 191/585 [01:43<01:53,  3.46it/s] 33%|███▎      | 192/585 [01:43<01:53,  3.46it/s] 33%|███▎      | 193/585 [01:43<01:53,  3.47it/s] 33%|███▎      | 194/585 [01:44<02:00,  3.24it/s] 33%|███▎      | 195/585 [01:44<01:57,  3.31it/s] 34%|███▎      | 196/585 [01:44<01:55,  3.36it/s] 34%|███▎      | 197/585 [01:45<01:54,  3.40it/s] 34%|███▍      | 198/585 [01:45<01:53,  3.42it/s] 34%|███▍      | 199/585 [01:45<01:52,  3.43it/s] 34%|███▍      | 200/585 [01:46<01:51,  3.45it/s] 34%|███▍      | 201/585 [01:46<01:51,  3.46it/s] 35%|███▍      | 202/585 [01:46<01:50,  3.46it/s] 35%|███▍      | 203/585 [01:46<01:50,  3.47it/s] 35%|███▍      | 204/585 [01:47<01:49,  3.47it/s] 35%|███▌      | 205/585 [01:47<01:53,  3.36it/s] 35%|███▌      | 206/585 [01:47<01:51,  3.40it/s] 35%|███▌      | 207/585 [01:48<01:50,  3.42it/s] 36%|███▌      | 208/585 [01:48<01:49,  3.43it/s] 36%|███▌      | 209/585 [01:48<01:49,  3.45it/s] 36%|███▌      | 210/585 [01:48<01:48,  3.45it/s] 36%|███▌      | 211/585 [01:49<01:48,  3.46it/s] 36%|███▌      | 212/585 [01:49<01:47,  3.46it/s] 36%|███▋      | 213/585 [01:49<01:47,  3.46it/s] 37%|███▋      | 214/585 [01:50<01:46,  3.47it/s] 37%|███▋      | 215/585 [01:50<01:46,  3.48it/s] 37%|███▋      | 216/585 [01:50<01:51,  3.30it/s] 37%|███▋      | 217/585 [01:51<01:49,  3.35it/s] 37%|███▋      | 218/585 [01:51<01:48,  3.39it/s] 37%|███▋      | 219/585 [01:51<01:47,  3.41it/s] 38%|███▊      | 220/585 [01:51<01:46,  3.43it/s] 38%|███▊      | 221/585 [01:52<01:45,  3.44it/s] 38%|███▊      | 222/585 [01:52<01:45,  3.45it/s] 38%|███▊      | 223/585 [01:52<01:44,  3.46it/s] 38%|███▊      | 224/585 [01:53<01:44,  3.46it/s] 38%|███▊      | 225/585 [01:53<01:44,  3.46it/s] 39%|███▊      | 226/585 [01:53<01:43,  3.47it/s] 39%|███▉      | 227/585 [01:53<01:48,  3.31it/s] 39%|███▉      | 228/585 [01:54<01:46,  3.36it/s] 39%|███▉      | 229/585 [01:54<01:44,  3.40it/s] 39%|███▉      | 230/585 [01:54<01:43,  3.41it/s] 39%|███▉      | 231/585 [01:55<01:43,  3.43it/s] 40%|███▉      | 232/585 [01:55<01:42,  3.44it/s] 40%|███▉      | 233/585 [01:55<01:41,  3.45it/s] 40%|████      | 234/585 [01:55<01:41,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 15:14:05,022 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 15:14:05,022 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 15:14:05,022 >>   Batch size = 8
{'eval_loss': 0.9949867725372314, 'eval_runtime': 24.2427, 'eval_samples_per_second': 353.426, 'eval_steps_per_second': 44.178, 'epoch': 1.0}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:18, 56.12it/s][A
  1%|          | 12/1071 [00:00<00:21, 49.27it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 47.48it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 46.47it/s][A
  3%|▎         | 27/1071 [00:00<00:22, 45.81it/s][A
  3%|▎         | 32/1071 [00:00<00:22, 45.35it/s][A
  3%|▎         | 37/1071 [00:00<00:27, 37.64it/s][A
  4%|▍         | 42/1071 [00:00<00:25, 39.81it/s][A
  4%|▍         | 47/1071 [00:01<00:24, 41.40it/s][A
  5%|▍         | 52/1071 [00:01<00:23, 42.52it/s][A
  5%|▌         | 57/1071 [00:01<00:23, 43.40it/s][A
  6%|▌         | 62/1071 [00:01<00:22, 43.93it/s][A
  6%|▋         | 67/1071 [00:01<00:22, 44.43it/s][A
  7%|▋         | 72/1071 [00:01<00:22, 44.57it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 44.26it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 44.25it/s][A
  8%|▊         | 87/1071 [00:01<00:22, 44.59it/s][A
  9%|▊         | 92/1071 [00:02<00:21, 44.82it/s][A
  9%|▉         | 97/1071 [00:02<00:21, 44.90it/s][A
 10%|▉         | 102/1071 [00:02<00:21, 45.02it/s][A
 10%|▉         | 107/1071 [00:02<00:21, 45.15it/s][A
 10%|█         | 112/1071 [00:02<00:21, 45.24it/s][A
 11%|█         | 117/1071 [00:02<00:21, 44.92it/s][A
 11%|█▏        | 122/1071 [00:02<00:21, 44.68it/s][A
 12%|█▏        | 127/1071 [00:02<00:21, 44.50it/s][A
 12%|█▏        | 132/1071 [00:02<00:20, 44.77it/s][A
 13%|█▎        | 137/1071 [00:03<00:20, 44.87it/s][A
 13%|█▎        | 142/1071 [00:03<00:20, 45.06it/s][A
 14%|█▎        | 147/1071 [00:03<00:20, 45.01it/s][A
 14%|█▍        | 152/1071 [00:03<00:20, 45.13it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 45.13it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 44.91it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 44.74it/s][A
 16%|█▌        | 172/1071 [00:03<00:23, 37.66it/s][A
 17%|█▋        | 177/1071 [00:04<00:22, 39.74it/s][A
 17%|█▋        | 182/1071 [00:04<00:21, 41.31it/s][A
 17%|█▋        | 187/1071 [00:04<00:20, 42.57it/s][A
 18%|█▊        | 192/1071 [00:04<00:20, 43.48it/s][A
 18%|█▊        | 197/1071 [00:04<00:19, 44.02it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 44.54it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 44.60it/s][A
 20%|█▉        | 212/1071 [00:04<00:19, 44.16it/s][A
 20%|██        | 217/1071 [00:04<00:19, 44.12it/s][A
 21%|██        | 222/1071 [00:05<00:19, 44.07it/s][A
 21%|██        | 227/1071 [00:05<00:19, 44.32it/s][A
 22%|██▏       | 232/1071 [00:05<00:18, 44.61it/s][A
 22%|██▏       | 237/1071 [00:05<00:18, 44.87it/s][A
 23%|██▎       | 242/1071 [00:05<00:18, 45.05it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 45.16it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 45.24it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 44.98it/s][A
 24%|██▍       | 262/1071 [00:05<00:18, 44.69it/s][A
 25%|██▍       | 267/1071 [00:06<00:18, 44.56it/s][A
 25%|██▌       | 272/1071 [00:06<00:17, 44.61it/s][A
 26%|██▌       | 277/1071 [00:06<00:17, 44.82it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 44.95it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 45.11it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 45.26it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 45.15it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 44.97it/s][A
 29%|██▊       | 307/1071 [00:07<00:20, 36.62it/s][A
 29%|██▉       | 312/1071 [00:07<00:19, 38.90it/s][A
 30%|██▉       | 317/1071 [00:07<00:18, 40.59it/s][A
 30%|███       | 322/1071 [00:07<00:17, 41.97it/s][A
 31%|███       | 327/1071 [00:07<00:17, 42.90it/s][A
 31%|███       | 332/1071 [00:07<00:16, 43.62it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 44.03it/s][A
 32%|███▏      | 342/1071 [00:07<00:16, 44.39it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 44.18it/s][A
 33%|███▎      | 352/1071 [00:08<00:16, 44.23it/s][A
 33%|███▎      | 357/1071 [00:08<00:16, 44.57it/s][A
 34%|███▍      | 362/1071 [00:08<00:15, 44.69it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.82it/s][A
 35%|███▍      | 372/1071 [00:08<00:15, 45.01it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 45.07it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 45.18it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 44.96it/s][A
 37%|███▋      | 392/1071 [00:08<00:15, 44.69it/s][A
 37%|███▋      | 397/1071 [00:09<00:15, 44.62it/s][A
 38%|███▊      | 402/1071 [00:09<00:14, 44.79it/s][A
 38%|███▊      | 407/1071 [00:09<00:14, 44.84it/s][A
 38%|███▊      | 412/1071 [00:09<00:14, 45.03it/s][A
 39%|███▉      | 417/1071 [00:09<00:14, 45.06it/s][A
 39%|███▉      | 422/1071 [00:09<00:14, 45.16it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 45.15it/s][A
 40%|████      | 432/1071 [00:09<00:14, 44.89it/s][A
 41%|████      | 437/1071 [00:09<00:14, 44.75it/s][A
 41%|████▏     | 442/1071 [00:10<00:16, 38.88it/s][A
 42%|████▏     | 447/1071 [00:10<00:15, 40.65it/s][A
 42%|████▏     | 452/1071 [00:10<00:14, 41.97it/s][A
 43%|████▎     | 457/1071 [00:10<00:14, 42.98it/s][A
 43%|████▎     | 462/1071 [00:10<00:13, 43.76it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 44.27it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 44.58it/s][A
 45%|████▍     | 477/1071 [00:10<00:13, 44.76it/s][A
 45%|████▌     | 482/1071 [00:10<00:13, 44.44it/s][A
 45%|████▌     | 487/1071 [00:11<00:13, 44.17it/s][A
 46%|████▌     | 492/1071 [00:11<00:13, 44.34it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.60it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.85it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 45.03it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 45.15it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 45.07it/s][A
 49%|████▊     | 522/1071 [00:11<00:12, 44.81it/s][A
 49%|████▉     | 527/1071 [00:11<00:12, 44.57it/s][A
 50%|████▉     | 532/1071 [00:12<00:12, 44.37it/s][A
 50%|█████     | 537/1071 [00:12<00:12, 44.43it/s][A
 51%|█████     | 542/1071 [00:12<00:11, 44.60it/s][A
 51%|█████     | 547/1071 [00:12<00:11, 44.85it/s][A
 52%|█████▏    | 552/1071 [00:12<00:11, 44.98it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 45.14it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 45.29it/s][A
 53%|█████▎    | 567/1071 [00:12<00:11, 45.10it/s][A
 53%|█████▎    | 572/1071 [00:13<00:11, 44.82it/s][A
 54%|█████▍    | 577/1071 [00:13<00:12, 38.68it/s][A
 54%|█████▍    | 582/1071 [00:13<00:12, 40.47it/s][A
 55%|█████▍    | 587/1071 [00:13<00:11, 41.86it/s][A
 55%|█████▌    | 592/1071 [00:13<00:11, 42.89it/s][A
 56%|█████▌    | 597/1071 [00:13<00:10, 43.43it/s][A
 56%|█████▌    | 602/1071 [00:13<00:10, 44.29it/s][A
 57%|█████▋    | 607/1071 [00:13<00:10, 44.60it/s][A
 57%|█████▋    | 612/1071 [00:13<00:10, 44.79it/s][A
 58%|█████▊    | 617/1071 [00:14<00:10, 44.37it/s][A
 58%|█████▊    | 622/1071 [00:14<00:10, 44.20it/s][A
 59%|█████▊    | 627/1071 [00:14<00:10, 44.31it/s][A
 59%|█████▉    | 632/1071 [00:14<00:09, 44.53it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.73it/s][A
 60%|█████▉    | 642/1071 [00:14<00:09, 44.93it/s][A
 60%|██████    | 647/1071 [00:14<00:09, 45.20it/s][A
 61%|██████    | 652/1071 [00:14<00:09, 45.22it/s][A
 61%|██████▏   | 657/1071 [00:14<00:09, 45.07it/s][A
 62%|██████▏   | 662/1071 [00:15<00:09, 44.73it/s][A
 62%|██████▏   | 667/1071 [00:15<00:09, 44.58it/s][A
 63%|██████▎   | 672/1071 [00:15<00:08, 44.50it/s][A
 63%|██████▎   | 677/1071 [00:15<00:08, 44.70it/s][A
 64%|██████▎   | 682/1071 [00:15<00:08, 44.77it/s][A
 64%|██████▍   | 687/1071 [00:15<00:08, 44.87it/s][A
 65%|██████▍   | 692/1071 [00:15<00:09, 40.18it/s][A
 65%|██████▌   | 697/1071 [00:15<00:08, 41.71it/s][A
 66%|██████▌   | 702/1071 [00:15<00:08, 42.73it/s][A
 66%|██████▌   | 707/1071 [00:16<00:08, 43.58it/s][A
 66%|██████▋   | 712/1071 [00:16<00:08, 44.04it/s][A
 67%|██████▋   | 717/1071 [00:16<00:07, 44.28it/s][A
 67%|██████▋   | 722/1071 [00:16<00:07, 44.50it/s][A
 68%|██████▊   | 727/1071 [00:16<00:07, 44.34it/s][A
 68%|██████▊   | 732/1071 [00:16<00:07, 44.38it/s][A
 69%|██████▉   | 737/1071 [00:16<00:07, 44.43it/s][A
 69%|██████▉   | 742/1071 [00:16<00:07, 44.65it/s][A
 70%|██████▉   | 747/1071 [00:16<00:07, 44.81it/s][A
 70%|███████   | 752/1071 [00:17<00:07, 45.06it/s][A
 71%|███████   | 757/1071 [00:17<00:06, 45.11it/s][A
 71%|███████   | 762/1071 [00:17<00:06, 45.13it/s][A
 72%|███████▏  | 767/1071 [00:17<00:06, 45.12it/s][A
 72%|███████▏  | 772/1071 [00:17<00:06, 44.87it/s][A
 73%|███████▎  | 777/1071 [00:17<00:06, 44.62it/s][A
 73%|███████▎  | 782/1071 [00:17<00:06, 44.55it/s][A
 73%|███████▎  | 787/1071 [00:17<00:06, 44.65it/s][A
 74%|███████▍  | 792/1071 [00:17<00:06, 44.79it/s][A
 74%|███████▍  | 797/1071 [00:18<00:06, 44.99it/s][A
 75%|███████▍  | 802/1071 [00:18<00:05, 45.18it/s][A
 75%|███████▌  | 807/1071 [00:18<00:05, 45.15it/s][A
 76%|███████▌  | 812/1071 [00:18<00:05, 45.08it/s][A
 76%|███████▋  | 817/1071 [00:18<00:05, 44.87it/s][A
 77%|███████▋  | 822/1071 [00:18<00:05, 44.67it/s][A
 77%|███████▋  | 827/1071 [00:18<00:05, 41.00it/s][A
 78%|███████▊  | 832/1071 [00:18<00:05, 42.26it/s][A
 78%|███████▊  | 837/1071 [00:19<00:05, 43.13it/s][A
 79%|███████▊  | 842/1071 [00:19<00:05, 43.88it/s][A
 79%|███████▉  | 847/1071 [00:19<00:05, 44.33it/s][A
 80%|███████▉  | 852/1071 [00:19<00:04, 44.68it/s][A
 80%|████████  | 857/1071 [00:19<00:04, 44.57it/s][A
 80%|████████  | 862/1071 [00:19<00:04, 44.76it/s][A
 81%|████████  | 867/1071 [00:19<00:04, 44.32it/s][A
 81%|████████▏ | 872/1071 [00:19<00:04, 44.31it/s][A
 82%|████████▏ | 877/1071 [00:19<00:04, 44.51it/s][A
 82%|████████▏ | 882/1071 [00:20<00:04, 44.78it/s][A
 83%|████████▎ | 887/1071 [00:20<00:04, 44.90it/s][A
 83%|████████▎ | 892/1071 [00:20<00:03, 45.17it/s][A
 84%|████████▍ | 897/1071 [00:20<00:03, 45.12it/s][A
 84%|████████▍ | 902/1071 [00:20<00:03, 45.16it/s][A
 85%|████████▍ | 907/1071 [00:20<00:03, 44.83it/s][A
 85%|████████▌ | 912/1071 [00:20<00:03, 44.61it/s][A
 86%|████████▌ | 917/1071 [00:20<00:03, 44.55it/s][A
 86%|████████▌ | 922/1071 [00:20<00:03, 44.60it/s][A
 87%|████████▋ | 927/1071 [00:21<00:03, 44.89it/s][A
 87%|████████▋ | 932/1071 [00:21<00:03, 45.06it/s][A
 87%|████████▋ | 937/1071 [00:21<00:02, 45.23it/s][A
 88%|████████▊ | 942/1071 [00:21<00:02, 45.21it/s][A
 88%|████████▊ | 947/1071 [00:21<00:02, 45.20it/s][A
 89%|████████▉ | 952/1071 [00:21<00:02, 44.87it/s][A
 89%|████████▉ | 957/1071 [00:21<00:02, 44.70it/s][A
 90%|████████▉ | 962/1071 [00:21<00:02, 41.70it/s][A
 90%|█████████ | 967/1071 [00:21<00:02, 42.71it/s][A
 91%|█████████ | 972/1071 [00:22<00:02, 43.54it/s][A
 91%|█████████ | 977/1071 [00:22<00:02, 44.10it/s][A
 92%|█████████▏| 982/1071 [00:22<00:01, 44.52it/s][A
 92%|█████████▏| 987/1071 [00:22<00:01, 44.53it/s][A
 93%|█████████▎| 992/1071 [00:22<00:01, 44.85it/s][A
 93%|█████████▎| 997/1071 [00:22<00:01, 44.70it/s][A
 94%|█████████▎| 1002/1071 [00:22<00:01, 44.15it/s][A
 94%|█████████▍| 1007/1071 [00:22<00:01, 44.24it/s][A
 94%|█████████▍| 1012/1071 [00:22<00:01, 44.59it/s][A
 95%|█████████▍| 1017/1071 [00:23<00:01, 44.76it/s][A
 95%|█████████▌| 1022/1071 [00:23<00:01, 45.00it/s][A
 96%|█████████▌| 1027/1071 [00:23<00:00, 45.20it/s][A
 96%|█████████▋| 1032/1071 [00:23<00:00, 45.16it/s][A
 97%|█████████▋| 1037/1071 [00:23<00:00, 45.01it/s][A
 97%|█████████▋| 1042/1071 [00:23<00:00, 44.85it/s][A
 98%|█████████▊| 1047/1071 [00:23<00:00, 44.62it/s][A
 98%|█████████▊| 1052/1071 [00:23<00:00, 44.52it/s][A
 99%|█████████▊| 1057/1071 [00:23<00:00, 44.71it/s][A
 99%|█████████▉| 1062/1071 [00:24<00:00, 44.83it/s][A
100%|█████████▉| 1067/1071 [00:24<00:00, 45.04it/s][A
                                                   [A                                                 
100%|██████████| 1071/1071 [00:24<00:00, 45.04it/s][A 40%|████      | 234/585 [02:20<01:41,  3.46it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-29 15:14:29,644 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 15:14:29,903 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 15:14:34,786 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 15:14:35,094 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 15:14:35,282 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:37<1:14:14, 12.73s/it] 40%|████      | 236/585 [02:38<52:24,  9.01s/it]   41%|████      | 237/585 [02:38<37:04,  6.39s/it] 41%|████      | 238/585 [02:38<26:23,  4.56s/it] 41%|████      | 239/585 [02:38<18:55,  3.28s/it] 41%|████      | 240/585 [02:39<13:43,  2.39s/it] 41%|████      | 241/585 [02:39<10:04,  1.76s/it] 41%|████▏     | 242/585 [02:39<07:31,  1.32s/it] 42%|████▏     | 243/585 [02:40<05:45,  1.01s/it] 42%|████▏     | 244/585 [02:40<04:30,  1.26it/s] 42%|████▏     | 245/585 [02:40<03:38,  1.55it/s] 42%|████▏     | 246/585 [02:40<03:02,  1.85it/s] 42%|████▏     | 247/585 [02:41<02:42,  2.09it/s] 42%|████▏     | 248/585 [02:41<02:22,  2.36it/s] 43%|████▎     | 249/585 [02:41<02:08,  2.61it/s] 43%|████▎     | 250/585 [02:42<01:59,  2.80it/s] 43%|████▎     | 251/585 [02:42<01:52,  2.97it/s] 43%|████▎     | 252/585 [02:42<01:48,  3.08it/s] 43%|████▎     | 253/585 [02:43<01:44,  3.17it/s] 43%|████▎     | 254/585 [02:43<01:42,  3.24it/s] 44%|████▎     | 255/585 [02:43<01:40,  3.29it/s] 44%|████▍     | 256/585 [02:43<01:38,  3.33it/s] 44%|████▍     | 257/585 [02:44<01:37,  3.36it/s] 44%|████▍     | 258/585 [02:44<01:36,  3.37it/s] 44%|████▍     | 259/585 [02:44<01:36,  3.38it/s] 44%|████▍     | 260/585 [02:45<01:35,  3.40it/s] 45%|████▍     | 261/585 [02:45<01:35,  3.40it/s] 45%|████▍     | 262/585 [02:45<01:37,  3.31it/s] 45%|████▍     | 263/585 [02:46<01:36,  3.33it/s] 45%|████▌     | 264/585 [02:46<01:35,  3.36it/s] 45%|████▌     | 265/585 [02:46<01:34,  3.37it/s] 45%|████▌     | 266/585 [02:46<01:34,  3.39it/s] 46%|████▌     | 267/585 [02:47<01:33,  3.40it/s] 46%|████▌     | 268/585 [02:47<01:33,  3.40it/s] 46%|████▌     | 269/585 [02:47<01:32,  3.40it/s] 46%|████▌     | 270/585 [02:48<01:32,  3.40it/s] 46%|████▋     | 271/585 [02:48<01:32,  3.41it/s] 46%|████▋     | 272/585 [02:48<01:32,  3.40it/s] 47%|████▋     | 273/585 [02:48<01:34,  3.30it/s] 47%|████▋     | 274/585 [02:49<01:33,  3.33it/s] 47%|████▋     | 275/585 [02:49<01:32,  3.36it/s] 47%|████▋     | 276/585 [02:49<01:31,  3.38it/s] 47%|████▋     | 277/585 [02:50<01:30,  3.39it/s] 48%|████▊     | 278/585 [02:50<01:30,  3.40it/s] 48%|████▊     | 279/585 [02:50<01:29,  3.41it/s] 48%|████▊     | 280/585 [02:51<01:29,  3.42it/s] 48%|████▊     | 281/585 [02:51<01:29,  3.41it/s] 48%|████▊     | 282/585 [02:51<01:28,  3.41it/s] 48%|████▊     | 283/585 [02:51<01:28,  3.40it/s] 49%|████▊     | 284/585 [02:52<01:34,  3.19it/s] 49%|████▊     | 285/585 [02:52<01:32,  3.25it/s] 49%|████▉     | 286/585 [02:52<01:30,  3.30it/s] 49%|████▉     | 287/585 [02:53<01:29,  3.34it/s] 49%|████▉     | 288/585 [02:53<01:28,  3.36it/s] 49%|████▉     | 289/585 [02:53<01:27,  3.37it/s] 50%|████▉     | 290/585 [02:54<01:27,  3.38it/s] 50%|████▉     | 291/585 [02:54<01:26,  3.39it/s] 50%|████▉     | 292/585 [02:54<01:26,  3.40it/s] 50%|█████     | 293/585 [02:54<01:25,  3.40it/s] 50%|█████     | 294/585 [02:55<01:25,  3.40it/s] 50%|█████     | 295/585 [02:55<01:31,  3.15it/s] 51%|█████     | 296/585 [02:55<01:29,  3.23it/s] 51%|█████     | 297/585 [02:56<01:27,  3.28it/s] 51%|█████     | 298/585 [02:56<01:26,  3.32it/s] 51%|█████     | 299/585 [02:56<01:25,  3.35it/s] 51%|█████▏    | 300/585 [02:57<01:24,  3.37it/s] 51%|█████▏    | 301/585 [02:57<01:23,  3.38it/s] 52%|█████▏    | 302/585 [02:57<01:23,  3.40it/s] 52%|█████▏    | 303/585 [02:57<01:22,  3.41it/s] 52%|█████▏    | 304/585 [02:58<01:22,  3.42it/s] 52%|█████▏    | 305/585 [02:58<01:30,  3.10it/s] 52%|█████▏    | 306/585 [02:58<01:26,  3.21it/s] 52%|█████▏    | 307/585 [02:59<01:24,  3.29it/s] 53%|█████▎    | 308/585 [02:59<01:22,  3.35it/s] 53%|█████▎    | 309/585 [02:59<01:21,  3.39it/s] 53%|█████▎    | 310/585 [03:00<01:20,  3.42it/s] 53%|█████▎    | 311/585 [03:00<01:19,  3.44it/s] 53%|█████▎    | 312/585 [03:00<01:19,  3.46it/s] 54%|█████▎    | 313/585 [03:00<01:18,  3.46it/s] 54%|█████▎    | 314/585 [03:01<01:18,  3.47it/s] 54%|█████▍    | 315/585 [03:01<01:17,  3.48it/s] 54%|█████▍    | 316/585 [03:01<01:23,  3.20it/s] 54%|█████▍    | 317/585 [03:02<01:21,  3.29it/s] 54%|█████▍    | 318/585 [03:02<01:19,  3.34it/s] 55%|█████▍    | 319/585 [03:02<01:18,  3.38it/s] 55%|█████▍    | 320/585 [03:02<01:17,  3.41it/s] 55%|█████▍    | 321/585 [03:03<01:16,  3.43it/s] 55%|█████▌    | 322/585 [03:03<01:16,  3.44it/s] 55%|█████▌    | 323/585 [03:03<01:15,  3.46it/s] 55%|█████▌    | 324/585 [03:04<01:15,  3.46it/s] 56%|█████▌    | 325/585 [03:04<01:14,  3.47it/s] 56%|█████▌    | 326/585 [03:04<01:14,  3.48it/s] 56%|█████▌    | 327/585 [03:05<01:21,  3.17it/s] 56%|█████▌    | 328/585 [03:05<01:18,  3.25it/s] 56%|█████▌    | 329/585 [03:05<01:17,  3.32it/s] 56%|█████▋    | 330/585 [03:05<01:15,  3.36it/s] 57%|█████▋    | 331/585 [03:06<01:14,  3.39it/s] 57%|█████▋    | 332/585 [03:06<01:14,  3.42it/s] 57%|█████▋    | 333/585 [03:06<01:13,  3.43it/s] 57%|█████▋    | 334/585 [03:07<01:12,  3.45it/s] 57%|█████▋    | 335/585 [03:07<01:12,  3.46it/s] 57%|█████▋    | 336/585 [03:07<01:11,  3.47it/s] 58%|█████▊    | 337/585 [03:07<01:11,  3.48it/s] 58%|█████▊    | 338/585 [03:08<01:16,  3.22it/s] 58%|█████▊    | 339/585 [03:08<01:14,  3.30it/s] 58%|█████▊    | 340/585 [03:08<01:13,  3.35it/s] 58%|█████▊    | 341/585 [03:09<01:12,  3.38it/s] 58%|█████▊    | 342/585 [03:09<01:11,  3.41it/s] 59%|█████▊    | 343/585 [03:09<01:10,  3.42it/s] 59%|█████▉    | 344/585 [03:10<01:10,  3.43it/s] 59%|█████▉    | 345/585 [03:10<01:09,  3.45it/s] 59%|█████▉    | 346/585 [03:10<01:09,  3.45it/s] 59%|█████▉    | 347/585 [03:10<01:08,  3.46it/s] 59%|█████▉    | 348/585 [03:11<01:08,  3.46it/s] 60%|█████▉    | 349/585 [03:11<01:11,  3.29it/s] 60%|█████▉    | 350/585 [03:11<01:10,  3.34it/s] 60%|██████    | 351/585 [03:12<01:09,  3.38it/s][INFO|trainer.py:2140] 2023-08-29 15:15:21,188 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 15:15:21,188 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 15:15:21,188 >>   Batch size = 8
{'eval_loss': 1.0201091766357422, 'eval_runtime': 24.2837, 'eval_samples_per_second': 352.829, 'eval_steps_per_second': 44.104, 'epoch': 2.0}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:19, 55.85it/s][A
  1%|          | 12/1071 [00:00<00:21, 49.23it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 47.28it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 46.28it/s][A
  3%|▎         | 27/1071 [00:00<00:22, 45.65it/s][A
  3%|▎         | 32/1071 [00:00<00:22, 45.28it/s][A
  3%|▎         | 37/1071 [00:00<00:22, 45.04it/s][A
  4%|▍         | 42/1071 [00:00<00:22, 44.80it/s][A
  4%|▍         | 47/1071 [00:01<00:22, 45.01it/s][A
  5%|▍         | 52/1071 [00:01<00:22, 45.14it/s][A
  5%|▌         | 57/1071 [00:01<00:22, 45.24it/s][A
  6%|▌         | 62/1071 [00:01<00:22, 45.24it/s][A
  6%|▋         | 67/1071 [00:01<00:22, 45.13it/s][A
  7%|▋         | 72/1071 [00:01<00:22, 44.92it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 44.89it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 44.75it/s][A
  8%|▊         | 87/1071 [00:01<00:22, 44.72it/s][A
  9%|▊         | 92/1071 [00:02<00:21, 44.86it/s][A
  9%|▉         | 97/1071 [00:02<00:21, 45.04it/s][A
 10%|▉         | 102/1071 [00:02<00:21, 45.08it/s][A
 10%|▉         | 107/1071 [00:02<00:21, 45.16it/s][A
 10%|█         | 112/1071 [00:02<00:21, 45.08it/s][A
 11%|█         | 117/1071 [00:02<00:21, 44.92it/s][A
 11%|█▏        | 122/1071 [00:02<00:21, 44.90it/s][A
 12%|█▏        | 127/1071 [00:02<00:21, 44.80it/s][A
 12%|█▏        | 132/1071 [00:02<00:20, 44.73it/s][A
 13%|█▎        | 137/1071 [00:03<00:20, 44.84it/s][A
 13%|█▎        | 142/1071 [00:03<00:20, 45.01it/s][A
 14%|█▎        | 147/1071 [00:03<00:20, 45.11it/s][A
 14%|█▍        | 152/1071 [00:03<00:20, 45.11it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 45.05it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 44.84it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 44.70it/s][A
 16%|█▌        | 172/1071 [00:03<00:20, 44.81it/s][A
 17%|█▋        | 177/1071 [00:03<00:19, 44.70it/s][A
 17%|█▋        | 182/1071 [00:04<00:21, 41.26it/s][A
 17%|█▋        | 187/1071 [00:04<00:20, 42.55it/s][A
 18%|█▊        | 192/1071 [00:04<00:20, 43.34it/s][A
 18%|█▊        | 197/1071 [00:04<00:19, 43.96it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 44.42it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 44.66it/s][A
 20%|█▉        | 212/1071 [00:04<00:19, 44.61it/s][A
 20%|██        | 217/1071 [00:04<00:19, 44.64it/s][A
 21%|██        | 222/1071 [00:04<00:19, 44.39it/s][A
 21%|██        | 227/1071 [00:05<00:18, 44.48it/s][A
 22%|██▏       | 232/1071 [00:05<00:18, 44.61it/s][A
 22%|██▏       | 237/1071 [00:05<00:18, 44.73it/s][A
 23%|██▎       | 242/1071 [00:05<00:18, 44.98it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 45.15it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 45.16it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 45.11it/s][A
 24%|██▍       | 262/1071 [00:05<00:18, 44.83it/s][A
 25%|██▍       | 267/1071 [00:05<00:17, 44.69it/s][A
 25%|██▌       | 272/1071 [00:06<00:17, 44.64it/s][A
 26%|██▌       | 277/1071 [00:06<00:17, 44.75it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 44.90it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 45.04it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 45.17it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 45.17it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 45.06it/s][A
 29%|██▊       | 307/1071 [00:06<00:17, 44.90it/s][A
 29%|██▉       | 312/1071 [00:06<00:16, 44.76it/s][A
 30%|██▉       | 317/1071 [00:07<00:17, 42.03it/s][A
 30%|███       | 322/1071 [00:07<00:17, 43.07it/s][A
 31%|███       | 327/1071 [00:07<00:16, 43.82it/s][A
 31%|███       | 332/1071 [00:07<00:16, 44.10it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 44.50it/s][A
 32%|███▏      | 342/1071 [00:07<00:16, 44.69it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 44.51it/s][A
 33%|███▎      | 352/1071 [00:07<00:16, 44.44it/s][A
 33%|███▎      | 357/1071 [00:07<00:16, 44.20it/s][A
 34%|███▍      | 362/1071 [00:08<00:15, 44.46it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.65it/s][A
 35%|███▍      | 372/1071 [00:08<00:15, 44.90it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 45.04it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 45.22it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 45.19it/s][A
 37%|███▋      | 392/1071 [00:08<00:15, 45.02it/s][A
 37%|███▋      | 397/1071 [00:08<00:15, 44.77it/s][A
 38%|███▊      | 402/1071 [00:08<00:15, 44.56it/s][A
 38%|███▊      | 407/1071 [00:09<00:14, 44.63it/s][A
 38%|███▊      | 412/1071 [00:09<00:14, 44.77it/s][A
 39%|███▉      | 417/1071 [00:09<00:14, 44.72it/s][A
 39%|███▉      | 422/1071 [00:09<00:14, 45.06it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 45.23it/s][A
 40%|████      | 432/1071 [00:09<00:14, 45.23it/s][A
 41%|████      | 437/1071 [00:09<00:14, 45.08it/s][A
 41%|████▏     | 442/1071 [00:09<00:14, 44.81it/s][A
 42%|████▏     | 447/1071 [00:10<00:13, 44.65it/s][A
 42%|████▏     | 452/1071 [00:10<00:15, 41.19it/s][A
 43%|████▎     | 457/1071 [00:10<00:14, 42.40it/s][A
 43%|████▎     | 462/1071 [00:10<00:14, 43.27it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 43.94it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 44.43it/s][A
 45%|████▍     | 477/1071 [00:10<00:13, 44.66it/s][A
 45%|████▌     | 482/1071 [00:10<00:13, 44.76it/s][A
 45%|████▌     | 487/1071 [00:10<00:13, 44.65it/s][A
 46%|████▌     | 492/1071 [00:11<00:13, 44.27it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.30it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.50it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 44.76it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 45.02it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 45.12it/s][A
 49%|████▊     | 522/1071 [00:11<00:12, 45.10it/s][A
 49%|████▉     | 527/1071 [00:11<00:12, 45.03it/s][A
 50%|████▉     | 532/1071 [00:11<00:12, 44.73it/s][A
 50%|█████     | 537/1071 [00:12<00:12, 44.43it/s][A
 51%|█████     | 542/1071 [00:12<00:11, 44.45it/s][A
 51%|█████     | 547/1071 [00:12<00:11, 44.60it/s][A
 52%|█████▏    | 552/1071 [00:12<00:11, 44.83it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 45.00it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 45.14it/s][A
 53%|█████▎    | 567/1071 [00:12<00:11, 45.20it/s][A
 53%|█████▎    | 572/1071 [00:12<00:11, 45.09it/s][A
 54%|█████▍    | 577/1071 [00:12<00:11, 44.85it/s][A
 54%|█████▍    | 582/1071 [00:13<00:10, 44.64it/s][A
 55%|█████▍    | 587/1071 [00:13<00:11, 42.03it/s][A
 55%|█████▌    | 592/1071 [00:13<00:11, 43.06it/s][A
 56%|█████▌    | 597/1071 [00:13<00:10, 43.79it/s][A
 56%|█████▌    | 602/1071 [00:13<00:10, 44.26it/s][A
 57%|█████▋    | 607/1071 [00:13<00:10, 44.65it/s][A
 57%|█████▋    | 612/1071 [00:13<00:10, 44.88it/s][A
 58%|█████▊    | 617/1071 [00:13<00:10, 44.79it/s][A
 58%|█████▊    | 622/1071 [00:13<00:10, 44.62it/s][A
 59%|█████▊    | 627/1071 [00:14<00:10, 44.29it/s][A
 59%|█████▉    | 632/1071 [00:14<00:09, 44.32it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.57it/s][A
 60%|█████▉    | 642/1071 [00:14<00:09, 44.72it/s][A
 60%|██████    | 647/1071 [00:14<00:09, 44.91it/s][A
 61%|██████    | 652/1071 [00:14<00:09, 45.05it/s][A
 61%|██████▏   | 657/1071 [00:14<00:09, 45.18it/s][A
 62%|██████▏   | 662/1071 [00:14<00:09, 45.27it/s][A
 62%|██████▏   | 667/1071 [00:14<00:08, 44.96it/s][A
 63%|██████▎   | 672/1071 [00:15<00:08, 44.67it/s][A
 63%|██████▎   | 677/1071 [00:15<00:08, 44.59it/s][A
 64%|██████▎   | 682/1071 [00:15<00:08, 44.73it/s][A
 64%|██████▍   | 687/1071 [00:15<00:08, 44.84it/s][A
 65%|██████▍   | 692/1071 [00:15<00:08, 44.90it/s][A
 65%|██████▌   | 697/1071 [00:15<00:08, 44.97it/s][A
 66%|██████▌   | 702/1071 [00:15<00:08, 45.05it/s][A
 66%|██████▌   | 707/1071 [00:15<00:08, 45.04it/s][A
 66%|██████▋   | 712/1071 [00:15<00:08, 44.74it/s][A
 67%|██████▋   | 717/1071 [00:16<00:07, 44.51it/s][A
 67%|██████▋   | 722/1071 [00:16<00:08, 41.82it/s][A
 68%|██████▊   | 727/1071 [00:16<00:08, 42.86it/s][A
 68%|██████▊   | 732/1071 [00:16<00:07, 43.65it/s][A
 69%|██████▉   | 737/1071 [00:16<00:07, 44.19it/s][A
 69%|██████▉   | 742/1071 [00:16<00:07, 44.55it/s][A
 70%|██████▉   | 747/1071 [00:16<00:07, 44.75it/s][A
 70%|███████   | 752/1071 [00:16<00:07, 44.70it/s][A
 71%|███████   | 757/1071 [00:16<00:07, 44.61it/s][A
 71%|███████   | 762/1071 [00:17<00:06, 44.30it/s][A
 72%|███████▏  | 767/1071 [00:17<00:06, 44.21it/s][A
 72%|███████▏  | 772/1071 [00:17<00:06, 44.34it/s][A
 73%|███████▎  | 777/1071 [00:17<00:06, 44.55it/s][A
 73%|███████▎  | 782/1071 [00:17<00:06, 44.58it/s][A
 73%|███████▎  | 787/1071 [00:17<00:06, 44.73it/s][A
 74%|███████▍  | 792/1071 [00:17<00:06, 44.73it/s][A
 74%|███████▍  | 797/1071 [00:17<00:06, 44.88it/s][A
 75%|███████▍  | 802/1071 [00:17<00:06, 44.64it/s][A
 75%|███████▌  | 807/1071 [00:18<00:05, 44.54it/s][A
 76%|███████▌  | 812/1071 [00:18<00:05, 44.50it/s][A
 76%|███████▋  | 817/1071 [00:18<00:05, 44.60it/s][A
 77%|███████▋  | 822/1071 [00:18<00:05, 44.80it/s][A
 77%|███████▋  | 827/1071 [00:18<00:05, 44.94it/s][A
 78%|███████▊  | 832/1071 [00:18<00:05, 45.13it/s][A
 78%|███████▊  | 837/1071 [00:18<00:05, 45.06it/s][A
 79%|███████▊  | 842/1071 [00:18<00:05, 45.13it/s][A
 79%|███████▉  | 847/1071 [00:18<00:05, 44.70it/s][A
 80%|███████▉  | 852/1071 [00:19<00:04, 44.73it/s][A
 80%|████████  | 857/1071 [00:19<00:05, 42.21it/s][A
 80%|████████  | 862/1071 [00:19<00:04, 43.16it/s][A
 81%|████████  | 867/1071 [00:19<00:04, 43.79it/s][A
 81%|████████▏ | 872/1071 [00:19<00:04, 44.13it/s][A
 82%|████████▏ | 877/1071 [00:19<00:04, 44.55it/s][A
 82%|████████▏ | 882/1071 [00:19<00:04, 44.66it/s][A
 83%|████████▎ | 887/1071 [00:19<00:04, 44.72it/s][A
 83%|████████▎ | 892/1071 [00:19<00:04, 44.59it/s][A
 84%|████████▍ | 897/1071 [00:20<00:03, 44.30it/s][A
 84%|████████▍ | 902/1071 [00:20<00:03, 44.51it/s][A
 85%|████████▍ | 907/1071 [00:20<00:03, 44.66it/s][A
 85%|████████▌ | 912/1071 [00:20<00:03, 44.89it/s][A
 86%|████████▌ | 917/1071 [00:20<00:03, 45.07it/s][A
 86%|████████▌ | 922/1071 [00:20<00:03, 45.20it/s][A
 87%|████████▋ | 927/1071 [00:20<00:03, 45.09it/s][A
 87%|████████▋ | 932/1071 [00:20<00:03, 44.99it/s][A
 87%|████████▋ | 937/1071 [00:20<00:02, 44.74it/s][A
 88%|████████▊ | 942/1071 [00:21<00:02, 44.30it/s][A
 88%|████████▊ | 947/1071 [00:21<00:02, 44.36it/s][A
 89%|████████▉ | 952/1071 [00:21<00:02, 44.65it/s][A
 89%|████████▉ | 957/1071 [00:21<00:02, 44.75it/s][A
 90%|████████▉ | 962/1071 [00:21<00:02, 44.89it/s][A
 90%|█████████ | 967/1071 [00:21<00:02, 45.06it/s][A
 91%|█████████ | 972/1071 [00:21<00:02, 45.16it/s][A
 91%|█████████ | 977/1071 [00:21<00:02, 44.97it/s][A
 92%|█████████▏| 982/1071 [00:22<00:01, 44.71it/s][A
 92%|█████████▏| 987/1071 [00:22<00:01, 44.52it/s][A
 93%|█████████▎| 992/1071 [00:22<00:01, 43.00it/s][A
 93%|█████████▎| 997/1071 [00:22<00:01, 43.73it/s][A
 94%|█████████▎| 1002/1071 [00:22<00:01, 44.19it/s][A
 94%|█████████▍| 1007/1071 [00:22<00:01, 44.51it/s][A
 94%|█████████▍| 1012/1071 [00:22<00:01, 44.82it/s][A
 95%|█████████▍| 1017/1071 [00:22<00:01, 44.77it/s][A
 95%|█████████▌| 1022/1071 [00:22<00:01, 44.66it/s][A
 96%|█████████▌| 1027/1071 [00:23<00:00, 44.43it/s][A
 96%|█████████▋| 1032/1071 [00:23<00:00, 44.26it/s][A
 97%|█████████▋| 1037/1071 [00:23<00:00, 44.41it/s][A
 97%|█████████▋| 1042/1071 [00:23<00:00, 44.61it/s][A
 98%|█████████▊| 1047/1071 [00:23<00:00, 44.81it/s][A
 98%|█████████▊| 1052/1071 [00:23<00:00, 44.97it/s][A
 99%|█████████▊| 1057/1071 [00:23<00:00, 45.17it/s][A
 99%|█████████▉| 1062/1071 [00:23<00:00, 45.18it/s][A
100%|█████████▉| 1067/1071 [00:23<00:00, 44.97it/s][A
                                                   [A                                                 
100%|██████████| 1071/1071 [00:24<00:00, 44.97it/s][A 60%|██████    | 351/585 [03:36<01:09,  3.38it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-29 15:15:45,693 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 15:15:45,979 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 15:15:49,549 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 15:15:49,730 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 15:15:49,812 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:50<45:29, 11.71s/it] 60%|██████    | 353/585 [03:50<32:05,  8.30s/it] 61%|██████    | 354/585 [03:51<22:42,  5.90s/it] 61%|██████    | 355/585 [03:51<16:09,  4.22s/it] 61%|██████    | 356/585 [03:51<11:35,  3.04s/it] 61%|██████    | 357/585 [03:51<08:25,  2.22s/it] 61%|██████    | 358/585 [03:52<06:11,  1.64s/it] 61%|██████▏   | 359/585 [03:52<04:39,  1.24s/it] 62%|██████▏   | 360/585 [03:52<03:34,  1.05it/s] 62%|██████▏   | 361/585 [03:53<02:49,  1.32it/s] 62%|██████▏   | 362/585 [03:53<02:17,  1.63it/s] 62%|██████▏   | 363/585 [03:53<01:54,  1.94it/s] 62%|██████▏   | 364/585 [03:54<01:42,  2.16it/s] 62%|██████▏   | 365/585 [03:54<01:30,  2.44it/s] 63%|██████▎   | 366/585 [03:54<01:21,  2.68it/s] 63%|██████▎   | 367/585 [03:54<01:15,  2.88it/s] 63%|██████▎   | 368/585 [03:55<01:11,  3.04it/s] 63%|██████▎   | 369/585 [03:55<01:08,  3.16it/s] 63%|██████▎   | 370/585 [03:55<01:06,  3.25it/s] 63%|██████▎   | 371/585 [03:56<01:04,  3.32it/s] 64%|██████▎   | 372/585 [03:56<01:03,  3.37it/s] 64%|██████▍   | 373/585 [03:56<01:02,  3.41it/s] 64%|██████▍   | 374/585 [03:56<01:01,  3.43it/s] 64%|██████▍   | 375/585 [03:57<01:04,  3.28it/s] 64%|██████▍   | 376/585 [03:57<01:02,  3.33it/s] 64%|██████▍   | 377/585 [03:57<01:01,  3.38it/s] 65%|██████▍   | 378/585 [03:58<01:00,  3.40it/s] 65%|██████▍   | 379/585 [03:58<01:00,  3.41it/s] 65%|██████▍   | 380/585 [03:58<00:59,  3.44it/s] 65%|██████▌   | 381/585 [03:59<00:58,  3.46it/s] 65%|██████▌   | 382/585 [03:59<00:58,  3.47it/s] 65%|██████▌   | 383/585 [03:59<00:58,  3.48it/s] 66%|██████▌   | 384/585 [03:59<00:57,  3.48it/s] 66%|██████▌   | 385/585 [04:00<00:57,  3.49it/s] 66%|██████▌   | 386/585 [04:00<01:00,  3.30it/s] 66%|██████▌   | 387/585 [04:00<00:59,  3.35it/s] 66%|██████▋   | 388/585 [04:01<00:58,  3.39it/s] 66%|██████▋   | 389/585 [04:01<00:57,  3.42it/s] 67%|██████▋   | 390/585 [04:01<00:56,  3.44it/s] 67%|██████▋   | 391/585 [04:01<00:56,  3.45it/s] 67%|██████▋   | 392/585 [04:02<00:55,  3.47it/s] 67%|██████▋   | 393/585 [04:02<00:55,  3.48it/s] 67%|██████▋   | 394/585 [04:02<00:54,  3.48it/s] 68%|██████▊   | 395/585 [04:03<00:54,  3.48it/s] 68%|██████▊   | 396/585 [04:03<00:54,  3.49it/s] 68%|██████▊   | 397/585 [04:03<01:00,  3.13it/s] 68%|██████▊   | 398/585 [04:04<00:57,  3.23it/s] 68%|██████▊   | 399/585 [04:04<00:56,  3.30it/s] 68%|██████▊   | 400/585 [04:04<00:55,  3.36it/s] 69%|██████▊   | 401/585 [04:04<00:54,  3.39it/s] 69%|██████▊   | 402/585 [04:05<00:53,  3.42it/s] 69%|██████▉   | 403/585 [04:05<00:52,  3.44it/s] 69%|██████▉   | 404/585 [04:05<00:52,  3.46it/s] 69%|██████▉   | 405/585 [04:06<00:52,  3.46it/s] 69%|██████▉   | 406/585 [04:06<00:51,  3.47it/s] 70%|██████▉   | 407/585 [04:06<00:51,  3.48it/s] 70%|██████▉   | 408/585 [04:06<00:54,  3.28it/s] 70%|██████▉   | 409/585 [04:07<00:52,  3.34it/s] 70%|███████   | 410/585 [04:07<00:51,  3.38it/s] 70%|███████   | 411/585 [04:07<00:50,  3.42it/s] 70%|███████   | 412/585 [04:08<00:50,  3.44it/s] 71%|███████   | 413/585 [04:08<00:49,  3.45it/s] 71%|███████   | 414/585 [04:08<00:49,  3.46it/s] 71%|███████   | 415/585 [04:08<00:49,  3.46it/s] 71%|███████   | 416/585 [04:09<00:48,  3.46it/s] 71%|███████▏  | 417/585 [04:09<00:48,  3.46it/s] 71%|███████▏  | 418/585 [04:09<00:48,  3.46it/s] 72%|███████▏  | 419/585 [04:10<00:49,  3.32it/s] 72%|███████▏  | 420/585 [04:10<00:49,  3.36it/s] 72%|███████▏  | 421/585 [04:10<00:48,  3.40it/s] 72%|███████▏  | 422/585 [04:11<00:47,  3.41it/s] 72%|███████▏  | 423/585 [04:11<00:47,  3.43it/s] 72%|███████▏  | 424/585 [04:11<00:46,  3.44it/s] 73%|███████▎  | 425/585 [04:11<00:46,  3.45it/s] 73%|███████▎  | 426/585 [04:12<00:46,  3.45it/s] 73%|███████▎  | 427/585 [04:12<00:45,  3.46it/s] 73%|███████▎  | 428/585 [04:12<00:45,  3.46it/s] 73%|███████▎  | 429/585 [04:13<00:44,  3.47it/s] 74%|███████▎  | 430/585 [04:13<00:45,  3.38it/s] 74%|███████▎  | 431/585 [04:13<00:45,  3.40it/s] 74%|███████▍  | 432/585 [04:13<00:44,  3.43it/s] 74%|███████▍  | 433/585 [04:14<00:44,  3.43it/s] 74%|███████▍  | 434/585 [04:14<00:43,  3.45it/s] 74%|███████▍  | 435/585 [04:14<00:44,  3.38it/s] 75%|███████▍  | 436/585 [04:15<00:43,  3.41it/s] 75%|███████▍  | 437/585 [04:15<00:43,  3.42it/s] 75%|███████▍  | 438/585 [04:15<00:42,  3.44it/s] 75%|███████▌  | 439/585 [04:15<00:42,  3.44it/s] 75%|███████▌  | 440/585 [04:16<00:42,  3.45it/s] 75%|███████▌  | 441/585 [04:16<00:41,  3.46it/s] 76%|███████▌  | 442/585 [04:16<00:41,  3.46it/s] 76%|███████▌  | 443/585 [04:17<00:40,  3.47it/s] 76%|███████▌  | 444/585 [04:17<00:40,  3.47it/s] 76%|███████▌  | 445/585 [04:17<00:40,  3.47it/s] 76%|███████▌  | 446/585 [04:18<00:41,  3.34it/s] 76%|███████▋  | 447/585 [04:18<00:40,  3.38it/s] 77%|███████▋  | 448/585 [04:18<00:40,  3.42it/s] 77%|███████▋  | 449/585 [04:18<00:39,  3.44it/s] 77%|███████▋  | 450/585 [04:19<00:39,  3.45it/s] 77%|███████▋  | 451/585 [04:19<00:38,  3.47it/s] 77%|███████▋  | 452/585 [04:19<00:38,  3.47it/s] 77%|███████▋  | 453/585 [04:20<00:37,  3.48it/s] 78%|███████▊  | 454/585 [04:20<00:37,  3.48it/s] 78%|███████▊  | 455/585 [04:20<00:37,  3.48it/s] 78%|███████▊  | 456/585 [04:20<00:37,  3.47it/s] 78%|███████▊  | 457/585 [04:21<00:38,  3.28it/s] 78%|███████▊  | 458/585 [04:21<00:38,  3.34it/s] 78%|███████▊  | 459/585 [04:21<00:37,  3.38it/s] 79%|███████▊  | 460/585 [04:22<00:36,  3.41it/s] 79%|███████▉  | 461/585 [04:22<00:36,  3.43it/s] 79%|███████▉  | 462/585 [04:22<00:35,  3.44it/s] 79%|███████▉  | 463/585 [04:22<00:35,  3.44it/s] 79%|███████▉  | 464/585 [04:23<00:35,  3.45it/s] 79%|███████▉  | 465/585 [04:23<00:34,  3.46it/s] 80%|███████▉  | 466/585 [04:23<00:34,  3.46it/s] 80%|███████▉  | 467/585 [04:24<00:34,  3.47it/s] 80%|████████  | 468/585 [04:24<00:35,  3.31it/s][INFO|trainer.py:2140] 2023-08-29 15:16:33,521 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 15:16:33,521 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 15:16:33,521 >>   Batch size = 8
{'eval_loss': 1.0406110286712646, 'eval_runtime': 24.0469, 'eval_samples_per_second': 356.304, 'eval_steps_per_second': 44.538, 'epoch': 3.0}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:18, 56.30it/s][A
  1%|          | 12/1071 [00:00<00:21, 49.31it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 47.47it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 46.64it/s][A
  3%|▎         | 27/1071 [00:00<00:22, 45.97it/s][A
  3%|▎         | 32/1071 [00:00<00:22, 45.35it/s][A
  3%|▎         | 37/1071 [00:00<00:22, 45.00it/s][A
  4%|▍         | 42/1071 [00:00<00:23, 44.70it/s][A
  4%|▍         | 47/1071 [00:01<00:22, 44.79it/s][A
  5%|▍         | 52/1071 [00:01<00:22, 45.06it/s][A
  5%|▌         | 57/1071 [00:01<00:22, 45.20it/s][A
  6%|▌         | 62/1071 [00:01<00:22, 45.24it/s][A
  6%|▋         | 67/1071 [00:01<00:22, 45.19it/s][A
  7%|▋         | 72/1071 [00:01<00:22, 44.88it/s][A
  7%|▋         | 77/1071 [00:01<00:22, 44.82it/s][A
  8%|▊         | 82/1071 [00:01<00:22, 44.48it/s][A
  8%|▊         | 87/1071 [00:01<00:22, 44.43it/s][A
  9%|▊         | 92/1071 [00:02<00:21, 44.61it/s][A
  9%|▉         | 97/1071 [00:02<00:21, 44.82it/s][A
 10%|▉         | 102/1071 [00:02<00:21, 45.05it/s][A
 10%|▉         | 107/1071 [00:02<00:21, 45.14it/s][A
 10%|█         | 112/1071 [00:02<00:21, 45.09it/s][A
 11%|█         | 117/1071 [00:02<00:21, 44.99it/s][A
 11%|█▏        | 122/1071 [00:02<00:21, 44.74it/s][A
 12%|█▏        | 127/1071 [00:02<00:22, 42.62it/s][A
 12%|█▏        | 132/1071 [00:02<00:21, 43.32it/s][A
 13%|█▎        | 137/1071 [00:03<00:21, 43.81it/s][A
 13%|█▎        | 142/1071 [00:03<00:21, 44.23it/s][A
 14%|█▎        | 147/1071 [00:03<00:20, 44.53it/s][A
 14%|█▍        | 152/1071 [00:03<00:20, 44.82it/s][A
 15%|█▍        | 157/1071 [00:03<00:20, 44.95it/s][A
 15%|█▌        | 162/1071 [00:03<00:20, 44.82it/s][A
 16%|█▌        | 167/1071 [00:03<00:20, 44.55it/s][A
 16%|█▌        | 172/1071 [00:03<00:20, 44.54it/s][A
 17%|█▋        | 177/1071 [00:03<00:20, 44.61it/s][A
 17%|█▋        | 182/1071 [00:04<00:19, 44.77it/s][A
 17%|█▋        | 187/1071 [00:04<00:19, 44.92it/s][A
 18%|█▊        | 192/1071 [00:04<00:19, 44.98it/s][A
 18%|█▊        | 197/1071 [00:04<00:19, 45.06it/s][A
 19%|█▉        | 202/1071 [00:04<00:19, 45.12it/s][A
 19%|█▉        | 207/1071 [00:04<00:19, 44.80it/s][A
 20%|█▉        | 212/1071 [00:04<00:19, 44.67it/s][A
 20%|██        | 217/1071 [00:04<00:19, 44.55it/s][A
 21%|██        | 222/1071 [00:04<00:19, 44.46it/s][A
 21%|██        | 227/1071 [00:05<00:18, 44.49it/s][A
 22%|██▏       | 232/1071 [00:05<00:18, 44.95it/s][A
 22%|██▏       | 237/1071 [00:05<00:18, 45.08it/s][A
 23%|██▎       | 242/1071 [00:05<00:18, 45.17it/s][A
 23%|██▎       | 247/1071 [00:05<00:18, 45.10it/s][A
 24%|██▎       | 252/1071 [00:05<00:18, 45.03it/s][A
 24%|██▍       | 257/1071 [00:05<00:18, 44.73it/s][A
 24%|██▍       | 262/1071 [00:05<00:19, 41.73it/s][A
 25%|██▍       | 267/1071 [00:05<00:18, 42.74it/s][A
 25%|██▌       | 272/1071 [00:06<00:18, 43.57it/s][A
 26%|██▌       | 277/1071 [00:06<00:18, 43.97it/s][A
 26%|██▋       | 282/1071 [00:06<00:17, 44.41it/s][A
 27%|██▋       | 287/1071 [00:06<00:17, 44.72it/s][A
 27%|██▋       | 292/1071 [00:06<00:17, 44.70it/s][A
 28%|██▊       | 297/1071 [00:06<00:17, 44.66it/s][A
 28%|██▊       | 302/1071 [00:06<00:17, 44.47it/s][A
 29%|██▊       | 307/1071 [00:06<00:17, 44.41it/s][A
 29%|██▉       | 312/1071 [00:06<00:17, 44.57it/s][A
 30%|██▉       | 317/1071 [00:07<00:16, 44.76it/s][A
 30%|███       | 322/1071 [00:07<00:16, 44.90it/s][A
 31%|███       | 327/1071 [00:07<00:16, 45.08it/s][A
 31%|███       | 332/1071 [00:07<00:16, 45.12it/s][A
 31%|███▏      | 337/1071 [00:07<00:16, 45.12it/s][A
 32%|███▏      | 342/1071 [00:07<00:16, 44.83it/s][A
 32%|███▏      | 347/1071 [00:07<00:16, 44.56it/s][A
 33%|███▎      | 352/1071 [00:07<00:16, 44.56it/s][A
 33%|███▎      | 357/1071 [00:07<00:15, 44.71it/s][A
 34%|███▍      | 362/1071 [00:08<00:15, 44.75it/s][A
 34%|███▍      | 367/1071 [00:08<00:15, 44.93it/s][A
 35%|███▍      | 372/1071 [00:08<00:15, 45.04it/s][A
 35%|███▌      | 377/1071 [00:08<00:15, 45.03it/s][A
 36%|███▌      | 382/1071 [00:08<00:15, 45.04it/s][A
 36%|███▌      | 387/1071 [00:08<00:15, 44.76it/s][A
 37%|███▋      | 392/1071 [00:08<00:15, 44.67it/s][A
 37%|███▋      | 397/1071 [00:08<00:16, 42.07it/s][A
 38%|███▊      | 402/1071 [00:08<00:15, 43.01it/s][A
 38%|███▊      | 407/1071 [00:09<00:15, 43.66it/s][A
 38%|███▊      | 412/1071 [00:09<00:14, 44.16it/s][A
 39%|███▉      | 417/1071 [00:09<00:14, 44.50it/s][A
 39%|███▉      | 422/1071 [00:09<00:14, 44.66it/s][A
 40%|███▉      | 427/1071 [00:09<00:14, 44.50it/s][A
 40%|████      | 432/1071 [00:09<00:14, 44.64it/s][A
 41%|████      | 437/1071 [00:09<00:14, 44.32it/s][A
 41%|████▏     | 442/1071 [00:09<00:14, 44.43it/s][A
 42%|████▏     | 447/1071 [00:10<00:14, 44.56it/s][A
 42%|████▏     | 452/1071 [00:10<00:13, 44.84it/s][A
 43%|████▎     | 457/1071 [00:10<00:13, 44.99it/s][A
 43%|████▎     | 462/1071 [00:10<00:13, 45.18it/s][A
 44%|████▎     | 467/1071 [00:10<00:13, 45.15it/s][A
 44%|████▍     | 472/1071 [00:10<00:13, 44.92it/s][A
 45%|████▍     | 477/1071 [00:10<00:13, 44.78it/s][A
 45%|████▌     | 482/1071 [00:10<00:13, 44.58it/s][A
 45%|████▌     | 487/1071 [00:10<00:13, 44.53it/s][A
 46%|████▌     | 492/1071 [00:11<00:12, 44.63it/s][A
 46%|████▋     | 497/1071 [00:11<00:12, 44.78it/s][A
 47%|████▋     | 502/1071 [00:11<00:12, 44.95it/s][A
 47%|████▋     | 507/1071 [00:11<00:12, 44.94it/s][A
 48%|████▊     | 512/1071 [00:11<00:12, 45.04it/s][A
 48%|████▊     | 517/1071 [00:11<00:12, 45.05it/s][A
 49%|████▊     | 522/1071 [00:11<00:12, 44.97it/s][A
 49%|████▉     | 527/1071 [00:11<00:12, 44.71it/s][A
 50%|████▉     | 532/1071 [00:11<00:13, 41.04it/s][A
 50%|█████     | 537/1071 [00:12<00:13, 39.29it/s][A
 51%|█████     | 542/1071 [00:12<00:12, 41.49it/s][A
 51%|█████     | 547/1071 [00:12<00:12, 42.63it/s][A
 52%|█████▏    | 552/1071 [00:12<00:11, 43.43it/s][A
 52%|█████▏    | 557/1071 [00:12<00:11, 43.92it/s][A
 52%|█████▏    | 562/1071 [00:12<00:11, 44.37it/s][A
 53%|█████▎    | 567/1071 [00:12<00:11, 44.51it/s][A
 53%|█████▎    | 572/1071 [00:12<00:11, 44.11it/s][A
 54%|█████▍    | 577/1071 [00:12<00:11, 44.28it/s][A
 54%|█████▍    | 582/1071 [00:13<00:11, 44.13it/s][A
 55%|█████▍    | 587/1071 [00:13<00:10, 44.42it/s][A
 55%|█████▌    | 592/1071 [00:13<00:10, 44.70it/s][A
 56%|█████▌    | 597/1071 [00:13<00:10, 44.84it/s][A
 56%|█████▌    | 602/1071 [00:13<00:10, 45.10it/s][A
 57%|█████▋    | 607/1071 [00:13<00:10, 45.18it/s][A
 57%|█████▋    | 612/1071 [00:13<00:10, 44.90it/s][A
 58%|█████▊    | 617/1071 [00:13<00:10, 44.68it/s][A
 58%|█████▊    | 622/1071 [00:13<00:10, 44.56it/s][A
 59%|█████▊    | 627/1071 [00:14<00:09, 44.55it/s][A
 59%|█████▉    | 632/1071 [00:14<00:09, 44.64it/s][A
 59%|█████▉    | 637/1071 [00:14<00:09, 44.71it/s][A
 60%|█████▉    | 642/1071 [00:14<00:09, 44.89it/s][A
 60%|██████    | 647/1071 [00:14<00:09, 45.07it/s][A
 61%|██████    | 652/1071 [00:14<00:09, 45.21it/s][A
 61%|██████▏   | 657/1071 [00:14<00:09, 44.99it/s][A
 62%|██████▏   | 662/1071 [00:14<00:09, 44.86it/s][A
 62%|██████▏   | 667/1071 [00:14<00:09, 42.62it/s][A
 63%|██████▎   | 672/1071 [00:15<00:09, 43.39it/s][A
 63%|██████▎   | 677/1071 [00:15<00:09, 40.59it/s][A
 64%|██████▍   | 683/1071 [00:15<00:08, 43.32it/s][A
 64%|██████▍   | 688/1071 [00:15<00:08, 43.88it/s][A
 65%|██████▍   | 693/1071 [00:15<00:08, 44.32it/s][A
 65%|██████▌   | 698/1071 [00:15<00:08, 44.68it/s][A
 66%|██████▌   | 703/1071 [00:15<00:08, 44.58it/s][A
 66%|██████▌   | 708/1071 [00:15<00:08, 44.32it/s][A
 67%|██████▋   | 713/1071 [00:16<00:08, 44.40it/s][A
 67%|██████▋   | 718/1071 [00:16<00:07, 44.60it/s][A
 68%|██████▊   | 723/1071 [00:16<00:07, 44.56it/s][A
 68%|██████▊   | 728/1071 [00:16<00:07, 44.67it/s][A
 68%|██████▊   | 733/1071 [00:16<00:07, 44.86it/s][A
 69%|██████▉   | 738/1071 [00:16<00:07, 44.96it/s][A
 69%|██████▉   | 743/1071 [00:16<00:07, 45.05it/s][A
 70%|██████▉   | 748/1071 [00:16<00:07, 44.82it/s][A
 70%|███████   | 753/1071 [00:17<00:14, 22.24it/s][A
 71%|███████   | 758/1071 [00:17<00:11, 26.29it/s][A
 71%|███████   | 763/1071 [00:17<00:10, 30.03it/s][A
 72%|███████▏  | 768/1071 [00:17<00:09, 33.40it/s][A
 72%|███████▏  | 773/1071 [00:17<00:08, 36.10it/s][A
 73%|███████▎  | 778/1071 [00:17<00:07, 38.47it/s][A
 73%|███████▎  | 783/1071 [00:17<00:07, 40.21it/s][A
 74%|███████▎  | 788/1071 [00:18<00:07, 38.75it/s][A
 74%|███████▍  | 793/1071 [00:18<00:06, 40.39it/s][A
 75%|███████▍  | 798/1071 [00:18<00:06, 41.71it/s][A
 75%|███████▍  | 803/1071 [00:18<00:06, 42.73it/s][A
 75%|███████▌  | 808/1071 [00:18<00:06, 43.53it/s][A
 76%|███████▌  | 813/1071 [00:18<00:05, 43.97it/s][A
 76%|███████▋  | 818/1071 [00:18<00:05, 44.23it/s][A
 77%|███████▋  | 823/1071 [00:18<00:05, 44.42it/s][A
 77%|███████▋  | 828/1071 [00:18<00:05, 44.14it/s][A
 78%|███████▊  | 833/1071 [00:19<00:05, 44.15it/s][A
 78%|███████▊  | 838/1071 [00:19<00:05, 44.16it/s][A
 79%|███████▊  | 843/1071 [00:19<00:05, 44.43it/s][A
 79%|███████▉  | 848/1071 [00:19<00:05, 44.53it/s][A
 80%|███████▉  | 853/1071 [00:19<00:04, 44.85it/s][A
 80%|████████  | 858/1071 [00:19<00:04, 45.03it/s][A
 81%|████████  | 863/1071 [00:19<00:04, 45.12it/s][A
 81%|████████  | 868/1071 [00:19<00:04, 44.98it/s][A
 82%|████████▏ | 873/1071 [00:19<00:04, 44.80it/s][A
 82%|████████▏ | 878/1071 [00:20<00:04, 44.63it/s][A
 82%|████████▏ | 883/1071 [00:20<00:04, 44.51it/s][A
 83%|████████▎ | 888/1071 [00:20<00:04, 44.63it/s][A
 83%|████████▎ | 893/1071 [00:20<00:03, 44.74it/s][A
 84%|████████▍ | 898/1071 [00:20<00:03, 44.85it/s][A
 84%|████████▍ | 903/1071 [00:20<00:03, 45.02it/s][A
 85%|████████▍ | 908/1071 [00:20<00:04, 39.71it/s][A
 85%|████████▌ | 913/1071 [00:20<00:03, 41.29it/s][A
 86%|████████▌ | 918/1071 [00:21<00:03, 42.31it/s][A
 86%|████████▌ | 923/1071 [00:21<00:03, 43.11it/s][A
 87%|████████▋ | 928/1071 [00:21<00:03, 43.70it/s][A
 87%|████████▋ | 933/1071 [00:21<00:03, 44.05it/s][A
 88%|████████▊ | 938/1071 [00:21<00:02, 44.41it/s][A
 88%|████████▊ | 943/1071 [00:21<00:02, 44.57it/s][A
 89%|████████▊ | 948/1071 [00:21<00:02, 44.30it/s][A
 89%|████████▉ | 953/1071 [00:21<00:02, 44.31it/s][A
 89%|████████▉ | 958/1071 [00:21<00:02, 44.53it/s][A
 90%|████████▉ | 963/1071 [00:22<00:02, 44.62it/s][A
 90%|█████████ | 968/1071 [00:22<00:02, 44.84it/s][A
 91%|█████████ | 973/1071 [00:22<00:02, 44.89it/s][A
 91%|█████████▏| 978/1071 [00:22<00:02, 44.86it/s][A
 92%|█████████▏| 983/1071 [00:22<00:01, 44.93it/s][A
 92%|█████████▏| 988/1071 [00:22<00:01, 44.68it/s][A
 93%|█████████▎| 993/1071 [00:22<00:01, 44.42it/s][A
 93%|█████████▎| 998/1071 [00:22<00:01, 44.38it/s][A
 94%|█████████▎| 1003/1071 [00:22<00:01, 44.51it/s][A
 94%|█████████▍| 1008/1071 [00:23<00:01, 44.72it/s][A
 95%|█████████▍| 1013/1071 [00:23<00:01, 44.85it/s][A
 95%|█████████▌| 1018/1071 [00:23<00:01, 44.93it/s][A
 96%|█████████▌| 1023/1071 [00:23<00:01, 45.02it/s][A
 96%|█████████▌| 1028/1071 [00:23<00:00, 44.75it/s][A
 96%|█████████▋| 1033/1071 [00:23<00:00, 44.76it/s][A
 97%|█████████▋| 1038/1071 [00:23<00:00, 44.64it/s][A
 97%|█████████▋| 1043/1071 [00:23<00:00, 43.13it/s][A
 98%|█████████▊| 1048/1071 [00:23<00:00, 43.82it/s][A
 98%|█████████▊| 1053/1071 [00:24<00:00, 44.18it/s][A
 99%|█████████▉| 1058/1071 [00:24<00:00, 44.42it/s][A
 99%|█████████▉| 1063/1071 [00:24<00:00, 44.73it/s][A
100%|█████████▉| 1068/1071 [00:24<00:00, 44.73it/s][A
                                                   [A                                                 
100%|██████████| 1071/1071 [00:24<00:00, 44.73it/s][A 80%|████████  | 468/585 [04:49<00:35,  3.31it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-29 15:16:58,362 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 15:16:58,667 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 15:17:03,719 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 15:17:03,967 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 15:17:04,132 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [05:06<24:35, 12.72s/it] 80%|████████  | 470/585 [05:06<17:16,  9.01s/it] 81%|████████  | 471/585 [05:06<12:09,  6.40s/it] 81%|████████  | 472/585 [05:07<08:35,  4.56s/it] 81%|████████  | 473/585 [05:07<06:07,  3.28s/it] 81%|████████  | 474/585 [05:07<04:24,  2.39s/it] 81%|████████  | 475/585 [05:07<03:13,  1.76s/it] 81%|████████▏ | 476/585 [05:08<02:23,  1.32s/it] 82%|████████▏ | 477/585 [05:08<01:48,  1.01s/it] 82%|████████▏ | 478/585 [05:08<01:24,  1.26it/s] 82%|████████▏ | 479/585 [05:09<01:08,  1.55it/s] 82%|████████▏ | 480/585 [05:09<00:56,  1.86it/s] 82%|████████▏ | 481/585 [05:09<00:50,  2.04it/s] 82%|████████▏ | 482/585 [05:10<00:44,  2.33it/s] 83%|████████▎ | 483/585 [05:10<00:39,  2.58it/s] 83%|████████▎ | 484/585 [05:10<00:36,  2.78it/s] 83%|████████▎ | 485/585 [05:10<00:33,  2.95it/s] 83%|████████▎ | 486/585 [05:11<00:32,  3.08it/s] 83%|████████▎ | 487/585 [05:11<00:30,  3.19it/s] 83%|████████▎ | 488/585 [05:11<00:29,  3.27it/s] 84%|████████▎ | 489/585 [05:12<00:28,  3.33it/s] 84%|████████▍ | 490/585 [05:12<00:28,  3.38it/s] 84%|████████▍ | 491/585 [05:12<00:27,  3.41it/s] 84%|████████▍ | 492/585 [05:13<00:27,  3.33it/s] 84%|████████▍ | 493/585 [05:13<00:27,  3.37it/s] 84%|████████▍ | 494/585 [05:13<00:26,  3.40it/s] 85%|████████▍ | 495/585 [05:13<00:26,  3.43it/s] 85%|████████▍ | 496/585 [05:14<00:25,  3.44it/s] 85%|████████▍ | 497/585 [05:14<00:25,  3.45it/s] 85%|████████▌ | 498/585 [05:14<00:25,  3.47it/s] 85%|████████▌ | 499/585 [05:15<00:24,  3.48it/s] 85%|████████▌ | 500/585 [05:15<00:24,  3.48it/s]                                                  85%|████████▌ | 500/585 [05:15<00:24,  3.48it/s] 86%|████████▌ | 501/585 [05:15<00:24,  3.48it/s] 86%|████████▌ | 502/585 [05:15<00:23,  3.48it/s] 86%|████████▌ | 503/585 [05:16<00:26,  3.05it/s] 86%|████████▌ | 504/585 [05:16<00:25,  3.17it/s] 86%|████████▋ | 505/585 [05:16<00:24,  3.26it/s] 86%|████████▋ | 506/585 [05:17<00:23,  3.33it/s] 87%|████████▋ | 507/585 [05:17<00:23,  3.37it/s] 87%|████████▋ | 508/585 [05:17<00:22,  3.40it/s] 87%|████████▋ | 509/585 [05:18<00:22,  3.42it/s] 87%|████████▋ | 510/585 [05:18<00:21,  3.44it/s] 87%|████████▋ | 511/585 [05:18<00:21,  3.46it/s] 88%|████████▊ | 512/585 [05:18<00:21,  3.47it/s] 88%|████████▊ | 513/585 [05:19<00:20,  3.47it/s] 88%|████████▊ | 514/585 [05:19<00:21,  3.34it/s] 88%|████████▊ | 515/585 [05:19<00:20,  3.37it/s] 88%|████████▊ | 516/585 [05:20<00:20,  3.40it/s] 88%|████████▊ | 517/585 [05:20<00:19,  3.42it/s] 89%|████████▊ | 518/585 [05:20<00:19,  3.44it/s] 89%|████████▊ | 519/585 [05:20<00:19,  3.45it/s] 89%|████████▉ | 520/585 [05:21<00:18,  3.45it/s] 89%|████████▉ | 521/585 [05:21<00:18,  3.47it/s] 89%|████████▉ | 522/585 [05:21<00:18,  3.48it/s] 89%|████████▉ | 523/585 [05:22<00:17,  3.48it/s] 90%|████████▉ | 524/585 [05:22<00:17,  3.48it/s] 90%|████████▉ | 525/585 [05:22<00:17,  3.37it/s] 90%|████████▉ | 526/585 [05:22<00:17,  3.41it/s] 90%|█████████ | 527/585 [05:23<00:16,  3.43it/s] 90%|█████████ | 528/585 [05:23<00:16,  3.44it/s] 90%|█████████ | 529/585 [05:23<00:16,  3.45it/s] 91%|█████████ | 530/585 [05:24<00:15,  3.45it/s] 91%|█████████ | 531/585 [05:24<00:15,  3.46it/s] 91%|█████████ | 532/585 [05:24<00:15,  3.47it/s] 91%|█████████ | 533/585 [05:25<00:14,  3.47it/s] 91%|█████████▏| 534/585 [05:25<00:14,  3.48it/s] 91%|█████████▏| 535/585 [05:25<00:14,  3.47it/s] 92%|█████████▏| 536/585 [05:25<00:14,  3.33it/s] 92%|█████████▏| 537/585 [05:26<00:14,  3.37it/s] 92%|█████████▏| 538/585 [05:26<00:13,  3.41it/s] 92%|█████████▏| 539/585 [05:26<00:13,  3.43it/s] 92%|█████████▏| 540/585 [05:27<00:13,  3.44it/s] 92%|█████████▏| 541/585 [05:27<00:12,  3.45it/s] 93%|█████████▎| 542/585 [05:27<00:12,  3.46it/s] 93%|█████████▎| 543/585 [05:27<00:12,  3.47it/s] 93%|█████████▎| 544/585 [05:28<00:11,  3.47it/s] 93%|█████████▎| 545/585 [05:28<00:11,  3.47it/s] 93%|█████████▎| 546/585 [05:28<00:11,  3.47it/s] 94%|█████████▎| 547/585 [05:29<00:11,  3.35it/s] 94%|█████████▎| 548/585 [05:29<00:10,  3.39it/s] 94%|█████████▍| 549/585 [05:29<00:10,  3.42it/s] 94%|█████████▍| 550/585 [05:29<00:10,  3.44it/s] 94%|█████████▍| 551/585 [05:30<00:09,  3.44it/s] 94%|█████████▍| 552/585 [05:30<00:09,  3.45it/s] 95%|█████████▍| 553/585 [05:30<00:09,  3.46it/s] 95%|█████████▍| 554/585 [05:31<00:08,  3.46it/s] 95%|█████████▍| 555/585 [05:31<00:08,  3.46it/s] 95%|█████████▌| 556/585 [05:31<00:08,  3.46it/s] 95%|█████████▌| 557/585 [05:31<00:08,  3.46it/s] 95%|█████████▌| 558/585 [05:32<00:08,  3.34it/s] 96%|█████████▌| 559/585 [05:32<00:07,  3.38it/s] 96%|█████████▌| 560/585 [05:32<00:07,  3.41it/s] 96%|█████████▌| 561/585 [05:33<00:06,  3.43it/s] 96%|█████████▌| 562/585 [05:33<00:06,  3.44it/s] 96%|█████████▌| 563/585 [05:33<00:06,  3.45it/s] 96%|█████████▋| 564/585 [05:34<00:06,  3.46it/s] 97%|█████████▋| 565/585 [05:34<00:05,  3.47it/s] 97%|█████████▋| 566/585 [05:34<00:05,  3.46it/s] 97%|█████████▋| 567/585 [05:34<00:05,  3.47it/s] 97%|█████████▋| 568/585 [05:35<00:04,  3.47it/s] 97%|█████████▋| 569/585 [05:35<00:04,  3.33it/s] 97%|█████████▋| 570/585 [05:35<00:04,  3.38it/s] 98%|█████████▊| 571/585 [05:36<00:04,  3.41it/s] 98%|█████████▊| 572/585 [05:36<00:03,  3.43it/s] 98%|█████████▊| 573/585 [05:36<00:03,  3.28it/s] 98%|█████████▊| 574/585 [05:37<00:03,  3.34it/s] 98%|█████████▊| 575/585 [05:37<00:02,  3.38it/s] 98%|█████████▊| 576/585 [05:37<00:02,  3.40it/s] 99%|█████████▊| 577/585 [05:37<00:02,  3.42it/s] 99%|█████████▉| 578/585 [05:38<00:02,  3.44it/s] 99%|█████████▉| 579/585 [05:38<00:01,  3.45it/s] 99%|█████████▉| 580/585 [05:38<00:01,  3.33it/s] 99%|█████████▉| 581/585 [05:39<00:01,  3.38it/s] 99%|█████████▉| 582/585 [05:39<00:00,  3.41it/s]100%|█████████▉| 583/585 [05:39<00:00,  3.43it/s]100%|█████████▉| 584/585 [05:40<00:00,  3.16it/s]100%|██████████| 585/585 [05:40<00:00,  3.25it/s][INFO|trainer.py:2140] 2023-08-29 15:17:49,310 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 15:17:49,310 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 15:17:49,310 >>   Batch size = 8
{'eval_loss': 1.0473569631576538, 'eval_runtime': 24.5103, 'eval_samples_per_second': 349.567, 'eval_steps_per_second': 43.696, 'epoch': 4.0}
{'loss': 0.4249, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/1071 [00:00<?, ?it/s][A
  1%|          | 6/1071 [00:00<00:18, 56.41it/s][A
  1%|          | 12/1071 [00:00<00:21, 49.20it/s][A
  2%|▏         | 17/1071 [00:00<00:22, 47.52it/s][A
  2%|▏         | 22/1071 [00:00<00:22, 46.65it/s][A
  3%|▎         | 27/1071 [00:00<00:22, 45.91it/s][A
  3%|▎         | 32/1071 [00:00<00:22, 45.25it/s][A
  3%|▎         | 37/1071 [00:00<00:22, 45.02it/s][A
  4%|▍         | 42/1071 [00:00<00:22, 45.01it/s][A
  4%|▍         | 47/1071 [00:01<00:22, 44.92it/s][A
  5%|▍         | 52/1071 [00:01<00:47, 21.63it/s][A
  5%|▌         | 56/1071 [00:01<00:42, 24.01it/s][A
  6%|▌         | 61/1071 [00:01<00:35, 28.22it/s][A
  6%|▌         | 66/1071 [00:01<00:31, 31.96it/s][A
  7%|▋         | 71/1071 [00:01<00:28, 35.18it/s][A
  7%|▋         | 76/1071 [00:02<00:26, 37.73it/s][A
  8%|▊         | 81/1071 [00:02<00:24, 39.75it/s][A
  8%|▊         | 86/1071 [00:02<00:23, 41.35it/s][A
  8%|▊         | 91/1071 [00:02<00:23, 42.34it/s][A
  9%|▉         | 96/1071 [00:02<00:22, 42.80it/s][A
  9%|▉         | 101/1071 [00:02<00:22, 43.31it/s][A
 10%|▉         | 106/1071 [00:02<00:22, 43.76it/s][A
 10%|█         | 111/1071 [00:02<00:21, 44.22it/s][A
 11%|█         | 116/1071 [00:02<00:21, 44.49it/s][A
 11%|█▏        | 121/1071 [00:03<00:21, 44.57it/s][A
 12%|█▏        | 126/1071 [00:03<00:21, 44.75it/s][A
 12%|█▏        | 131/1071 [00:03<00:20, 44.91it/s][A
 13%|█▎        | 136/1071 [00:03<00:20, 44.97it/s][A
 13%|█▎        | 141/1071 [00:03<00:20, 44.72it/s][A
 14%|█▎        | 146/1071 [00:03<00:20, 44.72it/s][A
 14%|█▍        | 151/1071 [00:03<00:20, 44.76it/s][A
 15%|█▍        | 156/1071 [00:03<00:20, 44.91it/s][A
 15%|█▌        | 161/1071 [00:03<00:20, 45.03it/s][A
 15%|█▌        | 166/1071 [00:04<00:20, 45.03it/s][A
 16%|█▌        | 171/1071 [00:04<00:19, 45.06it/s][A
 16%|█▋        | 176/1071 [00:04<00:19, 45.10it/s][A
 17%|█▋        | 181/1071 [00:04<00:19, 45.00it/s][A
 17%|█▋        | 186/1071 [00:04<00:19, 44.78it/s][A
 18%|█▊        | 191/1071 [00:04<00:22, 39.52it/s][A
 18%|█▊        | 196/1071 [00:04<00:21, 41.09it/s][A
 19%|█▉        | 201/1071 [00:04<00:20, 42.30it/s][A
 19%|█▉        | 206/1071 [00:05<00:20, 43.15it/s][A
 20%|█▉        | 211/1071 [00:05<00:19, 43.76it/s][A
 20%|██        | 216/1071 [00:05<00:19, 44.22it/s][A
 21%|██        | 221/1071 [00:05<00:19, 44.48it/s][A
 21%|██        | 226/1071 [00:05<00:18, 44.62it/s][A
 22%|██▏       | 231/1071 [00:05<00:18, 44.24it/s][A
 22%|██▏       | 236/1071 [00:05<00:18, 44.47it/s][A
 23%|██▎       | 241/1071 [00:05<00:18, 44.61it/s][A
 23%|██▎       | 246/1071 [00:05<00:18, 44.88it/s][A
 23%|██▎       | 251/1071 [00:06<00:18, 44.97it/s][A
 24%|██▍       | 256/1071 [00:06<00:18, 45.03it/s][A
 24%|██▍       | 261/1071 [00:06<00:17, 45.08it/s][A
 25%|██▍       | 266/1071 [00:06<00:17, 44.74it/s][A
 25%|██▌       | 271/1071 [00:06<00:18, 42.57it/s][A
 26%|██▌       | 276/1071 [00:06<00:18, 43.29it/s][A
 26%|██▌       | 281/1071 [00:06<00:18, 43.77it/s][A
 27%|██▋       | 286/1071 [00:06<00:17, 44.23it/s][A
 27%|██▋       | 291/1071 [00:06<00:17, 44.42it/s][A
 28%|██▊       | 296/1071 [00:07<00:17, 44.48it/s][A
 28%|██▊       | 301/1071 [00:07<00:17, 44.62it/s][A
 29%|██▊       | 306/1071 [00:07<00:17, 44.82it/s][A
 29%|██▉       | 311/1071 [00:07<00:17, 44.61it/s][A
 30%|██▉       | 316/1071 [00:07<00:16, 44.63it/s][A
 30%|██▉       | 321/1071 [00:07<00:16, 44.71it/s][A
 30%|███       | 326/1071 [00:07<00:16, 44.82it/s][A
 31%|███       | 331/1071 [00:07<00:16, 44.87it/s][A
 31%|███▏      | 336/1071 [00:07<00:16, 44.99it/s][A
 32%|███▏      | 341/1071 [00:08<00:16, 44.96it/s][A
 32%|███▏      | 346/1071 [00:08<00:16, 44.85it/s][A
 33%|███▎      | 351/1071 [00:08<00:16, 44.96it/s][A
 33%|███▎      | 356/1071 [00:08<00:15, 44.81it/s][A
 34%|███▎      | 361/1071 [00:08<00:15, 44.79it/s][A
 34%|███▍      | 366/1071 [00:08<00:15, 44.83it/s][A
 35%|███▍      | 371/1071 [00:08<00:15, 44.90it/s][A
 35%|███▌      | 376/1071 [00:08<00:15, 44.91it/s][A
 36%|███▌      | 381/1071 [00:08<00:15, 44.96it/s][A
 36%|███▌      | 386/1071 [00:09<00:15, 45.01it/s][A
 37%|███▋      | 391/1071 [00:09<00:15, 45.14it/s][A
 37%|███▋      | 396/1071 [00:09<00:14, 45.02it/s][A
 37%|███▋      | 401/1071 [00:09<00:14, 44.89it/s][A
 38%|███▊      | 406/1071 [00:09<00:15, 42.31it/s][A
 38%|███▊      | 411/1071 [00:09<00:15, 43.20it/s][A
 39%|███▉      | 416/1071 [00:09<00:14, 43.76it/s][A
 39%|███▉      | 421/1071 [00:09<00:14, 44.12it/s][A
 40%|███▉      | 426/1071 [00:09<00:14, 44.32it/s][A
 40%|████      | 431/1071 [00:10<00:14, 44.48it/s][A
 41%|████      | 436/1071 [00:10<00:14, 44.76it/s][A
 41%|████      | 441/1071 [00:10<00:14, 44.82it/s][A
 42%|████▏     | 446/1071 [00:10<00:14, 44.53it/s][A
 42%|████▏     | 451/1071 [00:10<00:13, 44.60it/s][A
 43%|████▎     | 456/1071 [00:10<00:13, 44.74it/s][A
 43%|████▎     | 461/1071 [00:10<00:13, 44.81it/s][A
 44%|████▎     | 466/1071 [00:10<00:13, 44.94it/s][A
 44%|████▍     | 471/1071 [00:10<00:13, 44.89it/s][A
 44%|████▍     | 476/1071 [00:11<00:13, 44.99it/s][A
 45%|████▍     | 481/1071 [00:11<00:13, 45.01it/s][A
 45%|████▌     | 486/1071 [00:11<00:13, 44.85it/s][A
 46%|████▌     | 491/1071 [00:11<00:12, 44.73it/s][A
 46%|████▋     | 496/1071 [00:11<00:12, 44.75it/s][A
 47%|████▋     | 501/1071 [00:11<00:12, 44.87it/s][A
 47%|████▋     | 506/1071 [00:11<00:12, 44.90it/s][A
 48%|████▊     | 511/1071 [00:11<00:12, 44.85it/s][A
 48%|████▊     | 516/1071 [00:11<00:12, 44.83it/s][A
 49%|████▊     | 521/1071 [00:12<00:12, 44.93it/s][A
 49%|████▉     | 526/1071 [00:12<00:12, 44.85it/s][A
 50%|████▉     | 531/1071 [00:12<00:12, 44.75it/s][A
 50%|█████     | 536/1071 [00:12<00:11, 44.72it/s][A
 51%|█████     | 541/1071 [00:12<00:12, 41.71it/s][A
 51%|█████     | 546/1071 [00:12<00:12, 42.78it/s][A
 51%|█████▏    | 551/1071 [00:12<00:11, 43.56it/s][A
 52%|█████▏    | 556/1071 [00:12<00:11, 44.12it/s][A
 52%|█████▏    | 561/1071 [00:12<00:11, 44.43it/s][A
 53%|█████▎    | 566/1071 [00:13<00:11, 44.68it/s][A
 53%|█████▎    | 571/1071 [00:13<00:11, 44.73it/s][A
 54%|█████▍    | 576/1071 [00:13<00:11, 44.56it/s][A
 54%|█████▍    | 581/1071 [00:13<00:11, 44.37it/s][A
 55%|█████▍    | 586/1071 [00:13<00:10, 44.36it/s][A
 55%|█████▌    | 591/1071 [00:13<00:10, 44.49it/s][A
 56%|█████▌    | 596/1071 [00:13<00:10, 44.88it/s][A
 56%|█████▌    | 601/1071 [00:13<00:10, 45.00it/s][A
 57%|█████▋    | 606/1071 [00:13<00:10, 45.12it/s][A
 57%|█████▋    | 611/1071 [00:14<00:10, 45.10it/s][A
 58%|█████▊    | 616/1071 [00:14<00:10, 45.07it/s][A
 58%|█████▊    | 621/1071 [00:14<00:10, 44.91it/s][A
 58%|█████▊    | 626/1071 [00:14<00:09, 44.65it/s][A
 59%|█████▉    | 631/1071 [00:14<00:09, 44.58it/s][A
 59%|█████▉    | 636/1071 [00:14<00:09, 44.65it/s][A
 60%|█████▉    | 641/1071 [00:14<00:09, 44.81it/s][A
 60%|██████    | 646/1071 [00:14<00:09, 44.97it/s][A
 61%|██████    | 651/1071 [00:14<00:09, 45.12it/s][A
 61%|██████▏   | 656/1071 [00:15<00:09, 45.08it/s][A
 62%|██████▏   | 661/1071 [00:15<00:09, 45.10it/s][A
 62%|██████▏   | 666/1071 [00:15<00:09, 44.85it/s][A
 63%|██████▎   | 671/1071 [00:15<00:08, 44.61it/s][A
 63%|██████▎   | 676/1071 [00:15<00:09, 41.57it/s][A
 64%|██████▎   | 681/1071 [00:15<00:09, 42.69it/s][A
 64%|██████▍   | 686/1071 [00:15<00:08, 43.42it/s][A
 65%|██████▍   | 691/1071 [00:15<00:08, 44.10it/s][A
 65%|██████▍   | 696/1071 [00:16<00:08, 44.52it/s][A
 65%|██████▌   | 701/1071 [00:16<00:08, 44.65it/s][A
 66%|██████▌   | 706/1071 [00:16<00:08, 44.71it/s][A
 66%|██████▋   | 711/1071 [00:16<00:08, 44.60it/s][A
 67%|██████▋   | 716/1071 [00:16<00:08, 44.27it/s][A
 67%|██████▋   | 721/1071 [00:16<00:07, 44.35it/s][A
 68%|██████▊   | 726/1071 [00:16<00:07, 44.68it/s][A
 68%|██████▊   | 731/1071 [00:16<00:07, 44.87it/s][A
 69%|██████▊   | 736/1071 [00:16<00:07, 45.06it/s][A
 69%|██████▉   | 741/1071 [00:17<00:07, 45.15it/s][A
 70%|██████▉   | 746/1071 [00:17<00:07, 45.17it/s][A
 70%|███████   | 751/1071 [00:17<00:07, 45.07it/s][A
 71%|███████   | 756/1071 [00:17<00:07, 44.73it/s][A
 71%|███████   | 761/1071 [00:17<00:06, 44.47it/s][A
 72%|███████▏  | 766/1071 [00:17<00:06, 44.51it/s][A
 72%|███████▏  | 771/1071 [00:17<00:06, 44.72it/s][A
 72%|███████▏  | 776/1071 [00:17<00:06, 44.61it/s][A
 73%|███████▎  | 781/1071 [00:17<00:06, 44.88it/s][A
 73%|███████▎  | 786/1071 [00:18<00:06, 45.07it/s][A
 74%|███████▍  | 791/1071 [00:18<00:06, 45.15it/s][A
 74%|███████▍  | 796/1071 [00:18<00:06, 45.13it/s][A
 75%|███████▍  | 801/1071 [00:18<00:06, 44.81it/s][A
 75%|███████▌  | 806/1071 [00:18<00:05, 44.62it/s][A
 76%|███████▌  | 811/1071 [00:18<00:06, 42.01it/s][A
 76%|███████▌  | 816/1071 [00:18<00:05, 43.02it/s][A
 77%|███████▋  | 821/1071 [00:18<00:05, 43.75it/s][A
 77%|███████▋  | 826/1071 [00:18<00:05, 44.21it/s][A
 78%|███████▊  | 831/1071 [00:19<00:05, 44.57it/s][A
 78%|███████▊  | 836/1071 [00:19<00:05, 44.82it/s][A
 79%|███████▊  | 841/1071 [00:19<00:05, 44.72it/s][A
 79%|███████▉  | 846/1071 [00:19<00:05, 44.61it/s][A
 79%|███████▉  | 851/1071 [00:19<00:04, 44.26it/s][A
 80%|███████▉  | 856/1071 [00:19<00:04, 44.36it/s][A
 80%|████████  | 861/1071 [00:19<00:04, 44.56it/s][A
 81%|████████  | 866/1071 [00:19<00:04, 44.78it/s][A
 81%|████████▏ | 871/1071 [00:19<00:04, 45.00it/s][A
 82%|████████▏ | 876/1071 [00:20<00:04, 45.18it/s][A
 82%|████████▏ | 881/1071 [00:20<00:04, 45.23it/s][A
 83%|████████▎ | 886/1071 [00:20<00:04, 44.92it/s][A
 83%|████████▎ | 891/1071 [00:20<00:04, 44.62it/s][A
 84%|████████▎ | 896/1071 [00:20<00:03, 44.46it/s][A
 84%|████████▍ | 901/1071 [00:20<00:03, 44.42it/s][A
 85%|████████▍ | 906/1071 [00:20<00:03, 44.56it/s][A
 85%|████████▌ | 911/1071 [00:20<00:03, 44.74it/s][A
 86%|████████▌ | 916/1071 [00:20<00:03, 44.98it/s][A
 86%|████████▌ | 921/1071 [00:21<00:03, 45.08it/s][A
 86%|████████▋ | 926/1071 [00:21<00:03, 45.13it/s][A
 87%|████████▋ | 931/1071 [00:21<00:03, 45.10it/s][A
 87%|████████▋ | 936/1071 [00:21<00:03, 44.86it/s][A
 88%|████████▊ | 941/1071 [00:21<00:02, 44.69it/s][A
 88%|████████▊ | 946/1071 [00:21<00:03, 40.38it/s][A
 89%|████████▉ | 951/1071 [00:21<00:02, 41.77it/s][A
 89%|████████▉ | 956/1071 [00:21<00:02, 42.69it/s][A
 90%|████████▉ | 961/1071 [00:21<00:02, 43.48it/s][A
 90%|█████████ | 966/1071 [00:22<00:02, 44.02it/s][A
 91%|█████████ | 971/1071 [00:22<00:02, 44.34it/s][A
 91%|█████████ | 976/1071 [00:22<00:02, 44.61it/s][A
 92%|█████████▏| 981/1071 [00:22<00:02, 44.75it/s][A
 92%|█████████▏| 986/1071 [00:22<00:01, 44.45it/s][A
 93%|█████████▎| 991/1071 [00:22<00:01, 44.49it/s][A
 93%|█████████▎| 996/1071 [00:22<00:01, 44.57it/s][A
 93%|█████████▎| 1001/1071 [00:22<00:01, 44.71it/s][A
 94%|█████████▍| 1006/1071 [00:22<00:01, 44.91it/s][A
 94%|█████████▍| 1011/1071 [00:23<00:01, 44.98it/s][A
 95%|█████████▍| 1016/1071 [00:23<00:01, 45.07it/s][A
 95%|█████████▌| 1021/1071 [00:23<00:01, 45.08it/s][A
 96%|█████████▌| 1026/1071 [00:23<00:01, 44.84it/s][A
 96%|█████████▋| 1031/1071 [00:23<00:00, 44.61it/s][A
 97%|█████████▋| 1036/1071 [00:23<00:00, 44.66it/s][A
 97%|█████████▋| 1041/1071 [00:23<00:00, 44.78it/s][A
 98%|█████████▊| 1046/1071 [00:23<00:00, 44.85it/s][A
 98%|█████████▊| 1051/1071 [00:23<00:00, 44.99it/s][A
 99%|█████████▊| 1056/1071 [00:24<00:00, 45.00it/s][A
 99%|█████████▉| 1061/1071 [00:24<00:00, 44.91it/s][A
100%|█████████▉| 1066/1071 [00:24<00:00, 44.90it/s][A
100%|██████████| 1071/1071 [00:24<00:00, 44.71it/s][A
                                                   [A                                                 
100%|██████████| 1071/1071 [00:24<00:00, 44.71it/s][A100%|██████████| 585/585 [06:04<00:00,  3.25it/s]
                                                   [A[INFO|trainer.py:1894] 2023-08-29 15:18:14,434 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 15:18:14,936 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 15:18:21,168 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 15:18:21,717 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 15:18:21,978 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 15:18:32,663 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 15:18:32,689 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117 (score: 0.9949867725372314).
                                                 100%|██████████| 585/585 [06:36<00:00,  3.25it/s]100%|██████████| 585/585 [06:36<00:00,  1.48it/s]
[INFO|trainer.py:1894] 2023-08-29 15:18:45,461 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 15:18:45,661 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 15:18:49,932 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 15:18:50,157 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 15:18:50,344 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 15:18:51,452 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:18:51,453 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:18:51,453 >>   train_loss               =      0.422
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:18:51,453 >>   train_runtime            = 0:06:36.37
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:18:51,453 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:18:51,453 >>   train_samples_per_second =     94.606
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:18:51,453 >>   train_steps_per_second   =      1.476
{'eval_loss': 1.056121826171875, 'eval_runtime': 24.4709, 'eval_samples_per_second': 350.13, 'eval_steps_per_second': 43.766, 'epoch': 5.0}
{'train_runtime': 396.3794, 'train_samples_per_second': 94.606, 'train_steps_per_second': 1.476, 'train_loss': 0.42202649890867056, 'epoch': 5.0}
08/29/2023 15:18:51 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 15:18:51,781 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 15:18:51,782 >>   Num examples = 8568
[INFO|trainer.py:2145] 2023-08-29 15:18:51,782 >>   Batch size = 8
  0%|          | 0/1071 [00:00<?, ?it/s]  1%|          | 6/1071 [00:00<00:18, 56.38it/s]  1%|          | 12/1071 [00:00<00:21, 50.01it/s]  2%|▏         | 18/1071 [00:00<00:21, 47.94it/s]  2%|▏         | 23/1071 [00:00<00:22, 47.29it/s]  3%|▎         | 28/1071 [00:00<00:22, 46.62it/s]  3%|▎         | 33/1071 [00:00<00:22, 46.43it/s]  4%|▎         | 38/1071 [00:00<00:22, 46.26it/s]  4%|▍         | 43/1071 [00:00<00:22, 45.84it/s]  4%|▍         | 48/1071 [00:01<00:22, 45.16it/s]  5%|▍         | 53/1071 [00:01<00:22, 45.11it/s]  5%|▌         | 58/1071 [00:01<00:23, 42.31it/s]  6%|▌         | 63/1071 [00:01<00:23, 43.28it/s]  6%|▋         | 68/1071 [00:01<00:22, 43.86it/s]  7%|▋         | 73/1071 [00:01<00:22, 44.54it/s]  7%|▋         | 78/1071 [00:01<00:22, 44.92it/s]  8%|▊         | 83/1071 [00:01<00:21, 45.22it/s]  8%|▊         | 88/1071 [00:01<00:21, 45.12it/s]  9%|▊         | 93/1071 [00:02<00:21, 44.92it/s]  9%|▉         | 98/1071 [00:02<00:21, 44.74it/s] 10%|▉         | 103/1071 [00:02<00:21, 44.58it/s] 10%|█         | 108/1071 [00:02<00:21, 44.81it/s] 11%|█         | 113/1071 [00:02<00:21, 45.04it/s] 11%|█         | 118/1071 [00:02<00:21, 45.25it/s] 11%|█▏        | 123/1071 [00:02<00:20, 45.41it/s] 12%|█▏        | 128/1071 [00:02<00:20, 45.52it/s] 12%|█▏        | 133/1071 [00:02<00:20, 45.43it/s] 13%|█▎        | 138/1071 [00:03<00:20, 45.19it/s] 13%|█▎        | 143/1071 [00:03<00:20, 45.00it/s] 14%|█▍        | 148/1071 [00:03<00:20, 44.93it/s] 14%|█▍        | 153/1071 [00:03<00:20, 44.81it/s] 15%|█▍        | 158/1071 [00:03<00:20, 44.93it/s] 15%|█▌        | 163/1071 [00:03<00:20, 45.23it/s] 16%|█▌        | 168/1071 [00:03<00:19, 45.25it/s] 16%|█▌        | 173/1071 [00:03<00:19, 45.38it/s] 17%|█▋        | 178/1071 [00:03<00:19, 45.43it/s] 17%|█▋        | 183/1071 [00:04<00:19, 45.02it/s] 18%|█▊        | 188/1071 [00:04<00:19, 45.37it/s] 18%|█▊        | 193/1071 [00:04<00:20, 42.94it/s] 18%|█▊        | 198/1071 [00:04<00:19, 43.67it/s] 19%|█▉        | 203/1071 [00:04<00:19, 44.11it/s] 19%|█▉        | 208/1071 [00:04<00:19, 44.30it/s] 20%|█▉        | 213/1071 [00:04<00:19, 44.64it/s] 20%|██        | 218/1071 [00:04<00:19, 44.81it/s] 21%|██        | 223/1071 [00:04<00:18, 44.97it/s] 21%|██▏       | 228/1071 [00:05<00:18, 44.93it/s] 22%|██▏       | 233/1071 [00:05<00:18, 44.68it/s] 22%|██▏       | 238/1071 [00:05<00:18, 44.88it/s] 23%|██▎       | 243/1071 [00:05<00:18, 44.96it/s] 23%|██▎       | 248/1071 [00:05<00:18, 43.92it/s] 24%|██▎       | 253/1071 [00:05<00:18, 44.43it/s] 24%|██▍       | 258/1071 [00:05<00:18, 44.56it/s] 25%|██▍       | 263/1071 [00:05<00:18, 44.87it/s] 25%|██▌       | 268/1071 [00:05<00:17, 44.93it/s] 25%|██▌       | 273/1071 [00:06<00:17, 44.83it/s] 26%|██▌       | 278/1071 [00:06<00:17, 44.69it/s] 26%|██▋       | 283/1071 [00:06<00:17, 44.72it/s] 27%|██▋       | 288/1071 [00:06<00:17, 44.83it/s] 27%|██▋       | 293/1071 [00:06<00:17, 45.01it/s] 28%|██▊       | 298/1071 [00:06<00:17, 45.26it/s] 28%|██▊       | 303/1071 [00:06<00:16, 45.37it/s] 29%|██▉       | 308/1071 [00:06<00:16, 45.46it/s] 29%|██▉       | 313/1071 [00:06<00:16, 45.36it/s] 30%|██▉       | 318/1071 [00:07<00:16, 45.18it/s] 30%|███       | 323/1071 [00:07<00:16, 45.02it/s] 31%|███       | 328/1071 [00:07<00:16, 45.01it/s] 31%|███       | 333/1071 [00:07<00:16, 44.98it/s] 32%|███▏      | 338/1071 [00:07<00:16, 45.00it/s] 32%|███▏      | 343/1071 [00:07<00:16, 45.24it/s] 32%|███▏      | 348/1071 [00:07<00:15, 45.42it/s] 33%|███▎      | 353/1071 [00:07<00:15, 45.40it/s] 33%|███▎      | 358/1071 [00:07<00:15, 45.40it/s] 34%|███▍      | 363/1071 [00:08<00:15, 45.26it/s] 34%|███▍      | 368/1071 [00:08<00:15, 45.03it/s] 35%|███▍      | 373/1071 [00:08<00:15, 45.04it/s] 35%|███▌      | 378/1071 [00:08<00:15, 45.00it/s] 36%|███▌      | 383/1071 [00:08<00:16, 42.22it/s] 36%|███▌      | 388/1071 [00:08<00:15, 43.21it/s] 37%|███▋      | 393/1071 [00:08<00:15, 43.93it/s] 37%|███▋      | 398/1071 [00:08<00:15, 44.41it/s] 38%|███▊      | 403/1071 [00:08<00:14, 44.74it/s] 38%|███▊      | 408/1071 [00:09<00:14, 44.89it/s] 39%|███▊      | 413/1071 [00:09<00:14, 44.92it/s] 39%|███▉      | 418/1071 [00:09<00:14, 44.94it/s] 39%|███▉      | 423/1071 [00:09<00:14, 44.67it/s] 40%|███▉      | 428/1071 [00:09<00:14, 44.70it/s] 40%|████      | 433/1071 [00:09<00:14, 44.95it/s] 41%|████      | 438/1071 [00:09<00:14, 45.21it/s] 41%|████▏     | 443/1071 [00:09<00:13, 45.37it/s] 42%|████▏     | 448/1071 [00:09<00:13, 45.35it/s] 42%|████▏     | 453/1071 [00:10<00:13, 45.32it/s] 43%|████▎     | 458/1071 [00:10<00:13, 45.22it/s] 43%|████▎     | 463/1071 [00:10<00:13, 45.05it/s] 44%|████▎     | 468/1071 [00:10<00:13, 44.85it/s] 44%|████▍     | 473/1071 [00:10<00:13, 44.80it/s] 45%|████▍     | 478/1071 [00:10<00:13, 44.98it/s] 45%|████▌     | 483/1071 [00:10<00:13, 45.15it/s] 46%|████▌     | 488/1071 [00:10<00:12, 45.21it/s] 46%|████▌     | 493/1071 [00:10<00:12, 45.30it/s] 46%|████▋     | 498/1071 [00:11<00:12, 45.30it/s] 47%|████▋     | 503/1071 [00:11<00:12, 45.04it/s] 47%|████▋     | 508/1071 [00:11<00:12, 45.00it/s] 48%|████▊     | 513/1071 [00:11<00:12, 44.83it/s] 48%|████▊     | 518/1071 [00:11<00:13, 41.24it/s] 49%|████▉     | 523/1071 [00:11<00:12, 42.57it/s] 49%|████▉     | 528/1071 [00:11<00:12, 43.48it/s] 50%|████▉     | 533/1071 [00:11<00:12, 44.12it/s] 50%|█████     | 538/1071 [00:11<00:11, 44.64it/s] 51%|█████     | 543/1071 [00:12<00:11, 44.93it/s] 51%|█████     | 548/1071 [00:12<00:11, 44.91it/s] 52%|█████▏    | 553/1071 [00:12<00:11, 44.78it/s] 52%|█████▏    | 558/1071 [00:12<00:11, 44.49it/s] 53%|█████▎    | 563/1071 [00:12<00:11, 44.47it/s] 53%|█████▎    | 568/1071 [00:12<00:11, 44.63it/s] 54%|█████▎    | 573/1071 [00:12<00:11, 44.87it/s] 54%|█████▍    | 578/1071 [00:12<00:10, 45.11it/s] 54%|█████▍    | 583/1071 [00:12<00:10, 45.25it/s] 55%|█████▍    | 588/1071 [00:13<00:10, 45.37it/s] 55%|█████▌    | 593/1071 [00:13<00:10, 45.23it/s] 56%|█████▌    | 598/1071 [00:13<00:10, 44.97it/s] 56%|█████▋    | 603/1071 [00:13<00:10, 44.60it/s] 57%|█████▋    | 608/1071 [00:13<00:10, 44.52it/s] 57%|█████▋    | 613/1071 [00:13<00:10, 44.85it/s] 58%|█████▊    | 618/1071 [00:13<00:10, 44.97it/s] 58%|█████▊    | 623/1071 [00:13<00:09, 45.13it/s] 59%|█████▊    | 628/1071 [00:13<00:09, 45.20it/s] 59%|█████▉    | 633/1071 [00:14<00:09, 45.32it/s] 60%|█████▉    | 638/1071 [00:14<00:09, 45.35it/s] 60%|██████    | 643/1071 [00:14<00:09, 45.05it/s] 61%|██████    | 648/1071 [00:14<00:09, 44.91it/s] 61%|██████    | 653/1071 [00:14<00:09, 43.10it/s] 61%|██████▏   | 658/1071 [00:14<00:09, 43.78it/s] 62%|██████▏   | 663/1071 [00:14<00:09, 44.25it/s] 62%|██████▏   | 668/1071 [00:14<00:09, 44.56it/s] 63%|██████▎   | 673/1071 [00:14<00:08, 44.75it/s] 63%|██████▎   | 678/1071 [00:15<00:08, 44.96it/s] 64%|██████▍   | 683/1071 [00:15<00:08, 45.11it/s] 64%|██████▍   | 688/1071 [00:15<00:08, 45.09it/s] 65%|██████▍   | 693/1071 [00:15<00:08, 44.69it/s] 65%|██████▌   | 698/1071 [00:15<00:08, 44.63it/s] 66%|██████▌   | 703/1071 [00:15<00:08, 44.87it/s] 66%|██████▌   | 708/1071 [00:15<00:08, 45.07it/s] 67%|██████▋   | 713/1071 [00:15<00:07, 45.10it/s] 67%|██████▋   | 718/1071 [00:15<00:07, 45.20it/s] 68%|██████▊   | 723/1071 [00:16<00:07, 45.25it/s] 68%|██████▊   | 728/1071 [00:16<00:07, 45.21it/s] 68%|██████▊   | 733/1071 [00:16<00:07, 44.86it/s] 69%|██████▉   | 738/1071 [00:16<00:07, 44.84it/s] 69%|██████▉   | 743/1071 [00:16<00:07, 44.86it/s] 70%|██████▉   | 748/1071 [00:16<00:07, 44.93it/s] 70%|███████   | 753/1071 [00:16<00:07, 45.18it/s] 71%|███████   | 758/1071 [00:16<00:06, 45.17it/s] 71%|███████   | 763/1071 [00:16<00:06, 45.26it/s] 72%|███████▏  | 768/1071 [00:17<00:06, 45.23it/s] 72%|███████▏  | 773/1071 [00:17<00:06, 45.19it/s] 73%|███████▎  | 778/1071 [00:17<00:06, 45.04it/s] 73%|███████▎  | 783/1071 [00:17<00:06, 44.93it/s] 74%|███████▎  | 788/1071 [00:17<00:06, 40.57it/s] 74%|███████▍  | 793/1071 [00:17<00:06, 41.93it/s] 75%|███████▍  | 798/1071 [00:17<00:06, 43.05it/s] 75%|███████▍  | 803/1071 [00:17<00:06, 43.68it/s] 75%|███████▌  | 808/1071 [00:18<00:05, 44.24it/s] 76%|███████▌  | 813/1071 [00:18<00:05, 44.66it/s] 76%|███████▋  | 818/1071 [00:18<00:05, 44.85it/s] 77%|███████▋  | 823/1071 [00:18<00:05, 44.80it/s] 77%|███████▋  | 828/1071 [00:18<00:05, 44.42it/s] 78%|███████▊  | 833/1071 [00:18<00:05, 44.36it/s] 78%|███████▊  | 838/1071 [00:18<00:05, 44.54it/s] 79%|███████▊  | 843/1071 [00:18<00:05, 44.85it/s] 79%|███████▉  | 848/1071 [00:18<00:04, 45.12it/s] 80%|███████▉  | 853/1071 [00:19<00:04, 45.23it/s] 80%|████████  | 858/1071 [00:19<00:04, 45.36it/s] 81%|████████  | 863/1071 [00:19<00:04, 45.37it/s] 81%|████████  | 868/1071 [00:19<00:04, 45.06it/s] 82%|████████▏ | 873/1071 [00:19<00:04, 44.86it/s] 82%|████████▏ | 878/1071 [00:19<00:04, 44.67it/s] 82%|████████▏ | 883/1071 [00:19<00:04, 44.67it/s] 83%|████████▎ | 888/1071 [00:19<00:04, 44.95it/s] 83%|████████▎ | 893/1071 [00:19<00:03, 45.19it/s] 84%|████████▍ | 898/1071 [00:20<00:03, 45.29it/s] 84%|████████▍ | 903/1071 [00:20<00:03, 45.31it/s] 85%|████████▍ | 908/1071 [00:20<00:03, 45.26it/s] 85%|████████▌ | 913/1071 [00:20<00:03, 45.01it/s] 86%|████████▌ | 918/1071 [00:20<00:03, 44.75it/s] 86%|████████▌ | 923/1071 [00:20<00:03, 40.20it/s] 87%|████████▋ | 928/1071 [00:20<00:03, 41.66it/s] 87%|████████▋ | 933/1071 [00:20<00:03, 42.85it/s] 88%|████████▊ | 938/1071 [00:20<00:03, 43.63it/s] 88%|████████▊ | 943/1071 [00:21<00:02, 44.14it/s] 89%|████████▊ | 948/1071 [00:21<00:02, 44.55it/s] 89%|████████▉ | 953/1071 [00:21<00:02, 44.84it/s] 89%|████████▉ | 958/1071 [00:21<00:02, 44.86it/s] 90%|████████▉ | 963/1071 [00:21<00:02, 44.60it/s] 90%|█████████ | 968/1071 [00:21<00:02, 44.16it/s] 91%|█████████ | 973/1071 [00:21<00:02, 44.46it/s] 91%|█████████▏| 978/1071 [00:21<00:02, 44.78it/s] 92%|█████████▏| 983/1071 [00:21<00:01, 44.98it/s] 92%|█████████▏| 988/1071 [00:22<00:01, 45.20it/s] 93%|█████████▎| 993/1071 [00:22<00:01, 45.32it/s] 93%|█████████▎| 998/1071 [00:22<00:01, 45.33it/s] 94%|█████████▎| 1003/1071 [00:22<00:01, 45.15it/s] 94%|█████████▍| 1008/1071 [00:22<00:01, 44.81it/s] 95%|█████████▍| 1013/1071 [00:22<00:01, 44.65it/s] 95%|█████████▌| 1018/1071 [00:22<00:01, 44.70it/s] 96%|█████████▌| 1023/1071 [00:22<00:01, 44.91it/s] 96%|█████████▌| 1028/1071 [00:22<00:00, 45.10it/s] 96%|█████████▋| 1033/1071 [00:23<00:00, 45.21it/s] 97%|█████████▋| 1038/1071 [00:23<00:00, 45.31it/s] 97%|█████████▋| 1043/1071 [00:23<00:00, 45.35it/s] 98%|█████████▊| 1048/1071 [00:23<00:00, 45.08it/s] 98%|█████████▊| 1053/1071 [00:23<00:00, 44.86it/s] 99%|█████████▉| 1058/1071 [00:23<00:00, 38.55it/s] 99%|█████████▉| 1063/1071 [00:23<00:00, 40.43it/s]100%|█████████▉| 1068/1071 [00:23<00:00, 41.84it/s]100%|██████████| 1071/1071 [00:23<00:00, 44.66it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 15:19:15,783 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:19:15,783 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:19:15,783 >>   eval_loss               =      0.995
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:19:15,783 >>   eval_runtime            = 0:00:24.00
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:19:15,783 >>   eval_samples            =       8568
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:19:15,783 >>   eval_samples_per_second =    356.983
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:19:15,783 >>   eval_steps_per_second   =     44.623
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:19:15,783 >>   perplexity              =     2.7047
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:20:35,858 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:20:35,895 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:20:35,895 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:20:35,896 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:20:35,896 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 15:20:36,930 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 15:20:36,931 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 15:20:37,603 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 15:20:48,733 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 15:20:48,774 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:20:51,847 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:20:51,880 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:20:51,880 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:20:51,880 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:20:51,880 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 15:20:53,042 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 15:20:53,043 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 15:20:53,738 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 15:20:55,590 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 15:20:55,619 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-585
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/generator/iter5/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl', 'labels': ['country', 'place of death', 'production company', 'screenwriter', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 21439
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21539, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:04,  1.57it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:06,  1.56it/s]Extractor Predicting: 12it [00:07,  1.52it/s]Extractor Predicting: 13it [00:08,  1.59it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.63it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.66it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:13,  1.66it/s]Extractor Predicting: 23it [00:14,  1.71it/s]Extractor Predicting: 24it [00:14,  1.66it/s]Extractor Predicting: 25it [00:15,  1.56it/s]Extractor Predicting: 26it [00:16,  1.52it/s]Extractor Predicting: 27it [00:16,  1.56it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:18,  1.63it/s]Extractor Predicting: 30it [00:18,  1.57it/s]Extractor Predicting: 31it [00:19,  1.54it/s]Extractor Predicting: 32it [00:20,  1.59it/s]Extractor Predicting: 33it [00:20,  1.62it/s]Extractor Predicting: 34it [00:21,  1.67it/s]Extractor Predicting: 35it [00:21,  1.73it/s]Extractor Predicting: 36it [00:22,  1.70it/s]Extractor Predicting: 37it [00:23,  1.62it/s]Extractor Predicting: 38it [00:23,  1.65it/s]Extractor Predicting: 39it [00:24,  1.66it/s]Extractor Predicting: 40it [00:24,  1.68it/s]Extractor Predicting: 41it [00:25,  1.71it/s]Extractor Predicting: 42it [00:25,  1.70it/s]Extractor Predicting: 43it [00:26,  1.71it/s]Extractor Predicting: 44it [00:27,  1.68it/s]Extractor Predicting: 45it [00:27,  1.72it/s]Extractor Predicting: 46it [00:28,  1.69it/s]Extractor Predicting: 47it [00:28,  1.70it/s]Extractor Predicting: 48it [00:29,  1.69it/s]Extractor Predicting: 49it [00:30,  1.73it/s]Extractor Predicting: 50it [00:30,  1.77it/s]Extractor Predicting: 51it [00:31,  1.79it/s]Extractor Predicting: 52it [00:31,  1.78it/s]Extractor Predicting: 53it [00:32,  1.80it/s]Extractor Predicting: 54it [00:32,  1.75it/s]Extractor Predicting: 55it [00:33,  1.76it/s]Extractor Predicting: 56it [00:33,  1.77it/s]Extractor Predicting: 57it [00:34,  1.80it/s]Extractor Predicting: 58it [00:35,  1.84it/s]Extractor Predicting: 59it [00:35,  1.73it/s]Extractor Predicting: 60it [00:36,  1.72it/s]Extractor Predicting: 61it [00:36,  1.71it/s]Extractor Predicting: 62it [00:37,  1.73it/s]Extractor Predicting: 63it [00:38,  1.71it/s]Extractor Predicting: 64it [00:38,  1.77it/s]Extractor Predicting: 65it [00:39,  1.82it/s]Extractor Predicting: 66it [00:39,  1.80it/s]Extractor Predicting: 67it [00:40,  1.78it/s]Extractor Predicting: 68it [00:40,  1.74it/s]Extractor Predicting: 69it [00:41,  1.77it/s]Extractor Predicting: 70it [00:41,  1.73it/s]Extractor Predicting: 71it [00:42,  1.77it/s]Extractor Predicting: 72it [00:43,  1.73it/s]Extractor Predicting: 73it [00:43,  1.70it/s]Extractor Predicting: 74it [00:44,  1.66it/s]Extractor Predicting: 75it [00:44,  1.67it/s]Extractor Predicting: 76it [00:45,  1.65it/s]Extractor Predicting: 77it [00:46,  1.62it/s]Extractor Predicting: 78it [00:46,  1.65it/s]Extractor Predicting: 79it [00:47,  1.58it/s]Extractor Predicting: 80it [00:48,  1.56it/s]Extractor Predicting: 81it [00:48,  1.54it/s]Extractor Predicting: 82it [00:49,  1.53it/s]Extractor Predicting: 83it [00:50,  1.53it/s]Extractor Predicting: 84it [00:50,  1.51it/s]Extractor Predicting: 85it [00:51,  1.51it/s]Extractor Predicting: 86it [00:52,  1.53it/s]Extractor Predicting: 87it [00:52,  1.53it/s]Extractor Predicting: 88it [00:53,  1.53it/s]Extractor Predicting: 89it [00:54,  1.52it/s]Extractor Predicting: 90it [00:54,  1.53it/s]Extractor Predicting: 91it [00:55,  1.52it/s]Extractor Predicting: 92it [00:56,  1.53it/s]Extractor Predicting: 93it [00:56,  1.52it/s]Extractor Predicting: 94it [00:57,  1.49it/s]Extractor Predicting: 95it [00:58,  1.52it/s]Extractor Predicting: 96it [00:58,  1.54it/s]Extractor Predicting: 97it [00:59,  1.57it/s]Extractor Predicting: 98it [00:59,  1.58it/s]Extractor Predicting: 99it [01:00,  1.56it/s]Extractor Predicting: 100it [01:01,  1.55it/s]Extractor Predicting: 101it [01:01,  1.56it/s]Extractor Predicting: 102it [01:02,  1.52it/s]Extractor Predicting: 103it [01:03,  1.54it/s]Extractor Predicting: 104it [01:04,  1.36it/s]Extractor Predicting: 105it [01:04,  1.40it/s]Extractor Predicting: 106it [01:05,  1.43it/s]Extractor Predicting: 107it [01:06,  1.45it/s]Extractor Predicting: 108it [01:06,  1.49it/s]Extractor Predicting: 109it [01:07,  1.47it/s]Extractor Predicting: 110it [01:08,  1.52it/s]Extractor Predicting: 111it [01:08,  1.53it/s]Extractor Predicting: 112it [01:09,  1.56it/s]Extractor Predicting: 113it [01:09,  1.57it/s]Extractor Predicting: 114it [01:10,  1.56it/s]Extractor Predicting: 115it [01:11,  1.50it/s]Extractor Predicting: 116it [01:11,  1.50it/s]Extractor Predicting: 117it [01:12,  1.51it/s]Extractor Predicting: 118it [01:13,  1.51it/s]Extractor Predicting: 119it [01:13,  1.50it/s]Extractor Predicting: 120it [01:14,  1.52it/s]Extractor Predicting: 121it [01:15,  1.51it/s]Extractor Predicting: 122it [01:15,  1.49it/s]Extractor Predicting: 123it [01:16,  1.51it/s]Extractor Predicting: 124it [01:17,  1.50it/s]Extractor Predicting: 125it [01:17,  1.52it/s]Extractor Predicting: 126it [01:18,  1.51it/s]Extractor Predicting: 127it [01:19,  1.58it/s]Extractor Predicting: 128it [01:19,  1.61it/s]Extractor Predicting: 129it [01:20,  1.63it/s]Extractor Predicting: 130it [01:20,  1.60it/s]Extractor Predicting: 131it [01:21,  1.55it/s]Extractor Predicting: 132it [01:22,  1.54it/s]Extractor Predicting: 133it [01:22,  1.56it/s]Extractor Predicting: 134it [01:23,  1.57it/s]Extractor Predicting: 135it [01:24,  1.60it/s]Extractor Predicting: 136it [01:24,  1.61it/s]Extractor Predicting: 137it [01:25,  1.62it/s]Extractor Predicting: 138it [01:25,  1.64it/s]Extractor Predicting: 139it [01:26,  1.66it/s]Extractor Predicting: 140it [01:27,  1.67it/s]Extractor Predicting: 141it [01:27,  1.68it/s]Extractor Predicting: 142it [01:28,  1.63it/s]Extractor Predicting: 143it [01:29,  1.60it/s]Extractor Predicting: 144it [01:29,  1.60it/s]Extractor Predicting: 145it [01:30,  1.66it/s]Extractor Predicting: 146it [01:30,  1.69it/s]Extractor Predicting: 147it [01:31,  1.67it/s]Extractor Predicting: 148it [01:31,  1.69it/s]Extractor Predicting: 149it [01:32,  1.67it/s]Extractor Predicting: 150it [01:33,  1.69it/s]Extractor Predicting: 151it [01:33,  1.65it/s]Extractor Predicting: 152it [01:34,  1.63it/s]Extractor Predicting: 153it [01:35,  1.63it/s]Extractor Predicting: 154it [01:35,  1.59it/s]Extractor Predicting: 155it [01:36,  1.59it/s]Extractor Predicting: 156it [01:36,  1.60it/s]Extractor Predicting: 157it [01:37,  1.59it/s]Extractor Predicting: 158it [01:38,  1.58it/s]Extractor Predicting: 159it [01:38,  1.59it/s]Extractor Predicting: 160it [01:39,  1.61it/s]Extractor Predicting: 161it [01:40,  1.61it/s]Extractor Predicting: 162it [01:40,  1.65it/s]Extractor Predicting: 163it [01:41,  1.79it/s]Extractor Predicting: 164it [01:41,  1.91it/s]Extractor Predicting: 165it [01:42,  1.93it/s]Extractor Predicting: 166it [01:42,  1.85it/s]Extractor Predicting: 167it [01:43,  1.77it/s]Extractor Predicting: 168it [01:43,  1.70it/s]Extractor Predicting: 169it [01:44,  1.65it/s]Extractor Predicting: 170it [01:45,  1.66it/s]Extractor Predicting: 171it [01:45,  1.65it/s]Extractor Predicting: 172it [01:46,  1.64it/s]Extractor Predicting: 173it [01:46,  1.64it/s]Extractor Predicting: 174it [01:47,  1.65it/s]Extractor Predicting: 175it [01:48,  1.62it/s]Extractor Predicting: 176it [01:48,  1.60it/s]Extractor Predicting: 177it [01:49,  1.61it/s]Extractor Predicting: 178it [01:50,  1.64it/s]Extractor Predicting: 179it [01:50,  1.59it/s]Extractor Predicting: 180it [01:51,  1.60it/s]Extractor Predicting: 181it [01:51,  1.61it/s]Extractor Predicting: 182it [01:52,  1.63it/s]Extractor Predicting: 183it [01:53,  1.60it/s]Extractor Predicting: 184it [01:53,  1.59it/s]Extractor Predicting: 185it [01:54,  1.58it/s]Extractor Predicting: 186it [01:55,  1.59it/s]Extractor Predicting: 187it [01:55,  1.61it/s]Extractor Predicting: 188it [01:56,  1.61it/s]Extractor Predicting: 189it [01:56,  1.62it/s]Extractor Predicting: 190it [01:57,  1.61it/s]Extractor Predicting: 191it [01:58,  1.60it/s]Extractor Predicting: 192it [01:58,  1.64it/s]Extractor Predicting: 193it [01:59,  1.64it/s]Extractor Predicting: 194it [01:59,  1.66it/s]Extractor Predicting: 195it [02:00,  1.61it/s]Extractor Predicting: 196it [02:01,  1.62it/s]Extractor Predicting: 197it [02:02,  1.44it/s]Extractor Predicting: 198it [02:02,  1.48it/s]Extractor Predicting: 199it [02:03,  1.50it/s]Extractor Predicting: 200it [02:04,  1.53it/s]Extractor Predicting: 201it [02:04,  1.48it/s]Extractor Predicting: 202it [02:05,  1.52it/s]Extractor Predicting: 203it [02:06,  1.52it/s]Extractor Predicting: 204it [02:06,  1.53it/s]Extractor Predicting: 205it [02:07,  1.55it/s]Extractor Predicting: 206it [02:07,  1.54it/s]Extractor Predicting: 207it [02:08,  1.60it/s]Extractor Predicting: 208it [02:09,  1.63it/s]Extractor Predicting: 209it [02:09,  1.60it/s]Extractor Predicting: 210it [02:10,  1.61it/s]Extractor Predicting: 211it [02:11,  1.61it/s]Extractor Predicting: 212it [02:11,  1.59it/s]Extractor Predicting: 213it [02:12,  1.55it/s]Extractor Predicting: 214it [02:12,  1.57it/s]Extractor Predicting: 215it [02:13,  1.59it/s]Extractor Predicting: 216it [02:14,  1.59it/s]Extractor Predicting: 217it [02:14,  1.58it/s]Extractor Predicting: 218it [02:15,  1.57it/s]Extractor Predicting: 219it [02:16,  1.38it/s]Extractor Predicting: 220it [02:17,  1.44it/s]Extractor Predicting: 221it [02:17,  1.51it/s]Extractor Predicting: 222it [02:18,  1.53it/s]Extractor Predicting: 223it [02:18,  1.57it/s]Extractor Predicting: 224it [02:19,  1.58it/s]Extractor Predicting: 225it [02:20,  1.64it/s]Extractor Predicting: 226it [02:20,  1.63it/s]Extractor Predicting: 227it [02:21,  1.62it/s]Extractor Predicting: 228it [02:21,  1.63it/s]Extractor Predicting: 229it [02:22,  1.50it/s]Extractor Predicting: 230it [02:23,  1.58it/s]Extractor Predicting: 231it [02:23,  1.60it/s]Extractor Predicting: 232it [02:24,  1.59it/s]Extractor Predicting: 233it [02:25,  1.58it/s]Extractor Predicting: 234it [02:25,  1.50it/s]Extractor Predicting: 235it [02:26,  1.53it/s]Extractor Predicting: 236it [02:27,  1.51it/s]Extractor Predicting: 237it [02:27,  1.47it/s]Extractor Predicting: 238it [02:28,  1.47it/s]Extractor Predicting: 239it [02:29,  1.43it/s]Extractor Predicting: 240it [02:29,  1.45it/s]Extractor Predicting: 241it [02:30,  1.50it/s]Extractor Predicting: 242it [02:31,  1.50it/s]Extractor Predicting: 243it [02:31,  1.55it/s]Extractor Predicting: 244it [02:32,  1.51it/s]Extractor Predicting: 245it [02:33,  1.56it/s]Extractor Predicting: 246it [02:33,  1.60it/s]Extractor Predicting: 247it [02:34,  1.63it/s]Extractor Predicting: 248it [02:34,  1.66it/s]Extractor Predicting: 249it [02:35,  1.67it/s]Extractor Predicting: 250it [02:36,  1.63it/s]Extractor Predicting: 251it [02:36,  1.67it/s]Extractor Predicting: 252it [02:37,  1.65it/s]Extractor Predicting: 253it [02:37,  1.64it/s]Extractor Predicting: 254it [02:38,  1.63it/s]Extractor Predicting: 255it [02:39,  1.65it/s]Extractor Predicting: 256it [02:39,  1.64it/s]Extractor Predicting: 257it [02:40,  1.66it/s]Extractor Predicting: 258it [02:40,  1.67it/s]Extractor Predicting: 259it [02:41,  1.64it/s]Extractor Predicting: 260it [02:42,  1.64it/s]Extractor Predicting: 261it [02:42,  1.63it/s]Extractor Predicting: 262it [02:43,  1.63it/s]Extractor Predicting: 263it [02:44,  1.60it/s]Extractor Predicting: 264it [02:44,  1.62it/s]Extractor Predicting: 265it [02:45,  1.63it/s]Extractor Predicting: 266it [02:45,  1.62it/s]Extractor Predicting: 267it [02:46,  1.62it/s]Extractor Predicting: 268it [02:47,  1.59it/s]Extractor Predicting: 269it [02:47,  1.57it/s]Extractor Predicting: 270it [02:48,  1.61it/s]Extractor Predicting: 271it [02:49,  1.63it/s]Extractor Predicting: 272it [02:49,  1.69it/s]Extractor Predicting: 273it [02:50,  1.67it/s]Extractor Predicting: 274it [02:50,  1.68it/s]Extractor Predicting: 275it [02:51,  1.64it/s]Extractor Predicting: 276it [02:52,  1.64it/s]Extractor Predicting: 277it [02:52,  1.66it/s]Extractor Predicting: 278it [02:53,  1.61it/s]Extractor Predicting: 279it [02:53,  1.64it/s]Extractor Predicting: 280it [02:54,  1.70it/s]Extractor Predicting: 281it [02:54,  1.69it/s]Extractor Predicting: 282it [02:55,  1.66it/s]Extractor Predicting: 283it [02:56,  1.61it/s]Extractor Predicting: 284it [02:56,  1.61it/s]Extractor Predicting: 285it [02:57,  1.58it/s]Extractor Predicting: 286it [02:58,  1.60it/s]Extractor Predicting: 287it [02:58,  1.58it/s]Extractor Predicting: 288it [02:59,  1.57it/s]Extractor Predicting: 289it [03:00,  1.56it/s]Extractor Predicting: 290it [03:00,  1.56it/s]Extractor Predicting: 291it [03:01,  1.60it/s]Extractor Predicting: 292it [03:01,  1.62it/s]Extractor Predicting: 293it [03:02,  1.59it/s]Extractor Predicting: 294it [03:03,  1.55it/s]Extractor Predicting: 295it [03:03,  1.56it/s]Extractor Predicting: 296it [03:04,  1.55it/s]Extractor Predicting: 297it [03:05,  1.52it/s]Extractor Predicting: 298it [03:05,  1.48it/s]Extractor Predicting: 299it [03:06,  1.47it/s]Extractor Predicting: 300it [03:07,  1.49it/s]Extractor Predicting: 301it [03:08,  1.46it/s]Extractor Predicting: 302it [03:08,  1.47it/s]Extractor Predicting: 303it [03:09,  1.31it/s]Extractor Predicting: 303it [03:09,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:24:26,197 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:24:26,302 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:24:26,302 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:24:26,302 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:24:26,302 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 15:24:27,388 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 15:24:27,389 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 15:24:28,070 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 15:24:29,294 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 15:24:29,360 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:24:32,627 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:24:32,704 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:24:32,704 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:24:32,704 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:24:32,704 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 15:24:33,600 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 15:24:33,601 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 15:24:34,238 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 15:24:34,510 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 15:24:34,510 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2052532833020638,
  "recall": 0.0638422035480859,
  "score": 0.09739161399448053,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'labels': ['continent', 'field of this occupation', 'field of work', 'founded by', 'movement', 'owned by', 'performer', 'producer', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20815
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20915, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.70it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:02,  1.68it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:04,  1.61it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:06,  1.64it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.64it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.64it/s]Extractor Predicting: 17it [00:10,  1.65it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:11,  1.61it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:12,  1.58it/s]Extractor Predicting: 22it [00:13,  1.57it/s]Extractor Predicting: 23it [00:14,  1.59it/s]Extractor Predicting: 24it [00:14,  1.59it/s]Extractor Predicting: 25it [00:15,  1.59it/s]Extractor Predicting: 26it [00:16,  1.61it/s]Extractor Predicting: 27it [00:16,  1.60it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:17,  1.59it/s]Extractor Predicting: 30it [00:18,  1.61it/s]Extractor Predicting: 31it [00:19,  1.62it/s]Extractor Predicting: 32it [00:19,  1.61it/s]Extractor Predicting: 33it [00:20,  1.62it/s]Extractor Predicting: 34it [00:21,  1.48it/s]Extractor Predicting: 35it [00:21,  1.47it/s]Extractor Predicting: 36it [00:22,  1.54it/s]Extractor Predicting: 37it [00:23,  1.56it/s]Extractor Predicting: 38it [00:23,  1.58it/s]Extractor Predicting: 39it [00:24,  1.62it/s]Extractor Predicting: 40it [00:24,  1.60it/s]Extractor Predicting: 41it [00:25,  1.61it/s]Extractor Predicting: 42it [00:26,  1.63it/s]Extractor Predicting: 43it [00:26,  1.61it/s]Extractor Predicting: 44it [00:27,  1.57it/s]Extractor Predicting: 45it [00:28,  1.54it/s]Extractor Predicting: 46it [00:28,  1.53it/s]Extractor Predicting: 47it [00:29,  1.55it/s]Extractor Predicting: 48it [00:30,  1.57it/s]Extractor Predicting: 49it [00:30,  1.58it/s]Extractor Predicting: 50it [00:31,  1.60it/s]Extractor Predicting: 51it [00:31,  1.61it/s]Extractor Predicting: 52it [00:32,  1.63it/s]Extractor Predicting: 53it [00:33,  1.63it/s]Extractor Predicting: 54it [00:33,  1.60it/s]Extractor Predicting: 55it [00:34,  1.61it/s]Extractor Predicting: 56it [00:34,  1.61it/s]Extractor Predicting: 57it [00:35,  1.63it/s]Extractor Predicting: 58it [00:36,  1.60it/s]Extractor Predicting: 59it [00:36,  1.61it/s]Extractor Predicting: 60it [00:37,  1.61it/s]Extractor Predicting: 61it [00:38,  1.61it/s]Extractor Predicting: 62it [00:38,  1.62it/s]Extractor Predicting: 63it [00:39,  1.64it/s]Extractor Predicting: 64it [00:39,  1.61it/s]Extractor Predicting: 65it [00:40,  1.53it/s]Extractor Predicting: 66it [00:41,  1.57it/s]Extractor Predicting: 67it [00:41,  1.62it/s]Extractor Predicting: 68it [00:42,  1.60it/s]Extractor Predicting: 69it [00:43,  1.55it/s]Extractor Predicting: 70it [00:43,  1.58it/s]Extractor Predicting: 71it [00:44,  1.60it/s]Extractor Predicting: 72it [00:44,  1.62it/s]Extractor Predicting: 73it [00:45,  1.64it/s]Extractor Predicting: 74it [00:46,  1.59it/s]Extractor Predicting: 75it [00:46,  1.59it/s]Extractor Predicting: 76it [00:47,  1.63it/s]Extractor Predicting: 77it [00:48,  1.63it/s]Extractor Predicting: 78it [00:48,  1.63it/s]Extractor Predicting: 79it [00:49,  1.60it/s]Extractor Predicting: 80it [00:49,  1.62it/s]Extractor Predicting: 81it [00:50,  1.63it/s]Extractor Predicting: 82it [00:51,  1.66it/s]Extractor Predicting: 83it [00:51,  1.65it/s]Extractor Predicting: 84it [00:52,  1.60it/s]Extractor Predicting: 85it [00:53,  1.61it/s]Extractor Predicting: 86it [00:53,  1.64it/s]Extractor Predicting: 87it [00:54,  1.61it/s]Extractor Predicting: 88it [00:54,  1.60it/s]Extractor Predicting: 89it [00:55,  1.58it/s]Extractor Predicting: 90it [00:56,  1.59it/s]Extractor Predicting: 91it [00:56,  1.61it/s]Extractor Predicting: 92it [00:57,  1.63it/s]Extractor Predicting: 93it [00:57,  1.62it/s]Extractor Predicting: 94it [00:58,  1.60it/s]Extractor Predicting: 95it [00:59,  1.63it/s]Extractor Predicting: 96it [00:59,  1.63it/s]Extractor Predicting: 97it [01:00,  1.60it/s]Extractor Predicting: 98it [01:01,  1.64it/s]Extractor Predicting: 99it [01:01,  1.62it/s]Extractor Predicting: 100it [01:02,  1.59it/s]Extractor Predicting: 101it [01:02,  1.62it/s]Extractor Predicting: 102it [01:03,  1.62it/s]Extractor Predicting: 103it [01:04,  1.62it/s]Extractor Predicting: 104it [01:04,  1.59it/s]Extractor Predicting: 105it [01:05,  1.59it/s]Extractor Predicting: 106it [01:06,  1.60it/s]Extractor Predicting: 107it [01:06,  1.63it/s]Extractor Predicting: 108it [01:07,  1.60it/s]Extractor Predicting: 109it [01:07,  1.60it/s]Extractor Predicting: 110it [01:08,  1.62it/s]Extractor Predicting: 111it [01:09,  1.66it/s]Extractor Predicting: 112it [01:09,  1.67it/s]Extractor Predicting: 113it [01:10,  1.68it/s]Extractor Predicting: 114it [01:10,  1.62it/s]Extractor Predicting: 115it [01:11,  1.63it/s]Extractor Predicting: 116it [01:12,  1.65it/s]Extractor Predicting: 117it [01:12,  1.63it/s]Extractor Predicting: 118it [01:13,  1.63it/s]Extractor Predicting: 119it [01:14,  1.61it/s]Extractor Predicting: 120it [01:14,  1.61it/s]Extractor Predicting: 121it [01:15,  1.63it/s]Extractor Predicting: 122it [01:15,  1.62it/s]Extractor Predicting: 123it [01:16,  1.65it/s]Extractor Predicting: 124it [01:17,  1.62it/s]Extractor Predicting: 125it [01:17,  1.65it/s]Extractor Predicting: 126it [01:18,  1.46it/s]Extractor Predicting: 127it [01:19,  1.50it/s]Extractor Predicting: 128it [01:19,  1.54it/s]Extractor Predicting: 129it [01:20,  1.60it/s]Extractor Predicting: 130it [01:20,  1.65it/s]Extractor Predicting: 131it [01:21,  1.64it/s]Extractor Predicting: 132it [01:22,  1.61it/s]Extractor Predicting: 133it [01:22,  1.56it/s]Extractor Predicting: 134it [01:23,  1.53it/s]Extractor Predicting: 135it [01:24,  1.56it/s]Extractor Predicting: 136it [01:24,  1.56it/s]Extractor Predicting: 137it [01:25,  1.57it/s]Extractor Predicting: 138it [01:26,  1.56it/s]Extractor Predicting: 139it [01:26,  1.53it/s]Extractor Predicting: 140it [01:27,  1.55it/s]Extractor Predicting: 141it [01:28,  1.54it/s]Extractor Predicting: 142it [01:28,  1.54it/s]Extractor Predicting: 143it [01:29,  1.54it/s]Extractor Predicting: 144it [01:29,  1.54it/s]Extractor Predicting: 145it [01:30,  1.55it/s]Extractor Predicting: 146it [01:31,  1.56it/s]Extractor Predicting: 147it [01:31,  1.54it/s]Extractor Predicting: 148it [01:32,  1.59it/s]Extractor Predicting: 149it [01:33,  1.55it/s]Extractor Predicting: 150it [01:33,  1.57it/s]Extractor Predicting: 151it [01:34,  1.57it/s]Extractor Predicting: 152it [01:34,  1.62it/s]Extractor Predicting: 153it [01:35,  1.64it/s]Extractor Predicting: 154it [01:36,  1.62it/s]Extractor Predicting: 155it [01:36,  1.62it/s]Extractor Predicting: 156it [01:37,  1.62it/s]Extractor Predicting: 157it [01:38,  1.64it/s]Extractor Predicting: 158it [01:38,  1.62it/s]Extractor Predicting: 159it [01:39,  1.61it/s]Extractor Predicting: 160it [01:39,  1.63it/s]Extractor Predicting: 161it [01:40,  1.62it/s]Extractor Predicting: 162it [01:41,  1.60it/s]Extractor Predicting: 163it [01:41,  1.62it/s]Extractor Predicting: 164it [01:42,  1.61it/s]Extractor Predicting: 165it [01:42,  1.65it/s]Extractor Predicting: 166it [01:43,  1.67it/s]Extractor Predicting: 167it [01:44,  1.59it/s]Extractor Predicting: 168it [01:44,  1.61it/s]Extractor Predicting: 169it [01:45,  1.63it/s]Extractor Predicting: 170it [01:46,  1.61it/s]Extractor Predicting: 171it [01:46,  1.59it/s]Extractor Predicting: 172it [01:47,  1.56it/s]Extractor Predicting: 173it [01:48,  1.58it/s]Extractor Predicting: 174it [01:48,  1.62it/s]Extractor Predicting: 175it [01:49,  1.65it/s]Extractor Predicting: 176it [01:49,  1.61it/s]Extractor Predicting: 177it [01:50,  1.57it/s]Extractor Predicting: 178it [01:51,  1.58it/s]Extractor Predicting: 179it [01:51,  1.57it/s]Extractor Predicting: 180it [01:52,  1.64it/s]Extractor Predicting: 181it [01:52,  1.66it/s]Extractor Predicting: 182it [01:53,  1.60it/s]Extractor Predicting: 183it [01:54,  1.64it/s]Extractor Predicting: 184it [01:54,  1.67it/s]Extractor Predicting: 185it [01:55,  1.66it/s]Extractor Predicting: 186it [01:55,  1.68it/s]Extractor Predicting: 187it [01:56,  1.66it/s]Extractor Predicting: 188it [01:57,  1.64it/s]Extractor Predicting: 189it [01:57,  1.60it/s]Extractor Predicting: 190it [01:58,  1.59it/s]Extractor Predicting: 191it [01:59,  1.60it/s]Extractor Predicting: 192it [01:59,  1.61it/s]Extractor Predicting: 193it [02:00,  1.60it/s]Extractor Predicting: 194it [02:00,  1.60it/s]Extractor Predicting: 195it [02:01,  1.62it/s]Extractor Predicting: 196it [02:02,  1.67it/s]Extractor Predicting: 197it [02:02,  1.64it/s]Extractor Predicting: 198it [02:03,  1.66it/s]Extractor Predicting: 199it [02:03,  1.66it/s]Extractor Predicting: 200it [02:04,  1.63it/s]Extractor Predicting: 201it [02:05,  1.64it/s]Extractor Predicting: 202it [02:05,  1.61it/s]Extractor Predicting: 203it [02:06,  1.63it/s]Extractor Predicting: 204it [02:07,  1.65it/s]Extractor Predicting: 205it [02:07,  1.68it/s]Extractor Predicting: 206it [02:08,  1.67it/s]Extractor Predicting: 207it [02:08,  1.63it/s]Extractor Predicting: 208it [02:09,  1.68it/s]Extractor Predicting: 209it [02:09,  1.68it/s]Extractor Predicting: 210it [02:10,  1.66it/s]Extractor Predicting: 211it [02:11,  1.67it/s]Extractor Predicting: 212it [02:11,  1.66it/s]Extractor Predicting: 213it [02:12,  1.62it/s]Extractor Predicting: 214it [02:13,  1.63it/s]Extractor Predicting: 215it [02:13,  1.63it/s]Extractor Predicting: 216it [02:14,  1.64it/s]Extractor Predicting: 217it [02:14,  1.61it/s]Extractor Predicting: 218it [02:15,  1.64it/s]Extractor Predicting: 219it [02:16,  1.66it/s]Extractor Predicting: 220it [02:16,  1.65it/s]Extractor Predicting: 221it [02:17,  1.63it/s]Extractor Predicting: 222it [02:17,  1.65it/s]Extractor Predicting: 223it [02:18,  1.67it/s]Extractor Predicting: 224it [02:19,  1.61it/s]Extractor Predicting: 225it [02:19,  1.66it/s]Extractor Predicting: 226it [02:20,  1.67it/s]Extractor Predicting: 227it [02:20,  1.68it/s]Extractor Predicting: 228it [02:21,  1.66it/s]Extractor Predicting: 229it [02:22,  1.65it/s]Extractor Predicting: 230it [02:22,  1.63it/s]Extractor Predicting: 231it [02:23,  1.66it/s]Extractor Predicting: 232it [02:23,  1.65it/s]Extractor Predicting: 233it [02:24,  1.65it/s]Extractor Predicting: 234it [02:25,  1.61it/s]Extractor Predicting: 235it [02:25,  1.64it/s]Extractor Predicting: 236it [02:26,  1.62it/s]Extractor Predicting: 237it [02:27,  1.65it/s]Extractor Predicting: 238it [02:27,  1.63it/s]Extractor Predicting: 239it [02:28,  1.64it/s]Extractor Predicting: 240it [02:28,  1.63it/s]Extractor Predicting: 241it [02:29,  1.62it/s]Extractor Predicting: 242it [02:30,  1.47it/s]Extractor Predicting: 243it [02:30,  1.55it/s]Extractor Predicting: 244it [02:31,  1.58it/s]Extractor Predicting: 245it [02:32,  1.63it/s]Extractor Predicting: 246it [02:32,  1.70it/s]Extractor Predicting: 247it [02:33,  1.69it/s]Extractor Predicting: 248it [02:33,  1.71it/s]Extractor Predicting: 249it [02:34,  1.71it/s]Extractor Predicting: 250it [02:34,  1.69it/s]Extractor Predicting: 251it [02:35,  1.70it/s]Extractor Predicting: 252it [02:36,  1.72it/s]Extractor Predicting: 253it [02:36,  1.72it/s]Extractor Predicting: 254it [02:37,  1.71it/s]Extractor Predicting: 255it [02:37,  1.74it/s]Extractor Predicting: 256it [02:38,  1.69it/s]Extractor Predicting: 257it [02:39,  1.70it/s]Extractor Predicting: 258it [02:39,  1.70it/s]Extractor Predicting: 259it [02:40,  1.69it/s]Extractor Predicting: 260it [02:40,  1.70it/s]Extractor Predicting: 261it [02:41,  1.71it/s]Extractor Predicting: 262it [02:41,  1.73it/s]Extractor Predicting: 263it [02:42,  1.73it/s]Extractor Predicting: 264it [02:43,  1.67it/s]Extractor Predicting: 265it [02:43,  1.63it/s]Extractor Predicting: 266it [02:44,  1.63it/s]Extractor Predicting: 267it [02:45,  1.62it/s]Extractor Predicting: 268it [02:45,  1.57it/s]Extractor Predicting: 269it [02:46,  1.62it/s]Extractor Predicting: 270it [02:46,  1.64it/s]Extractor Predicting: 271it [02:47,  1.65it/s]Extractor Predicting: 272it [02:48,  1.66it/s]Extractor Predicting: 273it [02:48,  1.60it/s]Extractor Predicting: 274it [02:49,  1.63it/s]Extractor Predicting: 275it [02:49,  1.68it/s]Extractor Predicting: 276it [02:50,  1.66it/s]Extractor Predicting: 277it [02:51,  1.61it/s]Extractor Predicting: 278it [02:51,  1.61it/s]Extractor Predicting: 279it [02:52,  1.67it/s]Extractor Predicting: 280it [02:52,  1.70it/s]Extractor Predicting: 281it [02:53,  1.73it/s]Extractor Predicting: 282it [02:54,  1.73it/s]Extractor Predicting: 283it [02:54,  1.75it/s]Extractor Predicting: 284it [02:55,  1.62it/s]Extractor Predicting: 285it [02:55,  1.62it/s]Extractor Predicting: 286it [02:56,  1.69it/s]Extractor Predicting: 287it [02:57,  1.69it/s]Extractor Predicting: 288it [02:57,  1.69it/s]Extractor Predicting: 289it [02:58,  1.66it/s]Extractor Predicting: 290it [02:58,  1.70it/s]Extractor Predicting: 291it [02:59,  1.71it/s]Extractor Predicting: 292it [03:00,  1.67it/s]Extractor Predicting: 293it [03:00,  1.67it/s]Extractor Predicting: 294it [03:01,  1.67it/s]Extractor Predicting: 295it [03:01,  1.65it/s]Extractor Predicting: 296it [03:02,  1.67it/s]Extractor Predicting: 297it [03:03,  1.66it/s]Extractor Predicting: 298it [03:03,  1.71it/s]Extractor Predicting: 299it [03:04,  1.66it/s]Extractor Predicting: 300it [03:04,  1.59it/s]Extractor Predicting: 301it [03:05,  1.54it/s]Extractor Predicting: 302it [03:06,  1.57it/s]Extractor Predicting: 303it [03:06,  1.56it/s]Extractor Predicting: 304it [03:07,  1.57it/s]Extractor Predicting: 305it [03:08,  1.54it/s]Extractor Predicting: 306it [03:08,  1.54it/s]Extractor Predicting: 306it [03:08,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:27:57,784 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:27:57,822 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:27:57,822 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:27:57,822 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:27:57,822 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 15:27:58,580 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 15:27:58,582 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 15:27:58,930 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 15:28:00,095 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 15:28:00,095 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:28:01,616 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:28:01,659 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:28:01,660 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:28:01,660 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:28:01,660 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 15:28:02,181 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 15:28:02,183 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 15:28:02,509 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 15:28:02,713 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 15:28:02,713 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.27145612343297976,
  "recall": 0.07666122004357298,
  "score": 0.11955829263113185,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'labels': ['continent', 'field of this occupation', 'field of work', 'founded by', 'movement', 'owned by', 'performer', 'producer', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6322
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6422, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.65it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.60it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:08,  1.55it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:10,  1.51it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.56it/s]Extractor Predicting: 20it [00:12,  1.55it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:14,  1.59it/s]Extractor Predicting: 24it [00:15,  1.62it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:16,  1.63it/s]Extractor Predicting: 27it [00:17,  1.63it/s]Extractor Predicting: 28it [00:17,  1.64it/s]Extractor Predicting: 29it [00:18,  1.61it/s]Extractor Predicting: 30it [00:18,  1.64it/s]Extractor Predicting: 31it [00:19,  1.60it/s]Extractor Predicting: 32it [00:20,  1.62it/s]Extractor Predicting: 33it [00:20,  1.62it/s]Extractor Predicting: 34it [00:21,  1.64it/s]Extractor Predicting: 35it [00:22,  1.65it/s]Extractor Predicting: 36it [00:22,  1.64it/s]Extractor Predicting: 37it [00:23,  1.65it/s]Extractor Predicting: 38it [00:23,  1.68it/s]Extractor Predicting: 39it [00:24,  1.66it/s]Extractor Predicting: 40it [00:25,  1.65it/s]Extractor Predicting: 41it [00:25,  1.61it/s]Extractor Predicting: 42it [00:26,  1.59it/s]Extractor Predicting: 43it [00:27,  1.57it/s]Extractor Predicting: 44it [00:27,  1.58it/s]Extractor Predicting: 45it [00:28,  1.60it/s]Extractor Predicting: 46it [00:29,  1.51it/s]Extractor Predicting: 47it [00:29,  1.54it/s]Extractor Predicting: 48it [00:30,  1.47it/s]Extractor Predicting: 49it [00:30,  1.52it/s]Extractor Predicting: 50it [00:31,  1.54it/s]Extractor Predicting: 51it [00:32,  1.54it/s]Extractor Predicting: 52it [00:32,  1.54it/s]Extractor Predicting: 53it [00:33,  1.57it/s]Extractor Predicting: 54it [00:34,  1.58it/s]Extractor Predicting: 55it [00:34,  1.60it/s]Extractor Predicting: 56it [00:35,  1.57it/s]Extractor Predicting: 57it [00:36,  1.54it/s]Extractor Predicting: 58it [00:36,  1.55it/s]Extractor Predicting: 59it [00:37,  1.54it/s]Extractor Predicting: 60it [00:37,  1.89it/s]Extractor Predicting: 60it [00:37,  1.59it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4831858407079646,
  "recall": 0.08507323153630414,
  "score": 0.14467408585055644,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/', 'labels': ['continent', 'field of this occupation', 'field of work', 'founded by', 'movement', 'owned by', 'performer', 'producer', 'record label', 'replaces'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_3/extractor/iter1/results_single_is_eval_True_limit5000.json'
