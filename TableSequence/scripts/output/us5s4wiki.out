Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_5_seed_4', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', data_dir='outputs/wrapper/wiki/unseen_5_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:19<02:55, 19.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:36<02:23, 17.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:04<02:37, 22.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:21<02:02, 20.38s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:38<01:35, 19.09s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:53<01:11, 17.89s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:09<00:51, 17.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:22<00:31, 15.97s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:37<00:15, 15.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:52<00:00, 15.52s/it]Generating: 100%|██████████| 10/10 [02:52<00:00, 17.29s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : given name .', 'success_rate': 0.80859375, 'errors': {'', 'too many values to unpack (expected 2)', "('William', 'given name', '', 'He was born in the province of Oise , his father being William , Duke of Normandy , 1st Earl of Normandy , and his mother being Mary .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 201, 'raw': 288}
{'target': 600, 'success': 226, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 287, 'raw': 416}
{'target': 600, 'success': 310, 'raw': 448}
{'target': 600, 'success': 333, 'raw': 480}
{'target': 600, 'success': 351, 'raw': 512}
{'target': 600, 'success': 377, 'raw': 544}
{'target': 600, 'success': 403, 'raw': 576}
{'target': 600, 'success': 424, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 468, 'raw': 672}
{'target': 600, 'success': 486, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 532, 'raw': 768}
{'target': 600, 'success': 554, 'raw': 800}
{'target': 600, 'success': 576, 'raw': 832}
{'target': 600, 'success': 601, 'raw': 864}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.6956018518518519, 'errors': {'', "('Estonia', 'languages spoken, written or signed', '', 'The language of Estonia is Estonian , the official language of the Republic of the Union ( DSO ) ) .')", "('Brazil', 'languages spoken, written or signed', '', 'Many languages are spoken in Brazil , and there are various communities and organizations advocating for a stronger free and open society .')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : lowest point . Context : Later in the year ( 1143 ) , Puyi and his allies made a conquest over the eastern part of the province of Sinti , which was occupied by the Franks and Normans . Head Entity : Sinti , Tail Entity : southern part .\n']
['Relation : lowest point . Context : Later in the year ( 1143 ) , Puyi and his allies made a conquest over the eastern part of the province of Sinti , which was occupied by the Franks and Normans . Head Entity : Sinti , Tail Entity : southern part .\n', 'Relation : lowest point . Context : After the death of Emperor Xuanzong his elder son Liu Bang was the first emperor to die at the age of only two . Head Entity : emperor , Tail Entity : highest point .\n']
{'target': 600, 'success': 12, 'raw': 32}
{'target': 600, 'success': 23, 'raw': 64}
{'target': 600, 'success': 32, 'raw': 96}
{'target': 600, 'success': 44, 'raw': 128}
{'target': 600, 'success': 55, 'raw': 160}
{'target': 600, 'success': 70, 'raw': 192}
{'target': 600, 'success': 86, 'raw': 224}
{'target': 600, 'success': 102, 'raw': 256}
{'target': 600, 'success': 115, 'raw': 288}
{'target': 600, 'success': 125, 'raw': 320}
{'target': 600, 'success': 141, 'raw': 352}
{'target': 600, 'success': 158, 'raw': 384}
{'target': 600, 'success': 176, 'raw': 416}
{'target': 600, 'success': 192, 'raw': 448}
{'target': 600, 'success': 206, 'raw': 480}
{'target': 600, 'success': 221, 'raw': 512}
{'target': 600, 'success': 233, 'raw': 544}
{'target': 600, 'success': 249, 'raw': 576}
{'target': 600, 'success': 265, 'raw': 608}
{'target': 600, 'success': 273, 'raw': 640}
{'target': 600, 'success': 288, 'raw': 672}
{'target': 600, 'success': 308, 'raw': 704}
{'target': 600, 'success': 317, 'raw': 736}
{'target': 600, 'success': 332, 'raw': 768}
{'target': 600, 'success': 342, 'raw': 800}
{'target': 600, 'success': 353, 'raw': 832}
{'target': 600, 'success': 369, 'raw': 864}
{'target': 600, 'success': 385, 'raw': 896}
{'target': 600, 'success': 394, 'raw': 928}
{'target': 600, 'success': 409, 'raw': 960}
{'target': 600, 'success': 423, 'raw': 992}
{'target': 600, 'success': 432, 'raw': 1024}
{'target': 600, 'success': 446, 'raw': 1056}
{'target': 600, 'success': 460, 'raw': 1088}
{'target': 600, 'success': 473, 'raw': 1120}
{'target': 600, 'success': 486, 'raw': 1152}
{'target': 600, 'success': 498, 'raw': 1184}
{'target': 600, 'success': 512, 'raw': 1216}
{'target': 600, 'success': 522, 'raw': 1248}
{'target': 600, 'success': 533, 'raw': 1280}
{'target': 600, 'success': 550, 'raw': 1312}
{'target': 600, 'success': 560, 'raw': 1344}
{'target': 600, 'success': 572, 'raw': 1376}
{'target': 600, 'success': 584, 'raw': 1408}
{'target': 600, 'success': 600, 'raw': 1440}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.4166666666666667, 'errors': {'', "('2004 FIFA World Cup', 'lowest point', '', 'The next day , he made a triumphant recovery and made his international debut at the 2004 FIFA World Cup against Ecuador .')", "('second', 'lowest point', '', 'He is currently playing with fellow former New Zealand prop Tom Paine in the Championship side Auckland Roosters , and he moved up the rankings to second .')", 'not enough values to unpack (expected 2, got 1)', "('lowest point', 'lowest point', '', 'For comparison , in 1998 , it was the lowest point per square mile since 1960 , when it was 1 . 10 .')", "('New York Giants', 'lowest point', '', 'In the second season , a team led by George Foreman and David Southee were allowed to make another bowl game against the New York Giants .')", "('Manchester', 'lowest point', '', 'The area was home to the city of Manchester ( now Manchester City ) , founded in 1859 by the English Civil War hero Sir Walter Scott .')", 'too many values to unpack (expected 2)', "('63', 'lowest point', '', 'On the night of 9 November 2014 , the Southeastern Conference led by Jaylen Brown was led by Duke Blue Devils in a 63 66 victory over the Southeast Division rival St. Charles .')"}}
['Relation : mother . Context : Later in Life , he came to love the beauty and natural beauty of the forests at the end of the third trimester , when she suffered death . Head Entity : forests at the end of the third trimester , Tail Entity : Mother .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : mother .', 'success_rate': 0.796875, 'errors': {'', "('king', 'mother', '', 'The first male and last female monarch of the Kingdom of Mexico , the king of the Andes ( d.')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mouth of the watercourse . Context : Later in the year ( 1143 ) , Puyi and his allies made atone for the fall of the last King of the Puebla , Puzco . Head Entity : Puyi , Tail Entity : Puducese .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 200, 'raw': 288}
{'target': 600, 'success': 221, 'raw': 320}
{'target': 600, 'success': 241, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 289, 'raw': 416}
{'target': 600, 'success': 311, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 357, 'raw': 512}
{'target': 600, 'success': 383, 'raw': 544}
{'target': 600, 'success': 404, 'raw': 576}
{'target': 600, 'success': 425, 'raw': 608}
{'target': 600, 'success': 440, 'raw': 640}
{'target': 600, 'success': 463, 'raw': 672}
{'target': 600, 'success': 488, 'raw': 704}
{'target': 600, 'success': 509, 'raw': 736}
{'target': 600, 'success': 536, 'raw': 768}
{'target': 600, 'success': 558, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 606, 'raw': 864}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.7013888888888888, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n']
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n', 'Relation : genre . Context : Eileen McClelland ( born June 24 , 1953 ) is an American radio talk show host who has appeared as a regular on both the Howard Stern Show and the Morning Mix . Head Entity : Howard Stern Show , Tail Entity : television talk show .\n']
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n', 'Relation : genre . Context : Eileen McClelland ( born June 24 , 1953 ) is an American radio talk show host who has appeared as a regular on both the Howard Stern Show and the Morning Mix . Head Entity : Howard Stern Show , Tail Entity : television talk show .\n', 'Relation : genre . Context : This film explores the social , social structure of the New York city of Times Square , which is populated and industrialized by wealthy Manhattanites . Head Entity : Times Square , Tail Entity : New York City .\n']
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 320, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 468, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 570, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : genre .', 'success_rate': 0.76625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.8020833333333334, 'errors': {'', "('programming languages', 'is a list of', '', 'It can be used primarily in conjunction with other programming languages such as .')", "('The albums', 'is a list of', '', 'The albums songs , songs played and discography of the albums members , the artist , are listed in alphabetical order .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8111979166666666, 'errors': {'', 'too many values to unpack (expected 2)', "('Its orbit', 'located on astronomical body', '', 'Its orbit shows an eccentricity of 0 . 18 and an inclination of 8 degrees from the plane of the ecliptic .')", "('Pluto', 'located on astronomical body', '', 'In the past , Pluto has been considered to be a binary system inhabited by one giant being , as well as being the first to be named after Pluto .')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8806818181818182, 'errors': {'', "('it', 'manufacturer', '', 'It is known in Japan for its high quality and performance components , along with its long range of performance airsoft s.')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8179347826086957, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/0_ext.jsonl'}}
estimate vocab size: 12731
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12831, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_5_seed_4/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:14, 14.18s/it]Extractor Estimating: 2it [00:18,  8.14s/it]Extractor Estimating: 3it [00:18,  4.74s/it]Extractor Estimating: 4it [00:19,  3.12s/it]Extractor Estimating: 5it [00:20,  2.22s/it]Extractor Estimating: 6it [00:20,  1.67s/it]Extractor Estimating: 7it [00:21,  1.32s/it]Extractor Estimating: 8it [00:21,  1.10s/it]Extractor Estimating: 9it [00:22,  1.07it/s]Extractor Estimating: 10it [00:23,  1.20it/s]Extractor Estimating: 11it [00:23,  1.33it/s]Extractor Estimating: 12it [00:24,  1.44it/s]Extractor Estimating: 13it [00:24,  1.49it/s]Extractor Estimating: 14it [00:25,  1.51it/s]Extractor Estimating: 15it [00:26,  1.55it/s]Extractor Estimating: 16it [00:26,  1.56it/s]Extractor Estimating: 17it [00:27,  1.25it/s]Extractor Estimating: 18it [00:28,  1.34it/s]Extractor Estimating: 19it [00:29,  1.40it/s]Extractor Estimating: 20it [00:29,  1.48it/s]Extractor Estimating: 21it [00:30,  1.53it/s]Extractor Estimating: 22it [00:30,  1.57it/s]Extractor Estimating: 23it [00:31,  1.51it/s]Extractor Estimating: 24it [00:32,  1.51it/s]Extractor Estimating: 25it [00:32,  1.62it/s]Extractor Estimating: 26it [00:33,  1.69it/s]Extractor Estimating: 27it [00:33,  1.71it/s]Extractor Estimating: 28it [00:34,  1.73it/s]Extractor Estimating: 29it [00:34,  1.77it/s]Extractor Estimating: 30it [00:35,  1.78it/s]Extractor Estimating: 31it [00:36,  1.76it/s]Extractor Estimating: 32it [00:36,  1.77it/s]Extractor Estimating: 33it [00:37,  1.80it/s]Extractor Estimating: 34it [00:37,  1.82it/s]Extractor Estimating: 35it [00:38,  1.76it/s]Extractor Estimating: 36it [00:38,  1.76it/s]Extractor Estimating: 37it [00:39,  1.74it/s]Extractor Estimating: 38it [00:40,  1.76it/s]Extractor Estimating: 39it [00:40,  1.81it/s]Extractor Estimating: 40it [00:41,  1.76it/s]Extractor Estimating: 41it [00:41,  1.74it/s]Extractor Estimating: 42it [00:42,  1.76it/s]Extractor Estimating: 43it [00:43,  1.32it/s]Extractor Estimating: 44it [00:44,  1.44it/s]Extractor Estimating: 45it [00:44,  1.55it/s]Extractor Estimating: 46it [00:45,  1.59it/s]Extractor Estimating: 47it [00:45,  1.66it/s]Extractor Estimating: 48it [00:46,  1.72it/s]Extractor Estimating: 49it [00:46,  1.77it/s]Extractor Estimating: 50it [00:47,  1.79it/s]Extractor Estimating: 51it [00:47,  1.72it/s]Extractor Estimating: 52it [00:48,  1.68it/s]Extractor Estimating: 53it [00:49,  1.67it/s]Extractor Estimating: 54it [00:49,  1.65it/s]Extractor Estimating: 55it [00:50,  1.62it/s]Extractor Estimating: 56it [00:51,  1.59it/s]Extractor Estimating: 57it [00:51,  1.56it/s]Extractor Estimating: 58it [00:52,  1.57it/s]Extractor Estimating: 59it [00:53,  1.62it/s]Extractor Estimating: 60it [00:53,  1.61it/s]Extractor Estimating: 61it [00:54,  1.58it/s]Extractor Estimating: 62it [00:54,  1.60it/s]Extractor Estimating: 63it [00:55,  1.55it/s]Extractor Estimating: 64it [00:56,  1.58it/s]Extractor Estimating: 65it [00:56,  1.61it/s]Extractor Estimating: 66it [00:57,  1.58it/s]Extractor Estimating: 67it [00:58,  1.59it/s]Extractor Estimating: 68it [00:58,  1.59it/s]Extractor Estimating: 69it [00:59,  1.58it/s]Extractor Estimating: 70it [00:59,  1.59it/s]Extractor Estimating: 71it [01:00,  1.56it/s]Extractor Estimating: 72it [01:01,  1.57it/s]Extractor Estimating: 73it [01:01,  1.57it/s]Extractor Estimating: 74it [01:02,  1.59it/s]Extractor Estimating: 75it [01:03,  1.58it/s]Extractor Estimating: 76it [01:03,  1.52it/s]Extractor Estimating: 77it [01:04,  1.54it/s]Extractor Estimating: 78it [01:05,  1.53it/s]Extractor Estimating: 79it [01:05,  1.48it/s]Extractor Estimating: 80it [01:06,  1.51it/s]Extractor Estimating: 81it [01:07,  1.52it/s]Extractor Estimating: 82it [01:07,  1.56it/s]Extractor Estimating: 83it [01:08,  1.52it/s]Extractor Estimating: 84it [01:09,  1.51it/s]Extractor Estimating: 85it [01:09,  1.49it/s]Extractor Estimating: 86it [01:11,  1.17it/s]Extractor Estimating: 87it [01:11,  1.24it/s]Extractor Estimating: 88it [01:12,  1.30it/s]Extractor Estimating: 89it [01:13,  1.26it/s]Extractor Estimating: 90it [01:14,  1.32it/s]Extractor Estimating: 91it [01:14,  1.38it/s]Extractor Estimating: 92it [01:15,  1.44it/s]Extractor Estimating: 93it [01:15,  1.47it/s]Extractor Estimating: 94it [01:16,  1.49it/s]Extractor Estimating: 95it [01:17,  1.51it/s]Extractor Estimating: 96it [01:17,  1.51it/s]Extractor Estimating: 97it [01:18,  1.57it/s]Extractor Estimating: 98it [01:19,  1.50it/s]Extractor Estimating: 99it [01:19,  1.50it/s]Extractor Estimating: 100it [01:20,  1.50it/s]Extractor Estimating: 101it [01:21,  1.50it/s]Extractor Estimating: 102it [01:21,  1.55it/s]Extractor Estimating: 103it [01:22,  1.58it/s]Extractor Estimating: 104it [01:22,  1.65it/s]Extractor Estimating: 105it [01:23,  1.63it/s]Extractor Estimating: 106it [01:24,  1.68it/s]Extractor Estimating: 107it [01:24,  1.71it/s]Extractor Estimating: 108it [01:25,  1.70it/s]Extractor Estimating: 109it [01:25,  1.71it/s]Extractor Estimating: 110it [01:26,  1.73it/s]Extractor Estimating: 111it [01:27,  1.72it/s]Extractor Estimating: 112it [01:27,  1.67it/s]Extractor Estimating: 113it [01:28,  1.63it/s]Extractor Estimating: 114it [01:28,  1.70it/s]Extractor Estimating: 115it [01:29,  1.67it/s]Extractor Estimating: 116it [01:30,  1.72it/s]Extractor Estimating: 117it [01:30,  1.67it/s]Extractor Estimating: 118it [01:31,  1.65it/s]Extractor Estimating: 119it [01:31,  1.60it/s]Extractor Estimating: 120it [01:32,  1.64it/s]Extractor Estimating: 121it [01:33,  1.59it/s]Extractor Estimating: 122it [01:33,  1.58it/s]Extractor Estimating: 123it [01:34,  1.60it/s]Extractor Estimating: 124it [01:35,  1.60it/s]Extractor Estimating: 125it [01:35,  1.64it/s]Extractor Estimating: 126it [01:36,  1.66it/s]Extractor Estimating: 127it [01:36,  1.63it/s]Extractor Estimating: 128it [01:37,  1.65it/s]Extractor Estimating: 129it [01:38,  1.65it/s]Extractor Estimating: 130it [01:38,  1.65it/s]Extractor Estimating: 131it [01:39,  1.68it/s]Extractor Estimating: 132it [01:39,  1.67it/s]Extractor Estimating: 133it [01:40,  1.67it/s]Extractor Estimating: 134it [01:41,  1.68it/s]Extractor Estimating: 135it [01:41,  1.64it/s]Extractor Estimating: 136it [01:42,  1.67it/s]Extractor Estimating: 137it [01:42,  1.60it/s]Extractor Estimating: 138it [01:43,  1.62it/s]Extractor Estimating: 139it [01:44,  1.65it/s]Extractor Estimating: 140it [01:44,  1.58it/s]Extractor Estimating: 141it [01:45,  1.59it/s]Extractor Estimating: 142it [01:46,  1.58it/s]Extractor Estimating: 143it [01:46,  1.60it/s]Extractor Estimating: 144it [01:47,  1.56it/s]Extractor Estimating: 145it [01:47,  1.61it/s]Extractor Estimating: 146it [01:48,  1.62it/s]Extractor Estimating: 147it [01:49,  1.64it/s]Extractor Estimating: 148it [01:49,  1.63it/s]Extractor Estimating: 149it [01:50,  1.59it/s]Extractor Estimating: 150it [01:50,  1.66it/s]Extractor Estimating: 151it [01:51,  1.69it/s]Extractor Estimating: 152it [01:52,  1.70it/s]Extractor Estimating: 153it [01:52,  1.74it/s]Extractor Estimating: 154it [01:53,  1.68it/s]Extractor Estimating: 155it [01:53,  1.70it/s]Extractor Estimating: 156it [01:54,  1.68it/s]Extractor Estimating: 157it [01:55,  1.50it/s]Extractor Estimating: 158it [01:55,  1.53it/s]Extractor Estimating: 159it [01:56,  1.59it/s]Extractor Estimating: 160it [01:57,  1.61it/s]Extractor Estimating: 161it [01:57,  1.67it/s]Extractor Estimating: 162it [01:58,  1.65it/s]Extractor Estimating: 163it [01:58,  1.67it/s]Extractor Estimating: 164it [01:59,  1.64it/s]Extractor Estimating: 165it [02:00,  1.65it/s]Extractor Estimating: 166it [02:00,  1.69it/s]Extractor Estimating: 167it [02:01,  1.74it/s]Extractor Estimating: 168it [02:01,  1.73it/s]Extractor Estimating: 169it [02:02,  1.70it/s]Extractor Estimating: 170it [02:03,  1.48it/s]Extractor Estimating: 171it [02:03,  1.53it/s]Extractor Estimating: 172it [02:04,  1.65it/s]Extractor Estimating: 173it [02:05,  1.60it/s]Extractor Estimating: 174it [02:05,  1.59it/s]Extractor Estimating: 175it [02:06,  1.65it/s]Extractor Estimating: 176it [02:06,  1.71it/s]Extractor Estimating: 177it [02:07,  1.71it/s]Extractor Estimating: 178it [02:07,  1.73it/s]Extractor Estimating: 179it [02:08,  1.78it/s]Extractor Estimating: 180it [02:09,  1.75it/s]Extractor Estimating: 181it [02:09,  1.76it/s]Extractor Estimating: 182it [02:10,  1.74it/s]Extractor Estimating: 183it [02:10,  1.80it/s]Extractor Estimating: 184it [02:11,  1.74it/s]Extractor Estimating: 185it [02:11,  1.74it/s]Extractor Estimating: 186it [02:12,  1.69it/s]Extractor Estimating: 187it [02:13,  1.69it/s]Extractor Estimating: 188it [02:13,  1.73it/s]Extractor Estimating: 189it [02:14,  1.70it/s]Extractor Estimating: 190it [02:14,  1.70it/s]Extractor Estimating: 191it [02:15,  1.71it/s]Extractor Estimating: 192it [02:15,  1.72it/s]Extractor Estimating: 193it [02:16,  1.81it/s]Extractor Estimating: 194it [02:17,  1.79it/s]Extractor Estimating: 195it [02:17,  1.71it/s]Extractor Estimating: 196it [02:18,  1.71it/s]Extractor Estimating: 197it [02:18,  1.74it/s]Extractor Estimating: 198it [02:19,  1.73it/s]Extractor Estimating: 199it [02:20,  1.69it/s]Extractor Estimating: 200it [02:20,  1.66it/s]Extractor Estimating: 201it [02:21,  1.66it/s]Extractor Estimating: 202it [02:21,  1.68it/s]Extractor Estimating: 203it [02:22,  1.71it/s]Extractor Estimating: 204it [02:22,  1.72it/s]Extractor Estimating: 205it [02:23,  1.65it/s]Extractor Estimating: 206it [02:24,  1.72it/s]Extractor Estimating: 207it [02:24,  1.68it/s]Extractor Estimating: 208it [02:25,  1.76it/s]Extractor Estimating: 209it [02:25,  1.72it/s]Extractor Estimating: 210it [02:26,  1.70it/s]Extractor Estimating: 211it [02:27,  1.63it/s]Extractor Estimating: 212it [02:27,  1.65it/s]Extractor Estimating: 213it [02:28,  1.69it/s]Extractor Estimating: 214it [02:28,  1.67it/s]Extractor Estimating: 215it [02:29,  1.67it/s]Extractor Estimating: 216it [02:30,  1.73it/s]Extractor Estimating: 217it [02:30,  1.73it/s]Extractor Estimating: 218it [02:31,  1.70it/s]Extractor Estimating: 219it [02:31,  1.72it/s]Extractor Estimating: 220it [02:32,  1.72it/s]Extractor Estimating: 221it [02:32,  1.77it/s]Extractor Estimating: 222it [02:33,  1.74it/s]Extractor Estimating: 223it [02:34,  1.75it/s]Extractor Estimating: 224it [02:34,  1.72it/s]Extractor Estimating: 225it [02:35,  1.69it/s]Extractor Estimating: 226it [02:35,  1.64it/s]Extractor Estimating: 227it [02:36,  1.63it/s]Extractor Estimating: 228it [02:37,  1.63it/s]Extractor Estimating: 229it [02:37,  1.62it/s]Extractor Estimating: 230it [02:38,  1.64it/s]Extractor Estimating: 231it [02:39,  1.64it/s]Extractor Estimating: 232it [02:39,  1.64it/s]Extractor Estimating: 233it [02:40,  1.56it/s]Extractor Estimating: 234it [02:40,  1.56it/s]Extractor Estimating: 235it [02:41,  1.61it/s]Extractor Estimating: 236it [02:42,  1.62it/s]Extractor Estimating: 237it [02:42,  1.56it/s]Extractor Estimating: 238it [02:43,  1.61it/s]Extractor Estimating: 239it [02:44,  1.60it/s]Extractor Estimating: 240it [02:44,  1.50it/s]Extractor Estimating: 241it [02:45,  1.52it/s]Extractor Estimating: 242it [02:46,  1.53it/s]Extractor Estimating: 243it [02:46,  1.59it/s]Extractor Estimating: 244it [02:47,  1.58it/s]Extractor Estimating: 245it [02:48,  1.55it/s]Extractor Estimating: 246it [02:48,  1.58it/s]Extractor Estimating: 247it [02:49,  1.60it/s]Extractor Estimating: 248it [02:49,  1.62it/s]Extractor Estimating: 249it [02:50,  1.62it/s]Extractor Estimating: 250it [02:51,  1.57it/s]Extractor Estimating: 250it [02:51,  1.46it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 4962 mean pseudo reward: 0.9321196511141194
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 24170
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24270, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_5_seed_4/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24270, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.217, loss:2512.1254
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.000, loss:1761.6561
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 93, avg_time 0.961, loss:1521.2325
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 193, avg_time 0.971, loss:1529.2163
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 86, avg_time 0.957, loss:1423.8325
>> valid entity prec:0.5157, rec:0.5492, f1:0.5319
>> valid relation prec:0.1857, rec:0.0238, f1:0.0422
>> valid relation with NER prec:0.1857, rec:0.0238, f1:0.0422
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 186, avg_time 2.316, loss:1356.8716
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 79, avg_time 0.967, loss:1335.4879
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 179, avg_time 0.963, loss:1249.5802
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 72, avg_time 0.968, loss:1165.2431
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 172, avg_time 0.967, loss:1172.8416
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5497, rec:0.5079, f1:0.5280
>> valid relation prec:0.2412, rec:0.0647, f1:0.1020
>> valid relation with NER prec:0.2412, rec:0.0647, f1:0.1020
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 65, avg_time 2.306, loss:1110.1625
g_step 1200, step 165, avg_time 0.966, loss:1060.4123
g_step 1300, step 58, avg_time 0.963, loss:1030.5599
g_step 1400, step 158, avg_time 0.981, loss:1029.5880
g_step 1500, step 51, avg_time 0.963, loss:982.8885
>> valid entity prec:0.5196, rec:0.5584, f1:0.5383
>> valid relation prec:0.3529, rec:0.0223, f1:0.0420
>> valid relation with NER prec:0.3529, rec:0.0223, f1:0.0420
new max entity f1 on valid!
g_step 1600, step 151, avg_time 2.295, loss:962.2380
g_step 1700, step 44, avg_time 0.968, loss:910.7025
g_step 1800, step 144, avg_time 0.964, loss:925.6746
g_step 1900, step 37, avg_time 0.967, loss:917.5712
g_step 2000, step 137, avg_time 0.961, loss:875.6821
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5200, rec:0.4813, f1:0.4999
>> valid relation prec:0.1948, rec:0.0166, f1:0.0306
>> valid relation with NER prec:0.1948, rec:0.0166, f1:0.0306
g_step 2100, step 30, avg_time 2.295, loss:852.6109
g_step 2200, step 130, avg_time 0.956, loss:835.0006
g_step 2300, step 23, avg_time 0.964, loss:849.1833
g_step 2400, step 123, avg_time 0.961, loss:793.8410
g_step 2500, step 16, avg_time 0.956, loss:816.9875
>> valid entity prec:0.5009, rec:0.4845, f1:0.4926
>> valid relation prec:0.2329, rec:0.0253, f1:0.0456
>> valid relation with NER prec:0.2329, rec:0.0253, f1:0.0456
g_step 2600, step 116, avg_time 2.305, loss:763.8102
g_step 2700, step 9, avg_time 0.965, loss:799.0878
g_step 2800, step 109, avg_time 0.970, loss:732.2884
g_step 2900, step 2, avg_time 0.970, loss:744.3815
g_step 3000, step 102, avg_time 0.969, loss:703.0977
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4957, rec:0.4453, f1:0.4692
>> valid relation prec:0.2083, rec:0.0362, f1:0.0617
>> valid relation with NER prec:0.2083, rec:0.0362, f1:0.0617
g_step 3100, step 202, avg_time 2.300, loss:727.9323
g_step 3200, step 95, avg_time 0.969, loss:672.2525
g_step 3300, step 195, avg_time 0.967, loss:696.9830
g_step 3400, step 88, avg_time 0.966, loss:652.3045
g_step 3500, step 188, avg_time 0.967, loss:664.3967
>> valid entity prec:0.5171, rec:0.5218, f1:0.5194
>> valid relation prec:0.2213, rec:0.0335, f1:0.0581
>> valid relation with NER prec:0.2213, rec:0.0335, f1:0.0581
g_step 3600, step 81, avg_time 2.297, loss:623.9340
g_step 3700, step 181, avg_time 0.970, loss:651.9074
g_step 3800, step 74, avg_time 0.960, loss:599.3515
g_step 3900, step 174, avg_time 0.971, loss:613.1090
g_step 4000, step 67, avg_time 0.967, loss:593.0576
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5223, rec:0.4042, f1:0.4557
>> valid relation prec:0.1844, rec:0.0340, f1:0.0574
>> valid relation with NER prec:0.1844, rec:0.0340, f1:0.0574
g_step 4100, step 167, avg_time 2.293, loss:579.8255
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/27/2023 22:28:29 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/27/2023 22:28:29 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug27_22-28-29_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/27/2023 22:28:30 - WARNING - datasets.builder -   Using custom data configuration default-c08d489ede365670
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-c08d489ede365670/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  9.49 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-27 22:28:30,415 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 22:28:30,416 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-27 22:28:30,416 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 22:28:30,417 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-27 22:28:30,428 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:28:30,432 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:28:30,432 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:28:30,432 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:28:30,432 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:28:30,432 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:28:30,433 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-27 22:28:30,628 >> loading weights file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-27 22:28:33,670 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-27 22:28:33,673 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_5_seed_4/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-c08d489ede365670/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/27/2023 22:28:33 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1519acfd9050> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.81ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.66ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.02ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  4.22ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.32ba/s]100%|██████████| 6/6 [00:01<00:00,  4.83ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.07ba/s] 40%|████      | 2/5 [00:00<00:00,  4.25ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.26ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.31ba/s]100%|██████████| 5/5 [00:00<00:00,  5.25ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  9.12ba/s] 50%|█████     | 3/6 [00:00<00:00, 10.17ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 10.51ba/s]100%|██████████| 6/6 [00:00<00:00, 12.33ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.29ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.47ba/s]100%|██████████| 5/5 [00:00<00:00, 12.89ba/s]
[INFO|trainer.py:414] 2023-08-27 22:28:37,052 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-27 22:28:37,064 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-27 22:28:37,064 >>   Num examples = 5019
[INFO|trainer.py:1149] 2023-08-27 22:28:37,064 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-27 22:28:37,064 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-27 22:28:37,064 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-27 22:28:37,064 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-27 22:28:37,064 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<02:34,  2.53it/s]  1%|          | 2/390 [00:00<02:09,  3.01it/s]  1%|          | 3/390 [00:00<02:01,  3.19it/s]  1%|          | 4/390 [00:01<01:57,  3.29it/s]  1%|▏         | 5/390 [00:01<01:55,  3.34it/s]  2%|▏         | 6/390 [00:01<01:53,  3.38it/s]  2%|▏         | 7/390 [00:02<01:53,  3.39it/s]  2%|▏         | 8/390 [00:02<01:52,  3.39it/s]  2%|▏         | 9/390 [00:02<01:52,  3.39it/s]  3%|▎         | 10/390 [00:03<01:52,  3.38it/s]  3%|▎         | 11/390 [00:03<01:51,  3.39it/s]  3%|▎         | 12/390 [00:03<01:50,  3.41it/s]  3%|▎         | 13/390 [00:03<01:50,  3.42it/s]  4%|▎         | 14/390 [00:04<01:49,  3.43it/s]  4%|▍         | 15/390 [00:04<01:49,  3.43it/s]  4%|▍         | 16/390 [00:04<01:48,  3.44it/s]  4%|▍         | 17/390 [00:05<01:48,  3.44it/s]  5%|▍         | 18/390 [00:05<01:48,  3.44it/s]  5%|▍         | 19/390 [00:05<01:47,  3.45it/s]  5%|▌         | 20/390 [00:05<01:47,  3.44it/s]  5%|▌         | 21/390 [00:06<01:47,  3.45it/s]  6%|▌         | 22/390 [00:06<01:46,  3.45it/s]  6%|▌         | 23/390 [00:06<01:46,  3.45it/s]  6%|▌         | 24/390 [00:07<01:46,  3.45it/s]  6%|▋         | 25/390 [00:07<01:45,  3.45it/s]  7%|▋         | 26/390 [00:07<01:45,  3.45it/s]  7%|▋         | 27/390 [00:07<01:45,  3.45it/s]  7%|▋         | 28/390 [00:08<01:45,  3.44it/s]  7%|▋         | 29/390 [00:08<01:44,  3.45it/s]  8%|▊         | 30/390 [00:08<01:44,  3.45it/s]  8%|▊         | 31/390 [00:09<01:44,  3.45it/s]  8%|▊         | 32/390 [00:09<01:43,  3.45it/s]  8%|▊         | 33/390 [00:09<01:43,  3.45it/s]  9%|▊         | 34/390 [00:09<01:43,  3.45it/s]  9%|▉         | 35/390 [00:10<01:42,  3.45it/s]  9%|▉         | 36/390 [00:10<01:42,  3.45it/s]  9%|▉         | 37/390 [00:10<01:42,  3.44it/s] 10%|▉         | 38/390 [00:11<01:42,  3.44it/s] 10%|█         | 39/390 [00:11<01:41,  3.44it/s] 10%|█         | 40/390 [00:11<01:41,  3.44it/s] 11%|█         | 41/390 [00:12<01:41,  3.44it/s] 11%|█         | 42/390 [00:12<01:41,  3.44it/s] 11%|█         | 43/390 [00:12<01:40,  3.44it/s] 11%|█▏        | 44/390 [00:12<01:40,  3.44it/s] 12%|█▏        | 45/390 [00:13<01:40,  3.44it/s] 12%|█▏        | 46/390 [00:13<01:39,  3.44it/s] 12%|█▏        | 47/390 [00:13<01:39,  3.44it/s] 12%|█▏        | 48/390 [00:14<01:39,  3.44it/s] 13%|█▎        | 49/390 [00:14<01:39,  3.44it/s] 13%|█▎        | 50/390 [00:14<01:38,  3.45it/s] 13%|█▎        | 51/390 [00:14<01:38,  3.44it/s] 13%|█▎        | 52/390 [00:15<01:38,  3.44it/s] 14%|█▎        | 53/390 [00:15<01:38,  3.44it/s] 14%|█▍        | 54/390 [00:15<01:37,  3.44it/s] 14%|█▍        | 55/390 [00:16<01:37,  3.44it/s] 14%|█▍        | 56/390 [00:16<01:37,  3.44it/s] 15%|█▍        | 57/390 [00:16<01:36,  3.44it/s] 15%|█▍        | 58/390 [00:16<01:36,  3.44it/s] 15%|█▌        | 59/390 [00:17<01:36,  3.44it/s] 15%|█▌        | 60/390 [00:17<01:35,  3.44it/s] 16%|█▌        | 61/390 [00:17<01:35,  3.44it/s] 16%|█▌        | 62/390 [00:18<01:35,  3.44it/s] 16%|█▌        | 63/390 [00:18<01:34,  3.44it/s] 16%|█▋        | 64/390 [00:18<01:34,  3.44it/s] 17%|█▋        | 65/390 [00:18<01:34,  3.44it/s] 17%|█▋        | 66/390 [00:19<01:34,  3.44it/s] 17%|█▋        | 67/390 [00:19<01:33,  3.44it/s] 17%|█▋        | 68/390 [00:19<01:33,  3.44it/s] 18%|█▊        | 69/390 [00:20<01:33,  3.44it/s] 18%|█▊        | 70/390 [00:20<01:33,  3.44it/s] 18%|█▊        | 71/390 [00:20<01:32,  3.44it/s] 18%|█▊        | 72/390 [00:21<01:32,  3.44it/s] 19%|█▊        | 73/390 [00:21<01:32,  3.44it/s] 19%|█▉        | 74/390 [00:21<01:31,  3.44it/s] 19%|█▉        | 75/390 [00:21<01:31,  3.44it/s] 19%|█▉        | 76/390 [00:22<01:31,  3.44it/s] 20%|█▉        | 77/390 [00:22<01:31,  3.44it/s] 20%|██        | 78/390 [00:22<01:30,  3.44it/s][INFO|trainer.py:2140] 2023-08-27 22:28:59,966 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:28:59,966 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-27 22:28:59,966 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.42it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.62it/s][A
  3%|▎         | 17/505 [00:00<00:10, 45.90it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.29it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.85it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.65it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.45it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.30it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.38it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.37it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.36it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.41it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.16it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.14it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.23it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.09it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.16it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.32it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.33it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.30it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.23it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.16it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.03it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.23it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.12it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.20it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.33it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.39it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.30it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.22it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.01it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.06it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.10it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.19it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.22it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.16it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.35it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.19it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.12it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.11it/s][A
 41%|████      | 207/505 [00:04<00:06, 43.95it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 43.97it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.12it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.20it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.41it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.23it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.24it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.22it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.10it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.12it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.06it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.14it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.28it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.37it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.33it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.25it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.18it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 43.91it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.00it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.09it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.11it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.18it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.28it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.35it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.24it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.25it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.18it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 43.99it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.16it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.11it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.21it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.30it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.26it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.26it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.21it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.11it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.09it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.05it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.20it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.27it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.21it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.22it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.21it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.12it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.08it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 43.97it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.18it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.20it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.25it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.30it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.20it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.19it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.17it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.09it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.10it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.09it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.23it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.26it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.18it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.22it/s][A                                                
                                                 [A 20%|██        | 78/390 [00:34<01:30,  3.44it/s]
100%|██████████| 505/505 [00:11<00:00, 44.22it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:29:11,424 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-27 22:29:11,442 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:29:13,149 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:29:13,158 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:29:13,171 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:39<27:39,  5.33s/it] 21%|██        | 80/390 [00:40<19:44,  3.82s/it] 21%|██        | 81/390 [00:40<14:14,  2.76s/it] 21%|██        | 82/390 [00:40<10:23,  2.02s/it] 21%|██▏       | 83/390 [00:41<07:41,  1.50s/it] 22%|██▏       | 84/390 [00:41<05:49,  1.14s/it] 22%|██▏       | 85/390 [00:41<04:30,  1.13it/s] 22%|██▏       | 86/390 [00:41<03:35,  1.41it/s] 22%|██▏       | 87/390 [00:42<02:56,  1.72it/s] 23%|██▎       | 88/390 [00:42<02:29,  2.02it/s] 23%|██▎       | 89/390 [00:42<02:10,  2.31it/s] 23%|██▎       | 90/390 [00:43<01:57,  2.56it/s] 23%|██▎       | 91/390 [00:43<01:47,  2.77it/s] 24%|██▎       | 92/390 [00:43<01:41,  2.94it/s] 24%|██▍       | 93/390 [00:43<01:36,  3.08it/s] 24%|██▍       | 94/390 [00:44<01:33,  3.18it/s] 24%|██▍       | 95/390 [00:44<01:30,  3.25it/s] 25%|██▍       | 96/390 [00:44<01:28,  3.30it/s] 25%|██▍       | 97/390 [00:45<01:27,  3.34it/s] 25%|██▌       | 98/390 [00:45<01:26,  3.37it/s] 25%|██▌       | 99/390 [00:45<01:25,  3.39it/s] 26%|██▌       | 100/390 [00:46<01:25,  3.41it/s] 26%|██▌       | 101/390 [00:46<01:24,  3.42it/s] 26%|██▌       | 102/390 [00:46<01:24,  3.43it/s] 26%|██▋       | 103/390 [00:46<01:23,  3.43it/s] 27%|██▋       | 104/390 [00:47<01:23,  3.41it/s] 27%|██▋       | 105/390 [00:47<01:23,  3.42it/s] 27%|██▋       | 106/390 [00:47<01:22,  3.42it/s] 27%|██▋       | 107/390 [00:48<01:22,  3.43it/s] 28%|██▊       | 108/390 [00:48<01:22,  3.43it/s] 28%|██▊       | 109/390 [00:48<01:21,  3.44it/s] 28%|██▊       | 110/390 [00:48<01:21,  3.44it/s] 28%|██▊       | 111/390 [00:49<01:21,  3.44it/s] 29%|██▊       | 112/390 [00:49<01:20,  3.44it/s] 29%|██▉       | 113/390 [00:49<01:20,  3.44it/s] 29%|██▉       | 114/390 [00:50<01:20,  3.44it/s] 29%|██▉       | 115/390 [00:50<01:20,  3.41it/s] 30%|██▉       | 116/390 [00:50<01:20,  3.42it/s] 30%|███       | 117/390 [00:50<01:19,  3.42it/s] 30%|███       | 118/390 [00:51<01:19,  3.43it/s] 31%|███       | 119/390 [00:51<01:18,  3.43it/s] 31%|███       | 120/390 [00:51<01:18,  3.43it/s] 31%|███       | 121/390 [00:52<01:20,  3.36it/s] 31%|███▏      | 122/390 [00:52<01:19,  3.38it/s] 32%|███▏      | 123/390 [00:52<01:18,  3.40it/s] 32%|███▏      | 124/390 [00:53<01:17,  3.41it/s] 32%|███▏      | 125/390 [00:53<01:17,  3.42it/s] 32%|███▏      | 126/390 [00:53<01:17,  3.42it/s] 33%|███▎      | 127/390 [00:53<01:16,  3.43it/s] 33%|███▎      | 128/390 [00:54<01:16,  3.43it/s] 33%|███▎      | 129/390 [00:54<01:16,  3.43it/s] 33%|███▎      | 130/390 [00:54<01:15,  3.44it/s] 34%|███▎      | 131/390 [00:55<01:15,  3.44it/s] 34%|███▍      | 132/390 [00:55<01:15,  3.44it/s] 34%|███▍      | 133/390 [00:55<01:14,  3.44it/s] 34%|███▍      | 134/390 [00:55<01:14,  3.44it/s] 35%|███▍      | 135/390 [00:56<01:14,  3.44it/s] 35%|███▍      | 136/390 [00:56<01:13,  3.44it/s] 35%|███▌      | 137/390 [00:56<01:13,  3.43it/s] 35%|███▌      | 138/390 [00:57<01:13,  3.43it/s] 36%|███▌      | 139/390 [00:57<01:13,  3.43it/s] 36%|███▌      | 140/390 [00:57<01:12,  3.43it/s] 36%|███▌      | 141/390 [00:57<01:12,  3.44it/s] 36%|███▋      | 142/390 [00:58<01:12,  3.44it/s] 37%|███▋      | 143/390 [00:58<01:11,  3.44it/s] 37%|███▋      | 144/390 [00:58<01:11,  3.44it/s] 37%|███▋      | 145/390 [00:59<01:11,  3.44it/s] 37%|███▋      | 146/390 [00:59<01:11,  3.44it/s] 38%|███▊      | 147/390 [00:59<01:10,  3.43it/s] 38%|███▊      | 148/390 [01:00<01:10,  3.43it/s] 38%|███▊      | 149/390 [01:00<01:10,  3.43it/s] 38%|███▊      | 150/390 [01:00<01:09,  3.44it/s] 39%|███▊      | 151/390 [01:00<01:09,  3.44it/s] 39%|███▉      | 152/390 [01:01<01:09,  3.44it/s] 39%|███▉      | 153/390 [01:01<01:08,  3.44it/s] 39%|███▉      | 154/390 [01:01<01:08,  3.44it/s] 40%|███▉      | 155/390 [01:02<01:08,  3.42it/s] 40%|████      | 156/390 [01:02<01:08,  3.41it/s][INFO|trainer.py:2140] 2023-08-27 22:29:39,524 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:29:39,524 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-27 22:29:39,524 >>   Batch size = 8
{'eval_loss': 1.0552363395690918, 'eval_runtime': 11.4312, 'eval_samples_per_second': 353.33, 'eval_steps_per_second': 44.177, 'epoch': 0.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.16it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.64it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.03it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.24it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.70it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.39it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.27it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.24it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.33it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.45it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.31it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.29it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.13it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.06it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.93it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.98it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.03it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.18it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.30it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.35it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.17it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.15it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.02it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 43.88it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.99it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.02it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.19it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.21it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.25it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.23it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.02it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.12it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.07it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.05it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.13it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.20it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.21it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.23it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.19it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.13it/s][A
 41%|████      | 207/505 [00:04<00:06, 43.98it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 43.97it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.01it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.05it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.15it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.18it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.15it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.16it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.15it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.04it/s][A
 51%|█████     | 257/505 [00:05<00:05, 43.97it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.07it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.06it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 43.95it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.22it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.14it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.22it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.14it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.10it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.00it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.08it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.08it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.15it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.09it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.21it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.25it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.20it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.08it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.05it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.15it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.10it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.15it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.16it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.25it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.23it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.08it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.08it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.01it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.01it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.10it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.06it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.12it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.22it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.11it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.08it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.07it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.05it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.09it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.12it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.11it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.21it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.06it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.16it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.15it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.04it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.12it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.07it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.09it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.20it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.18it/s][A                                                 
                                                 [A 40%|████      | 156/390 [01:13<01:08,  3.41it/s]
100%|██████████| 505/505 [00:11<00:00, 44.18it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:29:50,997 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-27 22:29:51,012 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:29:52,632 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:29:52,648 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:29:52,660 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:20<21:41,  5.59s/it] 41%|████      | 158/390 [01:20<15:28,  4.00s/it] 41%|████      | 159/390 [01:20<11:07,  2.89s/it] 41%|████      | 160/390 [01:21<08:05,  2.11s/it] 41%|████▏     | 161/390 [01:21<05:58,  1.57s/it] 42%|████▏     | 162/390 [01:21<04:30,  1.18s/it] 42%|████▏     | 163/390 [01:22<03:28,  1.09it/s] 42%|████▏     | 164/390 [01:22<02:45,  1.37it/s] 42%|████▏     | 165/390 [01:22<02:14,  1.67it/s] 43%|████▎     | 166/390 [01:22<01:53,  1.97it/s] 43%|████▎     | 167/390 [01:23<01:39,  2.25it/s] 43%|████▎     | 168/390 [01:23<01:28,  2.50it/s] 43%|████▎     | 169/390 [01:23<01:21,  2.71it/s] 44%|████▎     | 170/390 [01:24<01:16,  2.88it/s] 44%|████▍     | 171/390 [01:24<01:12,  3.02it/s] 44%|████▍     | 172/390 [01:24<01:09,  3.12it/s] 44%|████▍     | 173/390 [01:24<01:07,  3.20it/s] 45%|████▍     | 174/390 [01:25<01:06,  3.26it/s] 45%|████▍     | 175/390 [01:25<01:05,  3.29it/s] 45%|████▌     | 176/390 [01:25<01:04,  3.32it/s] 45%|████▌     | 177/390 [01:26<01:03,  3.35it/s] 46%|████▌     | 178/390 [01:26<01:03,  3.36it/s] 46%|████▌     | 179/390 [01:26<01:02,  3.37it/s] 46%|████▌     | 180/390 [01:27<01:02,  3.38it/s] 46%|████▋     | 181/390 [01:27<01:01,  3.38it/s] 47%|████▋     | 182/390 [01:27<01:02,  3.34it/s] 47%|████▋     | 183/390 [01:27<01:01,  3.36it/s] 47%|████▋     | 184/390 [01:28<01:01,  3.37it/s] 47%|████▋     | 185/390 [01:28<01:00,  3.38it/s] 48%|████▊     | 186/390 [01:28<01:00,  3.38it/s] 48%|████▊     | 187/390 [01:29<00:59,  3.39it/s] 48%|████▊     | 188/390 [01:29<00:59,  3.39it/s] 48%|████▊     | 189/390 [01:29<00:59,  3.39it/s] 49%|████▊     | 190/390 [01:30<00:58,  3.39it/s] 49%|████▉     | 191/390 [01:30<00:58,  3.39it/s] 49%|████▉     | 192/390 [01:30<00:58,  3.39it/s] 49%|████▉     | 193/390 [01:30<00:58,  3.38it/s] 50%|████▉     | 194/390 [01:31<00:57,  3.39it/s] 50%|█████     | 195/390 [01:31<00:57,  3.39it/s] 50%|█████     | 196/390 [01:31<00:57,  3.39it/s] 51%|█████     | 197/390 [01:32<00:56,  3.39it/s] 51%|█████     | 198/390 [01:32<00:56,  3.39it/s] 51%|█████     | 199/390 [01:32<00:56,  3.39it/s] 51%|█████▏    | 200/390 [01:32<00:56,  3.39it/s] 52%|█████▏    | 201/390 [01:33<00:55,  3.39it/s] 52%|█████▏    | 202/390 [01:33<00:55,  3.39it/s] 52%|█████▏    | 203/390 [01:33<00:55,  3.39it/s] 52%|█████▏    | 204/390 [01:34<00:54,  3.39it/s] 53%|█████▎    | 205/390 [01:34<00:54,  3.39it/s] 53%|█████▎    | 206/390 [01:34<00:54,  3.39it/s] 53%|█████▎    | 207/390 [01:35<00:53,  3.41it/s] 53%|█████▎    | 208/390 [01:35<00:53,  3.41it/s] 54%|█████▎    | 209/390 [01:35<00:52,  3.42it/s] 54%|█████▍    | 210/390 [01:35<00:52,  3.42it/s] 54%|█████▍    | 211/390 [01:36<00:52,  3.43it/s] 54%|█████▍    | 212/390 [01:36<00:51,  3.43it/s] 55%|█████▍    | 213/390 [01:36<00:51,  3.43it/s] 55%|█████▍    | 214/390 [01:37<00:51,  3.43it/s] 55%|█████▌    | 215/390 [01:37<00:51,  3.43it/s] 55%|█████▌    | 216/390 [01:37<00:50,  3.43it/s] 56%|█████▌    | 217/390 [01:37<00:50,  3.43it/s] 56%|█████▌    | 218/390 [01:38<00:50,  3.44it/s] 56%|█████▌    | 219/390 [01:38<00:49,  3.42it/s] 56%|█████▋    | 220/390 [01:38<00:49,  3.41it/s] 57%|█████▋    | 221/390 [01:39<00:49,  3.41it/s] 57%|█████▋    | 222/390 [01:39<00:49,  3.40it/s] 57%|█████▋    | 223/390 [01:39<00:49,  3.40it/s] 57%|█████▋    | 224/390 [01:40<00:48,  3.40it/s] 58%|█████▊    | 225/390 [01:40<00:48,  3.39it/s] 58%|█████▊    | 226/390 [01:40<00:48,  3.39it/s] 58%|█████▊    | 227/390 [01:40<00:48,  3.39it/s] 58%|█████▊    | 228/390 [01:41<00:47,  3.39it/s] 59%|█████▊    | 229/390 [01:41<00:47,  3.39it/s] 59%|█████▉    | 230/390 [01:41<00:47,  3.39it/s] 59%|█████▉    | 231/390 [01:42<00:46,  3.39it/s] 59%|█████▉    | 232/390 [01:42<00:46,  3.39it/s] 60%|█████▉    | 233/390 [01:42<00:46,  3.39it/s] 60%|██████    | 234/390 [01:42<00:45,  3.39it/s][INFO|trainer.py:2140] 2023-08-27 22:30:20,140 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:30:20,140 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-27 22:30:20,140 >>   Batch size = 8
{'eval_loss': 1.0396924018859863, 'eval_runtime': 11.45, 'eval_samples_per_second': 352.75, 'eval_steps_per_second': 44.105, 'epoch': 1.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 54.91it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.34it/s][A
  3%|▎         | 17/505 [00:00<00:10, 45.96it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.17it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.82it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.54it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.37it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.26it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.27it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.38it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.30it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 43.99it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.11it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.07it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.03it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.03it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.16it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.15it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.28it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.25it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.15it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.12it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.10it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 43.93it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.04it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.07it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.09it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.25it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.26it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.14it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.14it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 43.96it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 43.96it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.04it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.05it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.10it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.15it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.26it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.21it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.10it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.08it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.00it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.05it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.06it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.13it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.19it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.10it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.14it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.20it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.02it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.09it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.07it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 43.91it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.16it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.21it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.17it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.11it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.10it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.05it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.04it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.16it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.11it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.11it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.21it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.25it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.15it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.08it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.08it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.11it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 43.84it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.01it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.02it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.18it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.15it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.06it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.08it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.01it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.00it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.10it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 43.01it/s][A
 81%|████████  | 407/505 [00:09<00:02, 43.57it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 43.70it/s][A
 83%|████████▎ | 417/505 [00:09<00:02, 43.88it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 43.99it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 43.87it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 43.96it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.01it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 43.89it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 43.94it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.15it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.22it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.24it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.19it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 43.98it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.03it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.07it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.02it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.02it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.24it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.22it/s][A                                                 
                                                 [A 60%|██████    | 234/390 [01:54<00:45,  3.39it/s]
100%|██████████| 505/505 [00:11<00:00, 44.22it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:30:31,618 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-27 22:30:31,638 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:30:33,714 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:30:33,733 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:30:33,742 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [01:59<13:42,  5.31s/it] 61%|██████    | 236/390 [02:00<09:45,  3.80s/it] 61%|██████    | 237/390 [02:00<07:00,  2.75s/it] 61%|██████    | 238/390 [02:00<05:06,  2.01s/it] 61%|██████▏   | 239/390 [02:01<03:46,  1.50s/it] 62%|██████▏   | 240/390 [02:01<02:50,  1.14s/it] 62%|██████▏   | 241/390 [02:01<02:11,  1.13it/s] 62%|██████▏   | 242/390 [02:02<01:44,  1.41it/s] 62%|██████▏   | 243/390 [02:02<01:25,  1.71it/s] 63%|██████▎   | 244/390 [02:02<01:12,  2.01it/s] 63%|██████▎   | 245/390 [02:02<01:03,  2.29it/s] 63%|██████▎   | 246/390 [02:03<00:56,  2.54it/s] 63%|██████▎   | 247/390 [02:03<00:52,  2.74it/s] 64%|██████▎   | 248/390 [02:03<00:48,  2.91it/s] 64%|██████▍   | 249/390 [02:04<00:46,  3.04it/s] 64%|██████▍   | 250/390 [02:04<00:44,  3.14it/s] 64%|██████▍   | 251/390 [02:04<00:43,  3.21it/s] 65%|██████▍   | 252/390 [02:04<00:42,  3.27it/s] 65%|██████▍   | 253/390 [02:05<00:41,  3.30it/s] 65%|██████▌   | 254/390 [02:05<00:40,  3.33it/s] 65%|██████▌   | 255/390 [02:05<00:40,  3.35it/s] 66%|██████▌   | 256/390 [02:06<00:39,  3.36it/s] 66%|██████▌   | 257/390 [02:06<00:39,  3.37it/s] 66%|██████▌   | 258/390 [02:06<00:39,  3.36it/s] 66%|██████▋   | 259/390 [02:07<00:38,  3.37it/s] 67%|██████▋   | 260/390 [02:07<00:38,  3.37it/s] 67%|██████▋   | 261/390 [02:07<00:38,  3.38it/s] 67%|██████▋   | 262/390 [02:07<00:37,  3.39it/s] 67%|██████▋   | 263/390 [02:08<00:37,  3.39it/s] 68%|██████▊   | 264/390 [02:08<00:37,  3.39it/s] 68%|██████▊   | 265/390 [02:08<00:36,  3.39it/s] 68%|██████▊   | 266/390 [02:09<00:36,  3.39it/s] 68%|██████▊   | 267/390 [02:09<00:36,  3.39it/s] 69%|██████▊   | 268/390 [02:09<00:35,  3.39it/s] 69%|██████▉   | 269/390 [02:09<00:35,  3.39it/s] 69%|██████▉   | 270/390 [02:10<00:35,  3.39it/s] 69%|██████▉   | 271/390 [02:10<00:35,  3.39it/s] 70%|██████▉   | 272/390 [02:10<00:34,  3.39it/s] 70%|███████   | 273/390 [02:11<00:34,  3.39it/s] 70%|███████   | 274/390 [02:11<00:34,  3.39it/s] 71%|███████   | 275/390 [02:11<00:33,  3.39it/s] 71%|███████   | 276/390 [02:12<00:33,  3.39it/s] 71%|███████   | 277/390 [02:12<00:33,  3.40it/s] 71%|███████▏  | 278/390 [02:12<00:33,  3.39it/s] 72%|███████▏  | 279/390 [02:12<00:32,  3.39it/s] 72%|███████▏  | 280/390 [02:13<00:32,  3.38it/s] 72%|███████▏  | 281/390 [02:13<00:32,  3.38it/s] 72%|███████▏  | 282/390 [02:13<00:31,  3.39it/s] 73%|███████▎  | 283/390 [02:14<00:31,  3.39it/s] 73%|███████▎  | 284/390 [02:14<00:31,  3.39it/s] 73%|███████▎  | 285/390 [02:14<00:30,  3.40it/s] 73%|███████▎  | 286/390 [02:14<00:30,  3.41it/s] 74%|███████▎  | 287/390 [02:15<00:30,  3.42it/s] 74%|███████▍  | 288/390 [02:15<00:29,  3.42it/s] 74%|███████▍  | 289/390 [02:15<00:29,  3.43it/s] 74%|███████▍  | 290/390 [02:16<00:29,  3.43it/s] 75%|███████▍  | 291/390 [02:16<00:29,  3.40it/s] 75%|███████▍  | 292/390 [02:16<00:28,  3.41it/s] 75%|███████▌  | 293/390 [02:17<00:28,  3.42it/s] 75%|███████▌  | 294/390 [02:17<00:28,  3.42it/s] 76%|███████▌  | 295/390 [02:17<00:27,  3.43it/s] 76%|███████▌  | 296/390 [02:17<00:27,  3.43it/s] 76%|███████▌  | 297/390 [02:18<00:27,  3.43it/s] 76%|███████▋  | 298/390 [02:18<00:26,  3.44it/s] 77%|███████▋  | 299/390 [02:18<00:26,  3.44it/s] 77%|███████▋  | 300/390 [02:19<00:26,  3.44it/s] 77%|███████▋  | 301/390 [02:19<00:25,  3.44it/s] 77%|███████▋  | 302/390 [02:19<00:25,  3.43it/s] 78%|███████▊  | 303/390 [02:19<00:25,  3.43it/s] 78%|███████▊  | 304/390 [02:20<00:25,  3.44it/s] 78%|███████▊  | 305/390 [02:20<00:24,  3.44it/s] 78%|███████▊  | 306/390 [02:20<00:24,  3.44it/s] 79%|███████▊  | 307/390 [02:21<00:24,  3.44it/s] 79%|███████▉  | 308/390 [02:21<00:23,  3.44it/s] 79%|███████▉  | 309/390 [02:21<00:23,  3.44it/s] 79%|███████▉  | 310/390 [02:21<00:23,  3.44it/s] 80%|███████▉  | 311/390 [02:22<00:22,  3.44it/s] 80%|████████  | 312/390 [02:22<00:22,  3.44it/s][INFO|trainer.py:2140] 2023-08-27 22:30:59,750 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:30:59,750 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-27 22:30:59,750 >>   Batch size = 8
{'eval_loss': 1.0367531776428223, 'eval_runtime': 11.4597, 'eval_samples_per_second': 352.453, 'eval_steps_per_second': 44.068, 'epoch': 2.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 54.91it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.50it/s][A
  3%|▎         | 17/505 [00:00<00:10, 45.79it/s][A
  4%|▍         | 22/505 [00:00<00:10, 44.97it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.94it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.64it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.38it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.42it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.43it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.41it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.20it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.02it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.14it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.11it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.09it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.04it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.21it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.15it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.21it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.10it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.07it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.11it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.14it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.04it/s][A
 25%|██▌       | 127/505 [00:03<00:08, 44.11it/s][A
 26%|██▌       | 132/505 [00:03<00:14, 26.31it/s][A
 27%|██▋       | 137/505 [00:03<00:12, 29.93it/s][A
 28%|██▊       | 142/505 [00:03<00:10, 33.27it/s][A
 29%|██▉       | 147/505 [00:03<00:09, 35.95it/s][A
 30%|███       | 152/505 [00:03<00:09, 38.23it/s][A
 31%|███       | 157/505 [00:03<00:08, 39.95it/s][A
 32%|███▏      | 162/505 [00:03<00:08, 41.26it/s][A
 33%|███▎      | 167/505 [00:04<00:08, 42.05it/s][A
 34%|███▍      | 172/505 [00:04<00:07, 42.19it/s][A
 35%|███▌      | 177/505 [00:04<00:07, 42.51it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 42.93it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 43.44it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 43.74it/s][A
 39%|███▉      | 197/505 [00:04<00:07, 43.98it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.18it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.27it/s][A
 42%|████▏     | 212/505 [00:05<00:06, 44.06it/s][A
 43%|████▎     | 217/505 [00:05<00:06, 43.92it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 43.71it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 43.74it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.00it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.19it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.36it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.39it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.21it/s][A
 51%|█████     | 257/505 [00:06<00:05, 44.01it/s][A
 52%|█████▏    | 262/505 [00:06<00:05, 44.00it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 43.92it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 43.80it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.03it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.27it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.38it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.30it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.05it/s][A
 60%|█████▉    | 302/505 [00:07<00:04, 43.83it/s][A
 61%|██████    | 307/505 [00:07<00:04, 43.97it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 43.88it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 43.95it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.07it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.23it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.29it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.35it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.15it/s][A
 69%|██████▊   | 347/505 [00:08<00:03, 44.11it/s][A
 70%|██████▉   | 352/505 [00:08<00:03, 43.97it/s][A
 71%|███████   | 357/505 [00:08<00:03, 43.96it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 43.88it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.11it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.13it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.29it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.29it/s][A
 77%|███████▋  | 387/505 [00:09<00:02, 44.14it/s][A
 78%|███████▊  | 392/505 [00:09<00:02, 44.05it/s][A
 79%|███████▊  | 397/505 [00:09<00:02, 44.01it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.03it/s][A
 81%|████████  | 407/505 [00:09<00:02, 43.99it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.11it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.24it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.29it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.20it/s][A
 86%|████████▌ | 432/505 [00:10<00:01, 44.11it/s][A
 87%|████████▋ | 437/505 [00:10<00:01, 43.99it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 44.07it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.10it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.05it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.15it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.24it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.26it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.19it/s][A
 94%|█████████▍| 477/505 [00:11<00:00, 44.03it/s][A
 95%|█████████▌| 482/505 [00:11<00:00, 44.07it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.11it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.00it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 43.99it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.13it/s][A                                                 
                                                 [A 80%|████████  | 312/390 [02:34<00:22,  3.44it/s]
100%|██████████| 505/505 [00:11<00:00, 44.13it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:31:11,480 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-27 22:31:11,495 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:31:13,386 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:31:13,402 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:31:13,411 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:40<07:00,  5.47s/it] 81%|████████  | 314/390 [02:40<04:57,  3.92s/it] 81%|████████  | 315/390 [02:40<03:32,  2.83s/it] 81%|████████  | 316/390 [02:40<02:33,  2.07s/it] 81%|████████▏ | 317/390 [02:41<01:52,  1.54s/it] 82%|████████▏ | 318/390 [02:41<01:23,  1.16s/it] 82%|████████▏ | 319/390 [02:41<01:04,  1.11it/s] 82%|████████▏ | 320/390 [02:42<00:50,  1.39it/s] 82%|████████▏ | 321/390 [02:42<00:40,  1.69it/s] 83%|████████▎ | 322/390 [02:42<00:34,  1.99it/s] 83%|████████▎ | 323/390 [02:43<00:29,  2.27it/s] 83%|████████▎ | 324/390 [02:43<00:26,  2.52it/s] 83%|████████▎ | 325/390 [02:43<00:23,  2.72it/s] 84%|████████▎ | 326/390 [02:43<00:22,  2.89it/s] 84%|████████▍ | 327/390 [02:44<00:20,  3.02it/s] 84%|████████▍ | 328/390 [02:44<00:19,  3.13it/s] 84%|████████▍ | 329/390 [02:44<00:19,  3.20it/s] 85%|████████▍ | 330/390 [02:45<00:18,  3.26it/s] 85%|████████▍ | 331/390 [02:45<00:17,  3.30it/s] 85%|████████▌ | 332/390 [02:45<00:17,  3.33it/s] 85%|████████▌ | 333/390 [02:46<00:17,  3.35it/s] 86%|████████▌ | 334/390 [02:46<00:16,  3.35it/s] 86%|████████▌ | 335/390 [02:46<00:16,  3.36it/s] 86%|████████▌ | 336/390 [02:46<00:17,  3.09it/s] 86%|████████▋ | 337/390 [02:47<00:16,  3.18it/s] 87%|████████▋ | 338/390 [02:47<00:16,  3.24it/s] 87%|████████▋ | 339/390 [02:47<00:15,  3.28it/s] 87%|████████▋ | 340/390 [02:48<00:15,  3.31it/s] 87%|████████▋ | 341/390 [02:48<00:14,  3.33it/s] 88%|████████▊ | 342/390 [02:48<00:14,  3.35it/s] 88%|████████▊ | 343/390 [02:49<00:13,  3.36it/s] 88%|████████▊ | 344/390 [02:49<00:13,  3.37it/s] 88%|████████▊ | 345/390 [02:49<00:13,  3.38it/s] 89%|████████▊ | 346/390 [02:49<00:13,  3.38it/s] 89%|████████▉ | 347/390 [02:50<00:12,  3.38it/s] 89%|████████▉ | 348/390 [02:50<00:12,  3.38it/s] 89%|████████▉ | 349/390 [02:50<00:12,  3.39it/s] 90%|████████▉ | 350/390 [02:51<00:11,  3.37it/s] 90%|█████████ | 351/390 [02:51<00:11,  3.37it/s] 90%|█████████ | 352/390 [02:51<00:11,  3.38it/s] 91%|█████████ | 353/390 [02:52<00:10,  3.38it/s] 91%|█████████ | 354/390 [02:52<00:10,  3.33it/s] 91%|█████████ | 355/390 [02:52<00:10,  3.35it/s] 91%|█████████▏| 356/390 [02:52<00:10,  3.36it/s] 92%|█████████▏| 357/390 [02:53<00:09,  3.37it/s] 92%|█████████▏| 358/390 [02:53<00:09,  3.37it/s] 92%|█████████▏| 359/390 [02:53<00:09,  3.38it/s] 92%|█████████▏| 360/390 [02:54<00:08,  3.38it/s] 93%|█████████▎| 361/390 [02:54<00:08,  3.38it/s] 93%|█████████▎| 362/390 [02:54<00:08,  3.39it/s] 93%|█████████▎| 363/390 [02:54<00:07,  3.39it/s] 93%|█████████▎| 364/390 [02:55<00:07,  3.39it/s] 94%|█████████▎| 365/390 [02:55<00:07,  3.39it/s] 94%|█████████▍| 366/390 [02:55<00:07,  3.39it/s] 94%|█████████▍| 367/390 [02:56<00:06,  3.39it/s] 94%|█████████▍| 368/390 [02:56<00:06,  3.38it/s] 95%|█████████▍| 369/390 [02:56<00:06,  3.38it/s] 95%|█████████▍| 370/390 [02:57<00:05,  3.38it/s] 95%|█████████▌| 371/390 [02:57<00:05,  3.39it/s] 95%|█████████▌| 372/390 [02:57<00:05,  3.39it/s] 96%|█████████▌| 373/390 [02:57<00:05,  3.39it/s] 96%|█████████▌| 374/390 [02:58<00:04,  3.39it/s] 96%|█████████▌| 375/390 [02:58<00:04,  3.39it/s] 96%|█████████▋| 376/390 [02:58<00:04,  3.39it/s] 97%|█████████▋| 377/390 [02:59<00:03,  3.39it/s] 97%|█████████▋| 378/390 [02:59<00:03,  3.39it/s] 97%|█████████▋| 379/390 [02:59<00:03,  3.37it/s] 97%|█████████▋| 380/390 [02:59<00:02,  3.38it/s] 98%|█████████▊| 381/390 [03:00<00:02,  3.38it/s] 98%|█████████▊| 382/390 [03:00<00:02,  3.39it/s] 98%|█████████▊| 383/390 [03:00<00:02,  3.39it/s] 98%|█████████▊| 384/390 [03:01<00:01,  3.39it/s] 99%|█████████▊| 385/390 [03:01<00:01,  3.39it/s] 99%|█████████▉| 386/390 [03:01<00:01,  3.39it/s] 99%|█████████▉| 387/390 [03:02<00:00,  3.39it/s] 99%|█████████▉| 388/390 [03:02<00:00,  3.38it/s]100%|█████████▉| 389/390 [03:02<00:00,  3.38it/s]100%|██████████| 390/390 [03:02<00:00,  3.37it/s][INFO|trainer.py:2140] 2023-08-27 22:31:40,018 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:31:40,018 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-27 22:31:40,018 >>   Batch size = 8
{'eval_loss': 1.0414822101593018, 'eval_runtime': 11.7026, 'eval_samples_per_second': 345.138, 'eval_steps_per_second': 43.153, 'epoch': 3.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.54it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.78it/s][A
  3%|▎         | 17/505 [00:00<00:10, 45.99it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.36it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.89it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.29it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.32it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.25it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.34it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.42it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.30it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.19it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.21it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.11it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.98it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.00it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.08it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.12it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.21it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.16it/s][A
 21%|██        | 107/505 [00:02<00:09, 43.98it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.24it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.09it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.08it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.06it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.17it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.15it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.14it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.09it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.15it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.07it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 43.89it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.07it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.04it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.12it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.13it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.18it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.18it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.07it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.14it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.02it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.06it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 43.99it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.10it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.09it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.14it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.13it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 43.96it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 43.99it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 43.96it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.00it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.00it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.05it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.10it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.07it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.15it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.13it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.14it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.10it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.13it/s][A
 61%|██████    | 307/505 [00:06<00:04, 43.97it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 43.98it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.05it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.09it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.17it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.13it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.13it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.07it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.02it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.09it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.10it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.08it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.07it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.19it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.10it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.02it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 43.98it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.04it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.04it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.15it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.07it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.06it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.20it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.13it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.01it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.00it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.06it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 44.13it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.16it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.14it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.22it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.16it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.12it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.11it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.01it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.08it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.18it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.09it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.16it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.15it/s][A                                                 
                                                 [A100%|██████████| 390/390 [03:14<00:00,  3.37it/s]
100%|██████████| 505/505 [00:11<00:00, 44.15it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:31:51,486 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-27 22:31:51,503 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:31:53,167 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:31:53,177 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:31:53,191 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-27 22:31:56,997 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-27 22:31:57,002 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-234 (score: 1.0367531776428223).
                                                 100%|██████████| 390/390 [03:21<00:00,  3.37it/s]100%|██████████| 390/390 [03:21<00:00,  1.93it/s]
[INFO|trainer.py:1894] 2023-08-27 22:31:58,642 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-27 22:31:58,659 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:32:00,387 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:32:00,401 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:32:00,412 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-27 22:32:00,608 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:32:00,608 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:32:00,608 >>   train_loss               =     0.7479
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:32:00,608 >>   train_runtime            = 0:03:21.57
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:32:00,608 >>   train_samples            =       5019
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:32:00,608 >>   train_samples_per_second =    124.495
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:32:00,608 >>   train_steps_per_second   =      1.935
{'eval_loss': 1.0443717241287231, 'eval_runtime': 11.4546, 'eval_samples_per_second': 352.609, 'eval_steps_per_second': 44.087, 'epoch': 4.99}
{'train_runtime': 201.5742, 'train_samples_per_second': 124.495, 'train_steps_per_second': 1.935, 'train_loss': 0.7479400634765625, 'epoch': 4.99}
08/27/2023 22:32:00 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-27 22:32:00,653 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:32:00,653 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-27 22:32:00,653 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:08, 55.81it/s]  2%|▏         | 12/505 [00:00<00:10, 48.81it/s]  3%|▎         | 17/505 [00:00<00:10, 46.92it/s]  4%|▍         | 22/505 [00:00<00:10, 46.28it/s]  5%|▌         | 27/505 [00:00<00:10, 45.82it/s]  6%|▋         | 32/505 [00:00<00:10, 45.48it/s]  7%|▋         | 37/505 [00:00<00:10, 45.32it/s]  8%|▊         | 42/505 [00:00<00:10, 44.81it/s]  9%|▉         | 47/505 [00:01<00:10, 44.30it/s] 10%|█         | 52/505 [00:01<00:10, 43.97it/s] 11%|█▏        | 57/505 [00:01<00:10, 44.15it/s] 12%|█▏        | 62/505 [00:01<00:09, 44.31it/s] 13%|█▎        | 67/505 [00:01<00:09, 44.48it/s] 14%|█▍        | 72/505 [00:01<00:09, 44.61it/s] 15%|█▌        | 77/505 [00:01<00:09, 44.64it/s] 16%|█▌        | 82/505 [00:01<00:09, 44.59it/s] 17%|█▋        | 87/505 [00:01<00:09, 44.26it/s] 18%|█▊        | 92/505 [00:02<00:09, 44.01it/s] 19%|█▉        | 97/505 [00:02<00:09, 43.66it/s] 20%|██        | 102/505 [00:02<00:09, 44.07it/s] 21%|██        | 107/505 [00:02<00:08, 44.25it/s] 22%|██▏       | 112/505 [00:02<00:08, 44.37it/s] 23%|██▎       | 117/505 [00:02<00:08, 44.52it/s] 24%|██▍       | 122/505 [00:02<00:08, 44.59it/s] 25%|██▌       | 127/505 [00:02<00:08, 44.55it/s] 26%|██▌       | 132/505 [00:02<00:08, 44.31it/s] 27%|██▋       | 137/505 [00:03<00:08, 44.08it/s] 28%|██▊       | 142/505 [00:03<00:08, 43.96it/s] 29%|██▉       | 147/505 [00:03<00:08, 44.11it/s] 30%|███       | 152/505 [00:03<00:07, 44.25it/s] 31%|███       | 157/505 [00:03<00:12, 28.83it/s] 32%|███▏      | 162/505 [00:03<00:10, 32.31it/s] 33%|███▎      | 167/505 [00:03<00:09, 35.23it/s] 34%|███▍      | 172/505 [00:04<00:08, 37.66it/s] 35%|███▌      | 177/505 [00:04<00:08, 39.51it/s] 36%|███▌      | 182/505 [00:04<00:07, 40.98it/s] 37%|███▋      | 187/505 [00:04<00:07, 42.11it/s] 38%|███▊      | 192/505 [00:04<00:07, 42.73it/s] 39%|███▉      | 197/505 [00:04<00:07, 42.78it/s] 40%|████      | 202/505 [00:04<00:07, 42.99it/s] 41%|████      | 207/505 [00:04<00:06, 43.27it/s] 42%|████▏     | 212/505 [00:04<00:06, 43.67it/s] 43%|████▎     | 217/505 [00:05<00:06, 44.01it/s] 44%|████▍     | 222/505 [00:05<00:06, 44.19it/s] 45%|████▍     | 227/505 [00:05<00:06, 44.53it/s] 46%|████▌     | 232/505 [00:05<00:06, 44.62it/s] 47%|████▋     | 237/505 [00:05<00:06, 44.33it/s] 48%|████▊     | 242/505 [00:05<00:05, 43.97it/s] 49%|████▉     | 247/505 [00:05<00:05, 43.92it/s] 50%|████▉     | 252/505 [00:05<00:05, 43.79it/s] 51%|█████     | 257/505 [00:05<00:05, 43.96it/s] 52%|█████▏    | 262/505 [00:06<00:05, 44.06it/s] 53%|█████▎    | 267/505 [00:06<00:05, 44.32it/s] 54%|█████▍    | 272/505 [00:06<00:05, 44.52it/s] 55%|█████▍    | 277/505 [00:06<00:05, 44.66it/s] 56%|█████▌    | 282/505 [00:06<00:05, 44.21it/s] 57%|█████▋    | 287/505 [00:06<00:04, 44.05it/s] 58%|█████▊    | 292/505 [00:06<00:04, 43.96it/s] 59%|█████▉    | 297/505 [00:06<00:04, 43.96it/s] 60%|█████▉    | 302/505 [00:06<00:04, 43.99it/s] 61%|██████    | 307/505 [00:07<00:04, 44.21it/s] 62%|██████▏   | 312/505 [00:07<00:04, 44.43it/s] 63%|██████▎   | 317/505 [00:07<00:04, 44.59it/s] 64%|██████▍   | 322/505 [00:07<00:04, 44.57it/s] 65%|██████▍   | 327/505 [00:07<00:04, 44.32it/s] 66%|██████▌   | 332/505 [00:07<00:03, 44.18it/s] 67%|██████▋   | 337/505 [00:07<00:03, 44.05it/s] 68%|██████▊   | 342/505 [00:07<00:03, 43.91it/s] 69%|██████▊   | 347/505 [00:08<00:03, 44.05it/s] 70%|██████▉   | 352/505 [00:08<00:03, 44.26it/s] 71%|███████   | 357/505 [00:08<00:03, 44.44it/s] 72%|███████▏  | 362/505 [00:08<00:03, 44.53it/s] 73%|███████▎  | 367/505 [00:08<00:03, 44.43it/s] 74%|███████▎  | 372/505 [00:08<00:03, 44.29it/s] 75%|███████▍  | 377/505 [00:08<00:02, 44.24it/s] 76%|███████▌  | 382/505 [00:08<00:02, 44.03it/s] 77%|███████▋  | 387/505 [00:08<00:02, 43.81it/s] 78%|███████▊  | 392/505 [00:09<00:02, 44.04it/s] 79%|███████▊  | 397/505 [00:09<00:02, 44.31it/s] 80%|███████▉  | 402/505 [00:09<00:02, 44.45it/s] 81%|████████  | 407/505 [00:09<00:02, 44.45it/s] 82%|████████▏ | 412/505 [00:09<00:02, 44.38it/s] 83%|████████▎ | 417/505 [00:09<00:01, 44.36it/s] 84%|████████▎ | 422/505 [00:09<00:01, 44.22it/s] 85%|████████▍ | 427/505 [00:09<00:01, 44.00it/s] 86%|████████▌ | 432/505 [00:09<00:01, 43.98it/s] 87%|████████▋ | 437/505 [00:10<00:01, 44.15it/s] 88%|████████▊ | 442/505 [00:10<00:01, 44.28it/s] 89%|████████▊ | 447/505 [00:10<00:01, 44.41it/s] 90%|████████▉ | 452/505 [00:10<00:01, 44.30it/s] 90%|█████████ | 457/505 [00:10<00:01, 44.39it/s] 91%|█████████▏| 462/505 [00:10<00:00, 44.34it/s] 92%|█████████▏| 467/505 [00:10<00:00, 44.11it/s] 93%|█████████▎| 472/505 [00:10<00:00, 44.04it/s] 94%|█████████▍| 477/505 [00:10<00:00, 44.12it/s] 95%|█████████▌| 482/505 [00:11<00:00, 44.26it/s] 96%|█████████▋| 487/505 [00:11<00:00, 44.30it/s] 97%|█████████▋| 492/505 [00:11<00:00, 44.31it/s] 98%|█████████▊| 497/505 [00:11<00:00, 44.31it/s] 99%|█████████▉| 502/505 [00:11<00:00, 44.32it/s]100%|██████████| 505/505 [00:11<00:00, 43.58it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-27 22:32:12,258 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:32:12,258 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:32:12,258 >>   eval_loss               =     1.0368
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:32:12,258 >>   eval_runtime            = 0:00:11.60
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:32:12,258 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:32:12,258 >>   eval_samples_per_second =    348.046
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:32:12,258 >>   eval_steps_per_second   =     43.517
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:32:12,258 >>   perplexity              =       2.82
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-78
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 648, in main_dual
    path_test=path_dev, labels=labels_dev, mode='all_single', is_eval=True, model_size=model_size)
TypeError: run_eval() missing 1 required positional argument: 'model_size'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_5_seed_4', 'type': 'train', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', data_dir='outputs/wrapper/wiki/unseen_5_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:16<02:29, 16.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:33<02:14, 16.81s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:02<02:35, 22.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:19<02:00, 20.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:35<01:34, 18.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:51<01:11, 17.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:07<00:51, 17.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:20<00:32, 16.00s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:35<00:15, 15.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:50<00:00, 15.31s/it]Generating: 100%|██████████| 10/10 [02:50<00:00, 17.00s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : given name .', 'success_rate': 0.80859375, 'errors': {'', "('William', 'given name', '', 'He was born in the province of Oise , his father being William , Duke of Normandy , 1st Earl of Normandy , and his mother being Mary .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 201, 'raw': 288}
{'target': 600, 'success': 226, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 287, 'raw': 416}
{'target': 600, 'success': 310, 'raw': 448}
{'target': 600, 'success': 333, 'raw': 480}
{'target': 600, 'success': 351, 'raw': 512}
{'target': 600, 'success': 377, 'raw': 544}
{'target': 600, 'success': 403, 'raw': 576}
{'target': 600, 'success': 424, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 468, 'raw': 672}
{'target': 600, 'success': 486, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 532, 'raw': 768}
{'target': 600, 'success': 554, 'raw': 800}
{'target': 600, 'success': 576, 'raw': 832}
{'target': 600, 'success': 601, 'raw': 864}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.6956018518518519, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Brazil', 'languages spoken, written or signed', '', 'Many languages are spoken in Brazil , and there are various communities and organizations advocating for a stronger free and open society .')", "('Estonia', 'languages spoken, written or signed', '', 'The language of Estonia is Estonian , the official language of the Republic of the Union ( DSO ) ) .')"}}
['Relation : lowest point . Context : Later in the year ( 1143 ) , Puyi and his allies made a conquest over the eastern part of the province of Sinti , which was occupied by the Franks and Normans . Head Entity : Sinti , Tail Entity : southern part .\n']
['Relation : lowest point . Context : Later in the year ( 1143 ) , Puyi and his allies made a conquest over the eastern part of the province of Sinti , which was occupied by the Franks and Normans . Head Entity : Sinti , Tail Entity : southern part .\n', 'Relation : lowest point . Context : After the death of Emperor Xuanzong his elder son Liu Bang was the first emperor to die at the age of only two . Head Entity : emperor , Tail Entity : highest point .\n']
{'target': 600, 'success': 12, 'raw': 32}
{'target': 600, 'success': 23, 'raw': 64}
{'target': 600, 'success': 32, 'raw': 96}
{'target': 600, 'success': 44, 'raw': 128}
{'target': 600, 'success': 55, 'raw': 160}
{'target': 600, 'success': 70, 'raw': 192}
{'target': 600, 'success': 86, 'raw': 224}
{'target': 600, 'success': 102, 'raw': 256}
{'target': 600, 'success': 115, 'raw': 288}
{'target': 600, 'success': 125, 'raw': 320}
{'target': 600, 'success': 141, 'raw': 352}
{'target': 600, 'success': 158, 'raw': 384}
{'target': 600, 'success': 176, 'raw': 416}
{'target': 600, 'success': 192, 'raw': 448}
{'target': 600, 'success': 206, 'raw': 480}
{'target': 600, 'success': 221, 'raw': 512}
{'target': 600, 'success': 233, 'raw': 544}
{'target': 600, 'success': 249, 'raw': 576}
{'target': 600, 'success': 265, 'raw': 608}
{'target': 600, 'success': 273, 'raw': 640}
{'target': 600, 'success': 288, 'raw': 672}
{'target': 600, 'success': 308, 'raw': 704}
{'target': 600, 'success': 317, 'raw': 736}
{'target': 600, 'success': 332, 'raw': 768}
{'target': 600, 'success': 342, 'raw': 800}
{'target': 600, 'success': 353, 'raw': 832}
{'target': 600, 'success': 369, 'raw': 864}
{'target': 600, 'success': 385, 'raw': 896}
{'target': 600, 'success': 394, 'raw': 928}
{'target': 600, 'success': 409, 'raw': 960}
{'target': 600, 'success': 423, 'raw': 992}
{'target': 600, 'success': 432, 'raw': 1024}
{'target': 600, 'success': 446, 'raw': 1056}
{'target': 600, 'success': 460, 'raw': 1088}
{'target': 600, 'success': 473, 'raw': 1120}
{'target': 600, 'success': 486, 'raw': 1152}
{'target': 600, 'success': 498, 'raw': 1184}
{'target': 600, 'success': 512, 'raw': 1216}
{'target': 600, 'success': 522, 'raw': 1248}
{'target': 600, 'success': 533, 'raw': 1280}
{'target': 600, 'success': 550, 'raw': 1312}
{'target': 600, 'success': 560, 'raw': 1344}
{'target': 600, 'success': 572, 'raw': 1376}
{'target': 600, 'success': 584, 'raw': 1408}
{'target': 600, 'success': 600, 'raw': 1440}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.4166666666666667, 'errors': {'', "('2004 FIFA World Cup', 'lowest point', '', 'The next day , he made a triumphant recovery and made his international debut at the 2004 FIFA World Cup against Ecuador .')", 'not enough values to unpack (expected 2, got 1)', "('New York Giants', 'lowest point', '', 'In the second season , a team led by George Foreman and David Southee were allowed to make another bowl game against the New York Giants .')", 'too many values to unpack (expected 2)', "('lowest point', 'lowest point', '', 'For comparison , in 1998 , it was the lowest point per square mile since 1960 , when it was 1 . 10 .')", "('63', 'lowest point', '', 'On the night of 9 November 2014 , the Southeastern Conference led by Jaylen Brown was led by Duke Blue Devils in a 63 66 victory over the Southeast Division rival St. Charles .')", "('second', 'lowest point', '', 'He is currently playing with fellow former New Zealand prop Tom Paine in the Championship side Auckland Roosters , and he moved up the rankings to second .')", "('Manchester', 'lowest point', '', 'The area was home to the city of Manchester ( now Manchester City ) , founded in 1859 by the English Civil War hero Sir Walter Scott .')"}}
['Relation : mother . Context : Later in Life , he came to love the beauty and natural beauty of the forests at the end of the third trimester , when she suffered death . Head Entity : forests at the end of the third trimester , Tail Entity : Mother .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : mother .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('king', 'mother', '', 'The first male and last female monarch of the Kingdom of Mexico , the king of the Andes ( d.')"}}
['Relation : mouth of the watercourse . Context : Later in the year ( 1143 ) , Puyi and his allies made atone for the fall of the last King of the Puebla , Puzco . Head Entity : Puyi , Tail Entity : Puducese .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 200, 'raw': 288}
{'target': 600, 'success': 221, 'raw': 320}
{'target': 600, 'success': 241, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 289, 'raw': 416}
{'target': 600, 'success': 311, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 357, 'raw': 512}
{'target': 600, 'success': 383, 'raw': 544}
{'target': 600, 'success': 404, 'raw': 576}
{'target': 600, 'success': 425, 'raw': 608}
{'target': 600, 'success': 440, 'raw': 640}
{'target': 600, 'success': 463, 'raw': 672}
{'target': 600, 'success': 488, 'raw': 704}
{'target': 600, 'success': 509, 'raw': 736}
{'target': 600, 'success': 536, 'raw': 768}
{'target': 600, 'success': 558, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 606, 'raw': 864}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.7013888888888888, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n']
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n', 'Relation : genre . Context : Eileen McClelland ( born June 24 , 1953 ) is an American radio talk show host who has appeared as a regular on both the Howard Stern Show and the Morning Mix . Head Entity : Howard Stern Show , Tail Entity : television talk show .\n']
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n', 'Relation : genre . Context : Eileen McClelland ( born June 24 , 1953 ) is an American radio talk show host who has appeared as a regular on both the Howard Stern Show and the Morning Mix . Head Entity : Howard Stern Show , Tail Entity : television talk show .\n', 'Relation : genre . Context : This film explores the social , social structure of the New York city of Times Square , which is populated and industrialized by wealthy Manhattanites . Head Entity : Times Square , Tail Entity : New York City .\n']
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 320, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 468, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 570, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : genre .', 'success_rate': 0.76625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.8020833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('The albums', 'is a list of', '', 'The albums songs , songs played and discography of the albums members , the artist , are listed in alphabetical order .')", "('programming languages', 'is a list of', '', 'It can be used primarily in conjunction with other programming languages such as .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8111979166666666, 'errors': {'', "('Pluto', 'located on astronomical body', '', 'In the past , Pluto has been considered to be a binary system inhabited by one giant being , as well as being the first to be named after Pluto .')", 'too many values to unpack (expected 2)', "('Its orbit', 'located on astronomical body', '', 'Its orbit shows an eccentricity of 0 . 18 and an inclination of 8 degrees from the plane of the ecliptic .')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('it', 'manufacturer', '', 'It is known in Japan for its high quality and performance components , along with its long range of performance airsoft s.')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8179347826086957, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/0_ext.jsonl'}}
estimate vocab size: 12731
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12831, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_5_seed_4/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:14, 14.29s/it]Extractor Estimating: 2it [00:18,  8.19s/it]Extractor Estimating: 3it [00:18,  4.77s/it]Extractor Estimating: 4it [00:19,  3.14s/it]Extractor Estimating: 5it [00:20,  2.23s/it]Extractor Estimating: 6it [00:20,  1.68s/it]Extractor Estimating: 7it [00:21,  1.33s/it]Extractor Estimating: 8it [00:22,  1.11s/it]Extractor Estimating: 9it [00:22,  1.06it/s]Extractor Estimating: 10it [00:23,  1.19it/s]Extractor Estimating: 11it [00:23,  1.31it/s]Extractor Estimating: 12it [00:24,  1.43it/s]Extractor Estimating: 13it [00:24,  1.47it/s]Extractor Estimating: 14it [00:25,  1.49it/s]Extractor Estimating: 15it [00:26,  1.54it/s]Extractor Estimating: 16it [00:26,  1.55it/s]Extractor Estimating: 17it [00:28,  1.24it/s]Extractor Estimating: 18it [00:28,  1.32it/s]Extractor Estimating: 19it [00:29,  1.38it/s]Extractor Estimating: 20it [00:29,  1.47it/s]Extractor Estimating: 21it [00:30,  1.54it/s]Extractor Estimating: 22it [00:31,  1.57it/s]Extractor Estimating: 23it [00:31,  1.51it/s]Extractor Estimating: 24it [00:32,  1.59it/s]Extractor Estimating: 25it [00:32,  1.67it/s]Extractor Estimating: 26it [00:33,  1.64it/s]Extractor Estimating: 27it [00:34,  1.64it/s]Extractor Estimating: 28it [00:34,  1.68it/s]Extractor Estimating: 29it [00:35,  1.73it/s]Extractor Estimating: 30it [00:35,  1.74it/s]Extractor Estimating: 31it [00:36,  1.74it/s]Extractor Estimating: 32it [00:36,  1.76it/s]Extractor Estimating: 33it [00:37,  1.78it/s]Extractor Estimating: 34it [00:38,  1.81it/s]Extractor Estimating: 35it [00:38,  1.75it/s]Extractor Estimating: 36it [00:39,  1.75it/s]Extractor Estimating: 37it [00:39,  1.73it/s]Extractor Estimating: 38it [00:40,  1.76it/s]Extractor Estimating: 39it [00:40,  1.81it/s]Extractor Estimating: 40it [00:41,  1.76it/s]Extractor Estimating: 41it [00:42,  1.74it/s]Extractor Estimating: 42it [00:42,  1.76it/s]Extractor Estimating: 43it [00:43,  1.31it/s]Extractor Estimating: 44it [00:44,  1.43it/s]Extractor Estimating: 45it [00:44,  1.55it/s]Extractor Estimating: 46it [00:45,  1.59it/s]Extractor Estimating: 47it [00:46,  1.66it/s]Extractor Estimating: 48it [00:46,  1.72it/s]Extractor Estimating: 49it [00:47,  1.77it/s]Extractor Estimating: 50it [00:47,  1.78it/s]Extractor Estimating: 51it [00:48,  1.72it/s]Extractor Estimating: 52it [00:48,  1.67it/s]Extractor Estimating: 53it [00:49,  1.65it/s]Extractor Estimating: 54it [00:50,  1.63it/s]Extractor Estimating: 55it [00:50,  1.60it/s]Extractor Estimating: 56it [00:51,  1.58it/s]Extractor Estimating: 57it [00:52,  1.55it/s]Extractor Estimating: 58it [00:52,  1.56it/s]Extractor Estimating: 59it [00:53,  1.61it/s]Extractor Estimating: 60it [00:53,  1.60it/s]Extractor Estimating: 61it [00:54,  1.57it/s]Extractor Estimating: 62it [00:55,  1.59it/s]Extractor Estimating: 63it [00:55,  1.55it/s]Extractor Estimating: 64it [00:56,  1.58it/s]Extractor Estimating: 65it [00:57,  1.60it/s]Extractor Estimating: 66it [00:57,  1.57it/s]Extractor Estimating: 67it [00:58,  1.58it/s]Extractor Estimating: 68it [00:59,  1.58it/s]Extractor Estimating: 69it [00:59,  1.57it/s]Extractor Estimating: 70it [01:00,  1.58it/s]Extractor Estimating: 71it [01:01,  1.57it/s]Extractor Estimating: 72it [01:01,  1.58it/s]Extractor Estimating: 73it [01:02,  1.56it/s]Extractor Estimating: 74it [01:02,  1.58it/s]Extractor Estimating: 75it [01:03,  1.58it/s]Extractor Estimating: 76it [01:04,  1.51it/s]Extractor Estimating: 77it [01:04,  1.54it/s]Extractor Estimating: 78it [01:05,  1.52it/s]Extractor Estimating: 79it [01:06,  1.49it/s]Extractor Estimating: 80it [01:06,  1.52it/s]Extractor Estimating: 81it [01:07,  1.54it/s]Extractor Estimating: 82it [01:08,  1.56it/s]Extractor Estimating: 83it [01:08,  1.52it/s]Extractor Estimating: 84it [01:09,  1.51it/s]Extractor Estimating: 85it [01:10,  1.48it/s]Extractor Estimating: 86it [01:11,  1.16it/s]Extractor Estimating: 87it [01:12,  1.23it/s]Extractor Estimating: 88it [01:12,  1.29it/s]Extractor Estimating: 89it [01:13,  1.33it/s]Extractor Estimating: 90it [01:14,  1.38it/s]Extractor Estimating: 91it [01:15,  1.33it/s]Extractor Estimating: 92it [01:15,  1.39it/s]Extractor Estimating: 93it [01:16,  1.44it/s]Extractor Estimating: 94it [01:16,  1.47it/s]Extractor Estimating: 95it [01:17,  1.50it/s]Extractor Estimating: 96it [01:18,  1.50it/s]Extractor Estimating: 97it [01:18,  1.56it/s]Extractor Estimating: 98it [01:19,  1.49it/s]Extractor Estimating: 99it [01:20,  1.50it/s]Extractor Estimating: 100it [01:20,  1.49it/s]Extractor Estimating: 101it [01:21,  1.50it/s]Extractor Estimating: 102it [01:22,  1.55it/s]Extractor Estimating: 103it [01:22,  1.58it/s]Extractor Estimating: 104it [01:23,  1.65it/s]Extractor Estimating: 105it [01:23,  1.63it/s]Extractor Estimating: 106it [01:24,  1.68it/s]Extractor Estimating: 107it [01:25,  1.71it/s]Extractor Estimating: 108it [01:25,  1.70it/s]Extractor Estimating: 109it [01:26,  1.70it/s]Extractor Estimating: 110it [01:26,  1.72it/s]Extractor Estimating: 111it [01:27,  1.70it/s]Extractor Estimating: 112it [01:28,  1.66it/s]Extractor Estimating: 113it [01:28,  1.61it/s]Extractor Estimating: 114it [01:29,  1.68it/s]Extractor Estimating: 115it [01:29,  1.66it/s]Extractor Estimating: 116it [01:30,  1.70it/s]Extractor Estimating: 117it [01:31,  1.65it/s]Extractor Estimating: 118it [01:31,  1.64it/s]Extractor Estimating: 119it [01:32,  1.59it/s]Extractor Estimating: 120it [01:32,  1.62it/s]Extractor Estimating: 121it [01:33,  1.58it/s]Extractor Estimating: 122it [01:34,  1.58it/s]Extractor Estimating: 123it [01:34,  1.59it/s]Extractor Estimating: 124it [01:35,  1.59it/s]Extractor Estimating: 125it [01:36,  1.64it/s]Extractor Estimating: 126it [01:36,  1.65it/s]Extractor Estimating: 127it [01:37,  1.63it/s]Extractor Estimating: 128it [01:37,  1.64it/s]Extractor Estimating: 129it [01:38,  1.64it/s]Extractor Estimating: 130it [01:39,  1.64it/s]Extractor Estimating: 131it [01:39,  1.67it/s]Extractor Estimating: 132it [01:40,  1.65it/s]Extractor Estimating: 133it [01:40,  1.66it/s]Extractor Estimating: 134it [01:41,  1.67it/s]Extractor Estimating: 135it [01:42,  1.62it/s]Extractor Estimating: 136it [01:42,  1.65it/s]Extractor Estimating: 137it [01:43,  1.60it/s]Extractor Estimating: 138it [01:44,  1.61it/s]Extractor Estimating: 139it [01:44,  1.64it/s]Extractor Estimating: 140it [01:45,  1.57it/s]Extractor Estimating: 141it [01:45,  1.58it/s]Extractor Estimating: 142it [01:46,  1.58it/s]Extractor Estimating: 143it [01:47,  1.60it/s]Extractor Estimating: 144it [01:47,  1.55it/s]Extractor Estimating: 145it [01:48,  1.61it/s]Extractor Estimating: 146it [01:49,  1.61it/s]Extractor Estimating: 147it [01:49,  1.63it/s]Extractor Estimating: 148it [01:50,  1.61it/s]Extractor Estimating: 149it [01:50,  1.57it/s]Extractor Estimating: 150it [01:51,  1.66it/s]Extractor Estimating: 151it [01:52,  1.68it/s]Extractor Estimating: 152it [01:52,  1.69it/s]Extractor Estimating: 153it [01:53,  1.73it/s]Extractor Estimating: 154it [01:53,  1.67it/s]Extractor Estimating: 155it [01:54,  1.68it/s]Extractor Estimating: 156it [01:55,  1.66it/s]Extractor Estimating: 157it [01:55,  1.59it/s]Extractor Estimating: 158it [01:56,  1.61it/s]Extractor Estimating: 159it [01:56,  1.65it/s]Extractor Estimating: 160it [01:57,  1.64it/s]Extractor Estimating: 161it [01:58,  1.58it/s]Extractor Estimating: 162it [01:58,  1.58it/s]Extractor Estimating: 163it [01:59,  1.61it/s]Extractor Estimating: 164it [02:00,  1.60it/s]Extractor Estimating: 165it [02:00,  1.62it/s]Extractor Estimating: 166it [02:01,  1.67it/s]Extractor Estimating: 167it [02:01,  1.72it/s]Extractor Estimating: 168it [02:02,  1.71it/s]Extractor Estimating: 169it [02:03,  1.68it/s]Extractor Estimating: 170it [02:03,  1.69it/s]Extractor Estimating: 171it [02:04,  1.68it/s]Extractor Estimating: 172it [02:04,  1.77it/s]Extractor Estimating: 173it [02:05,  1.67it/s]Extractor Estimating: 174it [02:06,  1.64it/s]Extractor Estimating: 175it [02:06,  1.68it/s]Extractor Estimating: 176it [02:07,  1.73it/s]Extractor Estimating: 177it [02:07,  1.72it/s]Extractor Estimating: 178it [02:08,  1.74it/s]Extractor Estimating: 179it [02:08,  1.78it/s]Extractor Estimating: 180it [02:09,  1.76it/s]Extractor Estimating: 181it [02:09,  1.76it/s]Extractor Estimating: 182it [02:10,  1.74it/s]Extractor Estimating: 183it [02:11,  1.80it/s]Extractor Estimating: 184it [02:11,  1.74it/s]Extractor Estimating: 185it [02:12,  1.73it/s]Extractor Estimating: 186it [02:12,  1.69it/s]Extractor Estimating: 187it [02:13,  1.68it/s]Extractor Estimating: 188it [02:14,  1.72it/s]Extractor Estimating: 189it [02:14,  1.70it/s]Extractor Estimating: 190it [02:15,  1.69it/s]Extractor Estimating: 191it [02:15,  1.70it/s]Extractor Estimating: 192it [02:16,  1.71it/s]Extractor Estimating: 193it [02:16,  1.81it/s]Extractor Estimating: 194it [02:17,  1.78it/s]Extractor Estimating: 195it [02:18,  1.70it/s]Extractor Estimating: 196it [02:18,  1.71it/s]Extractor Estimating: 197it [02:19,  1.73it/s]Extractor Estimating: 198it [02:19,  1.72it/s]Extractor Estimating: 199it [02:20,  1.69it/s]Extractor Estimating: 200it [02:21,  1.67it/s]Extractor Estimating: 201it [02:21,  1.67it/s]Extractor Estimating: 202it [02:22,  1.68it/s]Extractor Estimating: 203it [02:22,  1.71it/s]Extractor Estimating: 204it [02:23,  1.71it/s]Extractor Estimating: 205it [02:24,  1.64it/s]Extractor Estimating: 206it [02:24,  1.71it/s]Extractor Estimating: 207it [02:25,  1.68it/s]Extractor Estimating: 208it [02:25,  1.75it/s]Extractor Estimating: 209it [02:26,  1.71it/s]Extractor Estimating: 210it [02:26,  1.70it/s]Extractor Estimating: 211it [02:27,  1.62it/s]Extractor Estimating: 212it [02:28,  1.64it/s]Extractor Estimating: 213it [02:28,  1.68it/s]Extractor Estimating: 214it [02:29,  1.67it/s]Extractor Estimating: 215it [02:29,  1.67it/s]Extractor Estimating: 216it [02:30,  1.72it/s]Extractor Estimating: 217it [02:31,  1.72it/s]Extractor Estimating: 218it [02:31,  1.70it/s]Extractor Estimating: 219it [02:32,  1.72it/s]Extractor Estimating: 220it [02:32,  1.71it/s]Extractor Estimating: 221it [02:33,  1.77it/s]Extractor Estimating: 222it [02:33,  1.74it/s]Extractor Estimating: 223it [02:34,  1.76it/s]Extractor Estimating: 224it [02:35,  1.73it/s]Extractor Estimating: 225it [02:35,  1.69it/s]Extractor Estimating: 226it [02:36,  1.65it/s]Extractor Estimating: 227it [02:36,  1.64it/s]Extractor Estimating: 228it [02:37,  1.64it/s]Extractor Estimating: 229it [02:38,  1.63it/s]Extractor Estimating: 230it [02:38,  1.65it/s]Extractor Estimating: 231it [02:39,  1.64it/s]Extractor Estimating: 232it [02:40,  1.64it/s]Extractor Estimating: 233it [02:40,  1.56it/s]Extractor Estimating: 234it [02:41,  1.56it/s]Extractor Estimating: 235it [02:41,  1.61it/s]Extractor Estimating: 236it [02:42,  1.62it/s]Extractor Estimating: 237it [02:43,  1.56it/s]Extractor Estimating: 238it [02:43,  1.61it/s]Extractor Estimating: 239it [02:44,  1.60it/s]Extractor Estimating: 240it [02:45,  1.63it/s]Extractor Estimating: 241it [02:45,  1.61it/s]Extractor Estimating: 242it [02:46,  1.59it/s]Extractor Estimating: 243it [02:46,  1.65it/s]Extractor Estimating: 244it [02:47,  1.49it/s]Extractor Estimating: 245it [02:48,  1.50it/s]Extractor Estimating: 246it [02:49,  1.53it/s]Extractor Estimating: 247it [02:50,  1.19it/s]Extractor Estimating: 248it [02:50,  1.30it/s]Extractor Estimating: 249it [02:51,  1.38it/s]Extractor Estimating: 250it [02:52,  1.40it/s]Extractor Estimating: 250it [02:52,  1.45it/s]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/numpy/core/_methods.py:230: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)
wrapper.py:469: RuntimeWarning: invalid value encountered in double_scalars
  std_func = lambda x, mean, std: ((x - mean) / std) if std != 0 else (x - mean)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 5102 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 24236
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24336, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_train_large/unseen_5_seed_4/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24336, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.257, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.974, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 87, avg_time 1.002, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 187, avg_time 0.974, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 74, avg_time 0.979, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 174, avg_time 2.102, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 61, avg_time 0.982, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 161, avg_time 0.990, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 48, avg_time 0.976, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 148, avg_time 0.991, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 35, avg_time 2.115, loss:nan
g_step 1200, step 135, avg_time 0.983, loss:nan
g_step 1300, step 22, avg_time 0.977, loss:nan
g_step 1400, step 122, avg_time 0.997, loss:nan
g_step 1500, step 9, avg_time 0.985, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 109, avg_time 2.116, loss:nan
g_step 1700, step 209, avg_time 0.983, loss:nan
g_step 1800, step 96, avg_time 0.985, loss:nan
g_step 1900, step 196, avg_time 0.985, loss:nan
g_step 2000, step 83, avg_time 0.980, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 183, avg_time 2.137, loss:nan
g_step 2200, step 70, avg_time 0.986, loss:nan
g_step 2300, step 170, avg_time 0.990, loss:nan
g_step 2400, step 57, avg_time 0.978, loss:nan
g_step 2500, step 157, avg_time 0.995, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 44, avg_time 2.096, loss:nan
g_step 2700, step 144, avg_time 0.977, loss:nan
g_step 2800, step 31, avg_time 0.975, loss:nan
g_step 2900, step 131, avg_time 0.983, loss:nan
g_step 3000, step 18, avg_time 0.972, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 118, avg_time 2.088, loss:nan
g_step 3200, step 5, avg_time 0.984, loss:nan
g_step 3300, step 105, avg_time 0.977, loss:nan
g_step 3400, step 205, avg_time 0.968, loss:nan
g_step 3500, step 92, avg_time 0.973, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 192, avg_time 2.088, loss:nan
g_step 3700, step 79, avg_time 0.976, loss:nan
g_step 3800, step 179, avg_time 0.970, loss:nan
g_step 3900, step 66, avg_time 0.973, loss:nan
g_step 4000, step 166, avg_time 0.977, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 53, avg_time 2.078, loss:nan
g_step 4200, step 153, avg_time 0.977, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 00:04:15 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 00:04:15 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_00-04-15_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 00:04:16 - WARNING - datasets.builder -   Using custom data configuration default-ab84187d15c47ef6
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-ab84187d15c47ef6/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 00:04:16,944 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:04:16,945 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 00:04:16,946 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:04:16,947 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 00:04:16,953 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:04:16,958 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:04:16,958 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:04:16,958 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:04:16,958 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:04:16,958 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:04:16,958 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 00:04:17,072 >> loading weights file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 00:04:20,247 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 00:04:20,250 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_5_seed_4/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-ab84187d15c47ef6/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 00:04:20 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x153571edc200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.08ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.88ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.19ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.32ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.41ba/s]100%|██████████| 6/6 [00:01<00:00,  4.88ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.81ba/s] 40%|████      | 2/5 [00:00<00:00,  4.14ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.27ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.34ba/s]100%|██████████| 5/5 [00:00<00:00,  5.24ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  8.57ba/s] 50%|█████     | 3/6 [00:00<00:00, 10.21ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 10.63ba/s]100%|██████████| 6/6 [00:00<00:00, 12.07ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.18ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.45ba/s]100%|██████████| 5/5 [00:00<00:00, 12.99ba/s]
[INFO|trainer.py:414] 2023-08-28 00:04:23,683 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 00:04:23,696 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 00:04:23,696 >>   Num examples = 5160
[INFO|trainer.py:1149] 2023-08-28 00:04:23,696 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 00:04:23,696 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 00:04:23,696 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 00:04:23,696 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 00:04:23,696 >>   Total optimization steps = 405
  0%|          | 0/405 [00:00<?, ?it/s]  0%|          | 1/405 [00:00<01:53,  3.55it/s]  0%|          | 2/405 [00:00<01:52,  3.58it/s]  1%|          | 3/405 [00:00<01:51,  3.60it/s]  1%|          | 4/405 [00:01<01:51,  3.61it/s]  1%|          | 5/405 [00:01<01:50,  3.62it/s]  1%|▏         | 6/405 [00:01<01:50,  3.62it/s]  2%|▏         | 7/405 [00:01<01:49,  3.62it/s]  2%|▏         | 8/405 [00:02<01:49,  3.63it/s]  2%|▏         | 9/405 [00:02<01:49,  3.63it/s]  2%|▏         | 10/405 [00:02<01:48,  3.62it/s]  3%|▎         | 11/405 [00:03<01:49,  3.61it/s]  3%|▎         | 12/405 [00:03<01:48,  3.61it/s]  3%|▎         | 13/405 [00:03<01:48,  3.62it/s]  3%|▎         | 14/405 [00:03<01:48,  3.62it/s]  4%|▎         | 15/405 [00:04<01:47,  3.62it/s]  4%|▍         | 16/405 [00:04<01:47,  3.62it/s]  4%|▍         | 17/405 [00:04<01:47,  3.62it/s]  4%|▍         | 18/405 [00:04<01:46,  3.62it/s]  5%|▍         | 19/405 [00:05<01:46,  3.62it/s]  5%|▍         | 20/405 [00:05<01:46,  3.62it/s]  5%|▌         | 21/405 [00:05<01:45,  3.62it/s]  5%|▌         | 22/405 [00:06<01:45,  3.62it/s]  6%|▌         | 23/405 [00:06<01:45,  3.62it/s]  6%|▌         | 24/405 [00:06<01:45,  3.62it/s]  6%|▌         | 25/405 [00:06<01:44,  3.62it/s]  6%|▋         | 26/405 [00:07<01:44,  3.62it/s]  7%|▋         | 27/405 [00:07<01:44,  3.62it/s]  7%|▋         | 28/405 [00:07<01:44,  3.62it/s]  7%|▋         | 29/405 [00:08<01:43,  3.62it/s]  7%|▋         | 30/405 [00:08<01:43,  3.62it/s]  8%|▊         | 31/405 [00:08<01:43,  3.62it/s]  8%|▊         | 32/405 [00:08<01:42,  3.62it/s]  8%|▊         | 33/405 [00:09<01:42,  3.62it/s]  8%|▊         | 34/405 [00:09<01:42,  3.62it/s]  9%|▊         | 35/405 [00:09<01:42,  3.62it/s]  9%|▉         | 36/405 [00:09<01:41,  3.62it/s]  9%|▉         | 37/405 [00:10<01:41,  3.62it/s]  9%|▉         | 38/405 [00:10<01:41,  3.62it/s] 10%|▉         | 39/405 [00:10<01:41,  3.62it/s] 10%|▉         | 40/405 [00:11<01:40,  3.61it/s] 10%|█         | 41/405 [00:11<01:40,  3.62it/s] 10%|█         | 42/405 [00:11<01:40,  3.62it/s] 11%|█         | 43/405 [00:11<01:39,  3.62it/s] 11%|█         | 44/405 [00:12<01:39,  3.62it/s] 11%|█         | 45/405 [00:12<01:39,  3.62it/s] 11%|█▏        | 46/405 [00:12<01:39,  3.62it/s] 12%|█▏        | 47/405 [00:12<01:38,  3.62it/s] 12%|█▏        | 48/405 [00:13<01:38,  3.62it/s] 12%|█▏        | 49/405 [00:13<01:38,  3.62it/s] 12%|█▏        | 50/405 [00:13<01:38,  3.62it/s] 13%|█▎        | 51/405 [00:14<01:37,  3.62it/s] 13%|█▎        | 52/405 [00:14<01:37,  3.62it/s] 13%|█▎        | 53/405 [00:14<01:37,  3.62it/s] 13%|█▎        | 54/405 [00:14<01:36,  3.62it/s] 14%|█▎        | 55/405 [00:15<01:37,  3.60it/s] 14%|█▍        | 56/405 [00:15<01:36,  3.61it/s] 14%|█▍        | 57/405 [00:15<01:36,  3.61it/s] 14%|█▍        | 58/405 [00:16<01:36,  3.61it/s] 15%|█▍        | 59/405 [00:16<01:35,  3.61it/s] 15%|█▍        | 60/405 [00:16<01:35,  3.61it/s] 15%|█▌        | 61/405 [00:16<01:35,  3.62it/s] 15%|█▌        | 62/405 [00:17<01:34,  3.62it/s] 16%|█▌        | 63/405 [00:17<01:34,  3.61it/s] 16%|█▌        | 64/405 [00:17<01:34,  3.62it/s] 16%|█▌        | 65/405 [00:17<01:33,  3.62it/s] 16%|█▋        | 66/405 [00:18<01:33,  3.62it/s] 17%|█▋        | 67/405 [00:18<01:33,  3.62it/s] 17%|█▋        | 68/405 [00:18<01:33,  3.62it/s] 17%|█▋        | 69/405 [00:19<01:32,  3.61it/s] 17%|█▋        | 70/405 [00:19<01:32,  3.61it/s] 18%|█▊        | 71/405 [00:19<01:32,  3.61it/s] 18%|█▊        | 72/405 [00:19<01:32,  3.61it/s] 18%|█▊        | 73/405 [00:20<01:31,  3.62it/s] 18%|█▊        | 74/405 [00:20<01:31,  3.62it/s] 19%|█▊        | 75/405 [00:20<01:31,  3.61it/s] 19%|█▉        | 76/405 [00:21<01:31,  3.61it/s] 19%|█▉        | 77/405 [00:21<01:30,  3.61it/s] 19%|█▉        | 78/405 [00:21<01:30,  3.62it/s] 20%|█▉        | 79/405 [00:21<01:30,  3.62it/s] 20%|█▉        | 80/405 [00:22<01:29,  3.62it/s] 20%|██        | 81/405 [00:22<01:20,  4.01it/s][INFO|trainer.py:2140] 2023-08-28 00:04:46,005 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:04:46,005 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 00:04:46,005 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.55it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.74it/s][A
  3%|▎         | 17/505 [00:00<00:15, 31.33it/s][A
  4%|▍         | 22/505 [00:00<00:13, 35.27it/s][A
  5%|▌         | 27/505 [00:00<00:12, 38.06it/s][A
  6%|▋         | 32/505 [00:00<00:11, 40.05it/s][A
  7%|▋         | 37/505 [00:00<00:11, 41.43it/s][A
  8%|▊         | 42/505 [00:01<00:10, 42.36it/s][A
  9%|▉         | 47/505 [00:01<00:10, 43.17it/s][A
 10%|█         | 52/505 [00:01<00:10, 43.52it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 43.33it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 43.30it/s][A
 13%|█▎        | 67/505 [00:01<00:10, 43.47it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 43.72it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.03it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.28it/s][A
 17%|█▋        | 87/505 [00:02<00:09, 44.29it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.50it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.43it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.09it/s][A
 21%|██        | 107/505 [00:02<00:09, 43.86it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 43.90it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.04it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.32it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.39it/s][A
 26%|██▌       | 132/505 [00:03<00:08, 44.39it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.42it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.04it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.31it/s][A
 30%|███       | 152/505 [00:03<00:08, 44.04it/s][A
 31%|███       | 157/505 [00:03<00:07, 43.97it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.10it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.32it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.35it/s][A
 35%|███▌      | 177/505 [00:04<00:07, 44.46it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.47it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.38it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.23it/s][A
 39%|███▉      | 197/505 [00:04<00:07, 43.97it/s][A
 40%|████      | 202/505 [00:04<00:06, 43.99it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.01it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.34it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.43it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.47it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.45it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.31it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.16it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.01it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.01it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.09it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.17it/s][A
 52%|█████▏    | 262/505 [00:06<00:05, 44.36it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.49it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.44it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.32it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.10it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.01it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 43.96it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.08it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.22it/s][A
 61%|██████    | 307/505 [00:07<00:04, 44.23it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.49it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.41it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.16it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.10it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.00it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 43.94it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.06it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.01it/s][A
 70%|██████▉   | 352/505 [00:08<00:03, 44.30it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.36it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.37it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.39it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.18it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.10it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 43.98it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.13it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.21it/s][A
 79%|███████▊  | 397/505 [00:09<00:02, 44.40it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.42it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.41it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.29it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.20it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.08it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.03it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.04it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.21it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 44.35it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.48it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.37it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.23it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.22it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.15it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.02it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.05it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.24it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.41it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.38it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.34it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.20it/s][A                                                
                                                 [A 20%|██        | 81/405 [00:33<01:20,  4.01it/s]
100%|██████████| 505/505 [00:11<00:00, 44.20it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:04:57,572 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-81
[INFO|configuration_utils.py:351] 2023-08-28 00:04:57,594 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-81/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:04:59,336 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-81/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:04:59,360 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-81/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:04:59,387 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-81/special_tokens_map.json
 20%|██        | 82/405 [00:36<23:28,  4.36s/it] 20%|██        | 83/405 [00:36<16:49,  3.14s/it] 21%|██        | 84/405 [00:36<12:11,  2.28s/it] 21%|██        | 85/405 [00:37<08:57,  1.68s/it] 21%|██        | 86/405 [00:37<06:41,  1.26s/it] 21%|██▏       | 87/405 [00:37<05:07,  1.04it/s] 22%|██▏       | 88/405 [00:37<04:01,  1.32it/s] 22%|██▏       | 89/405 [00:38<03:14,  1.62it/s] 22%|██▏       | 90/405 [00:38<02:42,  1.94it/s] 22%|██▏       | 91/405 [00:38<02:19,  2.25it/s] 23%|██▎       | 92/405 [00:39<02:03,  2.53it/s] 23%|██▎       | 93/405 [00:39<01:52,  2.77it/s] 23%|██▎       | 94/405 [00:39<01:44,  2.97it/s] 23%|██▎       | 95/405 [00:39<01:39,  3.12it/s] 24%|██▎       | 96/405 [00:40<01:35,  3.24it/s] 24%|██▍       | 97/405 [00:40<01:32,  3.33it/s] 24%|██▍       | 98/405 [00:40<01:30,  3.40it/s] 24%|██▍       | 99/405 [00:41<01:28,  3.44it/s] 25%|██▍       | 100/405 [00:41<01:27,  3.48it/s] 25%|██▍       | 101/405 [00:41<01:26,  3.50it/s] 25%|██▌       | 102/405 [00:41<01:26,  3.52it/s] 25%|██▌       | 103/405 [00:42<01:25,  3.53it/s] 26%|██▌       | 104/405 [00:42<01:25,  3.54it/s] 26%|██▌       | 105/405 [00:42<01:24,  3.54it/s] 26%|██▌       | 106/405 [00:42<01:24,  3.55it/s] 26%|██▋       | 107/405 [00:43<01:23,  3.55it/s] 27%|██▋       | 108/405 [00:43<01:23,  3.55it/s] 27%|██▋       | 109/405 [00:43<01:23,  3.56it/s] 27%|██▋       | 110/405 [00:44<01:22,  3.56it/s] 27%|██▋       | 111/405 [00:44<01:22,  3.56it/s] 28%|██▊       | 112/405 [00:44<01:22,  3.56it/s] 28%|██▊       | 113/405 [00:44<01:22,  3.56it/s] 28%|██▊       | 114/405 [00:45<01:21,  3.56it/s] 28%|██▊       | 115/405 [00:45<01:21,  3.56it/s] 29%|██▊       | 116/405 [00:45<01:21,  3.56it/s] 29%|██▉       | 117/405 [00:46<01:20,  3.56it/s] 29%|██▉       | 118/405 [00:46<01:20,  3.56it/s] 29%|██▉       | 119/405 [00:46<01:20,  3.56it/s] 30%|██▉       | 120/405 [00:46<01:20,  3.56it/s] 30%|██▉       | 121/405 [00:47<01:19,  3.58it/s] 30%|███       | 122/405 [00:47<01:18,  3.58it/s] 30%|███       | 123/405 [00:47<01:18,  3.59it/s] 31%|███       | 124/405 [00:48<01:18,  3.60it/s] 31%|███       | 125/405 [00:48<01:17,  3.60it/s] 31%|███       | 126/405 [00:48<01:17,  3.60it/s] 31%|███▏      | 127/405 [00:48<01:17,  3.60it/s] 32%|███▏      | 128/405 [00:49<01:16,  3.60it/s] 32%|███▏      | 129/405 [00:49<01:16,  3.60it/s] 32%|███▏      | 130/405 [00:49<01:16,  3.60it/s] 32%|███▏      | 131/405 [00:49<01:16,  3.59it/s] 33%|███▎      | 132/405 [00:50<01:15,  3.59it/s] 33%|███▎      | 133/405 [00:50<01:15,  3.60it/s] 33%|███▎      | 134/405 [00:50<01:15,  3.59it/s] 33%|███▎      | 135/405 [00:51<01:15,  3.59it/s] 34%|███▎      | 136/405 [00:51<01:14,  3.60it/s] 34%|███▍      | 137/405 [00:51<01:14,  3.60it/s] 34%|███▍      | 138/405 [00:51<01:16,  3.49it/s] 34%|███▍      | 139/405 [00:52<01:15,  3.50it/s] 35%|███▍      | 140/405 [00:52<01:14,  3.53it/s] 35%|███▍      | 141/405 [00:52<01:14,  3.56it/s] 35%|███▌      | 142/405 [00:53<01:14,  3.55it/s] 35%|███▌      | 143/405 [00:53<01:13,  3.57it/s] 36%|███▌      | 144/405 [00:53<01:12,  3.58it/s] 36%|███▌      | 145/405 [00:53<01:12,  3.59it/s] 36%|███▌      | 146/405 [00:54<01:12,  3.59it/s] 36%|███▋      | 147/405 [00:54<01:11,  3.60it/s] 37%|███▋      | 148/405 [00:54<01:11,  3.60it/s] 37%|███▋      | 149/405 [00:55<01:11,  3.60it/s] 37%|███▋      | 150/405 [00:55<01:10,  3.60it/s] 37%|███▋      | 151/405 [00:55<01:10,  3.61it/s] 38%|███▊      | 152/405 [00:55<01:10,  3.61it/s] 38%|███▊      | 153/405 [00:56<01:09,  3.61it/s] 38%|███▊      | 154/405 [00:56<01:09,  3.60it/s] 38%|███▊      | 155/405 [00:56<01:09,  3.59it/s] 39%|███▊      | 156/405 [00:56<01:09,  3.59it/s] 39%|███▉      | 157/405 [00:57<01:09,  3.59it/s] 39%|███▉      | 158/405 [00:57<01:08,  3.59it/s] 39%|███▉      | 159/405 [00:57<01:08,  3.60it/s] 40%|███▉      | 160/405 [00:58<01:08,  3.60it/s] 40%|███▉      | 161/405 [00:58<01:07,  3.60it/s] 40%|████      | 162/405 [00:58<01:00,  4.00it/s][INFO|trainer.py:2140] 2023-08-28 00:05:22,238 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:05:22,238 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 00:05:22,238 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.5373, 'eval_samples_per_second': 350.08, 'eval_steps_per_second': 43.771, 'epoch': 1.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.26it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.68it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.70it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.71it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.02it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.58it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.15it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.00it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.21it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.40it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.45it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.39it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.40it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.26it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.00it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.90it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.86it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.06it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.27it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.40it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.35it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.36it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.21it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.02it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.83it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.88it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 43.97it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.22it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.38it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.29it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.39it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.20it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.02it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.97it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 43.94it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.04it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.25it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.17it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.30it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.31it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.19it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.08it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 43.99it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.04it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 43.97it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.19it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.29it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.38it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.31it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.27it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.05it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.13it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 43.95it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 43.94it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.08it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.28it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.33it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.35it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.13it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.13it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.09it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.08it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.05it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.12it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.09it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.26it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.24it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.17it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.10it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.06it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.07it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.12it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.14it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.23it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.23it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.19it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.13it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.15it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.08it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.00it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.01it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.17it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.27it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.25it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.14it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.11it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.10it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.09it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.06it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.02it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.14it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.18it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.21it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.12it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.21it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.11it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.13it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.04it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.07it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.20it/s][A                                                 
                                                 [A 40%|████      | 162/405 [01:09<01:00,  4.00it/s]
100%|██████████| 505/505 [00:11<00:00, 44.20it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:05:33,772 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-162
[INFO|configuration_utils.py:351] 2023-08-28 00:05:33,804 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-162/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:05:36,993 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-162/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:05:37,010 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-162/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:05:37,019 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-162/special_tokens_map.json
 40%|████      | 163/405 [01:13<19:16,  4.78s/it] 40%|████      | 164/405 [01:14<13:46,  3.43s/it] 41%|████      | 165/405 [01:14<09:56,  2.49s/it] 41%|████      | 166/405 [01:14<07:15,  1.82s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/405 [01:15<05:25,  1.37s/it] 41%|████▏     | 168/405 [01:15<04:06,  1.04s/it] 42%|████▏     | 169/405 [01:15<03:11,  1.23it/s] 42%|████▏     | 170/405 [01:15<02:33,  1.53it/s] 42%|████▏     | 171/405 [01:16<02:06,  1.85it/s] 42%|████▏     | 172/405 [01:16<01:48,  2.16it/s] 43%|████▎     | 173/405 [01:16<01:35,  2.44it/s] 43%|████▎     | 174/405 [01:17<01:25,  2.69it/s] 43%|████▎     | 175/405 [01:17<01:19,  2.90it/s] 43%|████▎     | 176/405 [01:17<01:14,  3.08it/s] 44%|████▎     | 177/405 [01:17<01:11,  3.21it/s] 44%|████▍     | 178/405 [01:18<01:08,  3.31it/s] 44%|████▍     | 179/405 [01:18<01:06,  3.38it/s] 44%|████▍     | 180/405 [01:18<01:05,  3.43it/s] 45%|████▍     | 181/405 [01:18<01:04,  3.47it/s] 45%|████▍     | 182/405 [01:19<01:03,  3.50it/s] 45%|████▌     | 183/405 [01:19<01:03,  3.52it/s] 45%|████▌     | 184/405 [01:19<01:02,  3.51it/s] 46%|████▌     | 185/405 [01:20<01:02,  3.52it/s] 46%|████▌     | 186/405 [01:20<01:02,  3.53it/s] 46%|████▌     | 187/405 [01:20<01:01,  3.54it/s] 46%|████▋     | 188/405 [01:20<01:01,  3.54it/s] 47%|████▋     | 189/405 [01:21<01:00,  3.55it/s] 47%|████▋     | 190/405 [01:21<01:00,  3.55it/s] 47%|████▋     | 191/405 [01:21<01:00,  3.56it/s] 47%|████▋     | 192/405 [01:22<00:59,  3.55it/s] 48%|████▊     | 193/405 [01:22<00:59,  3.55it/s] 48%|████▊     | 194/405 [01:22<00:59,  3.56it/s] 48%|████▊     | 195/405 [01:22<00:59,  3.54it/s] 48%|████▊     | 196/405 [01:23<00:58,  3.54it/s] 49%|████▊     | 197/405 [01:23<00:58,  3.55it/s] 49%|████▉     | 198/405 [01:23<00:58,  3.55it/s] 49%|████▉     | 199/405 [01:24<00:58,  3.55it/s] 49%|████▉     | 200/405 [01:24<00:57,  3.55it/s] 50%|████▉     | 201/405 [01:24<00:57,  3.55it/s] 50%|████▉     | 202/405 [01:24<00:57,  3.55it/s] 50%|█████     | 203/405 [01:25<00:56,  3.56it/s] 50%|█████     | 204/405 [01:25<00:56,  3.56it/s] 51%|█████     | 205/405 [01:25<00:56,  3.56it/s] 51%|█████     | 206/405 [01:26<00:55,  3.56it/s] 51%|█████     | 207/405 [01:26<00:55,  3.56it/s] 51%|█████▏    | 208/405 [01:26<00:55,  3.56it/s] 52%|█████▏    | 209/405 [01:26<00:55,  3.56it/s] 52%|█████▏    | 210/405 [01:27<00:54,  3.56it/s] 52%|█████▏    | 211/405 [01:27<00:54,  3.55it/s] 52%|█████▏    | 212/405 [01:27<00:54,  3.55it/s] 53%|█████▎    | 213/405 [01:27<00:54,  3.55it/s] 53%|█████▎    | 214/405 [01:28<00:53,  3.57it/s] 53%|█████▎    | 215/405 [01:28<00:53,  3.58it/s] 53%|█████▎    | 216/405 [01:28<00:52,  3.59it/s] 54%|█████▎    | 217/405 [01:29<00:52,  3.59it/s] 54%|█████▍    | 218/405 [01:29<00:51,  3.60it/s] 54%|█████▍    | 219/405 [01:29<00:51,  3.60it/s] 54%|█████▍    | 220/405 [01:29<00:51,  3.60it/s] 55%|█████▍    | 221/405 [01:30<00:51,  3.60it/s] 55%|█████▍    | 222/405 [01:30<00:51,  3.59it/s] 55%|█████▌    | 223/405 [01:30<00:50,  3.59it/s] 55%|█████▌    | 224/405 [01:31<00:50,  3.59it/s] 56%|█████▌    | 225/405 [01:31<00:50,  3.59it/s] 56%|█████▌    | 226/405 [01:31<00:49,  3.60it/s] 56%|█████▌    | 227/405 [01:31<00:49,  3.60it/s] 56%|█████▋    | 228/405 [01:32<00:49,  3.61it/s] 57%|█████▋    | 229/405 [01:32<00:48,  3.61it/s] 57%|█████▋    | 230/405 [01:32<00:48,  3.61it/s] 57%|█████▋    | 231/405 [01:32<00:48,  3.61it/s] 57%|█████▋    | 232/405 [01:33<00:47,  3.61it/s] 58%|█████▊    | 233/405 [01:33<00:47,  3.59it/s] 58%|█████▊    | 234/405 [01:33<00:47,  3.59it/s] 58%|█████▊    | 235/405 [01:34<00:47,  3.60it/s] 58%|█████▊    | 236/405 [01:34<00:46,  3.60it/s] 59%|█████▊    | 237/405 [01:34<00:46,  3.60it/s] 59%|█████▉    | 238/405 [01:34<00:46,  3.61it/s] 59%|█████▉    | 239/405 [01:35<00:46,  3.61it/s] 59%|█████▉    | 240/405 [01:35<00:45,  3.61it/s] 60%|█████▉    | 241/405 [01:35<00:45,  3.61it/s] 60%|█████▉    | 242/405 [01:36<00:45,  3.61it/s] 60%|██████    | 243/405 [01:36<00:40,  4.00it/s][INFO|trainer.py:2140] 2023-08-28 00:05:59,914 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:05:59,914 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 00:05:59,914 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4364, 'eval_samples_per_second': 353.172, 'eval_steps_per_second': 44.157, 'epoch': 2.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.97it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.59it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.71it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.60it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.95it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.51it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.14it/s][A
  8%|▊         | 42/505 [00:00<00:10, 43.93it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.18it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.31it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.32it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.49it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.34it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.29it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.06it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.92it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.91it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.12it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.19it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.35it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.39it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.29it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.11it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.06it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.95it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.90it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.06it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.29it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.35it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.34it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.17it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.16it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.09it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.99it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 43.99it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.11it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.25it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.37it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.29it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.15it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.09it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.03it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.00it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.02it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.14it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.26it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.31it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.25it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.23it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.14it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.12it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 43.97it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.13it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.18it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.22it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.26it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.23it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.19it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.05it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.10it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.02it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.11it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.16it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.31it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.21it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.22it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.23it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.16it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.14it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.01it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.21it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.22it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.31it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.20it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.22it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.17it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.23it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.05it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.11it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.21it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.28it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.23it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.13it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.17it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.23it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.15it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.05it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.16it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.27it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.20it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.07it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.21it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.21it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.12it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.17it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.22it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.21it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.25it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.15it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.15it/s][A                                                 
                                                 [A 60%|██████    | 243/405 [01:47<00:40,  4.00it/s]
100%|██████████| 505/505 [00:11<00:00, 44.15it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:06:11,367 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-243
[INFO|configuration_utils.py:351] 2023-08-28 00:06:11,390 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-243/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:06:13,523 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-243/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:06:13,551 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-243/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:06:13,558 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-243/special_tokens_map.json
 60%|██████    | 244/405 [01:50<11:53,  4.43s/it] 60%|██████    | 245/405 [01:50<08:29,  3.19s/it] 61%|██████    | 246/405 [01:50<06:08,  2.32s/it] 61%|██████    | 247/405 [01:51<04:29,  1.70s/it] 61%|██████    | 248/405 [01:51<03:20,  1.28s/it] 61%|██████▏   | 249/405 [01:51<02:32,  1.02it/s] 62%|██████▏   | 250/405 [01:52<02:00,  1.29it/s] 62%|██████▏   | 251/405 [01:52<01:36,  1.59it/s] 62%|██████▏   | 252/405 [01:52<01:20,  1.91it/s] 62%|██████▏   | 253/405 [01:52<01:08,  2.22it/s] 63%|██████▎   | 254/405 [01:53<01:00,  2.49it/s] 63%|██████▎   | 255/405 [01:53<00:54,  2.74it/s] 63%|██████▎   | 256/405 [01:53<00:50,  2.94it/s] 63%|██████▎   | 257/405 [01:54<00:47,  3.11it/s] 64%|██████▎   | 258/405 [01:54<00:45,  3.23it/s] 64%|██████▍   | 259/405 [01:54<00:43,  3.32it/s] 64%|██████▍   | 260/405 [01:54<00:42,  3.39it/s] 64%|██████▍   | 261/405 [01:55<00:41,  3.43it/s] 65%|██████▍   | 262/405 [01:55<00:41,  3.47it/s] 65%|██████▍   | 263/405 [01:55<00:40,  3.50it/s] 65%|██████▌   | 264/405 [01:56<00:40,  3.51it/s] 65%|██████▌   | 265/405 [01:56<00:39,  3.53it/s] 66%|██████▌   | 266/405 [01:56<00:39,  3.54it/s] 66%|██████▌   | 267/405 [01:56<00:38,  3.54it/s] 66%|██████▌   | 268/405 [01:57<00:38,  3.55it/s] 66%|██████▋   | 269/405 [01:57<00:38,  3.55it/s] 67%|██████▋   | 270/405 [01:57<00:37,  3.57it/s] 67%|██████▋   | 271/405 [01:58<00:37,  3.58it/s] 67%|██████▋   | 272/405 [01:58<00:37,  3.59it/s] 67%|██████▋   | 273/405 [01:58<00:36,  3.59it/s] 68%|██████▊   | 274/405 [01:58<00:36,  3.60it/s] 68%|██████▊   | 275/405 [01:59<00:36,  3.60it/s] 68%|██████▊   | 276/405 [01:59<00:35,  3.61it/s] 68%|██████▊   | 277/405 [01:59<00:35,  3.61it/s] 69%|██████▊   | 278/405 [01:59<00:35,  3.61it/s] 69%|██████▉   | 279/405 [02:00<00:34,  3.61it/s] 69%|██████▉   | 280/405 [02:00<00:34,  3.61it/s] 69%|██████▉   | 281/405 [02:00<00:34,  3.61it/s] 70%|██████▉   | 282/405 [02:01<00:34,  3.59it/s] 70%|██████▉   | 283/405 [02:01<00:33,  3.59it/s] 70%|███████   | 284/405 [02:01<00:33,  3.60it/s] 70%|███████   | 285/405 [02:01<00:33,  3.60it/s] 71%|███████   | 286/405 [02:02<00:32,  3.61it/s] 71%|███████   | 287/405 [02:02<00:32,  3.61it/s] 71%|███████   | 288/405 [02:02<00:32,  3.61it/s] 71%|███████▏  | 289/405 [02:03<00:32,  3.60it/s] 72%|███████▏  | 290/405 [02:03<00:31,  3.60it/s] 72%|███████▏  | 291/405 [02:03<00:31,  3.61it/s] 72%|███████▏  | 292/405 [02:03<00:31,  3.60it/s] 72%|███████▏  | 293/405 [02:04<00:31,  3.56it/s] 73%|███████▎  | 294/405 [02:04<00:31,  3.57it/s] 73%|███████▎  | 295/405 [02:04<00:30,  3.58it/s] 73%|███████▎  | 296/405 [02:04<00:30,  3.59it/s] 73%|███████▎  | 297/405 [02:05<00:30,  3.60it/s] 74%|███████▎  | 298/405 [02:05<00:29,  3.60it/s] 74%|███████▍  | 299/405 [02:05<00:29,  3.60it/s] 74%|███████▍  | 300/405 [02:06<00:29,  3.60it/s] 74%|███████▍  | 301/405 [02:06<00:28,  3.60it/s] 75%|███████▍  | 302/405 [02:06<00:28,  3.61it/s] 75%|███████▍  | 303/405 [02:06<00:28,  3.61it/s] 75%|███████▌  | 304/405 [02:07<00:28,  3.58it/s] 75%|███████▌  | 305/405 [02:07<00:27,  3.59it/s] 76%|███████▌  | 306/405 [02:07<00:27,  3.60it/s] 76%|███████▌  | 307/405 [02:08<00:27,  3.60it/s] 76%|███████▌  | 308/405 [02:08<00:26,  3.61it/s] 76%|███████▋  | 309/405 [02:08<00:26,  3.61it/s] 77%|███████▋  | 310/405 [02:08<00:26,  3.61it/s] 77%|███████▋  | 311/405 [02:09<00:26,  3.61it/s] 77%|███████▋  | 312/405 [02:09<00:25,  3.61it/s] 77%|███████▋  | 313/405 [02:09<00:25,  3.61it/s] 78%|███████▊  | 314/405 [02:09<00:25,  3.60it/s] 78%|███████▊  | 315/405 [02:10<00:25,  3.60it/s] 78%|███████▊  | 316/405 [02:10<00:24,  3.60it/s] 78%|███████▊  | 317/405 [02:10<00:24,  3.60it/s] 79%|███████▊  | 318/405 [02:11<00:24,  3.61it/s] 79%|███████▉  | 319/405 [02:11<00:23,  3.60it/s] 79%|███████▉  | 320/405 [02:11<00:23,  3.61it/s] 79%|███████▉  | 321/405 [02:11<00:23,  3.61it/s] 80%|███████▉  | 322/405 [02:12<00:22,  3.61it/s] 80%|███████▉  | 323/405 [02:12<00:22,  3.61it/s] 80%|████████  | 324/405 [02:12<00:20,  4.01it/s][INFO|trainer.py:2140] 2023-08-28 00:06:36,329 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:06:36,329 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 00:06:36,329 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4335, 'eval_samples_per_second': 353.259, 'eval_steps_per_second': 44.168, 'epoch': 3.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.11it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.43it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.76it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.64it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.84it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.62it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.29it/s][A
  8%|▊         | 42/505 [00:00<00:10, 43.92it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.02it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.21it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.32it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.44it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.42it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.25it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.12it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.99it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.91it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.06it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.22it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.31it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.29it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.34it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.09it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.04it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.97it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.01it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.08it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.22it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.23it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.32it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.19it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.07it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.02it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.95it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.11it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.14it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.23it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.29it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.28it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.07it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.00it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.01it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.00it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.13it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.10it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.30it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.20it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.22it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.13it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.08it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.02it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.05it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.05it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.14it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.28it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.20it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.22it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.18it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.00it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.10it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.05it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.08it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.14it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.26it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.19it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.21it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.14it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.00it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.05it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.10it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.16it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.17it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.27it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.15it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.14it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.13it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.12it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.12it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.01it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.11it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.16it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.18it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.17it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.19it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.07it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.02it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.15it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.13it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.04it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.09it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.18it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.18it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.12it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.09it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.01it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.11it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.12it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.10it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.23it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.23it/s][A                                                 
                                                 [A 80%|████████  | 324/405 [02:24<00:20,  4.01it/s]
100%|██████████| 505/505 [00:11<00:00, 44.23it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:06:47,785 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-324
[INFO|configuration_utils.py:351] 2023-08-28 00:06:47,811 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-324/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:06:50,097 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-324/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:06:50,114 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-324/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:06:50,126 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-324/special_tokens_map.json
 80%|████████  | 325/405 [02:26<05:58,  4.48s/it] 80%|████████  | 326/405 [02:27<04:14,  3.22s/it] 81%|████████  | 327/405 [02:27<03:02,  2.34s/it] 81%|████████  | 328/405 [02:27<02:12,  1.72s/it] 81%|████████  | 329/405 [02:28<01:37,  1.29s/it] 81%|████████▏ | 330/405 [02:28<01:14,  1.01it/s] 82%|████████▏ | 331/405 [02:28<00:57,  1.29it/s] 82%|████████▏ | 332/405 [02:28<00:45,  1.59it/s] 82%|████████▏ | 333/405 [02:29<00:37,  1.91it/s] 82%|████████▏ | 334/405 [02:29<00:32,  2.22it/s] 83%|████████▎ | 335/405 [02:29<00:28,  2.50it/s] 83%|████████▎ | 336/405 [02:30<00:25,  2.74it/s] 83%|████████▎ | 337/405 [02:30<00:23,  2.95it/s] 83%|████████▎ | 338/405 [02:30<00:21,  3.10it/s] 84%|████████▎ | 339/405 [02:30<00:20,  3.23it/s] 84%|████████▍ | 340/405 [02:31<00:19,  3.32it/s] 84%|████████▍ | 341/405 [02:31<00:18,  3.39it/s] 84%|████████▍ | 342/405 [02:31<00:18,  3.43it/s] 85%|████████▍ | 343/405 [02:32<00:17,  3.46it/s] 85%|████████▍ | 344/405 [02:32<00:17,  3.49it/s] 85%|████████▌ | 345/405 [02:32<00:17,  3.51it/s] 85%|████████▌ | 346/405 [02:32<00:16,  3.52it/s] 86%|████████▌ | 347/405 [02:33<00:16,  3.53it/s] 86%|████████▌ | 348/405 [02:33<00:16,  3.54it/s] 86%|████████▌ | 349/405 [02:33<00:15,  3.54it/s] 86%|████████▋ | 350/405 [02:34<00:15,  3.54it/s] 87%|████████▋ | 351/405 [02:34<00:15,  3.55it/s] 87%|████████▋ | 352/405 [02:34<00:14,  3.55it/s] 87%|████████▋ | 353/405 [02:34<00:14,  3.54it/s] 87%|████████▋ | 354/405 [02:35<00:14,  3.54it/s] 88%|████████▊ | 355/405 [02:35<00:14,  3.55it/s] 88%|████████▊ | 356/405 [02:35<00:13,  3.55it/s] 88%|████████▊ | 357/405 [02:35<00:13,  3.55it/s] 88%|████████▊ | 358/405 [02:36<00:13,  3.56it/s] 89%|████████▊ | 359/405 [02:36<00:12,  3.56it/s] 89%|████████▉ | 360/405 [02:36<00:12,  3.55it/s] 89%|████████▉ | 361/405 [02:37<00:12,  3.55it/s] 89%|████████▉ | 362/405 [02:37<00:12,  3.55it/s] 90%|████████▉ | 363/405 [02:37<00:11,  3.55it/s] 90%|████████▉ | 364/405 [02:37<00:11,  3.54it/s] 90%|█████████ | 365/405 [02:38<00:11,  3.55it/s] 90%|█████████ | 366/405 [02:38<00:10,  3.55it/s] 91%|█████████ | 367/405 [02:38<00:10,  3.55it/s] 91%|█████████ | 368/405 [02:39<00:10,  3.55it/s] 91%|█████████ | 369/405 [02:39<00:10,  3.55it/s] 91%|█████████▏| 370/405 [02:39<00:09,  3.56it/s] 92%|█████████▏| 371/405 [02:39<00:09,  3.57it/s] 92%|█████████▏| 372/405 [02:40<00:09,  3.59it/s] 92%|█████████▏| 373/405 [02:40<00:08,  3.60it/s] 92%|█████████▏| 374/405 [02:40<00:08,  3.60it/s] 93%|█████████▎| 375/405 [02:41<00:08,  3.58it/s] 93%|█████████▎| 376/405 [02:41<00:08,  3.59it/s] 93%|█████████▎| 377/405 [02:41<00:07,  3.59it/s] 93%|█████████▎| 378/405 [02:41<00:07,  3.60it/s] 94%|█████████▎| 379/405 [02:42<00:07,  3.60it/s] 94%|█████████▍| 380/405 [02:42<00:06,  3.60it/s] 94%|█████████▍| 381/405 [02:42<00:06,  3.61it/s] 94%|█████████▍| 382/405 [02:42<00:06,  3.61it/s] 95%|█████████▍| 383/405 [02:43<00:06,  3.61it/s] 95%|█████████▍| 384/405 [02:43<00:05,  3.61it/s] 95%|█████████▌| 385/405 [02:43<00:05,  3.60it/s] 95%|█████████▌| 386/405 [02:44<00:05,  3.59it/s] 96%|█████████▌| 387/405 [02:44<00:05,  3.60it/s] 96%|█████████▌| 388/405 [02:44<00:04,  3.60it/s] 96%|█████████▌| 389/405 [02:44<00:04,  3.60it/s] 96%|█████████▋| 390/405 [02:45<00:04,  3.61it/s] 97%|█████████▋| 391/405 [02:45<00:03,  3.61it/s] 97%|█████████▋| 392/405 [02:45<00:03,  3.61it/s] 97%|█████████▋| 393/405 [02:46<00:03,  3.61it/s] 97%|█████████▋| 394/405 [02:46<00:03,  3.61it/s] 98%|█████████▊| 395/405 [02:46<00:02,  3.61it/s] 98%|█████████▊| 396/405 [02:46<00:02,  3.61it/s] 98%|█████████▊| 397/405 [02:47<00:02,  3.59it/s] 98%|█████████▊| 398/405 [02:47<00:01,  3.60it/s] 99%|█████████▊| 399/405 [02:47<00:01,  3.60it/s] 99%|█████████▉| 400/405 [02:47<00:01,  3.61it/s] 99%|█████████▉| 401/405 [02:48<00:01,  3.61it/s] 99%|█████████▉| 402/405 [02:48<00:00,  3.61it/s]100%|█████████▉| 403/405 [02:48<00:00,  3.61it/s]100%|█████████▉| 404/405 [02:49<00:00,  3.61it/s]100%|██████████| 405/405 [02:49<00:00,  4.01it/s][INFO|trainer.py:2140] 2023-08-28 00:07:12,968 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:07:12,968 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 00:07:12,968 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4403, 'eval_samples_per_second': 353.049, 'eval_steps_per_second': 44.142, 'epoch': 4.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.96it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.44it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.53it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.41it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.94it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.58it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.27it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.14it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.26it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.33it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.49it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.38it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.30it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.16it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.97it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.95it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.88it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.00it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.28it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.39it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.39it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.30it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.16it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.09it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 42.85it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.33it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 43.75it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.00it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.12it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.28it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.24it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.09it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.01it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.79it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 43.83it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.08it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.33it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.41it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.35it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.27it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.16it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 43.93it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 43.85it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 43.91it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.05it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.32it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.35it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.43it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.30it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.13it/s][A
 51%|█████     | 257/505 [00:05<00:05, 43.98it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 43.86it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 43.93it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.08it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.28it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.32it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.35it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.18it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.16it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.00it/s][A
 61%|██████    | 307/505 [00:06<00:04, 43.95it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 43.97it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.18it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.29it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.36it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.30it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.21it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.09it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 43.98it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.03it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.05it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.29it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.34it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.31it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.18it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.11it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 43.88it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 43.92it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.10it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.16it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.34it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.37it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.31it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.23it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.10it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 43.98it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.06it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.11it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.23it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.24it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.23it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.24it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.23it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 43.99it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 43.98it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 43.95it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.26it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.25it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.32it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.20it/s][A                                                 
                                                 [A100%|██████████| 405/405 [03:00<00:00,  4.01it/s]
100%|██████████| 505/505 [00:11<00:00, 44.20it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:07:24,432 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-405
[INFO|configuration_utils.py:351] 2023-08-28 00:07:24,455 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-405/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:07:26,075 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-405/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:07:26,089 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-405/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:07:26,099 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-405/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 00:07:26,367 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 00:07:26,367 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-81 (score: 1.2323094606399536).
                                                 100%|██████████| 405/405 [03:04<00:00,  4.01it/s]100%|██████████| 405/405 [03:04<00:00,  2.20it/s]
[INFO|trainer.py:1894] 2023-08-28 00:07:28,156 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 00:07:28,176 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:07:30,291 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:07:30,305 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:07:30,313 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:07:30,511 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:07:30,511 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:07:30,511 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:07:30,512 >>   train_runtime            = 0:03:04.45
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:07:30,512 >>   train_samples            =       5160
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:07:30,512 >>   train_samples_per_second =    139.873
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:07:30,512 >>   train_steps_per_second   =      2.196
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4429, 'eval_samples_per_second': 352.971, 'eval_steps_per_second': 44.132, 'epoch': 5.0}
{'train_runtime': 184.4527, 'train_samples_per_second': 139.873, 'train_steps_per_second': 2.196, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 00:07:30 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 00:07:30,547 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:07:30,547 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 00:07:30,547 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:09, 55.19it/s]  2%|▏         | 12/505 [00:00<00:10, 48.64it/s]  3%|▎         | 17/505 [00:00<00:10, 46.86it/s]  4%|▍         | 22/505 [00:00<00:10, 46.24it/s]  5%|▌         | 27/505 [00:00<00:10, 45.80it/s]  6%|▋         | 32/505 [00:00<00:10, 45.50it/s]  7%|▋         | 37/505 [00:00<00:10, 45.29it/s]  8%|▊         | 42/505 [00:00<00:10, 44.79it/s]  9%|▉         | 47/505 [00:01<00:10, 44.24it/s] 10%|█         | 52/505 [00:01<00:10, 43.95it/s] 11%|█▏        | 57/505 [00:01<00:10, 44.03it/s] 12%|█▏        | 62/505 [00:01<00:10, 44.24it/s] 13%|█▎        | 67/505 [00:01<00:09, 44.37it/s] 14%|█▍        | 72/505 [00:01<00:09, 44.64it/s] 15%|█▌        | 77/505 [00:01<00:09, 44.67it/s] 16%|█▌        | 82/505 [00:01<00:09, 44.68it/s] 17%|█▋        | 87/505 [00:01<00:09, 44.41it/s] 18%|█▊        | 92/505 [00:02<00:09, 44.03it/s] 19%|█▉        | 97/505 [00:02<00:09, 43.87it/s] 20%|██        | 102/505 [00:02<00:09, 43.97it/s] 21%|██        | 107/505 [00:02<00:09, 44.17it/s] 22%|██▏       | 112/505 [00:02<00:08, 44.41it/s] 23%|██▎       | 117/505 [00:02<00:08, 44.52it/s] 24%|██▍       | 122/505 [00:02<00:08, 44.63it/s] 25%|██▌       | 127/505 [00:02<00:08, 44.60it/s] 26%|██▌       | 132/505 [00:02<00:08, 44.34it/s] 27%|██▋       | 137/505 [00:03<00:08, 43.99it/s] 28%|██▊       | 142/505 [00:03<00:08, 43.97it/s] 29%|██▉       | 147/505 [00:03<00:08, 43.90it/s] 30%|███       | 152/505 [00:03<00:08, 44.07it/s] 31%|███       | 157/505 [00:03<00:07, 44.29it/s] 32%|███▏      | 162/505 [00:03<00:07, 44.42it/s] 33%|███▎      | 167/505 [00:03<00:07, 44.59it/s] 34%|███▍      | 172/505 [00:03<00:07, 44.55it/s] 35%|███▌      | 177/505 [00:03<00:07, 44.34it/s] 36%|███▌      | 182/505 [00:04<00:07, 44.15it/s] 37%|███▋      | 187/505 [00:04<00:07, 44.00it/s] 38%|███▊      | 192/505 [00:04<00:07, 44.02it/s] 39%|███▉      | 197/505 [00:04<00:06, 44.09it/s] 40%|████      | 202/505 [00:04<00:06, 44.26it/s] 41%|████      | 207/505 [00:04<00:06, 44.42it/s] 42%|████▏     | 212/505 [00:04<00:06, 44.58it/s] 43%|████▎     | 217/505 [00:04<00:06, 44.48it/s] 44%|████▍     | 222/505 [00:04<00:06, 44.31it/s] 45%|████▍     | 227/505 [00:05<00:06, 44.15it/s] 46%|████▌     | 232/505 [00:05<00:06, 43.99it/s] 47%|████▋     | 237/505 [00:05<00:06, 44.01it/s] 48%|████▊     | 242/505 [00:05<00:05, 44.15it/s] 49%|████▉     | 247/505 [00:05<00:05, 44.27it/s] 50%|████▉     | 252/505 [00:05<00:05, 44.48it/s] 51%|█████     | 257/505 [00:05<00:05, 44.55it/s] 52%|█████▏    | 262/505 [00:05<00:05, 44.47it/s] 53%|█████▎    | 267/505 [00:05<00:05, 44.13it/s] 54%|█████▍    | 272/505 [00:06<00:05, 44.15it/s] 55%|█████▍    | 277/505 [00:06<00:05, 44.08it/s] 56%|█████▌    | 282/505 [00:06<00:05, 44.05it/s] 57%|█████▋    | 287/505 [00:06<00:04, 44.10it/s] 58%|█████▊    | 292/505 [00:06<00:04, 44.16it/s] 59%|█████▉    | 297/505 [00:06<00:04, 44.44it/s] 60%|█████▉    | 302/505 [00:06<00:04, 44.55it/s] 61%|██████    | 307/505 [00:06<00:04, 44.37it/s] 62%|██████▏   | 312/505 [00:07<00:04, 44.30it/s] 63%|██████▎   | 317/505 [00:07<00:04, 44.08it/s] 64%|██████▍   | 322/505 [00:07<00:04, 44.07it/s] 65%|██████▍   | 327/505 [00:07<00:04, 44.12it/s] 66%|██████▌   | 332/505 [00:07<00:03, 44.13it/s] 67%|██████▋   | 337/505 [00:07<00:03, 44.27it/s] 68%|██████▊   | 342/505 [00:07<00:03, 44.43it/s] 69%|██████▊   | 347/505 [00:07<00:03, 44.48it/s] 70%|██████▉   | 352/505 [00:07<00:03, 44.39it/s] 71%|███████   | 357/505 [00:08<00:03, 44.31it/s] 72%|███████▏  | 362/505 [00:08<00:03, 44.18it/s] 73%|███████▎  | 367/505 [00:08<00:03, 44.11it/s] 74%|███████▎  | 372/505 [00:08<00:03, 44.16it/s] 75%|███████▍  | 377/505 [00:08<00:02, 44.17it/s] 76%|███████▌  | 382/505 [00:08<00:02, 44.27it/s] 77%|███████▋  | 387/505 [00:08<00:02, 44.40it/s] 78%|███████▊  | 392/505 [00:08<00:02, 44.39it/s] 79%|███████▊  | 397/505 [00:08<00:02, 44.39it/s] 80%|███████▉  | 402/505 [00:09<00:02, 44.27it/s] 81%|████████  | 407/505 [00:09<00:02, 44.19it/s] 82%|████████▏ | 412/505 [00:09<00:02, 44.14it/s] 83%|████████▎ | 417/505 [00:09<00:01, 44.12it/s] 84%|████████▎ | 422/505 [00:09<00:01, 44.21it/s] 85%|████████▍ | 427/505 [00:09<00:01, 44.27it/s] 86%|████████▌ | 432/505 [00:09<00:01, 44.38it/s] 87%|████████▋ | 437/505 [00:09<00:01, 44.40it/s] 88%|████████▊ | 442/505 [00:09<00:01, 44.36it/s] 89%|████████▊ | 447/505 [00:10<00:01, 44.26it/s] 90%|████████▉ | 452/505 [00:10<00:01, 44.17it/s] 90%|█████████ | 457/505 [00:10<00:01, 44.23it/s] 91%|█████████▏| 462/505 [00:10<00:00, 44.16it/s] 92%|█████████▏| 467/505 [00:10<00:00, 44.11it/s] 93%|█████████▎| 472/505 [00:10<00:00, 44.28it/s] 94%|█████████▍| 477/505 [00:10<00:00, 44.41it/s] 95%|█████████▌| 482/505 [00:10<00:00, 44.36it/s] 96%|█████████▋| 487/505 [00:10<00:00, 44.28it/s] 97%|█████████▋| 492/505 [00:11<00:00, 44.23it/s] 98%|█████████▊| 497/505 [00:11<00:00, 44.24it/s] 99%|█████████▉| 502/505 [00:11<00:00, 44.24it/s]100%|██████████| 505/505 [00:11<00:00, 44.34it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:07:41,953 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:07:41,953 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:07:41,953 >>   eval_loss               =     1.2323
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:07:41,953 >>   eval_runtime            = 0:00:11.40
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:07:41,953 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:07:41,953 >>   eval_samples_per_second =    354.143
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:07:41,953 >>   eval_steps_per_second   =     44.279
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:07:41,953 >>   perplexity              =     3.4291
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:07:46,851 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:07:46,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:07:46,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:07:46,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:07:46,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:07:47,166 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:07:47,166 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:07:47,871 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:07:48,927 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:07:48,927 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:07:51,997 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:07:52,001 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:07:52,001 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:07:52,001 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:07:52,001 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:07:52,655 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:07:52,657 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:07:53,310 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:07:53,482 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:07:53,482 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-405
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-162
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-324
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-81
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-243
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:02,  1.72it/s]Extractor Predicting: 6it [00:03,  1.67it/s]Extractor Predicting: 7it [00:04,  1.71it/s]Extractor Predicting: 8it [00:04,  1.74it/s]Extractor Predicting: 9it [00:05,  1.73it/s]Extractor Predicting: 10it [00:05,  1.76it/s]Extractor Predicting: 11it [00:06,  1.74it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.69it/s]Extractor Predicting: 14it [00:08,  1.69it/s]Extractor Predicting: 15it [00:08,  1.70it/s]Extractor Predicting: 16it [00:09,  1.68it/s]Extractor Predicting: 17it [00:10,  1.66it/s]Extractor Predicting: 18it [00:10,  1.71it/s]Extractor Predicting: 19it [00:11,  1.70it/s]Extractor Predicting: 20it [00:11,  1.68it/s]Extractor Predicting: 21it [00:12,  1.70it/s]Extractor Predicting: 22it [00:12,  1.68it/s]Extractor Predicting: 23it [00:13,  1.64it/s]Extractor Predicting: 24it [00:14,  1.65it/s]Extractor Predicting: 25it [00:14,  1.68it/s]Extractor Predicting: 26it [00:15,  1.70it/s]Extractor Predicting: 27it [00:15,  1.74it/s]Extractor Predicting: 28it [00:16,  1.71it/s]Extractor Predicting: 29it [00:17,  1.73it/s]Extractor Predicting: 30it [00:17,  1.69it/s]Extractor Predicting: 31it [00:18,  1.72it/s]Extractor Predicting: 32it [00:18,  1.70it/s]Extractor Predicting: 33it [00:19,  1.69it/s]Extractor Predicting: 34it [00:20,  1.68it/s]Extractor Predicting: 35it [00:20,  1.68it/s]Extractor Predicting: 36it [00:21,  1.74it/s]Extractor Predicting: 37it [00:21,  1.80it/s]Extractor Predicting: 38it [00:22,  1.77it/s]Extractor Predicting: 39it [00:22,  1.80it/s]Extractor Predicting: 40it [00:23,  1.77it/s]Extractor Predicting: 41it [00:23,  1.75it/s]Extractor Predicting: 42it [00:24,  1.74it/s]Extractor Predicting: 43it [00:25,  1.78it/s]Extractor Predicting: 44it [00:25,  1.77it/s]Extractor Predicting: 45it [00:26,  1.79it/s]Extractor Predicting: 46it [00:26,  1.75it/s]Extractor Predicting: 47it [00:27,  1.74it/s]Extractor Predicting: 48it [00:28,  1.66it/s]Extractor Predicting: 49it [00:28,  1.71it/s]Extractor Predicting: 50it [00:29,  1.74it/s]Extractor Predicting: 51it [00:29,  1.77it/s]Extractor Predicting: 52it [00:30,  1.82it/s]Extractor Predicting: 53it [00:30,  1.80it/s]Extractor Predicting: 54it [00:31,  1.80it/s]Extractor Predicting: 55it [00:31,  1.78it/s]Extractor Predicting: 56it [00:32,  1.78it/s]Extractor Predicting: 57it [00:33,  1.79it/s]Extractor Predicting: 58it [00:33,  1.73it/s]Extractor Predicting: 59it [00:34,  1.74it/s]Extractor Predicting: 60it [00:34,  1.59it/s]Extractor Predicting: 61it [00:35,  1.67it/s]Extractor Predicting: 62it [00:36,  1.70it/s]Extractor Predicting: 63it [00:36,  1.68it/s]Extractor Predicting: 64it [00:37,  1.69it/s]Extractor Predicting: 65it [00:37,  1.68it/s]Extractor Predicting: 66it [00:38,  1.71it/s]Extractor Predicting: 67it [00:39,  1.71it/s]Extractor Predicting: 68it [00:39,  1.67it/s]Extractor Predicting: 69it [00:40,  1.63it/s]Extractor Predicting: 70it [00:40,  1.67it/s]Extractor Predicting: 71it [00:41,  1.66it/s]Extractor Predicting: 72it [00:42,  1.61it/s]Extractor Predicting: 73it [00:42,  1.61it/s]Extractor Predicting: 74it [00:43,  1.55it/s]Extractor Predicting: 75it [00:44,  1.60it/s]Extractor Predicting: 76it [00:44,  1.63it/s]Extractor Predicting: 77it [00:45,  1.62it/s]Extractor Predicting: 78it [00:45,  1.64it/s]Extractor Predicting: 79it [00:46,  1.65it/s]Extractor Predicting: 80it [00:47,  1.65it/s]Extractor Predicting: 81it [00:47,  1.67it/s]Extractor Predicting: 82it [00:48,  1.71it/s]Extractor Predicting: 83it [00:48,  1.67it/s]Extractor Predicting: 84it [00:49,  1.68it/s]Extractor Predicting: 85it [00:50,  1.68it/s]Extractor Predicting: 86it [00:50,  1.70it/s]Extractor Predicting: 87it [00:51,  1.74it/s]Extractor Predicting: 88it [00:51,  1.74it/s]Extractor Predicting: 89it [00:52,  1.73it/s]Extractor Predicting: 90it [00:52,  1.74it/s]Extractor Predicting: 91it [00:53,  1.74it/s]Extractor Predicting: 92it [00:53,  1.76it/s]Extractor Predicting: 93it [00:54,  1.75it/s]Extractor Predicting: 94it [00:55,  1.72it/s]Extractor Predicting: 95it [00:55,  1.70it/s]Extractor Predicting: 96it [00:56,  1.68it/s]Extractor Predicting: 97it [00:56,  1.68it/s]Extractor Predicting: 98it [00:57,  1.68it/s]Extractor Predicting: 99it [00:58,  1.66it/s]Extractor Predicting: 100it [00:58,  1.67it/s]Extractor Predicting: 101it [00:59,  1.63it/s]Extractor Predicting: 102it [01:00,  1.64it/s]Extractor Predicting: 103it [01:00,  1.65it/s]Extractor Predicting: 104it [01:01,  1.69it/s]Extractor Predicting: 105it [01:01,  1.68it/s]Extractor Predicting: 106it [01:02,  1.72it/s]Extractor Predicting: 107it [01:02,  1.72it/s]Extractor Predicting: 108it [01:03,  1.74it/s]Extractor Predicting: 109it [01:04,  1.75it/s]Extractor Predicting: 110it [01:04,  1.72it/s]Extractor Predicting: 111it [01:05,  1.68it/s]Extractor Predicting: 112it [01:05,  1.73it/s]Extractor Predicting: 113it [01:06,  1.71it/s]Extractor Predicting: 114it [01:07,  1.69it/s]Extractor Predicting: 115it [01:07,  1.74it/s]Extractor Predicting: 116it [01:08,  1.74it/s]Extractor Predicting: 117it [01:08,  1.73it/s]Extractor Predicting: 118it [01:09,  1.66it/s]Extractor Predicting: 119it [01:09,  1.70it/s]Extractor Predicting: 120it [01:10,  1.71it/s]Extractor Predicting: 121it [01:11,  1.68it/s]Extractor Predicting: 122it [01:11,  1.69it/s]Extractor Predicting: 123it [01:12,  1.72it/s]Extractor Predicting: 124it [01:12,  1.71it/s]Extractor Predicting: 125it [01:13,  1.55it/s]Extractor Predicting: 126it [01:14,  1.61it/s]Extractor Predicting: 127it [01:14,  1.56it/s]Extractor Predicting: 128it [01:15,  1.55it/s]Extractor Predicting: 129it [01:16,  1.56it/s]Extractor Predicting: 130it [01:16,  1.56it/s]Extractor Predicting: 131it [01:17,  1.52it/s]Extractor Predicting: 132it [01:18,  1.53it/s]Extractor Predicting: 133it [01:18,  1.54it/s]Extractor Predicting: 134it [01:19,  1.59it/s]Extractor Predicting: 135it [01:20,  1.59it/s]Extractor Predicting: 136it [01:20,  1.46it/s]Extractor Predicting: 137it [01:21,  1.53it/s]Extractor Predicting: 138it [01:22,  1.51it/s]Extractor Predicting: 139it [01:22,  1.53it/s]Extractor Predicting: 140it [01:23,  1.53it/s]Extractor Predicting: 141it [01:24,  1.55it/s]Extractor Predicting: 142it [01:24,  1.57it/s]Extractor Predicting: 143it [01:25,  1.57it/s]Extractor Predicting: 144it [01:25,  1.58it/s]Extractor Predicting: 145it [01:26,  1.56it/s]Extractor Predicting: 146it [01:27,  1.58it/s]Extractor Predicting: 147it [01:27,  1.58it/s]Extractor Predicting: 148it [01:28,  1.54it/s]Extractor Predicting: 149it [01:29,  1.61it/s]Extractor Predicting: 149it [01:29,  1.67it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:09:30,693 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:09:30,695 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:09:30,695 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:09:30,695 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:09:30,695 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:09:31,332 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:09:31,335 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:09:31,892 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:09:32,931 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:09:32,931 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:09:36,014 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:09:36,030 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:09:36,030 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:09:36,030 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:09:36,030 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:09:36,669 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:09:36,670 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:09:37,273 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:09:37,447 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:09:37,447 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.68it/s]Extractor Predicting: 4it [00:02,  1.80it/s]Extractor Predicting: 5it [00:02,  1.77it/s]Extractor Predicting: 6it [00:03,  1.79it/s]Extractor Predicting: 7it [00:04,  1.78it/s]Extractor Predicting: 8it [00:04,  1.85it/s]Extractor Predicting: 9it [00:04,  1.95it/s]Extractor Predicting: 10it [00:05,  1.93it/s]Extractor Predicting: 11it [00:06,  1.93it/s]Extractor Predicting: 12it [00:06,  1.93it/s]Extractor Predicting: 13it [00:06,  2.03it/s]Extractor Predicting: 14it [00:07,  2.01it/s]Extractor Predicting: 15it [00:07,  2.03it/s]Extractor Predicting: 16it [00:08,  2.03it/s]Extractor Predicting: 17it [00:08,  2.01it/s]Extractor Predicting: 18it [00:09,  1.96it/s]Extractor Predicting: 19it [00:10,  1.91it/s]Extractor Predicting: 20it [00:10,  1.92it/s]Extractor Predicting: 21it [00:11,  1.93it/s]Extractor Predicting: 22it [00:11,  1.95it/s]Extractor Predicting: 23it [00:12,  1.98it/s]Extractor Predicting: 24it [00:12,  1.96it/s]Extractor Predicting: 25it [00:13,  1.99it/s]Extractor Predicting: 26it [00:13,  1.93it/s]Extractor Predicting: 27it [00:14,  2.00it/s]Extractor Predicting: 28it [00:14,  2.01it/s]Extractor Predicting: 29it [00:15,  1.98it/s]Extractor Predicting: 30it [00:15,  1.98it/s]Extractor Predicting: 31it [00:16,  2.01it/s]Extractor Predicting: 32it [00:16,  2.00it/s]Extractor Predicting: 33it [00:17,  1.94it/s]Extractor Predicting: 34it [00:17,  1.90it/s]Extractor Predicting: 35it [00:18,  1.87it/s]Extractor Predicting: 36it [00:18,  1.95it/s]Extractor Predicting: 37it [00:19,  1.92it/s]Extractor Predicting: 38it [00:19,  1.89it/s]Extractor Predicting: 39it [00:20,  1.91it/s]Extractor Predicting: 40it [00:20,  1.87it/s]Extractor Predicting: 41it [00:21,  1.85it/s]Extractor Predicting: 42it [00:21,  1.88it/s]Extractor Predicting: 43it [00:22,  1.91it/s]Extractor Predicting: 44it [00:22,  1.90it/s]Extractor Predicting: 45it [00:23,  1.73it/s]Extractor Predicting: 46it [00:24,  1.71it/s]Extractor Predicting: 47it [00:24,  1.74it/s]Extractor Predicting: 48it [00:25,  1.78it/s]Extractor Predicting: 49it [00:25,  1.83it/s]Extractor Predicting: 50it [00:26,  1.84it/s]Extractor Predicting: 51it [00:26,  1.89it/s]Extractor Predicting: 52it [00:27,  1.85it/s]Extractor Predicting: 53it [00:27,  1.87it/s]Extractor Predicting: 54it [00:28,  1.87it/s]Extractor Predicting: 55it [00:29,  1.85it/s]Extractor Predicting: 56it [00:29,  1.79it/s]Extractor Predicting: 57it [00:30,  1.79it/s]Extractor Predicting: 58it [00:30,  1.82it/s]Extractor Predicting: 59it [00:31,  1.79it/s]Extractor Predicting: 60it [00:31,  1.80it/s]Extractor Predicting: 61it [00:32,  1.81it/s]Extractor Predicting: 62it [00:32,  1.85it/s]Extractor Predicting: 63it [00:33,  1.81it/s]Extractor Predicting: 64it [00:34,  1.83it/s]Extractor Predicting: 65it [00:34,  1.86it/s]Extractor Predicting: 66it [00:35,  1.87it/s]Extractor Predicting: 67it [00:35,  1.85it/s]Extractor Predicting: 68it [00:36,  1.85it/s]Extractor Predicting: 69it [00:36,  1.81it/s]Extractor Predicting: 70it [00:37,  1.78it/s]Extractor Predicting: 71it [00:37,  1.78it/s]Extractor Predicting: 72it [00:38,  1.81it/s]Extractor Predicting: 73it [00:39,  1.79it/s]Extractor Predicting: 74it [00:39,  1.81it/s]Extractor Predicting: 75it [00:40,  1.84it/s]Extractor Predicting: 76it [00:40,  1.81it/s]Extractor Predicting: 77it [00:41,  1.82it/s]Extractor Predicting: 78it [00:41,  1.76it/s]Extractor Predicting: 79it [00:42,  1.80it/s]Extractor Predicting: 80it [00:42,  1.84it/s]Extractor Predicting: 81it [00:43,  1.80it/s]Extractor Predicting: 82it [00:44,  1.77it/s]Extractor Predicting: 83it [00:44,  1.77it/s]Extractor Predicting: 84it [00:45,  1.79it/s]Extractor Predicting: 85it [00:45,  1.77it/s]Extractor Predicting: 86it [00:46,  1.75it/s]Extractor Predicting: 87it [00:46,  1.79it/s]Extractor Predicting: 88it [00:47,  1.63it/s]Extractor Predicting: 89it [00:48,  1.73it/s]Extractor Predicting: 90it [00:48,  1.76it/s]Extractor Predicting: 91it [00:49,  1.78it/s]Extractor Predicting: 92it [00:49,  1.80it/s]Extractor Predicting: 93it [00:50,  1.86it/s]Extractor Predicting: 94it [00:50,  1.87it/s]Extractor Predicting: 95it [00:51,  1.86it/s]Extractor Predicting: 96it [00:51,  1.90it/s]Extractor Predicting: 97it [00:52,  1.93it/s]Extractor Predicting: 98it [00:52,  1.90it/s]Extractor Predicting: 99it [00:53,  1.87it/s]Extractor Predicting: 100it [00:53,  1.90it/s]Extractor Predicting: 101it [00:54,  1.87it/s]Extractor Predicting: 102it [00:55,  1.83it/s]Extractor Predicting: 103it [00:55,  1.81it/s]Extractor Predicting: 104it [00:56,  1.87it/s]Extractor Predicting: 105it [00:56,  1.80it/s]Extractor Predicting: 106it [00:57,  1.84it/s]Extractor Predicting: 107it [00:57,  1.85it/s]Extractor Predicting: 108it [00:58,  1.81it/s]Extractor Predicting: 109it [00:58,  1.80it/s]Extractor Predicting: 110it [00:59,  1.83it/s]Extractor Predicting: 111it [00:59,  1.82it/s]Extractor Predicting: 112it [01:00,  1.82it/s]Extractor Predicting: 113it [01:01,  1.85it/s]Extractor Predicting: 114it [01:01,  1.85it/s]Extractor Predicting: 115it [01:02,  1.86it/s]Extractor Predicting: 116it [01:02,  1.89it/s]Extractor Predicting: 117it [01:03,  1.90it/s]Extractor Predicting: 118it [01:03,  1.95it/s]Extractor Predicting: 119it [01:04,  1.96it/s]Extractor Predicting: 120it [01:04,  1.95it/s]Extractor Predicting: 121it [01:05,  1.95it/s]Extractor Predicting: 122it [01:05,  1.93it/s]Extractor Predicting: 123it [01:06,  1.90it/s]Extractor Predicting: 124it [01:06,  1.92it/s]Extractor Predicting: 125it [01:07,  1.91it/s]Extractor Predicting: 126it [01:07,  1.88it/s]Extractor Predicting: 127it [01:08,  1.85it/s]Extractor Predicting: 128it [01:08,  1.82it/s]Extractor Predicting: 129it [01:09,  1.79it/s]Extractor Predicting: 130it [01:10,  1.76it/s]Extractor Predicting: 131it [01:10,  1.73it/s]Extractor Predicting: 132it [01:11,  1.76it/s]Extractor Predicting: 133it [01:11,  1.77it/s]Extractor Predicting: 134it [01:12,  1.82it/s]Extractor Predicting: 135it [01:12,  1.74it/s]Extractor Predicting: 136it [01:13,  1.73it/s]Extractor Predicting: 137it [01:14,  1.72it/s]Extractor Predicting: 138it [01:14,  1.75it/s]Extractor Predicting: 139it [01:15,  1.75it/s]Extractor Predicting: 140it [01:15,  1.71it/s]Extractor Predicting: 141it [01:16,  1.72it/s]Extractor Predicting: 142it [01:17,  1.72it/s]Extractor Predicting: 143it [01:17,  1.75it/s]Extractor Predicting: 144it [01:18,  1.73it/s]Extractor Predicting: 145it [01:18,  1.72it/s]Extractor Predicting: 146it [01:19,  1.73it/s]Extractor Predicting: 147it [01:19,  1.74it/s]Extractor Predicting: 148it [01:20,  1.75it/s]Extractor Predicting: 149it [01:20,  1.78it/s]Extractor Predicting: 150it [01:21,  1.77it/s]Extractor Predicting: 151it [01:22,  1.76it/s]Extractor Predicting: 152it [01:22,  1.76it/s]Extractor Predicting: 153it [01:23,  1.73it/s]Extractor Predicting: 154it [01:23,  1.73it/s]Extractor Predicting: 155it [01:24,  1.75it/s]Extractor Predicting: 156it [01:24,  1.80it/s]Extractor Predicting: 157it [01:25,  1.88it/s]Extractor Predicting: 158it [01:25,  1.88it/s]Extractor Predicting: 159it [01:26,  1.88it/s]Extractor Predicting: 160it [01:27,  1.89it/s]Extractor Predicting: 161it [01:27,  1.94it/s]Extractor Predicting: 162it [01:28,  1.89it/s]Extractor Predicting: 163it [01:28,  1.86it/s]Extractor Predicting: 164it [01:29,  1.88it/s]Extractor Predicting: 165it [01:29,  1.93it/s]Extractor Predicting: 166it [01:30,  1.92it/s]Extractor Predicting: 167it [01:30,  1.99it/s]Extractor Predicting: 168it [01:31,  2.01it/s]Extractor Predicting: 169it [01:31,  2.03it/s]Extractor Predicting: 170it [01:32,  1.97it/s]Extractor Predicting: 171it [01:32,  1.88it/s]Extractor Predicting: 172it [01:33,  1.84it/s]Extractor Predicting: 173it [01:33,  1.91it/s]Extractor Predicting: 173it [01:33,  1.84it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:11:18,592 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:11:18,595 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:11:18,595 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:11:18,596 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:11:18,596 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:11:19,349 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:11:19,349 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:11:19,731 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:11:20,775 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:11:20,775 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:11:23,000 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:11:23,005 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:11:23,005 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:11:23,005 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:11:23,005 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:11:23,861 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:11:23,862 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:11:24,226 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:11:24,399 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:11:24,399 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.80it/s]Extractor Predicting: 2it [00:01,  1.72it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.68it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.59it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.62it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:07,  1.63it/s]Extractor Predicting: 14it [00:08,  1.71it/s]Extractor Predicting: 15it [00:08,  1.81it/s]Extractor Predicting: 16it [00:09,  1.92it/s]Extractor Predicting: 17it [00:09,  2.03it/s]Extractor Predicting: 18it [00:10,  2.08it/s]Extractor Predicting: 19it [00:10,  2.09it/s]Extractor Predicting: 20it [00:11,  2.12it/s]Extractor Predicting: 21it [00:11,  2.16it/s]Extractor Predicting: 22it [00:12,  2.12it/s]Extractor Predicting: 23it [00:12,  2.11it/s]Extractor Predicting: 24it [00:13,  2.10it/s]Extractor Predicting: 25it [00:13,  2.10it/s]Extractor Predicting: 26it [00:14,  2.15it/s]Extractor Predicting: 27it [00:14,  2.09it/s]Extractor Predicting: 28it [00:15,  2.11it/s]Extractor Predicting: 29it [00:15,  2.13it/s]Extractor Predicting: 30it [00:15,  2.17it/s]Extractor Predicting: 31it [00:16,  2.20it/s]Extractor Predicting: 32it [00:16,  2.19it/s]Extractor Predicting: 33it [00:17,  2.20it/s]Extractor Predicting: 34it [00:17,  2.16it/s]Extractor Predicting: 35it [00:18,  2.17it/s]Extractor Predicting: 36it [00:18,  2.10it/s]Extractor Predicting: 37it [00:19,  2.06it/s]Extractor Predicting: 38it [00:19,  2.14it/s]Extractor Predicting: 39it [00:20,  2.13it/s]Extractor Predicting: 40it [00:20,  2.14it/s]Extractor Predicting: 41it [00:21,  2.12it/s]Extractor Predicting: 42it [00:21,  2.17it/s]Extractor Predicting: 43it [00:21,  2.12it/s]Extractor Predicting: 44it [00:22,  1.94it/s]Extractor Predicting: 45it [00:23,  1.82it/s]Extractor Predicting: 46it [00:23,  1.74it/s]Extractor Predicting: 47it [00:24,  1.71it/s]Extractor Predicting: 48it [00:25,  1.68it/s]Extractor Predicting: 49it [00:25,  1.68it/s]Extractor Predicting: 50it [00:26,  1.66it/s]Extractor Predicting: 51it [00:26,  1.65it/s]Extractor Predicting: 52it [00:27,  1.63it/s]Extractor Predicting: 53it [00:28,  1.62it/s]Extractor Predicting: 54it [00:28,  1.62it/s]Extractor Predicting: 55it [00:29,  1.64it/s]Extractor Predicting: 56it [00:30,  1.61it/s]Extractor Predicting: 57it [00:30,  1.61it/s]Extractor Predicting: 58it [00:31,  1.61it/s]Extractor Predicting: 59it [00:31,  1.94it/s]Extractor Predicting: 59it [00:31,  1.87it/s]
[INFO|configuration_utils.py:515] 2023-08-28 00:11:56,954 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:11:56,955 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 00:11:56,963 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:11:56,964 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 00:11:56,966 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 00:11:59,981 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 00:11:59,987 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 00:12:00,002 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:12:00,003 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 00:12:00,009 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:12:00,012 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:12:00,012 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:12:00,012 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:12:00,012 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:12:00,012 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:12:00,012 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 00:12:00,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:00,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:01,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:02,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:03,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:03,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:04,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:04,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:05,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:06,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:06,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:07,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:08,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:08,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:09,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:09,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:10,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:11,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:11,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:12,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:12,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:13,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:14,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:14,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:15<02:17, 15.27s/it][WARNING|generation_utils.py:914] 2023-08-28 00:12:15,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:16,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:16,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:17,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:17,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:18,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:19,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:19,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:20,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:20,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:21,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:22,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:22,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:23,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:24,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:24,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:25,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:26,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:26,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:27,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:28,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:28,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:29,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:29,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:30,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:30,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:31,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:31<02:07, 15.93s/it][WARNING|generation_utils.py:914] 2023-08-28 00:12:31,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:32,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:33,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:33,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:34,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:35,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:35,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:36,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:37,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:37,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:38,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:38,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:39,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:40,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:40,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:41,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:41,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:42,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:43,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:43,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:44,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:44,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:45,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:46,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:46,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:47,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:47,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:48,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:49,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:49,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:50,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:50,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:51,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:52,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:52,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:53,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:53,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:54,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:54,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:55,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:56,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:56,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:57,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:57,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:58,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:58<02:27, 21.12s/it][WARNING|generation_utils.py:914] 2023-08-28 00:12:59,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:12:59,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:00,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:01,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:02,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:02,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:03,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:04,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:04,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:05,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:06,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:06,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:07,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:08,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:08,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:09,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:10,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:10,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:11,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:12,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:12,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:13,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:14,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:14,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:15<01:55, 19.22s/it][WARNING|generation_utils.py:914] 2023-08-28 00:13:15,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:16,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:16,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:17,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:18,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:18,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:19,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:19,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:20,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:20,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:21,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:22,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:22,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:23,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:23,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:24,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:25,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:25,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:26,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:26,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:27,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:28,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:28,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:29,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:30,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:30,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:31,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:31<01:30, 18.13s/it][WARNING|generation_utils.py:914] 2023-08-28 00:13:31,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:32,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:33,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:33,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:34,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:34,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:35,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:36,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:36,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:37,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:37,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:38,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:39,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:39,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:40,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:41,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:41,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:42,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:42,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:43,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:43,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:44,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:45,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:45,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:46,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:46<01:08, 17.14s/it][WARNING|generation_utils.py:914] 2023-08-28 00:13:46,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:47,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:48,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:48,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:49,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:49,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:51,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:51,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:52,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:52,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:53,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:54,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:54,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:55,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:56,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:56,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:57,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:57,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:58,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:59,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:13:59,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:00,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:00,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:01,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:01<00:49, 16.50s/it][WARNING|generation_utils.py:914] 2023-08-28 00:14:02,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:02,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:03,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:03,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:04,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:04,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:05,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:05,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:06,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:06,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:07,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:08,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:08,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:09,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:09,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:10,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:10,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:11,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:11,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:12,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:13,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:13,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:14,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:14,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:15<00:30, 15.47s/it][WARNING|generation_utils.py:914] 2023-08-28 00:14:15,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:15,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:16,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:17,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:18,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:18,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:19,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:19,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:20,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:21,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:22,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:22,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:23,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:24,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:24,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:25,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:25,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:26,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:27,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:27,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:28,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:28,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:29<00:15, 15.02s/it][WARNING|generation_utils.py:914] 2023-08-28 00:14:29,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:30,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:30,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:31,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:31,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:32,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:33,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:33,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:34,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:35,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:35,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:36,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:36,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:37,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:38,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:38,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:39,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:40,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:40,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:41,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:41,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:42,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 00:14:43,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:43<00:00, 14.81s/it]Generating: 100%|██████████| 10/10 [02:43<00:00, 16.35s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:14:50,070 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:14:50,074 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:14:50,074 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:14:50,074 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:14:50,074 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:14:50,762 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:14:50,763 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:14:51,338 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:14:52,432 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:14:52,433 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:14:55,377 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:14:55,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:14:55,384 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:14:55,384 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:14:55,384 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:14:56,069 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:14:56,070 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:14:56,706 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:14:56,878 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:14:56,878 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : given name .', 'success_rate': 0.80859375, 'errors': {'', "('William', 'given name', '', 'He was born in the province of Oise , his father being William , Duke of Normandy , 1st Earl of Normandy , and his mother being Mary .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 201, 'raw': 288}
{'target': 600, 'success': 226, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 287, 'raw': 416}
{'target': 600, 'success': 310, 'raw': 448}
{'target': 600, 'success': 333, 'raw': 480}
{'target': 600, 'success': 351, 'raw': 512}
{'target': 600, 'success': 377, 'raw': 544}
{'target': 600, 'success': 403, 'raw': 576}
{'target': 600, 'success': 424, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 468, 'raw': 672}
{'target': 600, 'success': 486, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 532, 'raw': 768}
{'target': 600, 'success': 554, 'raw': 800}
{'target': 600, 'success': 576, 'raw': 832}
{'target': 600, 'success': 601, 'raw': 864}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.6956018518518519, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Brazil', 'languages spoken, written or signed', '', 'Many languages are spoken in Brazil , and there are various communities and organizations advocating for a stronger free and open society .')", "('Estonia', 'languages spoken, written or signed', '', 'The language of Estonia is Estonian , the official language of the Republic of the Union ( DSO ) ) .')"}}
['Relation : lowest point . Context : Later in the year ( 1143 ) , Puyi and his allies made a conquest over the eastern part of the province of Sinti , which was occupied by the Franks and Normans . Head Entity : Sinti , Tail Entity : southern part .\n']
['Relation : lowest point . Context : Later in the year ( 1143 ) , Puyi and his allies made a conquest over the eastern part of the province of Sinti , which was occupied by the Franks and Normans . Head Entity : Sinti , Tail Entity : southern part .\n', 'Relation : lowest point . Context : After the death of Emperor Xuanzong his elder son Liu Bang was the first emperor to die at the age of only two . Head Entity : emperor , Tail Entity : highest point .\n']
{'target': 600, 'success': 12, 'raw': 32}
{'target': 600, 'success': 23, 'raw': 64}
{'target': 600, 'success': 32, 'raw': 96}
{'target': 600, 'success': 44, 'raw': 128}
{'target': 600, 'success': 55, 'raw': 160}
{'target': 600, 'success': 70, 'raw': 192}
{'target': 600, 'success': 86, 'raw': 224}
{'target': 600, 'success': 102, 'raw': 256}
{'target': 600, 'success': 115, 'raw': 288}
{'target': 600, 'success': 125, 'raw': 320}
{'target': 600, 'success': 141, 'raw': 352}
{'target': 600, 'success': 158, 'raw': 384}
{'target': 600, 'success': 176, 'raw': 416}
{'target': 600, 'success': 192, 'raw': 448}
{'target': 600, 'success': 206, 'raw': 480}
{'target': 600, 'success': 221, 'raw': 512}
{'target': 600, 'success': 233, 'raw': 544}
{'target': 600, 'success': 249, 'raw': 576}
{'target': 600, 'success': 265, 'raw': 608}
{'target': 600, 'success': 273, 'raw': 640}
{'target': 600, 'success': 288, 'raw': 672}
{'target': 600, 'success': 308, 'raw': 704}
{'target': 600, 'success': 317, 'raw': 736}
{'target': 600, 'success': 332, 'raw': 768}
{'target': 600, 'success': 342, 'raw': 800}
{'target': 600, 'success': 353, 'raw': 832}
{'target': 600, 'success': 369, 'raw': 864}
{'target': 600, 'success': 385, 'raw': 896}
{'target': 600, 'success': 394, 'raw': 928}
{'target': 600, 'success': 409, 'raw': 960}
{'target': 600, 'success': 423, 'raw': 992}
{'target': 600, 'success': 432, 'raw': 1024}
{'target': 600, 'success': 446, 'raw': 1056}
{'target': 600, 'success': 460, 'raw': 1088}
{'target': 600, 'success': 473, 'raw': 1120}
{'target': 600, 'success': 486, 'raw': 1152}
{'target': 600, 'success': 498, 'raw': 1184}
{'target': 600, 'success': 512, 'raw': 1216}
{'target': 600, 'success': 522, 'raw': 1248}
{'target': 600, 'success': 533, 'raw': 1280}
{'target': 600, 'success': 550, 'raw': 1312}
{'target': 600, 'success': 560, 'raw': 1344}
{'target': 600, 'success': 572, 'raw': 1376}
{'target': 600, 'success': 584, 'raw': 1408}
{'target': 600, 'success': 600, 'raw': 1440}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.4166666666666667, 'errors': {'', "('2004 FIFA World Cup', 'lowest point', '', 'The next day , he made a triumphant recovery and made his international debut at the 2004 FIFA World Cup against Ecuador .')", 'not enough values to unpack (expected 2, got 1)', "('New York Giants', 'lowest point', '', 'In the second season , a team led by George Foreman and David Southee were allowed to make another bowl game against the New York Giants .')", 'too many values to unpack (expected 2)', "('lowest point', 'lowest point', '', 'For comparison , in 1998 , it was the lowest point per square mile since 1960 , when it was 1 . 10 .')", "('63', 'lowest point', '', 'On the night of 9 November 2014 , the Southeastern Conference led by Jaylen Brown was led by Duke Blue Devils in a 63 66 victory over the Southeast Division rival St. Charles .')", "('second', 'lowest point', '', 'He is currently playing with fellow former New Zealand prop Tom Paine in the Championship side Auckland Roosters , and he moved up the rankings to second .')", "('Manchester', 'lowest point', '', 'The area was home to the city of Manchester ( now Manchester City ) , founded in 1859 by the English Civil War hero Sir Walter Scott .')"}}
['Relation : mother . Context : Later in Life , he came to love the beauty and natural beauty of the forests at the end of the third trimester , when she suffered death . Head Entity : forests at the end of the third trimester , Tail Entity : Mother .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : mother .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('king', 'mother', '', 'The first male and last female monarch of the Kingdom of Mexico , the king of the Andes ( d.')"}}
['Relation : mouth of the watercourse . Context : Later in the year ( 1143 ) , Puyi and his allies made atone for the fall of the last King of the Puebla , Puzco . Head Entity : Puyi , Tail Entity : Puducese .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 200, 'raw': 288}
{'target': 600, 'success': 221, 'raw': 320}
{'target': 600, 'success': 241, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 289, 'raw': 416}
{'target': 600, 'success': 311, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 357, 'raw': 512}
{'target': 600, 'success': 383, 'raw': 544}
{'target': 600, 'success': 404, 'raw': 576}
{'target': 600, 'success': 425, 'raw': 608}
{'target': 600, 'success': 440, 'raw': 640}
{'target': 600, 'success': 463, 'raw': 672}
{'target': 600, 'success': 488, 'raw': 704}
{'target': 600, 'success': 509, 'raw': 736}
{'target': 600, 'success': 536, 'raw': 768}
{'target': 600, 'success': 558, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 606, 'raw': 864}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.7013888888888888, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n']
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n', 'Relation : genre . Context : Eileen McClelland ( born June 24 , 1953 ) is an American radio talk show host who has appeared as a regular on both the Howard Stern Show and the Morning Mix . Head Entity : Howard Stern Show , Tail Entity : television talk show .\n']
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n', 'Relation : genre . Context : Eileen McClelland ( born June 24 , 1953 ) is an American radio talk show host who has appeared as a regular on both the Howard Stern Show and the Morning Mix . Head Entity : Howard Stern Show , Tail Entity : television talk show .\n', 'Relation : genre . Context : This film explores the social , social structure of the New York city of Times Square , which is populated and industrialized by wealthy Manhattanites . Head Entity : Times Square , Tail Entity : New York City .\n']
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 320, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 468, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 570, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : genre .', 'success_rate': 0.76625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.8020833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('The albums', 'is a list of', '', 'The albums songs , songs played and discography of the albums members , the artist , are listed in alphabetical order .')", "('programming languages', 'is a list of', '', 'It can be used primarily in conjunction with other programming languages such as .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8111979166666666, 'errors': {'', "('Pluto', 'located on astronomical body', '', 'In the past , Pluto has been considered to be a binary system inhabited by one giant being , as well as being the first to be named after Pluto .')", 'too many values to unpack (expected 2)', "('Its orbit', 'located on astronomical body', '', 'Its orbit shows an eccentricity of 0 . 18 and an inclination of 8 degrees from the plane of the ecliptic .')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('it', 'manufacturer', '', 'It is known in Japan for its high quality and performance components , along with its long range of performance airsoft s.')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8179347826086957, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/1_ext.jsonl'}}
estimate vocab size: 12731
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12831, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.70it/s]Extractor Estimating: 2it [00:01,  1.48it/s]Extractor Estimating: 3it [00:02,  1.45it/s]Extractor Estimating: 4it [00:02,  1.48it/s]Extractor Estimating: 5it [00:03,  1.54it/s]Extractor Estimating: 6it [00:03,  1.57it/s]Extractor Estimating: 7it [00:04,  1.54it/s]Extractor Estimating: 8it [00:05,  1.55it/s]Extractor Estimating: 9it [00:05,  1.60it/s]Extractor Estimating: 10it [00:06,  1.61it/s]Extractor Estimating: 11it [00:06,  1.65it/s]Extractor Estimating: 12it [00:07,  1.68it/s]Extractor Estimating: 13it [00:08,  1.65it/s]Extractor Estimating: 14it [00:08,  1.61it/s]Extractor Estimating: 15it [00:09,  1.63it/s]Extractor Estimating: 16it [00:10,  1.61it/s]Extractor Estimating: 17it [00:10,  1.58it/s]Extractor Estimating: 18it [00:11,  1.59it/s]Extractor Estimating: 19it [00:12,  1.57it/s]Extractor Estimating: 20it [00:12,  1.62it/s]Extractor Estimating: 21it [00:13,  1.64it/s]Extractor Estimating: 22it [00:13,  1.65it/s]Extractor Estimating: 23it [00:14,  1.56it/s]Extractor Estimating: 24it [00:15,  1.62it/s]Extractor Estimating: 25it [00:15,  1.69it/s]Extractor Estimating: 26it [00:16,  1.75it/s]Extractor Estimating: 27it [00:16,  1.73it/s]Extractor Estimating: 28it [00:17,  1.75it/s]Extractor Estimating: 29it [00:17,  1.78it/s]Extractor Estimating: 30it [00:18,  1.79it/s]Extractor Estimating: 31it [00:18,  1.77it/s]Extractor Estimating: 32it [00:19,  1.78it/s]Extractor Estimating: 33it [00:20,  1.79it/s]Extractor Estimating: 34it [00:20,  1.81it/s]Extractor Estimating: 35it [00:21,  1.75it/s]Extractor Estimating: 36it [00:21,  1.75it/s]Extractor Estimating: 37it [00:22,  1.73it/s]Extractor Estimating: 38it [00:22,  1.75it/s]Extractor Estimating: 39it [00:23,  1.80it/s]Extractor Estimating: 40it [00:24,  1.75it/s]Extractor Estimating: 41it [00:24,  1.73it/s]Extractor Estimating: 42it [00:25,  1.76it/s]Extractor Estimating: 43it [00:25,  1.65it/s]Extractor Estimating: 44it [00:26,  1.70it/s]Extractor Estimating: 45it [00:26,  1.76it/s]Extractor Estimating: 46it [00:27,  1.74it/s]Extractor Estimating: 47it [00:28,  1.77it/s]Extractor Estimating: 48it [00:28,  1.80it/s]Extractor Estimating: 49it [00:29,  1.83it/s]Extractor Estimating: 50it [00:29,  1.83it/s]Extractor Estimating: 51it [00:30,  1.75it/s]Extractor Estimating: 52it [00:30,  1.70it/s]Extractor Estimating: 53it [00:31,  1.68it/s]Extractor Estimating: 54it [00:32,  1.65it/s]Extractor Estimating: 55it [00:32,  1.62it/s]Extractor Estimating: 56it [00:33,  1.59it/s]Extractor Estimating: 57it [00:34,  1.56it/s]Extractor Estimating: 58it [00:34,  1.57it/s]Extractor Estimating: 59it [00:35,  1.50it/s]Extractor Estimating: 60it [00:36,  1.52it/s]Extractor Estimating: 61it [00:36,  1.51it/s]Extractor Estimating: 62it [00:37,  1.55it/s]Extractor Estimating: 63it [00:38,  1.52it/s]Extractor Estimating: 64it [00:38,  1.55it/s]Extractor Estimating: 65it [00:39,  1.59it/s]Extractor Estimating: 66it [00:39,  1.56it/s]Extractor Estimating: 67it [00:40,  1.57it/s]Extractor Estimating: 68it [00:41,  1.57it/s]Extractor Estimating: 69it [00:41,  1.57it/s]Extractor Estimating: 70it [00:42,  1.59it/s]Extractor Estimating: 71it [00:43,  1.57it/s]Extractor Estimating: 72it [00:43,  1.58it/s]Extractor Estimating: 73it [00:44,  1.57it/s]Extractor Estimating: 74it [00:45,  1.58it/s]Extractor Estimating: 75it [00:45,  1.58it/s]Extractor Estimating: 76it [00:46,  1.52it/s]Extractor Estimating: 77it [00:47,  1.54it/s]Extractor Estimating: 78it [00:47,  1.53it/s]Extractor Estimating: 79it [00:48,  1.50it/s]Extractor Estimating: 80it [00:49,  1.53it/s]Extractor Estimating: 81it [00:49,  1.54it/s]Extractor Estimating: 82it [00:50,  1.57it/s]Extractor Estimating: 83it [00:50,  1.53it/s]Extractor Estimating: 84it [00:51,  1.52it/s]Extractor Estimating: 85it [00:52,  1.49it/s]Extractor Estimating: 86it [00:53,  1.43it/s]Extractor Estimating: 87it [00:53,  1.43it/s]Extractor Estimating: 88it [00:54,  1.44it/s]Extractor Estimating: 89it [00:55,  1.44it/s]Extractor Estimating: 90it [00:55,  1.45it/s]Extractor Estimating: 91it [00:56,  1.48it/s]Extractor Estimating: 92it [00:57,  1.50it/s]Extractor Estimating: 93it [00:57,  1.52it/s]Extractor Estimating: 94it [00:58,  1.52it/s]Extractor Estimating: 95it [00:59,  1.54it/s]Extractor Estimating: 96it [00:59,  1.53it/s]Extractor Estimating: 97it [01:00,  1.58it/s]Extractor Estimating: 98it [01:01,  1.51it/s]Extractor Estimating: 99it [01:01,  1.49it/s]Extractor Estimating: 100it [01:02,  1.49it/s]Extractor Estimating: 101it [01:03,  1.50it/s]Extractor Estimating: 102it [01:03,  1.55it/s]Extractor Estimating: 103it [01:04,  1.58it/s]Extractor Estimating: 104it [01:04,  1.65it/s]Extractor Estimating: 105it [01:05,  1.62it/s]Extractor Estimating: 106it [01:05,  1.68it/s]Extractor Estimating: 107it [01:06,  1.70it/s]Extractor Estimating: 108it [01:07,  1.70it/s]Extractor Estimating: 109it [01:07,  1.70it/s]Extractor Estimating: 110it [01:08,  1.73it/s]Extractor Estimating: 111it [01:08,  1.71it/s]Extractor Estimating: 112it [01:09,  1.66it/s]Extractor Estimating: 113it [01:10,  1.62it/s]Extractor Estimating: 114it [01:10,  1.68it/s]Extractor Estimating: 115it [01:11,  1.66it/s]Extractor Estimating: 116it [01:11,  1.70it/s]Extractor Estimating: 117it [01:12,  1.66it/s]Extractor Estimating: 118it [01:13,  1.64it/s]Extractor Estimating: 119it [01:13,  1.59it/s]Extractor Estimating: 120it [01:14,  1.62it/s]Extractor Estimating: 121it [01:15,  1.58it/s]Extractor Estimating: 122it [01:15,  1.57it/s]Extractor Estimating: 123it [01:16,  1.58it/s]Extractor Estimating: 124it [01:16,  1.59it/s]Extractor Estimating: 125it [01:17,  1.64it/s]Extractor Estimating: 126it [01:18,  1.65it/s]Extractor Estimating: 127it [01:18,  1.50it/s]Extractor Estimating: 128it [01:19,  1.55it/s]Extractor Estimating: 129it [01:20,  1.58it/s]Extractor Estimating: 130it [01:20,  1.60it/s]Extractor Estimating: 131it [01:21,  1.64it/s]Extractor Estimating: 132it [01:21,  1.64it/s]Extractor Estimating: 133it [01:22,  1.65it/s]Extractor Estimating: 134it [01:23,  1.67it/s]Extractor Estimating: 135it [01:23,  1.62it/s]Extractor Estimating: 136it [01:24,  1.65it/s]Extractor Estimating: 137it [01:25,  1.59it/s]Extractor Estimating: 138it [01:25,  1.61it/s]Extractor Estimating: 139it [01:26,  1.64it/s]Extractor Estimating: 140it [01:26,  1.57it/s]Extractor Estimating: 141it [01:27,  1.58it/s]Extractor Estimating: 142it [01:28,  1.57it/s]Extractor Estimating: 143it [01:28,  1.59it/s]Extractor Estimating: 144it [01:29,  1.56it/s]Extractor Estimating: 145it [01:30,  1.61it/s]Extractor Estimating: 146it [01:30,  1.62it/s]Extractor Estimating: 147it [01:31,  1.64it/s]Extractor Estimating: 148it [01:31,  1.62it/s]Extractor Estimating: 149it [01:32,  1.58it/s]Extractor Estimating: 150it [01:33,  1.67it/s]Extractor Estimating: 151it [01:33,  1.69it/s]Extractor Estimating: 152it [01:34,  1.70it/s]Extractor Estimating: 153it [01:34,  1.73it/s]Extractor Estimating: 154it [01:35,  1.67it/s]Extractor Estimating: 155it [01:36,  1.69it/s]Extractor Estimating: 156it [01:36,  1.67it/s]Extractor Estimating: 157it [01:37,  1.60it/s]Extractor Estimating: 158it [01:37,  1.61it/s]Extractor Estimating: 159it [01:38,  1.65it/s]Extractor Estimating: 160it [01:39,  1.65it/s]Extractor Estimating: 161it [01:39,  1.70it/s]Extractor Estimating: 162it [01:40,  1.67it/s]Extractor Estimating: 163it [01:40,  1.68it/s]Extractor Estimating: 164it [01:41,  1.64it/s]Extractor Estimating: 165it [01:42,  1.65it/s]Extractor Estimating: 166it [01:42,  1.69it/s]Extractor Estimating: 167it [01:43,  1.73it/s]Extractor Estimating: 168it [01:43,  1.72it/s]Extractor Estimating: 169it [01:44,  1.69it/s]Extractor Estimating: 170it [01:45,  1.68it/s]Extractor Estimating: 171it [01:45,  1.68it/s]Extractor Estimating: 172it [01:46,  1.76it/s]Extractor Estimating: 173it [01:46,  1.67it/s]Extractor Estimating: 174it [01:47,  1.64it/s]Extractor Estimating: 175it [01:48,  1.67it/s]Extractor Estimating: 176it [01:48,  1.72it/s]Extractor Estimating: 177it [01:49,  1.71it/s]Extractor Estimating: 178it [01:49,  1.74it/s]Extractor Estimating: 179it [01:50,  1.78it/s]Extractor Estimating: 180it [01:50,  1.75it/s]Extractor Estimating: 181it [01:51,  1.75it/s]Extractor Estimating: 182it [01:51,  1.74it/s]Extractor Estimating: 183it [01:52,  1.79it/s]Extractor Estimating: 184it [01:53,  1.73it/s]Extractor Estimating: 185it [01:53,  1.73it/s]Extractor Estimating: 186it [01:54,  1.68it/s]Extractor Estimating: 187it [01:54,  1.68it/s]Extractor Estimating: 188it [01:55,  1.72it/s]Extractor Estimating: 189it [01:56,  1.69it/s]Extractor Estimating: 190it [01:56,  1.69it/s]Extractor Estimating: 191it [01:57,  1.70it/s]Extractor Estimating: 192it [01:57,  1.71it/s]Extractor Estimating: 193it [01:58,  1.80it/s]Extractor Estimating: 194it [01:58,  1.76it/s]Extractor Estimating: 195it [01:59,  1.56it/s]Extractor Estimating: 196it [02:00,  1.60it/s]Extractor Estimating: 197it [02:00,  1.66it/s]Extractor Estimating: 198it [02:01,  1.67it/s]Extractor Estimating: 199it [02:02,  1.65it/s]Extractor Estimating: 200it [02:02,  1.64it/s]Extractor Estimating: 201it [02:03,  1.65it/s]Extractor Estimating: 202it [02:03,  1.67it/s]Extractor Estimating: 203it [02:04,  1.70it/s]Extractor Estimating: 204it [02:05,  1.70it/s]Extractor Estimating: 205it [02:05,  1.62it/s]Extractor Estimating: 206it [02:06,  1.70it/s]Extractor Estimating: 207it [02:06,  1.67it/s]Extractor Estimating: 208it [02:07,  1.74it/s]Extractor Estimating: 209it [02:07,  1.71it/s]Extractor Estimating: 210it [02:08,  1.69it/s]Extractor Estimating: 211it [02:09,  1.61it/s]Extractor Estimating: 212it [02:09,  1.64it/s]Extractor Estimating: 213it [02:10,  1.68it/s]Extractor Estimating: 214it [02:11,  1.66it/s]Extractor Estimating: 215it [02:11,  1.67it/s]Extractor Estimating: 216it [02:12,  1.72it/s]Extractor Estimating: 217it [02:12,  1.72it/s]Extractor Estimating: 218it [02:13,  1.70it/s]Extractor Estimating: 219it [02:13,  1.72it/s]Extractor Estimating: 220it [02:14,  1.72it/s]Extractor Estimating: 221it [02:15,  1.77it/s]Extractor Estimating: 222it [02:15,  1.74it/s]Extractor Estimating: 223it [02:16,  1.75it/s]Extractor Estimating: 224it [02:16,  1.72it/s]Extractor Estimating: 225it [02:17,  1.69it/s]Extractor Estimating: 226it [02:18,  1.65it/s]Extractor Estimating: 227it [02:18,  1.64it/s]Extractor Estimating: 228it [02:19,  1.64it/s]Extractor Estimating: 229it [02:19,  1.63it/s]Extractor Estimating: 230it [02:20,  1.64it/s]Extractor Estimating: 231it [02:21,  1.64it/s]Extractor Estimating: 232it [02:21,  1.64it/s]Extractor Estimating: 233it [02:22,  1.56it/s]Extractor Estimating: 234it [02:23,  1.57it/s]Extractor Estimating: 235it [02:23,  1.61it/s]Extractor Estimating: 236it [02:24,  1.62it/s]Extractor Estimating: 237it [02:24,  1.57it/s]Extractor Estimating: 238it [02:25,  1.62it/s]Extractor Estimating: 239it [02:26,  1.61it/s]Extractor Estimating: 240it [02:26,  1.63it/s]Extractor Estimating: 241it [02:27,  1.62it/s]Extractor Estimating: 242it [02:28,  1.60it/s]Extractor Estimating: 243it [02:28,  1.66it/s]Extractor Estimating: 244it [02:29,  1.62it/s]Extractor Estimating: 245it [02:29,  1.58it/s]Extractor Estimating: 246it [02:30,  1.60it/s]Extractor Estimating: 247it [02:31,  1.61it/s]Extractor Estimating: 248it [02:31,  1.63it/s]Extractor Estimating: 249it [02:32,  1.62it/s]Extractor Estimating: 250it [02:33,  1.56it/s]Extractor Estimating: 250it [02:33,  1.63it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:17:46,320 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:17:46,325 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:17:46,325 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:17:46,325 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:17:46,325 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:17:46,710 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:17:46,711 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:17:47,098 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:17:48,184 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:17:48,184 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:17:50,822 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:17:50,827 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:17:50,827 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:17:50,827 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:17:50,828 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:17:51,593 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:17:51,594 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:17:51,973 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:17:52,149 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:17:52,150 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 01:44:49,034 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 01:44:49,037 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 5204 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 23350
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23450, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23450, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.990, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.004, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 83, avg_time 0.994, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 183, avg_time 0.999, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 66, avg_time 0.994, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 166, avg_time 2.099, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 49, avg_time 1.000, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 149, avg_time 1.000, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 32, avg_time 0.998, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 132, avg_time 0.999, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 15, avg_time 2.112, loss:nan
g_step 1200, step 115, avg_time 1.002, loss:nan
g_step 1300, step 215, avg_time 0.985, loss:nan
g_step 1400, step 98, avg_time 0.994, loss:nan
g_step 1500, step 198, avg_time 1.004, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 81, avg_time 2.099, loss:nan
g_step 1700, step 181, avg_time 1.005, loss:nan
g_step 1800, step 64, avg_time 0.999, loss:nan
g_step 1900, step 164, avg_time 0.997, loss:nan
g_step 2000, step 47, avg_time 1.006, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 147, avg_time 2.099, loss:nan
g_step 2200, step 30, avg_time 1.004, loss:nan
g_step 2300, step 130, avg_time 0.993, loss:nan
g_step 2400, step 13, avg_time 0.995, loss:nan
g_step 2500, step 113, avg_time 0.998, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 213, avg_time 2.106, loss:nan
g_step 2700, step 96, avg_time 0.991, loss:nan
g_step 2800, step 196, avg_time 0.997, loss:nan
g_step 2900, step 79, avg_time 0.998, loss:nan
g_step 3000, step 179, avg_time 0.999, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 62, avg_time 2.089, loss:nan
g_step 3200, step 162, avg_time 1.010, loss:nan
g_step 3300, step 45, avg_time 0.996, loss:nan
g_step 3400, step 145, avg_time 0.995, loss:nan
g_step 3500, step 28, avg_time 0.995, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 128, avg_time 2.117, loss:nan
g_step 3700, step 11, avg_time 1.000, loss:nan
g_step 3800, step 111, avg_time 0.988, loss:nan
g_step 3900, step 211, avg_time 1.003, loss:nan
g_step 4000, step 94, avg_time 0.995, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 194, avg_time 2.109, loss:nan
g_step 4200, step 77, avg_time 0.981, loss:nan
g_step 4300, step 177, avg_time 0.999, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 01:44:49 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 01:44:49 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_01-44-49_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 01:44:49 - WARNING - datasets.builder -   Using custom data configuration default-464f458cc247cdb6
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-464f458cc247cdb6/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 01:44:50,289 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:44:50,290 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 01:44:50,290 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:44:50,291 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 01:44:50,311 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:44:50,316 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:44:50,316 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:44:50,316 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:44:50,316 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:44:50,317 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:44:50,317 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 01:44:50,462 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 01:44:53,570 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 01:44:53,570 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-464f458cc247cdb6/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:02,  2.25ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.26ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.74ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  4.04ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.22ba/s]100%|██████████| 6/6 [00:01<00:00,  4.38ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.83ba/s] 40%|████      | 2/5 [00:00<00:00,  4.15ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.25ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.31ba/s]100%|██████████| 5/5 [00:00<00:00,  5.21ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  5.82ba/s] 50%|█████     | 3/6 [00:00<00:00,  8.96ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.74ba/s]100%|██████████| 6/6 [00:00<00:00, 10.63ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.30ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.70ba/s]100%|██████████| 5/5 [00:00<00:00, 13.13ba/s]
[INFO|trainer.py:414] 2023-08-28 01:44:57,522 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 01:44:57,541 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 01:44:57,541 >>   Num examples = 5240
[INFO|trainer.py:1149] 2023-08-28 01:44:57,541 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 01:44:57,541 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 01:44:57,541 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 01:44:57,541 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 01:44:57,541 >>   Total optimization steps = 410
  0%|          | 0/410 [00:00<?, ?it/s]  0%|          | 1/410 [00:00<01:56,  3.51it/s]  0%|          | 2/410 [00:00<01:53,  3.58it/s]  1%|          | 3/410 [00:00<01:52,  3.62it/s]  1%|          | 4/410 [00:01<01:52,  3.62it/s]  1%|          | 5/410 [00:01<01:51,  3.62it/s]  1%|▏         | 6/410 [00:01<01:52,  3.60it/s]  2%|▏         | 7/410 [00:01<01:52,  3.59it/s]  2%|▏         | 8/410 [00:02<01:52,  3.59it/s]  2%|▏         | 9/410 [00:02<01:52,  3.58it/s]  2%|▏         | 10/410 [00:02<01:51,  3.58it/s]  3%|▎         | 11/410 [00:03<01:52,  3.56it/s]  3%|▎         | 12/410 [00:03<01:51,  3.57it/s]  3%|▎         | 13/410 [00:03<01:51,  3.57it/s]  3%|▎         | 14/410 [00:03<01:50,  3.57it/s]  4%|▎         | 15/410 [00:04<01:50,  3.57it/s]  4%|▍         | 16/410 [00:04<01:49,  3.59it/s]  4%|▍         | 17/410 [00:04<01:49,  3.60it/s]  4%|▍         | 18/410 [00:05<01:48,  3.60it/s]  5%|▍         | 19/410 [00:05<01:48,  3.61it/s]  5%|▍         | 20/410 [00:05<01:47,  3.62it/s]  5%|▌         | 21/410 [00:05<01:47,  3.62it/s]  5%|▌         | 22/410 [00:06<01:48,  3.59it/s]  6%|▌         | 23/410 [00:06<01:47,  3.60it/s]  6%|▌         | 24/410 [00:06<01:47,  3.61it/s]  6%|▌         | 25/410 [00:06<01:46,  3.62it/s]  6%|▋         | 26/410 [00:07<01:46,  3.62it/s]  7%|▋         | 27/410 [00:07<01:45,  3.62it/s]  7%|▋         | 28/410 [00:07<01:45,  3.62it/s]  7%|▋         | 29/410 [00:08<01:45,  3.62it/s]  7%|▋         | 30/410 [00:08<01:44,  3.62it/s]  8%|▊         | 31/410 [00:08<01:44,  3.62it/s]  8%|▊         | 32/410 [00:08<01:44,  3.62it/s]  8%|▊         | 33/410 [00:09<01:44,  3.60it/s]  8%|▊         | 34/410 [00:09<01:44,  3.61it/s]  9%|▊         | 35/410 [00:09<01:43,  3.61it/s]  9%|▉         | 36/410 [00:09<01:43,  3.61it/s]  9%|▉         | 37/410 [00:10<01:43,  3.62it/s]  9%|▉         | 38/410 [00:10<01:42,  3.62it/s] 10%|▉         | 39/410 [00:10<01:42,  3.62it/s] 10%|▉         | 40/410 [00:11<01:42,  3.62it/s] 10%|█         | 41/410 [00:11<01:41,  3.62it/s] 10%|█         | 42/410 [00:11<01:41,  3.62it/s] 10%|█         | 43/410 [00:11<01:41,  3.62it/s] 11%|█         | 44/410 [00:12<01:41,  3.61it/s] 11%|█         | 45/410 [00:12<01:41,  3.61it/s] 11%|█         | 46/410 [00:12<01:40,  3.62it/s] 11%|█▏        | 47/410 [00:13<01:40,  3.62it/s] 12%|█▏        | 48/410 [00:13<01:40,  3.62it/s] 12%|█▏        | 49/410 [00:13<01:39,  3.62it/s] 12%|█▏        | 50/410 [00:13<01:39,  3.62it/s] 12%|█▏        | 51/410 [00:14<01:39,  3.62it/s] 13%|█▎        | 52/410 [00:14<01:39,  3.61it/s] 13%|█▎        | 53/410 [00:14<01:38,  3.62it/s] 13%|█▎        | 54/410 [00:14<01:38,  3.62it/s] 13%|█▎        | 55/410 [00:15<01:39,  3.58it/s] 14%|█▎        | 56/410 [00:15<01:38,  3.59it/s] 14%|█▍        | 57/410 [00:15<01:38,  3.60it/s] 14%|█▍        | 58/410 [00:16<01:37,  3.60it/s] 14%|█▍        | 59/410 [00:16<01:37,  3.61it/s] 15%|█▍        | 60/410 [00:16<01:36,  3.61it/s] 15%|█▍        | 61/410 [00:16<01:36,  3.62it/s] 15%|█▌        | 62/410 [00:17<01:36,  3.62it/s] 15%|█▌        | 63/410 [00:17<01:35,  3.62it/s] 16%|█▌        | 64/410 [00:17<01:35,  3.62it/s] 16%|█▌        | 65/410 [00:18<01:35,  3.62it/s] 16%|█▌        | 66/410 [00:18<01:35,  3.61it/s] 16%|█▋        | 67/410 [00:18<01:34,  3.61it/s] 17%|█▋        | 68/410 [00:18<01:34,  3.62it/s] 17%|█▋        | 69/410 [00:19<01:34,  3.62it/s] 17%|█▋        | 70/410 [00:19<01:33,  3.62it/s] 17%|█▋        | 71/410 [00:19<01:33,  3.62it/s] 18%|█▊        | 72/410 [00:19<01:33,  3.62it/s] 18%|█▊        | 73/410 [00:20<01:33,  3.62it/s] 18%|█▊        | 74/410 [00:20<01:32,  3.62it/s] 18%|█▊        | 75/410 [00:20<01:32,  3.62it/s] 19%|█▊        | 76/410 [00:21<01:32,  3.62it/s] 19%|█▉        | 77/410 [00:21<01:32,  3.58it/s] 19%|█▉        | 78/410 [00:21<01:32,  3.60it/s] 19%|█▉        | 79/410 [00:21<01:31,  3.61it/s] 20%|█▉        | 80/410 [00:22<01:31,  3.61it/s] 20%|█▉        | 81/410 [00:22<01:30,  3.62it/s] 20%|██        | 82/410 [00:22<01:27,  3.75it/s][INFO|trainer.py:2140] 2023-08-28 01:45:20,237 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:45:20,237 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 01:45:20,237 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.50it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.27it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.41it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.40it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.94it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.62it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.28it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.25it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.34it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.41it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.52it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.17it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.19it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.17it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.12it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.08it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.99it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.23it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.41it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.50it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.41it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.08it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.05it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.09it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.01it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.92it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.06it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.24it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.30it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.29it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.21it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 43.98it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 43.91it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.02it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.13it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.22it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.19it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.24it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.21it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.11it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.19it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.06it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.02it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.17it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.22it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.24it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.28it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.16it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.17it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.20it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.18it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.15it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.16it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.27it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.23it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.24it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.15it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.16it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.12it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.07it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.08it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.19it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.21it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.29it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.19it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.18it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.20it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.12it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 43.94it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.13it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.24it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.22it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.06it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.16it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.18it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.25it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.23it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.05it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.12it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.12it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.17it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.19it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.25it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.22it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.11it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.09it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.11it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.14it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.16it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.20it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.19it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.11it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.22it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.21it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.16it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.04it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.14it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.18it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.18it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.11it/s][A                                                
                                                 [A 20%|██        | 82/410 [00:34<01:27,  3.75it/s]
100%|██████████| 505/505 [00:11<00:00, 44.11it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 01:45:31,697 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-82
[INFO|configuration_utils.py:351] 2023-08-28 01:45:31,730 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-82/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:45:33,838 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-82/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:45:33,850 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-82/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:45:33,862 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-82/special_tokens_map.json
 20%|██        | 83/410 [00:36<24:10,  4.43s/it] 20%|██        | 84/410 [00:37<17:19,  3.19s/it] 21%|██        | 85/410 [00:37<12:32,  2.32s/it] 21%|██        | 86/410 [00:37<09:12,  1.71s/it] 21%|██        | 87/410 [00:37<06:53,  1.28s/it] 21%|██▏       | 88/410 [00:38<05:15,  1.02it/s] 22%|██▏       | 89/410 [00:38<04:06,  1.30it/s] 22%|██▏       | 90/410 [00:38<03:18,  1.61it/s] 22%|██▏       | 91/410 [00:39<02:45,  1.93it/s] 22%|██▏       | 92/410 [00:39<02:21,  2.24it/s] 23%|██▎       | 93/410 [00:39<02:05,  2.53it/s] 23%|██▎       | 94/410 [00:39<01:53,  2.79it/s] 23%|██▎       | 95/410 [00:40<01:45,  2.99it/s] 23%|██▎       | 96/410 [00:40<01:39,  3.16it/s] 24%|██▎       | 97/410 [00:40<01:35,  3.28it/s] 24%|██▍       | 98/410 [00:41<01:32,  3.37it/s] 24%|██▍       | 99/410 [00:41<01:30,  3.44it/s] 24%|██▍       | 100/410 [00:41<01:28,  3.50it/s] 25%|██▍       | 101/410 [00:41<01:27,  3.53it/s] 25%|██▍       | 102/410 [00:42<01:26,  3.56it/s] 25%|██▌       | 103/410 [00:42<01:25,  3.57it/s] 25%|██▌       | 104/410 [00:42<01:25,  3.59it/s] 26%|██▌       | 105/410 [00:42<01:24,  3.60it/s] 26%|██▌       | 106/410 [00:43<01:24,  3.61it/s] 26%|██▌       | 107/410 [00:43<01:23,  3.62it/s] 26%|██▋       | 108/410 [00:43<01:23,  3.62it/s] 27%|██▋       | 109/410 [00:44<01:23,  3.61it/s] 27%|██▋       | 110/410 [00:44<01:23,  3.61it/s] 27%|██▋       | 111/410 [00:44<01:22,  3.62it/s] 27%|██▋       | 112/410 [00:44<01:22,  3.61it/s] 28%|██▊       | 113/410 [00:45<01:22,  3.61it/s] 28%|██▊       | 114/410 [00:45<01:21,  3.61it/s] 28%|██▊       | 115/410 [00:45<01:21,  3.61it/s] 28%|██▊       | 116/410 [00:46<01:21,  3.61it/s] 29%|██▊       | 117/410 [00:46<01:21,  3.61it/s] 29%|██▉       | 118/410 [00:46<01:20,  3.62it/s] 29%|██▉       | 119/410 [00:46<01:20,  3.62it/s] 29%|██▉       | 120/410 [00:47<01:20,  3.60it/s] 30%|██▉       | 121/410 [00:47<01:20,  3.61it/s] 30%|██▉       | 122/410 [00:47<01:19,  3.61it/s] 30%|███       | 123/410 [00:47<01:19,  3.60it/s] 30%|███       | 124/410 [00:48<01:19,  3.61it/s] 30%|███       | 125/410 [00:48<01:18,  3.61it/s] 31%|███       | 126/410 [00:48<01:18,  3.61it/s] 31%|███       | 127/410 [00:49<01:18,  3.61it/s] 31%|███       | 128/410 [00:49<01:18,  3.61it/s] 31%|███▏      | 129/410 [00:49<01:17,  3.61it/s] 32%|███▏      | 130/410 [00:49<01:17,  3.61it/s] 32%|███▏      | 131/410 [00:50<01:17,  3.58it/s] 32%|███▏      | 132/410 [00:50<01:17,  3.59it/s] 32%|███▏      | 133/410 [00:50<01:17,  3.59it/s] 33%|███▎      | 134/410 [00:50<01:16,  3.60it/s] 33%|███▎      | 135/410 [00:51<01:16,  3.60it/s] 33%|███▎      | 136/410 [00:51<01:15,  3.61it/s] 33%|███▎      | 137/410 [00:51<01:15,  3.61it/s] 34%|███▎      | 138/410 [00:52<01:15,  3.61it/s] 34%|███▍      | 139/410 [00:52<01:15,  3.61it/s] 34%|███▍      | 140/410 [00:52<01:14,  3.61it/s] 34%|███▍      | 141/410 [00:52<01:14,  3.61it/s] 35%|███▍      | 142/410 [00:53<01:14,  3.60it/s] 35%|███▍      | 143/410 [00:53<01:14,  3.60it/s] 35%|███▌      | 144/410 [00:53<01:13,  3.61it/s] 35%|███▌      | 145/410 [00:54<01:13,  3.61it/s] 36%|███▌      | 146/410 [00:54<01:13,  3.61it/s] 36%|███▌      | 147/410 [00:54<01:12,  3.61it/s] 36%|███▌      | 148/410 [00:54<01:12,  3.61it/s] 36%|███▋      | 149/410 [00:55<01:12,  3.61it/s] 37%|███▋      | 150/410 [00:55<01:12,  3.61it/s] 37%|███▋      | 151/410 [00:55<01:11,  3.61it/s] 37%|███▋      | 152/410 [00:55<01:11,  3.61it/s] 37%|███▋      | 153/410 [00:56<01:12,  3.56it/s] 38%|███▊      | 154/410 [00:56<01:11,  3.57it/s] 38%|███▊      | 155/410 [00:56<01:11,  3.58it/s] 38%|███▊      | 156/410 [00:57<01:10,  3.59it/s] 38%|███▊      | 157/410 [00:57<01:10,  3.59it/s] 39%|███▊      | 158/410 [00:57<01:10,  3.60it/s] 39%|███▉      | 159/410 [00:57<01:09,  3.60it/s] 39%|███▉      | 160/410 [00:58<01:09,  3.60it/s] 39%|███▉      | 161/410 [00:58<01:09,  3.61it/s] 40%|███▉      | 162/410 [00:58<01:08,  3.61it/s] 40%|███▉      | 163/410 [00:59<01:08,  3.61it/s] 40%|████      | 164/410 [00:59<01:06,  3.72it/s][INFO|trainer.py:2140] 2023-08-28 01:45:56,842 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:45:56,842 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 01:45:56,842 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4338, 'eval_samples_per_second': 353.25, 'eval_steps_per_second': 44.167, 'epoch': 1.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.76it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.51it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.47it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.35it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.80it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.28it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.04it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.00it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.04it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.16it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.31it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.27it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.29it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.08it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.45it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.50it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.72it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 43.92it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.04it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.15it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.16it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.19it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.04it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 43.85it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.87it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.95it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.18it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.28it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.18it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.19it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.00it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 43.87it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 43.81it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.93it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.03it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.07it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.17it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.22it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.12it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.02it/s][A
 41%|████      | 207/505 [00:04<00:06, 43.81it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 43.87it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 43.88it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.04it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.16it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.18it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.20it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.13it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 43.92it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 43.95it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.01it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.04it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.09it/s][A
 54%|█████▍    | 272/505 [00:06<00:07, 29.32it/s][A
 55%|█████▍    | 277/505 [00:06<00:06, 32.73it/s][A
 56%|█████▌    | 282/505 [00:06<00:06, 35.63it/s][A
 57%|█████▋    | 287/505 [00:06<00:05, 37.95it/s][A
 58%|█████▊    | 292/505 [00:06<00:05, 39.72it/s][A
 59%|█████▉    | 297/505 [00:06<00:05, 41.01it/s][A
 60%|█████▉    | 302/505 [00:07<00:04, 42.06it/s][A
 61%|██████    | 307/505 [00:07<00:04, 42.49it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 42.56it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 42.62it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 43.07it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 43.60it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 43.90it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.07it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.17it/s][A
 69%|██████▊   | 347/505 [00:08<00:03, 44.21it/s][A
 70%|██████▉   | 352/505 [00:08<00:03, 43.95it/s][A
 71%|███████   | 357/505 [00:08<00:03, 43.76it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 43.58it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 43.62it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 43.91it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.05it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.32it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.43it/s][A
 78%|███████▊  | 392/505 [00:09<00:02, 44.25it/s][A
 79%|███████▊  | 397/505 [00:09<00:02, 43.96it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 43.86it/s][A
 81%|████████  | 407/505 [00:09<00:02, 43.74it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 43.80it/s][A
 83%|████████▎ | 417/505 [00:09<00:02, 43.87it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.10it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.28it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.17it/s][A
 87%|████████▋ | 437/505 [00:10<00:01, 44.29it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 44.13it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 43.81it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 43.85it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 43.91it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.06it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.19it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.18it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.25it/s][A
 95%|█████████▌| 482/505 [00:11<00:00, 44.22it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.07it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 43.92it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 43.89it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 43.99it/s][A                                                 
                                                 [A 40%|████      | 164/410 [01:10<01:06,  3.72it/s]
100%|██████████| 505/505 [00:11<00:00, 43.99it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 01:46:08,516 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 01:46:08,540 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:46:10,090 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:46:10,104 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:46:10,112 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-164/special_tokens_map.json
 40%|████      | 165/410 [01:13<17:41,  4.33s/it] 40%|████      | 166/410 [01:13<12:40,  3.12s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/410 [01:13<09:10,  2.27s/it] 41%|████      | 168/410 [01:13<06:44,  1.67s/it] 41%|████      | 169/410 [01:14<05:02,  1.25s/it] 41%|████▏     | 170/410 [01:14<03:50,  1.04it/s] 42%|████▏     | 171/410 [01:14<03:00,  1.32it/s] 42%|████▏     | 172/410 [01:15<02:26,  1.63it/s] 42%|████▏     | 173/410 [01:15<02:02,  1.94it/s] 42%|████▏     | 174/410 [01:15<01:45,  2.25it/s] 43%|████▎     | 175/410 [01:15<01:33,  2.52it/s] 43%|████▎     | 176/410 [01:16<01:24,  2.76it/s] 43%|████▎     | 177/410 [01:16<01:18,  2.96it/s] 43%|████▎     | 178/410 [01:16<01:14,  3.11it/s] 44%|████▎     | 179/410 [01:17<01:11,  3.23it/s] 44%|████▍     | 180/410 [01:17<01:09,  3.33it/s] 44%|████▍     | 181/410 [01:17<01:07,  3.39it/s] 44%|████▍     | 182/410 [01:17<01:06,  3.44it/s] 45%|████▍     | 183/410 [01:18<01:05,  3.48it/s] 45%|████▍     | 184/410 [01:18<01:04,  3.50it/s] 45%|████▌     | 185/410 [01:18<01:03,  3.52it/s] 45%|████▌     | 186/410 [01:19<01:03,  3.51it/s] 46%|████▌     | 187/410 [01:19<01:03,  3.53it/s] 46%|████▌     | 188/410 [01:19<01:02,  3.56it/s] 46%|████▌     | 189/410 [01:19<01:01,  3.57it/s] 46%|████▋     | 190/410 [01:20<01:01,  3.58it/s] 47%|████▋     | 191/410 [01:20<01:00,  3.60it/s] 47%|████▋     | 192/410 [01:20<01:00,  3.60it/s] 47%|████▋     | 193/410 [01:20<01:00,  3.61it/s] 47%|████▋     | 194/410 [01:21<00:59,  3.60it/s] 48%|████▊     | 195/410 [01:21<00:59,  3.61it/s] 48%|████▊     | 196/410 [01:21<00:59,  3.61it/s] 48%|████▊     | 197/410 [01:22<00:59,  3.59it/s] 48%|████▊     | 198/410 [01:22<00:58,  3.60it/s] 49%|████▊     | 199/410 [01:22<00:58,  3.60it/s] 49%|████▉     | 200/410 [01:22<00:58,  3.61it/s] 49%|████▉     | 201/410 [01:23<00:57,  3.61it/s] 49%|████▉     | 202/410 [01:23<00:57,  3.61it/s] 50%|████▉     | 203/410 [01:23<00:57,  3.61it/s] 50%|████▉     | 204/410 [01:24<00:57,  3.61it/s] 50%|█████     | 205/410 [01:24<00:56,  3.61it/s] 50%|█████     | 206/410 [01:24<00:56,  3.61it/s] 50%|█████     | 207/410 [01:24<00:56,  3.61it/s] 51%|█████     | 208/410 [01:25<00:56,  3.59it/s] 51%|█████     | 209/410 [01:25<00:55,  3.60it/s] 51%|█████     | 210/410 [01:25<00:55,  3.60it/s] 51%|█████▏    | 211/410 [01:25<00:55,  3.60it/s] 52%|█████▏    | 212/410 [01:26<00:54,  3.60it/s] 52%|█████▏    | 213/410 [01:26<00:54,  3.61it/s] 52%|█████▏    | 214/410 [01:26<00:54,  3.61it/s] 52%|█████▏    | 215/410 [01:27<00:54,  3.60it/s] 53%|█████▎    | 216/410 [01:27<00:53,  3.60it/s] 53%|█████▎    | 217/410 [01:27<00:53,  3.60it/s] 53%|█████▎    | 218/410 [01:27<00:53,  3.61it/s] 53%|█████▎    | 219/410 [01:28<00:53,  3.58it/s] 54%|█████▎    | 220/410 [01:28<00:52,  3.59it/s] 54%|█████▍    | 221/410 [01:28<00:52,  3.59it/s] 54%|█████▍    | 222/410 [01:29<00:52,  3.60it/s] 54%|█████▍    | 223/410 [01:29<00:51,  3.60it/s] 55%|█████▍    | 224/410 [01:29<00:51,  3.61it/s] 55%|█████▍    | 225/410 [01:29<00:51,  3.60it/s] 55%|█████▌    | 226/410 [01:30<00:51,  3.61it/s] 55%|█████▌    | 227/410 [01:30<00:50,  3.60it/s] 56%|█████▌    | 228/410 [01:30<00:50,  3.61it/s] 56%|█████▌    | 229/410 [01:30<00:50,  3.60it/s] 56%|█████▌    | 230/410 [01:31<00:50,  3.59it/s] 56%|█████▋    | 231/410 [01:31<00:49,  3.59it/s] 57%|█████▋    | 232/410 [01:31<00:49,  3.60it/s] 57%|█████▋    | 233/410 [01:32<00:49,  3.60it/s] 57%|█████▋    | 234/410 [01:32<00:48,  3.60it/s] 57%|█████▋    | 235/410 [01:32<00:48,  3.60it/s] 58%|█████▊    | 236/410 [01:32<00:48,  3.61it/s] 58%|█████▊    | 237/410 [01:33<00:47,  3.61it/s] 58%|█████▊    | 238/410 [01:33<00:47,  3.60it/s] 58%|█████▊    | 239/410 [01:33<00:47,  3.61it/s] 59%|█████▊    | 240/410 [01:34<00:47,  3.61it/s] 59%|█████▉    | 241/410 [01:34<00:47,  3.59it/s] 59%|█████▉    | 242/410 [01:34<00:46,  3.59it/s] 59%|█████▉    | 243/410 [01:34<00:46,  3.60it/s] 60%|█████▉    | 244/410 [01:35<00:46,  3.60it/s] 60%|█████▉    | 245/410 [01:35<00:45,  3.60it/s] 60%|██████    | 246/410 [01:35<00:43,  3.74it/s][INFO|trainer.py:2140] 2023-08-28 01:46:33,186 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:46:33,186 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 01:46:33,186 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.6527, 'eval_samples_per_second': 346.614, 'eval_steps_per_second': 43.338, 'epoch': 2.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.17it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.43it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.38it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.28it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.82it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.40it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.05it/s][A
  8%|▊         | 42/505 [00:00<00:10, 43.96it/s][A
  9%|▉         | 47/505 [00:01<00:10, 43.99it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.23it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.27it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.34it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.25it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.07it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.94it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.82it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.90it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 43.89it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.16it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.24it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.18it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.10it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.00it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 43.85it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.90it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.96it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.03it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.17it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.12it/s][A
 30%|███       | 152/505 [00:03<00:08, 43.92it/s][A
 31%|███       | 157/505 [00:03<00:08, 41.58it/s][A
 32%|███▏      | 162/505 [00:03<00:08, 42.38it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 42.83it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.20it/s][A
 35%|███▌      | 177/505 [00:04<00:07, 43.39it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 43.73it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 43.88it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 43.83it/s][A
 39%|███▉      | 197/505 [00:04<00:07, 43.73it/s][A
 40%|████      | 202/505 [00:04<00:06, 43.80it/s][A
 41%|████      | 207/505 [00:04<00:06, 43.96it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.02it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 43.95it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 43.97it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.06it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.12it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 43.95it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 43.93it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 43.91it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.05it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.06it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.03it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 43.98it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.14it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.05it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 43.99it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.04it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.02it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.01it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 43.97it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.10it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.03it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.10it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.08it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 43.99it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.10it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.02it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 43.96it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 43.92it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.06it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.09it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.07it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.05it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.01it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.01it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.06it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.00it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 43.88it/s][A
 79%|███████▊  | 397/505 [00:09<00:02, 43.98it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.11it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.09it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.08it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.15it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 43.96it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.02it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.00it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 43.89it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 44.06it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.00it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 43.94it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.10it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.01it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.05it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 43.95it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 43.98it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.06it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.03it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.03it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 43.97it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.05it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.05it/s][A 60%|██████    | 246/410 [01:47<00:43,  3.74it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 01:46:44,694 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 01:46:44,715 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:46:46,489 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:46:46,504 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:46:46,512 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-246/special_tokens_map.json
 60%|██████    | 247/410 [01:49<11:49,  4.36s/it] 60%|██████    | 248/410 [01:49<08:27,  3.13s/it] 61%|██████    | 249/410 [01:50<06:06,  2.28s/it] 61%|██████    | 250/410 [01:50<04:28,  1.68s/it] 61%|██████    | 251/410 [01:50<03:20,  1.26s/it] 61%|██████▏   | 252/410 [01:50<02:32,  1.04it/s] 62%|██████▏   | 253/410 [01:51<01:59,  1.32it/s] 62%|██████▏   | 254/410 [01:51<01:36,  1.62it/s] 62%|██████▏   | 255/410 [01:51<01:19,  1.94it/s] 62%|██████▏   | 256/410 [01:52<01:08,  2.25it/s] 63%|██████▎   | 257/410 [01:52<01:00,  2.51it/s] 63%|██████▎   | 258/410 [01:52<00:55,  2.76it/s] 63%|██████▎   | 259/410 [01:52<00:51,  2.95it/s] 63%|██████▎   | 260/410 [01:53<00:48,  3.11it/s] 64%|██████▎   | 261/410 [01:53<00:46,  3.23it/s] 64%|██████▍   | 262/410 [01:53<00:44,  3.32it/s] 64%|██████▍   | 263/410 [01:54<00:43,  3.38it/s] 64%|██████▍   | 264/410 [01:54<00:42,  3.44it/s] 65%|██████▍   | 265/410 [01:54<00:41,  3.47it/s] 65%|██████▍   | 266/410 [01:54<00:41,  3.50it/s] 65%|██████▌   | 267/410 [01:55<00:40,  3.52it/s] 65%|██████▌   | 268/410 [01:55<00:40,  3.52it/s] 66%|██████▌   | 269/410 [01:55<00:39,  3.53it/s] 66%|██████▌   | 270/410 [01:56<00:39,  3.54it/s] 66%|██████▌   | 271/410 [01:56<00:39,  3.54it/s] 66%|██████▋   | 272/410 [01:56<00:38,  3.54it/s] 67%|██████▋   | 273/410 [01:56<00:38,  3.55it/s] 67%|██████▋   | 274/410 [01:57<00:38,  3.55it/s] 67%|██████▋   | 275/410 [01:57<00:37,  3.55it/s] 67%|██████▋   | 276/410 [01:57<00:37,  3.56it/s] 68%|██████▊   | 277/410 [01:57<00:37,  3.56it/s] 68%|██████▊   | 278/410 [01:58<00:37,  3.56it/s] 68%|██████▊   | 279/410 [01:58<00:37,  3.51it/s] 68%|██████▊   | 280/410 [01:58<00:36,  3.52it/s] 69%|██████▊   | 281/410 [01:59<00:36,  3.53it/s] 69%|██████▉   | 282/410 [01:59<00:36,  3.54it/s] 69%|██████▉   | 283/410 [01:59<00:35,  3.55it/s] 69%|██████▉   | 284/410 [01:59<00:35,  3.55it/s] 70%|██████▉   | 285/410 [02:00<00:35,  3.55it/s] 70%|██████▉   | 286/410 [02:00<00:34,  3.55it/s] 70%|███████   | 287/410 [02:00<00:34,  3.55it/s] 70%|███████   | 288/410 [02:01<00:35,  3.46it/s] 70%|███████   | 289/410 [02:01<00:34,  3.48it/s] 71%|███████   | 290/410 [02:01<00:34,  3.49it/s] 71%|███████   | 291/410 [02:01<00:33,  3.51it/s] 71%|███████   | 292/410 [02:02<00:33,  3.52it/s] 71%|███████▏  | 293/410 [02:02<00:33,  3.53it/s] 72%|███████▏  | 294/410 [02:02<00:32,  3.56it/s] 72%|███████▏  | 295/410 [02:03<00:32,  3.57it/s] 72%|███████▏  | 296/410 [02:03<00:31,  3.59it/s] 72%|███████▏  | 297/410 [02:03<00:31,  3.59it/s] 73%|███████▎  | 298/410 [02:03<00:31,  3.60it/s] 73%|███████▎  | 299/410 [02:04<00:30,  3.60it/s] 73%|███████▎  | 300/410 [02:04<00:30,  3.61it/s] 73%|███████▎  | 301/410 [02:04<00:30,  3.59it/s] 74%|███████▎  | 302/410 [02:05<00:29,  3.60it/s] 74%|███████▍  | 303/410 [02:05<00:29,  3.61it/s] 74%|███████▍  | 304/410 [02:05<00:29,  3.61it/s] 74%|███████▍  | 305/410 [02:05<00:29,  3.61it/s] 75%|███████▍  | 306/410 [02:06<00:28,  3.61it/s] 75%|███████▍  | 307/410 [02:06<00:28,  3.61it/s] 75%|███████▌  | 308/410 [02:06<00:28,  3.62it/s] 75%|███████▌  | 309/410 [02:06<00:27,  3.61it/s] 76%|███████▌  | 310/410 [02:07<00:27,  3.61it/s] 76%|███████▌  | 311/410 [02:07<00:27,  3.61it/s] 76%|███████▌  | 312/410 [02:07<00:27,  3.61it/s] 76%|███████▋  | 313/410 [02:08<00:26,  3.62it/s] 77%|███████▋  | 314/410 [02:08<00:26,  3.62it/s] 77%|███████▋  | 315/410 [02:08<00:26,  3.62it/s] 77%|███████▋  | 316/410 [02:08<00:25,  3.62it/s] 77%|███████▋  | 317/410 [02:09<00:25,  3.61it/s] 78%|███████▊  | 318/410 [02:09<00:25,  3.61it/s] 78%|███████▊  | 319/410 [02:09<00:25,  3.61it/s] 78%|███████▊  | 320/410 [02:09<00:24,  3.60it/s] 78%|███████▊  | 321/410 [02:10<00:24,  3.61it/s] 79%|███████▊  | 322/410 [02:10<00:24,  3.61it/s] 79%|███████▉  | 323/410 [02:10<00:24,  3.61it/s] 79%|███████▉  | 324/410 [02:11<00:23,  3.60it/s] 79%|███████▉  | 325/410 [02:11<00:23,  3.60it/s] 80%|███████▉  | 326/410 [02:11<00:23,  3.60it/s] 80%|███████▉  | 327/410 [02:11<00:23,  3.61it/s] 80%|████████  | 328/410 [02:12<00:21,  3.74it/s][INFO|trainer.py:2140] 2023-08-28 01:47:09,726 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:47:09,726 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 01:47:09,726 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4903, 'eval_samples_per_second': 351.513, 'eval_steps_per_second': 43.95, 'epoch': 3.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.04it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.43it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.41it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.38it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.72it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.38it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.24it/s][A
  8%|▊         | 42/505 [00:00<00:10, 43.93it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.13it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.26it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.41it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.24it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.24it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.12it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.98it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.86it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.83it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.04it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.08it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.37it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.27it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.16it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.02it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 43.90it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.86it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.83it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 43.98it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.13it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.20it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.24it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.07it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 43.97it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 43.81it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.84it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 43.92it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 43.96it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.08it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.16it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.19it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.10it/s][A
 41%|████      | 207/505 [00:04<00:06, 43.83it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 43.58it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 43.52it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 43.69it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 43.78it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.06it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.10it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.16it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 43.94it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 43.78it/s][A
 51%|█████     | 257/505 [00:05<00:05, 43.85it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 43.94it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.01it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.07it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.10it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.24it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.22it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.10it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 43.91it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 43.88it/s][A
 61%|██████    | 307/505 [00:06<00:04, 43.96it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.01it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 43.97it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.17it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.17it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.20it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.13it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 43.97it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 43.87it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.01it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.06it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.05it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.01it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.13it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.13it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.14it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.06it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 43.99it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 43.94it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 43.99it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.05it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.13it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.18it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.03it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.11it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 43.99it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 43.94it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 44.02it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.08it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.07it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.11it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.12it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.17it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.01it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 43.93it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.00it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.04it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.02it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.10it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.02it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.02it/s][A 80%|████████  | 328/410 [02:23<00:21,  3.74it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 01:47:21,211 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 01:47:21,235 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:47:23,308 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:47:23,330 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:47:23,339 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-328/special_tokens_map.json
 80%|████████  | 329/410 [02:26<06:00,  4.44s/it] 80%|████████  | 330/410 [02:26<04:15,  3.20s/it] 81%|████████  | 331/410 [02:26<03:03,  2.32s/it] 81%|████████  | 332/410 [02:27<02:13,  1.71s/it] 81%|████████  | 333/410 [02:27<01:38,  1.28s/it] 81%|████████▏ | 334/410 [02:27<01:14,  1.02it/s] 82%|████████▏ | 335/410 [02:28<00:57,  1.30it/s] 82%|████████▏ | 336/410 [02:28<00:46,  1.60it/s] 82%|████████▏ | 337/410 [02:28<00:38,  1.92it/s] 82%|████████▏ | 338/410 [02:28<00:32,  2.23it/s] 83%|████████▎ | 339/410 [02:29<00:28,  2.50it/s] 83%|████████▎ | 340/410 [02:29<00:25,  2.75it/s] 83%|████████▎ | 341/410 [02:29<00:23,  2.95it/s] 83%|████████▎ | 342/410 [02:30<00:21,  3.11it/s] 84%|████████▎ | 343/410 [02:30<00:20,  3.23it/s] 84%|████████▍ | 344/410 [02:30<00:19,  3.32it/s] 84%|████████▍ | 345/410 [02:30<00:19,  3.39it/s] 84%|████████▍ | 346/410 [02:31<00:18,  3.45it/s] 85%|████████▍ | 347/410 [02:31<00:18,  3.50it/s] 85%|████████▍ | 348/410 [02:31<00:17,  3.53it/s] 85%|████████▌ | 349/410 [02:31<00:17,  3.56it/s] 85%|████████▌ | 350/410 [02:32<00:16,  3.56it/s] 86%|████████▌ | 351/410 [02:32<00:16,  3.57it/s] 86%|████████▌ | 352/410 [02:32<00:16,  3.59it/s] 86%|████████▌ | 353/410 [02:33<00:15,  3.59it/s] 86%|████████▋ | 354/410 [02:33<00:15,  3.59it/s] 87%|████████▋ | 355/410 [02:33<00:15,  3.60it/s] 87%|████████▋ | 356/410 [02:33<00:14,  3.60it/s] 87%|████████▋ | 357/410 [02:34<00:14,  3.61it/s] 87%|████████▋ | 358/410 [02:34<00:14,  3.61it/s] 88%|████████▊ | 359/410 [02:34<00:14,  3.61it/s] 88%|████████▊ | 360/410 [02:35<00:13,  3.61it/s] 88%|████████▊ | 361/410 [02:35<00:13,  3.59it/s] 88%|████████▊ | 362/410 [02:35<00:13,  3.60it/s] 89%|████████▊ | 363/410 [02:35<00:13,  3.60it/s] 89%|████████▉ | 364/410 [02:36<00:12,  3.61it/s] 89%|████████▉ | 365/410 [02:36<00:12,  3.61it/s] 89%|████████▉ | 366/410 [02:36<00:12,  3.61it/s] 90%|████████▉ | 367/410 [02:36<00:11,  3.61it/s] 90%|████████▉ | 368/410 [02:37<00:11,  3.61it/s] 90%|█████████ | 369/410 [02:37<00:11,  3.61it/s] 90%|█████████ | 370/410 [02:37<00:11,  3.61it/s] 90%|█████████ | 371/410 [02:38<00:10,  3.61it/s] 91%|█████████ | 372/410 [02:38<00:10,  3.61it/s] 91%|█████████ | 373/410 [02:38<00:10,  3.61it/s] 91%|█████████ | 374/410 [02:38<00:09,  3.61it/s] 91%|█████████▏| 375/410 [02:39<00:09,  3.61it/s] 92%|█████████▏| 376/410 [02:39<00:09,  3.61it/s] 92%|█████████▏| 377/410 [02:39<00:09,  3.60it/s] 92%|█████████▏| 378/410 [02:40<00:08,  3.61it/s] 92%|█████████▏| 379/410 [02:40<00:08,  3.61it/s] 93%|█████████▎| 380/410 [02:40<00:08,  3.59it/s] 93%|█████████▎| 381/410 [02:40<00:08,  3.57it/s] 93%|█████████▎| 382/410 [02:41<00:07,  3.56it/s] 93%|█████████▎| 383/410 [02:41<00:07,  3.56it/s] 94%|█████████▎| 384/410 [02:41<00:07,  3.56it/s] 94%|█████████▍| 385/410 [02:41<00:07,  3.55it/s] 94%|█████████▍| 386/410 [02:42<00:06,  3.55it/s] 94%|█████████▍| 387/410 [02:42<00:06,  3.55it/s] 95%|█████████▍| 388/410 [02:42<00:06,  3.55it/s] 95%|█████████▍| 389/410 [02:43<00:05,  3.56it/s] 95%|█████████▌| 390/410 [02:43<00:05,  3.56it/s] 95%|█████████▌| 391/410 [02:43<00:05,  3.56it/s] 96%|█████████▌| 392/410 [02:43<00:05,  3.55it/s] 96%|█████████▌| 393/410 [02:44<00:04,  3.55it/s] 96%|█████████▌| 394/410 [02:44<00:04,  3.55it/s] 96%|█████████▋| 395/410 [02:44<00:04,  3.55it/s] 97%|█████████▋| 396/410 [02:45<00:03,  3.55it/s] 97%|█████████▋| 397/410 [02:45<00:03,  3.55it/s] 97%|█████████▋| 398/410 [02:45<00:03,  3.55it/s] 97%|█████████▋| 399/410 [02:45<00:03,  3.55it/s] 98%|█████████▊| 400/410 [02:46<00:02,  3.55it/s] 98%|█████████▊| 401/410 [02:46<00:02,  3.56it/s] 98%|█████████▊| 402/410 [02:46<00:02,  3.56it/s] 98%|█████████▊| 403/410 [02:47<00:01,  3.55it/s] 99%|█████████▊| 404/410 [02:47<00:01,  3.55it/s] 99%|█████████▉| 405/410 [02:47<00:01,  3.55it/s] 99%|█████████▉| 406/410 [02:47<00:01,  3.55it/s] 99%|█████████▉| 407/410 [02:48<00:00,  3.55it/s]100%|█████████▉| 408/410 [02:48<00:00,  3.55it/s]100%|█████████▉| 409/410 [02:48<00:00,  3.55it/s]100%|██████████| 410/410 [02:48<00:00,  3.69it/s][INFO|trainer.py:2140] 2023-08-28 01:47:46,543 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:47:46,543 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 01:47:46,544 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4681, 'eval_samples_per_second': 352.194, 'eval_steps_per_second': 44.035, 'epoch': 4.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.10it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.41it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.63it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.45it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.74it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.42it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.06it/s][A
  8%|▊         | 42/505 [00:00<00:10, 43.99it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.11it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.24it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.34it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.35it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.18it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.08it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.75it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.62it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.87it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.03it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.23it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.26it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.14it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.19it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 43.99it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 43.81it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.76it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.88it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.10it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.20it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.23it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.19it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.22it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 43.94it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 43.87it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.88it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.06it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.16it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.14it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.22it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.16it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.15it/s][A
 41%|████      | 207/505 [00:04<00:06, 43.98it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 43.82it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 43.93it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.03it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.15it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.16it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.22it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.08it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.01it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.00it/s][A
 51%|█████     | 257/505 [00:05<00:05, 43.98it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 43.97it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.03it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.11it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.16it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.15it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.15it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 43.98it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.03it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 43.98it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.02it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.00it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.16it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.13it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.14it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.05it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.05it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.01it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 43.99it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.05it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.05it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.11it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.14it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 43.99it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.06it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 43.98it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.05it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.13it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.04it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.08it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.09it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.10it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.03it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 43.98it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.02it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.07it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.04it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 44.08it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 43.90it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.00it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.07it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.11it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.02it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.01it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.03it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.07it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.05it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.07it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 43.98it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.09it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.09it/s][A100%|██████████| 410/410 [03:00<00:00,  3.69it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 01:47:58,039 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-410
[INFO|configuration_utils.py:351] 2023-08-28 01:47:58,075 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-410/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:48:00,685 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-410/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:48:00,707 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-410/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:48:00,720 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-410/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 01:48:01,098 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 01:48:01,098 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-82 (score: 1.2323094606399536).
                                                 100%|██████████| 410/410 [03:05<00:00,  3.69it/s]100%|██████████| 410/410 [03:05<00:00,  2.21it/s]
[INFO|trainer.py:1894] 2023-08-28 01:48:02,836 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 01:48:02,849 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:48:04,781 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:48:04,797 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:48:04,804 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 01:48:05,043 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:48:05,043 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:48:05,043 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:48:05,043 >>   train_runtime            = 0:03:05.29
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:48:05,043 >>   train_samples            =       5240
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:48:05,043 >>   train_samples_per_second =    141.399
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:48:05,043 >>   train_steps_per_second   =      2.213
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4641, 'eval_samples_per_second': 352.317, 'eval_steps_per_second': 44.05, 'epoch': 5.0}
{'train_runtime': 185.2914, 'train_samples_per_second': 141.399, 'train_steps_per_second': 2.213, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 01:48:05 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 01:48:05,080 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:48:05,080 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 01:48:05,080 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:09, 55.07it/s]  2%|▏         | 12/505 [00:00<00:10, 48.60it/s]  3%|▎         | 17/505 [00:00<00:10, 47.13it/s]  4%|▍         | 22/505 [00:00<00:10, 46.35it/s]  5%|▌         | 27/505 [00:00<00:10, 45.76it/s]  6%|▋         | 32/505 [00:00<00:10, 45.36it/s]  7%|▋         | 37/505 [00:00<00:10, 45.14it/s]  8%|▊         | 42/505 [00:00<00:10, 44.65it/s]  9%|▉         | 47/505 [00:01<00:10, 44.10it/s] 10%|█         | 52/505 [00:01<00:10, 43.92it/s] 11%|█▏        | 57/505 [00:01<00:10, 44.02it/s] 12%|█▏        | 62/505 [00:01<00:10, 44.29it/s] 13%|█▎        | 67/505 [00:01<00:09, 44.36it/s] 14%|█▍        | 72/505 [00:01<00:09, 44.47it/s] 15%|█▌        | 77/505 [00:01<00:09, 44.48it/s] 16%|█▌        | 82/505 [00:01<00:09, 44.26it/s] 17%|█▋        | 87/505 [00:01<00:09, 44.03it/s] 18%|█▊        | 92/505 [00:02<00:09, 43.82it/s] 19%|█▉        | 97/505 [00:02<00:09, 43.76it/s] 20%|██        | 102/505 [00:02<00:09, 43.97it/s] 21%|██        | 107/505 [00:02<00:09, 44.07it/s] 22%|██▏       | 112/505 [00:02<00:08, 44.20it/s] 23%|██▎       | 117/505 [00:02<00:08, 44.39it/s] 24%|██▍       | 122/505 [00:02<00:08, 44.54it/s] 25%|██▌       | 127/505 [00:02<00:08, 44.47it/s] 26%|██▌       | 132/505 [00:02<00:08, 44.16it/s] 27%|██▋       | 137/505 [00:03<00:08, 43.99it/s] 28%|██▊       | 142/505 [00:03<00:08, 43.94it/s] 29%|██▉       | 147/505 [00:03<00:08, 44.02it/s] 30%|███       | 152/505 [00:03<00:07, 44.16it/s] 31%|███       | 157/505 [00:03<00:07, 44.25it/s] 32%|███▏      | 162/505 [00:03<00:07, 44.40it/s] 33%|███▎      | 167/505 [00:03<00:07, 44.46it/s] 34%|███▍      | 172/505 [00:03<00:07, 44.31it/s] 35%|███▌      | 177/505 [00:03<00:07, 44.21it/s] 36%|███▌      | 182/505 [00:04<00:07, 43.91it/s] 37%|███▋      | 187/505 [00:04<00:07, 43.95it/s] 38%|███▊      | 192/505 [00:04<00:07, 44.02it/s] 39%|███▉      | 197/505 [00:04<00:06, 44.18it/s] 40%|████      | 202/505 [00:04<00:06, 44.27it/s] 41%|████      | 207/505 [00:04<00:06, 44.27it/s] 42%|████▏     | 212/505 [00:04<00:06, 44.37it/s] 43%|████▎     | 217/505 [00:04<00:06, 44.33it/s] 44%|████▍     | 222/505 [00:04<00:06, 44.12it/s] 45%|████▍     | 227/505 [00:05<00:06, 44.08it/s] 46%|████▌     | 232/505 [00:05<00:06, 43.92it/s] 47%|████▋     | 237/505 [00:05<00:06, 44.03it/s] 48%|████▊     | 242/505 [00:05<00:05, 44.20it/s] 49%|████▉     | 247/505 [00:05<00:05, 44.33it/s] 50%|████▉     | 252/505 [00:05<00:05, 44.42it/s] 51%|█████     | 257/505 [00:05<00:05, 44.28it/s] 52%|█████▏    | 262/505 [00:05<00:05, 44.22it/s] 53%|█████▎    | 267/505 [00:06<00:05, 44.19it/s] 54%|█████▍    | 272/505 [00:06<00:05, 44.08it/s] 55%|█████▍    | 277/505 [00:06<00:05, 44.13it/s] 56%|█████▌    | 282/505 [00:06<00:05, 44.04it/s] 57%|█████▋    | 287/505 [00:06<00:04, 44.21it/s] 58%|█████▊    | 292/505 [00:06<00:04, 44.38it/s] 59%|█████▉    | 297/505 [00:06<00:04, 44.28it/s] 60%|█████▉    | 302/505 [00:06<00:04, 44.25it/s] 61%|██████    | 307/505 [00:06<00:04, 44.10it/s] 62%|██████▏   | 312/505 [00:07<00:04, 44.07it/s] 63%|██████▎   | 317/505 [00:07<00:04, 44.01it/s] 64%|██████▍   | 322/505 [00:07<00:04, 44.03it/s] 65%|██████▍   | 327/505 [00:07<00:04, 44.11it/s] 66%|██████▌   | 332/505 [00:07<00:03, 44.21it/s] 67%|██████▋   | 337/505 [00:07<00:03, 44.27it/s] 68%|██████▊   | 342/505 [00:07<00:03, 44.38it/s] 69%|██████▊   | 347/505 [00:07<00:03, 44.27it/s] 70%|██████▉   | 352/505 [00:07<00:03, 44.21it/s] 71%|███████   | 357/505 [00:08<00:03, 44.01it/s] 72%|███████▏  | 362/505 [00:08<00:03, 44.02it/s] 73%|███████▎  | 367/505 [00:08<00:03, 44.04it/s] 74%|███████▎  | 372/505 [00:08<00:03, 44.13it/s] 75%|███████▍  | 377/505 [00:08<00:02, 44.25it/s] 76%|███████▌  | 382/505 [00:08<00:02, 44.31it/s] 77%|███████▋  | 387/505 [00:08<00:02, 44.25it/s] 78%|███████▊  | 392/505 [00:08<00:02, 44.20it/s] 79%|███████▊  | 397/505 [00:08<00:02, 44.01it/s] 80%|███████▉  | 402/505 [00:09<00:02, 44.09it/s] 81%|████████  | 407/505 [00:09<00:02, 44.06it/s] 82%|████████▏ | 412/505 [00:09<00:02, 44.10it/s] 83%|████████▎ | 417/505 [00:09<00:01, 44.22it/s] 84%|████████▎ | 422/505 [00:09<00:01, 44.25it/s] 85%|████████▍ | 427/505 [00:09<00:01, 44.30it/s] 86%|████████▌ | 432/505 [00:09<00:01, 44.28it/s] 87%|████████▋ | 437/505 [00:09<00:01, 44.17it/s] 88%|████████▊ | 442/505 [00:09<00:01, 44.03it/s] 89%|████████▊ | 447/505 [00:10<00:01, 44.03it/s] 90%|████████▉ | 452/505 [00:10<00:01, 44.04it/s] 90%|█████████ | 457/505 [00:10<00:01, 44.16it/s] 91%|█████████▏| 462/505 [00:10<00:00, 44.15it/s] 92%|█████████▏| 467/505 [00:10<00:00, 44.17it/s] 93%|█████████▎| 472/505 [00:10<00:00, 44.28it/s] 94%|█████████▍| 477/505 [00:10<00:00, 44.23it/s] 95%|█████████▌| 482/505 [00:10<00:00, 44.22it/s] 96%|█████████▋| 487/505 [00:10<00:00, 44.11it/s] 97%|█████████▋| 492/505 [00:11<00:00, 43.97it/s] 98%|█████████▊| 497/505 [00:11<00:00, 43.97it/s] 99%|█████████▉| 502/505 [00:11<00:00, 44.09it/s]100%|██████████| 505/505 [00:11<00:00, 44.25it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 01:48:16,508 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:48:16,508 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:48:16,509 >>   eval_loss               =     1.2323
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:48:16,509 >>   eval_runtime            = 0:00:11.42
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:48:16,509 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:48:16,509 >>   eval_samples_per_second =    353.425
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:48:16,509 >>   eval_steps_per_second   =     44.189
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:48:16,509 >>   perplexity              =     3.4291
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:48:22,142 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:48:22,146 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:48:22,147 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:48:22,147 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:48:22,147 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:48:22,422 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:48:22,423 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:48:23,085 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:48:24,239 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:48:24,240 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:48:27,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:48:27,243 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:48:27,243 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:48:27,243 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:48:27,243 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:48:27,850 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:48:27,851 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:48:28,423 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:48:28,593 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:48:28,593 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-246
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-164
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-410
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-82
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-328
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:02,  1.71it/s]Extractor Predicting: 6it [00:03,  1.66it/s]Extractor Predicting: 7it [00:04,  1.70it/s]Extractor Predicting: 8it [00:04,  1.72it/s]Extractor Predicting: 9it [00:05,  1.72it/s]Extractor Predicting: 10it [00:05,  1.74it/s]Extractor Predicting: 11it [00:06,  1.73it/s]Extractor Predicting: 12it [00:07,  1.68it/s]Extractor Predicting: 13it [00:07,  1.68it/s]Extractor Predicting: 14it [00:08,  1.68it/s]Extractor Predicting: 15it [00:08,  1.70it/s]Extractor Predicting: 16it [00:09,  1.68it/s]Extractor Predicting: 17it [00:10,  1.66it/s]Extractor Predicting: 18it [00:10,  1.71it/s]Extractor Predicting: 19it [00:11,  1.70it/s]Extractor Predicting: 20it [00:11,  1.69it/s]Extractor Predicting: 21it [00:12,  1.71it/s]Extractor Predicting: 22it [00:13,  1.68it/s]Extractor Predicting: 23it [00:13,  1.64it/s]Extractor Predicting: 24it [00:14,  1.45it/s]Extractor Predicting: 25it [00:15,  1.53it/s]Extractor Predicting: 26it [00:15,  1.59it/s]Extractor Predicting: 27it [00:16,  1.66it/s]Extractor Predicting: 28it [00:16,  1.66it/s]Extractor Predicting: 29it [00:17,  1.69it/s]Extractor Predicting: 30it [00:18,  1.66it/s]Extractor Predicting: 31it [00:18,  1.69it/s]Extractor Predicting: 32it [00:19,  1.68it/s]Extractor Predicting: 33it [00:19,  1.68it/s]Extractor Predicting: 34it [00:20,  1.67it/s]Extractor Predicting: 35it [00:21,  1.66it/s]Extractor Predicting: 36it [00:21,  1.73it/s]Extractor Predicting: 37it [00:22,  1.79it/s]Extractor Predicting: 38it [00:22,  1.76it/s]Extractor Predicting: 39it [00:23,  1.79it/s]Extractor Predicting: 40it [00:23,  1.76it/s]Extractor Predicting: 41it [00:24,  1.74it/s]Extractor Predicting: 42it [00:24,  1.73it/s]Extractor Predicting: 43it [00:25,  1.78it/s]Extractor Predicting: 44it [00:26,  1.76it/s]Extractor Predicting: 45it [00:26,  1.79it/s]Extractor Predicting: 46it [00:27,  1.75it/s]Extractor Predicting: 47it [00:27,  1.73it/s]Extractor Predicting: 48it [00:28,  1.65it/s]Extractor Predicting: 49it [00:29,  1.70it/s]Extractor Predicting: 50it [00:29,  1.60it/s]Extractor Predicting: 51it [00:30,  1.66it/s]Extractor Predicting: 52it [00:30,  1.73it/s]Extractor Predicting: 53it [00:31,  1.73it/s]Extractor Predicting: 54it [00:31,  1.75it/s]Extractor Predicting: 55it [00:32,  1.74it/s]Extractor Predicting: 56it [00:33,  1.76it/s]Extractor Predicting: 57it [00:33,  1.77it/s]Extractor Predicting: 58it [00:34,  1.71it/s]Extractor Predicting: 59it [00:34,  1.73it/s]Extractor Predicting: 60it [00:35,  1.72it/s]Extractor Predicting: 61it [00:35,  1.77it/s]Extractor Predicting: 62it [00:36,  1.77it/s]Extractor Predicting: 63it [00:37,  1.72it/s]Extractor Predicting: 64it [00:37,  1.72it/s]Extractor Predicting: 65it [00:38,  1.71it/s]Extractor Predicting: 66it [00:38,  1.73it/s]Extractor Predicting: 67it [00:39,  1.71it/s]Extractor Predicting: 68it [00:40,  1.66it/s]Extractor Predicting: 69it [00:40,  1.63it/s]Extractor Predicting: 70it [00:41,  1.67it/s]Extractor Predicting: 71it [00:41,  1.67it/s]Extractor Predicting: 72it [00:42,  1.62it/s]Extractor Predicting: 73it [00:43,  1.62it/s]Extractor Predicting: 74it [00:43,  1.56it/s]Extractor Predicting: 75it [00:44,  1.59it/s]Extractor Predicting: 76it [00:45,  1.63it/s]Extractor Predicting: 77it [00:45,  1.63it/s]Extractor Predicting: 78it [00:46,  1.63it/s]Extractor Predicting: 79it [00:46,  1.64it/s]Extractor Predicting: 80it [00:47,  1.64it/s]Extractor Predicting: 81it [00:48,  1.66it/s]Extractor Predicting: 82it [00:48,  1.71it/s]Extractor Predicting: 83it [00:49,  1.66it/s]Extractor Predicting: 84it [00:49,  1.68it/s]Extractor Predicting: 85it [00:50,  1.67it/s]Extractor Predicting: 86it [00:51,  1.69it/s]Extractor Predicting: 87it [00:51,  1.72it/s]Extractor Predicting: 88it [00:52,  1.73it/s]Extractor Predicting: 89it [00:52,  1.72it/s]Extractor Predicting: 90it [00:53,  1.74it/s]Extractor Predicting: 91it [00:53,  1.75it/s]Extractor Predicting: 92it [00:54,  1.77it/s]Extractor Predicting: 93it [00:54,  1.76it/s]Extractor Predicting: 94it [00:55,  1.73it/s]Extractor Predicting: 95it [00:56,  1.71it/s]Extractor Predicting: 96it [00:56,  1.68it/s]Extractor Predicting: 97it [00:57,  1.69it/s]Extractor Predicting: 98it [00:57,  1.69it/s]Extractor Predicting: 99it [00:58,  1.68it/s]Extractor Predicting: 100it [00:59,  1.69it/s]Extractor Predicting: 101it [00:59,  1.65it/s]Extractor Predicting: 102it [01:00,  1.65it/s]Extractor Predicting: 103it [01:01,  1.66it/s]Extractor Predicting: 104it [01:01,  1.70it/s]Extractor Predicting: 105it [01:02,  1.69it/s]Extractor Predicting: 106it [01:02,  1.73it/s]Extractor Predicting: 107it [01:03,  1.72it/s]Extractor Predicting: 108it [01:03,  1.74it/s]Extractor Predicting: 109it [01:04,  1.74it/s]Extractor Predicting: 110it [01:05,  1.72it/s]Extractor Predicting: 111it [01:05,  1.67it/s]Extractor Predicting: 112it [01:06,  1.72it/s]Extractor Predicting: 113it [01:06,  1.70it/s]Extractor Predicting: 114it [01:07,  1.68it/s]Extractor Predicting: 115it [01:07,  1.73it/s]Extractor Predicting: 116it [01:08,  1.73it/s]Extractor Predicting: 117it [01:09,  1.56it/s]Extractor Predicting: 118it [01:09,  1.55it/s]Extractor Predicting: 119it [01:10,  1.62it/s]Extractor Predicting: 120it [01:11,  1.65it/s]Extractor Predicting: 121it [01:11,  1.63it/s]Extractor Predicting: 122it [01:12,  1.66it/s]Extractor Predicting: 123it [01:12,  1.69it/s]Extractor Predicting: 124it [01:13,  1.70it/s]Extractor Predicting: 125it [01:14,  1.55it/s]Extractor Predicting: 126it [01:14,  1.61it/s]Extractor Predicting: 127it [01:15,  1.56it/s]Extractor Predicting: 128it [01:16,  1.54it/s]Extractor Predicting: 129it [01:16,  1.55it/s]Extractor Predicting: 130it [01:17,  1.55it/s]Extractor Predicting: 131it [01:18,  1.52it/s]Extractor Predicting: 132it [01:18,  1.52it/s]Extractor Predicting: 133it [01:19,  1.53it/s]Extractor Predicting: 134it [01:20,  1.58it/s]Extractor Predicting: 135it [01:20,  1.60it/s]Extractor Predicting: 136it [01:21,  1.60it/s]Extractor Predicting: 137it [01:21,  1.62it/s]Extractor Predicting: 138it [01:22,  1.57it/s]Extractor Predicting: 139it [01:23,  1.57it/s]Extractor Predicting: 140it [01:23,  1.56it/s]Extractor Predicting: 141it [01:24,  1.57it/s]Extractor Predicting: 142it [01:25,  1.58it/s]Extractor Predicting: 143it [01:25,  1.57it/s]Extractor Predicting: 144it [01:26,  1.59it/s]Extractor Predicting: 145it [01:26,  1.57it/s]Extractor Predicting: 146it [01:27,  1.58it/s]Extractor Predicting: 147it [01:28,  1.60it/s]Extractor Predicting: 148it [01:28,  1.55it/s]Extractor Predicting: 149it [01:29,  1.61it/s]Extractor Predicting: 149it [01:29,  1.67it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:05,721 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:05,725 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:05,725 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:05,725 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:05,725 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:50:06,323 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:50:06,324 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:50:06,897 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:50:07,917 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:50:07,917 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:10,772 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:10,774 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:10,775 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:10,775 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:10,775 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:50:11,423 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:50:11,424 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:50:12,001 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:50:12,165 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:50:12,165 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.78it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.74it/s]Extractor Predicting: 4it [00:02,  1.83it/s]Extractor Predicting: 5it [00:02,  1.78it/s]Extractor Predicting: 6it [00:03,  1.78it/s]Extractor Predicting: 7it [00:03,  1.78it/s]Extractor Predicting: 8it [00:04,  1.85it/s]Extractor Predicting: 9it [00:04,  1.95it/s]Extractor Predicting: 10it [00:05,  1.91it/s]Extractor Predicting: 11it [00:05,  1.91it/s]Extractor Predicting: 12it [00:06,  1.91it/s]Extractor Predicting: 13it [00:06,  2.01it/s]Extractor Predicting: 14it [00:07,  1.99it/s]Extractor Predicting: 15it [00:07,  2.01it/s]Extractor Predicting: 16it [00:08,  2.00it/s]Extractor Predicting: 17it [00:08,  1.99it/s]Extractor Predicting: 18it [00:09,  1.94it/s]Extractor Predicting: 19it [00:10,  1.89it/s]Extractor Predicting: 20it [00:10,  1.90it/s]Extractor Predicting: 21it [00:11,  1.90it/s]Extractor Predicting: 22it [00:11,  1.93it/s]Extractor Predicting: 23it [00:12,  1.97it/s]Extractor Predicting: 24it [00:12,  1.95it/s]Extractor Predicting: 25it [00:13,  1.98it/s]Extractor Predicting: 26it [00:13,  1.92it/s]Extractor Predicting: 27it [00:14,  2.00it/s]Extractor Predicting: 28it [00:14,  2.03it/s]Extractor Predicting: 29it [00:15,  2.01it/s]Extractor Predicting: 30it [00:15,  2.00it/s]Extractor Predicting: 31it [00:16,  2.03it/s]Extractor Predicting: 32it [00:16,  2.01it/s]Extractor Predicting: 33it [00:17,  1.94it/s]Extractor Predicting: 34it [00:17,  1.91it/s]Extractor Predicting: 35it [00:18,  1.89it/s]Extractor Predicting: 36it [00:18,  1.96it/s]Extractor Predicting: 37it [00:19,  1.93it/s]Extractor Predicting: 38it [00:19,  1.90it/s]Extractor Predicting: 39it [00:20,  1.91it/s]Extractor Predicting: 40it [00:20,  1.87it/s]Extractor Predicting: 41it [00:21,  1.84it/s]Extractor Predicting: 42it [00:21,  1.87it/s]Extractor Predicting: 43it [00:22,  1.90it/s]Extractor Predicting: 44it [00:22,  1.89it/s]Extractor Predicting: 45it [00:23,  1.71it/s]Extractor Predicting: 46it [00:24,  1.70it/s]Extractor Predicting: 47it [00:24,  1.73it/s]Extractor Predicting: 48it [00:25,  1.76it/s]Extractor Predicting: 49it [00:25,  1.81it/s]Extractor Predicting: 50it [00:26,  1.83it/s]Extractor Predicting: 51it [00:26,  1.89it/s]Extractor Predicting: 52it [00:27,  1.85it/s]Extractor Predicting: 53it [00:28,  1.87it/s]Extractor Predicting: 54it [00:28,  1.87it/s]Extractor Predicting: 55it [00:29,  1.85it/s]Extractor Predicting: 56it [00:29,  1.78it/s]Extractor Predicting: 57it [00:30,  1.79it/s]Extractor Predicting: 58it [00:30,  1.81it/s]Extractor Predicting: 59it [00:31,  1.79it/s]Extractor Predicting: 60it [00:31,  1.80it/s]Extractor Predicting: 61it [00:32,  1.80it/s]Extractor Predicting: 62it [00:32,  1.84it/s]Extractor Predicting: 63it [00:33,  1.81it/s]Extractor Predicting: 64it [00:34,  1.84it/s]Extractor Predicting: 65it [00:34,  1.87it/s]Extractor Predicting: 66it [00:35,  1.88it/s]Extractor Predicting: 67it [00:35,  1.85it/s]Extractor Predicting: 68it [00:36,  1.85it/s]Extractor Predicting: 69it [00:36,  1.80it/s]Extractor Predicting: 70it [00:37,  1.78it/s]Extractor Predicting: 71it [00:38,  1.62it/s]Extractor Predicting: 72it [00:38,  1.68it/s]Extractor Predicting: 73it [00:39,  1.70it/s]Extractor Predicting: 74it [00:39,  1.73it/s]Extractor Predicting: 75it [00:40,  1.78it/s]Extractor Predicting: 76it [00:40,  1.77it/s]Extractor Predicting: 77it [00:41,  1.79it/s]Extractor Predicting: 78it [00:42,  1.73it/s]Extractor Predicting: 79it [00:42,  1.77it/s]Extractor Predicting: 80it [00:43,  1.82it/s]Extractor Predicting: 81it [00:43,  1.79it/s]Extractor Predicting: 82it [00:44,  1.76it/s]Extractor Predicting: 83it [00:44,  1.77it/s]Extractor Predicting: 84it [00:45,  1.80it/s]Extractor Predicting: 85it [00:45,  1.79it/s]Extractor Predicting: 86it [00:46,  1.76it/s]Extractor Predicting: 87it [00:47,  1.80it/s]Extractor Predicting: 88it [00:47,  1.82it/s]Extractor Predicting: 89it [00:48,  1.88it/s]Extractor Predicting: 90it [00:48,  1.86it/s]Extractor Predicting: 91it [00:49,  1.85it/s]Extractor Predicting: 92it [00:49,  1.85it/s]Extractor Predicting: 93it [00:50,  1.90it/s]Extractor Predicting: 94it [00:50,  1.91it/s]Extractor Predicting: 95it [00:51,  1.88it/s]Extractor Predicting: 96it [00:51,  1.92it/s]Extractor Predicting: 97it [00:52,  1.95it/s]Extractor Predicting: 98it [00:52,  1.89it/s]Extractor Predicting: 99it [00:53,  1.87it/s]Extractor Predicting: 100it [00:53,  1.90it/s]Extractor Predicting: 101it [00:54,  1.88it/s]Extractor Predicting: 102it [00:55,  1.83it/s]Extractor Predicting: 103it [00:55,  1.82it/s]Extractor Predicting: 104it [00:56,  1.87it/s]Extractor Predicting: 105it [00:56,  1.80it/s]Extractor Predicting: 106it [00:57,  1.83it/s]Extractor Predicting: 107it [00:57,  1.84it/s]Extractor Predicting: 108it [00:58,  1.80it/s]Extractor Predicting: 109it [00:58,  1.79it/s]Extractor Predicting: 110it [00:59,  1.82it/s]Extractor Predicting: 111it [00:59,  1.81it/s]Extractor Predicting: 112it [01:00,  1.82it/s]Extractor Predicting: 113it [01:01,  1.85it/s]Extractor Predicting: 114it [01:01,  1.85it/s]Extractor Predicting: 115it [01:02,  1.85it/s]Extractor Predicting: 116it [01:02,  1.88it/s]Extractor Predicting: 117it [01:03,  1.88it/s]Extractor Predicting: 118it [01:03,  1.93it/s]Extractor Predicting: 119it [01:04,  1.95it/s]Extractor Predicting: 120it [01:04,  1.94it/s]Extractor Predicting: 121it [01:05,  1.94it/s]Extractor Predicting: 122it [01:05,  1.92it/s]Extractor Predicting: 123it [01:06,  1.89it/s]Extractor Predicting: 124it [01:06,  1.91it/s]Extractor Predicting: 125it [01:07,  1.90it/s]Extractor Predicting: 126it [01:07,  1.88it/s]Extractor Predicting: 127it [01:08,  1.83it/s]Extractor Predicting: 128it [01:09,  1.81it/s]Extractor Predicting: 129it [01:09,  1.78it/s]Extractor Predicting: 130it [01:10,  1.76it/s]Extractor Predicting: 131it [01:10,  1.73it/s]Extractor Predicting: 132it [01:11,  1.75it/s]Extractor Predicting: 133it [01:11,  1.77it/s]Extractor Predicting: 134it [01:12,  1.81it/s]Extractor Predicting: 135it [01:13,  1.74it/s]Extractor Predicting: 136it [01:13,  1.73it/s]Extractor Predicting: 137it [01:14,  1.72it/s]Extractor Predicting: 138it [01:14,  1.74it/s]Extractor Predicting: 139it [01:15,  1.75it/s]Extractor Predicting: 140it [01:15,  1.71it/s]Extractor Predicting: 141it [01:16,  1.72it/s]Extractor Predicting: 142it [01:17,  1.71it/s]Extractor Predicting: 143it [01:17,  1.74it/s]Extractor Predicting: 144it [01:18,  1.73it/s]Extractor Predicting: 145it [01:18,  1.72it/s]Extractor Predicting: 146it [01:19,  1.72it/s]Extractor Predicting: 147it [01:19,  1.74it/s]Extractor Predicting: 148it [01:20,  1.74it/s]Extractor Predicting: 149it [01:21,  1.78it/s]Extractor Predicting: 150it [01:21,  1.77it/s]Extractor Predicting: 151it [01:22,  1.75it/s]Extractor Predicting: 152it [01:22,  1.76it/s]Extractor Predicting: 153it [01:23,  1.72it/s]Extractor Predicting: 154it [01:24,  1.71it/s]Extractor Predicting: 155it [01:24,  1.73it/s]Extractor Predicting: 156it [01:25,  1.78it/s]Extractor Predicting: 157it [01:25,  1.86it/s]Extractor Predicting: 158it [01:26,  1.87it/s]Extractor Predicting: 159it [01:26,  1.87it/s]Extractor Predicting: 160it [01:27,  1.88it/s]Extractor Predicting: 161it [01:27,  1.73it/s]Extractor Predicting: 162it [01:28,  1.74it/s]Extractor Predicting: 163it [01:28,  1.76it/s]Extractor Predicting: 164it [01:29,  1.81it/s]Extractor Predicting: 165it [01:29,  1.87it/s]Extractor Predicting: 166it [01:30,  1.88it/s]Extractor Predicting: 167it [01:30,  1.96it/s]Extractor Predicting: 168it [01:31,  1.97it/s]Extractor Predicting: 169it [01:31,  2.01it/s]Extractor Predicting: 170it [01:32,  1.95it/s]Extractor Predicting: 171it [01:33,  1.87it/s]Extractor Predicting: 172it [01:33,  1.83it/s]Extractor Predicting: 173it [01:34,  1.89it/s]Extractor Predicting: 173it [01:34,  1.84it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:51:53,640 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:51:53,645 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:51:53,645 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:51:53,645 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:51:53,645 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:51:54,261 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:51:54,262 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:51:54,874 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:51:55,877 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:51:55,877 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:51:58,733 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:51:58,739 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:51:58,740 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:51:58,740 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:51:58,740 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:51:59,374 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:51:59,375 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:51:59,949 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:52:00,124 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:52:00,124 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.83it/s]Extractor Predicting: 2it [00:01,  1.73it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.68it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.59it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:06,  1.58it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:07,  1.62it/s]Extractor Predicting: 14it [00:08,  1.72it/s]Extractor Predicting: 15it [00:08,  1.81it/s]Extractor Predicting: 16it [00:09,  1.92it/s]Extractor Predicting: 17it [00:09,  2.03it/s]Extractor Predicting: 18it [00:10,  2.09it/s]Extractor Predicting: 19it [00:10,  2.10it/s]Extractor Predicting: 20it [00:11,  2.12it/s]Extractor Predicting: 21it [00:11,  2.16it/s]Extractor Predicting: 22it [00:12,  2.11it/s]Extractor Predicting: 23it [00:12,  2.11it/s]Extractor Predicting: 24it [00:13,  2.10it/s]Extractor Predicting: 25it [00:13,  2.11it/s]Extractor Predicting: 26it [00:14,  2.15it/s]Extractor Predicting: 27it [00:14,  2.08it/s]Extractor Predicting: 28it [00:15,  2.11it/s]Extractor Predicting: 29it [00:15,  2.12it/s]Extractor Predicting: 30it [00:15,  2.17it/s]Extractor Predicting: 31it [00:16,  2.20it/s]Extractor Predicting: 32it [00:16,  2.19it/s]Extractor Predicting: 33it [00:17,  2.19it/s]Extractor Predicting: 34it [00:17,  2.16it/s]Extractor Predicting: 35it [00:18,  2.16it/s]Extractor Predicting: 36it [00:18,  2.10it/s]Extractor Predicting: 37it [00:19,  2.06it/s]Extractor Predicting: 38it [00:19,  2.14it/s]Extractor Predicting: 39it [00:20,  2.13it/s]Extractor Predicting: 40it [00:20,  2.13it/s]Extractor Predicting: 41it [00:21,  2.10it/s]Extractor Predicting: 42it [00:21,  2.15it/s]Extractor Predicting: 43it [00:22,  2.11it/s]Extractor Predicting: 44it [00:22,  1.93it/s]Extractor Predicting: 45it [00:23,  1.82it/s]Extractor Predicting: 46it [00:23,  1.74it/s]Extractor Predicting: 47it [00:24,  1.71it/s]Extractor Predicting: 48it [00:25,  1.68it/s]Extractor Predicting: 49it [00:25,  1.68it/s]Extractor Predicting: 50it [00:26,  1.66it/s]Extractor Predicting: 51it [00:26,  1.65it/s]Extractor Predicting: 52it [00:27,  1.63it/s]Extractor Predicting: 53it [00:28,  1.62it/s]Extractor Predicting: 54it [00:28,  1.62it/s]Extractor Predicting: 55it [00:29,  1.52it/s]Extractor Predicting: 56it [00:30,  1.52it/s]Extractor Predicting: 57it [00:30,  1.52it/s]Extractor Predicting: 58it [00:31,  1.55it/s]Extractor Predicting: 59it [00:31,  1.88it/s]Extractor Predicting: 59it [00:31,  1.86it/s]
[INFO|configuration_utils.py:515] 2023-08-28 01:52:32,691 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:52:32,692 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 01:52:32,696 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:52:32,697 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 01:52:32,700 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 01:52:35,768 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 01:52:35,770 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 01:52:35,797 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:52:35,798 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 01:52:35,814 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:52:35,819 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:52:35,819 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:52:35,819 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:52:35,819 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:52:35,819 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:52:35,819 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 01:52:36,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:36,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:37,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:38,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:38,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:39,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:40,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:40,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:41,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:41,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:42,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:43,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:43,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:44,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:44,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:45,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:46,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:46,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:47,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:48,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:48,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:49,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:50,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:50,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:15<02:16, 15.16s/it][WARNING|generation_utils.py:914] 2023-08-28 01:52:51,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:51,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:52,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:52,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:53,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:54,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:54,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:55,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:55,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:56,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:56,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:57,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:58,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:59,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:59,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:00,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:01,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:01,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:02,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:02,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:03,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:04,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:04,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:05,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:05,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:06,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:06,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:31<02:06, 15.81s/it][WARNING|generation_utils.py:914] 2023-08-28 01:53:07,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:08,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:08,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:09,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:10,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:10,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:11,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:11,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:12,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:13,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:13,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:14,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:14,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:15,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:16,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:16,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:17,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:17,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:18,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:19,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:19,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:20,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:20,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:21,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:22,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:22,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:23,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:23,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:24,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:25,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:25,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:26,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:26,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:27,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:28,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:28,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:29,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:29,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:30,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:30,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:31,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:32,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:32,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:33,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:33,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:58<02:26, 20.99s/it][WARNING|generation_utils.py:914] 2023-08-28 01:53:34,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:35,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:36,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:36,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:37,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:38,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:38,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:39,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:40,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:41,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:41,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:42,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:42,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:43,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:44,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:44,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:45,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:46,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:46,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:47,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:48,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:48,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:49,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:50,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:14<01:54, 19.12s/it][WARNING|generation_utils.py:914] 2023-08-28 01:53:50,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:51,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:52,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:52,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:53,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:53,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:54,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:55,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:55,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:56,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:56,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:57,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:57,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:58,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:59,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:59,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:00,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:00,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:01,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:02,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:02,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:03,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:03,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:04,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:05,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:05,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:06,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:30<01:30, 18.01s/it][WARNING|generation_utils.py:914] 2023-08-28 01:54:06,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:07,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:08,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:08,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:09,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:10,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:10,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:11,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:11,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:12,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:13,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:13,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:14,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:14,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:15,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:16,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:16,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:17,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:18,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:18,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:19,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:19,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:20,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:21,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:21,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:46<01:08, 17.08s/it][WARNING|generation_utils.py:914] 2023-08-28 01:54:22,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:22,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:23,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:23,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:24,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:25,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:26,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:26,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:27,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:28,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:28,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:29,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:30,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:30,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:31,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:31,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:32,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:32,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:33,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:34,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:34,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:35,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:35,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:36,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:01<00:49, 16.45s/it][WARNING|generation_utils.py:914] 2023-08-28 01:54:37,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:37,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:38,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:38,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:39,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:39,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:40,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:41,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:41,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:42,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:42,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:43,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:43,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:44,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:44,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:45,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:45,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:46,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:47,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:47,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:48,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:48,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:49,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:49,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:14<00:30, 15.42s/it][WARNING|generation_utils.py:914] 2023-08-28 01:54:50,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:51,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:51,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:52,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:53,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:53,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:54,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:55,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:55,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:56,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:57,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:57,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:58,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:59,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:59,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:00,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:00,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:01,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:02,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:02,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:03,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:03,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:28<00:14, 14.95s/it][WARNING|generation_utils.py:914] 2023-08-28 01:55:04,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:05,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:05,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:06,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:07,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:07,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:08,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:08,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:09,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:10,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:10,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:11,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:12,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:12,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:13,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:14,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:14,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:15,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:16,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:16,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:17,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:17,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:18,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:43<00:00, 14.87s/it]Generating: 100%|██████████| 10/10 [02:43<00:00, 16.31s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:25,227 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:25,232 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:25,232 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:25,232 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:25,232 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:55:25,821 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:55:25,822 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:55:26,395 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:55:27,482 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:55:27,482 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:30,350 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:30,354 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:30,354 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:30,354 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:30,354 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:55:30,997 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:55:30,998 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:55:31,579 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:55:31,748 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:55:31,748 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : given name .', 'success_rate': 0.80859375, 'errors': {'', "('William', 'given name', '', 'He was born in the province of Oise , his father being William , Duke of Normandy , 1st Earl of Normandy , and his mother being Mary .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 201, 'raw': 288}
{'target': 600, 'success': 226, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 287, 'raw': 416}
{'target': 600, 'success': 310, 'raw': 448}
{'target': 600, 'success': 333, 'raw': 480}
{'target': 600, 'success': 351, 'raw': 512}
{'target': 600, 'success': 377, 'raw': 544}
{'target': 600, 'success': 403, 'raw': 576}
{'target': 600, 'success': 424, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 468, 'raw': 672}
{'target': 600, 'success': 486, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 532, 'raw': 768}
{'target': 600, 'success': 554, 'raw': 800}
{'target': 600, 'success': 576, 'raw': 832}
{'target': 600, 'success': 601, 'raw': 864}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.6956018518518519, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Brazil', 'languages spoken, written or signed', '', 'Many languages are spoken in Brazil , and there are various communities and organizations advocating for a stronger free and open society .')", "('Estonia', 'languages spoken, written or signed', '', 'The language of Estonia is Estonian , the official language of the Republic of the Union ( DSO ) ) .')"}}
['Relation : lowest point . Context : Later in the year ( 1143 ) , Puyi and his allies made a conquest over the eastern part of the province of Sinti , which was occupied by the Franks and Normans . Head Entity : Sinti , Tail Entity : southern part .\n']
['Relation : lowest point . Context : Later in the year ( 1143 ) , Puyi and his allies made a conquest over the eastern part of the province of Sinti , which was occupied by the Franks and Normans . Head Entity : Sinti , Tail Entity : southern part .\n', 'Relation : lowest point . Context : After the death of Emperor Xuanzong his elder son Liu Bang was the first emperor to die at the age of only two . Head Entity : emperor , Tail Entity : highest point .\n']
{'target': 600, 'success': 12, 'raw': 32}
{'target': 600, 'success': 23, 'raw': 64}
{'target': 600, 'success': 32, 'raw': 96}
{'target': 600, 'success': 44, 'raw': 128}
{'target': 600, 'success': 55, 'raw': 160}
{'target': 600, 'success': 70, 'raw': 192}
{'target': 600, 'success': 86, 'raw': 224}
{'target': 600, 'success': 102, 'raw': 256}
{'target': 600, 'success': 115, 'raw': 288}
{'target': 600, 'success': 125, 'raw': 320}
{'target': 600, 'success': 141, 'raw': 352}
{'target': 600, 'success': 158, 'raw': 384}
{'target': 600, 'success': 176, 'raw': 416}
{'target': 600, 'success': 192, 'raw': 448}
{'target': 600, 'success': 206, 'raw': 480}
{'target': 600, 'success': 221, 'raw': 512}
{'target': 600, 'success': 233, 'raw': 544}
{'target': 600, 'success': 249, 'raw': 576}
{'target': 600, 'success': 265, 'raw': 608}
{'target': 600, 'success': 273, 'raw': 640}
{'target': 600, 'success': 288, 'raw': 672}
{'target': 600, 'success': 308, 'raw': 704}
{'target': 600, 'success': 317, 'raw': 736}
{'target': 600, 'success': 332, 'raw': 768}
{'target': 600, 'success': 342, 'raw': 800}
{'target': 600, 'success': 353, 'raw': 832}
{'target': 600, 'success': 369, 'raw': 864}
{'target': 600, 'success': 385, 'raw': 896}
{'target': 600, 'success': 394, 'raw': 928}
{'target': 600, 'success': 409, 'raw': 960}
{'target': 600, 'success': 423, 'raw': 992}
{'target': 600, 'success': 432, 'raw': 1024}
{'target': 600, 'success': 446, 'raw': 1056}
{'target': 600, 'success': 460, 'raw': 1088}
{'target': 600, 'success': 473, 'raw': 1120}
{'target': 600, 'success': 486, 'raw': 1152}
{'target': 600, 'success': 498, 'raw': 1184}
{'target': 600, 'success': 512, 'raw': 1216}
{'target': 600, 'success': 522, 'raw': 1248}
{'target': 600, 'success': 533, 'raw': 1280}
{'target': 600, 'success': 550, 'raw': 1312}
{'target': 600, 'success': 560, 'raw': 1344}
{'target': 600, 'success': 572, 'raw': 1376}
{'target': 600, 'success': 584, 'raw': 1408}
{'target': 600, 'success': 600, 'raw': 1440}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.4166666666666667, 'errors': {'', "('2004 FIFA World Cup', 'lowest point', '', 'The next day , he made a triumphant recovery and made his international debut at the 2004 FIFA World Cup against Ecuador .')", 'not enough values to unpack (expected 2, got 1)', "('New York Giants', 'lowest point', '', 'In the second season , a team led by George Foreman and David Southee were allowed to make another bowl game against the New York Giants .')", 'too many values to unpack (expected 2)', "('lowest point', 'lowest point', '', 'For comparison , in 1998 , it was the lowest point per square mile since 1960 , when it was 1 . 10 .')", "('63', 'lowest point', '', 'On the night of 9 November 2014 , the Southeastern Conference led by Jaylen Brown was led by Duke Blue Devils in a 63 66 victory over the Southeast Division rival St. Charles .')", "('second', 'lowest point', '', 'He is currently playing with fellow former New Zealand prop Tom Paine in the Championship side Auckland Roosters , and he moved up the rankings to second .')", "('Manchester', 'lowest point', '', 'The area was home to the city of Manchester ( now Manchester City ) , founded in 1859 by the English Civil War hero Sir Walter Scott .')"}}
['Relation : mother . Context : Later in Life , he came to love the beauty and natural beauty of the forests at the end of the third trimester , when she suffered death . Head Entity : forests at the end of the third trimester , Tail Entity : Mother .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : mother .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('king', 'mother', '', 'The first male and last female monarch of the Kingdom of Mexico , the king of the Andes ( d.')"}}
['Relation : mouth of the watercourse . Context : Later in the year ( 1143 ) , Puyi and his allies made atone for the fall of the last King of the Puebla , Puzco . Head Entity : Puyi , Tail Entity : Puducese .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 200, 'raw': 288}
{'target': 600, 'success': 221, 'raw': 320}
{'target': 600, 'success': 241, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 289, 'raw': 416}
{'target': 600, 'success': 311, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 357, 'raw': 512}
{'target': 600, 'success': 383, 'raw': 544}
{'target': 600, 'success': 404, 'raw': 576}
{'target': 600, 'success': 425, 'raw': 608}
{'target': 600, 'success': 440, 'raw': 640}
{'target': 600, 'success': 463, 'raw': 672}
{'target': 600, 'success': 488, 'raw': 704}
{'target': 600, 'success': 509, 'raw': 736}
{'target': 600, 'success': 536, 'raw': 768}
{'target': 600, 'success': 558, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 606, 'raw': 864}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.7013888888888888, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n']
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n', 'Relation : genre . Context : Eileen McClelland ( born June 24 , 1953 ) is an American radio talk show host who has appeared as a regular on both the Howard Stern Show and the Morning Mix . Head Entity : Howard Stern Show , Tail Entity : television talk show .\n']
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n', 'Relation : genre . Context : Eileen McClelland ( born June 24 , 1953 ) is an American radio talk show host who has appeared as a regular on both the Howard Stern Show and the Morning Mix . Head Entity : Howard Stern Show , Tail Entity : television talk show .\n', 'Relation : genre . Context : This film explores the social , social structure of the New York city of Times Square , which is populated and industrialized by wealthy Manhattanites . Head Entity : Times Square , Tail Entity : New York City .\n']
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 320, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 468, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 570, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : genre .', 'success_rate': 0.76625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.8020833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('The albums', 'is a list of', '', 'The albums songs , songs played and discography of the albums members , the artist , are listed in alphabetical order .')", "('programming languages', 'is a list of', '', 'It can be used primarily in conjunction with other programming languages such as .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8111979166666666, 'errors': {'', "('Pluto', 'located on astronomical body', '', 'In the past , Pluto has been considered to be a binary system inhabited by one giant being , as well as being the first to be named after Pluto .')", 'too many values to unpack (expected 2)', "('Its orbit', 'located on astronomical body', '', 'Its orbit shows an eccentricity of 0 . 18 and an inclination of 8 degrees from the plane of the ecliptic .')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('it', 'manufacturer', '', 'It is known in Japan for its high quality and performance components , along with its long range of performance airsoft s.')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8179347826086957, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/2_ext.jsonl'}}
estimate vocab size: 12731
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12831, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.69it/s]Extractor Estimating: 2it [00:01,  1.46it/s]Extractor Estimating: 3it [00:02,  1.45it/s]Extractor Estimating: 4it [00:02,  1.48it/s]Extractor Estimating: 5it [00:03,  1.53it/s]Extractor Estimating: 6it [00:03,  1.56it/s]Extractor Estimating: 7it [00:04,  1.57it/s]Extractor Estimating: 8it [00:05,  1.57it/s]Extractor Estimating: 9it [00:05,  1.61it/s]Extractor Estimating: 10it [00:06,  1.62it/s]Extractor Estimating: 11it [00:06,  1.64it/s]Extractor Estimating: 12it [00:07,  1.68it/s]Extractor Estimating: 13it [00:08,  1.65it/s]Extractor Estimating: 14it [00:08,  1.62it/s]Extractor Estimating: 15it [00:09,  1.62it/s]Extractor Estimating: 16it [00:10,  1.60it/s]Extractor Estimating: 17it [00:10,  1.59it/s]Extractor Estimating: 18it [00:11,  1.59it/s]Extractor Estimating: 19it [00:11,  1.58it/s]Extractor Estimating: 20it [00:12,  1.63it/s]Extractor Estimating: 21it [00:13,  1.66it/s]Extractor Estimating: 22it [00:13,  1.67it/s]Extractor Estimating: 23it [00:14,  1.58it/s]Extractor Estimating: 24it [00:14,  1.65it/s]Extractor Estimating: 25it [00:15,  1.72it/s]Extractor Estimating: 26it [00:16,  1.77it/s]Extractor Estimating: 27it [00:16,  1.75it/s]Extractor Estimating: 28it [00:17,  1.77it/s]Extractor Estimating: 29it [00:17,  1.80it/s]Extractor Estimating: 30it [00:18,  1.81it/s]Extractor Estimating: 31it [00:18,  1.78it/s]Extractor Estimating: 32it [00:19,  1.79it/s]Extractor Estimating: 33it [00:19,  1.81it/s]Extractor Estimating: 34it [00:20,  1.84it/s]Extractor Estimating: 35it [00:21,  1.77it/s]Extractor Estimating: 36it [00:21,  1.76it/s]Extractor Estimating: 37it [00:22,  1.73it/s]Extractor Estimating: 38it [00:22,  1.75it/s]Extractor Estimating: 39it [00:23,  1.80it/s]Extractor Estimating: 40it [00:24,  1.62it/s]Extractor Estimating: 41it [00:24,  1.64it/s]Extractor Estimating: 42it [00:25,  1.69it/s]Extractor Estimating: 43it [00:25,  1.61it/s]Extractor Estimating: 44it [00:26,  1.66it/s]Extractor Estimating: 45it [00:26,  1.73it/s]Extractor Estimating: 46it [00:27,  1.71it/s]Extractor Estimating: 47it [00:28,  1.76it/s]Extractor Estimating: 48it [00:28,  1.79it/s]Extractor Estimating: 49it [00:29,  1.82it/s]Extractor Estimating: 50it [00:29,  1.82it/s]Extractor Estimating: 51it [00:30,  1.74it/s]Extractor Estimating: 52it [00:30,  1.69it/s]Extractor Estimating: 53it [00:31,  1.67it/s]Extractor Estimating: 54it [00:32,  1.65it/s]Extractor Estimating: 55it [00:32,  1.60it/s]Extractor Estimating: 56it [00:33,  1.58it/s]Extractor Estimating: 57it [00:34,  1.55it/s]Extractor Estimating: 58it [00:34,  1.57it/s]Extractor Estimating: 59it [00:35,  1.62it/s]Extractor Estimating: 60it [00:36,  1.60it/s]Extractor Estimating: 61it [00:36,  1.57it/s]Extractor Estimating: 62it [00:37,  1.59it/s]Extractor Estimating: 63it [00:37,  1.55it/s]Extractor Estimating: 64it [00:38,  1.58it/s]Extractor Estimating: 65it [00:39,  1.61it/s]Extractor Estimating: 66it [00:39,  1.58it/s]Extractor Estimating: 67it [00:40,  1.58it/s]Extractor Estimating: 68it [00:41,  1.58it/s]Extractor Estimating: 69it [00:41,  1.57it/s]Extractor Estimating: 70it [00:42,  1.59it/s]Extractor Estimating: 71it [00:43,  1.56it/s]Extractor Estimating: 72it [00:43,  1.57it/s]Extractor Estimating: 73it [00:44,  1.56it/s]Extractor Estimating: 74it [00:44,  1.58it/s]Extractor Estimating: 75it [00:45,  1.58it/s]Extractor Estimating: 76it [00:46,  1.51it/s]Extractor Estimating: 77it [00:46,  1.54it/s]Extractor Estimating: 78it [00:47,  1.52it/s]Extractor Estimating: 79it [00:48,  1.49it/s]Extractor Estimating: 80it [00:48,  1.52it/s]Extractor Estimating: 81it [00:49,  1.53it/s]Extractor Estimating: 82it [00:50,  1.56it/s]Extractor Estimating: 83it [00:50,  1.52it/s]Extractor Estimating: 84it [00:51,  1.51it/s]Extractor Estimating: 85it [00:52,  1.48it/s]Extractor Estimating: 86it [00:53,  1.42it/s]Extractor Estimating: 87it [00:53,  1.43it/s]Extractor Estimating: 88it [00:54,  1.44it/s]Extractor Estimating: 89it [00:55,  1.44it/s]Extractor Estimating: 90it [00:55,  1.45it/s]Extractor Estimating: 91it [00:56,  1.47it/s]Extractor Estimating: 92it [00:57,  1.49it/s]Extractor Estimating: 93it [00:57,  1.52it/s]Extractor Estimating: 94it [00:58,  1.52it/s]Extractor Estimating: 95it [00:59,  1.54it/s]Extractor Estimating: 96it [00:59,  1.51it/s]Extractor Estimating: 97it [01:00,  1.57it/s]Extractor Estimating: 98it [01:01,  1.50it/s]Extractor Estimating: 99it [01:01,  1.49it/s]Extractor Estimating: 100it [01:02,  1.49it/s]Extractor Estimating: 101it [01:03,  1.49it/s]Extractor Estimating: 102it [01:03,  1.54it/s]Extractor Estimating: 103it [01:04,  1.58it/s]Extractor Estimating: 104it [01:04,  1.65it/s]Extractor Estimating: 105it [01:05,  1.62it/s]Extractor Estimating: 106it [01:05,  1.67it/s]Extractor Estimating: 107it [01:06,  1.70it/s]Extractor Estimating: 108it [01:07,  1.69it/s]Extractor Estimating: 109it [01:07,  1.70it/s]Extractor Estimating: 110it [01:08,  1.58it/s]Extractor Estimating: 111it [01:09,  1.60it/s]Extractor Estimating: 112it [01:09,  1.58it/s]Extractor Estimating: 113it [01:10,  1.56it/s]Extractor Estimating: 114it [01:10,  1.64it/s]Extractor Estimating: 115it [01:11,  1.63it/s]Extractor Estimating: 116it [01:12,  1.68it/s]Extractor Estimating: 117it [01:12,  1.64it/s]Extractor Estimating: 118it [01:13,  1.62it/s]Extractor Estimating: 119it [01:14,  1.58it/s]Extractor Estimating: 120it [01:14,  1.62it/s]Extractor Estimating: 121it [01:15,  1.58it/s]Extractor Estimating: 122it [01:15,  1.57it/s]Extractor Estimating: 123it [01:16,  1.59it/s]Extractor Estimating: 124it [01:17,  1.59it/s]Extractor Estimating: 125it [01:17,  1.64it/s]Extractor Estimating: 126it [01:18,  1.65it/s]Extractor Estimating: 127it [01:18,  1.62it/s]Extractor Estimating: 128it [01:19,  1.63it/s]Extractor Estimating: 129it [01:20,  1.63it/s]Extractor Estimating: 130it [01:20,  1.64it/s]Extractor Estimating: 131it [01:21,  1.67it/s]Extractor Estimating: 132it [01:21,  1.65it/s]Extractor Estimating: 133it [01:22,  1.66it/s]Extractor Estimating: 134it [01:23,  1.67it/s]Extractor Estimating: 135it [01:23,  1.62it/s]Extractor Estimating: 136it [01:24,  1.65it/s]Extractor Estimating: 137it [01:25,  1.59it/s]Extractor Estimating: 138it [01:25,  1.61it/s]Extractor Estimating: 139it [01:26,  1.63it/s]Extractor Estimating: 140it [01:26,  1.56it/s]Extractor Estimating: 141it [01:27,  1.57it/s]Extractor Estimating: 142it [01:28,  1.58it/s]Extractor Estimating: 143it [01:28,  1.59it/s]Extractor Estimating: 144it [01:29,  1.56it/s]Extractor Estimating: 145it [01:30,  1.61it/s]Extractor Estimating: 146it [01:30,  1.61it/s]Extractor Estimating: 147it [01:31,  1.64it/s]Extractor Estimating: 148it [01:31,  1.62it/s]Extractor Estimating: 149it [01:32,  1.58it/s]Extractor Estimating: 150it [01:33,  1.65it/s]Extractor Estimating: 151it [01:33,  1.68it/s]Extractor Estimating: 152it [01:34,  1.69it/s]Extractor Estimating: 153it [01:34,  1.72it/s]Extractor Estimating: 154it [01:35,  1.67it/s]Extractor Estimating: 155it [01:36,  1.68it/s]Extractor Estimating: 156it [01:36,  1.66it/s]Extractor Estimating: 157it [01:37,  1.59it/s]Extractor Estimating: 158it [01:38,  1.61it/s]Extractor Estimating: 159it [01:38,  1.65it/s]Extractor Estimating: 160it [01:39,  1.64it/s]Extractor Estimating: 161it [01:39,  1.69it/s]Extractor Estimating: 162it [01:40,  1.66it/s]Extractor Estimating: 163it [01:40,  1.68it/s]Extractor Estimating: 164it [01:41,  1.65it/s]Extractor Estimating: 165it [01:42,  1.65it/s]Extractor Estimating: 166it [01:42,  1.69it/s]Extractor Estimating: 167it [01:43,  1.73it/s]Extractor Estimating: 168it [01:43,  1.73it/s]Extractor Estimating: 169it [01:44,  1.67it/s]Extractor Estimating: 170it [01:45,  1.67it/s]Extractor Estimating: 171it [01:45,  1.67it/s]Extractor Estimating: 172it [01:46,  1.75it/s]Extractor Estimating: 173it [01:46,  1.66it/s]Extractor Estimating: 174it [01:47,  1.51it/s]Extractor Estimating: 175it [01:48,  1.59it/s]Extractor Estimating: 176it [01:48,  1.65it/s]Extractor Estimating: 177it [01:49,  1.66it/s]Extractor Estimating: 178it [01:49,  1.69it/s]Extractor Estimating: 179it [01:50,  1.74it/s]Extractor Estimating: 180it [01:51,  1.72it/s]Extractor Estimating: 181it [01:51,  1.73it/s]Extractor Estimating: 182it [01:52,  1.72it/s]Extractor Estimating: 183it [01:52,  1.78it/s]Extractor Estimating: 184it [01:53,  1.72it/s]Extractor Estimating: 185it [01:53,  1.72it/s]Extractor Estimating: 186it [01:54,  1.69it/s]Extractor Estimating: 187it [01:55,  1.68it/s]Extractor Estimating: 188it [01:55,  1.72it/s]Extractor Estimating: 189it [01:56,  1.70it/s]Extractor Estimating: 190it [01:56,  1.69it/s]Extractor Estimating: 191it [01:57,  1.70it/s]Extractor Estimating: 192it [01:58,  1.70it/s]Extractor Estimating: 193it [01:58,  1.80it/s]Extractor Estimating: 194it [01:59,  1.77it/s]Extractor Estimating: 195it [01:59,  1.69it/s]Extractor Estimating: 196it [02:00,  1.70it/s]Extractor Estimating: 197it [02:00,  1.73it/s]Extractor Estimating: 198it [02:01,  1.71it/s]Extractor Estimating: 199it [02:02,  1.69it/s]Extractor Estimating: 200it [02:02,  1.67it/s]Extractor Estimating: 201it [02:03,  1.67it/s]Extractor Estimating: 202it [02:03,  1.68it/s]Extractor Estimating: 203it [02:04,  1.70it/s]Extractor Estimating: 204it [02:05,  1.71it/s]Extractor Estimating: 205it [02:05,  1.63it/s]Extractor Estimating: 206it [02:06,  1.71it/s]Extractor Estimating: 207it [02:06,  1.67it/s]Extractor Estimating: 208it [02:07,  1.74it/s]Extractor Estimating: 209it [02:08,  1.70it/s]Extractor Estimating: 210it [02:08,  1.69it/s]Extractor Estimating: 211it [02:09,  1.61it/s]Extractor Estimating: 212it [02:09,  1.64it/s]Extractor Estimating: 213it [02:10,  1.67it/s]Extractor Estimating: 214it [02:11,  1.66it/s]Extractor Estimating: 215it [02:11,  1.67it/s]Extractor Estimating: 216it [02:12,  1.72it/s]Extractor Estimating: 217it [02:12,  1.72it/s]Extractor Estimating: 218it [02:13,  1.69it/s]Extractor Estimating: 219it [02:14,  1.71it/s]Extractor Estimating: 220it [02:14,  1.71it/s]Extractor Estimating: 221it [02:15,  1.76it/s]Extractor Estimating: 222it [02:15,  1.73it/s]Extractor Estimating: 223it [02:16,  1.75it/s]Extractor Estimating: 224it [02:16,  1.72it/s]Extractor Estimating: 225it [02:17,  1.68it/s]Extractor Estimating: 226it [02:18,  1.64it/s]Extractor Estimating: 227it [02:18,  1.63it/s]Extractor Estimating: 228it [02:19,  1.63it/s]Extractor Estimating: 229it [02:20,  1.62it/s]Extractor Estimating: 230it [02:20,  1.64it/s]Extractor Estimating: 231it [02:21,  1.64it/s]Extractor Estimating: 232it [02:21,  1.64it/s]Extractor Estimating: 233it [02:22,  1.56it/s]Extractor Estimating: 234it [02:23,  1.56it/s]Extractor Estimating: 235it [02:23,  1.61it/s]Extractor Estimating: 236it [02:24,  1.62it/s]Extractor Estimating: 237it [02:25,  1.56it/s]Extractor Estimating: 238it [02:25,  1.61it/s]Extractor Estimating: 239it [02:26,  1.62it/s]Extractor Estimating: 240it [02:26,  1.64it/s]Extractor Estimating: 241it [02:27,  1.63it/s]Extractor Estimating: 242it [02:28,  1.60it/s]Extractor Estimating: 243it [02:28,  1.66it/s]Extractor Estimating: 244it [02:29,  1.62it/s]Extractor Estimating: 245it [02:30,  1.59it/s]Extractor Estimating: 246it [02:30,  1.60it/s]Extractor Estimating: 247it [02:31,  1.62it/s]Extractor Estimating: 248it [02:31,  1.64it/s]Extractor Estimating: 249it [02:32,  1.64it/s]Extractor Estimating: 250it [02:33,  1.57it/s]Extractor Estimating: 250it [02:33,  1.63it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:20,511 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:20,515 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:20,515 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:20,515 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:20,516 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:58:20,793 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:58:20,794 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:58:21,456 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:58:22,510 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:58:22,510 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:24,187 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:24,196 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:24,196 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:24,196 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:24,196 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:58:24,497 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:58:24,499 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:58:24,752 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:58:24,885 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:58:24,885 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 03:28:19,134 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 03:28:19,166 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 5284 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 22145
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22245, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22245, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.020, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.027, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 79, avg_time 1.026, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 179, avg_time 1.014, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 58, avg_time 1.009, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 158, avg_time 2.133, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 37, avg_time 1.025, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 137, avg_time 1.028, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 16, avg_time 1.010, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 116, avg_time 1.028, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 216, avg_time 2.132, loss:nan
g_step 1200, step 95, avg_time 1.016, loss:nan
g_step 1300, step 195, avg_time 1.029, loss:nan
g_step 1400, step 74, avg_time 1.020, loss:nan
g_step 1500, step 174, avg_time 1.012, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 53, avg_time 2.140, loss:nan
g_step 1700, step 153, avg_time 1.020, loss:nan
g_step 1800, step 32, avg_time 1.005, loss:nan
g_step 1900, step 132, avg_time 1.035, loss:nan
g_step 2000, step 11, avg_time 1.011, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 111, avg_time 2.132, loss:nan
g_step 2200, step 211, avg_time 1.028, loss:nan
g_step 2300, step 90, avg_time 1.009, loss:nan
g_step 2400, step 190, avg_time 1.027, loss:nan
g_step 2500, step 69, avg_time 1.013, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 169, avg_time 2.128, loss:nan
g_step 2700, step 48, avg_time 1.019, loss:nan
g_step 2800, step 148, avg_time 1.029, loss:nan
g_step 2900, step 27, avg_time 1.016, loss:nan
g_step 3000, step 127, avg_time 1.019, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 6, avg_time 2.133, loss:nan
g_step 3200, step 106, avg_time 1.015, loss:nan
g_step 3300, step 206, avg_time 1.030, loss:nan
g_step 3400, step 85, avg_time 1.008, loss:nan
g_step 3500, step 185, avg_time 1.022, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 64, avg_time 2.117, loss:nan
g_step 3700, step 164, avg_time 1.021, loss:nan
g_step 3800, step 43, avg_time 1.008, loss:nan
g_step 3900, step 143, avg_time 1.020, loss:nan
g_step 4000, step 22, avg_time 1.008, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 122, avg_time 2.124, loss:nan
g_step 4200, step 1, avg_time 1.009, loss:nan
g_step 4300, step 101, avg_time 1.021, loss:nan
g_step 4400, step 201, avg_time 1.019, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 03:28:19 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 03:28:19 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_03-28-19_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 03:28:20 - WARNING - datasets.builder -   Using custom data configuration default-f7c20c8e97dcc155
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-f7c20c8e97dcc155/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 03:28:20,430 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:28:20,431 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 03:28:20,431 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:28:20,432 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 03:28:20,440 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:28:20,445 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:28:20,445 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:28:20,445 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:28:20,445 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:28:20,445 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:28:20,445 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 03:28:20,581 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 03:28:23,658 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 03:28:23,662 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-f7c20c8e97dcc155/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.19ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.99ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.47ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.85ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.10ba/s]100%|██████████| 6/6 [00:01<00:00,  4.44ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.11ba/s] 40%|████      | 2/5 [00:00<00:00,  4.29ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.33ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.38ba/s]100%|██████████| 5/5 [00:00<00:00,  5.33ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  8.89ba/s] 50%|█████     | 3/6 [00:00<00:00, 10.30ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 10.69ba/s]100%|██████████| 6/6 [00:00<00:00, 11.88ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.01ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.19ba/s]100%|██████████| 5/5 [00:00<00:00, 12.72ba/s]
[INFO|trainer.py:414] 2023-08-28 03:28:27,231 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 03:28:27,247 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 03:28:27,247 >>   Num examples = 5300
[INFO|trainer.py:1149] 2023-08-28 03:28:27,247 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 03:28:27,247 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 03:28:27,247 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 03:28:27,247 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 03:28:27,247 >>   Total optimization steps = 415
  0%|          | 0/415 [00:00<?, ?it/s]  0%|          | 1/415 [00:00<01:57,  3.51it/s]  0%|          | 2/415 [00:00<01:55,  3.58it/s]  1%|          | 3/415 [00:00<01:54,  3.60it/s]  1%|          | 4/415 [00:01<01:53,  3.61it/s]  1%|          | 5/415 [00:01<01:53,  3.61it/s]  1%|▏         | 6/415 [00:01<01:53,  3.61it/s]  2%|▏         | 7/415 [00:01<01:52,  3.61it/s]  2%|▏         | 8/415 [00:02<01:52,  3.62it/s]  2%|▏         | 9/415 [00:02<01:52,  3.62it/s]  2%|▏         | 10/415 [00:02<01:52,  3.61it/s]  3%|▎         | 11/415 [00:03<01:51,  3.61it/s]  3%|▎         | 12/415 [00:03<01:51,  3.62it/s]  3%|▎         | 13/415 [00:03<01:51,  3.62it/s]  3%|▎         | 14/415 [00:03<01:50,  3.62it/s]  4%|▎         | 15/415 [00:04<01:50,  3.62it/s]  4%|▍         | 16/415 [00:04<01:50,  3.62it/s]  4%|▍         | 17/415 [00:04<01:49,  3.62it/s]  4%|▍         | 18/415 [00:04<01:49,  3.62it/s]  5%|▍         | 19/415 [00:05<01:49,  3.62it/s]  5%|▍         | 20/415 [00:05<01:49,  3.62it/s]  5%|▌         | 21/415 [00:05<01:49,  3.61it/s]  5%|▌         | 22/415 [00:06<01:48,  3.61it/s]  6%|▌         | 23/415 [00:06<01:48,  3.61it/s]  6%|▌         | 24/415 [00:06<01:48,  3.62it/s]  6%|▌         | 25/415 [00:06<01:47,  3.62it/s]  6%|▋         | 26/415 [00:07<01:47,  3.62it/s]  7%|▋         | 27/415 [00:07<01:47,  3.62it/s]  7%|▋         | 28/415 [00:07<01:47,  3.62it/s]  7%|▋         | 29/415 [00:08<01:46,  3.61it/s]  7%|▋         | 30/415 [00:08<01:46,  3.61it/s]  7%|▋         | 31/415 [00:08<01:46,  3.61it/s]  8%|▊         | 32/415 [00:08<01:46,  3.58it/s]  8%|▊         | 33/415 [00:09<01:46,  3.59it/s]  8%|▊         | 34/415 [00:09<01:45,  3.60it/s]  8%|▊         | 35/415 [00:09<01:45,  3.60it/s]  9%|▊         | 36/415 [00:09<01:44,  3.61it/s]  9%|▉         | 37/415 [00:10<01:44,  3.61it/s]  9%|▉         | 38/415 [00:10<01:44,  3.61it/s]  9%|▉         | 39/415 [00:10<01:43,  3.62it/s] 10%|▉         | 40/415 [00:11<01:43,  3.62it/s] 10%|▉         | 41/415 [00:11<01:43,  3.62it/s] 10%|█         | 42/415 [00:11<01:43,  3.62it/s] 10%|█         | 43/415 [00:11<01:42,  3.61it/s] 11%|█         | 44/415 [00:12<01:42,  3.61it/s] 11%|█         | 45/415 [00:12<01:42,  3.61it/s] 11%|█         | 46/415 [00:12<01:42,  3.62it/s] 11%|█▏        | 47/415 [00:13<01:41,  3.62it/s] 12%|█▏        | 48/415 [00:13<01:41,  3.62it/s] 12%|█▏        | 49/415 [00:13<01:41,  3.62it/s] 12%|█▏        | 50/415 [00:13<01:40,  3.61it/s] 12%|█▏        | 51/415 [00:14<01:40,  3.61it/s] 13%|█▎        | 52/415 [00:14<01:40,  3.61it/s] 13%|█▎        | 53/415 [00:14<01:40,  3.61it/s] 13%|█▎        | 54/415 [00:14<01:40,  3.60it/s] 13%|█▎        | 55/415 [00:15<01:39,  3.61it/s] 13%|█▎        | 56/415 [00:15<01:39,  3.61it/s] 14%|█▎        | 57/415 [00:15<01:39,  3.62it/s] 14%|█▍        | 58/415 [00:16<01:38,  3.61it/s] 14%|█▍        | 59/415 [00:16<01:38,  3.61it/s] 14%|█▍        | 60/415 [00:16<01:38,  3.61it/s] 15%|█▍        | 61/415 [00:16<01:37,  3.61it/s] 15%|█▍        | 62/415 [00:17<01:37,  3.61it/s] 15%|█▌        | 63/415 [00:17<01:37,  3.61it/s] 15%|█▌        | 64/415 [00:17<01:37,  3.61it/s] 16%|█▌        | 65/415 [00:17<01:37,  3.60it/s] 16%|█▌        | 66/415 [00:18<01:36,  3.60it/s] 16%|█▌        | 67/415 [00:18<01:36,  3.61it/s] 16%|█▋        | 68/415 [00:18<01:36,  3.61it/s] 17%|█▋        | 69/415 [00:19<01:35,  3.61it/s] 17%|█▋        | 70/415 [00:19<01:35,  3.61it/s] 17%|█▋        | 71/415 [00:19<01:35,  3.61it/s] 17%|█▋        | 72/415 [00:19<01:34,  3.61it/s] 18%|█▊        | 73/415 [00:20<01:34,  3.62it/s] 18%|█▊        | 74/415 [00:20<01:34,  3.61it/s] 18%|█▊        | 75/415 [00:20<01:34,  3.61it/s] 18%|█▊        | 76/415 [00:21<01:34,  3.58it/s] 19%|█▊        | 77/415 [00:21<01:34,  3.59it/s] 19%|█▉        | 78/415 [00:21<01:33,  3.60it/s] 19%|█▉        | 79/415 [00:21<01:33,  3.60it/s] 19%|█▉        | 80/415 [00:22<01:33,  3.60it/s] 20%|█▉        | 81/415 [00:22<01:32,  3.60it/s] 20%|█▉        | 82/415 [00:22<01:32,  3.60it/s] 20%|██        | 83/415 [00:22<01:27,  3.80it/s][INFO|trainer.py:2140] 2023-08-28 03:28:50,195 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:28:50,195 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 03:28:50,195 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.71it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.45it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.40it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.54it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.91it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.64it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.43it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.15it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.36it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.44it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.53it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.45it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.34it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.29it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.16it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.99it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.09it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.28it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.38it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.49it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.40it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.22it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.22it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.21it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.12it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.16it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.20it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.29it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.45it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.29it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.30it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.16it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.17it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.16it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.21it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.30it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.37it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.33it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.32it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.21it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.14it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.03it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.17it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.23it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.31it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.40it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.30it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.22it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.23it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.18it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.17it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.06it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.12it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.33it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.41it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.41it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.14it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.15it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.15it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.08it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.05it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.16it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.28it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.36it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.36it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.19it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.18it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.16it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.16it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.14it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.19it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.31it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.33it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.33it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.19it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.13it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.13it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.17it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.14it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.10it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.28it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.32it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.31it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.22it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.15it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.15it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.11it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 43.97it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.15it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.17it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.24it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.27it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.28it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.23it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.21it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.08it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.13it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.17it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.19it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.13it/s][A                                                
                                                 [A 20%|██        | 83/415 [00:34<01:27,  3.80it/s]
100%|██████████| 505/505 [00:11<00:00, 44.13it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:29:01,629 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-83
[INFO|configuration_utils.py:351] 2023-08-28 03:29:01,647 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-83/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:29:03,849 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-83/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:29:03,871 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-83/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:29:03,884 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-83/special_tokens_map.json
 20%|██        | 84/415 [00:37<24:36,  4.46s/it] 20%|██        | 85/415 [00:37<17:38,  3.21s/it] 21%|██        | 86/415 [00:37<12:45,  2.33s/it] 21%|██        | 87/415 [00:38<09:21,  1.71s/it] 21%|██        | 88/415 [00:38<06:59,  1.28s/it] 21%|██▏       | 89/415 [00:38<05:19,  1.02it/s] 22%|██▏       | 90/415 [00:38<04:09,  1.30it/s] 22%|██▏       | 91/415 [00:39<03:21,  1.61it/s] 22%|██▏       | 92/415 [00:39<02:47,  1.93it/s] 22%|██▏       | 93/415 [00:39<02:23,  2.24it/s] 23%|██▎       | 94/415 [00:39<02:07,  2.52it/s] 23%|██▎       | 95/415 [00:40<01:55,  2.77it/s] 23%|██▎       | 96/415 [00:40<01:47,  2.98it/s] 23%|██▎       | 97/415 [00:40<01:41,  3.14it/s] 24%|██▎       | 98/415 [00:41<01:36,  3.27it/s] 24%|██▍       | 99/415 [00:41<01:33,  3.37it/s] 24%|██▍       | 100/415 [00:41<01:31,  3.44it/s] 24%|██▍       | 101/415 [00:41<01:30,  3.49it/s] 25%|██▍       | 102/415 [00:42<01:28,  3.52it/s] 25%|██▍       | 103/415 [00:42<01:28,  3.55it/s] 25%|██▌       | 104/415 [00:42<01:27,  3.56it/s] 25%|██▌       | 105/415 [00:43<01:27,  3.55it/s] 26%|██▌       | 106/415 [00:43<01:26,  3.56it/s] 26%|██▌       | 107/415 [00:43<01:26,  3.58it/s] 26%|██▌       | 108/415 [00:43<01:25,  3.59it/s] 26%|██▋       | 109/415 [00:44<01:25,  3.59it/s] 27%|██▋       | 110/415 [00:44<01:24,  3.60it/s] 27%|██▋       | 111/415 [00:44<01:24,  3.60it/s] 27%|██▋       | 112/415 [00:44<01:24,  3.60it/s] 27%|██▋       | 113/415 [00:45<01:23,  3.60it/s] 27%|██▋       | 114/415 [00:45<01:23,  3.60it/s] 28%|██▊       | 115/415 [00:45<01:23,  3.61it/s] 28%|██▊       | 116/415 [00:46<01:23,  3.58it/s] 28%|██▊       | 117/415 [00:46<01:23,  3.58it/s] 28%|██▊       | 118/415 [00:46<01:22,  3.59it/s] 29%|██▊       | 119/415 [00:46<01:22,  3.59it/s] 29%|██▉       | 120/415 [00:47<01:21,  3.60it/s] 29%|██▉       | 121/415 [00:47<01:21,  3.60it/s] 29%|██▉       | 122/415 [00:47<01:21,  3.60it/s] 30%|██▉       | 123/415 [00:48<01:21,  3.60it/s] 30%|██▉       | 124/415 [00:48<01:20,  3.60it/s] 30%|███       | 125/415 [00:48<01:20,  3.60it/s] 30%|███       | 126/415 [00:48<01:20,  3.60it/s] 31%|███       | 127/415 [00:49<01:20,  3.58it/s] 31%|███       | 128/415 [00:49<01:20,  3.59it/s] 31%|███       | 129/415 [00:49<01:19,  3.59it/s] 31%|███▏      | 130/415 [00:49<01:19,  3.60it/s] 32%|███▏      | 131/415 [00:50<01:18,  3.60it/s] 32%|███▏      | 132/415 [00:50<01:18,  3.60it/s] 32%|███▏      | 133/415 [00:50<01:18,  3.60it/s] 32%|███▏      | 134/415 [00:51<01:18,  3.58it/s] 33%|███▎      | 135/415 [00:51<01:18,  3.59it/s] 33%|███▎      | 136/415 [00:51<01:17,  3.59it/s] 33%|███▎      | 137/415 [00:51<01:20,  3.47it/s] 33%|███▎      | 138/415 [00:52<01:19,  3.49it/s] 33%|███▎      | 139/415 [00:52<01:18,  3.53it/s] 34%|███▎      | 140/415 [00:52<01:17,  3.55it/s] 34%|███▍      | 141/415 [00:53<01:16,  3.56it/s] 34%|███▍      | 142/415 [00:53<01:16,  3.57it/s] 34%|███▍      | 143/415 [00:53<01:16,  3.58it/s] 35%|███▍      | 144/415 [00:53<01:15,  3.58it/s] 35%|███▍      | 145/415 [00:54<01:15,  3.58it/s] 35%|███▌      | 146/415 [00:54<01:15,  3.58it/s] 35%|███▌      | 147/415 [00:54<01:14,  3.59it/s] 36%|███▌      | 148/415 [00:55<01:14,  3.59it/s] 36%|███▌      | 149/415 [00:55<01:14,  3.59it/s] 36%|███▌      | 150/415 [00:55<01:13,  3.60it/s] 36%|███▋      | 151/415 [00:55<01:13,  3.60it/s] 37%|███▋      | 152/415 [00:56<01:13,  3.60it/s] 37%|███▋      | 153/415 [00:56<01:12,  3.60it/s] 37%|███▋      | 154/415 [00:56<01:12,  3.60it/s] 37%|███▋      | 155/415 [00:56<01:12,  3.60it/s] 38%|███▊      | 156/415 [00:57<01:12,  3.60it/s] 38%|███▊      | 157/415 [00:57<01:12,  3.58it/s] 38%|███▊      | 158/415 [00:57<01:11,  3.58it/s] 38%|███▊      | 159/415 [00:58<01:11,  3.59it/s] 39%|███▊      | 160/415 [00:58<01:11,  3.59it/s] 39%|███▉      | 161/415 [00:58<01:10,  3.59it/s] 39%|███▉      | 162/415 [00:58<01:10,  3.60it/s] 39%|███▉      | 163/415 [00:59<01:10,  3.60it/s] 40%|███▉      | 164/415 [00:59<01:09,  3.60it/s] 40%|███▉      | 165/415 [00:59<01:09,  3.60it/s] 40%|████      | 166/415 [00:59<01:05,  3.80it/s][INFO|trainer.py:2140] 2023-08-28 03:29:27,236 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:29:27,236 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 03:29:27,236 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.419, 'eval_samples_per_second': 353.71, 'eval_steps_per_second': 44.225, 'epoch': 1.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.19it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.44it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.36it/s][A
  4%|▍         | 22/505 [00:00<00:13, 36.80it/s][A
  5%|▌         | 27/505 [00:00<00:12, 39.15it/s][A
  6%|▋         | 32/505 [00:00<00:11, 40.87it/s][A
  7%|▋         | 37/505 [00:00<00:11, 41.93it/s][A
  8%|▊         | 42/505 [00:00<00:10, 42.63it/s][A
  9%|▉         | 47/505 [00:01<00:10, 43.09it/s][A
 10%|█         | 52/505 [00:01<00:10, 43.50it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 43.68it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 43.49it/s][A
 13%|█▎        | 67/505 [00:01<00:10, 43.40it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 43.67it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.86it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.07it/s][A
 17%|█▋        | 87/505 [00:02<00:09, 44.12it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.28it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.30it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.12it/s][A
 21%|██        | 107/505 [00:02<00:09, 43.81it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 43.69it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 43.86it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.03it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.21it/s][A
 26%|██▌       | 132/505 [00:03<00:08, 44.25it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.30it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.32it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.06it/s][A
 30%|███       | 152/505 [00:03<00:08, 43.92it/s][A
 31%|███       | 157/505 [00:03<00:07, 43.88it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 43.81it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.05it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.23it/s][A
 35%|███▌      | 177/505 [00:04<00:07, 44.33it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.35it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.30it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.01it/s][A
 39%|███▉      | 197/505 [00:04<00:07, 43.99it/s][A
 40%|████      | 202/505 [00:04<00:06, 43.97it/s][A
 41%|████      | 207/505 [00:04<00:06, 43.92it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.01it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.20it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.27it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.35it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.21it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.06it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 43.96it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 43.99it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 43.92it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.08it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.22it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.31it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.27it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.05it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.06it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.09it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 43.99it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 43.99it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.11it/s][A
 61%|██████    | 307/505 [00:07<00:04, 44.14it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.27it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.29it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.09it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.10it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 43.97it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 43.93it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.01it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.02it/s][A
 70%|██████▉   | 352/505 [00:08<00:03, 44.22it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.22it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.14it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.11it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.04it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 43.99it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.11it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.01it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.07it/s][A
 79%|███████▊  | 397/505 [00:09<00:02, 44.21it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.17it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.10it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 43.96it/s][A
 83%|████████▎ | 417/505 [00:09<00:02, 43.98it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.02it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.17it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.10it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.13it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 44.20it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.08it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.01it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.07it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 43.97it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 43.99it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.15it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.15it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.14it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.08it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.08it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.07it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.16it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.16it/s][A 40%|████      | 166/415 [01:11<01:05,  3.80it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:29:38,795 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-166
[INFO|configuration_utils.py:351] 2023-08-28 03:29:38,818 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-166/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:29:40,637 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-166/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:29:40,654 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-166/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:29:40,666 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-166/special_tokens_map.json
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 40%|████      | 167/415 [01:14<18:09,  4.39s/it] 40%|████      | 168/415 [01:14<13:00,  3.16s/it] 41%|████      | 169/415 [01:14<09:24,  2.30s/it] 41%|████      | 170/415 [01:14<06:54,  1.69s/it] 41%|████      | 171/415 [01:15<05:09,  1.27s/it] 41%|████▏     | 172/415 [01:15<03:56,  1.03it/s] 42%|████▏     | 173/415 [01:15<03:05,  1.31it/s] 42%|████▏     | 174/415 [01:15<02:29,  1.61it/s] 42%|████▏     | 175/415 [01:16<02:04,  1.93it/s] 42%|████▏     | 176/415 [01:16<01:46,  2.24it/s] 43%|████▎     | 177/415 [01:16<01:34,  2.51it/s] 43%|████▎     | 178/415 [01:17<01:26,  2.75it/s] 43%|████▎     | 179/415 [01:17<01:19,  2.95it/s] 43%|████▎     | 180/415 [01:17<01:15,  3.11it/s] 44%|████▎     | 181/415 [01:17<01:12,  3.23it/s] 44%|████▍     | 182/415 [01:18<01:10,  3.32it/s] 44%|████▍     | 183/415 [01:18<01:08,  3.39it/s] 44%|████▍     | 184/415 [01:18<01:07,  3.44it/s] 45%|████▍     | 185/415 [01:19<01:06,  3.47it/s] 45%|████▍     | 186/415 [01:19<01:05,  3.49it/s] 45%|████▌     | 187/415 [01:19<01:04,  3.51it/s] 45%|████▌     | 188/415 [01:19<01:04,  3.52it/s] 46%|████▌     | 189/415 [01:20<01:03,  3.55it/s] 46%|████▌     | 190/415 [01:20<01:03,  3.57it/s] 46%|████▌     | 191/415 [01:20<01:02,  3.58it/s] 46%|████▋     | 192/415 [01:21<01:02,  3.58it/s] 47%|████▋     | 193/415 [01:21<01:01,  3.59it/s] 47%|████▋     | 194/415 [01:21<01:01,  3.59it/s] 47%|████▋     | 195/415 [01:21<01:01,  3.59it/s] 47%|████▋     | 196/415 [01:22<01:00,  3.60it/s] 47%|████▋     | 197/415 [01:22<01:00,  3.60it/s] 48%|████▊     | 198/415 [01:22<01:00,  3.60it/s] 48%|████▊     | 199/415 [01:22<01:00,  3.58it/s] 48%|████▊     | 200/415 [01:23<00:59,  3.59it/s] 48%|████▊     | 201/415 [01:23<00:59,  3.59it/s] 49%|████▊     | 202/415 [01:23<00:59,  3.60it/s] 49%|████▉     | 203/415 [01:24<00:58,  3.60it/s] 49%|████▉     | 204/415 [01:24<00:58,  3.60it/s] 49%|████▉     | 205/415 [01:24<00:58,  3.60it/s] 50%|████▉     | 206/415 [01:24<00:58,  3.60it/s] 50%|████▉     | 207/415 [01:25<00:57,  3.60it/s] 50%|█████     | 208/415 [01:25<00:57,  3.60it/s] 50%|█████     | 209/415 [01:25<00:57,  3.60it/s] 51%|█████     | 210/415 [01:26<00:56,  3.61it/s] 51%|█████     | 211/415 [01:26<00:56,  3.61it/s] 51%|█████     | 212/415 [01:26<00:56,  3.61it/s] 51%|█████▏    | 213/415 [01:26<00:56,  3.60it/s] 52%|█████▏    | 214/415 [01:27<00:55,  3.60it/s] 52%|█████▏    | 215/415 [01:27<00:55,  3.60it/s] 52%|█████▏    | 216/415 [01:27<00:55,  3.60it/s] 52%|█████▏    | 217/415 [01:27<00:54,  3.60it/s] 53%|█████▎    | 218/415 [01:28<00:54,  3.58it/s] 53%|█████▎    | 219/415 [01:28<00:54,  3.59it/s] 53%|█████▎    | 220/415 [01:28<00:54,  3.60it/s] 53%|█████▎    | 221/415 [01:29<00:53,  3.60it/s] 53%|█████▎    | 222/415 [01:29<00:53,  3.60it/s] 54%|█████▎    | 223/415 [01:29<00:53,  3.60it/s] 54%|█████▍    | 224/415 [01:29<00:53,  3.60it/s] 54%|█████▍    | 225/415 [01:30<00:52,  3.60it/s] 54%|█████▍    | 226/415 [01:30<00:52,  3.59it/s] 55%|█████▍    | 227/415 [01:30<00:52,  3.59it/s] 55%|█████▍    | 228/415 [01:31<00:52,  3.59it/s] 55%|█████▌    | 229/415 [01:31<00:51,  3.58it/s] 55%|█████▌    | 230/415 [01:31<00:51,  3.59it/s] 56%|█████▌    | 231/415 [01:31<00:51,  3.59it/s] 56%|█████▌    | 232/415 [01:32<00:50,  3.60it/s] 56%|█████▌    | 233/415 [01:32<00:50,  3.60it/s] 56%|█████▋    | 234/415 [01:32<00:50,  3.59it/s] 57%|█████▋    | 235/415 [01:32<00:50,  3.59it/s] 57%|█████▋    | 236/415 [01:33<00:49,  3.59it/s] 57%|█████▋    | 237/415 [01:33<00:49,  3.59it/s] 57%|█████▋    | 238/415 [01:33<00:49,  3.59it/s] 58%|█████▊    | 239/415 [01:34<00:48,  3.59it/s] 58%|█████▊    | 240/415 [01:34<00:48,  3.58it/s] 58%|█████▊    | 241/415 [01:34<00:48,  3.58it/s] 58%|█████▊    | 242/415 [01:34<00:48,  3.59it/s] 59%|█████▊    | 243/415 [01:35<00:47,  3.59it/s] 59%|█████▉    | 244/415 [01:35<00:47,  3.60it/s] 59%|█████▉    | 245/415 [01:35<00:47,  3.60it/s] 59%|█████▉    | 246/415 [01:36<00:46,  3.60it/s] 60%|█████▉    | 247/415 [01:36<00:46,  3.60it/s] 60%|█████▉    | 248/415 [01:36<00:46,  3.60it/s] 60%|██████    | 249/415 [01:36<00:43,  3.79it/s][INFO|trainer.py:2140] 2023-08-28 03:30:04,081 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:30:04,081 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 03:30:04,081 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.5171, 'eval_samples_per_second': 350.696, 'eval_steps_per_second': 43.848, 'epoch': 2.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.34it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.48it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.52it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.32it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.80it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.29it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.16it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.01it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.11it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.32it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.44it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.30it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.23it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.03it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.96it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.84it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.98it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.10it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.14it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.24it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.29it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.15it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 43.96it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 43.87it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.94it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.98it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.03it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.16it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.28it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.31it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.05it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.03it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 43.89it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.91it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 43.97it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.11it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.10it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.22it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.06it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.07it/s][A
 41%|████      | 207/505 [00:04<00:06, 43.86it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 43.82it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 43.89it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.04it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.08it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.16it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.18it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.20it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.06it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 43.89it/s][A
 51%|█████     | 257/505 [00:05<00:05, 43.89it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 43.87it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.00it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.13it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.25it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.25it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.16it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.11it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 43.99it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 43.91it/s][A
 61%|██████    | 307/505 [00:06<00:04, 43.96it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 43.91it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 43.99it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.27it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.20it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.19it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 43.99it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 43.90it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 43.89it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 43.98it/s][A
 71%|███████   | 357/505 [00:08<00:03, 43.98it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 43.99it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.00it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.19it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.02it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 43.96it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 43.94it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 43.87it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 43.86it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 43.99it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.08it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.15it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.20it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.12it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.01it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.05it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 43.97it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 44.03it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.07it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.12it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.13it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.18it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 43.97it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.06it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.01it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 43.97it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.01it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.09it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.11it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.16it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.16it/s][A 60%|██████    | 249/415 [01:48<00:43,  3.79it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:30:15,563 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-249
[INFO|configuration_utils.py:351] 2023-08-28 03:30:15,601 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-249/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:30:17,441 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-249/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:30:17,458 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-249/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:30:17,467 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-249/special_tokens_map.json
 60%|██████    | 250/415 [01:50<12:00,  4.37s/it] 60%|██████    | 251/415 [01:51<08:35,  3.14s/it] 61%|██████    | 252/415 [01:51<06:12,  2.29s/it] 61%|██████    | 253/415 [01:51<04:32,  1.68s/it] 61%|██████    | 254/415 [01:51<03:23,  1.26s/it] 61%|██████▏   | 255/415 [01:52<02:36,  1.03it/s] 62%|██████▏   | 256/415 [01:52<02:02,  1.30it/s] 62%|██████▏   | 257/415 [01:52<01:38,  1.61it/s] 62%|██████▏   | 258/415 [01:53<01:21,  1.92it/s] 62%|██████▏   | 259/415 [01:53<01:09,  2.23it/s] 63%|██████▎   | 260/415 [01:53<01:01,  2.51it/s] 63%|██████▎   | 261/415 [01:53<00:56,  2.75it/s] 63%|██████▎   | 262/415 [01:54<00:51,  2.95it/s] 63%|██████▎   | 263/415 [01:54<00:48,  3.10it/s] 64%|██████▎   | 264/415 [01:54<00:46,  3.22it/s] 64%|██████▍   | 265/415 [01:55<00:45,  3.31it/s] 64%|██████▍   | 266/415 [01:55<00:44,  3.38it/s] 64%|██████▍   | 267/415 [01:55<00:43,  3.44it/s] 65%|██████▍   | 268/415 [01:55<00:42,  3.49it/s] 65%|██████▍   | 269/415 [01:56<00:41,  3.52it/s] 65%|██████▌   | 270/415 [01:56<00:40,  3.54it/s] 65%|██████▌   | 271/415 [01:56<00:40,  3.56it/s] 66%|██████▌   | 272/415 [01:56<00:40,  3.57it/s] 66%|██████▌   | 273/415 [01:57<00:39,  3.58it/s] 66%|██████▌   | 274/415 [01:57<00:39,  3.59it/s] 66%|██████▋   | 275/415 [01:57<00:38,  3.60it/s] 67%|██████▋   | 276/415 [01:58<00:38,  3.60it/s] 67%|██████▋   | 277/415 [01:58<00:38,  3.60it/s] 67%|██████▋   | 278/415 [01:58<00:38,  3.60it/s] 67%|██████▋   | 279/415 [01:59<00:43,  3.10it/s] 67%|██████▋   | 280/415 [01:59<00:41,  3.23it/s] 68%|██████▊   | 281/415 [01:59<00:40,  3.33it/s] 68%|██████▊   | 282/415 [01:59<00:39,  3.41it/s] 68%|██████▊   | 283/415 [02:00<00:38,  3.46it/s] 68%|██████▊   | 284/415 [02:00<00:37,  3.50it/s] 69%|██████▊   | 285/415 [02:00<00:36,  3.53it/s] 69%|██████▉   | 286/415 [02:01<00:36,  3.55it/s] 69%|██████▉   | 287/415 [02:01<00:35,  3.56it/s] 69%|██████▉   | 288/415 [02:01<00:35,  3.57it/s] 70%|██████▉   | 289/415 [02:01<00:35,  3.58it/s] 70%|██████▉   | 290/415 [02:02<00:34,  3.58it/s] 70%|███████   | 291/415 [02:02<00:34,  3.59it/s] 70%|███████   | 292/415 [02:02<00:34,  3.59it/s] 71%|███████   | 293/415 [02:02<00:33,  3.59it/s] 71%|███████   | 294/415 [02:03<00:33,  3.59it/s] 71%|███████   | 295/415 [02:03<00:33,  3.59it/s] 71%|███████▏  | 296/415 [02:03<00:33,  3.59it/s] 72%|███████▏  | 297/415 [02:04<00:32,  3.59it/s] 72%|███████▏  | 298/415 [02:04<00:32,  3.59it/s] 72%|███████▏  | 299/415 [02:04<00:32,  3.60it/s] 72%|███████▏  | 300/415 [02:04<00:31,  3.60it/s] 73%|███████▎  | 301/415 [02:05<00:31,  3.58it/s] 73%|███████▎  | 302/415 [02:05<00:31,  3.59it/s] 73%|███████▎  | 303/415 [02:05<00:31,  3.59it/s] 73%|███████▎  | 304/415 [02:06<00:30,  3.59it/s] 73%|███████▎  | 305/415 [02:06<00:30,  3.59it/s] 74%|███████▎  | 306/415 [02:06<00:30,  3.59it/s] 74%|███████▍  | 307/415 [02:06<00:30,  3.59it/s] 74%|███████▍  | 308/415 [02:07<00:29,  3.59it/s] 74%|███████▍  | 309/415 [02:07<00:29,  3.59it/s] 75%|███████▍  | 310/415 [02:07<00:29,  3.59it/s] 75%|███████▍  | 311/415 [02:07<00:28,  3.59it/s] 75%|███████▌  | 312/415 [02:08<00:28,  3.58it/s] 75%|███████▌  | 313/415 [02:08<00:28,  3.59it/s] 76%|███████▌  | 314/415 [02:08<00:28,  3.59it/s] 76%|███████▌  | 315/415 [02:09<00:27,  3.59it/s] 76%|███████▌  | 316/415 [02:09<00:27,  3.59it/s] 76%|███████▋  | 317/415 [02:09<00:27,  3.60it/s] 77%|███████▋  | 318/415 [02:09<00:26,  3.60it/s] 77%|███████▋  | 319/415 [02:10<00:26,  3.60it/s] 77%|███████▋  | 320/415 [02:10<00:26,  3.60it/s] 77%|███████▋  | 321/415 [02:10<00:26,  3.60it/s] 78%|███████▊  | 322/415 [02:11<00:25,  3.60it/s] 78%|███████▊  | 323/415 [02:11<00:25,  3.59it/s] 78%|███████▊  | 324/415 [02:11<00:25,  3.59it/s] 78%|███████▊  | 325/415 [02:11<00:25,  3.59it/s] 79%|███████▊  | 326/415 [02:12<00:24,  3.60it/s] 79%|███████▉  | 327/415 [02:12<00:24,  3.60it/s] 79%|███████▉  | 328/415 [02:12<00:24,  3.60it/s] 79%|███████▉  | 329/415 [02:12<00:23,  3.60it/s] 80%|███████▉  | 330/415 [02:13<00:23,  3.60it/s] 80%|███████▉  | 331/415 [02:13<00:23,  3.60it/s] 80%|████████  | 332/415 [02:13<00:21,  3.79it/s][INFO|trainer.py:2140] 2023-08-28 03:30:41,028 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:30:41,028 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 03:30:41,028 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4622, 'eval_samples_per_second': 352.376, 'eval_steps_per_second': 44.058, 'epoch': 3.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.95it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.66it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.91it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.79it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.00it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.54it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.10it/s][A
  8%|▊         | 42/505 [00:00<00:10, 43.90it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.02it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.25it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.33it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.44it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.37it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.08it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.82it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.75it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.81it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 43.96it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.16it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.33it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.38it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.33it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.08it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.01it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.86it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.91it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.03it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.18it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.36it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.36it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.26it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.03it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 43.86it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.95it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 43.99it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.07it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.15it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.26it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.28it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.21it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.12it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 43.95it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 43.89it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 43.98it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.03it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.18it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.21it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.21it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.18it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.19it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.04it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 43.99it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.01it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.00it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.13it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.22it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.15it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.13it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.10it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 43.94it/s][A
 61%|██████    | 307/505 [00:06<00:04, 43.99it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 43.96it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 43.92it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 43.97it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.11it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.05it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.02it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.01it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 43.99it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.02it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.04it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.16it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.21it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.13it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.11it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.00it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.09it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.06it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.06it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.13it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.16it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.17it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.10it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.16it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.04it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.01it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.07it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.12it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.18it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.16it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.10it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.14it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.06it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 43.86it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 43.91it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.05it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.17it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.18it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.13it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.18it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.18it/s][A 80%|████████  | 332/415 [02:25<00:21,  3.79it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:30:52,489 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-332
[INFO|configuration_utils.py:351] 2023-08-28 03:30:52,506 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-332/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:30:54,717 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-332/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:30:54,730 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-332/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:30:54,744 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-332/special_tokens_map.json
 80%|████████  | 333/415 [02:28<06:05,  4.46s/it] 80%|████████  | 334/415 [02:28<04:19,  3.21s/it] 81%|████████  | 335/415 [02:28<03:06,  2.33s/it] 81%|████████  | 336/415 [02:28<02:15,  1.71s/it] 81%|████████  | 337/415 [02:29<01:40,  1.28s/it] 81%|████████▏ | 338/415 [02:29<01:15,  1.02it/s] 82%|████████▏ | 339/415 [02:29<00:58,  1.29it/s] 82%|████████▏ | 340/415 [02:30<00:47,  1.59it/s] 82%|████████▏ | 341/415 [02:30<00:38,  1.91it/s] 82%|████████▏ | 342/415 [02:30<00:32,  2.21it/s] 83%|████████▎ | 343/415 [02:30<00:28,  2.50it/s] 83%|████████▎ | 344/415 [02:31<00:25,  2.74it/s] 83%|████████▎ | 345/415 [02:31<00:23,  2.94it/s] 83%|████████▎ | 346/415 [02:31<00:22,  3.11it/s] 84%|████████▎ | 347/415 [02:31<00:21,  3.23it/s] 84%|████████▍ | 348/415 [02:32<00:20,  3.32it/s] 84%|████████▍ | 349/415 [02:32<00:19,  3.40it/s] 84%|████████▍ | 350/415 [02:32<00:18,  3.45it/s] 85%|████████▍ | 351/415 [02:33<00:18,  3.50it/s] 85%|████████▍ | 352/415 [02:33<00:17,  3.53it/s] 85%|████████▌ | 353/415 [02:33<00:17,  3.55it/s] 85%|████████▌ | 354/415 [02:33<00:17,  3.57it/s] 86%|████████▌ | 355/415 [02:34<00:16,  3.58it/s] 86%|████████▌ | 356/415 [02:34<00:16,  3.58it/s] 86%|████████▌ | 357/415 [02:34<00:16,  3.59it/s] 86%|████████▋ | 358/415 [02:35<00:15,  3.59it/s] 87%|████████▋ | 359/415 [02:35<00:15,  3.60it/s] 87%|████████▋ | 360/415 [02:35<00:15,  3.60it/s] 87%|████████▋ | 361/415 [02:35<00:15,  3.59it/s] 87%|████████▋ | 362/415 [02:36<00:14,  3.60it/s] 87%|████████▋ | 363/415 [02:36<00:14,  3.60it/s] 88%|████████▊ | 364/415 [02:36<00:14,  3.59it/s] 88%|████████▊ | 365/415 [02:36<00:13,  3.60it/s] 88%|████████▊ | 366/415 [02:37<00:13,  3.60it/s] 88%|████████▊ | 367/415 [02:37<00:13,  3.60it/s] 89%|████████▊ | 368/415 [02:37<00:13,  3.60it/s] 89%|████████▉ | 369/415 [02:38<00:12,  3.60it/s] 89%|████████▉ | 370/415 [02:38<00:12,  3.61it/s] 89%|████████▉ | 371/415 [02:38<00:12,  3.61it/s] 90%|████████▉ | 372/415 [02:38<00:12,  3.58it/s] 90%|████████▉ | 373/415 [02:39<00:11,  3.59it/s] 90%|█████████ | 374/415 [02:39<00:11,  3.59it/s] 90%|█████████ | 375/415 [02:39<00:11,  3.59it/s] 91%|█████████ | 376/415 [02:40<00:10,  3.59it/s] 91%|█████████ | 377/415 [02:40<00:10,  3.59it/s] 91%|█████████ | 378/415 [02:40<00:10,  3.60it/s] 91%|█████████▏| 379/415 [02:40<00:10,  3.60it/s] 92%|█████████▏| 380/415 [02:41<00:09,  3.60it/s] 92%|█████████▏| 381/415 [02:41<00:09,  3.60it/s] 92%|█████████▏| 382/415 [02:41<00:09,  3.60it/s] 92%|█████████▏| 383/415 [02:41<00:08,  3.59it/s] 93%|█████████▎| 384/415 [02:42<00:08,  3.60it/s] 93%|█████████▎| 385/415 [02:42<00:08,  3.60it/s] 93%|█████████▎| 386/415 [02:42<00:08,  3.60it/s] 93%|█████████▎| 387/415 [02:43<00:07,  3.60it/s] 93%|█████████▎| 388/415 [02:43<00:07,  3.60it/s] 94%|█████████▎| 389/415 [02:43<00:07,  3.60it/s] 94%|█████████▍| 390/415 [02:43<00:06,  3.61it/s] 94%|█████████▍| 391/415 [02:44<00:06,  3.61it/s] 94%|█████████▍| 392/415 [02:44<00:06,  3.61it/s] 95%|█████████▍| 393/415 [02:44<00:06,  3.61it/s] 95%|█████████▍| 394/415 [02:45<00:05,  3.56it/s] 95%|█████████▌| 395/415 [02:45<00:05,  3.58it/s] 95%|█████████▌| 396/415 [02:45<00:05,  3.59it/s] 96%|█████████▌| 397/415 [02:45<00:05,  3.58it/s] 96%|█████████▌| 398/415 [02:46<00:04,  3.59it/s] 96%|█████████▌| 399/415 [02:46<00:04,  3.59it/s] 96%|█████████▋| 400/415 [02:46<00:04,  3.59it/s] 97%|█████████▋| 401/415 [02:46<00:03,  3.59it/s] 97%|█████████▋| 402/415 [02:47<00:03,  3.59it/s] 97%|█████████▋| 403/415 [02:47<00:03,  3.60it/s] 97%|█████████▋| 404/415 [02:47<00:03,  3.60it/s] 98%|█████████▊| 405/415 [02:48<00:02,  3.58it/s] 98%|█████████▊| 406/415 [02:48<00:02,  3.59it/s] 98%|█████████▊| 407/415 [02:48<00:02,  3.60it/s] 98%|█████████▊| 408/415 [02:48<00:01,  3.60it/s] 99%|█████████▊| 409/415 [02:49<00:01,  3.60it/s] 99%|█████████▉| 410/415 [02:49<00:01,  3.60it/s] 99%|█████████▉| 411/415 [02:49<00:01,  3.60it/s] 99%|█████████▉| 412/415 [02:50<00:00,  3.60it/s]100%|█████████▉| 413/415 [02:50<00:00,  3.60it/s]100%|█████████▉| 414/415 [02:50<00:00,  3.60it/s]100%|██████████| 415/415 [02:50<00:00,  3.80it/s][INFO|trainer.py:2140] 2023-08-28 03:31:18,089 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:31:18,089 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 03:31:18,089 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4507, 'eval_samples_per_second': 352.729, 'eval_steps_per_second': 44.102, 'epoch': 4.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.95it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.36it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.34it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.35it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.80it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.55it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.30it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.12it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.20it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.36it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.31it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 42.42it/s][A
 13%|█▎        | 67/505 [00:01<00:10, 43.54it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 43.75it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.80it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.74it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.86it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.00it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.17it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.12it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.08it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.14it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.08it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.07it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.02it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.05it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.13it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.09it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.14it/s][A
 30%|███       | 152/505 [00:03<00:08, 43.99it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.08it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.16it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.13it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.07it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.09it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.01it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.14it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.16it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.07it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.10it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.13it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.10it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.10it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.08it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.06it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.04it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.16it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.16it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.04it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.07it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.14it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.04it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.12it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.13it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.06it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.08it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.09it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.04it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.06it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.00it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.11it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.11it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.13it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.19it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 43.98it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.10it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.07it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.08it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.11it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.02it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.11it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.22it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.18it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.12it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.13it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.06it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.09it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.10it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.14it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.08it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.11it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.18it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.11it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.06it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.02it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.06it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.05it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 44.10it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.09it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.11it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.18it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.16it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.13it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.05it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.05it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.11it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.10it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.09it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.10it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.06it/s][A                                                 
                                                 [A100%|██████████| 415/415 [03:02<00:00,  3.80it/s]
100%|██████████| 505/505 [00:11<00:00, 44.06it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:31:29,569 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-415
[INFO|configuration_utils.py:351] 2023-08-28 03:31:29,586 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-415/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:31:31,663 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-415/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:31:31,679 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-415/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:31:31,686 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-415/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 03:31:31,960 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 03:31:31,960 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-83 (score: 1.2323094606399536).
                                                 100%|██████████| 415/415 [03:06<00:00,  3.80it/s]100%|██████████| 415/415 [03:06<00:00,  2.22it/s]
[INFO|trainer.py:1894] 2023-08-28 03:31:34,114 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 03:31:34,131 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:31:36,303 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:31:36,317 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:31:36,326 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 03:31:36,520 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:31:36,520 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:31:36,520 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:31:36,520 >>   train_runtime            = 0:03:06.86
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:31:36,521 >>   train_samples            =       5300
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:31:36,521 >>   train_samples_per_second =    141.815
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:31:36,521 >>   train_steps_per_second   =      2.221
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4604, 'eval_samples_per_second': 352.43, 'eval_steps_per_second': 44.065, 'epoch': 5.0}
{'train_runtime': 186.863, 'train_samples_per_second': 141.815, 'train_steps_per_second': 2.221, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 03:31:36 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 03:31:36,571 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:31:36,572 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 03:31:36,572 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:08, 55.79it/s]  2%|▏         | 12/505 [00:00<00:10, 48.68it/s]  3%|▎         | 17/505 [00:00<00:10, 46.95it/s]  4%|▍         | 22/505 [00:00<00:10, 46.37it/s]  5%|▌         | 27/505 [00:00<00:10, 45.87it/s]  6%|▋         | 32/505 [00:00<00:10, 45.54it/s]  7%|▋         | 37/505 [00:00<00:10, 45.39it/s]  8%|▊         | 42/505 [00:00<00:10, 44.74it/s]  9%|▉         | 47/505 [00:01<00:10, 43.94it/s] 10%|█         | 52/505 [00:01<00:10, 43.79it/s] 11%|█▏        | 57/505 [00:01<00:10, 44.06it/s] 12%|█▏        | 62/505 [00:01<00:10, 44.24it/s] 13%|█▎        | 67/505 [00:01<00:09, 44.40it/s] 14%|█▍        | 72/505 [00:01<00:09, 44.41it/s] 15%|█▌        | 77/505 [00:01<00:09, 44.56it/s] 16%|█▌        | 82/505 [00:01<00:09, 44.59it/s] 17%|█▋        | 87/505 [00:01<00:09, 44.33it/s] 18%|█▊        | 92/505 [00:02<00:09, 43.96it/s] 19%|█▉        | 97/505 [00:02<00:09, 43.86it/s] 20%|██        | 102/505 [00:02<00:09, 43.93it/s] 21%|██        | 107/505 [00:02<00:09, 44.18it/s] 22%|██▏       | 112/505 [00:02<00:08, 44.36it/s] 23%|██▎       | 117/505 [00:02<00:08, 44.42it/s] 24%|██▍       | 122/505 [00:02<00:08, 44.55it/s] 25%|██▌       | 127/505 [00:02<00:08, 44.50it/s] 26%|██▌       | 132/505 [00:02<00:08, 44.25it/s] 27%|██▋       | 137/505 [00:03<00:08, 44.05it/s] 28%|██▊       | 142/505 [00:03<00:08, 43.94it/s] 29%|██▉       | 147/505 [00:03<00:08, 43.98it/s] 30%|███       | 152/505 [00:03<00:07, 44.14it/s] 31%|███       | 157/505 [00:03<00:07, 44.33it/s] 32%|███▏      | 162/505 [00:03<00:07, 44.39it/s] 33%|███▎      | 167/505 [00:03<00:07, 44.50it/s] 34%|███▍      | 172/505 [00:03<00:07, 44.40it/s] 35%|███▌      | 177/505 [00:03<00:07, 44.10it/s] 36%|███▌      | 182/505 [00:04<00:07, 44.09it/s] 37%|███▋      | 187/505 [00:04<00:07, 44.03it/s] 38%|███▊      | 192/505 [00:04<00:07, 44.06it/s] 39%|███▉      | 197/505 [00:04<00:06, 44.20it/s] 40%|████      | 202/505 [00:04<00:06, 44.30it/s] 41%|████      | 207/505 [00:04<00:06, 44.43it/s] 42%|████▏     | 212/505 [00:04<00:06, 44.46it/s] 43%|████▎     | 217/505 [00:04<00:06, 44.37it/s] 44%|████▍     | 222/505 [00:04<00:06, 44.25it/s] 45%|████▍     | 227/505 [00:05<00:06, 44.11it/s] 46%|████▌     | 232/505 [00:05<00:06, 44.03it/s] 47%|████▋     | 237/505 [00:05<00:06, 44.05it/s] 48%|████▊     | 242/505 [00:05<00:05, 44.21it/s] 49%|████▉     | 247/505 [00:05<00:05, 44.28it/s] 50%|████▉     | 252/505 [00:05<00:05, 44.47it/s] 51%|█████     | 257/505 [00:05<00:05, 44.42it/s] 52%|█████▏    | 262/505 [00:05<00:05, 44.28it/s] 53%|█████▎    | 267/505 [00:06<00:05, 44.22it/s] 54%|█████▍    | 272/505 [00:06<00:05, 44.13it/s] 55%|█████▍    | 277/505 [00:06<00:05, 43.98it/s] 56%|█████▌    | 282/505 [00:06<00:05, 44.17it/s] 57%|█████▋    | 287/505 [00:06<00:04, 44.25it/s] 58%|█████▊    | 292/505 [00:06<00:04, 44.43it/s] 59%|█████▉    | 297/505 [00:06<00:04, 44.51it/s] 60%|█████▉    | 302/505 [00:06<00:04, 44.32it/s] 61%|██████    | 307/505 [00:06<00:04, 44.31it/s] 62%|██████▏   | 312/505 [00:07<00:04, 44.22it/s] 63%|██████▎   | 317/505 [00:07<00:04, 44.01it/s] 64%|██████▍   | 322/505 [00:07<00:04, 44.04it/s] 65%|██████▍   | 327/505 [00:07<00:04, 44.07it/s] 66%|██████▌   | 332/505 [00:07<00:03, 44.16it/s] 67%|██████▋   | 337/505 [00:07<00:03, 44.38it/s] 68%|██████▊   | 342/505 [00:07<00:03, 44.49it/s] 69%|██████▊   | 347/505 [00:07<00:03, 44.39it/s] 70%|██████▉   | 352/505 [00:07<00:03, 44.30it/s] 71%|███████   | 357/505 [00:08<00:03, 44.19it/s] 72%|███████▏  | 362/505 [00:08<00:03, 44.13it/s] 73%|███████▎  | 367/505 [00:08<00:03, 44.16it/s] 74%|███████▎  | 372/505 [00:08<00:03, 44.09it/s] 75%|███████▍  | 377/505 [00:08<00:02, 44.07it/s] 76%|███████▌  | 382/505 [00:08<00:02, 44.36it/s] 77%|███████▋  | 387/505 [00:08<00:02, 44.39it/s] 78%|███████▊  | 392/505 [00:08<00:02, 44.33it/s] 79%|███████▊  | 397/505 [00:08<00:02, 44.26it/s] 80%|███████▉  | 402/505 [00:09<00:02, 44.04it/s] 81%|████████  | 407/505 [00:09<00:02, 44.08it/s] 82%|████████▏ | 412/505 [00:09<00:02, 44.09it/s] 83%|████████▎ | 417/505 [00:09<00:01, 44.10it/s] 84%|████████▎ | 422/505 [00:09<00:01, 44.17it/s] 85%|████████▍ | 427/505 [00:09<00:01, 44.23it/s] 86%|████████▌ | 432/505 [00:09<00:01, 44.38it/s] 87%|████████▋ | 437/505 [00:09<00:01, 44.41it/s] 88%|████████▊ | 442/505 [00:09<00:01, 44.20it/s] 89%|████████▊ | 447/505 [00:10<00:01, 44.14it/s] 90%|████████▉ | 452/505 [00:10<00:01, 44.06it/s] 90%|█████████ | 457/505 [00:10<00:01, 44.13it/s] 91%|█████████▏| 462/505 [00:10<00:00, 44.13it/s] 92%|█████████▏| 467/505 [00:10<00:00, 44.20it/s] 93%|█████████▎| 472/505 [00:10<00:00, 44.30it/s] 94%|█████████▍| 477/505 [00:10<00:00, 44.40it/s] 95%|█████████▌| 482/505 [00:10<00:00, 44.25it/s] 96%|█████████▋| 487/505 [00:10<00:00, 44.22it/s] 97%|█████████▋| 492/505 [00:11<00:00, 44.15it/s] 98%|█████████▊| 497/505 [00:11<00:00, 44.15it/s] 99%|█████████▉| 502/505 [00:11<00:00, 44.22it/s]100%|██████████| 505/505 [00:11<00:00, 44.30it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 03:31:47,986 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:31:47,987 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:31:47,987 >>   eval_loss               =     1.2323
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:31:47,987 >>   eval_runtime            = 0:00:11.41
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:31:47,987 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:31:47,987 >>   eval_samples_per_second =    353.839
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:31:47,987 >>   eval_steps_per_second   =     44.241
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:31:47,987 >>   perplexity              =     3.4291
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:54,544 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:54,549 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:54,549 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:54,549 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:54,549 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:31:55,139 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:31:55,141 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:31:55,723 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:31:56,758 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:31:56,758 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:59,619 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:59,625 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:59,625 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:59,625 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:31:59,625 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:32:00,270 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:32:00,271 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:32:00,830 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:32:00,985 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:32:00,986 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-166
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-83
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-249
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-332
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-415
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.35it/s]Extractor Predicting: 2it [00:01,  1.45it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.67it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.66it/s]Extractor Predicting: 8it [00:04,  1.70it/s]Extractor Predicting: 9it [00:05,  1.70it/s]Extractor Predicting: 10it [00:06,  1.71it/s]Extractor Predicting: 11it [00:06,  1.70it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:07,  1.66it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:09,  1.68it/s]Extractor Predicting: 16it [00:09,  1.66it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:10,  1.68it/s]Extractor Predicting: 19it [00:11,  1.68it/s]Extractor Predicting: 20it [00:12,  1.67it/s]Extractor Predicting: 21it [00:12,  1.69it/s]Extractor Predicting: 22it [00:13,  1.67it/s]Extractor Predicting: 23it [00:13,  1.63it/s]Extractor Predicting: 24it [00:14,  1.63it/s]Extractor Predicting: 25it [00:15,  1.66it/s]Extractor Predicting: 26it [00:15,  1.68it/s]Extractor Predicting: 27it [00:16,  1.72it/s]Extractor Predicting: 28it [00:16,  1.69it/s]Extractor Predicting: 29it [00:17,  1.72it/s]Extractor Predicting: 30it [00:18,  1.68it/s]Extractor Predicting: 31it [00:18,  1.70it/s]Extractor Predicting: 32it [00:19,  1.69it/s]Extractor Predicting: 33it [00:19,  1.68it/s]Extractor Predicting: 34it [00:20,  1.67it/s]Extractor Predicting: 35it [00:21,  1.67it/s]Extractor Predicting: 36it [00:21,  1.73it/s]Extractor Predicting: 37it [00:22,  1.79it/s]Extractor Predicting: 38it [00:22,  1.76it/s]Extractor Predicting: 39it [00:23,  1.79it/s]Extractor Predicting: 40it [00:23,  1.76it/s]Extractor Predicting: 41it [00:24,  1.74it/s]Extractor Predicting: 42it [00:24,  1.73it/s]Extractor Predicting: 43it [00:25,  1.77it/s]Extractor Predicting: 44it [00:26,  1.76it/s]Extractor Predicting: 45it [00:26,  1.78it/s]Extractor Predicting: 46it [00:27,  1.73it/s]Extractor Predicting: 47it [00:27,  1.73it/s]Extractor Predicting: 48it [00:28,  1.65it/s]Extractor Predicting: 49it [00:29,  1.71it/s]Extractor Predicting: 50it [00:29,  1.75it/s]Extractor Predicting: 51it [00:30,  1.78it/s]Extractor Predicting: 52it [00:30,  1.83it/s]Extractor Predicting: 53it [00:31,  1.82it/s]Extractor Predicting: 54it [00:31,  1.82it/s]Extractor Predicting: 55it [00:32,  1.79it/s]Extractor Predicting: 56it [00:32,  1.79it/s]Extractor Predicting: 57it [00:33,  1.80it/s]Extractor Predicting: 58it [00:34,  1.74it/s]Extractor Predicting: 59it [00:34,  1.75it/s]Extractor Predicting: 60it [00:35,  1.73it/s]Extractor Predicting: 61it [00:35,  1.80it/s]Extractor Predicting: 62it [00:36,  1.79it/s]Extractor Predicting: 63it [00:36,  1.75it/s]Extractor Predicting: 64it [00:37,  1.74it/s]Extractor Predicting: 65it [00:38,  1.72it/s]Extractor Predicting: 66it [00:38,  1.74it/s]Extractor Predicting: 67it [00:39,  1.73it/s]Extractor Predicting: 68it [00:39,  1.68it/s]Extractor Predicting: 69it [00:40,  1.65it/s]Extractor Predicting: 70it [00:41,  1.68it/s]Extractor Predicting: 71it [00:41,  1.67it/s]Extractor Predicting: 72it [00:42,  1.62it/s]Extractor Predicting: 73it [00:42,  1.62it/s]Extractor Predicting: 74it [00:43,  1.56it/s]Extractor Predicting: 75it [00:44,  1.60it/s]Extractor Predicting: 76it [00:44,  1.63it/s]Extractor Predicting: 77it [00:45,  1.63it/s]Extractor Predicting: 78it [00:45,  1.64it/s]Extractor Predicting: 79it [00:46,  1.65it/s]Extractor Predicting: 80it [00:47,  1.65it/s]Extractor Predicting: 81it [00:47,  1.66it/s]Extractor Predicting: 82it [00:48,  1.71it/s]Extractor Predicting: 83it [00:48,  1.66it/s]Extractor Predicting: 84it [00:49,  1.68it/s]Extractor Predicting: 85it [00:50,  1.67it/s]Extractor Predicting: 86it [00:50,  1.69it/s]Extractor Predicting: 87it [00:51,  1.73it/s]Extractor Predicting: 88it [00:51,  1.74it/s]Extractor Predicting: 89it [00:52,  1.72it/s]Extractor Predicting: 90it [00:53,  1.74it/s]Extractor Predicting: 91it [00:53,  1.74it/s]Extractor Predicting: 92it [00:54,  1.59it/s]Extractor Predicting: 93it [00:54,  1.62it/s]Extractor Predicting: 94it [00:55,  1.63it/s]Extractor Predicting: 95it [00:56,  1.63it/s]Extractor Predicting: 96it [00:56,  1.63it/s]Extractor Predicting: 97it [00:57,  1.63it/s]Extractor Predicting: 98it [00:57,  1.63it/s]Extractor Predicting: 99it [00:58,  1.64it/s]Extractor Predicting: 100it [00:59,  1.64it/s]Extractor Predicting: 101it [00:59,  1.60it/s]Extractor Predicting: 102it [01:00,  1.61it/s]Extractor Predicting: 103it [01:01,  1.61it/s]Extractor Predicting: 104it [01:01,  1.67it/s]Extractor Predicting: 105it [01:02,  1.67it/s]Extractor Predicting: 106it [01:02,  1.71it/s]Extractor Predicting: 107it [01:03,  1.71it/s]Extractor Predicting: 108it [01:03,  1.73it/s]Extractor Predicting: 109it [01:04,  1.73it/s]Extractor Predicting: 110it [01:05,  1.71it/s]Extractor Predicting: 111it [01:05,  1.67it/s]Extractor Predicting: 112it [01:06,  1.71it/s]Extractor Predicting: 113it [01:06,  1.70it/s]Extractor Predicting: 114it [01:07,  1.67it/s]Extractor Predicting: 115it [01:08,  1.73it/s]Extractor Predicting: 116it [01:08,  1.73it/s]Extractor Predicting: 117it [01:09,  1.72it/s]Extractor Predicting: 118it [01:09,  1.65it/s]Extractor Predicting: 119it [01:10,  1.70it/s]Extractor Predicting: 120it [01:10,  1.71it/s]Extractor Predicting: 121it [01:11,  1.68it/s]Extractor Predicting: 122it [01:12,  1.69it/s]Extractor Predicting: 123it [01:12,  1.72it/s]Extractor Predicting: 124it [01:13,  1.72it/s]Extractor Predicting: 125it [01:14,  1.55it/s]Extractor Predicting: 126it [01:14,  1.61it/s]Extractor Predicting: 127it [01:15,  1.56it/s]Extractor Predicting: 128it [01:16,  1.54it/s]Extractor Predicting: 129it [01:16,  1.55it/s]Extractor Predicting: 130it [01:17,  1.55it/s]Extractor Predicting: 131it [01:18,  1.52it/s]Extractor Predicting: 132it [01:18,  1.52it/s]Extractor Predicting: 133it [01:19,  1.54it/s]Extractor Predicting: 134it [01:19,  1.58it/s]Extractor Predicting: 135it [01:20,  1.59it/s]Extractor Predicting: 136it [01:21,  1.59it/s]Extractor Predicting: 137it [01:21,  1.62it/s]Extractor Predicting: 138it [01:22,  1.57it/s]Extractor Predicting: 139it [01:23,  1.57it/s]Extractor Predicting: 140it [01:23,  1.57it/s]Extractor Predicting: 141it [01:24,  1.57it/s]Extractor Predicting: 142it [01:24,  1.58it/s]Extractor Predicting: 143it [01:25,  1.58it/s]Extractor Predicting: 144it [01:26,  1.59it/s]Extractor Predicting: 145it [01:26,  1.58it/s]Extractor Predicting: 146it [01:27,  1.59it/s]Extractor Predicting: 147it [01:28,  1.60it/s]Extractor Predicting: 148it [01:28,  1.55it/s]Extractor Predicting: 149it [01:29,  1.60it/s]Extractor Predicting: 149it [01:29,  1.67it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:38,169 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:38,176 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:38,176 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:38,176 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:38,176 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:33:38,760 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:33:38,761 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:33:39,359 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:33:40,366 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:33:40,366 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:43,224 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:43,230 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:43,230 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:43,230 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:43,230 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:33:43,848 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:33:43,849 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:33:44,440 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:33:44,597 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:33:44,597 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.77it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.74it/s]Extractor Predicting: 4it [00:02,  1.83it/s]Extractor Predicting: 5it [00:02,  1.78it/s]Extractor Predicting: 6it [00:03,  1.78it/s]Extractor Predicting: 7it [00:03,  1.78it/s]Extractor Predicting: 8it [00:04,  1.85it/s]Extractor Predicting: 9it [00:04,  1.94it/s]Extractor Predicting: 10it [00:05,  1.92it/s]Extractor Predicting: 11it [00:05,  1.90it/s]Extractor Predicting: 12it [00:06,  1.91it/s]Extractor Predicting: 13it [00:06,  2.00it/s]Extractor Predicting: 14it [00:07,  1.98it/s]Extractor Predicting: 15it [00:07,  1.99it/s]Extractor Predicting: 16it [00:08,  1.99it/s]Extractor Predicting: 17it [00:08,  1.99it/s]Extractor Predicting: 18it [00:09,  1.94it/s]Extractor Predicting: 19it [00:10,  1.89it/s]Extractor Predicting: 20it [00:10,  1.90it/s]Extractor Predicting: 21it [00:11,  1.90it/s]Extractor Predicting: 22it [00:11,  1.93it/s]Extractor Predicting: 23it [00:12,  1.97it/s]Extractor Predicting: 24it [00:12,  1.95it/s]Extractor Predicting: 25it [00:13,  1.97it/s]Extractor Predicting: 26it [00:13,  1.92it/s]Extractor Predicting: 27it [00:14,  1.98it/s]Extractor Predicting: 28it [00:14,  2.00it/s]Extractor Predicting: 29it [00:15,  1.98it/s]Extractor Predicting: 30it [00:15,  1.98it/s]Extractor Predicting: 31it [00:16,  2.00it/s]Extractor Predicting: 32it [00:16,  1.85it/s]Extractor Predicting: 33it [00:17,  1.83it/s]Extractor Predicting: 34it [00:17,  1.82it/s]Extractor Predicting: 35it [00:18,  1.81it/s]Extractor Predicting: 36it [00:18,  1.89it/s]Extractor Predicting: 37it [00:19,  1.86it/s]Extractor Predicting: 38it [00:20,  1.84it/s]Extractor Predicting: 39it [00:20,  1.86it/s]Extractor Predicting: 40it [00:21,  1.83it/s]Extractor Predicting: 41it [00:21,  1.80it/s]Extractor Predicting: 42it [00:22,  1.84it/s]Extractor Predicting: 43it [00:22,  1.86it/s]Extractor Predicting: 44it [00:23,  1.86it/s]Extractor Predicting: 45it [00:23,  1.70it/s]Extractor Predicting: 46it [00:24,  1.69it/s]Extractor Predicting: 47it [00:25,  1.71it/s]Extractor Predicting: 48it [00:25,  1.75it/s]Extractor Predicting: 49it [00:26,  1.79it/s]Extractor Predicting: 50it [00:26,  1.81it/s]Extractor Predicting: 51it [00:27,  1.86it/s]Extractor Predicting: 52it [00:27,  1.83it/s]Extractor Predicting: 53it [00:28,  1.85it/s]Extractor Predicting: 54it [00:28,  1.86it/s]Extractor Predicting: 55it [00:29,  1.84it/s]Extractor Predicting: 56it [00:30,  1.77it/s]Extractor Predicting: 57it [00:30,  1.78it/s]Extractor Predicting: 58it [00:31,  1.80it/s]Extractor Predicting: 59it [00:31,  1.78it/s]Extractor Predicting: 60it [00:32,  1.80it/s]Extractor Predicting: 61it [00:32,  1.80it/s]Extractor Predicting: 62it [00:33,  1.84it/s]Extractor Predicting: 63it [00:33,  1.81it/s]Extractor Predicting: 64it [00:34,  1.84it/s]Extractor Predicting: 65it [00:34,  1.87it/s]Extractor Predicting: 66it [00:35,  1.87it/s]Extractor Predicting: 67it [00:36,  1.84it/s]Extractor Predicting: 68it [00:36,  1.84it/s]Extractor Predicting: 69it [00:37,  1.80it/s]Extractor Predicting: 70it [00:37,  1.78it/s]Extractor Predicting: 71it [00:38,  1.78it/s]Extractor Predicting: 72it [00:38,  1.81it/s]Extractor Predicting: 73it [00:39,  1.79it/s]Extractor Predicting: 74it [00:39,  1.81it/s]Extractor Predicting: 75it [00:40,  1.83it/s]Extractor Predicting: 76it [00:41,  1.81it/s]Extractor Predicting: 77it [00:41,  1.81it/s]Extractor Predicting: 78it [00:42,  1.75it/s]Extractor Predicting: 79it [00:42,  1.78it/s]Extractor Predicting: 80it [00:43,  1.83it/s]Extractor Predicting: 81it [00:43,  1.80it/s]Extractor Predicting: 82it [00:44,  1.77it/s]Extractor Predicting: 83it [00:45,  1.77it/s]Extractor Predicting: 84it [00:45,  1.80it/s]Extractor Predicting: 85it [00:46,  1.78it/s]Extractor Predicting: 86it [00:46,  1.76it/s]Extractor Predicting: 87it [00:47,  1.81it/s]Extractor Predicting: 88it [00:47,  1.83it/s]Extractor Predicting: 89it [00:48,  1.88it/s]Extractor Predicting: 90it [00:48,  1.87it/s]Extractor Predicting: 91it [00:49,  1.86it/s]Extractor Predicting: 92it [00:49,  1.85it/s]Extractor Predicting: 93it [00:50,  1.90it/s]Extractor Predicting: 94it [00:50,  1.90it/s]Extractor Predicting: 95it [00:51,  1.88it/s]Extractor Predicting: 96it [00:51,  1.91it/s]Extractor Predicting: 97it [00:52,  1.94it/s]Extractor Predicting: 98it [00:53,  1.90it/s]Extractor Predicting: 99it [00:53,  1.88it/s]Extractor Predicting: 100it [00:54,  1.91it/s]Extractor Predicting: 101it [00:54,  1.90it/s]Extractor Predicting: 102it [00:55,  1.85it/s]Extractor Predicting: 103it [00:55,  1.83it/s]Extractor Predicting: 104it [00:56,  1.88it/s]Extractor Predicting: 105it [00:56,  1.81it/s]Extractor Predicting: 106it [00:57,  1.85it/s]Extractor Predicting: 107it [00:57,  1.86it/s]Extractor Predicting: 108it [00:58,  1.82it/s]Extractor Predicting: 109it [00:59,  1.81it/s]Extractor Predicting: 110it [00:59,  1.83it/s]Extractor Predicting: 111it [01:00,  1.82it/s]Extractor Predicting: 112it [01:00,  1.82it/s]Extractor Predicting: 113it [01:01,  1.86it/s]Extractor Predicting: 114it [01:01,  1.85it/s]Extractor Predicting: 115it [01:02,  1.85it/s]Extractor Predicting: 116it [01:02,  1.89it/s]Extractor Predicting: 117it [01:03,  1.89it/s]Extractor Predicting: 118it [01:03,  1.94it/s]Extractor Predicting: 119it [01:04,  1.97it/s]Extractor Predicting: 120it [01:04,  1.96it/s]Extractor Predicting: 121it [01:05,  1.95it/s]Extractor Predicting: 122it [01:05,  1.93it/s]Extractor Predicting: 123it [01:06,  1.89it/s]Extractor Predicting: 124it [01:06,  1.92it/s]Extractor Predicting: 125it [01:07,  1.70it/s]Extractor Predicting: 126it [01:08,  1.73it/s]Extractor Predicting: 127it [01:08,  1.73it/s]Extractor Predicting: 128it [01:09,  1.73it/s]Extractor Predicting: 129it [01:09,  1.72it/s]Extractor Predicting: 130it [01:10,  1.71it/s]Extractor Predicting: 131it [01:11,  1.69it/s]Extractor Predicting: 132it [01:11,  1.71it/s]Extractor Predicting: 133it [01:12,  1.73it/s]Extractor Predicting: 134it [01:12,  1.78it/s]Extractor Predicting: 135it [01:13,  1.71it/s]Extractor Predicting: 136it [01:14,  1.70it/s]Extractor Predicting: 137it [01:14,  1.70it/s]Extractor Predicting: 138it [01:15,  1.72it/s]Extractor Predicting: 139it [01:15,  1.73it/s]Extractor Predicting: 140it [01:16,  1.69it/s]Extractor Predicting: 141it [01:16,  1.70it/s]Extractor Predicting: 142it [01:17,  1.70it/s]Extractor Predicting: 143it [01:18,  1.73it/s]Extractor Predicting: 144it [01:18,  1.72it/s]Extractor Predicting: 145it [01:19,  1.71it/s]Extractor Predicting: 146it [01:19,  1.72it/s]Extractor Predicting: 147it [01:20,  1.74it/s]Extractor Predicting: 148it [01:20,  1.74it/s]Extractor Predicting: 149it [01:21,  1.77it/s]Extractor Predicting: 150it [01:22,  1.77it/s]Extractor Predicting: 151it [01:22,  1.75it/s]Extractor Predicting: 152it [01:23,  1.76it/s]Extractor Predicting: 153it [01:23,  1.73it/s]Extractor Predicting: 154it [01:24,  1.72it/s]Extractor Predicting: 155it [01:24,  1.74it/s]Extractor Predicting: 156it [01:25,  1.79it/s]Extractor Predicting: 157it [01:25,  1.86it/s]Extractor Predicting: 158it [01:26,  1.86it/s]Extractor Predicting: 159it [01:27,  1.86it/s]Extractor Predicting: 160it [01:27,  1.87it/s]Extractor Predicting: 161it [01:28,  1.91it/s]Extractor Predicting: 162it [01:28,  1.86it/s]Extractor Predicting: 163it [01:29,  1.85it/s]Extractor Predicting: 164it [01:29,  1.87it/s]Extractor Predicting: 165it [01:30,  1.92it/s]Extractor Predicting: 166it [01:30,  1.91it/s]Extractor Predicting: 167it [01:31,  1.99it/s]Extractor Predicting: 168it [01:31,  2.00it/s]Extractor Predicting: 169it [01:32,  2.03it/s]Extractor Predicting: 170it [01:32,  1.97it/s]Extractor Predicting: 171it [01:33,  1.88it/s]Extractor Predicting: 172it [01:33,  1.83it/s]Extractor Predicting: 173it [01:34,  1.88it/s]Extractor Predicting: 173it [01:34,  1.83it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:26,578 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:26,583 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:26,584 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:26,584 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:26,584 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:35:27,218 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:35:27,219 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:35:27,787 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:35:28,797 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:35:28,797 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:31,428 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:31,430 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:31,430 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:31,430 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:31,430 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:35:32,247 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:35:32,248 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:35:33,005 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:35:33,313 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:35:33,313 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.85it/s]Extractor Predicting: 2it [00:01,  1.75it/s]Extractor Predicting: 3it [00:01,  1.67it/s]Extractor Predicting: 4it [00:02,  1.72it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.68it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:04,  1.54it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.57it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.63it/s]Extractor Predicting: 14it [00:08,  1.71it/s]Extractor Predicting: 15it [00:09,  1.82it/s]Extractor Predicting: 16it [00:09,  1.92it/s]Extractor Predicting: 17it [00:09,  2.04it/s]Extractor Predicting: 18it [00:10,  2.10it/s]Extractor Predicting: 19it [00:10,  2.10it/s]Extractor Predicting: 20it [00:11,  2.15it/s]Extractor Predicting: 21it [00:11,  2.18it/s]Extractor Predicting: 22it [00:12,  2.12it/s]Extractor Predicting: 23it [00:12,  2.09it/s]Extractor Predicting: 24it [00:13,  2.08it/s]Extractor Predicting: 25it [00:13,  2.09it/s]Extractor Predicting: 26it [00:14,  2.13it/s]Extractor Predicting: 27it [00:14,  2.07it/s]Extractor Predicting: 28it [00:15,  2.10it/s]Extractor Predicting: 29it [00:15,  2.11it/s]Extractor Predicting: 30it [00:15,  2.16it/s]Extractor Predicting: 31it [00:16,  2.18it/s]Extractor Predicting: 32it [00:16,  2.17it/s]Extractor Predicting: 33it [00:17,  2.18it/s]Extractor Predicting: 34it [00:17,  2.15it/s]Extractor Predicting: 35it [00:18,  2.16it/s]Extractor Predicting: 36it [00:18,  2.09it/s]Extractor Predicting: 37it [00:19,  2.04it/s]Extractor Predicting: 38it [00:19,  2.12it/s]Extractor Predicting: 39it [00:20,  2.11it/s]Extractor Predicting: 40it [00:20,  2.12it/s]Extractor Predicting: 41it [00:21,  2.12it/s]Extractor Predicting: 42it [00:21,  2.15it/s]Extractor Predicting: 43it [00:22,  2.11it/s]Extractor Predicting: 44it [00:22,  1.92it/s]Extractor Predicting: 45it [00:23,  1.81it/s]Extractor Predicting: 46it [00:23,  1.73it/s]Extractor Predicting: 47it [00:24,  1.70it/s]Extractor Predicting: 48it [00:25,  1.68it/s]Extractor Predicting: 49it [00:25,  1.67it/s]Extractor Predicting: 50it [00:26,  1.65it/s]Extractor Predicting: 51it [00:27,  1.64it/s]Extractor Predicting: 52it [00:27,  1.63it/s]Extractor Predicting: 53it [00:28,  1.62it/s]Extractor Predicting: 54it [00:28,  1.60it/s]Extractor Predicting: 55it [00:29,  1.63it/s]Extractor Predicting: 56it [00:30,  1.60it/s]Extractor Predicting: 57it [00:30,  1.59it/s]Extractor Predicting: 58it [00:31,  1.60it/s]Extractor Predicting: 59it [00:31,  1.92it/s]Extractor Predicting: 59it [00:31,  1.86it/s]
[INFO|configuration_utils.py:515] 2023-08-28 03:36:06,222 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:36:06,225 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 03:36:06,229 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:36:06,229 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 03:36:06,231 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 03:36:09,261 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 03:36:09,264 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 03:36:09,279 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:36:09,280 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 03:36:09,285 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:36:09,289 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:36:09,289 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:36:09,289 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:36:09,289 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:36:09,289 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:36:09,289 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 03:36:09,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:10,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:11,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:11,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:12,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:13,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:13,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:14,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:14,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:15,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:15,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:16,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:17,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:17,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:18,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:19,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:19,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:20,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:20,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:21,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:22,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:22,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:23,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:24,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:15<02:17, 15.23s/it][WARNING|generation_utils.py:914] 2023-08-28 03:36:24,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:25,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:25,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:26,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:27,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:27,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:28,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:28,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:29,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:29,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:30,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:31,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:32,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:32,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:33,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:34,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:34,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:35,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:35,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:36,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:37,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:37,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:38,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:38,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:39,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:40,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:40,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:31<02:06, 15.87s/it][WARNING|generation_utils.py:914] 2023-08-28 03:36:41,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:41,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:42,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:43,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:43,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:44,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:44,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:45,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:46,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:46,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:47,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:47,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:48,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:49,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:49,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:50,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:50,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:51,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:52,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:52,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:53,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:54,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:54,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:55,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:55,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:56,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:57,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:57,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:58,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:58,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:59,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:00,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:00,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:01,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:01,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:02,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:02,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:03,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:04,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:04,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:05,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:05,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:06,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:07,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:07,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:58<02:27, 21.10s/it][WARNING|generation_utils.py:914] 2023-08-28 03:37:08,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:09,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:10,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:10,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:11,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:12,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:12,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:13,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:14,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:14,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:15,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:16,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:16,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:17,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:18,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:18,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:19,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:19,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:20,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:21,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:21,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:22,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:23,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:24,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:15<01:55, 19.30s/it][WARNING|generation_utils.py:914] 2023-08-28 03:37:24,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:25,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:26,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:26,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:27,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:28,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:28,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:29,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:29,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:30,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:30,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:31,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:32,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:32,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:33,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:33,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:34,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:35,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:35,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:36,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:36,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:37,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:38,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:38,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:39,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:40,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:40,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:31<01:30, 18.20s/it][WARNING|generation_utils.py:914] 2023-08-28 03:37:41,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:41,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:42,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:43,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:43,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:44,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:44,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:45,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:46,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:46,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:47,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:47,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:48,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:49,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:49,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:50,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:51,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:51,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:52,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:52,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:53,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:54,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:54,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:55,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:55,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:46<01:08, 17.20s/it][WARNING|generation_utils.py:914] 2023-08-28 03:37:56,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:56,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:57,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:58,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:58,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:59,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:00,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:01,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:01,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:02,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:03,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:03,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:04,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:04,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:05,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:06,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:06,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:07,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:07,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:08,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:09,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:09,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:10,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:11,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:02<00:49, 16.55s/it][WARNING|generation_utils.py:914] 2023-08-28 03:38:11,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:12,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:12,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:13,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:13,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:14,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:14,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:15,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:15,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:16,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:17,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:17,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:18,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:18,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:19,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:19,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:20,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:20,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:21,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:22,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:22,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:23,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:23,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:24,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:15<00:31, 15.53s/it][WARNING|generation_utils.py:914] 2023-08-28 03:38:24,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:25,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:26,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:26,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:27,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:28,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:28,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:29,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:30,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:30,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:31,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:32,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:33,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:33,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:34,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:34,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:35,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:35,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:36,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:37,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:37,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:38,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:29<00:14, 14.99s/it][WARNING|generation_utils.py:914] 2023-08-28 03:38:38,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:39,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:40,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:40,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:41,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:41,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:42,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:43,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:43,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:44,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:44,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:45,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:46,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:46,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:47,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:48,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:48,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:49,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:49,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:50,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:51,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:51,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:52,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:43<00:00, 14.75s/it]Generating: 100%|██████████| 10/10 [02:43<00:00, 16.35s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:38:59,096 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:38:59,102 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:38:59,102 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:38:59,102 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:38:59,102 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:38:59,704 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:38:59,705 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:39:00,294 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:39:01,354 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:39:01,354 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:03,451 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:03,456 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:03,456 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:03,456 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:03,456 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:39:04,126 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:39:04,127 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:39:04,689 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:39:04,862 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:39:04,862 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : given name .', 'success_rate': 0.80859375, 'errors': {'', "('William', 'given name', '', 'He was born in the province of Oise , his father being William , Duke of Normandy , 1st Earl of Normandy , and his mother being Mary .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 201, 'raw': 288}
{'target': 600, 'success': 226, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 287, 'raw': 416}
{'target': 600, 'success': 310, 'raw': 448}
{'target': 600, 'success': 333, 'raw': 480}
{'target': 600, 'success': 351, 'raw': 512}
{'target': 600, 'success': 377, 'raw': 544}
{'target': 600, 'success': 403, 'raw': 576}
{'target': 600, 'success': 424, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 468, 'raw': 672}
{'target': 600, 'success': 486, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 532, 'raw': 768}
{'target': 600, 'success': 554, 'raw': 800}
{'target': 600, 'success': 576, 'raw': 832}
{'target': 600, 'success': 601, 'raw': 864}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.6956018518518519, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Brazil', 'languages spoken, written or signed', '', 'Many languages are spoken in Brazil , and there are various communities and organizations advocating for a stronger free and open society .')", "('Estonia', 'languages spoken, written or signed', '', 'The language of Estonia is Estonian , the official language of the Republic of the Union ( DSO ) ) .')"}}
['Relation : lowest point . Context : Later in the year ( 1143 ) , Puyi and his allies made a conquest over the eastern part of the province of Sinti , which was occupied by the Franks and Normans . Head Entity : Sinti , Tail Entity : southern part .\n']
['Relation : lowest point . Context : Later in the year ( 1143 ) , Puyi and his allies made a conquest over the eastern part of the province of Sinti , which was occupied by the Franks and Normans . Head Entity : Sinti , Tail Entity : southern part .\n', 'Relation : lowest point . Context : After the death of Emperor Xuanzong his elder son Liu Bang was the first emperor to die at the age of only two . Head Entity : emperor , Tail Entity : highest point .\n']
{'target': 600, 'success': 12, 'raw': 32}
{'target': 600, 'success': 23, 'raw': 64}
{'target': 600, 'success': 32, 'raw': 96}
{'target': 600, 'success': 44, 'raw': 128}
{'target': 600, 'success': 55, 'raw': 160}
{'target': 600, 'success': 70, 'raw': 192}
{'target': 600, 'success': 86, 'raw': 224}
{'target': 600, 'success': 102, 'raw': 256}
{'target': 600, 'success': 115, 'raw': 288}
{'target': 600, 'success': 125, 'raw': 320}
{'target': 600, 'success': 141, 'raw': 352}
{'target': 600, 'success': 158, 'raw': 384}
{'target': 600, 'success': 176, 'raw': 416}
{'target': 600, 'success': 192, 'raw': 448}
{'target': 600, 'success': 206, 'raw': 480}
{'target': 600, 'success': 221, 'raw': 512}
{'target': 600, 'success': 233, 'raw': 544}
{'target': 600, 'success': 249, 'raw': 576}
{'target': 600, 'success': 265, 'raw': 608}
{'target': 600, 'success': 273, 'raw': 640}
{'target': 600, 'success': 288, 'raw': 672}
{'target': 600, 'success': 308, 'raw': 704}
{'target': 600, 'success': 317, 'raw': 736}
{'target': 600, 'success': 332, 'raw': 768}
{'target': 600, 'success': 342, 'raw': 800}
{'target': 600, 'success': 353, 'raw': 832}
{'target': 600, 'success': 369, 'raw': 864}
{'target': 600, 'success': 385, 'raw': 896}
{'target': 600, 'success': 394, 'raw': 928}
{'target': 600, 'success': 409, 'raw': 960}
{'target': 600, 'success': 423, 'raw': 992}
{'target': 600, 'success': 432, 'raw': 1024}
{'target': 600, 'success': 446, 'raw': 1056}
{'target': 600, 'success': 460, 'raw': 1088}
{'target': 600, 'success': 473, 'raw': 1120}
{'target': 600, 'success': 486, 'raw': 1152}
{'target': 600, 'success': 498, 'raw': 1184}
{'target': 600, 'success': 512, 'raw': 1216}
{'target': 600, 'success': 522, 'raw': 1248}
{'target': 600, 'success': 533, 'raw': 1280}
{'target': 600, 'success': 550, 'raw': 1312}
{'target': 600, 'success': 560, 'raw': 1344}
{'target': 600, 'success': 572, 'raw': 1376}
{'target': 600, 'success': 584, 'raw': 1408}
{'target': 600, 'success': 600, 'raw': 1440}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.4166666666666667, 'errors': {'', "('2004 FIFA World Cup', 'lowest point', '', 'The next day , he made a triumphant recovery and made his international debut at the 2004 FIFA World Cup against Ecuador .')", 'not enough values to unpack (expected 2, got 1)', "('New York Giants', 'lowest point', '', 'In the second season , a team led by George Foreman and David Southee were allowed to make another bowl game against the New York Giants .')", 'too many values to unpack (expected 2)', "('lowest point', 'lowest point', '', 'For comparison , in 1998 , it was the lowest point per square mile since 1960 , when it was 1 . 10 .')", "('63', 'lowest point', '', 'On the night of 9 November 2014 , the Southeastern Conference led by Jaylen Brown was led by Duke Blue Devils in a 63 66 victory over the Southeast Division rival St. Charles .')", "('second', 'lowest point', '', 'He is currently playing with fellow former New Zealand prop Tom Paine in the Championship side Auckland Roosters , and he moved up the rankings to second .')", "('Manchester', 'lowest point', '', 'The area was home to the city of Manchester ( now Manchester City ) , founded in 1859 by the English Civil War hero Sir Walter Scott .')"}}
['Relation : mother . Context : Later in Life , he came to love the beauty and natural beauty of the forests at the end of the third trimester , when she suffered death . Head Entity : forests at the end of the third trimester , Tail Entity : Mother .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : mother .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('king', 'mother', '', 'The first male and last female monarch of the Kingdom of Mexico , the king of the Andes ( d.')"}}
['Relation : mouth of the watercourse . Context : Later in the year ( 1143 ) , Puyi and his allies made atone for the fall of the last King of the Puebla , Puzco . Head Entity : Puyi , Tail Entity : Puducese .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 200, 'raw': 288}
{'target': 600, 'success': 221, 'raw': 320}
{'target': 600, 'success': 241, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 289, 'raw': 416}
{'target': 600, 'success': 311, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 357, 'raw': 512}
{'target': 600, 'success': 383, 'raw': 544}
{'target': 600, 'success': 404, 'raw': 576}
{'target': 600, 'success': 425, 'raw': 608}
{'target': 600, 'success': 440, 'raw': 640}
{'target': 600, 'success': 463, 'raw': 672}
{'target': 600, 'success': 488, 'raw': 704}
{'target': 600, 'success': 509, 'raw': 736}
{'target': 600, 'success': 536, 'raw': 768}
{'target': 600, 'success': 558, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 606, 'raw': 864}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.7013888888888888, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n']
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n', 'Relation : genre . Context : Eileen McClelland ( born June 24 , 1953 ) is an American radio talk show host who has appeared as a regular on both the Howard Stern Show and the Morning Mix . Head Entity : Howard Stern Show , Tail Entity : television talk show .\n']
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n', 'Relation : genre . Context : Eileen McClelland ( born June 24 , 1953 ) is an American radio talk show host who has appeared as a regular on both the Howard Stern Show and the Morning Mix . Head Entity : Howard Stern Show , Tail Entity : television talk show .\n', 'Relation : genre . Context : This film explores the social , social structure of the New York city of Times Square , which is populated and industrialized by wealthy Manhattanites . Head Entity : Times Square , Tail Entity : New York City .\n']
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 320, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 468, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 570, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : genre .', 'success_rate': 0.76625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.8020833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('The albums', 'is a list of', '', 'The albums songs , songs played and discography of the albums members , the artist , are listed in alphabetical order .')", "('programming languages', 'is a list of', '', 'It can be used primarily in conjunction with other programming languages such as .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8111979166666666, 'errors': {'', "('Pluto', 'located on astronomical body', '', 'In the past , Pluto has been considered to be a binary system inhabited by one giant being , as well as being the first to be named after Pluto .')", 'too many values to unpack (expected 2)', "('Its orbit', 'located on astronomical body', '', 'Its orbit shows an eccentricity of 0 . 18 and an inclination of 8 degrees from the plane of the ecliptic .')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('it', 'manufacturer', '', 'It is known in Japan for its high quality and performance components , along with its long range of performance airsoft s.')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8179347826086957, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/3_ext.jsonl'}}
estimate vocab size: 12731
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12831, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.69it/s]Extractor Estimating: 2it [00:01,  1.47it/s]Extractor Estimating: 3it [00:02,  1.44it/s]Extractor Estimating: 4it [00:02,  1.47it/s]Extractor Estimating: 5it [00:03,  1.51it/s]Extractor Estimating: 6it [00:03,  1.55it/s]Extractor Estimating: 7it [00:04,  1.57it/s]Extractor Estimating: 8it [00:05,  1.57it/s]Extractor Estimating: 9it [00:05,  1.54it/s]Extractor Estimating: 10it [00:06,  1.55it/s]Extractor Estimating: 11it [00:07,  1.60it/s]Extractor Estimating: 12it [00:07,  1.65it/s]Extractor Estimating: 13it [00:08,  1.62it/s]Extractor Estimating: 14it [00:08,  1.59it/s]Extractor Estimating: 15it [00:09,  1.60it/s]Extractor Estimating: 16it [00:10,  1.59it/s]Extractor Estimating: 17it [00:10,  1.58it/s]Extractor Estimating: 18it [00:11,  1.58it/s]Extractor Estimating: 19it [00:12,  1.56it/s]Extractor Estimating: 20it [00:12,  1.60it/s]Extractor Estimating: 21it [00:13,  1.63it/s]Extractor Estimating: 22it [00:13,  1.64it/s]Extractor Estimating: 23it [00:14,  1.54it/s]Extractor Estimating: 24it [00:15,  1.61it/s]Extractor Estimating: 25it [00:15,  1.68it/s]Extractor Estimating: 26it [00:16,  1.73it/s]Extractor Estimating: 27it [00:16,  1.73it/s]Extractor Estimating: 28it [00:17,  1.74it/s]Extractor Estimating: 29it [00:17,  1.76it/s]Extractor Estimating: 30it [00:18,  1.77it/s]Extractor Estimating: 31it [00:19,  1.75it/s]Extractor Estimating: 32it [00:19,  1.77it/s]Extractor Estimating: 33it [00:20,  1.78it/s]Extractor Estimating: 34it [00:20,  1.81it/s]Extractor Estimating: 35it [00:21,  1.74it/s]Extractor Estimating: 36it [00:21,  1.75it/s]Extractor Estimating: 37it [00:22,  1.72it/s]Extractor Estimating: 38it [00:23,  1.75it/s]Extractor Estimating: 39it [00:23,  1.80it/s]Extractor Estimating: 40it [00:24,  1.75it/s]Extractor Estimating: 41it [00:24,  1.73it/s]Extractor Estimating: 42it [00:25,  1.76it/s]Extractor Estimating: 43it [00:26,  1.63it/s]Extractor Estimating: 44it [00:26,  1.68it/s]Extractor Estimating: 45it [00:27,  1.74it/s]Extractor Estimating: 46it [00:27,  1.73it/s]Extractor Estimating: 47it [00:28,  1.76it/s]Extractor Estimating: 48it [00:28,  1.80it/s]Extractor Estimating: 49it [00:29,  1.82it/s]Extractor Estimating: 50it [00:29,  1.83it/s]Extractor Estimating: 51it [00:30,  1.75it/s]Extractor Estimating: 52it [00:31,  1.69it/s]Extractor Estimating: 53it [00:31,  1.67it/s]Extractor Estimating: 54it [00:32,  1.64it/s]Extractor Estimating: 55it [00:33,  1.61it/s]Extractor Estimating: 56it [00:33,  1.58it/s]Extractor Estimating: 57it [00:34,  1.55it/s]Extractor Estimating: 58it [00:35,  1.56it/s]Extractor Estimating: 59it [00:35,  1.62it/s]Extractor Estimating: 60it [00:36,  1.60it/s]Extractor Estimating: 61it [00:36,  1.57it/s]Extractor Estimating: 62it [00:37,  1.59it/s]Extractor Estimating: 63it [00:38,  1.55it/s]Extractor Estimating: 64it [00:38,  1.57it/s]Extractor Estimating: 65it [00:39,  1.61it/s]Extractor Estimating: 66it [00:40,  1.57it/s]Extractor Estimating: 67it [00:40,  1.58it/s]Extractor Estimating: 68it [00:41,  1.58it/s]Extractor Estimating: 69it [00:41,  1.57it/s]Extractor Estimating: 70it [00:42,  1.59it/s]Extractor Estimating: 71it [00:43,  1.56it/s]Extractor Estimating: 72it [00:43,  1.57it/s]Extractor Estimating: 73it [00:44,  1.56it/s]Extractor Estimating: 74it [00:45,  1.58it/s]Extractor Estimating: 75it [00:45,  1.57it/s]Extractor Estimating: 76it [00:46,  1.39it/s]Extractor Estimating: 77it [00:47,  1.45it/s]Extractor Estimating: 78it [00:48,  1.45it/s]Extractor Estimating: 79it [00:48,  1.44it/s]Extractor Estimating: 80it [00:49,  1.48it/s]Extractor Estimating: 81it [00:49,  1.50it/s]Extractor Estimating: 82it [00:50,  1.54it/s]Extractor Estimating: 83it [00:51,  1.50it/s]Extractor Estimating: 84it [00:51,  1.49it/s]Extractor Estimating: 85it [00:52,  1.47it/s]Extractor Estimating: 86it [00:53,  1.42it/s]Extractor Estimating: 87it [00:54,  1.43it/s]Extractor Estimating: 88it [00:54,  1.43it/s]Extractor Estimating: 89it [00:55,  1.43it/s]Extractor Estimating: 90it [00:56,  1.44it/s]Extractor Estimating: 91it [00:57,  1.36it/s]Extractor Estimating: 92it [00:57,  1.41it/s]Extractor Estimating: 93it [00:58,  1.45it/s]Extractor Estimating: 94it [00:59,  1.47it/s]Extractor Estimating: 95it [00:59,  1.50it/s]Extractor Estimating: 96it [01:00,  1.50it/s]Extractor Estimating: 97it [01:00,  1.55it/s]Extractor Estimating: 98it [01:01,  1.49it/s]Extractor Estimating: 99it [01:02,  1.49it/s]Extractor Estimating: 100it [01:02,  1.49it/s]Extractor Estimating: 101it [01:03,  1.48it/s]Extractor Estimating: 102it [01:04,  1.53it/s]Extractor Estimating: 103it [01:04,  1.57it/s]Extractor Estimating: 104it [01:05,  1.63it/s]Extractor Estimating: 105it [01:06,  1.61it/s]Extractor Estimating: 106it [01:06,  1.66it/s]Extractor Estimating: 107it [01:07,  1.68it/s]Extractor Estimating: 108it [01:07,  1.67it/s]Extractor Estimating: 109it [01:08,  1.68it/s]Extractor Estimating: 110it [01:08,  1.71it/s]Extractor Estimating: 111it [01:09,  1.69it/s]Extractor Estimating: 112it [01:10,  1.64it/s]Extractor Estimating: 113it [01:10,  1.60it/s]Extractor Estimating: 114it [01:11,  1.67it/s]Extractor Estimating: 115it [01:12,  1.65it/s]Extractor Estimating: 116it [01:12,  1.69it/s]Extractor Estimating: 117it [01:13,  1.65it/s]Extractor Estimating: 118it [01:13,  1.63it/s]Extractor Estimating: 119it [01:14,  1.59it/s]Extractor Estimating: 120it [01:15,  1.62it/s]Extractor Estimating: 121it [01:15,  1.57it/s]Extractor Estimating: 122it [01:16,  1.57it/s]Extractor Estimating: 123it [01:17,  1.58it/s]Extractor Estimating: 124it [01:17,  1.59it/s]Extractor Estimating: 125it [01:18,  1.63it/s]Extractor Estimating: 126it [01:18,  1.64it/s]Extractor Estimating: 127it [01:19,  1.62it/s]Extractor Estimating: 128it [01:20,  1.63it/s]Extractor Estimating: 129it [01:20,  1.63it/s]Extractor Estimating: 130it [01:21,  1.63it/s]Extractor Estimating: 131it [01:21,  1.66it/s]Extractor Estimating: 132it [01:22,  1.65it/s]Extractor Estimating: 133it [01:23,  1.65it/s]Extractor Estimating: 134it [01:23,  1.66it/s]Extractor Estimating: 135it [01:24,  1.61it/s]Extractor Estimating: 136it [01:24,  1.65it/s]Extractor Estimating: 137it [01:25,  1.59it/s]Extractor Estimating: 138it [01:26,  1.60it/s]Extractor Estimating: 139it [01:26,  1.63it/s]Extractor Estimating: 140it [01:27,  1.56it/s]Extractor Estimating: 141it [01:28,  1.57it/s]Extractor Estimating: 142it [01:28,  1.58it/s]Extractor Estimating: 143it [01:29,  1.59it/s]Extractor Estimating: 144it [01:30,  1.55it/s]Extractor Estimating: 145it [01:30,  1.60it/s]Extractor Estimating: 146it [01:31,  1.61it/s]Extractor Estimating: 147it [01:31,  1.63it/s]Extractor Estimating: 148it [01:32,  1.48it/s]Extractor Estimating: 149it [01:33,  1.48it/s]Extractor Estimating: 150it [01:33,  1.57it/s]Extractor Estimating: 151it [01:34,  1.62it/s]Extractor Estimating: 152it [01:35,  1.64it/s]Extractor Estimating: 153it [01:35,  1.68it/s]Extractor Estimating: 154it [01:36,  1.63it/s]Extractor Estimating: 155it [01:36,  1.66it/s]Extractor Estimating: 156it [01:37,  1.64it/s]Extractor Estimating: 157it [01:38,  1.57it/s]Extractor Estimating: 158it [01:38,  1.59it/s]Extractor Estimating: 159it [01:39,  1.63it/s]Extractor Estimating: 160it [01:40,  1.63it/s]Extractor Estimating: 161it [01:40,  1.68it/s]Extractor Estimating: 162it [01:41,  1.64it/s]Extractor Estimating: 163it [01:41,  1.66it/s]Extractor Estimating: 164it [01:42,  1.63it/s]Extractor Estimating: 165it [01:43,  1.64it/s]Extractor Estimating: 166it [01:43,  1.68it/s]Extractor Estimating: 167it [01:44,  1.72it/s]Extractor Estimating: 168it [01:44,  1.72it/s]Extractor Estimating: 169it [01:45,  1.69it/s]Extractor Estimating: 170it [01:45,  1.68it/s]Extractor Estimating: 171it [01:46,  1.67it/s]Extractor Estimating: 172it [01:47,  1.75it/s]Extractor Estimating: 173it [01:47,  1.64it/s]Extractor Estimating: 174it [01:48,  1.62it/s]Extractor Estimating: 175it [01:48,  1.67it/s]Extractor Estimating: 176it [01:49,  1.72it/s]Extractor Estimating: 177it [01:50,  1.71it/s]Extractor Estimating: 178it [01:50,  1.73it/s]Extractor Estimating: 179it [01:51,  1.78it/s]Extractor Estimating: 180it [01:51,  1.74it/s]Extractor Estimating: 181it [01:52,  1.75it/s]Extractor Estimating: 182it [01:52,  1.72it/s]Extractor Estimating: 183it [01:53,  1.78it/s]Extractor Estimating: 184it [01:54,  1.73it/s]Extractor Estimating: 185it [01:54,  1.73it/s]Extractor Estimating: 186it [01:55,  1.68it/s]Extractor Estimating: 187it [01:55,  1.68it/s]Extractor Estimating: 188it [01:56,  1.72it/s]Extractor Estimating: 189it [01:57,  1.69it/s]Extractor Estimating: 190it [01:57,  1.68it/s]Extractor Estimating: 191it [01:58,  1.69it/s]Extractor Estimating: 192it [01:58,  1.69it/s]Extractor Estimating: 193it [01:59,  1.79it/s]Extractor Estimating: 194it [01:59,  1.77it/s]Extractor Estimating: 195it [02:00,  1.69it/s]Extractor Estimating: 196it [02:01,  1.68it/s]Extractor Estimating: 197it [02:01,  1.72it/s]Extractor Estimating: 198it [02:02,  1.71it/s]Extractor Estimating: 199it [02:02,  1.68it/s]Extractor Estimating: 200it [02:03,  1.66it/s]Extractor Estimating: 201it [02:04,  1.66it/s]Extractor Estimating: 202it [02:04,  1.68it/s]Extractor Estimating: 203it [02:05,  1.71it/s]Extractor Estimating: 204it [02:05,  1.71it/s]Extractor Estimating: 205it [02:06,  1.62it/s]Extractor Estimating: 206it [02:07,  1.69it/s]Extractor Estimating: 207it [02:07,  1.66it/s]Extractor Estimating: 208it [02:08,  1.74it/s]Extractor Estimating: 209it [02:08,  1.70it/s]Extractor Estimating: 210it [02:09,  1.69it/s]Extractor Estimating: 211it [02:10,  1.61it/s]Extractor Estimating: 212it [02:10,  1.63it/s]Extractor Estimating: 213it [02:11,  1.67it/s]Extractor Estimating: 214it [02:11,  1.66it/s]Extractor Estimating: 215it [02:12,  1.66it/s]Extractor Estimating: 216it [02:13,  1.72it/s]Extractor Estimating: 217it [02:13,  1.57it/s]Extractor Estimating: 218it [02:14,  1.59it/s]Extractor Estimating: 219it [02:14,  1.63it/s]Extractor Estimating: 220it [02:15,  1.64it/s]Extractor Estimating: 221it [02:16,  1.71it/s]Extractor Estimating: 222it [02:16,  1.69it/s]Extractor Estimating: 223it [02:17,  1.71it/s]Extractor Estimating: 224it [02:17,  1.69it/s]Extractor Estimating: 225it [02:18,  1.65it/s]Extractor Estimating: 226it [02:19,  1.61it/s]Extractor Estimating: 227it [02:19,  1.61it/s]Extractor Estimating: 228it [02:20,  1.62it/s]Extractor Estimating: 229it [02:21,  1.60it/s]Extractor Estimating: 230it [02:21,  1.62it/s]Extractor Estimating: 231it [02:22,  1.62it/s]Extractor Estimating: 232it [02:22,  1.63it/s]Extractor Estimating: 233it [02:23,  1.55it/s]Extractor Estimating: 234it [02:24,  1.56it/s]Extractor Estimating: 235it [02:24,  1.60it/s]Extractor Estimating: 236it [02:25,  1.61it/s]Extractor Estimating: 237it [02:26,  1.56it/s]Extractor Estimating: 238it [02:26,  1.61it/s]Extractor Estimating: 239it [02:27,  1.60it/s]Extractor Estimating: 240it [02:27,  1.62it/s]Extractor Estimating: 241it [02:28,  1.61it/s]Extractor Estimating: 242it [02:29,  1.59it/s]Extractor Estimating: 243it [02:29,  1.65it/s]Extractor Estimating: 244it [02:30,  1.61it/s]Extractor Estimating: 245it [02:31,  1.57it/s]Extractor Estimating: 246it [02:31,  1.58it/s]Extractor Estimating: 247it [02:32,  1.60it/s]Extractor Estimating: 248it [02:32,  1.62it/s]Extractor Estimating: 249it [02:33,  1.62it/s]Extractor Estimating: 250it [02:34,  1.54it/s]Extractor Estimating: 250it [02:34,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:41:55,814 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:41:55,820 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:41:55,821 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:41:55,821 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:41:55,821 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:41:56,431 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:41:56,431 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:41:57,027 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:41:58,065 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:41:58,065 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:42:00,906 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:42:00,911 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:42:00,912 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:42:00,912 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:42:00,912 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:42:01,618 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:42:01,619 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:42:02,197 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:42:02,369 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:42:02,369 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 05:11:56,215 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 05:11:56,242 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 5237 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 21031
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21131, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=21131, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.029, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.041, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 81, avg_time 1.025, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 181, avg_time 1.025, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 62, avg_time 1.024, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 162, avg_time 2.140, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 43, avg_time 1.032, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 143, avg_time 1.027, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 24, avg_time 1.031, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 124, avg_time 1.028, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 5, avg_time 2.139, loss:nan
g_step 1200, step 105, avg_time 1.043, loss:nan
g_step 1300, step 205, avg_time 1.025, loss:nan
g_step 1400, step 86, avg_time 1.016, loss:nan
g_step 1500, step 186, avg_time 1.043, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 67, avg_time 2.119, loss:nan
g_step 1700, step 167, avg_time 1.045, loss:nan
g_step 1800, step 48, avg_time 1.029, loss:nan
g_step 1900, step 148, avg_time 1.013, loss:nan
g_step 2000, step 29, avg_time 1.033, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 129, avg_time 2.145, loss:nan
g_step 2200, step 10, avg_time 1.015, loss:nan
g_step 2300, step 110, avg_time 1.025, loss:nan
g_step 2400, step 210, avg_time 1.030, loss:nan
g_step 2500, step 91, avg_time 1.024, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 191, avg_time 2.139, loss:nan
g_step 2700, step 72, avg_time 1.024, loss:nan
g_step 2800, step 172, avg_time 1.035, loss:nan
g_step 2900, step 53, avg_time 1.020, loss:nan
g_step 3000, step 153, avg_time 1.029, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 34, avg_time 2.135, loss:nan
g_step 3200, step 134, avg_time 1.027, loss:nan
g_step 3300, step 15, avg_time 1.027, loss:nan
g_step 3400, step 115, avg_time 1.037, loss:nan
g_step 3500, step 215, avg_time 1.028, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 96, avg_time 2.138, loss:nan
g_step 3700, step 196, avg_time 1.029, loss:nan
g_step 3800, step 77, avg_time 1.024, loss:nan
g_step 3900, step 177, avg_time 1.028, loss:nan
g_step 4000, step 58, avg_time 1.035, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 158, avg_time 2.144, loss:nan
g_step 4200, step 39, avg_time 1.013, loss:nan
g_step 4300, step 139, avg_time 1.037, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 05:11:56 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 05:11:56 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_05-11-56_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 05:11:57 - WARNING - datasets.builder -   Using custom data configuration default-e4cc4ed58dedf5e2
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-e4cc4ed58dedf5e2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 05:11:57,490 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:11:57,491 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 05:11:57,491 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:11:57,492 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 05:11:57,500 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:11:57,505 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:11:57,505 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:11:57,505 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:11:57,505 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:11:57,505 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:11:57,506 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 05:11:57,646 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 05:12:00,697 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 05:12:00,699 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-e4cc4ed58dedf5e2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.14ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.97ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.43ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.84ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.10ba/s]100%|██████████| 6/6 [00:01<00:00,  4.46ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.11ba/s] 40%|████      | 2/5 [00:00<00:00,  4.31ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.36ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.39ba/s]100%|██████████| 5/5 [00:00<00:00,  5.35ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  9.26ba/s] 50%|█████     | 3/6 [00:00<00:00, 10.52ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 10.71ba/s]100%|██████████| 6/6 [00:00<00:00, 12.12ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.34ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.38ba/s]100%|██████████| 5/5 [00:00<00:00, 12.75ba/s]
[INFO|trainer.py:414] 2023-08-28 05:12:04,242 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 05:12:04,257 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 05:12:04,257 >>   Num examples = 5239
[INFO|trainer.py:1149] 2023-08-28 05:12:04,257 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 05:12:04,257 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 05:12:04,257 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 05:12:04,257 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 05:12:04,257 >>   Total optimization steps = 410
  0%|          | 0/410 [00:00<?, ?it/s]  0%|          | 1/410 [00:00<01:56,  3.50it/s]  0%|          | 2/410 [00:00<01:54,  3.56it/s]  1%|          | 3/410 [00:00<01:53,  3.58it/s]  1%|          | 4/410 [00:01<01:52,  3.60it/s]  1%|          | 5/410 [00:01<01:52,  3.61it/s]  1%|▏         | 6/410 [00:01<01:51,  3.61it/s]  2%|▏         | 7/410 [00:01<01:51,  3.61it/s]  2%|▏         | 8/410 [00:02<01:51,  3.61it/s]  2%|▏         | 9/410 [00:02<01:51,  3.61it/s]  2%|▏         | 10/410 [00:02<01:51,  3.58it/s]  3%|▎         | 11/410 [00:03<01:51,  3.59it/s]  3%|▎         | 12/410 [00:03<01:50,  3.60it/s]  3%|▎         | 13/410 [00:03<01:50,  3.60it/s]  3%|▎         | 14/410 [00:03<01:50,  3.59it/s]  4%|▎         | 15/410 [00:04<01:50,  3.59it/s]  4%|▍         | 16/410 [00:04<01:50,  3.58it/s]  4%|▍         | 17/410 [00:04<01:49,  3.58it/s]  4%|▍         | 18/410 [00:05<01:49,  3.57it/s]  5%|▍         | 19/410 [00:05<01:49,  3.57it/s]  5%|▍         | 20/410 [00:05<01:49,  3.56it/s]  5%|▌         | 21/410 [00:05<01:49,  3.55it/s]  5%|▌         | 22/410 [00:06<01:49,  3.55it/s]  6%|▌         | 23/410 [00:06<01:48,  3.55it/s]  6%|▌         | 24/410 [00:06<01:48,  3.56it/s]  6%|▌         | 25/410 [00:06<01:48,  3.55it/s]  6%|▋         | 26/410 [00:07<01:47,  3.56it/s]  7%|▋         | 27/410 [00:07<01:47,  3.56it/s]  7%|▋         | 28/410 [00:07<01:47,  3.56it/s]  7%|▋         | 29/410 [00:08<01:46,  3.56it/s]  7%|▋         | 30/410 [00:08<01:46,  3.56it/s]  8%|▊         | 31/410 [00:08<01:46,  3.56it/s]  8%|▊         | 32/410 [00:08<01:46,  3.55it/s]  8%|▊         | 33/410 [00:09<01:45,  3.56it/s]  8%|▊         | 34/410 [00:09<01:45,  3.56it/s]  9%|▊         | 35/410 [00:09<01:45,  3.56it/s]  9%|▉         | 36/410 [00:10<01:45,  3.56it/s]  9%|▉         | 37/410 [00:10<01:44,  3.56it/s]  9%|▉         | 38/410 [00:10<01:44,  3.55it/s] 10%|▉         | 39/410 [00:10<01:44,  3.55it/s] 10%|▉         | 40/410 [00:11<01:44,  3.55it/s] 10%|█         | 41/410 [00:11<01:43,  3.55it/s] 10%|█         | 42/410 [00:11<01:43,  3.55it/s] 10%|█         | 43/410 [00:12<01:44,  3.51it/s] 11%|█         | 44/410 [00:12<01:43,  3.52it/s] 11%|█         | 45/410 [00:12<01:43,  3.53it/s] 11%|█         | 46/410 [00:12<01:42,  3.54it/s] 11%|█▏        | 47/410 [00:13<01:42,  3.54it/s] 12%|█▏        | 48/410 [00:13<01:41,  3.55it/s] 12%|█▏        | 49/410 [00:13<01:41,  3.55it/s] 12%|█▏        | 50/410 [00:14<01:41,  3.56it/s] 12%|█▏        | 51/410 [00:14<01:40,  3.56it/s] 13%|█▎        | 52/410 [00:14<01:40,  3.56it/s] 13%|█▎        | 53/410 [00:14<01:40,  3.56it/s] 13%|█▎        | 54/410 [00:15<01:40,  3.55it/s] 13%|█▎        | 55/410 [00:15<01:39,  3.55it/s] 14%|█▎        | 56/410 [00:15<01:39,  3.55it/s] 14%|█▍        | 57/410 [00:15<01:39,  3.55it/s] 14%|█▍        | 58/410 [00:16<01:39,  3.55it/s] 14%|█▍        | 59/410 [00:16<01:38,  3.55it/s] 15%|█▍        | 60/410 [00:16<01:38,  3.55it/s] 15%|█▍        | 61/410 [00:17<01:38,  3.55it/s] 15%|█▌        | 62/410 [00:17<01:38,  3.55it/s] 15%|█▌        | 63/410 [00:17<01:37,  3.55it/s] 16%|█▌        | 64/410 [00:17<01:37,  3.55it/s] 16%|█▌        | 65/410 [00:18<01:37,  3.54it/s] 16%|█▌        | 66/410 [00:18<01:37,  3.55it/s] 16%|█▋        | 67/410 [00:18<01:36,  3.55it/s] 17%|█▋        | 68/410 [00:19<01:36,  3.55it/s] 17%|█▋        | 69/410 [00:19<01:36,  3.55it/s] 17%|█▋        | 70/410 [00:19<01:35,  3.55it/s] 17%|█▋        | 71/410 [00:19<01:35,  3.55it/s] 18%|█▊        | 72/410 [00:20<01:35,  3.55it/s] 18%|█▊        | 73/410 [00:20<01:34,  3.56it/s] 18%|█▊        | 74/410 [00:20<01:34,  3.56it/s] 18%|█▊        | 75/410 [00:21<01:34,  3.56it/s] 19%|█▊        | 76/410 [00:21<01:34,  3.55it/s] 19%|█▉        | 77/410 [00:21<01:33,  3.55it/s] 19%|█▉        | 78/410 [00:21<01:33,  3.55it/s] 19%|█▉        | 79/410 [00:22<01:33,  3.55it/s] 20%|█▉        | 80/410 [00:22<01:33,  3.55it/s] 20%|█▉        | 81/410 [00:22<01:32,  3.55it/s] 20%|██        | 82/410 [00:23<01:28,  3.70it/s][INFO|trainer.py:2140] 2023-08-28 05:12:27,264 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:12:27,265 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 05:12:27,265 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.98it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.54it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.82it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.64it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.94it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.51it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.21it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.16it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.20it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.32it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.42it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.43it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.55it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.39it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.24it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.09it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.00it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.12it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.28it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.38it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.48it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.44it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.35it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 43.98it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.02it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.95it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.09it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.24it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.35it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.48it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.45it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.36it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.17it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.97it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 43.96it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.03it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.11it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.32it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.45it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.46it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.40it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.18it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.06it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.01it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.13it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.22it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.40it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.36it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.45it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.28it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.15it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.03it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.04it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.10it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.26it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.33it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.41it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.39it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.25it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.19it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.03it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.03it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.13it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.30it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.36it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.34it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.30it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.23it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.19it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.05it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.08it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.18it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.39it/s][A
 74%|███████▎  | 372/505 [00:08<00:02, 44.38it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.35it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.25it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.22it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.06it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.12it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.08it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.20it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.18it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.31it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.26it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.27it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.21it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.12it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.12it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.11it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.18it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.26it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.27it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.27it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.27it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.20it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.23it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.09it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.10it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.15it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.27it/s][A                                                
                                                 [A 20%|██        | 82/410 [00:34<01:28,  3.70it/s]
100%|██████████| 505/505 [00:11<00:00, 44.27it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:12:38,701 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-82
[INFO|configuration_utils.py:351] 2023-08-28 05:12:38,725 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-82/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:12:40,630 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-82/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:12:40,646 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-82/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:12:40,655 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-82/special_tokens_map.json
 20%|██        | 83/410 [00:36<23:53,  4.38s/it] 20%|██        | 84/410 [00:37<17:07,  3.15s/it] 21%|██        | 85/410 [00:37<12:24,  2.29s/it] 21%|██        | 86/410 [00:37<09:07,  1.69s/it] 21%|██        | 87/410 [00:38<06:49,  1.27s/it] 21%|██▏       | 88/410 [00:38<05:12,  1.03it/s] 22%|██▏       | 89/410 [00:38<04:05,  1.31it/s] 22%|██▏       | 90/410 [00:38<03:18,  1.61it/s] 22%|██▏       | 91/410 [00:39<02:45,  1.93it/s] 22%|██▏       | 92/410 [00:39<02:22,  2.24it/s] 23%|██▎       | 93/410 [00:39<02:06,  2.51it/s] 23%|██▎       | 94/410 [00:40<01:54,  2.75it/s] 23%|██▎       | 95/410 [00:40<01:46,  2.95it/s] 23%|██▎       | 96/410 [00:40<01:40,  3.11it/s] 24%|██▎       | 97/410 [00:40<01:36,  3.23it/s] 24%|██▍       | 98/410 [00:41<01:33,  3.32it/s] 24%|██▍       | 99/410 [00:41<01:31,  3.38it/s] 24%|██▍       | 100/410 [00:41<01:30,  3.43it/s] 25%|██▍       | 101/410 [00:42<01:28,  3.48it/s] 25%|██▍       | 102/410 [00:42<01:27,  3.51it/s] 25%|██▌       | 103/410 [00:42<01:26,  3.54it/s] 25%|██▌       | 104/410 [00:42<01:26,  3.54it/s] 26%|██▌       | 105/410 [00:43<01:25,  3.56it/s] 26%|██▌       | 106/410 [00:43<01:25,  3.57it/s] 26%|██▌       | 107/410 [00:43<01:24,  3.59it/s] 26%|██▋       | 108/410 [00:43<01:24,  3.59it/s] 27%|██▋       | 109/410 [00:44<01:23,  3.60it/s] 27%|██▋       | 110/410 [00:44<01:23,  3.60it/s] 27%|██▋       | 111/410 [00:44<01:23,  3.60it/s] 27%|██▋       | 112/410 [00:45<01:22,  3.61it/s] 28%|██▊       | 113/410 [00:45<01:22,  3.61it/s] 28%|██▊       | 114/410 [00:45<01:22,  3.61it/s] 28%|██▊       | 115/410 [00:45<01:22,  3.60it/s] 28%|██▊       | 116/410 [00:46<01:21,  3.60it/s] 29%|██▊       | 117/410 [00:46<01:21,  3.60it/s] 29%|██▉       | 118/410 [00:46<01:21,  3.60it/s] 29%|██▉       | 119/410 [00:47<01:20,  3.60it/s] 29%|██▉       | 120/410 [00:47<01:20,  3.60it/s] 30%|██▉       | 121/410 [00:47<01:20,  3.60it/s] 30%|██▉       | 122/410 [00:47<01:19,  3.60it/s] 30%|███       | 123/410 [00:48<01:19,  3.60it/s] 30%|███       | 124/410 [00:48<01:19,  3.61it/s] 30%|███       | 125/410 [00:48<01:19,  3.61it/s] 31%|███       | 126/410 [00:48<01:19,  3.59it/s] 31%|███       | 127/410 [00:49<01:18,  3.60it/s] 31%|███       | 128/410 [00:49<01:18,  3.60it/s] 31%|███▏      | 129/410 [00:49<01:18,  3.60it/s] 32%|███▏      | 130/410 [00:50<01:17,  3.60it/s] 32%|███▏      | 131/410 [00:50<01:17,  3.60it/s] 32%|███▏      | 132/410 [00:50<01:17,  3.60it/s] 32%|███▏      | 133/410 [00:50<01:16,  3.60it/s] 33%|███▎      | 134/410 [00:51<01:16,  3.59it/s] 33%|███▎      | 135/410 [00:51<01:16,  3.59it/s] 33%|███▎      | 136/410 [00:51<01:16,  3.60it/s] 33%|███▎      | 137/410 [00:52<01:18,  3.50it/s] 34%|███▎      | 138/410 [00:52<01:17,  3.53it/s] 34%|███▍      | 139/410 [00:52<01:16,  3.55it/s] 34%|███▍      | 140/410 [00:52<01:15,  3.57it/s] 34%|███▍      | 141/410 [00:53<01:15,  3.58it/s] 35%|███▍      | 142/410 [00:53<01:14,  3.58it/s] 35%|███▍      | 143/410 [00:53<01:14,  3.59it/s] 35%|███▌      | 144/410 [00:54<01:14,  3.59it/s] 35%|███▌      | 145/410 [00:54<01:13,  3.60it/s] 36%|███▌      | 146/410 [00:54<01:13,  3.60it/s] 36%|███▌      | 147/410 [00:54<01:13,  3.60it/s] 36%|███▌      | 148/410 [00:55<01:12,  3.60it/s] 36%|███▋      | 149/410 [00:55<01:12,  3.60it/s] 37%|███▋      | 150/410 [00:55<01:12,  3.60it/s] 37%|███▋      | 151/410 [00:55<01:11,  3.60it/s] 37%|███▋      | 152/410 [00:56<01:11,  3.60it/s] 37%|███▋      | 153/410 [00:56<01:11,  3.61it/s] 38%|███▊      | 154/410 [00:56<01:11,  3.59it/s] 38%|███▊      | 155/410 [00:57<01:10,  3.60it/s] 38%|███▊      | 156/410 [00:57<01:10,  3.60it/s] 38%|███▊      | 157/410 [00:57<01:10,  3.60it/s] 39%|███▊      | 158/410 [00:57<01:10,  3.60it/s] 39%|███▉      | 159/410 [00:58<01:09,  3.60it/s] 39%|███▉      | 160/410 [00:58<01:09,  3.60it/s] 39%|███▉      | 161/410 [00:58<01:09,  3.60it/s] 40%|███▉      | 162/410 [00:59<01:08,  3.60it/s] 40%|███▉      | 163/410 [00:59<01:08,  3.60it/s] 40%|████      | 164/410 [00:59<01:05,  3.75it/s][INFO|trainer.py:2140] 2023-08-28 05:13:03,805 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:13:03,805 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 05:13:03,805 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4201, 'eval_samples_per_second': 353.675, 'eval_steps_per_second': 44.22, 'epoch': 1.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.86it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.49it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.57it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.36it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.79it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.45it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.21it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.14it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.24it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.40it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.49it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.32it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.24it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.10it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.00it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.92it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.01it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 43.99it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.21it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.34it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.31it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.24it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.12it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 43.98it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.02it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.99it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.18it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.15it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.26it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.32it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.17it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.11it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 43.94it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.98it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.08it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.17it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.16it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.25it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.31it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.21it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.04it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 43.96it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 43.96it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 43.97it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.14it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.13it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.18it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.24it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.26it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.09it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.00it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 43.85it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.06it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.11it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.08it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.23it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.19it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.18it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.03it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.05it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.05it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.12it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.11it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.14it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.16it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.21it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.11it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.05it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.07it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.05it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.04it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.13it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.12it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.19it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.12it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.07it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 43.98it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.07it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.00it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.07it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.09it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.18it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.11it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.18it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.06it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 43.99it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.01it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.10it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.05it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.18it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.27it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.18it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.22it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 43.99it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 43.98it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 43.98it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.12it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.09it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.15it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.27it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.27it/s][A 40%|████      | 164/410 [01:10<01:05,  3.75it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:13:15,277 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 05:13:15,299 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:13:16,721 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:13:16,735 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:13:16,745 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-164/special_tokens_map.json
 40%|████      | 165/410 [01:13<17:20,  4.25s/it] 40%|████      | 166/410 [01:13<12:25,  3.06s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/410 [01:13<09:00,  2.22s/it] 41%|████      | 168/410 [01:13<06:37,  1.64s/it] 41%|████      | 169/410 [01:14<04:57,  1.23s/it] 41%|████▏     | 170/410 [01:14<03:47,  1.05it/s] 42%|████▏     | 171/410 [01:14<02:58,  1.34it/s] 42%|████▏     | 172/410 [01:15<02:24,  1.64it/s] 42%|████▏     | 173/410 [01:15<02:00,  1.96it/s] 42%|████▏     | 174/410 [01:15<01:44,  2.27it/s] 43%|████▎     | 175/410 [01:15<01:32,  2.54it/s] 43%|████▎     | 176/410 [01:16<01:24,  2.77it/s] 43%|████▎     | 177/410 [01:16<01:18,  2.97it/s] 43%|████▎     | 178/410 [01:16<01:14,  3.12it/s] 44%|████▎     | 179/410 [01:17<01:11,  3.24it/s] 44%|████▍     | 180/410 [01:17<01:09,  3.32it/s] 44%|████▍     | 181/410 [01:17<01:07,  3.38it/s] 44%|████▍     | 182/410 [01:17<01:06,  3.43it/s] 45%|████▍     | 183/410 [01:18<01:05,  3.47it/s] 45%|████▍     | 184/410 [01:18<01:04,  3.49it/s] 45%|████▌     | 185/410 [01:18<01:04,  3.51it/s] 45%|████▌     | 186/410 [01:18<01:03,  3.51it/s] 46%|████▌     | 187/410 [01:19<01:03,  3.52it/s] 46%|████▌     | 188/410 [01:19<01:02,  3.52it/s] 46%|████▌     | 189/410 [01:19<01:02,  3.53it/s] 46%|████▋     | 190/410 [01:20<01:02,  3.54it/s] 47%|████▋     | 191/410 [01:20<01:01,  3.54it/s] 47%|████▋     | 192/410 [01:20<01:01,  3.54it/s] 47%|████▋     | 193/410 [01:20<01:01,  3.55it/s] 47%|████▋     | 194/410 [01:21<01:00,  3.55it/s] 48%|████▊     | 195/410 [01:21<01:00,  3.55it/s] 48%|████▊     | 196/410 [01:21<01:00,  3.56it/s] 48%|████▊     | 197/410 [01:22<00:59,  3.56it/s] 48%|████▊     | 198/410 [01:22<00:59,  3.58it/s] 49%|████▊     | 199/410 [01:22<00:58,  3.59it/s] 49%|████▉     | 200/410 [01:22<00:58,  3.59it/s] 49%|████▉     | 201/410 [01:23<00:58,  3.59it/s] 49%|████▉     | 202/410 [01:23<00:57,  3.59it/s] 50%|████▉     | 203/410 [01:23<00:57,  3.60it/s] 50%|████▉     | 204/410 [01:24<00:57,  3.60it/s] 50%|█████     | 205/410 [01:24<00:56,  3.60it/s] 50%|█████     | 206/410 [01:24<00:56,  3.60it/s] 50%|█████     | 207/410 [01:24<00:56,  3.60it/s] 51%|█████     | 208/410 [01:25<00:56,  3.60it/s] 51%|█████     | 209/410 [01:25<00:55,  3.60it/s] 51%|█████     | 210/410 [01:25<00:55,  3.60it/s] 51%|█████▏    | 211/410 [01:25<00:55,  3.60it/s] 52%|█████▏    | 212/410 [01:26<00:55,  3.60it/s] 52%|█████▏    | 213/410 [01:26<00:54,  3.60it/s] 52%|█████▏    | 214/410 [01:26<00:54,  3.60it/s] 52%|█████▏    | 215/410 [01:27<00:54,  3.60it/s] 53%|█████▎    | 216/410 [01:27<00:54,  3.59it/s] 53%|█████▎    | 217/410 [01:27<00:53,  3.60it/s] 53%|█████▎    | 218/410 [01:27<00:53,  3.60it/s] 53%|█████▎    | 219/410 [01:28<00:53,  3.60it/s] 54%|█████▎    | 220/410 [01:28<00:52,  3.60it/s] 54%|█████▍    | 221/410 [01:28<00:52,  3.60it/s] 54%|█████▍    | 222/410 [01:29<00:52,  3.60it/s] 54%|█████▍    | 223/410 [01:29<00:52,  3.59it/s] 55%|█████▍    | 224/410 [01:29<00:51,  3.59it/s] 55%|█████▍    | 225/410 [01:29<00:51,  3.60it/s] 55%|█████▌    | 226/410 [01:30<00:51,  3.60it/s] 55%|█████▌    | 227/410 [01:30<00:51,  3.58it/s] 56%|█████▌    | 228/410 [01:30<00:50,  3.59it/s] 56%|█████▌    | 229/410 [01:30<00:50,  3.60it/s] 56%|█████▌    | 230/410 [01:31<00:50,  3.60it/s] 56%|█████▋    | 231/410 [01:31<00:49,  3.60it/s] 57%|█████▋    | 232/410 [01:31<00:49,  3.60it/s] 57%|█████▋    | 233/410 [01:32<00:49,  3.60it/s] 57%|█████▋    | 234/410 [01:32<00:48,  3.60it/s] 57%|█████▋    | 235/410 [01:32<00:48,  3.60it/s] 58%|█████▊    | 236/410 [01:32<00:48,  3.60it/s] 58%|█████▊    | 237/410 [01:33<00:48,  3.60it/s] 58%|█████▊    | 238/410 [01:33<00:48,  3.57it/s] 58%|█████▊    | 239/410 [01:33<00:47,  3.58it/s] 59%|█████▊    | 240/410 [01:34<00:47,  3.59it/s] 59%|█████▉    | 241/410 [01:34<00:46,  3.60it/s] 59%|█████▉    | 242/410 [01:34<00:46,  3.60it/s] 59%|█████▉    | 243/410 [01:34<00:46,  3.60it/s] 60%|█████▉    | 244/410 [01:35<00:46,  3.60it/s] 60%|█████▉    | 245/410 [01:35<00:45,  3.60it/s] 60%|██████    | 246/410 [01:35<00:43,  3.74it/s][INFO|trainer.py:2140] 2023-08-28 05:13:39,936 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:13:39,936 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 05:13:39,936 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4464, 'eval_samples_per_second': 352.864, 'eval_steps_per_second': 44.119, 'epoch': 2.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.55it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.38it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.56it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.38it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.73it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.27it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.09it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.04it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.18it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.28it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.36it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.39it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.27it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.10it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.96it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.88it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.91it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.01it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.18it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.17it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.25it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.17it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 43.98it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 43.92it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.89it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.91it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.07it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.17it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.26it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.13it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.11it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.02it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 43.97it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.88it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.00it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.10it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.18it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.28it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.08it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.05it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.02it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 43.94it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 43.95it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.02it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.11it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.18it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.20it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.05it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.04it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 43.98it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.03it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.01it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.09it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.04it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.15it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.14it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.05it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 43.97it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 43.94it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 43.96it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.08it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.13it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.05it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.23it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.12it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.12it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.01it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.03it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 43.94it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.01it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.05it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.08it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.20it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.16it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.02it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.02it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.02it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 43.93it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.01it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.01it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.15it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.11it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.15it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.13it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.10it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.02it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 43.97it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 44.00it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.01it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.14it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.10it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.16it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.10it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.05it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 43.94it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.05it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 43.88it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.00it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.13it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.09it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.09it/s][A 60%|██████    | 246/410 [01:47<00:43,  3.74it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:13:51,418 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 05:13:51,435 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:13:53,579 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:13:53,598 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:13:53,606 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-246/special_tokens_map.json
 60%|██████    | 247/410 [01:49<12:06,  4.45s/it] 60%|██████    | 248/410 [01:50<08:38,  3.20s/it] 61%|██████    | 249/410 [01:50<06:14,  2.33s/it] 61%|██████    | 250/410 [01:50<04:33,  1.71s/it] 61%|██████    | 251/410 [01:51<03:23,  1.28s/it] 61%|██████▏   | 252/410 [01:51<02:35,  1.02it/s] 62%|██████▏   | 253/410 [01:51<02:01,  1.29it/s] 62%|██████▏   | 254/410 [01:51<01:37,  1.60it/s] 62%|██████▏   | 255/410 [01:52<01:22,  1.88it/s] 62%|██████▏   | 256/410 [01:52<01:10,  2.18it/s] 63%|██████▎   | 257/410 [01:52<01:02,  2.46it/s] 63%|██████▎   | 258/410 [01:53<00:56,  2.71it/s] 63%|██████▎   | 259/410 [01:53<00:51,  2.92it/s] 63%|██████▎   | 260/410 [01:53<00:48,  3.08it/s] 64%|██████▎   | 261/410 [01:53<00:46,  3.21it/s] 64%|██████▍   | 262/410 [01:54<00:44,  3.30it/s] 64%|██████▍   | 263/410 [01:54<00:43,  3.37it/s] 64%|██████▍   | 264/410 [01:54<00:42,  3.42it/s] 65%|██████▍   | 265/410 [01:55<00:42,  3.45it/s] 65%|██████▍   | 266/410 [01:55<00:41,  3.48it/s] 65%|██████▌   | 267/410 [01:55<00:40,  3.50it/s] 65%|██████▌   | 268/410 [01:55<00:40,  3.52it/s] 66%|██████▌   | 269/410 [01:56<00:39,  3.53it/s] 66%|██████▌   | 270/410 [01:56<00:39,  3.54it/s] 66%|██████▌   | 271/410 [01:56<00:39,  3.56it/s] 66%|██████▋   | 272/410 [01:56<00:38,  3.58it/s] 67%|██████▋   | 273/410 [01:57<00:38,  3.58it/s] 67%|██████▋   | 274/410 [01:57<00:37,  3.59it/s] 67%|██████▋   | 275/410 [01:57<00:37,  3.59it/s] 67%|██████▋   | 276/410 [01:58<00:37,  3.57it/s] 68%|██████▊   | 277/410 [01:58<00:37,  3.58it/s] 68%|██████▊   | 278/410 [01:58<00:36,  3.58it/s] 68%|██████▊   | 279/410 [01:58<00:36,  3.58it/s] 68%|██████▊   | 280/410 [01:59<00:36,  3.59it/s] 69%|██████▊   | 281/410 [01:59<00:35,  3.59it/s] 69%|██████▉   | 282/410 [01:59<00:35,  3.59it/s] 69%|██████▉   | 283/410 [02:00<00:35,  3.59it/s] 69%|██████▉   | 284/410 [02:00<00:35,  3.60it/s] 70%|██████▉   | 285/410 [02:00<00:34,  3.60it/s] 70%|██████▉   | 286/410 [02:00<00:34,  3.60it/s] 70%|███████   | 287/410 [02:01<00:34,  3.58it/s] 70%|███████   | 288/410 [02:01<00:34,  3.59it/s] 70%|███████   | 289/410 [02:01<00:33,  3.59it/s] 71%|███████   | 290/410 [02:01<00:33,  3.59it/s] 71%|███████   | 291/410 [02:02<00:33,  3.59it/s] 71%|███████   | 292/410 [02:02<00:32,  3.59it/s] 71%|███████▏  | 293/410 [02:02<00:32,  3.59it/s] 72%|███████▏  | 294/410 [02:03<00:32,  3.59it/s] 72%|███████▏  | 295/410 [02:03<00:31,  3.59it/s] 72%|███████▏  | 296/410 [02:03<00:31,  3.60it/s] 72%|███████▏  | 297/410 [02:03<00:31,  3.60it/s] 73%|███████▎  | 298/410 [02:04<00:31,  3.59it/s] 73%|███████▎  | 299/410 [02:04<00:30,  3.60it/s] 73%|███████▎  | 300/410 [02:04<00:30,  3.60it/s] 73%|███████▎  | 301/410 [02:05<00:30,  3.60it/s] 74%|███████▎  | 302/410 [02:05<00:30,  3.60it/s] 74%|███████▍  | 303/410 [02:05<00:29,  3.60it/s] 74%|███████▍  | 304/410 [02:05<00:29,  3.60it/s] 74%|███████▍  | 305/410 [02:06<00:29,  3.60it/s] 75%|███████▍  | 306/410 [02:06<00:28,  3.60it/s] 75%|███████▍  | 307/410 [02:06<00:28,  3.61it/s] 75%|███████▌  | 308/410 [02:06<00:28,  3.61it/s] 75%|███████▌  | 309/410 [02:07<00:28,  3.56it/s] 76%|███████▌  | 310/410 [02:07<00:28,  3.57it/s] 76%|███████▌  | 311/410 [02:07<00:27,  3.58it/s] 76%|███████▌  | 312/410 [02:08<00:27,  3.59it/s] 76%|███████▋  | 313/410 [02:08<00:26,  3.60it/s] 77%|███████▋  | 314/410 [02:08<00:26,  3.60it/s] 77%|███████▋  | 315/410 [02:08<00:26,  3.60it/s] 77%|███████▋  | 316/410 [02:09<00:26,  3.60it/s] 77%|███████▋  | 317/410 [02:09<00:25,  3.60it/s] 78%|███████▊  | 318/410 [02:09<00:25,  3.60it/s] 78%|███████▊  | 319/410 [02:10<00:25,  3.60it/s] 78%|███████▊  | 320/410 [02:10<00:25,  3.59it/s] 78%|███████▊  | 321/410 [02:10<00:24,  3.59it/s] 79%|███████▊  | 322/410 [02:10<00:24,  3.60it/s] 79%|███████▉  | 323/410 [02:11<00:24,  3.60it/s] 79%|███████▉  | 324/410 [02:11<00:23,  3.60it/s] 79%|███████▉  | 325/410 [02:11<00:23,  3.60it/s] 80%|███████▉  | 326/410 [02:12<00:23,  3.60it/s] 80%|███████▉  | 327/410 [02:12<00:23,  3.60it/s] 80%|████████  | 328/410 [02:12<00:21,  3.75it/s][INFO|trainer.py:2140] 2023-08-28 05:14:16,782 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:14:16,782 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 05:14:16,782 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4599, 'eval_samples_per_second': 352.446, 'eval_steps_per_second': 44.067, 'epoch': 3.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.98it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.46it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.38it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.31it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.82it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.46it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.20it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.07it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.26it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.41it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.42it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.18it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.01it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 43.97it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.91it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.95it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.90it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.12it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.33it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.34it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.20it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.04it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.02it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 43.99it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.93it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.02it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.24it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.21it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.32it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.21it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.06it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 43.98it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 43.99it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.99it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.04it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.20it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.29it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.25it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.09it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.02it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.00it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 43.94it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 43.98it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.05it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.03it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.29it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.29it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.20it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.11it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 43.99it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.07it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.05it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.08it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.24it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.29it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.18it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.08it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.07it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.01it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 43.92it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.01it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.06it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.14it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.26it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.09it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.11it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.10it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.03it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.03it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.01it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.11it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.13it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.19it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.08it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.17it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.06it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.10it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.13it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.08it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.07it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.13it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.14it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.18it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.08it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.08it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.13it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.10it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.05it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.05it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.07it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.18it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.15it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.15it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.10it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.06it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.12it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.11it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 43.96it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.06it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.18it/s][A                                                 
                                                 [A 80%|████████  | 328/410 [02:23<00:21,  3.75it/s]
100%|██████████| 505/505 [00:11<00:00, 44.18it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:14:28,249 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 05:14:28,270 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:14:30,329 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:14:30,356 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:14:30,365 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-328/special_tokens_map.json
 80%|████████  | 329/410 [02:26<05:58,  4.43s/it] 80%|████████  | 330/410 [02:26<04:14,  3.18s/it] 81%|████████  | 331/410 [02:27<03:02,  2.31s/it] 81%|████████  | 332/410 [02:27<02:12,  1.70s/it] 81%|████████  | 333/410 [02:27<01:38,  1.28s/it] 81%|████████▏ | 334/410 [02:28<01:14,  1.02it/s] 82%|████████▏ | 335/410 [02:28<00:57,  1.30it/s] 82%|████████▏ | 336/410 [02:28<00:46,  1.61it/s] 82%|████████▏ | 337/410 [02:28<00:38,  1.92it/s] 82%|████████▏ | 338/410 [02:29<00:32,  2.23it/s] 83%|████████▎ | 339/410 [02:29<00:28,  2.51it/s] 83%|████████▎ | 340/410 [02:29<00:25,  2.75it/s] 83%|████████▎ | 341/410 [02:30<00:23,  2.95it/s] 83%|████████▎ | 342/410 [02:30<00:21,  3.10it/s] 84%|████████▎ | 343/410 [02:30<00:20,  3.23it/s] 84%|████████▍ | 344/410 [02:30<00:19,  3.32it/s] 84%|████████▍ | 345/410 [02:31<00:19,  3.39it/s] 84%|████████▍ | 346/410 [02:31<00:18,  3.44it/s] 85%|████████▍ | 347/410 [02:31<00:18,  3.47it/s] 85%|████████▍ | 348/410 [02:32<00:17,  3.48it/s] 85%|████████▌ | 349/410 [02:32<00:17,  3.50it/s] 85%|████████▌ | 350/410 [02:32<00:17,  3.51it/s] 86%|████████▌ | 351/410 [02:32<00:16,  3.52it/s] 86%|████████▌ | 352/410 [02:33<00:16,  3.53it/s] 86%|████████▌ | 353/410 [02:33<00:16,  3.53it/s] 86%|████████▋ | 354/410 [02:33<00:15,  3.54it/s] 87%|████████▋ | 355/410 [02:33<00:15,  3.54it/s] 87%|████████▋ | 356/410 [02:34<00:15,  3.55it/s] 87%|████████▋ | 357/410 [02:34<00:14,  3.55it/s] 87%|████████▋ | 358/410 [02:34<00:14,  3.55it/s] 88%|████████▊ | 359/410 [02:35<00:14,  3.53it/s] 88%|████████▊ | 360/410 [02:35<00:14,  3.54it/s] 88%|████████▊ | 361/410 [02:35<00:13,  3.54it/s] 88%|████████▊ | 362/410 [02:35<00:13,  3.55it/s] 89%|████████▊ | 363/410 [02:36<00:13,  3.55it/s] 89%|████████▉ | 364/410 [02:36<00:12,  3.55it/s] 89%|████████▉ | 365/410 [02:36<00:12,  3.56it/s] 89%|████████▉ | 366/410 [02:37<00:12,  3.55it/s] 90%|████████▉ | 367/410 [02:37<00:12,  3.55it/s] 90%|████████▉ | 368/410 [02:37<00:11,  3.55it/s] 90%|█████████ | 369/410 [02:37<00:11,  3.55it/s] 90%|█████████ | 370/410 [02:38<00:11,  3.54it/s] 90%|█████████ | 371/410 [02:38<00:11,  3.54it/s] 91%|█████████ | 372/410 [02:38<00:10,  3.55it/s] 91%|█████████ | 373/410 [02:39<00:10,  3.55it/s] 91%|█████████ | 374/410 [02:39<00:10,  3.55it/s] 91%|█████████▏| 375/410 [02:39<00:09,  3.55it/s] 92%|█████████▏| 376/410 [02:39<00:09,  3.56it/s] 92%|█████████▏| 377/410 [02:40<00:09,  3.55it/s] 92%|█████████▏| 378/410 [02:40<00:09,  3.55it/s] 92%|█████████▏| 379/410 [02:40<00:08,  3.54it/s] 93%|█████████▎| 380/410 [02:41<00:08,  3.54it/s] 93%|█████████▎| 381/410 [02:41<00:08,  3.47it/s] 93%|█████████▎| 382/410 [02:41<00:08,  3.49it/s] 93%|█████████▎| 383/410 [02:41<00:07,  3.51it/s] 94%|█████████▎| 384/410 [02:42<00:07,  3.53it/s] 94%|█████████▍| 385/410 [02:42<00:07,  3.53it/s] 94%|█████████▍| 386/410 [02:42<00:06,  3.54it/s] 94%|█████████▍| 387/410 [02:43<00:06,  3.55it/s] 95%|█████████▍| 388/410 [02:43<00:06,  3.55it/s] 95%|█████████▍| 389/410 [02:43<00:05,  3.54it/s] 95%|█████████▌| 390/410 [02:43<00:05,  3.55it/s] 95%|█████████▌| 391/410 [02:44<00:05,  3.55it/s] 96%|█████████▌| 392/410 [02:44<00:05,  3.52it/s] 96%|█████████▌| 393/410 [02:44<00:04,  3.54it/s] 96%|█████████▌| 394/410 [02:44<00:04,  3.56it/s] 96%|█████████▋| 395/410 [02:45<00:04,  3.57it/s] 97%|█████████▋| 396/410 [02:45<00:03,  3.58it/s] 97%|█████████▋| 397/410 [02:45<00:03,  3.59it/s] 97%|█████████▋| 398/410 [02:46<00:03,  3.59it/s] 97%|█████████▋| 399/410 [02:46<00:03,  3.59it/s] 98%|█████████▊| 400/410 [02:46<00:02,  3.60it/s] 98%|█████████▊| 401/410 [02:46<00:02,  3.60it/s] 98%|█████████▊| 402/410 [02:47<00:02,  3.60it/s] 98%|█████████▊| 403/410 [02:47<00:01,  3.59it/s] 99%|█████████▊| 404/410 [02:47<00:01,  3.59it/s] 99%|█████████▉| 405/410 [02:48<00:01,  3.59it/s] 99%|█████████▉| 406/410 [02:48<00:01,  3.59it/s] 99%|█████████▉| 407/410 [02:48<00:00,  3.59it/s]100%|█████████▉| 408/410 [02:48<00:00,  3.59it/s]100%|█████████▉| 409/410 [02:49<00:00,  3.60it/s]100%|██████████| 410/410 [02:49<00:00,  3.75it/s][INFO|trainer.py:2140] 2023-08-28 05:14:53,669 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:14:53,669 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 05:14:53,669 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.45, 'eval_samples_per_second': 352.751, 'eval_steps_per_second': 44.105, 'epoch': 4.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.01it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.58it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.40it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.31it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.79it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.43it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.07it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.00it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.18it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.39it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.50it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.27it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.16it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.11it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.99it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.80it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.85it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.07it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.24it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.35it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.24it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.10it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.03it/s][A
 24%|██▍       | 122/505 [00:02<00:09, 41.33it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.47it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.77it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 43.95it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.19it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.24it/s][A
 30%|███       | 152/505 [00:03<00:08, 44.09it/s][A
 31%|███       | 157/505 [00:03<00:07, 43.90it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 43.92it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 43.72it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.84it/s][A
 35%|███▌      | 177/505 [00:04<00:07, 43.95it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.07it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.33it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.27it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.23it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.02it/s][A
 41%|████      | 207/505 [00:04<00:06, 43.88it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 43.84it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 43.95it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.03it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.18it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.34it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.34it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.22it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.01it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 43.89it/s][A
 51%|█████     | 257/505 [00:05<00:05, 43.91it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 43.92it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 43.94it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.27it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.33it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.30it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.13it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.02it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 43.90it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 43.99it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.05it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.16it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.24it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.23it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.28it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.13it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.08it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 43.86it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 43.92it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.06it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.19it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.26it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.18it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.26it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.17it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.10it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 43.94it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 43.93it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.02it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.20it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.21it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.22it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.17it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.08it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 43.95it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 43.92it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 43.91it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 44.08it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.20it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.28it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.17it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.16it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.13it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.05it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.02it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.03it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.07it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.21it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.21it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.16it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.16it/s][A100%|██████████| 410/410 [03:00<00:00,  3.75it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:15:05,148 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-410
[INFO|configuration_utils.py:351] 2023-08-28 05:15:05,169 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-410/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:15:06,967 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-410/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:15:06,981 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-410/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:15:06,989 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-410/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 05:15:07,254 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 05:15:07,254 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-82 (score: 1.2323094606399536).
                                                 100%|██████████| 410/410 [03:04<00:00,  3.75it/s]100%|██████████| 410/410 [03:04<00:00,  2.22it/s]
[INFO|trainer.py:1894] 2023-08-28 05:15:09,063 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 05:15:09,074 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:15:11,357 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:15:11,373 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:15:11,388 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 05:15:11,610 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:15:11,610 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:15:11,610 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:15:11,610 >>   train_runtime            = 0:03:04.80
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:15:11,610 >>   train_samples            =       5239
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:15:11,610 >>   train_samples_per_second =    141.746
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:15:11,610 >>   train_steps_per_second   =      2.219
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4621, 'eval_samples_per_second': 352.378, 'eval_steps_per_second': 44.058, 'epoch': 5.0}
{'train_runtime': 184.8026, 'train_samples_per_second': 141.746, 'train_steps_per_second': 2.219, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 05:15:11 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 05:15:11,652 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:15:11,652 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 05:15:11,652 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:08, 56.26it/s]  2%|▏         | 12/505 [00:00<00:10, 48.72it/s]  3%|▎         | 17/505 [00:00<00:10, 47.08it/s]  4%|▍         | 22/505 [00:00<00:10, 46.31it/s]  5%|▌         | 27/505 [00:00<00:10, 45.84it/s]  6%|▋         | 32/505 [00:00<00:10, 45.51it/s]  7%|▋         | 37/505 [00:00<00:10, 45.24it/s]  8%|▊         | 42/505 [00:00<00:10, 44.52it/s]  9%|▉         | 47/505 [00:01<00:10, 44.03it/s] 10%|█         | 52/505 [00:01<00:10, 43.90it/s] 11%|█▏        | 57/505 [00:01<00:10, 43.99it/s] 12%|█▏        | 62/505 [00:01<00:09, 44.32it/s] 13%|█▎        | 67/505 [00:01<00:09, 44.52it/s] 14%|█▍        | 72/505 [00:01<00:09, 44.60it/s] 15%|█▌        | 77/505 [00:01<00:09, 44.65it/s] 16%|█▌        | 82/505 [00:01<00:09, 44.52it/s] 17%|█▋        | 87/505 [00:01<00:09, 44.21it/s] 18%|█▊        | 92/505 [00:02<00:09, 43.98it/s] 19%|█▉        | 97/505 [00:02<00:09, 43.91it/s] 20%|██        | 102/505 [00:02<00:09, 44.00it/s] 21%|██        | 107/505 [00:02<00:08, 44.22it/s] 22%|██▏       | 112/505 [00:02<00:08, 44.38it/s] 23%|██▎       | 117/505 [00:02<00:08, 44.56it/s] 24%|██▍       | 122/505 [00:02<00:08, 44.62it/s] 25%|██▌       | 127/505 [00:02<00:08, 44.46it/s] 26%|██▌       | 132/505 [00:02<00:08, 44.21it/s] 27%|██▋       | 137/505 [00:03<00:08, 43.96it/s] 28%|██▊       | 142/505 [00:03<00:08, 43.97it/s] 29%|██▉       | 147/505 [00:03<00:08, 44.05it/s] 30%|███       | 152/505 [00:03<00:07, 44.25it/s] 31%|███       | 157/505 [00:03<00:07, 44.43it/s] 32%|███▏      | 162/505 [00:03<00:07, 44.52it/s] 33%|███▎      | 167/505 [00:03<00:07, 44.47it/s] 34%|███▍      | 172/505 [00:03<00:07, 44.32it/s] 35%|███▌      | 177/505 [00:03<00:07, 44.22it/s] 36%|███▌      | 182/505 [00:04<00:07, 44.12it/s] 37%|███▋      | 187/505 [00:04<00:07, 43.99it/s] 38%|███▊      | 192/505 [00:04<00:07, 44.07it/s] 39%|███▉      | 197/505 [00:04<00:06, 44.15it/s] 40%|████      | 202/505 [00:04<00:06, 44.39it/s] 41%|████      | 207/505 [00:04<00:06, 44.48it/s] 42%|████▏     | 212/505 [00:04<00:06, 44.44it/s] 43%|████▎     | 217/505 [00:04<00:06, 44.21it/s] 44%|████▍     | 222/505 [00:04<00:06, 44.24it/s] 45%|████▍     | 227/505 [00:05<00:06, 44.08it/s] 46%|████▌     | 232/505 [00:05<00:06, 44.04it/s] 47%|████▋     | 237/505 [00:05<00:06, 44.10it/s] 48%|████▊     | 242/505 [00:05<00:05, 44.26it/s] 49%|████▉     | 247/505 [00:05<00:05, 44.37it/s] 50%|████▉     | 252/505 [00:05<00:05, 44.47it/s] 51%|█████     | 257/505 [00:05<00:05, 44.41it/s] 52%|█████▏    | 262/505 [00:05<00:05, 44.38it/s] 53%|█████▎    | 267/505 [00:05<00:05, 44.25it/s] 54%|█████▍    | 272/505 [00:06<00:05, 44.06it/s] 55%|█████▍    | 277/505 [00:06<00:05, 44.08it/s] 56%|█████▌    | 282/505 [00:06<00:05, 44.16it/s] 57%|█████▋    | 287/505 [00:06<00:04, 44.26it/s] 58%|█████▊    | 292/505 [00:06<00:04, 44.37it/s] 59%|█████▉    | 297/505 [00:06<00:04, 44.40it/s] 60%|█████▉    | 302/505 [00:06<00:04, 44.29it/s] 61%|██████    | 307/505 [00:06<00:04, 44.33it/s] 62%|██████▏   | 312/505 [00:07<00:04, 44.19it/s] 63%|██████▎   | 317/505 [00:07<00:04, 44.06it/s] 64%|██████▍   | 322/505 [00:07<00:04, 44.16it/s] 65%|██████▍   | 327/505 [00:07<00:04, 44.22it/s] 66%|██████▌   | 332/505 [00:07<00:03, 44.22it/s] 67%|██████▋   | 337/505 [00:07<00:03, 44.33it/s] 68%|██████▊   | 342/505 [00:07<00:03, 44.41it/s] 69%|██████▊   | 347/505 [00:07<00:03, 44.41it/s] 70%|██████▉   | 352/505 [00:07<00:03, 44.28it/s] 71%|███████   | 357/505 [00:08<00:03, 44.15it/s] 72%|███████▏  | 362/505 [00:08<00:03, 44.14it/s] 73%|███████▎  | 367/505 [00:08<00:03, 44.19it/s] 74%|███████▎  | 372/505 [00:08<00:03, 44.21it/s] 75%|███████▍  | 377/505 [00:08<00:02, 44.25it/s] 76%|███████▌  | 382/505 [00:08<00:02, 44.36it/s] 77%|███████▋  | 387/505 [00:08<00:02, 44.42it/s] 78%|███████▊  | 392/505 [00:08<00:02, 44.32it/s] 79%|███████▊  | 397/505 [00:08<00:02, 44.22it/s] 80%|███████▉  | 402/505 [00:09<00:02, 44.19it/s] 81%|████████  | 407/505 [00:09<00:02, 44.15it/s] 82%|████████▏ | 412/505 [00:09<00:02, 44.18it/s] 83%|████████▎ | 417/505 [00:09<00:01, 44.26it/s] 84%|████████▎ | 422/505 [00:09<00:01, 44.27it/s] 85%|████████▍ | 427/505 [00:09<00:01, 44.33it/s] 86%|████████▌ | 432/505 [00:09<00:01, 44.38it/s] 87%|████████▋ | 437/505 [00:09<00:01, 44.22it/s] 88%|████████▊ | 442/505 [00:09<00:01, 44.09it/s] 89%|████████▊ | 447/505 [00:10<00:01, 44.18it/s] 90%|████████▉ | 452/505 [00:10<00:01, 44.08it/s] 90%|█████████ | 457/505 [00:10<00:01, 44.18it/s] 91%|█████████▏| 462/505 [00:10<00:00, 44.29it/s] 92%|█████████▏| 467/505 [00:10<00:00, 44.30it/s] 93%|█████████▎| 472/505 [00:10<00:00, 44.27it/s] 94%|█████████▍| 477/505 [00:10<00:00, 44.20it/s] 95%|█████████▌| 482/505 [00:10<00:00, 44.17it/s] 96%|█████████▋| 487/505 [00:10<00:00, 44.18it/s] 97%|█████████▋| 492/505 [00:11<00:00, 44.19it/s] 98%|█████████▊| 497/505 [00:11<00:00, 44.23it/s] 99%|█████████▉| 502/505 [00:11<00:00, 44.16it/s]100%|██████████| 505/505 [00:11<00:00, 44.33it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 05:15:23,063 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:15:23,063 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:15:23,063 >>   eval_loss               =     1.2323
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:15:23,063 >>   eval_runtime            = 0:00:11.41
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:15:23,063 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:15:23,063 >>   eval_samples_per_second =    353.974
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:15:23,063 >>   eval_steps_per_second   =     44.258
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:15:23,063 >>   perplexity              =     3.4291
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:15:28,564 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:15:28,569 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:15:28,569 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:15:28,569 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:15:28,570 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:15:29,268 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:15:29,269 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:15:29,835 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:15:30,863 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:15:30,863 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:15:33,779 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:15:33,781 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:15:33,781 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:15:33,781 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:15:33,781 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:15:34,419 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:15:34,423 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:15:34,996 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:15:35,147 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:15:35,147 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-410
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-246
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-82
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-328
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-164
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:03,  1.70it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.69it/s]Extractor Predicting: 8it [00:04,  1.72it/s]Extractor Predicting: 9it [00:05,  1.71it/s]Extractor Predicting: 10it [00:05,  1.73it/s]Extractor Predicting: 11it [00:06,  1.72it/s]Extractor Predicting: 12it [00:07,  1.67it/s]Extractor Predicting: 13it [00:07,  1.67it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:08,  1.68it/s]Extractor Predicting: 16it [00:09,  1.66it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:10,  1.68it/s]Extractor Predicting: 19it [00:11,  1.67it/s]Extractor Predicting: 20it [00:11,  1.66it/s]Extractor Predicting: 21it [00:12,  1.68it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:13,  1.62it/s]Extractor Predicting: 24it [00:14,  1.63it/s]Extractor Predicting: 25it [00:15,  1.67it/s]Extractor Predicting: 26it [00:15,  1.67it/s]Extractor Predicting: 27it [00:16,  1.71it/s]Extractor Predicting: 28it [00:16,  1.69it/s]Extractor Predicting: 29it [00:17,  1.71it/s]Extractor Predicting: 30it [00:17,  1.67it/s]Extractor Predicting: 31it [00:18,  1.70it/s]Extractor Predicting: 32it [00:19,  1.68it/s]Extractor Predicting: 33it [00:19,  1.68it/s]Extractor Predicting: 34it [00:20,  1.67it/s]Extractor Predicting: 35it [00:20,  1.66it/s]Extractor Predicting: 36it [00:21,  1.72it/s]Extractor Predicting: 37it [00:21,  1.79it/s]Extractor Predicting: 38it [00:22,  1.76it/s]Extractor Predicting: 39it [00:23,  1.79it/s]Extractor Predicting: 40it [00:23,  1.76it/s]Extractor Predicting: 41it [00:24,  1.74it/s]Extractor Predicting: 42it [00:24,  1.73it/s]Extractor Predicting: 43it [00:25,  1.77it/s]Extractor Predicting: 44it [00:25,  1.76it/s]Extractor Predicting: 45it [00:26,  1.78it/s]Extractor Predicting: 46it [00:27,  1.73it/s]Extractor Predicting: 47it [00:27,  1.72it/s]Extractor Predicting: 48it [00:28,  1.64it/s]Extractor Predicting: 49it [00:28,  1.69it/s]Extractor Predicting: 50it [00:29,  1.72it/s]Extractor Predicting: 51it [00:30,  1.75it/s]Extractor Predicting: 52it [00:30,  1.80it/s]Extractor Predicting: 53it [00:31,  1.78it/s]Extractor Predicting: 54it [00:31,  1.79it/s]Extractor Predicting: 55it [00:32,  1.77it/s]Extractor Predicting: 56it [00:32,  1.78it/s]Extractor Predicting: 57it [00:33,  1.78it/s]Extractor Predicting: 58it [00:34,  1.72it/s]Extractor Predicting: 59it [00:34,  1.74it/s]Extractor Predicting: 60it [00:35,  1.72it/s]Extractor Predicting: 61it [00:35,  1.78it/s]Extractor Predicting: 62it [00:36,  1.76it/s]Extractor Predicting: 63it [00:36,  1.72it/s]Extractor Predicting: 64it [00:37,  1.72it/s]Extractor Predicting: 65it [00:38,  1.70it/s]Extractor Predicting: 66it [00:38,  1.73it/s]Extractor Predicting: 67it [00:39,  1.72it/s]Extractor Predicting: 68it [00:39,  1.68it/s]Extractor Predicting: 69it [00:40,  1.64it/s]Extractor Predicting: 70it [00:41,  1.68it/s]Extractor Predicting: 71it [00:41,  1.67it/s]Extractor Predicting: 72it [00:42,  1.62it/s]Extractor Predicting: 73it [00:42,  1.62it/s]Extractor Predicting: 74it [00:43,  1.56it/s]Extractor Predicting: 75it [00:44,  1.60it/s]Extractor Predicting: 76it [00:44,  1.63it/s]Extractor Predicting: 77it [00:45,  1.63it/s]Extractor Predicting: 78it [00:46,  1.64it/s]Extractor Predicting: 79it [00:46,  1.65it/s]Extractor Predicting: 80it [00:47,  1.65it/s]Extractor Predicting: 81it [00:47,  1.67it/s]Extractor Predicting: 82it [00:48,  1.72it/s]Extractor Predicting: 83it [00:48,  1.67it/s]Extractor Predicting: 84it [00:49,  1.54it/s]Extractor Predicting: 85it [00:50,  1.56it/s]Extractor Predicting: 86it [00:50,  1.60it/s]Extractor Predicting: 87it [00:51,  1.66it/s]Extractor Predicting: 88it [00:52,  1.69it/s]Extractor Predicting: 89it [00:52,  1.69it/s]Extractor Predicting: 90it [00:53,  1.70it/s]Extractor Predicting: 91it [00:53,  1.71it/s]Extractor Predicting: 92it [00:54,  1.74it/s]Extractor Predicting: 93it [00:54,  1.73it/s]Extractor Predicting: 94it [00:55,  1.70it/s]Extractor Predicting: 95it [00:56,  1.68it/s]Extractor Predicting: 96it [00:56,  1.66it/s]Extractor Predicting: 97it [00:57,  1.66it/s]Extractor Predicting: 98it [00:58,  1.66it/s]Extractor Predicting: 99it [00:58,  1.66it/s]Extractor Predicting: 100it [00:59,  1.66it/s]Extractor Predicting: 101it [00:59,  1.62it/s]Extractor Predicting: 102it [01:00,  1.62it/s]Extractor Predicting: 103it [01:01,  1.63it/s]Extractor Predicting: 104it [01:01,  1.67it/s]Extractor Predicting: 105it [01:02,  1.67it/s]Extractor Predicting: 106it [01:02,  1.70it/s]Extractor Predicting: 107it [01:03,  1.70it/s]Extractor Predicting: 108it [01:03,  1.72it/s]Extractor Predicting: 109it [01:04,  1.73it/s]Extractor Predicting: 110it [01:05,  1.70it/s]Extractor Predicting: 111it [01:05,  1.64it/s]Extractor Predicting: 112it [01:06,  1.67it/s]Extractor Predicting: 113it [01:06,  1.67it/s]Extractor Predicting: 114it [01:07,  1.64it/s]Extractor Predicting: 115it [01:08,  1.71it/s]Extractor Predicting: 116it [01:08,  1.71it/s]Extractor Predicting: 117it [01:09,  1.70it/s]Extractor Predicting: 118it [01:10,  1.64it/s]Extractor Predicting: 119it [01:10,  1.68it/s]Extractor Predicting: 120it [01:11,  1.69it/s]Extractor Predicting: 121it [01:11,  1.67it/s]Extractor Predicting: 122it [01:12,  1.68it/s]Extractor Predicting: 123it [01:12,  1.71it/s]Extractor Predicting: 124it [01:13,  1.70it/s]Extractor Predicting: 125it [01:14,  1.54it/s]Extractor Predicting: 126it [01:14,  1.61it/s]Extractor Predicting: 127it [01:15,  1.55it/s]Extractor Predicting: 128it [01:16,  1.53it/s]Extractor Predicting: 129it [01:16,  1.54it/s]Extractor Predicting: 130it [01:17,  1.55it/s]Extractor Predicting: 131it [01:18,  1.52it/s]Extractor Predicting: 132it [01:18,  1.52it/s]Extractor Predicting: 133it [01:19,  1.40it/s]Extractor Predicting: 134it [01:20,  1.48it/s]Extractor Predicting: 135it [01:20,  1.52it/s]Extractor Predicting: 136it [01:21,  1.54it/s]Extractor Predicting: 137it [01:22,  1.58it/s]Extractor Predicting: 138it [01:22,  1.54it/s]Extractor Predicting: 139it [01:23,  1.55it/s]Extractor Predicting: 140it [01:24,  1.55it/s]Extractor Predicting: 141it [01:24,  1.56it/s]Extractor Predicting: 142it [01:25,  1.58it/s]Extractor Predicting: 143it [01:25,  1.56it/s]Extractor Predicting: 144it [01:26,  1.58it/s]Extractor Predicting: 145it [01:27,  1.57it/s]Extractor Predicting: 146it [01:27,  1.58it/s]Extractor Predicting: 147it [01:28,  1.59it/s]Extractor Predicting: 148it [01:29,  1.55it/s]Extractor Predicting: 149it [01:29,  1.59it/s]Extractor Predicting: 149it [01:29,  1.66it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:12,723 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:12,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:12,728 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:12,728 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:12,728 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:17:13,346 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:17:13,347 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:17:13,952 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:17:14,962 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:17:14,962 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:17,871 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:17,877 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:17,877 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:17,877 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:17:17,877 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:17:18,504 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:17:18,505 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:17:19,061 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:17:19,227 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:17:19,227 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.72it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.68it/s]Extractor Predicting: 4it [00:02,  1.78it/s]Extractor Predicting: 5it [00:02,  1.74it/s]Extractor Predicting: 6it [00:03,  1.75it/s]Extractor Predicting: 7it [00:04,  1.74it/s]Extractor Predicting: 8it [00:04,  1.81it/s]Extractor Predicting: 9it [00:05,  1.89it/s]Extractor Predicting: 10it [00:05,  1.86it/s]Extractor Predicting: 11it [00:06,  1.87it/s]Extractor Predicting: 12it [00:06,  1.89it/s]Extractor Predicting: 13it [00:07,  1.99it/s]Extractor Predicting: 14it [00:07,  1.97it/s]Extractor Predicting: 15it [00:08,  1.99it/s]Extractor Predicting: 16it [00:08,  1.99it/s]Extractor Predicting: 17it [00:09,  1.98it/s]Extractor Predicting: 18it [00:09,  1.94it/s]Extractor Predicting: 19it [00:10,  1.89it/s]Extractor Predicting: 20it [00:10,  1.90it/s]Extractor Predicting: 21it [00:11,  1.90it/s]Extractor Predicting: 22it [00:11,  1.93it/s]Extractor Predicting: 23it [00:12,  1.96it/s]Extractor Predicting: 24it [00:12,  1.94it/s]Extractor Predicting: 25it [00:13,  1.97it/s]Extractor Predicting: 26it [00:13,  1.91it/s]Extractor Predicting: 27it [00:14,  1.98it/s]Extractor Predicting: 28it [00:14,  1.99it/s]Extractor Predicting: 29it [00:15,  1.97it/s]Extractor Predicting: 30it [00:15,  1.97it/s]Extractor Predicting: 31it [00:16,  1.99it/s]Extractor Predicting: 32it [00:16,  1.98it/s]Extractor Predicting: 33it [00:17,  1.92it/s]Extractor Predicting: 34it [00:17,  1.87it/s]Extractor Predicting: 35it [00:18,  1.85it/s]Extractor Predicting: 36it [00:18,  1.92it/s]Extractor Predicting: 37it [00:19,  1.76it/s]Extractor Predicting: 38it [00:20,  1.78it/s]Extractor Predicting: 39it [00:20,  1.81it/s]Extractor Predicting: 40it [00:21,  1.79it/s]Extractor Predicting: 41it [00:21,  1.79it/s]Extractor Predicting: 42it [00:22,  1.83it/s]Extractor Predicting: 43it [00:22,  1.87it/s]Extractor Predicting: 44it [00:23,  1.86it/s]Extractor Predicting: 45it [00:24,  1.69it/s]Extractor Predicting: 46it [00:24,  1.68it/s]Extractor Predicting: 47it [00:25,  1.73it/s]Extractor Predicting: 48it [00:25,  1.76it/s]Extractor Predicting: 49it [00:26,  1.81it/s]Extractor Predicting: 50it [00:26,  1.83it/s]Extractor Predicting: 51it [00:27,  1.86it/s]Extractor Predicting: 52it [00:27,  1.82it/s]Extractor Predicting: 53it [00:28,  1.84it/s]Extractor Predicting: 54it [00:29,  1.86it/s]Extractor Predicting: 55it [00:29,  1.84it/s]Extractor Predicting: 56it [00:30,  1.78it/s]Extractor Predicting: 57it [00:30,  1.79it/s]Extractor Predicting: 58it [00:31,  1.81it/s]Extractor Predicting: 59it [00:31,  1.78it/s]Extractor Predicting: 60it [00:32,  1.79it/s]Extractor Predicting: 61it [00:32,  1.78it/s]Extractor Predicting: 62it [00:33,  1.81it/s]Extractor Predicting: 63it [00:34,  1.78it/s]Extractor Predicting: 64it [00:34,  1.82it/s]Extractor Predicting: 65it [00:35,  1.85it/s]Extractor Predicting: 66it [00:35,  1.86it/s]Extractor Predicting: 67it [00:36,  1.84it/s]Extractor Predicting: 68it [00:36,  1.83it/s]Extractor Predicting: 69it [00:37,  1.79it/s]Extractor Predicting: 70it [00:37,  1.76it/s]Extractor Predicting: 71it [00:38,  1.76it/s]Extractor Predicting: 72it [00:39,  1.78it/s]Extractor Predicting: 73it [00:39,  1.77it/s]Extractor Predicting: 74it [00:40,  1.78it/s]Extractor Predicting: 75it [00:40,  1.81it/s]Extractor Predicting: 76it [00:41,  1.78it/s]Extractor Predicting: 77it [00:41,  1.79it/s]Extractor Predicting: 78it [00:42,  1.74it/s]Extractor Predicting: 79it [00:42,  1.78it/s]Extractor Predicting: 80it [00:43,  1.83it/s]Extractor Predicting: 81it [00:44,  1.79it/s]Extractor Predicting: 82it [00:44,  1.76it/s]Extractor Predicting: 83it [00:45,  1.76it/s]Extractor Predicting: 84it [00:45,  1.79it/s]Extractor Predicting: 85it [00:46,  1.77it/s]Extractor Predicting: 86it [00:46,  1.75it/s]Extractor Predicting: 87it [00:47,  1.80it/s]Extractor Predicting: 88it [00:48,  1.81it/s]Extractor Predicting: 89it [00:48,  1.86it/s]Extractor Predicting: 90it [00:49,  1.86it/s]Extractor Predicting: 91it [00:49,  1.85it/s]Extractor Predicting: 92it [00:50,  1.85it/s]Extractor Predicting: 93it [00:50,  1.89it/s]Extractor Predicting: 94it [00:51,  1.90it/s]Extractor Predicting: 95it [00:51,  1.87it/s]Extractor Predicting: 96it [00:52,  1.90it/s]Extractor Predicting: 97it [00:52,  1.93it/s]Extractor Predicting: 98it [00:53,  1.90it/s]Extractor Predicting: 99it [00:53,  1.87it/s]Extractor Predicting: 100it [00:54,  1.90it/s]Extractor Predicting: 101it [00:54,  1.88it/s]Extractor Predicting: 102it [00:55,  1.84it/s]Extractor Predicting: 103it [00:56,  1.82it/s]Extractor Predicting: 104it [00:56,  1.88it/s]Extractor Predicting: 105it [00:57,  1.81it/s]Extractor Predicting: 106it [00:57,  1.85it/s]Extractor Predicting: 107it [00:58,  1.85it/s]Extractor Predicting: 108it [00:58,  1.81it/s]Extractor Predicting: 109it [00:59,  1.80it/s]Extractor Predicting: 110it [00:59,  1.83it/s]Extractor Predicting: 111it [01:00,  1.81it/s]Extractor Predicting: 112it [01:00,  1.82it/s]Extractor Predicting: 113it [01:01,  1.84it/s]Extractor Predicting: 114it [01:02,  1.84it/s]Extractor Predicting: 115it [01:02,  1.85it/s]Extractor Predicting: 116it [01:03,  1.88it/s]Extractor Predicting: 117it [01:03,  1.89it/s]Extractor Predicting: 118it [01:04,  1.94it/s]Extractor Predicting: 119it [01:04,  1.96it/s]Extractor Predicting: 120it [01:05,  1.95it/s]Extractor Predicting: 121it [01:05,  1.94it/s]Extractor Predicting: 122it [01:06,  1.93it/s]Extractor Predicting: 123it [01:06,  1.89it/s]Extractor Predicting: 124it [01:07,  1.91it/s]Extractor Predicting: 125it [01:07,  1.90it/s]Extractor Predicting: 126it [01:08,  1.87it/s]Extractor Predicting: 127it [01:08,  1.82it/s]Extractor Predicting: 128it [01:09,  1.80it/s]Extractor Predicting: 129it [01:10,  1.77it/s]Extractor Predicting: 130it [01:10,  1.75it/s]Extractor Predicting: 131it [01:11,  1.72it/s]Extractor Predicting: 132it [01:11,  1.74it/s]Extractor Predicting: 133it [01:12,  1.76it/s]Extractor Predicting: 134it [01:12,  1.80it/s]Extractor Predicting: 135it [01:13,  1.73it/s]Extractor Predicting: 136it [01:14,  1.73it/s]Extractor Predicting: 137it [01:14,  1.56it/s]Extractor Predicting: 138it [01:15,  1.61it/s]Extractor Predicting: 139it [01:16,  1.64it/s]Extractor Predicting: 140it [01:16,  1.62it/s]Extractor Predicting: 141it [01:17,  1.65it/s]Extractor Predicting: 142it [01:17,  1.66it/s]Extractor Predicting: 143it [01:18,  1.70it/s]Extractor Predicting: 144it [01:18,  1.69it/s]Extractor Predicting: 145it [01:19,  1.69it/s]Extractor Predicting: 146it [01:20,  1.69it/s]Extractor Predicting: 147it [01:20,  1.70it/s]Extractor Predicting: 148it [01:21,  1.71it/s]Extractor Predicting: 149it [01:21,  1.75it/s]Extractor Predicting: 150it [01:22,  1.74it/s]Extractor Predicting: 151it [01:23,  1.73it/s]Extractor Predicting: 152it [01:23,  1.73it/s]Extractor Predicting: 153it [01:24,  1.71it/s]Extractor Predicting: 154it [01:24,  1.70it/s]Extractor Predicting: 155it [01:25,  1.73it/s]Extractor Predicting: 156it [01:25,  1.77it/s]Extractor Predicting: 157it [01:26,  1.84it/s]Extractor Predicting: 158it [01:26,  1.85it/s]Extractor Predicting: 159it [01:27,  1.85it/s]Extractor Predicting: 160it [01:27,  1.87it/s]Extractor Predicting: 161it [01:28,  1.92it/s]Extractor Predicting: 162it [01:29,  1.86it/s]Extractor Predicting: 163it [01:29,  1.84it/s]Extractor Predicting: 164it [01:30,  1.87it/s]Extractor Predicting: 165it [01:30,  1.92it/s]Extractor Predicting: 166it [01:31,  1.91it/s]Extractor Predicting: 167it [01:31,  1.98it/s]Extractor Predicting: 168it [01:32,  1.99it/s]Extractor Predicting: 169it [01:32,  2.02it/s]Extractor Predicting: 170it [01:33,  1.95it/s]Extractor Predicting: 171it [01:33,  1.87it/s]Extractor Predicting: 172it [01:34,  1.82it/s]Extractor Predicting: 173it [01:34,  1.87it/s]Extractor Predicting: 173it [01:34,  1.83it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:01,607 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:01,612 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:01,612 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:01,612 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:01,612 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:19:01,906 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:19:01,907 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:19:02,238 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:19:03,245 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:19:03,245 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:06,291 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:06,293 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:06,293 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:06,293 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:06,293 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:19:06,923 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:19:06,924 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:19:07,500 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:19:07,657 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:19:07,657 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.82it/s]Extractor Predicting: 2it [00:01,  1.72it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:02,  1.68it/s]Extractor Predicting: 6it [00:03,  1.66it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:04,  1.57it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:06,  1.58it/s]Extractor Predicting: 12it [00:07,  1.46it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:09,  1.77it/s]Extractor Predicting: 16it [00:09,  1.88it/s]Extractor Predicting: 17it [00:10,  1.99it/s]Extractor Predicting: 18it [00:10,  2.05it/s]Extractor Predicting: 19it [00:10,  2.07it/s]Extractor Predicting: 20it [00:11,  2.10it/s]Extractor Predicting: 21it [00:11,  2.15it/s]Extractor Predicting: 22it [00:12,  2.10it/s]Extractor Predicting: 23it [00:12,  2.10it/s]Extractor Predicting: 24it [00:13,  2.08it/s]Extractor Predicting: 25it [00:13,  2.08it/s]Extractor Predicting: 26it [00:14,  2.13it/s]Extractor Predicting: 27it [00:14,  2.07it/s]Extractor Predicting: 28it [00:15,  2.09it/s]Extractor Predicting: 29it [00:15,  2.10it/s]Extractor Predicting: 30it [00:16,  2.16it/s]Extractor Predicting: 31it [00:16,  2.18it/s]Extractor Predicting: 32it [00:17,  2.17it/s]Extractor Predicting: 33it [00:17,  2.18it/s]Extractor Predicting: 34it [00:17,  2.14it/s]Extractor Predicting: 35it [00:18,  2.14it/s]Extractor Predicting: 36it [00:18,  2.08it/s]Extractor Predicting: 37it [00:19,  2.04it/s]Extractor Predicting: 38it [00:19,  2.12it/s]Extractor Predicting: 39it [00:20,  2.11it/s]Extractor Predicting: 40it [00:20,  2.11it/s]Extractor Predicting: 41it [00:21,  2.10it/s]Extractor Predicting: 42it [00:21,  2.14it/s]Extractor Predicting: 43it [00:22,  2.09it/s]Extractor Predicting: 44it [00:22,  1.91it/s]Extractor Predicting: 45it [00:23,  1.79it/s]Extractor Predicting: 46it [00:24,  1.71it/s]Extractor Predicting: 47it [00:24,  1.69it/s]Extractor Predicting: 48it [00:25,  1.66it/s]Extractor Predicting: 49it [00:26,  1.66it/s]Extractor Predicting: 50it [00:26,  1.64it/s]Extractor Predicting: 51it [00:27,  1.63it/s]Extractor Predicting: 52it [00:27,  1.61it/s]Extractor Predicting: 53it [00:28,  1.60it/s]Extractor Predicting: 54it [00:29,  1.60it/s]Extractor Predicting: 55it [00:29,  1.62it/s]Extractor Predicting: 56it [00:30,  1.59it/s]Extractor Predicting: 57it [00:31,  1.59it/s]Extractor Predicting: 58it [00:31,  1.60it/s]Extractor Predicting: 59it [00:31,  1.91it/s]Extractor Predicting: 59it [00:31,  1.85it/s]
[INFO|configuration_utils.py:515] 2023-08-28 05:19:40,431 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:19:40,432 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 05:19:40,436 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:19:40,437 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 05:19:40,439 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 05:19:43,338 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 05:19:43,342 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 05:19:43,358 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:19:43,358 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 05:19:43,364 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:19:43,369 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:19:43,369 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:19:43,369 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:19:43,369 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:19:43,370 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:19:43,370 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 05:19:43,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:44,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:45,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:45,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:46,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:47,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:47,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:48,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:48,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:49,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:50,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:50,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:51,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:52,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:52,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:53,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:54,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:54,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:55,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:55,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:56,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:57,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:57,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:58,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:15<02:19, 15.51s/it][WARNING|generation_utils.py:914] 2023-08-28 05:19:59,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:19:59,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:00,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:00,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:01,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:01,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:02,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:03,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:03,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:04,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:04,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:05,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:06,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:06,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:07,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:08,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:08,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:09,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:09,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:10,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:11,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:12,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:12,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:13,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:13,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:14,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:14,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:31<02:08, 16.00s/it][WARNING|generation_utils.py:914] 2023-08-28 05:20:15,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:16,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:16,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:17,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:18,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:18,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:19,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:20,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:20,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:21,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:21,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:22,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:23,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:23,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:24,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:24,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:25,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:26,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:26,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:27,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:27,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:28,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:29,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:29,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:30,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:31,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:31,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:32,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:32,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:33,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:34,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:34,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:35,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:35,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:36,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:36,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:37,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:38,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:38,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:39,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:39,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:40,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:41,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:41,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:42,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:59<02:29, 21.31s/it][WARNING|generation_utils.py:914] 2023-08-28 05:20:43,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:43,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:44,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:45,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:46,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:46,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:47,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:48,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:48,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:49,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:50,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:50,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:51,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:52,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:52,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:53,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:54,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:54,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:55,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:56,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:56,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:57,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:58,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:20:58,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:15<01:56, 19.41s/it][WARNING|generation_utils.py:914] 2023-08-28 05:20:59,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:00,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:00,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:01,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:02,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:02,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:03,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:03,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:04,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:05,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:05,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:06,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:06,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:07,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:08,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:08,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:09,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:09,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:10,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:11,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:11,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:12,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:12,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:13,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:14,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:14,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:15,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:32<01:31, 18.33s/it][WARNING|generation_utils.py:914] 2023-08-28 05:21:15,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:16,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:17,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:17,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:18,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:19,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:19,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:20,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:20,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:21,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:22,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:22,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:23,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:24,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:24,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:25,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:25,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:26,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:27,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:27,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:28,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:28,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:29,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:30,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:30,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:47<01:09, 17.33s/it][WARNING|generation_utils.py:914] 2023-08-28 05:21:31,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:31,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:32,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:33,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:33,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:34,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:35,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:36,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:36,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:37,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:38,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:38,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:39,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:39,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:40,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:41,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:41,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:42,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:42,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:43,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:43,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:44,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:45,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:45,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:02<00:49, 16.59s/it][WARNING|generation_utils.py:914] 2023-08-28 05:21:46,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:47,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:47,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:48,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:48,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:49,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:49,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:50,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:50,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:51,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:51,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:52,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:53,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:53,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:54,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:54,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:55,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:55,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:56,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:57,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:57,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:58,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:58,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:21:59,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:16<00:31, 15.61s/it][WARNING|generation_utils.py:914] 2023-08-28 05:21:59,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:00,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:01,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:01,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:02,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:03,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:03,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:04,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:05,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:05,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:06,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:07,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:08,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:08,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:09,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:09,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:10,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:10,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:11,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:12,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:12,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:13,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:30<00:15, 15.10s/it][WARNING|generation_utils.py:914] 2023-08-28 05:22:13,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:14,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:15,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:15,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:16,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:17,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:17,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:18,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:19,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:19,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:20,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:20,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:21,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:22,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:22,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:23,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:24,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:24,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:25,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:26,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:26,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:27,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:22:27,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:44<00:00, 14.93s/it]Generating: 100%|██████████| 10/10 [02:44<00:00, 16.49s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:33,306 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:33,313 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:33,313 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:33,313 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:33,313 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:22:33,589 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:22:33,590 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:22:34,259 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:22:35,309 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:22:35,309 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:38,206 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:38,212 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:38,212 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:38,212 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:38,212 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:22:38,937 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:22:38,938 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:22:39,258 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:22:39,428 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:22:39,428 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : given name .', 'success_rate': 0.80859375, 'errors': {'', "('William', 'given name', '', 'He was born in the province of Oise , his father being William , Duke of Normandy , 1st Earl of Normandy , and his mother being Mary .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 201, 'raw': 288}
{'target': 600, 'success': 226, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 287, 'raw': 416}
{'target': 600, 'success': 310, 'raw': 448}
{'target': 600, 'success': 333, 'raw': 480}
{'target': 600, 'success': 351, 'raw': 512}
{'target': 600, 'success': 377, 'raw': 544}
{'target': 600, 'success': 403, 'raw': 576}
{'target': 600, 'success': 424, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 468, 'raw': 672}
{'target': 600, 'success': 486, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 532, 'raw': 768}
{'target': 600, 'success': 554, 'raw': 800}
{'target': 600, 'success': 576, 'raw': 832}
{'target': 600, 'success': 601, 'raw': 864}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.6956018518518519, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Brazil', 'languages spoken, written or signed', '', 'Many languages are spoken in Brazil , and there are various communities and organizations advocating for a stronger free and open society .')", "('Estonia', 'languages spoken, written or signed', '', 'The language of Estonia is Estonian , the official language of the Republic of the Union ( DSO ) ) .')"}}
['Relation : lowest point . Context : Later in the year ( 1143 ) , Puyi and his allies made a conquest over the eastern part of the province of Sinti , which was occupied by the Franks and Normans . Head Entity : Sinti , Tail Entity : southern part .\n']
['Relation : lowest point . Context : Later in the year ( 1143 ) , Puyi and his allies made a conquest over the eastern part of the province of Sinti , which was occupied by the Franks and Normans . Head Entity : Sinti , Tail Entity : southern part .\n', 'Relation : lowest point . Context : After the death of Emperor Xuanzong his elder son Liu Bang was the first emperor to die at the age of only two . Head Entity : emperor , Tail Entity : highest point .\n']
{'target': 600, 'success': 12, 'raw': 32}
{'target': 600, 'success': 23, 'raw': 64}
{'target': 600, 'success': 32, 'raw': 96}
{'target': 600, 'success': 44, 'raw': 128}
{'target': 600, 'success': 55, 'raw': 160}
{'target': 600, 'success': 70, 'raw': 192}
{'target': 600, 'success': 86, 'raw': 224}
{'target': 600, 'success': 102, 'raw': 256}
{'target': 600, 'success': 115, 'raw': 288}
{'target': 600, 'success': 125, 'raw': 320}
{'target': 600, 'success': 141, 'raw': 352}
{'target': 600, 'success': 158, 'raw': 384}
{'target': 600, 'success': 176, 'raw': 416}
{'target': 600, 'success': 192, 'raw': 448}
{'target': 600, 'success': 206, 'raw': 480}
{'target': 600, 'success': 221, 'raw': 512}
{'target': 600, 'success': 233, 'raw': 544}
{'target': 600, 'success': 249, 'raw': 576}
{'target': 600, 'success': 265, 'raw': 608}
{'target': 600, 'success': 273, 'raw': 640}
{'target': 600, 'success': 288, 'raw': 672}
{'target': 600, 'success': 308, 'raw': 704}
{'target': 600, 'success': 317, 'raw': 736}
{'target': 600, 'success': 332, 'raw': 768}
{'target': 600, 'success': 342, 'raw': 800}
{'target': 600, 'success': 353, 'raw': 832}
{'target': 600, 'success': 369, 'raw': 864}
{'target': 600, 'success': 385, 'raw': 896}
{'target': 600, 'success': 394, 'raw': 928}
{'target': 600, 'success': 409, 'raw': 960}
{'target': 600, 'success': 423, 'raw': 992}
{'target': 600, 'success': 432, 'raw': 1024}
{'target': 600, 'success': 446, 'raw': 1056}
{'target': 600, 'success': 460, 'raw': 1088}
{'target': 600, 'success': 473, 'raw': 1120}
{'target': 600, 'success': 486, 'raw': 1152}
{'target': 600, 'success': 498, 'raw': 1184}
{'target': 600, 'success': 512, 'raw': 1216}
{'target': 600, 'success': 522, 'raw': 1248}
{'target': 600, 'success': 533, 'raw': 1280}
{'target': 600, 'success': 550, 'raw': 1312}
{'target': 600, 'success': 560, 'raw': 1344}
{'target': 600, 'success': 572, 'raw': 1376}
{'target': 600, 'success': 584, 'raw': 1408}
{'target': 600, 'success': 600, 'raw': 1440}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.4166666666666667, 'errors': {'', "('2004 FIFA World Cup', 'lowest point', '', 'The next day , he made a triumphant recovery and made his international debut at the 2004 FIFA World Cup against Ecuador .')", 'not enough values to unpack (expected 2, got 1)', "('New York Giants', 'lowest point', '', 'In the second season , a team led by George Foreman and David Southee were allowed to make another bowl game against the New York Giants .')", 'too many values to unpack (expected 2)', "('lowest point', 'lowest point', '', 'For comparison , in 1998 , it was the lowest point per square mile since 1960 , when it was 1 . 10 .')", "('63', 'lowest point', '', 'On the night of 9 November 2014 , the Southeastern Conference led by Jaylen Brown was led by Duke Blue Devils in a 63 66 victory over the Southeast Division rival St. Charles .')", "('second', 'lowest point', '', 'He is currently playing with fellow former New Zealand prop Tom Paine in the Championship side Auckland Roosters , and he moved up the rankings to second .')", "('Manchester', 'lowest point', '', 'The area was home to the city of Manchester ( now Manchester City ) , founded in 1859 by the English Civil War hero Sir Walter Scott .')"}}
['Relation : mother . Context : Later in Life , he came to love the beauty and natural beauty of the forests at the end of the third trimester , when she suffered death . Head Entity : forests at the end of the third trimester , Tail Entity : Mother .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : mother .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('king', 'mother', '', 'The first male and last female monarch of the Kingdom of Mexico , the king of the Andes ( d.')"}}
['Relation : mouth of the watercourse . Context : Later in the year ( 1143 ) , Puyi and his allies made atone for the fall of the last King of the Puebla , Puzco . Head Entity : Puyi , Tail Entity : Puducese .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 200, 'raw': 288}
{'target': 600, 'success': 221, 'raw': 320}
{'target': 600, 'success': 241, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 289, 'raw': 416}
{'target': 600, 'success': 311, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 357, 'raw': 512}
{'target': 600, 'success': 383, 'raw': 544}
{'target': 600, 'success': 404, 'raw': 576}
{'target': 600, 'success': 425, 'raw': 608}
{'target': 600, 'success': 440, 'raw': 640}
{'target': 600, 'success': 463, 'raw': 672}
{'target': 600, 'success': 488, 'raw': 704}
{'target': 600, 'success': 509, 'raw': 736}
{'target': 600, 'success': 536, 'raw': 768}
{'target': 600, 'success': 558, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 606, 'raw': 864}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.7013888888888888, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n']
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n', 'Relation : genre . Context : Eileen McClelland ( born June 24 , 1953 ) is an American radio talk show host who has appeared as a regular on both the Howard Stern Show and the Morning Mix . Head Entity : Howard Stern Show , Tail Entity : television talk show .\n']
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n', 'Relation : genre . Context : Eileen McClelland ( born June 24 , 1953 ) is an American radio talk show host who has appeared as a regular on both the Howard Stern Show and the Morning Mix . Head Entity : Howard Stern Show , Tail Entity : television talk show .\n', 'Relation : genre . Context : This film explores the social , social structure of the New York city of Times Square , which is populated and industrialized by wealthy Manhattanites . Head Entity : Times Square , Tail Entity : New York City .\n']
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 320, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 468, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 570, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : genre .', 'success_rate': 0.76625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.8020833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('The albums', 'is a list of', '', 'The albums songs , songs played and discography of the albums members , the artist , are listed in alphabetical order .')", "('programming languages', 'is a list of', '', 'It can be used primarily in conjunction with other programming languages such as .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8111979166666666, 'errors': {'', "('Pluto', 'located on astronomical body', '', 'In the past , Pluto has been considered to be a binary system inhabited by one giant being , as well as being the first to be named after Pluto .')", 'too many values to unpack (expected 2)', "('Its orbit', 'located on astronomical body', '', 'Its orbit shows an eccentricity of 0 . 18 and an inclination of 8 degrees from the plane of the ecliptic .')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('it', 'manufacturer', '', 'It is known in Japan for its high quality and performance components , along with its long range of performance airsoft s.')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8179347826086957, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/4_ext.jsonl'}}
estimate vocab size: 12731
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12831, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.71it/s]Extractor Estimating: 2it [00:01,  1.49it/s]Extractor Estimating: 3it [00:02,  1.45it/s]Extractor Estimating: 4it [00:02,  1.48it/s]Extractor Estimating: 5it [00:03,  1.52it/s]Extractor Estimating: 6it [00:03,  1.55it/s]Extractor Estimating: 7it [00:04,  1.57it/s]Extractor Estimating: 8it [00:05,  1.56it/s]Extractor Estimating: 9it [00:05,  1.53it/s]Extractor Estimating: 10it [00:06,  1.56it/s]Extractor Estimating: 11it [00:07,  1.60it/s]Extractor Estimating: 12it [00:07,  1.65it/s]Extractor Estimating: 13it [00:08,  1.61it/s]Extractor Estimating: 14it [00:08,  1.59it/s]Extractor Estimating: 15it [00:09,  1.60it/s]Extractor Estimating: 16it [00:10,  1.59it/s]Extractor Estimating: 17it [00:10,  1.57it/s]Extractor Estimating: 18it [00:11,  1.57it/s]Extractor Estimating: 19it [00:12,  1.56it/s]Extractor Estimating: 20it [00:12,  1.60it/s]Extractor Estimating: 21it [00:13,  1.63it/s]Extractor Estimating: 22it [00:13,  1.62it/s]Extractor Estimating: 23it [00:14,  1.53it/s]Extractor Estimating: 24it [00:15,  1.59it/s]Extractor Estimating: 25it [00:15,  1.67it/s]Extractor Estimating: 26it [00:16,  1.72it/s]Extractor Estimating: 27it [00:16,  1.71it/s]Extractor Estimating: 28it [00:17,  1.72it/s]Extractor Estimating: 29it [00:18,  1.75it/s]Extractor Estimating: 30it [00:18,  1.75it/s]Extractor Estimating: 31it [00:19,  1.74it/s]Extractor Estimating: 32it [00:19,  1.74it/s]Extractor Estimating: 33it [00:20,  1.76it/s]Extractor Estimating: 34it [00:20,  1.78it/s]Extractor Estimating: 35it [00:21,  1.72it/s]Extractor Estimating: 36it [00:22,  1.73it/s]Extractor Estimating: 37it [00:22,  1.70it/s]Extractor Estimating: 38it [00:23,  1.73it/s]Extractor Estimating: 39it [00:23,  1.78it/s]Extractor Estimating: 40it [00:24,  1.72it/s]Extractor Estimating: 41it [00:24,  1.72it/s]Extractor Estimating: 42it [00:25,  1.74it/s]Extractor Estimating: 43it [00:26,  1.63it/s]Extractor Estimating: 44it [00:26,  1.68it/s]Extractor Estimating: 45it [00:27,  1.74it/s]Extractor Estimating: 46it [00:27,  1.72it/s]Extractor Estimating: 47it [00:28,  1.76it/s]Extractor Estimating: 48it [00:28,  1.80it/s]Extractor Estimating: 49it [00:29,  1.82it/s]Extractor Estimating: 50it [00:30,  1.83it/s]Extractor Estimating: 51it [00:30,  1.73it/s]Extractor Estimating: 52it [00:31,  1.69it/s]Extractor Estimating: 53it [00:31,  1.67it/s]Extractor Estimating: 54it [00:32,  1.64it/s]Extractor Estimating: 55it [00:33,  1.61it/s]Extractor Estimating: 56it [00:33,  1.58it/s]Extractor Estimating: 57it [00:34,  1.56it/s]Extractor Estimating: 58it [00:35,  1.56it/s]Extractor Estimating: 59it [00:35,  1.61it/s]Extractor Estimating: 60it [00:36,  1.60it/s]Extractor Estimating: 61it [00:37,  1.56it/s]Extractor Estimating: 62it [00:37,  1.59it/s]Extractor Estimating: 63it [00:38,  1.54it/s]Extractor Estimating: 64it [00:38,  1.57it/s]Extractor Estimating: 65it [00:39,  1.60it/s]Extractor Estimating: 66it [00:40,  1.57it/s]Extractor Estimating: 67it [00:40,  1.58it/s]Extractor Estimating: 68it [00:41,  1.57it/s]Extractor Estimating: 69it [00:42,  1.57it/s]Extractor Estimating: 70it [00:42,  1.58it/s]Extractor Estimating: 71it [00:43,  1.56it/s]Extractor Estimating: 72it [00:44,  1.57it/s]Extractor Estimating: 73it [00:44,  1.56it/s]Extractor Estimating: 74it [00:45,  1.57it/s]Extractor Estimating: 75it [00:45,  1.58it/s]Extractor Estimating: 76it [00:46,  1.51it/s]Extractor Estimating: 77it [00:47,  1.54it/s]Extractor Estimating: 78it [00:47,  1.53it/s]Extractor Estimating: 79it [00:48,  1.38it/s]Extractor Estimating: 80it [00:49,  1.44it/s]Extractor Estimating: 81it [00:50,  1.47it/s]Extractor Estimating: 82it [00:50,  1.51it/s]Extractor Estimating: 83it [00:51,  1.48it/s]Extractor Estimating: 84it [00:52,  1.48it/s]Extractor Estimating: 85it [00:52,  1.46it/s]Extractor Estimating: 86it [00:53,  1.40it/s]Extractor Estimating: 87it [00:54,  1.41it/s]Extractor Estimating: 88it [00:55,  1.42it/s]Extractor Estimating: 89it [00:55,  1.42it/s]Extractor Estimating: 90it [00:56,  1.43it/s]Extractor Estimating: 91it [00:57,  1.46it/s]Extractor Estimating: 92it [00:57,  1.48it/s]Extractor Estimating: 93it [00:58,  1.50it/s]Extractor Estimating: 94it [00:59,  1.51it/s]Extractor Estimating: 95it [00:59,  1.52it/s]Extractor Estimating: 96it [01:00,  1.51it/s]Extractor Estimating: 97it [01:00,  1.57it/s]Extractor Estimating: 98it [01:01,  1.49it/s]Extractor Estimating: 99it [01:02,  1.50it/s]Extractor Estimating: 100it [01:02,  1.49it/s]Extractor Estimating: 101it [01:03,  1.49it/s]Extractor Estimating: 102it [01:04,  1.54it/s]Extractor Estimating: 103it [01:04,  1.57it/s]Extractor Estimating: 104it [01:05,  1.64it/s]Extractor Estimating: 105it [01:06,  1.60it/s]Extractor Estimating: 106it [01:06,  1.66it/s]Extractor Estimating: 107it [01:07,  1.69it/s]Extractor Estimating: 108it [01:07,  1.69it/s]Extractor Estimating: 109it [01:08,  1.69it/s]Extractor Estimating: 110it [01:08,  1.72it/s]Extractor Estimating: 111it [01:09,  1.70it/s]Extractor Estimating: 112it [01:10,  1.65it/s]Extractor Estimating: 113it [01:10,  1.61it/s]Extractor Estimating: 114it [01:11,  1.67it/s]Extractor Estimating: 115it [01:12,  1.65it/s]Extractor Estimating: 116it [01:12,  1.70it/s]Extractor Estimating: 117it [01:13,  1.65it/s]Extractor Estimating: 118it [01:13,  1.63it/s]Extractor Estimating: 119it [01:14,  1.58it/s]Extractor Estimating: 120it [01:15,  1.62it/s]Extractor Estimating: 121it [01:15,  1.57it/s]Extractor Estimating: 122it [01:16,  1.56it/s]Extractor Estimating: 123it [01:17,  1.58it/s]Extractor Estimating: 124it [01:17,  1.59it/s]Extractor Estimating: 125it [01:18,  1.64it/s]Extractor Estimating: 126it [01:18,  1.64it/s]Extractor Estimating: 127it [01:19,  1.61it/s]Extractor Estimating: 128it [01:20,  1.63it/s]Extractor Estimating: 129it [01:20,  1.63it/s]Extractor Estimating: 130it [01:21,  1.63it/s]Extractor Estimating: 131it [01:21,  1.66it/s]Extractor Estimating: 132it [01:22,  1.65it/s]Extractor Estimating: 133it [01:23,  1.65it/s]Extractor Estimating: 134it [01:23,  1.67it/s]Extractor Estimating: 135it [01:24,  1.62it/s]Extractor Estimating: 136it [01:24,  1.63it/s]Extractor Estimating: 137it [01:25,  1.58it/s]Extractor Estimating: 138it [01:26,  1.59it/s]Extractor Estimating: 139it [01:26,  1.62it/s]Extractor Estimating: 140it [01:27,  1.56it/s]Extractor Estimating: 141it [01:28,  1.56it/s]Extractor Estimating: 142it [01:28,  1.57it/s]Extractor Estimating: 143it [01:29,  1.59it/s]Extractor Estimating: 144it [01:30,  1.55it/s]Extractor Estimating: 145it [01:30,  1.48it/s]Extractor Estimating: 146it [01:31,  1.51it/s]Extractor Estimating: 147it [01:32,  1.56it/s]Extractor Estimating: 148it [01:32,  1.56it/s]Extractor Estimating: 149it [01:33,  1.53it/s]Extractor Estimating: 150it [01:33,  1.62it/s]Extractor Estimating: 151it [01:34,  1.65it/s]Extractor Estimating: 152it [01:35,  1.66it/s]Extractor Estimating: 153it [01:35,  1.70it/s]Extractor Estimating: 154it [01:36,  1.64it/s]Extractor Estimating: 155it [01:36,  1.66it/s]Extractor Estimating: 156it [01:37,  1.64it/s]Extractor Estimating: 157it [01:38,  1.57it/s]Extractor Estimating: 158it [01:38,  1.59it/s]Extractor Estimating: 159it [01:39,  1.63it/s]Extractor Estimating: 160it [01:40,  1.63it/s]Extractor Estimating: 161it [01:40,  1.68it/s]Extractor Estimating: 162it [01:41,  1.64it/s]Extractor Estimating: 163it [01:41,  1.66it/s]Extractor Estimating: 164it [01:42,  1.63it/s]Extractor Estimating: 165it [01:43,  1.62it/s]Extractor Estimating: 166it [01:43,  1.67it/s]Extractor Estimating: 167it [01:44,  1.71it/s]Extractor Estimating: 168it [01:44,  1.71it/s]Extractor Estimating: 169it [01:45,  1.67it/s]Extractor Estimating: 170it [01:45,  1.68it/s]Extractor Estimating: 171it [01:46,  1.66it/s]Extractor Estimating: 172it [01:47,  1.75it/s]Extractor Estimating: 173it [01:47,  1.66it/s]Extractor Estimating: 174it [01:48,  1.63it/s]Extractor Estimating: 175it [01:48,  1.68it/s]Extractor Estimating: 176it [01:49,  1.73it/s]Extractor Estimating: 177it [01:50,  1.71it/s]Extractor Estimating: 178it [01:50,  1.74it/s]Extractor Estimating: 179it [01:51,  1.77it/s]Extractor Estimating: 180it [01:51,  1.74it/s]Extractor Estimating: 181it [01:52,  1.75it/s]Extractor Estimating: 182it [01:52,  1.73it/s]Extractor Estimating: 183it [01:53,  1.78it/s]Extractor Estimating: 184it [01:54,  1.73it/s]Extractor Estimating: 185it [01:54,  1.73it/s]Extractor Estimating: 186it [01:55,  1.69it/s]Extractor Estimating: 187it [01:55,  1.68it/s]Extractor Estimating: 188it [01:56,  1.72it/s]Extractor Estimating: 189it [01:57,  1.68it/s]Extractor Estimating: 190it [01:57,  1.68it/s]Extractor Estimating: 191it [01:58,  1.69it/s]Extractor Estimating: 192it [01:58,  1.69it/s]Extractor Estimating: 193it [01:59,  1.79it/s]Extractor Estimating: 194it [01:59,  1.76it/s]Extractor Estimating: 195it [02:00,  1.68it/s]Extractor Estimating: 196it [02:01,  1.69it/s]Extractor Estimating: 197it [02:01,  1.72it/s]Extractor Estimating: 198it [02:02,  1.71it/s]Extractor Estimating: 199it [02:02,  1.68it/s]Extractor Estimating: 200it [02:03,  1.66it/s]Extractor Estimating: 201it [02:04,  1.66it/s]Extractor Estimating: 202it [02:04,  1.68it/s]Extractor Estimating: 203it [02:05,  1.71it/s]Extractor Estimating: 204it [02:05,  1.71it/s]Extractor Estimating: 205it [02:06,  1.63it/s]Extractor Estimating: 206it [02:07,  1.70it/s]Extractor Estimating: 207it [02:07,  1.66it/s]Extractor Estimating: 208it [02:08,  1.74it/s]Extractor Estimating: 209it [02:08,  1.71it/s]Extractor Estimating: 210it [02:09,  1.69it/s]Extractor Estimating: 211it [02:10,  1.62it/s]Extractor Estimating: 212it [02:10,  1.64it/s]Extractor Estimating: 213it [02:11,  1.67it/s]Extractor Estimating: 214it [02:11,  1.66it/s]Extractor Estimating: 215it [02:12,  1.67it/s]Extractor Estimating: 216it [02:13,  1.72it/s]Extractor Estimating: 217it [02:13,  1.72it/s]Extractor Estimating: 218it [02:14,  1.70it/s]Extractor Estimating: 219it [02:14,  1.72it/s]Extractor Estimating: 220it [02:15,  1.71it/s]Extractor Estimating: 221it [02:15,  1.77it/s]Extractor Estimating: 222it [02:16,  1.74it/s]Extractor Estimating: 223it [02:17,  1.75it/s]Extractor Estimating: 224it [02:17,  1.72it/s]Extractor Estimating: 225it [02:18,  1.68it/s]Extractor Estimating: 226it [02:18,  1.64it/s]Extractor Estimating: 227it [02:19,  1.50it/s]Extractor Estimating: 228it [02:20,  1.54it/s]Extractor Estimating: 229it [02:20,  1.55it/s]Extractor Estimating: 230it [02:21,  1.58it/s]Extractor Estimating: 231it [02:22,  1.60it/s]Extractor Estimating: 232it [02:22,  1.61it/s]Extractor Estimating: 233it [02:23,  1.54it/s]Extractor Estimating: 234it [02:24,  1.54it/s]Extractor Estimating: 235it [02:24,  1.59it/s]Extractor Estimating: 236it [02:25,  1.61it/s]Extractor Estimating: 237it [02:26,  1.55it/s]Extractor Estimating: 238it [02:26,  1.60it/s]Extractor Estimating: 239it [02:27,  1.59it/s]Extractor Estimating: 240it [02:27,  1.61it/s]Extractor Estimating: 241it [02:28,  1.60it/s]Extractor Estimating: 242it [02:29,  1.58it/s]Extractor Estimating: 243it [02:29,  1.64it/s]Extractor Estimating: 244it [02:30,  1.60it/s]Extractor Estimating: 245it [02:31,  1.56it/s]Extractor Estimating: 246it [02:31,  1.57it/s]Extractor Estimating: 247it [02:32,  1.58it/s]Extractor Estimating: 248it [02:32,  1.59it/s]Extractor Estimating: 249it [02:33,  1.59it/s]Extractor Estimating: 250it [02:34,  1.52it/s]Extractor Estimating: 250it [02:34,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:30,542 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:30,549 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:30,549 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:30,549 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:30,549 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:25:31,149 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:25:31,150 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:25:31,713 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:25:32,723 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:25:32,723 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:35,680 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:35,686 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:35,686 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:35,686 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:25:35,686 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:25:36,490 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:25:36,492 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:25:37,064 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:25:37,230 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:25:37,230 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 06:54:07,552 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 06:54:07,555 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 5099 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 19859
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19959, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=19959, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.042, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.036, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 87, avg_time 1.039, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 187, avg_time 1.034, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 74, avg_time 1.039, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 174, avg_time 2.161, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 61, avg_time 1.043, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 161, avg_time 1.041, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 48, avg_time 1.028, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 148, avg_time 1.050, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 35, avg_time 2.155, loss:nan
g_step 1200, step 135, avg_time 1.034, loss:nan
g_step 1300, step 22, avg_time 1.030, loss:nan
g_step 1400, step 122, avg_time 1.026, loss:nan
g_step 1500, step 9, avg_time 1.054, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 109, avg_time 2.157, loss:nan
g_step 1700, step 209, avg_time 1.025, loss:nan
g_step 1800, step 96, avg_time 1.045, loss:nan
g_step 1900, step 196, avg_time 1.026, loss:nan
g_step 2000, step 83, avg_time 1.034, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 183, avg_time 2.158, loss:nan
g_step 2200, step 70, avg_time 1.022, loss:nan
g_step 2300, step 170, avg_time 1.045, loss:nan
g_step 2400, step 57, avg_time 1.027, loss:nan
g_step 2500, step 157, avg_time 1.040, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 44, avg_time 2.143, loss:nan
g_step 2700, step 144, avg_time 1.037, loss:nan
g_step 2800, step 31, avg_time 1.039, loss:nan
g_step 2900, step 131, avg_time 1.043, loss:nan
g_step 3000, step 18, avg_time 1.045, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 118, avg_time 2.151, loss:nan
g_step 3200, step 5, avg_time 1.027, loss:nan
g_step 3300, step 105, avg_time 1.031, loss:nan
g_step 3400, step 205, avg_time 1.048, loss:nan
g_step 3500, step 92, avg_time 1.041, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 192, avg_time 2.148, loss:nan
g_step 3700, step 79, avg_time 1.043, loss:nan
g_step 3800, step 179, avg_time 1.017, loss:nan
g_step 3900, step 66, avg_time 1.034, loss:nan
g_step 4000, step 166, avg_time 1.043, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 53, avg_time 2.144, loss:nan
g_step 4200, step 153, avg_time 1.035, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 06:54:07 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 06:54:07 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_06-54-07_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 06:54:08 - WARNING - datasets.builder -   Using custom data configuration default-6143c3dfd4b19f5f
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-6143c3dfd4b19f5f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 06:54:08,862 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:54:08,863 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 06:54:08,863 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:54:08,864 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 06:54:08,874 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:54:08,881 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:54:08,881 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:54:08,881 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:54:08,881 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:54:08,881 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:54:08,881 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 06:54:09,025 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 06:54:12,165 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 06:54:12,168 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-6143c3dfd4b19f5f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.29ba/s] 33%|███▎      | 2/6 [00:00<00:00,  4.06ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.41ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.55ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.63ba/s]100%|██████████| 6/6 [00:01<00:00,  5.18ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.12ba/s] 40%|████      | 2/5 [00:00<00:00,  4.32ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.38ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.39ba/s]100%|██████████| 5/5 [00:00<00:00,  5.36ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  9.51ba/s] 50%|█████     | 3/6 [00:00<00:00, 10.36ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 10.47ba/s]100%|██████████| 6/6 [00:00<00:00, 12.19ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.35ba/s] 60%|██████    | 3/5 [00:00<00:00,  6.90ba/s]100%|██████████| 5/5 [00:00<00:00,  8.67ba/s]
[INFO|trainer.py:414] 2023-08-28 06:54:16,099 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 06:54:16,111 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 06:54:16,111 >>   Num examples = 5100
[INFO|trainer.py:1149] 2023-08-28 06:54:16,111 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 06:54:16,111 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 06:54:16,111 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 06:54:16,111 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 06:54:16,111 >>   Total optimization steps = 400
  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 1/400 [00:00<02:19,  2.86it/s]  0%|          | 2/400 [00:00<02:04,  3.18it/s]  1%|          | 3/400 [00:00<01:58,  3.35it/s]  1%|          | 4/400 [00:01<01:56,  3.40it/s]  1%|▏         | 5/400 [00:01<01:54,  3.45it/s]  2%|▏         | 6/400 [00:01<01:53,  3.48it/s]  2%|▏         | 7/400 [00:02<01:53,  3.46it/s]  2%|▏         | 8/400 [00:02<01:52,  3.49it/s]  2%|▏         | 9/400 [00:02<01:51,  3.49it/s]  2%|▎         | 10/400 [00:02<01:51,  3.51it/s]  3%|▎         | 11/400 [00:03<01:50,  3.53it/s]  3%|▎         | 12/400 [00:03<01:49,  3.53it/s]  3%|▎         | 13/400 [00:03<01:49,  3.54it/s]  4%|▎         | 14/400 [00:04<01:48,  3.54it/s]  4%|▍         | 15/400 [00:04<01:48,  3.55it/s]  4%|▍         | 16/400 [00:04<01:47,  3.56it/s]  4%|▍         | 17/400 [00:04<01:47,  3.56it/s]  4%|▍         | 18/400 [00:05<01:47,  3.56it/s]  5%|▍         | 19/400 [00:05<01:47,  3.56it/s]  5%|▌         | 20/400 [00:05<01:47,  3.55it/s]  5%|▌         | 21/400 [00:06<01:46,  3.55it/s]  6%|▌         | 22/400 [00:06<01:46,  3.55it/s]  6%|▌         | 23/400 [00:06<01:46,  3.55it/s]  6%|▌         | 24/400 [00:06<01:45,  3.55it/s]  6%|▋         | 25/400 [00:07<01:45,  3.56it/s]  6%|▋         | 26/400 [00:07<01:45,  3.56it/s]  7%|▋         | 27/400 [00:07<01:44,  3.56it/s]  7%|▋         | 28/400 [00:07<01:44,  3.56it/s]  7%|▋         | 29/400 [00:08<01:43,  3.57it/s]  8%|▊         | 30/400 [00:08<01:43,  3.57it/s]  8%|▊         | 31/400 [00:08<01:43,  3.55it/s]  8%|▊         | 32/400 [00:09<01:43,  3.56it/s]  8%|▊         | 33/400 [00:09<01:43,  3.56it/s]  8%|▊         | 34/400 [00:09<01:42,  3.56it/s]  9%|▉         | 35/400 [00:09<01:42,  3.56it/s]  9%|▉         | 36/400 [00:10<01:42,  3.56it/s]  9%|▉         | 37/400 [00:10<01:41,  3.56it/s] 10%|▉         | 38/400 [00:10<01:41,  3.56it/s] 10%|▉         | 39/400 [00:11<01:41,  3.56it/s] 10%|█         | 40/400 [00:11<01:41,  3.56it/s] 10%|█         | 41/400 [00:11<01:40,  3.56it/s] 10%|█         | 42/400 [00:11<01:45,  3.41it/s] 11%|█         | 43/400 [00:12<01:43,  3.45it/s] 11%|█         | 44/400 [00:12<01:42,  3.48it/s] 11%|█▏        | 45/400 [00:12<01:41,  3.51it/s] 12%|█▏        | 46/400 [00:13<01:40,  3.52it/s] 12%|█▏        | 47/400 [00:13<01:39,  3.54it/s] 12%|█▏        | 48/400 [00:13<01:39,  3.55it/s] 12%|█▏        | 49/400 [00:13<01:38,  3.55it/s] 12%|█▎        | 50/400 [00:14<01:38,  3.56it/s] 13%|█▎        | 51/400 [00:14<01:38,  3.56it/s] 13%|█▎        | 52/400 [00:14<01:37,  3.56it/s] 13%|█▎        | 53/400 [00:15<01:37,  3.54it/s] 14%|█▎        | 54/400 [00:15<01:37,  3.55it/s] 14%|█▍        | 55/400 [00:15<01:37,  3.55it/s] 14%|█▍        | 56/400 [00:15<01:37,  3.54it/s] 14%|█▍        | 57/400 [00:16<01:36,  3.55it/s] 14%|█▍        | 58/400 [00:16<01:36,  3.55it/s] 15%|█▍        | 59/400 [00:16<01:35,  3.55it/s] 15%|█▌        | 60/400 [00:17<01:35,  3.56it/s] 15%|█▌        | 61/400 [00:17<01:35,  3.56it/s] 16%|█▌        | 62/400 [00:17<01:34,  3.56it/s] 16%|█▌        | 63/400 [00:17<01:34,  3.56it/s] 16%|█▌        | 64/400 [00:18<01:34,  3.55it/s] 16%|█▋        | 65/400 [00:18<01:34,  3.55it/s] 16%|█▋        | 66/400 [00:18<01:34,  3.55it/s] 17%|█▋        | 67/400 [00:18<01:33,  3.55it/s] 17%|█▋        | 68/400 [00:19<01:33,  3.55it/s] 17%|█▋        | 69/400 [00:19<01:33,  3.55it/s] 18%|█▊        | 70/400 [00:19<01:32,  3.55it/s] 18%|█▊        | 71/400 [00:20<01:32,  3.55it/s] 18%|█▊        | 72/400 [00:20<01:32,  3.55it/s] 18%|█▊        | 73/400 [00:20<01:32,  3.55it/s] 18%|█▊        | 74/400 [00:20<01:31,  3.55it/s] 19%|█▉        | 75/400 [00:21<01:31,  3.55it/s] 19%|█▉        | 76/400 [00:21<01:31,  3.55it/s] 19%|█▉        | 77/400 [00:21<01:30,  3.55it/s] 20%|█▉        | 78/400 [00:22<01:30,  3.55it/s] 20%|█▉        | 79/400 [00:22<01:30,  3.56it/s] 20%|██        | 80/400 [00:22<01:22,  3.89it/s][INFO|trainer.py:2140] 2023-08-28 06:54:38,668 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:54:38,668 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 06:54:38,668 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.19it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.68it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.87it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.64it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.10it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.75it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.41it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.25it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.33it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.53it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.61it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.61it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.51it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.37it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.23it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.08it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.17it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.21it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.29it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.44it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.56it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.48it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.32it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.15it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.95it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.12it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.33it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.37it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.32it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.32it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.47it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.31it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.21it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.08it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.04it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.35it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.27it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.35it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.43it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.34it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.20it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.15it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.08it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.19it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.26it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.19it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.28it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.34it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.16it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.33it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.08it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.19it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.23it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.19it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.30it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.47it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.39it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.32it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.29it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.23it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.18it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.20it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.24it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.28it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.37it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.31it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.37it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.33it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.25it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.26it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.22it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.09it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.27it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.17it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.26it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.32it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.19it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.13it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.18it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.25it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.12it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.24it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.19it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.17it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.25it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.18it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.03it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 43.97it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.08it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.12it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.22it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.24it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.18it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.17it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.24it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.25it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.28it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.11it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.25it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.31it/s][A                                                
                                                 [A 20%|██        | 80/400 [00:33<01:22,  3.89it/s]
100%|██████████| 505/505 [00:11<00:00, 44.31it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:54:50,102 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-80
[INFO|configuration_utils.py:351] 2023-08-28 06:54:50,120 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-80/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:54:51,732 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-80/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:54:51,745 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-80/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:54:51,760 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-80/special_tokens_map.json
 20%|██        | 81/400 [00:36<22:47,  4.29s/it] 20%|██        | 82/400 [00:36<16:21,  3.09s/it] 21%|██        | 83/400 [00:36<11:51,  2.24s/it] 21%|██        | 84/400 [00:37<08:42,  1.65s/it] 21%|██▏       | 85/400 [00:37<06:31,  1.24s/it] 22%|██▏       | 86/400 [00:37<05:00,  1.05it/s] 22%|██▏       | 87/400 [00:37<03:55,  1.33it/s] 22%|██▏       | 88/400 [00:38<03:10,  1.64it/s] 22%|██▏       | 89/400 [00:38<02:38,  1.96it/s] 22%|██▎       | 90/400 [00:38<02:16,  2.27it/s] 23%|██▎       | 91/400 [00:39<02:00,  2.56it/s] 23%|██▎       | 92/400 [00:39<01:49,  2.80it/s] 23%|██▎       | 93/400 [00:39<01:42,  3.00it/s] 24%|██▎       | 94/400 [00:39<01:36,  3.16it/s] 24%|██▍       | 95/400 [00:40<01:32,  3.28it/s] 24%|██▍       | 96/400 [00:40<01:30,  3.36it/s] 24%|██▍       | 97/400 [00:40<01:28,  3.43it/s] 24%|██▍       | 98/400 [00:40<01:26,  3.49it/s] 25%|██▍       | 99/400 [00:41<01:25,  3.52it/s] 25%|██▌       | 100/400 [00:41<01:24,  3.55it/s] 25%|██▌       | 101/400 [00:41<01:23,  3.57it/s] 26%|██▌       | 102/400 [00:42<01:23,  3.58it/s] 26%|██▌       | 103/400 [00:42<01:22,  3.59it/s] 26%|██▌       | 104/400 [00:42<01:22,  3.59it/s] 26%|██▋       | 105/400 [00:42<01:21,  3.60it/s] 26%|██▋       | 106/400 [00:43<01:21,  3.60it/s] 27%|██▋       | 107/400 [00:43<01:21,  3.59it/s] 27%|██▋       | 108/400 [00:43<01:21,  3.60it/s] 27%|██▋       | 109/400 [00:44<01:20,  3.60it/s] 28%|██▊       | 110/400 [00:44<01:20,  3.61it/s] 28%|██▊       | 111/400 [00:44<01:20,  3.60it/s] 28%|██▊       | 112/400 [00:44<01:19,  3.60it/s] 28%|██▊       | 113/400 [00:45<01:19,  3.60it/s] 28%|██▊       | 114/400 [00:45<01:19,  3.60it/s] 29%|██▉       | 115/400 [00:45<01:19,  3.61it/s] 29%|██▉       | 116/400 [00:45<01:18,  3.61it/s] 29%|██▉       | 117/400 [00:46<01:18,  3.61it/s] 30%|██▉       | 118/400 [00:46<01:18,  3.58it/s] 30%|██▉       | 119/400 [00:46<01:18,  3.59it/s] 30%|███       | 120/400 [00:47<01:17,  3.59it/s] 30%|███       | 121/400 [00:47<01:17,  3.59it/s] 30%|███       | 122/400 [00:47<01:17,  3.60it/s] 31%|███       | 123/400 [00:47<01:16,  3.60it/s] 31%|███       | 124/400 [00:48<01:16,  3.60it/s] 31%|███▏      | 125/400 [00:48<01:16,  3.61it/s] 32%|███▏      | 126/400 [00:48<01:15,  3.61it/s] 32%|███▏      | 127/400 [00:49<01:15,  3.61it/s] 32%|███▏      | 128/400 [00:49<01:15,  3.61it/s] 32%|███▏      | 129/400 [00:49<01:29,  3.03it/s] 32%|███▎      | 130/400 [00:50<01:24,  3.19it/s] 33%|███▎      | 131/400 [00:50<01:21,  3.30it/s] 33%|███▎      | 132/400 [00:50<01:19,  3.39it/s] 33%|███▎      | 133/400 [00:50<01:17,  3.45it/s] 34%|███▎      | 134/400 [00:51<01:16,  3.50it/s] 34%|███▍      | 135/400 [00:51<01:15,  3.53it/s] 34%|███▍      | 136/400 [00:51<01:14,  3.55it/s] 34%|███▍      | 137/400 [00:51<01:13,  3.57it/s] 34%|███▍      | 138/400 [00:52<01:13,  3.58it/s] 35%|███▍      | 139/400 [00:52<01:12,  3.59it/s] 35%|███▌      | 140/400 [00:52<01:12,  3.59it/s] 35%|███▌      | 141/400 [00:53<01:12,  3.59it/s] 36%|███▌      | 142/400 [00:53<01:11,  3.60it/s] 36%|███▌      | 143/400 [00:53<01:11,  3.60it/s] 36%|███▌      | 144/400 [00:53<01:11,  3.60it/s] 36%|███▋      | 145/400 [00:54<01:10,  3.60it/s] 36%|███▋      | 146/400 [00:54<01:10,  3.61it/s] 37%|███▋      | 147/400 [00:54<01:10,  3.61it/s] 37%|███▋      | 148/400 [00:55<01:09,  3.61it/s] 37%|███▋      | 149/400 [00:55<01:09,  3.61it/s] 38%|███▊      | 150/400 [00:55<01:09,  3.60it/s] 38%|███▊      | 151/400 [00:55<01:09,  3.59it/s] 38%|███▊      | 152/400 [00:56<01:09,  3.59it/s] 38%|███▊      | 153/400 [00:56<01:08,  3.59it/s] 38%|███▊      | 154/400 [00:56<01:08,  3.59it/s] 39%|███▉      | 155/400 [00:56<01:08,  3.60it/s] 39%|███▉      | 156/400 [00:57<01:07,  3.60it/s] 39%|███▉      | 157/400 [00:57<01:07,  3.60it/s] 40%|███▉      | 158/400 [00:57<01:07,  3.60it/s] 40%|███▉      | 159/400 [00:58<01:06,  3.60it/s] 40%|████      | 160/400 [00:58<01:00,  3.94it/s][INFO|trainer.py:2140] 2023-08-28 06:55:14,405 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:55:14,405 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 06:55:14,405 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4118, 'eval_samples_per_second': 353.932, 'eval_steps_per_second': 44.252, 'epoch': 1.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.98it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.47it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.43it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.40it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.92it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.46it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.27it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.03it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.08it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.28it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.34it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.40it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.32it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.26it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.12it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.93it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.98it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.10it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.24it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.37it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.29it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.29it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.16it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.06it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.98it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.03it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 43.90it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 43.99it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.25it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.24it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.20it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.12it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.04it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.08it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.10it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.04it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.20it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.33it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.36it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.21it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.11it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.00it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.05it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.00it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.00it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.04it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.24it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.40it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.34it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.25it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.12it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.10it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.01it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.08it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.17it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.32it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.32it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.30it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.21it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.17it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.05it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.00it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 43.96it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.11it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.26it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.24it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.12it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.26it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.18it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.10it/s][A
 71%|███████   | 357/505 [00:08<00:03, 43.96it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.12it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.19it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.19it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.27it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.19it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.20it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.19it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.08it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.11it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.10it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.15it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.13it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.19it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.12it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.02it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.16it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 43.93it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.11it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.19it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.17it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.17it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.25it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.16it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.19it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.19it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.15it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.05it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.11it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.14it/s][A                                                 
                                                 [A 40%|████      | 160/400 [01:09<01:00,  3.94it/s]
100%|██████████| 505/505 [00:11<00:00, 44.14it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:55:25,873 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-160
[INFO|configuration_utils.py:351] 2023-08-28 06:55:25,895 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-160/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:55:27,528 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-160/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:55:27,546 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-160/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:55:27,561 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-160/special_tokens_map.json
 40%|████      | 161/400 [01:12<17:08,  4.30s/it] 40%|████      | 162/400 [01:12<12:17,  3.10s/it] 41%|████      | 163/400 [01:12<08:53,  2.25s/it] 41%|████      | 164/400 [01:12<06:32,  1.66s/it] 41%|████▏     | 165/400 [01:13<04:53,  1.25s/it] 42%|████▏     | 166/400 [01:13<03:44,  1.04it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 42%|████▏     | 167/400 [01:13<02:55,  1.33it/s] 42%|████▏     | 168/400 [01:14<02:22,  1.63it/s] 42%|████▏     | 169/400 [01:14<01:58,  1.95it/s] 42%|████▎     | 170/400 [01:14<01:42,  2.25it/s] 43%|████▎     | 171/400 [01:14<01:30,  2.53it/s] 43%|████▎     | 172/400 [01:15<01:22,  2.77it/s] 43%|████▎     | 173/400 [01:15<01:16,  2.96it/s] 44%|████▎     | 174/400 [01:15<01:12,  3.12it/s] 44%|████▍     | 175/400 [01:15<01:09,  3.23it/s] 44%|████▍     | 176/400 [01:16<01:07,  3.32it/s] 44%|████▍     | 177/400 [01:16<01:05,  3.39it/s] 44%|████▍     | 178/400 [01:16<01:04,  3.44it/s] 45%|████▍     | 179/400 [01:17<01:03,  3.48it/s] 45%|████▌     | 180/400 [01:17<01:02,  3.50it/s] 45%|████▌     | 181/400 [01:17<01:02,  3.52it/s] 46%|████▌     | 182/400 [01:17<01:02,  3.51it/s] 46%|████▌     | 183/400 [01:18<01:01,  3.52it/s] 46%|████▌     | 184/400 [01:18<01:01,  3.53it/s] 46%|████▋     | 185/400 [01:18<01:00,  3.53it/s] 46%|████▋     | 186/400 [01:19<01:00,  3.54it/s] 47%|████▋     | 187/400 [01:19<01:00,  3.54it/s] 47%|████▋     | 188/400 [01:19<00:59,  3.55it/s] 47%|████▋     | 189/400 [01:19<00:59,  3.55it/s] 48%|████▊     | 190/400 [01:20<00:59,  3.55it/s] 48%|████▊     | 191/400 [01:20<00:58,  3.56it/s] 48%|████▊     | 192/400 [01:20<00:58,  3.57it/s] 48%|████▊     | 193/400 [01:21<00:57,  3.57it/s] 48%|████▊     | 194/400 [01:21<00:57,  3.58it/s] 49%|████▉     | 195/400 [01:21<00:57,  3.59it/s] 49%|████▉     | 196/400 [01:21<00:56,  3.59it/s] 49%|████▉     | 197/400 [01:22<00:56,  3.60it/s] 50%|████▉     | 198/400 [01:22<00:56,  3.59it/s] 50%|████▉     | 199/400 [01:22<00:55,  3.59it/s] 50%|█████     | 200/400 [01:22<00:55,  3.60it/s] 50%|█████     | 201/400 [01:23<00:55,  3.60it/s] 50%|█████     | 202/400 [01:23<00:54,  3.60it/s] 51%|█████     | 203/400 [01:23<00:54,  3.60it/s] 51%|█████     | 204/400 [01:24<00:54,  3.60it/s] 51%|█████▏    | 205/400 [01:24<00:54,  3.60it/s] 52%|█████▏    | 206/400 [01:24<00:53,  3.60it/s] 52%|█████▏    | 207/400 [01:24<00:53,  3.60it/s] 52%|█████▏    | 208/400 [01:25<00:53,  3.60it/s] 52%|█████▏    | 209/400 [01:25<00:53,  3.60it/s] 52%|█████▎    | 210/400 [01:25<00:52,  3.60it/s] 53%|█████▎    | 211/400 [01:26<00:52,  3.60it/s] 53%|█████▎    | 212/400 [01:26<00:52,  3.60it/s] 53%|█████▎    | 213/400 [01:26<00:52,  3.57it/s] 54%|█████▎    | 214/400 [01:26<00:51,  3.58it/s] 54%|█████▍    | 215/400 [01:27<00:51,  3.59it/s] 54%|█████▍    | 216/400 [01:27<00:51,  3.59it/s] 54%|█████▍    | 217/400 [01:27<00:50,  3.59it/s] 55%|█████▍    | 218/400 [01:27<00:50,  3.60it/s] 55%|█████▍    | 219/400 [01:28<00:50,  3.60it/s] 55%|█████▌    | 220/400 [01:28<00:50,  3.60it/s] 55%|█████▌    | 221/400 [01:28<00:49,  3.60it/s] 56%|█████▌    | 222/400 [01:29<00:49,  3.60it/s] 56%|█████▌    | 223/400 [01:29<00:49,  3.60it/s] 56%|█████▌    | 224/400 [01:29<00:48,  3.59it/s] 56%|█████▋    | 225/400 [01:29<00:48,  3.59it/s] 56%|█████▋    | 226/400 [01:30<00:48,  3.59it/s] 57%|█████▋    | 227/400 [01:30<00:48,  3.59it/s] 57%|█████▋    | 228/400 [01:30<00:47,  3.59it/s] 57%|█████▋    | 229/400 [01:31<00:47,  3.60it/s] 57%|█████▊    | 230/400 [01:31<00:47,  3.59it/s] 58%|█████▊    | 231/400 [01:31<00:47,  3.60it/s] 58%|█████▊    | 232/400 [01:31<00:46,  3.60it/s] 58%|█████▊    | 233/400 [01:32<00:46,  3.60it/s] 58%|█████▊    | 234/400 [01:32<00:46,  3.60it/s] 59%|█████▉    | 235/400 [01:32<00:45,  3.59it/s] 59%|█████▉    | 236/400 [01:33<00:45,  3.60it/s] 59%|█████▉    | 237/400 [01:33<00:45,  3.60it/s] 60%|█████▉    | 238/400 [01:33<00:45,  3.60it/s] 60%|█████▉    | 239/400 [01:33<00:44,  3.60it/s] 60%|██████    | 240/400 [01:34<00:40,  3.93it/s][INFO|trainer.py:2140] 2023-08-28 06:55:50,148 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:55:50,148 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 06:55:50,148 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4385, 'eval_samples_per_second': 353.106, 'eval_steps_per_second': 44.149, 'epoch': 2.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.74it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.49it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.50it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.47it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.87it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.44it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.16it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.04it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.14it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.33it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.41it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.22it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.19it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.14it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.04it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 42.63it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.22it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 43.64it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 43.99it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.12it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.15it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.11it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.06it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 43.86it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.84it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.94it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.18it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.24it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.32it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.27it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.12it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.08it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 43.92it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.95it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 43.98it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.12it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.25it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.36it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.27it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.26it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.12it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 43.89it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 43.97it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.10it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.19it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.38it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.21it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.19it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.21it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.05it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.02it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 43.94it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.12it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.23it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.33it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 43.75it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.30it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.25it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.15it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.03it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.13it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.07it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.17it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.29it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.26it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.21it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.08it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.14it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.10it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.12it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.11it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.13it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.24it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.25it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.17it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.15it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 43.99it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.07it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.10it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.09it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.23it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.24it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.17it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.18it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.19it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.16it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.10it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 43.98it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.10it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.21it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.27it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.15it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.18it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.19it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.16it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.02it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.06it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.07it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.15it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.22it/s][A                                                 
                                                 [A 60%|██████    | 240/400 [01:45<00:40,  3.93it/s]
100%|██████████| 505/505 [00:11<00:00, 44.22it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:56:01,613 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-240
[INFO|configuration_utils.py:351] 2023-08-28 06:56:01,648 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-240/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:56:03,475 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-240/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:56:03,493 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-240/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:56:03,514 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-240/special_tokens_map.json
 60%|██████    | 241/400 [01:47<11:33,  4.36s/it] 60%|██████    | 242/400 [01:48<08:16,  3.14s/it] 61%|██████    | 243/400 [01:48<05:58,  2.28s/it] 61%|██████    | 244/400 [01:48<04:22,  1.68s/it] 61%|██████▏   | 245/400 [01:49<03:15,  1.26s/it] 62%|██████▏   | 246/400 [01:49<02:29,  1.03it/s] 62%|██████▏   | 247/400 [01:49<01:56,  1.31it/s] 62%|██████▏   | 248/400 [01:49<01:33,  1.62it/s] 62%|██████▏   | 249/400 [01:50<01:18,  1.93it/s] 62%|██████▎   | 250/400 [01:50<01:06,  2.24it/s] 63%|██████▎   | 251/400 [01:50<00:59,  2.52it/s] 63%|██████▎   | 252/400 [01:51<00:53,  2.76it/s] 63%|██████▎   | 253/400 [01:51<00:49,  2.95it/s] 64%|██████▎   | 254/400 [01:51<00:47,  3.11it/s] 64%|██████▍   | 255/400 [01:51<00:44,  3.23it/s] 64%|██████▍   | 256/400 [01:52<00:43,  3.33it/s] 64%|██████▍   | 257/400 [01:52<00:42,  3.39it/s] 64%|██████▍   | 258/400 [01:52<00:41,  3.44it/s] 65%|██████▍   | 259/400 [01:53<00:40,  3.47it/s] 65%|██████▌   | 260/400 [01:53<00:39,  3.50it/s] 65%|██████▌   | 261/400 [01:53<00:39,  3.51it/s] 66%|██████▌   | 262/400 [01:53<00:39,  3.52it/s] 66%|██████▌   | 263/400 [01:54<00:38,  3.53it/s] 66%|██████▌   | 264/400 [01:54<00:38,  3.54it/s] 66%|██████▋   | 265/400 [01:54<00:38,  3.54it/s] 66%|██████▋   | 266/400 [01:55<00:37,  3.54it/s] 67%|██████▋   | 267/400 [01:55<00:37,  3.55it/s] 67%|██████▋   | 268/400 [01:55<00:37,  3.55it/s] 67%|██████▋   | 269/400 [01:55<00:36,  3.55it/s] 68%|██████▊   | 270/400 [01:56<00:36,  3.55it/s] 68%|██████▊   | 271/400 [01:56<00:36,  3.55it/s] 68%|██████▊   | 272/400 [01:56<00:36,  3.55it/s] 68%|██████▊   | 273/400 [01:57<00:35,  3.55it/s] 68%|██████▊   | 274/400 [01:57<00:35,  3.53it/s] 69%|██████▉   | 275/400 [01:57<00:35,  3.54it/s] 69%|██████▉   | 276/400 [01:57<00:35,  3.54it/s] 69%|██████▉   | 277/400 [01:58<00:34,  3.55it/s] 70%|██████▉   | 278/400 [01:58<00:34,  3.55it/s] 70%|██████▉   | 279/400 [01:58<00:34,  3.55it/s] 70%|███████   | 280/400 [01:58<00:33,  3.55it/s] 70%|███████   | 281/400 [01:59<00:33,  3.55it/s] 70%|███████   | 282/400 [01:59<00:33,  3.55it/s] 71%|███████   | 283/400 [01:59<00:32,  3.55it/s] 71%|███████   | 284/400 [02:00<00:32,  3.55it/s] 71%|███████▏  | 285/400 [02:00<00:32,  3.54it/s] 72%|███████▏  | 286/400 [02:00<00:32,  3.54it/s] 72%|███████▏  | 287/400 [02:00<00:31,  3.54it/s] 72%|███████▏  | 288/400 [02:01<00:31,  3.55it/s] 72%|███████▏  | 289/400 [02:01<00:31,  3.55it/s] 72%|███████▎  | 290/400 [02:01<00:30,  3.55it/s] 73%|███████▎  | 291/400 [02:02<00:30,  3.56it/s] 73%|███████▎  | 292/400 [02:02<00:30,  3.57it/s] 73%|███████▎  | 293/400 [02:02<00:29,  3.58it/s] 74%|███████▎  | 294/400 [02:02<00:29,  3.59it/s] 74%|███████▍  | 295/400 [02:03<00:29,  3.59it/s] 74%|███████▍  | 296/400 [02:03<00:28,  3.59it/s] 74%|███████▍  | 297/400 [02:03<00:28,  3.59it/s] 74%|███████▍  | 298/400 [02:04<00:28,  3.59it/s] 75%|███████▍  | 299/400 [02:04<00:28,  3.60it/s] 75%|███████▌  | 300/400 [02:04<00:27,  3.60it/s] 75%|███████▌  | 301/400 [02:04<00:27,  3.60it/s] 76%|███████▌  | 302/400 [02:05<00:27,  3.60it/s] 76%|███████▌  | 303/400 [02:05<00:26,  3.60it/s] 76%|███████▌  | 304/400 [02:05<00:26,  3.60it/s] 76%|███████▋  | 305/400 [02:05<00:26,  3.60it/s] 76%|███████▋  | 306/400 [02:06<00:26,  3.60it/s] 77%|███████▋  | 307/400 [02:06<00:25,  3.59it/s] 77%|███████▋  | 308/400 [02:06<00:25,  3.59it/s] 77%|███████▋  | 309/400 [02:07<00:25,  3.59it/s] 78%|███████▊  | 310/400 [02:07<00:25,  3.59it/s] 78%|███████▊  | 311/400 [02:07<00:24,  3.59it/s] 78%|███████▊  | 312/400 [02:07<00:24,  3.60it/s] 78%|███████▊  | 313/400 [02:08<00:24,  3.60it/s] 78%|███████▊  | 314/400 [02:08<00:23,  3.60it/s] 79%|███████▉  | 315/400 [02:08<00:23,  3.60it/s] 79%|███████▉  | 316/400 [02:09<00:23,  3.60it/s] 79%|███████▉  | 317/400 [02:09<00:23,  3.60it/s] 80%|███████▉  | 318/400 [02:09<00:22,  3.59it/s] 80%|███████▉  | 319/400 [02:09<00:22,  3.59it/s] 80%|████████  | 320/400 [02:10<00:20,  3.93it/s][INFO|trainer.py:2140] 2023-08-28 06:56:26,176 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:56:26,176 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 06:56:26,176 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4499, 'eval_samples_per_second': 352.754, 'eval_steps_per_second': 44.105, 'epoch': 3.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.27it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.63it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.55it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.29it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.74it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.48it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.24it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.08it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.15it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.34it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.44it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.46it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.33it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.06it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.88it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.89it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.95it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.11it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.22it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.34it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.33it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.11it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.03it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 43.91it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.85it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.01it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.19it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.28it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.42it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.25it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.14it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.04it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 43.99it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.91it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 43.96it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.14it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.29it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.42it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.32it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.09it/s][A
 41%|████      | 207/505 [00:04<00:06, 43.91it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.05it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.05it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.01it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.09it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.29it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.33it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.23it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.04it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.07it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.01it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.01it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.06it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.23it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.28it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.18it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.14it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.06it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.04it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.05it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.00it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.09it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.21it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.26it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.20it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.15it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.06it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.02it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.00it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.04it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.04it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.18it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.13it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.17it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.16it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.10it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.15it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.05it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.08it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.13it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.18it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.16it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.19it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.17it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.14it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.12it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.07it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.09it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.11it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.16it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.20it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.10it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.14it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.11it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.20it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.12it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.16it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.23it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.17it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.18it/s][A                                                 
                                                 [A 80%|████████  | 320/400 [02:21<00:20,  3.93it/s]
100%|██████████| 505/505 [00:11<00:00, 44.18it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:56:37,641 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-320
[INFO|configuration_utils.py:351] 2023-08-28 06:56:37,663 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-320/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:56:39,709 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-320/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:56:39,741 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-320/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:56:39,760 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-320/special_tokens_map.json
 80%|████████  | 321/400 [02:24<05:49,  4.43s/it] 80%|████████  | 322/400 [02:24<04:08,  3.18s/it] 81%|████████  | 323/400 [02:24<02:58,  2.31s/it] 81%|████████  | 324/400 [02:25<02:09,  1.70s/it] 81%|████████▏ | 325/400 [02:25<01:35,  1.28s/it] 82%|████████▏ | 326/400 [02:25<01:12,  1.02it/s] 82%|████████▏ | 327/400 [02:25<00:56,  1.30it/s] 82%|████████▏ | 328/400 [02:26<00:44,  1.60it/s] 82%|████████▏ | 329/400 [02:26<00:37,  1.92it/s] 82%|████████▎ | 330/400 [02:26<00:31,  2.23it/s] 83%|████████▎ | 331/400 [02:27<00:27,  2.50it/s] 83%|████████▎ | 332/400 [02:27<00:24,  2.74it/s] 83%|████████▎ | 333/400 [02:27<00:22,  2.94it/s] 84%|████████▎ | 334/400 [02:27<00:21,  3.11it/s] 84%|████████▍ | 335/400 [02:28<00:20,  3.24it/s] 84%|████████▍ | 336/400 [02:28<00:19,  3.34it/s] 84%|████████▍ | 337/400 [02:28<00:18,  3.42it/s] 84%|████████▍ | 338/400 [02:28<00:17,  3.48it/s] 85%|████████▍ | 339/400 [02:29<00:17,  3.51it/s] 85%|████████▌ | 340/400 [02:29<00:16,  3.54it/s] 85%|████████▌ | 341/400 [02:29<00:16,  3.56it/s] 86%|████████▌ | 342/400 [02:30<00:16,  3.57it/s] 86%|████████▌ | 343/400 [02:30<00:15,  3.58it/s] 86%|████████▌ | 344/400 [02:30<00:15,  3.58it/s] 86%|████████▋ | 345/400 [02:30<00:15,  3.59it/s] 86%|████████▋ | 346/400 [02:31<00:15,  3.59it/s] 87%|████████▋ | 347/400 [02:31<00:14,  3.59it/s] 87%|████████▋ | 348/400 [02:31<00:14,  3.60it/s] 87%|████████▋ | 349/400 [02:32<00:14,  3.60it/s] 88%|████████▊ | 350/400 [02:32<00:13,  3.60it/s] 88%|████████▊ | 351/400 [02:32<00:13,  3.60it/s] 88%|████████▊ | 352/400 [02:32<00:13,  3.60it/s] 88%|████████▊ | 353/400 [02:33<00:13,  3.59it/s] 88%|████████▊ | 354/400 [02:33<00:12,  3.59it/s] 89%|████████▉ | 355/400 [02:33<00:12,  3.59it/s] 89%|████████▉ | 356/400 [02:33<00:12,  3.60it/s] 89%|████████▉ | 357/400 [02:34<00:11,  3.60it/s] 90%|████████▉ | 358/400 [02:34<00:11,  3.60it/s] 90%|████████▉ | 359/400 [02:34<00:11,  3.60it/s] 90%|█████████ | 360/400 [02:35<00:11,  3.60it/s] 90%|█████████ | 361/400 [02:35<00:10,  3.61it/s] 90%|█████████ | 362/400 [02:35<00:10,  3.59it/s] 91%|█████████ | 363/400 [02:35<00:10,  3.49it/s] 91%|█████████ | 364/400 [02:36<00:10,  3.50it/s] 91%|█████████▏| 365/400 [02:36<00:09,  3.53it/s] 92%|█████████▏| 366/400 [02:36<00:09,  3.55it/s] 92%|█████████▏| 367/400 [02:37<00:09,  3.56it/s] 92%|█████████▏| 368/400 [02:37<00:08,  3.58it/s] 92%|█████████▏| 369/400 [02:37<00:08,  3.58it/s] 92%|█████████▎| 370/400 [02:37<00:08,  3.59it/s] 93%|█████████▎| 371/400 [02:38<00:08,  3.59it/s] 93%|█████████▎| 372/400 [02:38<00:07,  3.60it/s] 93%|█████████▎| 373/400 [02:38<00:07,  3.60it/s] 94%|█████████▎| 374/400 [02:39<00:07,  3.60it/s] 94%|█████████▍| 375/400 [02:39<00:06,  3.59it/s] 94%|█████████▍| 376/400 [02:39<00:06,  3.60it/s] 94%|█████████▍| 377/400 [02:39<00:06,  3.60it/s] 94%|█████████▍| 378/400 [02:40<00:06,  3.60it/s] 95%|█████████▍| 379/400 [02:40<00:05,  3.60it/s] 95%|█████████▌| 380/400 [02:40<00:05,  3.60it/s] 95%|█████████▌| 381/400 [02:40<00:05,  3.60it/s] 96%|█████████▌| 382/400 [02:41<00:05,  3.60it/s] 96%|█████████▌| 383/400 [02:41<00:04,  3.60it/s] 96%|█████████▌| 384/400 [02:41<00:04,  3.60it/s] 96%|█████████▋| 385/400 [02:42<00:04,  3.60it/s] 96%|█████████▋| 386/400 [02:42<00:03,  3.58it/s] 97%|█████████▋| 387/400 [02:42<00:03,  3.59it/s] 97%|█████████▋| 388/400 [02:42<00:03,  3.59it/s] 97%|█████████▋| 389/400 [02:43<00:03,  3.60it/s] 98%|█████████▊| 390/400 [02:43<00:02,  3.60it/s] 98%|█████████▊| 391/400 [02:43<00:02,  3.60it/s] 98%|█████████▊| 392/400 [02:44<00:02,  3.60it/s] 98%|█████████▊| 393/400 [02:44<00:01,  3.60it/s] 98%|█████████▊| 394/400 [02:44<00:01,  3.59it/s] 99%|█████████▉| 395/400 [02:44<00:01,  3.60it/s] 99%|█████████▉| 396/400 [02:45<00:01,  3.60it/s] 99%|█████████▉| 397/400 [02:45<00:00,  3.60it/s]100%|█████████▉| 398/400 [02:45<00:00,  3.60it/s]100%|█████████▉| 399/400 [02:45<00:00,  3.60it/s]100%|██████████| 400/400 [02:46<00:00,  3.94it/s][INFO|trainer.py:2140] 2023-08-28 06:57:02,291 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:57:02,291 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 06:57:02,291 >>   Batch size = 8
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4431, 'eval_samples_per_second': 352.962, 'eval_steps_per_second': 44.131, 'epoch': 4.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.79it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.57it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.64it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.43it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.89it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.54it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.28it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.03it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.09it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.23it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.39it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.40it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.43it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.21it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.14it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.03it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.98it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 43.96it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.07it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.25it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.40it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.36it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.24it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.11it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.00it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.98it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.00it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.12it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.28it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.33it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.31it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.09it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.10it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.93it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 43.98it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.01it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.07it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.17it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.29it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.32it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.21it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.14it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.06it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.06it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.10it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.12it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.21it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.29it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.31it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.25it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.05it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 43.99it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 43.99it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.06it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.03it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.19it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.30it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.32it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.22it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.14it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.12it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.02it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.10it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.08it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.13it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.27it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.23it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.23it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.15it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.12it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.17it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.13it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.20it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.23it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.18it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.20it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.22it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.15it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.06it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.11it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.10it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.14it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.28it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.23it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.18it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.18it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.15it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.07it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.15it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.15it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.22it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.23it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.21it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.08it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 43.99it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.12it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.06it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.17it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.18it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.24it/s][A                                                 
                                                 [A100%|██████████| 400/400 [02:57<00:00,  3.94it/s]
100%|██████████| 505/505 [00:11<00:00, 44.24it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:57:13,739 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-400
[INFO|configuration_utils.py:351] 2023-08-28 06:57:13,759 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-400/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:57:15,586 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-400/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:57:15,602 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:57:15,617 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-400/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 06:57:15,898 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 06:57:15,898 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-80 (score: 1.2323094606399536).
                                                 100%|██████████| 400/400 [03:01<00:00,  3.94it/s]100%|██████████| 400/400 [03:01<00:00,  2.20it/s]
[INFO|trainer.py:1894] 2023-08-28 06:57:17,685 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 06:57:17,700 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:57:19,506 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:57:19,524 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:57:19,533 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:57:19,734 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:57:19,734 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:57:19,734 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:57:19,734 >>   train_runtime            = 0:03:01.56
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:57:19,734 >>   train_samples            =       5100
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:57:19,734 >>   train_samples_per_second =    140.442
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:57:19,734 >>   train_steps_per_second   =      2.203
{'eval_loss': 1.2323094606399536, 'eval_runtime': 11.4357, 'eval_samples_per_second': 353.191, 'eval_steps_per_second': 44.16, 'epoch': 5.0}
{'train_runtime': 181.57, 'train_samples_per_second': 140.442, 'train_steps_per_second': 2.203, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 06:57:19 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 06:57:19,827 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:57:19,827 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 06:57:19,827 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:08, 56.06it/s]  2%|▏         | 12/505 [00:00<00:10, 48.88it/s]  3%|▎         | 17/505 [00:00<00:10, 47.13it/s]  4%|▍         | 22/505 [00:00<00:10, 46.25it/s]  5%|▌         | 27/505 [00:00<00:10, 45.87it/s]  6%|▋         | 32/505 [00:00<00:10, 45.48it/s]  7%|▋         | 37/505 [00:00<00:10, 45.24it/s]  8%|▊         | 42/505 [00:00<00:10, 44.70it/s]  9%|▉         | 47/505 [00:01<00:10, 44.09it/s] 10%|█         | 52/505 [00:01<00:10, 44.00it/s] 11%|█▏        | 57/505 [00:01<00:10, 44.13it/s] 12%|█▏        | 62/505 [00:01<00:10, 44.25it/s] 13%|█▎        | 67/505 [00:01<00:09, 44.32it/s] 14%|█▍        | 72/505 [00:01<00:09, 44.46it/s] 15%|█▌        | 77/505 [00:01<00:09, 44.48it/s] 16%|█▌        | 82/505 [00:01<00:09, 44.64it/s] 17%|█▋        | 87/505 [00:01<00:09, 44.40it/s] 18%|█▊        | 92/505 [00:02<00:09, 44.05it/s] 19%|█▉        | 97/505 [00:02<00:09, 44.01it/s] 20%|██        | 102/505 [00:02<00:09, 44.17it/s] 21%|██        | 107/505 [00:02<00:08, 44.28it/s] 22%|██▏       | 112/505 [00:02<00:08, 44.44it/s] 23%|██▎       | 117/505 [00:02<00:08, 44.43it/s] 24%|██▍       | 122/505 [00:02<00:08, 44.48it/s] 25%|██▌       | 127/505 [00:02<00:08, 44.54it/s] 26%|██▌       | 132/505 [00:02<00:08, 44.34it/s] 27%|██▋       | 137/505 [00:03<00:08, 44.12it/s] 28%|██▊       | 142/505 [00:03<00:08, 44.14it/s] 29%|██▉       | 147/505 [00:03<00:08, 44.02it/s] 30%|███       | 152/505 [00:03<00:07, 44.18it/s] 31%|███       | 157/505 [00:03<00:07, 44.36it/s] 32%|███▏      | 162/505 [00:03<00:07, 44.42it/s] 33%|███▎      | 167/505 [00:03<00:07, 44.53it/s] 34%|███▍      | 172/505 [00:03<00:07, 44.43it/s] 35%|███▌      | 177/505 [00:03<00:07, 44.19it/s] 36%|███▌      | 182/505 [00:04<00:07, 44.06it/s] 37%|███▋      | 187/505 [00:04<00:07, 44.10it/s] 38%|███▊      | 192/505 [00:04<00:07, 44.12it/s] 39%|███▉      | 197/505 [00:04<00:06, 44.24it/s] 40%|████      | 202/505 [00:04<00:06, 44.39it/s] 41%|████      | 207/505 [00:04<00:06, 44.50it/s] 42%|████▏     | 212/505 [00:04<00:06, 44.52it/s] 43%|████▎     | 217/505 [00:04<00:06, 44.41it/s] 44%|████▍     | 222/505 [00:04<00:06, 44.16it/s] 45%|████▍     | 227/505 [00:05<00:06, 44.12it/s] 46%|████▌     | 232/505 [00:05<00:06, 44.12it/s] 47%|████▋     | 237/505 [00:05<00:06, 44.14it/s] 48%|████▊     | 242/505 [00:05<00:05, 44.23it/s] 49%|████▉     | 247/505 [00:05<00:05, 44.29it/s] 50%|████▉     | 252/505 [00:05<00:05, 44.40it/s] 51%|█████     | 257/505 [00:05<00:05, 44.42it/s] 52%|█████▏    | 262/505 [00:05<00:05, 44.36it/s] 53%|█████▎    | 267/505 [00:05<00:05, 44.14it/s] 54%|█████▍    | 272/505 [00:06<00:05, 44.23it/s] 55%|█████▍    | 277/505 [00:06<00:05, 44.22it/s] 56%|█████▌    | 282/505 [00:06<00:05, 44.14it/s] 57%|█████▋    | 287/505 [00:06<00:04, 44.29it/s] 58%|█████▊    | 292/505 [00:06<00:04, 44.35it/s] 59%|█████▉    | 297/505 [00:06<00:04, 44.42it/s] 60%|█████▉    | 302/505 [00:06<00:04, 44.41it/s] 61%|██████    | 307/505 [00:06<00:04, 44.24it/s] 62%|██████▏   | 312/505 [00:07<00:04, 44.20it/s] 63%|██████▎   | 317/505 [00:07<00:04, 44.22it/s] 64%|██████▍   | 322/505 [00:07<00:04, 44.20it/s] 65%|██████▍   | 327/505 [00:07<00:04, 44.19it/s] 66%|██████▌   | 332/505 [00:07<00:03, 44.29it/s] 67%|██████▋   | 337/505 [00:07<00:03, 44.41it/s] 68%|██████▊   | 342/505 [00:07<00:03, 44.39it/s] 69%|██████▊   | 347/505 [00:07<00:03, 44.37it/s] 70%|██████▉   | 352/505 [00:07<00:03, 44.25it/s] 71%|███████   | 357/505 [00:08<00:03, 44.13it/s] 72%|███████▏  | 362/505 [00:08<00:03, 44.22it/s] 73%|███████▎  | 367/505 [00:08<00:03, 44.17it/s] 74%|███████▎  | 372/505 [00:08<00:03, 44.23it/s] 75%|███████▍  | 377/505 [00:08<00:02, 44.30it/s] 76%|███████▌  | 382/505 [00:08<00:02, 44.38it/s] 77%|███████▋  | 387/505 [00:08<00:02, 44.36it/s] 78%|███████▊  | 392/505 [00:08<00:02, 44.29it/s] 79%|███████▊  | 397/505 [00:08<00:02, 44.24it/s] 80%|███████▉  | 402/505 [00:09<00:02, 44.19it/s] 81%|████████  | 407/505 [00:09<00:02, 44.16it/s] 82%|████████▏ | 412/505 [00:09<00:02, 44.12it/s] 83%|████████▎ | 417/505 [00:09<00:01, 44.21it/s] 84%|████████▎ | 422/505 [00:09<00:01, 44.28it/s] 85%|████████▍ | 427/505 [00:09<00:01, 44.32it/s] 86%|████████▌ | 432/505 [00:09<00:01, 44.38it/s] 87%|████████▋ | 437/505 [00:09<00:01, 44.27it/s] 88%|████████▊ | 442/505 [00:09<00:01, 44.36it/s] 89%|████████▊ | 447/505 [00:10<00:01, 44.24it/s] 90%|████████▉ | 452/505 [00:10<00:01, 44.20it/s] 90%|█████████ | 457/505 [00:10<00:01, 44.19it/s] 91%|█████████▏| 462/505 [00:10<00:00, 44.20it/s] 92%|█████████▏| 467/505 [00:10<00:00, 44.20it/s] 93%|█████████▎| 472/505 [00:10<00:00, 44.36it/s] 94%|█████████▍| 477/505 [00:10<00:00, 44.38it/s] 95%|█████████▌| 482/505 [00:10<00:00, 44.34it/s] 96%|█████████▋| 487/505 [00:10<00:00, 44.03it/s] 97%|█████████▋| 492/505 [00:11<00:00, 44.27it/s] 98%|█████████▊| 497/505 [00:11<00:00, 44.28it/s] 99%|█████████▉| 502/505 [00:11<00:00, 44.24it/s]100%|██████████| 505/505 [00:11<00:00, 44.35it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:57:31,230 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:57:31,230 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:57:31,230 >>   eval_loss               =     1.2323
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:57:31,230 >>   eval_runtime            = 0:00:11.40
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:57:31,230 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:57:31,230 >>   eval_samples_per_second =    354.201
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:57:31,230 >>   eval_steps_per_second   =     44.286
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:57:31,230 >>   perplexity              =     3.4291
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:37,557 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:37,559 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:37,559 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:37,560 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:37,560 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:57:38,147 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:57:38,148 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:57:38,726 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:57:39,732 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:57:39,732 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:42,086 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:42,090 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:42,090 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:42,090 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:42,090 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:57:42,398 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:57:42,399 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:57:42,666 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:57:42,827 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:57:42,827 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-240
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-400
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-160
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-320
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-80
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.62it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.70it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.69it/s]Extractor Predicting: 8it [00:04,  1.72it/s]Extractor Predicting: 9it [00:05,  1.71it/s]Extractor Predicting: 10it [00:05,  1.73it/s]Extractor Predicting: 11it [00:06,  1.72it/s]Extractor Predicting: 12it [00:07,  1.67it/s]Extractor Predicting: 13it [00:07,  1.67it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:08,  1.68it/s]Extractor Predicting: 16it [00:09,  1.66it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:10,  1.70it/s]Extractor Predicting: 19it [00:11,  1.69it/s]Extractor Predicting: 20it [00:11,  1.68it/s]Extractor Predicting: 21it [00:12,  1.69it/s]Extractor Predicting: 22it [00:13,  1.67it/s]Extractor Predicting: 23it [00:13,  1.63it/s]Extractor Predicting: 24it [00:14,  1.64it/s]Extractor Predicting: 25it [00:14,  1.68it/s]Extractor Predicting: 26it [00:15,  1.70it/s]Extractor Predicting: 27it [00:16,  1.72it/s]Extractor Predicting: 28it [00:16,  1.70it/s]Extractor Predicting: 29it [00:17,  1.72it/s]Extractor Predicting: 30it [00:17,  1.68it/s]Extractor Predicting: 31it [00:18,  1.71it/s]Extractor Predicting: 32it [00:19,  1.69it/s]Extractor Predicting: 33it [00:19,  1.68it/s]Extractor Predicting: 34it [00:20,  1.67it/s]Extractor Predicting: 35it [00:20,  1.66it/s]Extractor Predicting: 36it [00:21,  1.73it/s]Extractor Predicting: 37it [00:21,  1.79it/s]Extractor Predicting: 38it [00:22,  1.75it/s]Extractor Predicting: 39it [00:23,  1.78it/s]Extractor Predicting: 40it [00:23,  1.75it/s]Extractor Predicting: 41it [00:24,  1.73it/s]Extractor Predicting: 42it [00:24,  1.73it/s]Extractor Predicting: 43it [00:25,  1.65it/s]Extractor Predicting: 44it [00:26,  1.67it/s]Extractor Predicting: 45it [00:26,  1.71it/s]Extractor Predicting: 46it [00:27,  1.69it/s]Extractor Predicting: 47it [00:27,  1.68it/s]Extractor Predicting: 48it [00:28,  1.60it/s]Extractor Predicting: 49it [00:29,  1.66it/s]Extractor Predicting: 50it [00:29,  1.70it/s]Extractor Predicting: 51it [00:30,  1.73it/s]Extractor Predicting: 52it [00:30,  1.78it/s]Extractor Predicting: 53it [00:31,  1.77it/s]Extractor Predicting: 54it [00:31,  1.77it/s]Extractor Predicting: 55it [00:32,  1.76it/s]Extractor Predicting: 56it [00:32,  1.76it/s]Extractor Predicting: 57it [00:33,  1.77it/s]Extractor Predicting: 58it [00:34,  1.71it/s]Extractor Predicting: 59it [00:34,  1.71it/s]Extractor Predicting: 60it [00:35,  1.71it/s]Extractor Predicting: 61it [00:35,  1.76it/s]Extractor Predicting: 62it [00:36,  1.76it/s]Extractor Predicting: 63it [00:37,  1.71it/s]Extractor Predicting: 64it [00:37,  1.71it/s]Extractor Predicting: 65it [00:38,  1.69it/s]Extractor Predicting: 66it [00:38,  1.72it/s]Extractor Predicting: 67it [00:39,  1.71it/s]Extractor Predicting: 68it [00:40,  1.67it/s]Extractor Predicting: 69it [00:40,  1.63it/s]Extractor Predicting: 70it [00:41,  1.67it/s]Extractor Predicting: 71it [00:41,  1.66it/s]Extractor Predicting: 72it [00:42,  1.62it/s]Extractor Predicting: 73it [00:43,  1.61it/s]Extractor Predicting: 74it [00:43,  1.56it/s]Extractor Predicting: 75it [00:44,  1.59it/s]Extractor Predicting: 76it [00:45,  1.63it/s]Extractor Predicting: 77it [00:45,  1.63it/s]Extractor Predicting: 78it [00:46,  1.64it/s]Extractor Predicting: 79it [00:46,  1.65it/s]Extractor Predicting: 80it [00:47,  1.66it/s]Extractor Predicting: 81it [00:48,  1.67it/s]Extractor Predicting: 82it [00:48,  1.72it/s]Extractor Predicting: 83it [00:49,  1.67it/s]Extractor Predicting: 84it [00:49,  1.69it/s]Extractor Predicting: 85it [00:50,  1.68it/s]Extractor Predicting: 86it [00:50,  1.70it/s]Extractor Predicting: 87it [00:51,  1.73it/s]Extractor Predicting: 88it [00:52,  1.74it/s]Extractor Predicting: 89it [00:52,  1.72it/s]Extractor Predicting: 90it [00:53,  1.74it/s]Extractor Predicting: 91it [00:53,  1.74it/s]Extractor Predicting: 92it [00:54,  1.76it/s]Extractor Predicting: 93it [00:54,  1.74it/s]Extractor Predicting: 94it [00:55,  1.72it/s]Extractor Predicting: 95it [00:56,  1.70it/s]Extractor Predicting: 96it [00:56,  1.68it/s]Extractor Predicting: 97it [00:57,  1.68it/s]Extractor Predicting: 98it [00:57,  1.68it/s]Extractor Predicting: 99it [00:58,  1.67it/s]Extractor Predicting: 100it [00:59,  1.67it/s]Extractor Predicting: 101it [00:59,  1.64it/s]Extractor Predicting: 102it [01:00,  1.65it/s]Extractor Predicting: 103it [01:00,  1.66it/s]Extractor Predicting: 104it [01:01,  1.70it/s]Extractor Predicting: 105it [01:02,  1.69it/s]Extractor Predicting: 106it [01:02,  1.73it/s]Extractor Predicting: 107it [01:03,  1.73it/s]Extractor Predicting: 108it [01:03,  1.74it/s]Extractor Predicting: 109it [01:04,  1.75it/s]Extractor Predicting: 110it [01:04,  1.72it/s]Extractor Predicting: 111it [01:05,  1.68it/s]Extractor Predicting: 112it [01:06,  1.73it/s]Extractor Predicting: 113it [01:06,  1.71it/s]Extractor Predicting: 114it [01:07,  1.69it/s]Extractor Predicting: 115it [01:07,  1.74it/s]Extractor Predicting: 116it [01:08,  1.74it/s]Extractor Predicting: 117it [01:09,  1.73it/s]Extractor Predicting: 118it [01:09,  1.51it/s]Extractor Predicting: 119it [01:10,  1.58it/s]Extractor Predicting: 120it [01:11,  1.62it/s]Extractor Predicting: 121it [01:11,  1.61it/s]Extractor Predicting: 122it [01:12,  1.63it/s]Extractor Predicting: 123it [01:12,  1.67it/s]Extractor Predicting: 124it [01:13,  1.67it/s]Extractor Predicting: 125it [01:14,  1.52it/s]Extractor Predicting: 126it [01:14,  1.57it/s]Extractor Predicting: 127it [01:15,  1.53it/s]Extractor Predicting: 128it [01:16,  1.51it/s]Extractor Predicting: 129it [01:16,  1.53it/s]Extractor Predicting: 130it [01:17,  1.53it/s]Extractor Predicting: 131it [01:18,  1.50it/s]Extractor Predicting: 132it [01:18,  1.50it/s]Extractor Predicting: 133it [01:19,  1.52it/s]Extractor Predicting: 134it [01:20,  1.57it/s]Extractor Predicting: 135it [01:20,  1.59it/s]Extractor Predicting: 136it [01:21,  1.59it/s]Extractor Predicting: 137it [01:21,  1.62it/s]Extractor Predicting: 138it [01:22,  1.57it/s]Extractor Predicting: 139it [01:23,  1.57it/s]Extractor Predicting: 140it [01:23,  1.56it/s]Extractor Predicting: 141it [01:24,  1.57it/s]Extractor Predicting: 142it [01:25,  1.58it/s]Extractor Predicting: 143it [01:25,  1.58it/s]Extractor Predicting: 144it [01:26,  1.59it/s]Extractor Predicting: 145it [01:27,  1.57it/s]Extractor Predicting: 146it [01:27,  1.58it/s]Extractor Predicting: 147it [01:28,  1.60it/s]Extractor Predicting: 148it [01:28,  1.54it/s]Extractor Predicting: 149it [01:29,  1.58it/s]Extractor Predicting: 149it [01:29,  1.66it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:59:19,945 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:59:19,950 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:59:19,950 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:59:19,950 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:59:19,950 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:59:20,663 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:59:20,664 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:59:21,272 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:59:22,534 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:59:22,535 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:59:25,407 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:59:25,412 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:59:25,412 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:59:25,412 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:59:25,412 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:59:26,076 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:59:26,077 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:59:26,651 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:59:26,809 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:59:26,809 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.76it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.73it/s]Extractor Predicting: 4it [00:02,  1.82it/s]Extractor Predicting: 5it [00:02,  1.78it/s]Extractor Predicting: 6it [00:03,  1.78it/s]Extractor Predicting: 7it [00:03,  1.78it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.80it/s]Extractor Predicting: 10it [00:05,  1.82it/s]Extractor Predicting: 11it [00:06,  1.84it/s]Extractor Predicting: 12it [00:06,  1.87it/s]Extractor Predicting: 13it [00:07,  1.97it/s]Extractor Predicting: 14it [00:07,  1.95it/s]Extractor Predicting: 15it [00:08,  1.97it/s]Extractor Predicting: 16it [00:08,  1.98it/s]Extractor Predicting: 17it [00:09,  1.98it/s]Extractor Predicting: 18it [00:09,  1.93it/s]Extractor Predicting: 19it [00:10,  1.89it/s]Extractor Predicting: 20it [00:10,  1.89it/s]Extractor Predicting: 21it [00:11,  1.90it/s]Extractor Predicting: 22it [00:11,  1.93it/s]Extractor Predicting: 23it [00:12,  1.96it/s]Extractor Predicting: 24it [00:12,  1.94it/s]Extractor Predicting: 25it [00:13,  1.97it/s]Extractor Predicting: 26it [00:13,  1.91it/s]Extractor Predicting: 27it [00:14,  1.98it/s]Extractor Predicting: 28it [00:14,  2.00it/s]Extractor Predicting: 29it [00:15,  1.97it/s]Extractor Predicting: 30it [00:15,  1.97it/s]Extractor Predicting: 31it [00:16,  1.99it/s]Extractor Predicting: 32it [00:16,  1.99it/s]Extractor Predicting: 33it [00:17,  1.93it/s]Extractor Predicting: 34it [00:17,  1.89it/s]Extractor Predicting: 35it [00:18,  1.87it/s]Extractor Predicting: 36it [00:18,  1.93it/s]Extractor Predicting: 37it [00:19,  1.90it/s]Extractor Predicting: 38it [00:20,  1.88it/s]Extractor Predicting: 39it [00:20,  1.89it/s]Extractor Predicting: 40it [00:21,  1.86it/s]Extractor Predicting: 41it [00:21,  1.84it/s]Extractor Predicting: 42it [00:22,  1.87it/s]Extractor Predicting: 43it [00:22,  1.90it/s]Extractor Predicting: 44it [00:23,  1.89it/s]Extractor Predicting: 45it [00:23,  1.71it/s]Extractor Predicting: 46it [00:24,  1.70it/s]Extractor Predicting: 47it [00:25,  1.73it/s]Extractor Predicting: 48it [00:25,  1.76it/s]Extractor Predicting: 49it [00:26,  1.81it/s]Extractor Predicting: 50it [00:26,  1.83it/s]Extractor Predicting: 51it [00:27,  1.88it/s]Extractor Predicting: 52it [00:27,  1.85it/s]Extractor Predicting: 53it [00:28,  1.84it/s]Extractor Predicting: 54it [00:28,  1.85it/s]Extractor Predicting: 55it [00:29,  1.83it/s]Extractor Predicting: 56it [00:30,  1.77it/s]Extractor Predicting: 57it [00:30,  1.78it/s]Extractor Predicting: 58it [00:31,  1.81it/s]Extractor Predicting: 59it [00:31,  1.78it/s]Extractor Predicting: 60it [00:32,  1.79it/s]Extractor Predicting: 61it [00:32,  1.79it/s]Extractor Predicting: 62it [00:33,  1.84it/s]Extractor Predicting: 63it [00:33,  1.81it/s]Extractor Predicting: 64it [00:34,  1.83it/s]Extractor Predicting: 65it [00:34,  1.87it/s]Extractor Predicting: 66it [00:35,  1.87it/s]Extractor Predicting: 67it [00:36,  1.85it/s]Extractor Predicting: 68it [00:36,  1.85it/s]Extractor Predicting: 69it [00:37,  1.81it/s]Extractor Predicting: 70it [00:37,  1.79it/s]Extractor Predicting: 71it [00:38,  1.79it/s]Extractor Predicting: 72it [00:38,  1.80it/s]Extractor Predicting: 73it [00:39,  1.79it/s]Extractor Predicting: 74it [00:39,  1.80it/s]Extractor Predicting: 75it [00:40,  1.83it/s]Extractor Predicting: 76it [00:41,  1.80it/s]Extractor Predicting: 77it [00:41,  1.82it/s]Extractor Predicting: 78it [00:42,  1.75it/s]Extractor Predicting: 79it [00:42,  1.79it/s]Extractor Predicting: 80it [00:43,  1.84it/s]Extractor Predicting: 81it [00:43,  1.80it/s]Extractor Predicting: 82it [00:44,  1.76it/s]Extractor Predicting: 83it [00:45,  1.76it/s]Extractor Predicting: 84it [00:45,  1.78it/s]Extractor Predicting: 85it [00:46,  1.77it/s]Extractor Predicting: 86it [00:46,  1.75it/s]Extractor Predicting: 87it [00:47,  1.80it/s]Extractor Predicting: 88it [00:47,  1.65it/s]Extractor Predicting: 89it [00:48,  1.74it/s]Extractor Predicting: 90it [00:48,  1.77it/s]Extractor Predicting: 91it [00:49,  1.78it/s]Extractor Predicting: 92it [00:50,  1.79it/s]Extractor Predicting: 93it [00:50,  1.84it/s]Extractor Predicting: 94it [00:51,  1.86it/s]Extractor Predicting: 95it [00:51,  1.83it/s]Extractor Predicting: 96it [00:52,  1.88it/s]Extractor Predicting: 97it [00:52,  1.90it/s]Extractor Predicting: 98it [00:53,  1.87it/s]Extractor Predicting: 99it [00:53,  1.85it/s]Extractor Predicting: 100it [00:54,  1.89it/s]Extractor Predicting: 101it [00:54,  1.89it/s]Extractor Predicting: 102it [00:55,  1.83it/s]Extractor Predicting: 103it [00:55,  1.81it/s]Extractor Predicting: 104it [00:56,  1.85it/s]Extractor Predicting: 105it [00:57,  1.79it/s]Extractor Predicting: 106it [00:57,  1.83it/s]Extractor Predicting: 107it [00:58,  1.85it/s]Extractor Predicting: 108it [00:58,  1.81it/s]Extractor Predicting: 109it [00:59,  1.81it/s]Extractor Predicting: 110it [00:59,  1.82it/s]Extractor Predicting: 111it [01:00,  1.82it/s]Extractor Predicting: 112it [01:00,  1.82it/s]Extractor Predicting: 113it [01:01,  1.87it/s]Extractor Predicting: 114it [01:01,  1.85it/s]Extractor Predicting: 115it [01:02,  1.84it/s]Extractor Predicting: 116it [01:03,  1.88it/s]Extractor Predicting: 117it [01:03,  1.89it/s]Extractor Predicting: 118it [01:04,  1.95it/s]Extractor Predicting: 119it [01:04,  1.97it/s]Extractor Predicting: 120it [01:05,  1.96it/s]Extractor Predicting: 121it [01:05,  1.95it/s]Extractor Predicting: 122it [01:06,  1.93it/s]Extractor Predicting: 123it [01:06,  1.91it/s]Extractor Predicting: 124it [01:07,  1.91it/s]Extractor Predicting: 125it [01:07,  1.90it/s]Extractor Predicting: 126it [01:08,  1.87it/s]Extractor Predicting: 127it [01:08,  1.83it/s]Extractor Predicting: 128it [01:09,  1.79it/s]Extractor Predicting: 129it [01:09,  1.77it/s]Extractor Predicting: 130it [01:10,  1.75it/s]Extractor Predicting: 131it [01:11,  1.73it/s]Extractor Predicting: 132it [01:11,  1.75it/s]Extractor Predicting: 133it [01:12,  1.76it/s]Extractor Predicting: 134it [01:12,  1.80it/s]Extractor Predicting: 135it [01:13,  1.73it/s]Extractor Predicting: 136it [01:14,  1.73it/s]Extractor Predicting: 137it [01:14,  1.72it/s]Extractor Predicting: 138it [01:15,  1.74it/s]Extractor Predicting: 139it [01:15,  1.74it/s]Extractor Predicting: 140it [01:16,  1.71it/s]Extractor Predicting: 141it [01:16,  1.71it/s]Extractor Predicting: 142it [01:17,  1.71it/s]Extractor Predicting: 143it [01:18,  1.73it/s]Extractor Predicting: 144it [01:18,  1.72it/s]Extractor Predicting: 145it [01:19,  1.71it/s]Extractor Predicting: 146it [01:19,  1.72it/s]Extractor Predicting: 147it [01:20,  1.73it/s]Extractor Predicting: 148it [01:20,  1.74it/s]Extractor Predicting: 149it [01:21,  1.77it/s]Extractor Predicting: 150it [01:22,  1.77it/s]Extractor Predicting: 151it [01:22,  1.75it/s]Extractor Predicting: 152it [01:23,  1.75it/s]Extractor Predicting: 153it [01:23,  1.71it/s]Extractor Predicting: 154it [01:24,  1.71it/s]Extractor Predicting: 155it [01:24,  1.73it/s]Extractor Predicting: 156it [01:25,  1.79it/s]Extractor Predicting: 157it [01:25,  1.86it/s]Extractor Predicting: 158it [01:26,  1.87it/s]Extractor Predicting: 159it [01:27,  1.87it/s]Extractor Predicting: 160it [01:27,  1.88it/s]Extractor Predicting: 161it [01:28,  1.92it/s]Extractor Predicting: 162it [01:28,  1.87it/s]Extractor Predicting: 163it [01:29,  1.85it/s]Extractor Predicting: 164it [01:29,  1.87it/s]Extractor Predicting: 165it [01:30,  1.93it/s]Extractor Predicting: 166it [01:30,  1.91it/s]Extractor Predicting: 167it [01:31,  1.99it/s]Extractor Predicting: 168it [01:31,  2.00it/s]Extractor Predicting: 169it [01:32,  2.04it/s]Extractor Predicting: 170it [01:32,  1.97it/s]Extractor Predicting: 171it [01:33,  1.88it/s]Extractor Predicting: 172it [01:33,  1.83it/s]Extractor Predicting: 173it [01:34,  1.86it/s]Extractor Predicting: 173it [01:34,  1.83it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:08,726 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:08,730 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:08,730 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:08,730 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:08,730 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:01:09,319 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:01:09,320 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:01:09,896 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:01:10,906 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:01:10,906 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:13,770 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:13,772 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:13,772 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:13,773 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:13,773 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:01:14,411 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:01:14,413 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:01:14,971 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:01:15,134 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:01:15,139 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.80it/s]Extractor Predicting: 2it [00:01,  1.70it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:02,  1.68it/s]Extractor Predicting: 6it [00:03,  1.66it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:04,  1.57it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.60it/s]Extractor Predicting: 11it [00:06,  1.58it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.70it/s]Extractor Predicting: 15it [00:09,  1.79it/s]Extractor Predicting: 16it [00:09,  1.90it/s]Extractor Predicting: 17it [00:09,  2.01it/s]Extractor Predicting: 18it [00:10,  2.06it/s]Extractor Predicting: 19it [00:10,  2.07it/s]Extractor Predicting: 20it [00:11,  2.11it/s]Extractor Predicting: 21it [00:11,  2.15it/s]Extractor Predicting: 22it [00:12,  2.10it/s]Extractor Predicting: 23it [00:12,  2.09it/s]Extractor Predicting: 24it [00:13,  2.08it/s]Extractor Predicting: 25it [00:13,  2.09it/s]Extractor Predicting: 26it [00:14,  2.13it/s]Extractor Predicting: 27it [00:14,  2.08it/s]Extractor Predicting: 28it [00:15,  2.10it/s]Extractor Predicting: 29it [00:15,  2.11it/s]Extractor Predicting: 30it [00:16,  2.16it/s]Extractor Predicting: 31it [00:16,  2.19it/s]Extractor Predicting: 32it [00:16,  2.17it/s]Extractor Predicting: 33it [00:17,  2.17it/s]Extractor Predicting: 34it [00:17,  2.16it/s]Extractor Predicting: 35it [00:18,  2.16it/s]Extractor Predicting: 36it [00:18,  2.09it/s]Extractor Predicting: 37it [00:19,  2.05it/s]Extractor Predicting: 38it [00:19,  2.13it/s]Extractor Predicting: 39it [00:20,  2.12it/s]Extractor Predicting: 40it [00:20,  2.12it/s]Extractor Predicting: 41it [00:21,  2.11it/s]Extractor Predicting: 42it [00:21,  2.16it/s]Extractor Predicting: 43it [00:22,  2.11it/s]Extractor Predicting: 44it [00:22,  1.93it/s]Extractor Predicting: 45it [00:23,  1.81it/s]Extractor Predicting: 46it [00:24,  1.73it/s]Extractor Predicting: 47it [00:24,  1.71it/s]Extractor Predicting: 48it [00:25,  1.68it/s]Extractor Predicting: 49it [00:25,  1.67it/s]Extractor Predicting: 50it [00:26,  1.65it/s]Extractor Predicting: 51it [00:27,  1.64it/s]Extractor Predicting: 52it [00:27,  1.63it/s]Extractor Predicting: 53it [00:28,  1.62it/s]Extractor Predicting: 54it [00:28,  1.61it/s]Extractor Predicting: 55it [00:29,  1.64it/s]Extractor Predicting: 56it [00:30,  1.61it/s]Extractor Predicting: 57it [00:30,  1.60it/s]Extractor Predicting: 58it [00:31,  1.61it/s]Extractor Predicting: 59it [00:31,  1.91it/s]Extractor Predicting: 59it [00:31,  1.86it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_5_seed_4', 'type': 'filtered', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', data_dir='outputs/wrapper/wiki/unseen_5_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:16<02:26, 16.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:32<02:11, 16.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:00<02:30, 21.48s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:16<01:56, 19.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:33<01:32, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:48<01:09, 17.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:04<00:50, 16.74s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:17<00:31, 15.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:31<00:15, 15.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:45<00:00, 14.96s/it]Generating: 100%|██████████| 10/10 [02:45<00:00, 16.59s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : given name .', 'success_rate': 0.80859375, 'errors': {'', "('William', 'given name', '', 'He was born in the province of Oise , his father being William , Duke of Normandy , 1st Earl of Normandy , and his mother being Mary .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 201, 'raw': 288}
{'target': 600, 'success': 226, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 287, 'raw': 416}
{'target': 600, 'success': 310, 'raw': 448}
{'target': 600, 'success': 333, 'raw': 480}
{'target': 600, 'success': 351, 'raw': 512}
{'target': 600, 'success': 377, 'raw': 544}
{'target': 600, 'success': 403, 'raw': 576}
{'target': 600, 'success': 424, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 468, 'raw': 672}
{'target': 600, 'success': 486, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 532, 'raw': 768}
{'target': 600, 'success': 554, 'raw': 800}
{'target': 600, 'success': 576, 'raw': 832}
{'target': 600, 'success': 601, 'raw': 864}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.6956018518518519, 'errors': {'', "('Brazil', 'languages spoken, written or signed', '', 'Many languages are spoken in Brazil , and there are various communities and organizations advocating for a stronger free and open society .')", 'not enough values to unpack (expected 2, got 1)', "('Estonia', 'languages spoken, written or signed', '', 'The language of Estonia is Estonian , the official language of the Republic of the Union ( DSO ) ) .')"}}
['Relation : lowest point . Context : Later in the year ( 1143 ) , Puyi and his allies made a conquest over the eastern part of the province of Sinti , which was occupied by the Franks and Normans . Head Entity : Sinti , Tail Entity : southern part .\n']
['Relation : lowest point . Context : Later in the year ( 1143 ) , Puyi and his allies made a conquest over the eastern part of the province of Sinti , which was occupied by the Franks and Normans . Head Entity : Sinti , Tail Entity : southern part .\n', 'Relation : lowest point . Context : After the death of Emperor Xuanzong his elder son Liu Bang was the first emperor to die at the age of only two . Head Entity : emperor , Tail Entity : highest point .\n']
{'target': 600, 'success': 12, 'raw': 32}
{'target': 600, 'success': 23, 'raw': 64}
{'target': 600, 'success': 32, 'raw': 96}
{'target': 600, 'success': 44, 'raw': 128}
{'target': 600, 'success': 55, 'raw': 160}
{'target': 600, 'success': 70, 'raw': 192}
{'target': 600, 'success': 86, 'raw': 224}
{'target': 600, 'success': 102, 'raw': 256}
{'target': 600, 'success': 115, 'raw': 288}
{'target': 600, 'success': 125, 'raw': 320}
{'target': 600, 'success': 141, 'raw': 352}
{'target': 600, 'success': 158, 'raw': 384}
{'target': 600, 'success': 176, 'raw': 416}
{'target': 600, 'success': 192, 'raw': 448}
{'target': 600, 'success': 206, 'raw': 480}
{'target': 600, 'success': 221, 'raw': 512}
{'target': 600, 'success': 233, 'raw': 544}
{'target': 600, 'success': 249, 'raw': 576}
{'target': 600, 'success': 265, 'raw': 608}
{'target': 600, 'success': 273, 'raw': 640}
{'target': 600, 'success': 288, 'raw': 672}
{'target': 600, 'success': 308, 'raw': 704}
{'target': 600, 'success': 317, 'raw': 736}
{'target': 600, 'success': 332, 'raw': 768}
{'target': 600, 'success': 342, 'raw': 800}
{'target': 600, 'success': 353, 'raw': 832}
{'target': 600, 'success': 369, 'raw': 864}
{'target': 600, 'success': 385, 'raw': 896}
{'target': 600, 'success': 394, 'raw': 928}
{'target': 600, 'success': 409, 'raw': 960}
{'target': 600, 'success': 423, 'raw': 992}
{'target': 600, 'success': 432, 'raw': 1024}
{'target': 600, 'success': 446, 'raw': 1056}
{'target': 600, 'success': 460, 'raw': 1088}
{'target': 600, 'success': 473, 'raw': 1120}
{'target': 600, 'success': 486, 'raw': 1152}
{'target': 600, 'success': 498, 'raw': 1184}
{'target': 600, 'success': 512, 'raw': 1216}
{'target': 600, 'success': 522, 'raw': 1248}
{'target': 600, 'success': 533, 'raw': 1280}
{'target': 600, 'success': 550, 'raw': 1312}
{'target': 600, 'success': 560, 'raw': 1344}
{'target': 600, 'success': 572, 'raw': 1376}
{'target': 600, 'success': 584, 'raw': 1408}
{'target': 600, 'success': 600, 'raw': 1440}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.4166666666666667, 'errors': {'', "('Manchester', 'lowest point', '', 'The area was home to the city of Manchester ( now Manchester City ) , founded in 1859 by the English Civil War hero Sir Walter Scott .')", "('second', 'lowest point', '', 'He is currently playing with fellow former New Zealand prop Tom Paine in the Championship side Auckland Roosters , and he moved up the rankings to second .')", 'not enough values to unpack (expected 2, got 1)', "('63', 'lowest point', '', 'On the night of 9 November 2014 , the Southeastern Conference led by Jaylen Brown was led by Duke Blue Devils in a 63 66 victory over the Southeast Division rival St. Charles .')", "('2004 FIFA World Cup', 'lowest point', '', 'The next day , he made a triumphant recovery and made his international debut at the 2004 FIFA World Cup against Ecuador .')", "('New York Giants', 'lowest point', '', 'In the second season , a team led by George Foreman and David Southee were allowed to make another bowl game against the New York Giants .')", "('lowest point', 'lowest point', '', 'For comparison , in 1998 , it was the lowest point per square mile since 1960 , when it was 1 . 10 .')", 'too many values to unpack (expected 2)'}}
['Relation : mother . Context : Later in Life , he came to love the beauty and natural beauty of the forests at the end of the third trimester , when she suffered death . Head Entity : forests at the end of the third trimester , Tail Entity : Mother .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : mother .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('king', 'mother', '', 'The first male and last female monarch of the Kingdom of Mexico , the king of the Andes ( d.')"}}
['Relation : mouth of the watercourse . Context : Later in the year ( 1143 ) , Puyi and his allies made atone for the fall of the last King of the Puebla , Puzco . Head Entity : Puyi , Tail Entity : Puducese .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 200, 'raw': 288}
{'target': 600, 'success': 221, 'raw': 320}
{'target': 600, 'success': 241, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 289, 'raw': 416}
{'target': 600, 'success': 311, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 357, 'raw': 512}
{'target': 600, 'success': 383, 'raw': 544}
{'target': 600, 'success': 404, 'raw': 576}
{'target': 600, 'success': 425, 'raw': 608}
{'target': 600, 'success': 440, 'raw': 640}
{'target': 600, 'success': 463, 'raw': 672}
{'target': 600, 'success': 488, 'raw': 704}
{'target': 600, 'success': 509, 'raw': 736}
{'target': 600, 'success': 536, 'raw': 768}
{'target': 600, 'success': 558, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 606, 'raw': 864}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.7013888888888888, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n']
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n', 'Relation : genre . Context : Eileen McClelland ( born June 24 , 1953 ) is an American radio talk show host who has appeared as a regular on both the Howard Stern Show and the Morning Mix . Head Entity : Howard Stern Show , Tail Entity : television talk show .\n']
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n', 'Relation : genre . Context : Eileen McClelland ( born June 24 , 1953 ) is an American radio talk show host who has appeared as a regular on both the Howard Stern Show and the Morning Mix . Head Entity : Howard Stern Show , Tail Entity : television talk show .\n', 'Relation : genre . Context : This film explores the social , social structure of the New York city of Times Square , which is populated and industrialized by wealthy Manhattanites . Head Entity : Times Square , Tail Entity : New York City .\n']
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 320, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 468, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 570, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : genre .', 'success_rate': 0.76625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.8020833333333334, 'errors': {'', "('programming languages', 'is a list of', '', 'It can be used primarily in conjunction with other programming languages such as .')", 'not enough values to unpack (expected 2, got 1)', "('The albums', 'is a list of', '', 'The albums songs , songs played and discography of the albums members , the artist , are listed in alphabetical order .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8111979166666666, 'errors': {'', "('Pluto', 'located on astronomical body', '', 'In the past , Pluto has been considered to be a binary system inhabited by one giant being , as well as being the first to be named after Pluto .')", "('Its orbit', 'located on astronomical body', '', 'Its orbit shows an eccentricity of 0 . 18 and an inclination of 8 degrees from the plane of the ecliptic .')", 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('it', 'manufacturer', '', 'It is known in Japan for its high quality and performance components , along with its long range of performance airsoft s.')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8179347826086957, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/0_ext.jsonl'}}
estimate vocab size: 12731
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12831, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_filtered_large/unseen_5_seed_4/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:14, 14.23s/it]Extractor Estimating: 2it [00:18,  8.16s/it]Extractor Estimating: 3it [00:18,  4.76s/it]Extractor Estimating: 4it [00:19,  3.13s/it]Extractor Estimating: 5it [00:20,  2.22s/it]Extractor Estimating: 6it [00:20,  1.67s/it]Extractor Estimating: 7it [00:21,  1.33s/it]Extractor Estimating: 8it [00:21,  1.10s/it]Extractor Estimating: 9it [00:22,  1.07it/s]Extractor Estimating: 10it [00:23,  1.19it/s]Extractor Estimating: 11it [00:23,  1.32it/s]Extractor Estimating: 12it [00:24,  1.44it/s]Extractor Estimating: 13it [00:24,  1.48it/s]Extractor Estimating: 14it [00:25,  1.50it/s]Extractor Estimating: 15it [00:26,  1.55it/s]Extractor Estimating: 16it [00:26,  1.56it/s]Extractor Estimating: 17it [00:27,  1.25it/s]Extractor Estimating: 18it [00:28,  1.34it/s]Extractor Estimating: 19it [00:29,  1.39it/s]Extractor Estimating: 20it [00:29,  1.48it/s]Extractor Estimating: 21it [00:30,  1.55it/s]Extractor Estimating: 22it [00:30,  1.58it/s]Extractor Estimating: 23it [00:31,  1.52it/s]Extractor Estimating: 24it [00:32,  1.59it/s]Extractor Estimating: 25it [00:32,  1.68it/s]Extractor Estimating: 26it [00:33,  1.65it/s]Extractor Estimating: 27it [00:33,  1.68it/s]Extractor Estimating: 28it [00:34,  1.71it/s]Extractor Estimating: 29it [00:35,  1.75it/s]Extractor Estimating: 30it [00:35,  1.77it/s]Extractor Estimating: 31it [00:36,  1.76it/s]Extractor Estimating: 32it [00:36,  1.78it/s]Extractor Estimating: 33it [00:37,  1.80it/s]Extractor Estimating: 34it [00:37,  1.82it/s]Extractor Estimating: 35it [00:38,  1.76it/s]Extractor Estimating: 36it [00:39,  1.76it/s]Extractor Estimating: 37it [00:39,  1.74it/s]Extractor Estimating: 38it [00:40,  1.76it/s]Extractor Estimating: 39it [00:40,  1.81it/s]Extractor Estimating: 40it [00:41,  1.76it/s]Extractor Estimating: 41it [00:41,  1.75it/s]Extractor Estimating: 42it [00:42,  1.77it/s]Extractor Estimating: 43it [00:43,  1.31it/s]Extractor Estimating: 44it [00:44,  1.43it/s]Extractor Estimating: 45it [00:44,  1.55it/s]Extractor Estimating: 46it [00:45,  1.59it/s]Extractor Estimating: 47it [00:45,  1.66it/s]Extractor Estimating: 48it [00:46,  1.73it/s]Extractor Estimating: 49it [00:46,  1.78it/s]Extractor Estimating: 50it [00:47,  1.80it/s]Extractor Estimating: 51it [00:48,  1.73it/s]Extractor Estimating: 52it [00:48,  1.69it/s]Extractor Estimating: 53it [00:49,  1.67it/s]Extractor Estimating: 54it [00:49,  1.65it/s]Extractor Estimating: 55it [00:50,  1.62it/s]Extractor Estimating: 56it [00:51,  1.59it/s]Extractor Estimating: 57it [00:51,  1.56it/s]Extractor Estimating: 58it [00:52,  1.56it/s]Extractor Estimating: 59it [00:53,  1.61it/s]Extractor Estimating: 60it [00:53,  1.60it/s]Extractor Estimating: 61it [00:54,  1.57it/s]Extractor Estimating: 62it [00:54,  1.59it/s]Extractor Estimating: 63it [00:55,  1.55it/s]Extractor Estimating: 64it [00:56,  1.57it/s]Extractor Estimating: 65it [00:56,  1.61it/s]Extractor Estimating: 66it [00:57,  1.58it/s]Extractor Estimating: 67it [00:58,  1.58it/s]Extractor Estimating: 68it [00:58,  1.58it/s]Extractor Estimating: 69it [00:59,  1.57it/s]Extractor Estimating: 70it [01:00,  1.59it/s]Extractor Estimating: 71it [01:00,  1.57it/s]Extractor Estimating: 72it [01:01,  1.58it/s]Extractor Estimating: 73it [01:01,  1.56it/s]Extractor Estimating: 74it [01:02,  1.58it/s]Extractor Estimating: 75it [01:03,  1.57it/s]Extractor Estimating: 76it [01:03,  1.51it/s]Extractor Estimating: 77it [01:04,  1.54it/s]Extractor Estimating: 78it [01:05,  1.53it/s]Extractor Estimating: 79it [01:05,  1.50it/s]Extractor Estimating: 80it [01:06,  1.53it/s]Extractor Estimating: 81it [01:07,  1.54it/s]Extractor Estimating: 82it [01:07,  1.57it/s]Extractor Estimating: 83it [01:08,  1.53it/s]Extractor Estimating: 84it [01:09,  1.52it/s]Extractor Estimating: 85it [01:09,  1.48it/s]Extractor Estimating: 86it [01:11,  1.17it/s]Extractor Estimating: 87it [01:11,  1.24it/s]Extractor Estimating: 88it [01:12,  1.30it/s]Extractor Estimating: 89it [01:13,  1.34it/s]Extractor Estimating: 90it [01:13,  1.38it/s]Extractor Estimating: 91it [01:14,  1.34it/s]Extractor Estimating: 92it [01:15,  1.40it/s]Extractor Estimating: 93it [01:16,  1.45it/s]Extractor Estimating: 94it [01:16,  1.47it/s]Extractor Estimating: 95it [01:17,  1.50it/s]Extractor Estimating: 96it [01:17,  1.50it/s]Extractor Estimating: 97it [01:18,  1.57it/s]Extractor Estimating: 98it [01:19,  1.50it/s]Extractor Estimating: 99it [01:19,  1.50it/s]Extractor Estimating: 100it [01:20,  1.50it/s]Extractor Estimating: 101it [01:21,  1.50it/s]Extractor Estimating: 102it [01:21,  1.55it/s]Extractor Estimating: 103it [01:22,  1.59it/s]Extractor Estimating: 104it [01:22,  1.65it/s]Extractor Estimating: 105it [01:23,  1.62it/s]Extractor Estimating: 106it [01:24,  1.68it/s]Extractor Estimating: 107it [01:24,  1.71it/s]Extractor Estimating: 108it [01:25,  1.70it/s]Extractor Estimating: 109it [01:25,  1.71it/s]Extractor Estimating: 110it [01:26,  1.73it/s]Extractor Estimating: 111it [01:27,  1.71it/s]Extractor Estimating: 112it [01:27,  1.67it/s]Extractor Estimating: 113it [01:28,  1.62it/s]Extractor Estimating: 114it [01:28,  1.69it/s]Extractor Estimating: 115it [01:29,  1.67it/s]Extractor Estimating: 116it [01:30,  1.71it/s]Extractor Estimating: 117it [01:30,  1.66it/s]Extractor Estimating: 118it [01:31,  1.65it/s]Extractor Estimating: 119it [01:32,  1.60it/s]Extractor Estimating: 120it [01:32,  1.63it/s]Extractor Estimating: 121it [01:33,  1.59it/s]Extractor Estimating: 122it [01:33,  1.58it/s]Extractor Estimating: 123it [01:34,  1.59it/s]Extractor Estimating: 124it [01:35,  1.60it/s]Extractor Estimating: 125it [01:35,  1.65it/s]Extractor Estimating: 126it [01:36,  1.65it/s]Extractor Estimating: 127it [01:36,  1.63it/s]Extractor Estimating: 128it [01:37,  1.64it/s]Extractor Estimating: 129it [01:38,  1.64it/s]Extractor Estimating: 130it [01:38,  1.64it/s]Extractor Estimating: 131it [01:39,  1.68it/s]Extractor Estimating: 132it [01:39,  1.66it/s]Extractor Estimating: 133it [01:40,  1.66it/s]Extractor Estimating: 134it [01:41,  1.68it/s]Extractor Estimating: 135it [01:41,  1.63it/s]Extractor Estimating: 136it [01:42,  1.66it/s]Extractor Estimating: 137it [01:43,  1.60it/s]Extractor Estimating: 138it [01:43,  1.61it/s]Extractor Estimating: 139it [01:44,  1.64it/s]Extractor Estimating: 140it [01:44,  1.58it/s]Extractor Estimating: 141it [01:45,  1.58it/s]Extractor Estimating: 142it [01:46,  1.59it/s]Extractor Estimating: 143it [01:46,  1.60it/s]Extractor Estimating: 144it [01:47,  1.56it/s]Extractor Estimating: 145it [01:48,  1.61it/s]Extractor Estimating: 146it [01:48,  1.62it/s]Extractor Estimating: 147it [01:49,  1.65it/s]Extractor Estimating: 148it [01:49,  1.63it/s]Extractor Estimating: 149it [01:50,  1.58it/s]Extractor Estimating: 150it [01:51,  1.67it/s]Extractor Estimating: 151it [01:51,  1.70it/s]Extractor Estimating: 152it [01:52,  1.69it/s]Extractor Estimating: 153it [01:52,  1.72it/s]Extractor Estimating: 154it [01:53,  1.65it/s]Extractor Estimating: 155it [01:53,  1.68it/s]Extractor Estimating: 156it [01:54,  1.66it/s]Extractor Estimating: 157it [01:55,  1.59it/s]Extractor Estimating: 158it [01:55,  1.59it/s]Extractor Estimating: 159it [01:56,  1.64it/s]Extractor Estimating: 160it [01:57,  1.65it/s]Extractor Estimating: 161it [01:57,  1.57it/s]Extractor Estimating: 162it [01:58,  1.58it/s]Extractor Estimating: 163it [01:59,  1.61it/s]Extractor Estimating: 164it [01:59,  1.60it/s]Extractor Estimating: 165it [02:00,  1.62it/s]Extractor Estimating: 166it [02:00,  1.67it/s]Extractor Estimating: 167it [02:01,  1.72it/s]Extractor Estimating: 168it [02:01,  1.72it/s]Extractor Estimating: 169it [02:02,  1.69it/s]Extractor Estimating: 170it [02:03,  1.69it/s]Extractor Estimating: 171it [02:03,  1.68it/s]Extractor Estimating: 172it [02:04,  1.77it/s]Extractor Estimating: 173it [02:04,  1.68it/s]Extractor Estimating: 174it [02:05,  1.65it/s]Extractor Estimating: 175it [02:06,  1.69it/s]Extractor Estimating: 176it [02:06,  1.74it/s]Extractor Estimating: 177it [02:07,  1.73it/s]Extractor Estimating: 178it [02:07,  1.75it/s]Extractor Estimating: 179it [02:08,  1.79it/s]Extractor Estimating: 180it [02:08,  1.76it/s]Extractor Estimating: 181it [02:09,  1.76it/s]Extractor Estimating: 182it [02:10,  1.75it/s]Extractor Estimating: 183it [02:10,  1.80it/s]Extractor Estimating: 184it [02:11,  1.75it/s]Extractor Estimating: 185it [02:11,  1.74it/s]Extractor Estimating: 186it [02:12,  1.70it/s]Extractor Estimating: 187it [02:12,  1.70it/s]Extractor Estimating: 188it [02:13,  1.74it/s]Extractor Estimating: 189it [02:14,  1.71it/s]Extractor Estimating: 190it [02:14,  1.70it/s]Extractor Estimating: 191it [02:15,  1.72it/s]Extractor Estimating: 192it [02:15,  1.72it/s]Extractor Estimating: 193it [02:16,  1.82it/s]Extractor Estimating: 194it [02:16,  1.79it/s]Extractor Estimating: 195it [02:17,  1.71it/s]Extractor Estimating: 196it [02:18,  1.72it/s]Extractor Estimating: 197it [02:18,  1.75it/s]Extractor Estimating: 198it [02:19,  1.73it/s]Extractor Estimating: 199it [02:19,  1.70it/s]Extractor Estimating: 200it [02:20,  1.68it/s]Extractor Estimating: 201it [02:21,  1.68it/s]Extractor Estimating: 202it [02:21,  1.70it/s]Extractor Estimating: 203it [02:22,  1.72it/s]Extractor Estimating: 204it [02:22,  1.73it/s]Extractor Estimating: 205it [02:23,  1.64it/s]Extractor Estimating: 206it [02:23,  1.72it/s]Extractor Estimating: 207it [02:24,  1.68it/s]Extractor Estimating: 208it [02:25,  1.76it/s]Extractor Estimating: 209it [02:25,  1.72it/s]Extractor Estimating: 210it [02:26,  1.70it/s]Extractor Estimating: 211it [02:27,  1.62it/s]Extractor Estimating: 212it [02:27,  1.65it/s]Extractor Estimating: 213it [02:28,  1.68it/s]Extractor Estimating: 214it [02:28,  1.67it/s]Extractor Estimating: 215it [02:29,  1.68it/s]Extractor Estimating: 216it [02:29,  1.73it/s]Extractor Estimating: 217it [02:30,  1.73it/s]Extractor Estimating: 218it [02:31,  1.71it/s]Extractor Estimating: 219it [02:31,  1.73it/s]Extractor Estimating: 220it [02:32,  1.73it/s]Extractor Estimating: 221it [02:32,  1.78it/s]Extractor Estimating: 222it [02:33,  1.74it/s]Extractor Estimating: 223it [02:33,  1.76it/s]Extractor Estimating: 224it [02:34,  1.73it/s]Extractor Estimating: 225it [02:35,  1.70it/s]Extractor Estimating: 226it [02:35,  1.65it/s]Extractor Estimating: 227it [02:36,  1.64it/s]Extractor Estimating: 228it [02:36,  1.64it/s]Extractor Estimating: 229it [02:37,  1.63it/s]Extractor Estimating: 230it [02:38,  1.64it/s]Extractor Estimating: 231it [02:38,  1.65it/s]Extractor Estimating: 232it [02:39,  1.64it/s]Extractor Estimating: 233it [02:40,  1.57it/s]Extractor Estimating: 234it [02:40,  1.58it/s]Extractor Estimating: 235it [02:41,  1.62it/s]Extractor Estimating: 236it [02:41,  1.62it/s]Extractor Estimating: 237it [02:42,  1.56it/s]Extractor Estimating: 238it [02:43,  1.62it/s]Extractor Estimating: 239it [02:43,  1.61it/s]Extractor Estimating: 240it [02:44,  1.62it/s]Extractor Estimating: 241it [02:45,  1.58it/s]Extractor Estimating: 242it [02:45,  1.55it/s]Extractor Estimating: 243it [02:46,  1.62it/s]Extractor Estimating: 244it [02:47,  1.45it/s]Extractor Estimating: 245it [02:47,  1.47it/s]Extractor Estimating: 246it [02:48,  1.51it/s]Extractor Estimating: 247it [02:49,  1.55it/s]Extractor Estimating: 248it [02:49,  1.58it/s]Extractor Estimating: 249it [02:50,  1.59it/s]Extractor Estimating: 250it [02:50,  1.56it/s]Extractor Estimating: 250it [02:50,  1.46it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 4945 mean pseudo reward: 0.9210028786853381
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 24142
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24242, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_filtered_large/unseen_5_seed_4/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24242, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.263, loss:1463.6357
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.969, loss:1426.7128
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 93, avg_time 0.982, loss:1359.6566
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 193, avg_time 0.983, loss:1357.5368
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 86, avg_time 0.986, loss:1270.7820
>> valid entity prec:0.5152, rec:0.5141, f1:0.5146
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 186, avg_time 2.302, loss:1303.8658
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 79, avg_time 0.980, loss:1229.6780
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 179, avg_time 0.970, loss:1216.1702
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 72, avg_time 0.970, loss:1163.3866
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 172, avg_time 0.981, loss:1181.7738
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5125, rec:0.4154, f1:0.4589
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 65, avg_time 2.301, loss:1143.6488
g_step 1200, step 165, avg_time 0.972, loss:1135.4039
g_step 1300, step 58, avg_time 0.964, loss:1072.1231
g_step 1400, step 158, avg_time 0.976, loss:1070.0274
g_step 1500, step 51, avg_time 0.964, loss:1051.0538
>> valid entity prec:0.5144, rec:0.4253, f1:0.4656
>> valid relation prec:0.0949, rec:0.0032, f1:0.0062
>> valid relation with NER prec:0.0949, rec:0.0032, f1:0.0062
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 151, avg_time 2.300, loss:1046.9614
g_step 1700, step 44, avg_time 0.963, loss:1003.5447
g_step 1800, step 144, avg_time 0.976, loss:1008.2030
g_step 1900, step 37, avg_time 0.983, loss:1002.5040
g_step 2000, step 137, avg_time 0.975, loss:960.4479
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5102, rec:0.4467, f1:0.4764
>> valid relation prec:0.1000, rec:0.0035, f1:0.0067
>> valid relation with NER prec:0.1000, rec:0.0035, f1:0.0067
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 30, avg_time 2.279, loss:942.0983
g_step 2200, step 130, avg_time 0.977, loss:923.6557
g_step 2300, step 23, avg_time 0.965, loss:921.8677
g_step 2400, step 123, avg_time 0.980, loss:896.0979
g_step 2500, step 16, avg_time 0.969, loss:899.4307
>> valid entity prec:0.5135, rec:0.5069, f1:0.5102
>> valid relation prec:0.0792, rec:0.0020, f1:0.0039
>> valid relation with NER prec:0.0792, rec:0.0020, f1:0.0039
g_step 2600, step 116, avg_time 2.292, loss:846.8083
g_step 2700, step 9, avg_time 0.974, loss:878.9232
g_step 2800, step 109, avg_time 0.970, loss:828.1178
g_step 2900, step 2, avg_time 0.976, loss:832.7882
g_step 3000, step 102, avg_time 0.983, loss:791.5692
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5343, rec:0.4825, f1:0.5071
>> valid relation prec:0.1503, rec:0.0129, f1:0.0237
>> valid relation with NER prec:0.1503, rec:0.0129, f1:0.0237
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 202, avg_time 2.296, loss:841.3496
g_step 3200, step 95, avg_time 0.966, loss:764.2751
g_step 3300, step 195, avg_time 0.985, loss:794.7237
g_step 3400, step 88, avg_time 0.964, loss:758.8329
g_step 3500, step 188, avg_time 0.985, loss:756.9939
>> valid entity prec:0.5504, rec:0.3571, f1:0.4332
>> valid relation prec:0.0641, rec:0.0037, f1:0.0070
>> valid relation with NER prec:0.0641, rec:0.0037, f1:0.0070
g_step 3600, step 81, avg_time 2.299, loss:715.7271
g_step 3700, step 181, avg_time 0.975, loss:720.8257
g_step 3800, step 74, avg_time 0.984, loss:696.2873
g_step 3900, step 174, avg_time 0.973, loss:729.8463
g_step 4000, step 67, avg_time 0.967, loss:669.7059
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4956, rec:0.4828, f1:0.4891
>> valid relation prec:0.0576, rec:0.0042, f1:0.0079
>> valid relation with NER prec:0.0576, rec:0.0042, f1:0.0079
g_step 4100, step 167, avg_time 2.299, loss:690.9011
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 08:34:03 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 08:34:03 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_08-34-03_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 08:34:04 - WARNING - datasets.builder -   Using custom data configuration default-c6d06c984aeb8f7c
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-c6d06c984aeb8f7c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 08:34:04,368 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 08:34:04,370 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 08:34:04,370 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 08:34:04,371 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 08:34:04,378 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:34:04,382 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:34:04,382 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:34:04,382 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:34:04,382 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:34:04,382 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:34:04,382 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 08:34:04,494 >> loading weights file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 08:34:07,548 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 08:34:07,551 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_5_seed_4/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-c6d06c984aeb8f7c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 08:34:07 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14a484cd8950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.86ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.75ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.11ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.28ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.40ba/s]100%|██████████| 6/6 [00:01<00:00,  4.94ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.17ba/s] 40%|████      | 2/5 [00:00<00:00,  4.32ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.38ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.42ba/s]100%|██████████| 5/5 [00:00<00:00,  5.38ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  8.33ba/s] 50%|█████     | 3/6 [00:00<00:00, 10.04ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 10.52ba/s]100%|██████████| 6/6 [00:00<00:00, 12.27ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.22ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.98ba/s]100%|██████████| 5/5 [00:00<00:00, 12.42ba/s]
[INFO|trainer.py:414] 2023-08-28 08:34:10,923 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 08:34:10,937 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 08:34:10,937 >>   Num examples = 5002
[INFO|trainer.py:1149] 2023-08-28 08:34:10,937 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 08:34:10,937 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 08:34:10,937 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 08:34:10,937 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 08:34:10,937 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:55,  3.36it/s]  1%|          | 2/390 [00:00<01:53,  3.41it/s]  1%|          | 3/390 [00:00<01:52,  3.43it/s]  1%|          | 4/390 [00:01<01:52,  3.44it/s]  1%|▏         | 5/390 [00:01<01:51,  3.45it/s]  2%|▏         | 6/390 [00:01<01:51,  3.45it/s]  2%|▏         | 7/390 [00:02<01:50,  3.45it/s]  2%|▏         | 8/390 [00:02<01:50,  3.45it/s]  2%|▏         | 9/390 [00:02<01:50,  3.45it/s]  3%|▎         | 10/390 [00:02<01:50,  3.45it/s]  3%|▎         | 11/390 [00:03<01:50,  3.43it/s]  3%|▎         | 12/390 [00:03<01:50,  3.44it/s]  3%|▎         | 13/390 [00:03<01:49,  3.44it/s]  4%|▎         | 14/390 [00:04<01:49,  3.44it/s]  4%|▍         | 15/390 [00:04<01:48,  3.45it/s]  4%|▍         | 16/390 [00:04<01:48,  3.45it/s]  4%|▍         | 17/390 [00:04<01:48,  3.45it/s]  5%|▍         | 18/390 [00:05<01:47,  3.45it/s]  5%|▍         | 19/390 [00:05<01:47,  3.45it/s]  5%|▌         | 20/390 [00:05<01:47,  3.45it/s]  5%|▌         | 21/390 [00:06<01:46,  3.45it/s]  6%|▌         | 22/390 [00:06<01:46,  3.44it/s]  6%|▌         | 23/390 [00:06<01:46,  3.44it/s]  6%|▌         | 24/390 [00:06<01:46,  3.45it/s]  6%|▋         | 25/390 [00:07<01:45,  3.45it/s]  7%|▋         | 26/390 [00:07<01:45,  3.45it/s]  7%|▋         | 27/390 [00:07<01:45,  3.45it/s]  7%|▋         | 28/390 [00:08<01:45,  3.45it/s]  7%|▋         | 29/390 [00:08<01:44,  3.45it/s]  8%|▊         | 30/390 [00:08<01:44,  3.44it/s]  8%|▊         | 31/390 [00:08<01:44,  3.45it/s]  8%|▊         | 32/390 [00:09<01:43,  3.45it/s]  8%|▊         | 33/390 [00:09<01:43,  3.44it/s]  9%|▊         | 34/390 [00:09<01:43,  3.44it/s]  9%|▉         | 35/390 [00:10<01:43,  3.44it/s]  9%|▉         | 36/390 [00:10<01:42,  3.44it/s]  9%|▉         | 37/390 [00:10<01:42,  3.45it/s] 10%|▉         | 38/390 [00:11<01:42,  3.45it/s] 10%|█         | 39/390 [00:11<01:41,  3.44it/s] 10%|█         | 40/390 [00:11<01:41,  3.44it/s] 11%|█         | 41/390 [00:11<01:41,  3.44it/s] 11%|█         | 42/390 [00:12<01:40,  3.45it/s] 11%|█         | 43/390 [00:12<01:40,  3.45it/s] 11%|█▏        | 44/390 [00:12<01:40,  3.44it/s] 12%|█▏        | 45/390 [00:13<01:40,  3.44it/s] 12%|█▏        | 46/390 [00:13<01:39,  3.44it/s] 12%|█▏        | 47/390 [00:13<01:39,  3.44it/s] 12%|█▏        | 48/390 [00:13<01:39,  3.45it/s] 13%|█▎        | 49/390 [00:14<01:38,  3.45it/s] 13%|█▎        | 50/390 [00:14<01:38,  3.45it/s] 13%|█▎        | 51/390 [00:14<01:38,  3.45it/s] 13%|█▎        | 52/390 [00:15<01:38,  3.45it/s] 14%|█▎        | 53/390 [00:15<01:37,  3.45it/s] 14%|█▍        | 54/390 [00:15<01:37,  3.45it/s] 14%|█▍        | 55/390 [00:15<01:37,  3.44it/s] 14%|█▍        | 56/390 [00:16<01:37,  3.44it/s] 15%|█▍        | 57/390 [00:16<01:36,  3.44it/s] 15%|█▍        | 58/390 [00:16<01:36,  3.44it/s] 15%|█▌        | 59/390 [00:17<01:36,  3.44it/s] 15%|█▌        | 60/390 [00:17<01:35,  3.45it/s] 16%|█▌        | 61/390 [00:17<01:35,  3.45it/s] 16%|█▌        | 62/390 [00:17<01:35,  3.45it/s] 16%|█▌        | 63/390 [00:18<01:34,  3.45it/s] 16%|█▋        | 64/390 [00:18<01:34,  3.45it/s] 17%|█▋        | 65/390 [00:18<01:34,  3.45it/s] 17%|█▋        | 66/390 [00:19<01:34,  3.43it/s] 17%|█▋        | 67/390 [00:19<01:33,  3.44it/s] 17%|█▋        | 68/390 [00:19<01:33,  3.44it/s] 18%|█▊        | 69/390 [00:20<01:33,  3.44it/s] 18%|█▊        | 70/390 [00:20<01:32,  3.44it/s] 18%|█▊        | 71/390 [00:20<01:32,  3.44it/s] 18%|█▊        | 72/390 [00:20<01:32,  3.44it/s] 19%|█▊        | 73/390 [00:21<01:32,  3.44it/s] 19%|█▉        | 74/390 [00:21<01:31,  3.45it/s] 19%|█▉        | 75/390 [00:21<01:31,  3.45it/s] 19%|█▉        | 76/390 [00:22<01:31,  3.44it/s] 20%|█▉        | 77/390 [00:22<01:31,  3.43it/s] 20%|██        | 78/390 [00:22<01:30,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 08:34:33,622 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:34:33,622 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 08:34:33,622 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.23it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.82it/s][A
  3%|▎         | 17/505 [00:00<00:10, 45.87it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.22it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.76it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.63it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.64it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.45it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.57it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.60it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.44it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.19it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.18it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.13it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.26it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.25it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.33it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.40it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.45it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.34it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.24it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.12it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.11it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.17it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.31it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.25it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.20it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.35it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.34it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.14it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.17it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.16it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.26it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.26it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.29it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.39it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.40it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.30it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.19it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.18it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.22it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.23it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.29it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.37it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.38it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.39it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.28it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.21it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.16it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.18it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.17it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.28it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.31it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.32it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.28it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.35it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.26it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.22it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.16it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.18it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.30it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.28it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.22it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.29it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.32it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.31it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.27it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.18it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.23it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.33it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.33it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.25it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.24it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.32it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.30it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.18it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.11it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.22it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.26it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.35it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.26it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.22it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.30it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.34it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.25it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.11it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.21it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.27it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.33it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.19it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.23it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.34it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.35it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.17it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.07it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.22it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.31it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.28it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.27it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.21it/s][A                                                
                                                 [A 20%|██        | 78/390 [00:34<01:30,  3.44it/s]
100%|██████████| 505/505 [00:11<00:00, 44.21it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 08:34:45,065 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 08:34:45,091 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:34:46,997 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:34:47,014 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:34:47,023 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:39<27:46,  5.36s/it] 21%|██        | 80/390 [00:40<19:49,  3.84s/it] 21%|██        | 81/390 [00:40<14:17,  2.78s/it] 21%|██        | 82/390 [00:40<10:25,  2.03s/it] 21%|██▏       | 83/390 [00:41<07:43,  1.51s/it] 22%|██▏       | 84/390 [00:41<05:50,  1.15s/it] 22%|██▏       | 85/390 [00:41<04:31,  1.12it/s] 22%|██▏       | 86/390 [00:41<03:36,  1.41it/s] 22%|██▏       | 87/390 [00:42<02:57,  1.71it/s] 23%|██▎       | 88/390 [00:42<02:30,  2.01it/s] 23%|██▎       | 89/390 [00:42<02:11,  2.29it/s] 23%|██▎       | 90/390 [00:43<01:58,  2.54it/s] 23%|██▎       | 91/390 [00:43<01:49,  2.74it/s] 24%|██▎       | 92/390 [00:43<01:42,  2.91it/s] 24%|██▍       | 93/390 [00:43<01:37,  3.04it/s] 24%|██▍       | 94/390 [00:44<01:34,  3.14it/s] 24%|██▍       | 95/390 [00:44<01:31,  3.21it/s] 25%|██▍       | 96/390 [00:44<01:29,  3.27it/s] 25%|██▍       | 97/390 [00:45<01:28,  3.31it/s] 25%|██▌       | 98/390 [00:45<01:27,  3.33it/s] 25%|██▌       | 99/390 [00:45<01:26,  3.35it/s] 26%|██▌       | 100/390 [00:46<01:26,  3.37it/s] 26%|██▌       | 101/390 [00:46<01:25,  3.38it/s] 26%|██▌       | 102/390 [00:46<01:25,  3.37it/s] 26%|██▋       | 103/390 [00:46<01:24,  3.38it/s] 27%|██▋       | 104/390 [00:47<01:24,  3.38it/s] 27%|██▋       | 105/390 [00:47<01:24,  3.39it/s] 27%|██▋       | 106/390 [00:47<01:23,  3.39it/s] 27%|██▋       | 107/390 [00:48<01:23,  3.39it/s] 28%|██▊       | 108/390 [00:48<01:23,  3.40it/s] 28%|██▊       | 109/390 [00:48<01:22,  3.40it/s] 28%|██▊       | 110/390 [00:48<01:22,  3.40it/s] 28%|██▊       | 111/390 [00:49<01:22,  3.40it/s] 29%|██▊       | 112/390 [00:49<01:21,  3.40it/s] 29%|██▉       | 113/390 [00:49<01:21,  3.38it/s] 29%|██▉       | 114/390 [00:50<01:21,  3.38it/s] 29%|██▉       | 115/390 [00:50<01:21,  3.39it/s] 30%|██▉       | 116/390 [00:50<01:20,  3.39it/s] 30%|███       | 117/390 [00:51<01:20,  3.39it/s] 30%|███       | 118/390 [00:51<01:20,  3.39it/s] 31%|███       | 119/390 [00:51<01:19,  3.39it/s] 31%|███       | 120/390 [00:51<01:19,  3.39it/s] 31%|███       | 121/390 [00:52<01:20,  3.32it/s] 31%|███▏      | 122/390 [00:52<01:20,  3.35it/s] 32%|███▏      | 123/390 [00:52<01:19,  3.36it/s] 32%|███▏      | 124/390 [00:53<01:19,  3.37it/s] 32%|███▏      | 125/390 [00:53<01:18,  3.37it/s] 32%|███▏      | 126/390 [00:53<01:18,  3.38it/s] 33%|███▎      | 127/390 [00:53<01:17,  3.39it/s] 33%|███▎      | 128/390 [00:54<01:17,  3.39it/s] 33%|███▎      | 129/390 [00:54<01:16,  3.39it/s] 33%|███▎      | 130/390 [00:54<01:16,  3.39it/s] 34%|███▎      | 131/390 [00:55<01:16,  3.40it/s] 34%|███▍      | 132/390 [00:55<01:16,  3.39it/s] 34%|███▍      | 133/390 [00:55<01:15,  3.39it/s] 34%|███▍      | 134/390 [00:56<01:15,  3.39it/s] 35%|███▍      | 135/390 [00:56<01:15,  3.40it/s] 35%|███▍      | 136/390 [00:56<01:14,  3.39it/s] 35%|███▌      | 137/390 [00:56<01:14,  3.38it/s] 35%|███▌      | 138/390 [00:57<01:14,  3.38it/s] 36%|███▌      | 139/390 [00:57<01:14,  3.39it/s] 36%|███▌      | 140/390 [00:57<01:13,  3.39it/s] 36%|███▌      | 141/390 [00:58<01:13,  3.39it/s] 36%|███▋      | 142/390 [00:58<01:13,  3.39it/s] 37%|███▋      | 143/390 [00:58<01:12,  3.39it/s] 37%|███▋      | 144/390 [00:58<01:12,  3.39it/s] 37%|███▋      | 145/390 [00:59<01:12,  3.39it/s] 37%|███▋      | 146/390 [00:59<01:11,  3.39it/s] 38%|███▊      | 147/390 [00:59<01:11,  3.41it/s] 38%|███▊      | 148/390 [01:00<01:11,  3.41it/s] 38%|███▊      | 149/390 [01:00<01:10,  3.42it/s] 38%|███▊      | 150/390 [01:00<01:10,  3.43it/s] 39%|███▊      | 151/390 [01:01<01:09,  3.43it/s] 39%|███▉      | 152/390 [01:01<01:09,  3.43it/s] 39%|███▉      | 153/390 [01:01<01:09,  3.43it/s] 39%|███▉      | 154/390 [01:01<01:08,  3.43it/s] 40%|███▉      | 155/390 [01:02<01:08,  3.43it/s] 40%|████      | 156/390 [01:02<01:08,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 08:35:13,472 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:35:13,472 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 08:35:13,472 >>   Batch size = 8
{'eval_loss': 1.0817701816558838, 'eval_runtime': 11.4247, 'eval_samples_per_second': 353.533, 'eval_steps_per_second': 44.203, 'epoch': 0.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.18it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.77it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.01it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.30it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.78it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.62it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.59it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.32it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.25it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.34it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.30it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.26it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.01it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.14it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.21it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.23it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.12it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.07it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.19it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.21it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.18it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.16it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.20it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.22it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.14it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.12it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.00it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.14it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.15it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.19it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.11it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.21it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.27it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.19it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.19it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.12it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.15it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.16it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.17it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.09it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.23it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.14it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.18it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.03it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.15it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.17it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.24it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.23it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.20it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.24it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.20it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.14it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.09it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.10it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.10it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.20it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.26it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.21it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.21it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.21it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.24it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.21it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.16it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.18it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.22it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.21it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.09it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.14it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.14it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.19it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.10it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.13it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.15it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.21it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.14it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.10it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.18it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.17it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.22it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.19it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.22it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.16it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.23it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.05it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.13it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 43.89it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.14it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.26it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.22it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.23it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.16it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.19it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.12it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.18it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.15it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.20it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.20it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.19it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.24it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.15it/s][A                                                 
                                                 [A 40%|████      | 156/390 [01:13<01:08,  3.44it/s]
100%|██████████| 505/505 [00:11<00:00, 44.15it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 08:35:24,950 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 08:35:24,983 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:35:26,683 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:35:26,698 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:35:26,710 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:19<20:38,  5.32s/it] 41%|████      | 158/390 [01:19<14:43,  3.81s/it] 41%|████      | 159/390 [01:20<10:36,  2.76s/it] 41%|████      | 160/390 [01:20<07:43,  2.02s/it] 41%|████▏     | 161/390 [01:20<05:43,  1.50s/it] 42%|████▏     | 162/390 [01:21<04:19,  1.14s/it] 42%|████▏     | 163/390 [01:21<03:20,  1.13it/s] 42%|████▏     | 164/390 [01:21<02:40,  1.41it/s] 42%|████▏     | 165/390 [01:21<02:11,  1.71it/s] 43%|████▎     | 166/390 [01:22<01:51,  2.01it/s] 43%|████▎     | 167/390 [01:22<01:37,  2.29it/s] 43%|████▎     | 168/390 [01:22<01:27,  2.54it/s] 43%|████▎     | 169/390 [01:23<01:20,  2.76it/s] 44%|████▎     | 170/390 [01:23<01:15,  2.93it/s] 44%|████▍     | 171/390 [01:23<01:11,  3.07it/s] 44%|████▍     | 172/390 [01:23<01:08,  3.17it/s] 44%|████▍     | 173/390 [01:24<01:06,  3.25it/s] 45%|████▍     | 174/390 [01:24<01:05,  3.30it/s] 45%|████▍     | 175/390 [01:24<01:04,  3.35it/s] 45%|████▌     | 176/390 [01:25<01:03,  3.37it/s] 45%|████▌     | 177/390 [01:25<01:02,  3.39it/s] 46%|████▌     | 178/390 [01:25<01:02,  3.41it/s] 46%|████▌     | 179/390 [01:25<01:01,  3.42it/s] 46%|████▌     | 180/390 [01:26<01:01,  3.43it/s] 46%|████▋     | 181/390 [01:26<01:00,  3.43it/s] 47%|████▋     | 182/390 [01:26<01:00,  3.44it/s] 47%|████▋     | 183/390 [01:27<01:00,  3.44it/s] 47%|████▋     | 184/390 [01:27<00:59,  3.44it/s] 47%|████▋     | 185/390 [01:27<01:06,  3.10it/s] 48%|████▊     | 186/390 [01:28<01:03,  3.19it/s] 48%|████▊     | 187/390 [01:28<01:02,  3.26it/s] 48%|████▊     | 188/390 [01:28<01:00,  3.32it/s] 48%|████▊     | 189/390 [01:28<00:59,  3.35it/s] 49%|████▊     | 190/390 [01:29<00:59,  3.38it/s] 49%|████▉     | 191/390 [01:29<00:58,  3.40it/s] 49%|████▉     | 192/390 [01:29<00:58,  3.41it/s] 49%|████▉     | 193/390 [01:30<00:57,  3.42it/s] 50%|████▉     | 194/390 [01:30<00:57,  3.43it/s] 50%|█████     | 195/390 [01:30<00:56,  3.42it/s] 50%|█████     | 196/390 [01:31<00:56,  3.43it/s] 51%|█████     | 197/390 [01:31<00:56,  3.43it/s] 51%|█████     | 198/390 [01:31<00:55,  3.44it/s] 51%|█████     | 199/390 [01:31<00:55,  3.44it/s] 51%|█████▏    | 200/390 [01:32<00:55,  3.44it/s] 52%|█████▏    | 201/390 [01:32<00:54,  3.44it/s] 52%|█████▏    | 202/390 [01:32<00:54,  3.44it/s] 52%|█████▏    | 203/390 [01:33<00:54,  3.44it/s] 52%|█████▏    | 204/390 [01:33<00:54,  3.44it/s] 53%|█████▎    | 205/390 [01:33<00:53,  3.44it/s] 53%|█████▎    | 206/390 [01:33<00:53,  3.43it/s] 53%|█████▎    | 207/390 [01:34<00:53,  3.43it/s] 53%|█████▎    | 208/390 [01:34<00:52,  3.43it/s] 54%|█████▎    | 209/390 [01:34<00:52,  3.43it/s] 54%|█████▍    | 210/390 [01:35<00:52,  3.44it/s] 54%|█████▍    | 211/390 [01:35<00:52,  3.44it/s] 54%|█████▍    | 212/390 [01:35<00:51,  3.44it/s] 55%|█████▍    | 213/390 [01:35<00:51,  3.44it/s] 55%|█████▍    | 214/390 [01:36<00:51,  3.44it/s] 55%|█████▌    | 215/390 [01:36<00:50,  3.44it/s] 55%|█████▌    | 216/390 [01:36<00:50,  3.44it/s] 56%|█████▌    | 217/390 [01:37<00:50,  3.44it/s] 56%|█████▌    | 218/390 [01:37<00:50,  3.44it/s] 56%|█████▌    | 219/390 [01:37<00:49,  3.44it/s] 56%|█████▋    | 220/390 [01:37<00:49,  3.44it/s] 57%|█████▋    | 221/390 [01:38<00:49,  3.44it/s] 57%|█████▋    | 222/390 [01:38<00:48,  3.44it/s] 57%|█████▋    | 223/390 [01:38<00:48,  3.44it/s] 57%|█████▋    | 224/390 [01:39<00:48,  3.44it/s] 58%|█████▊    | 225/390 [01:39<00:47,  3.44it/s] 58%|█████▊    | 226/390 [01:39<00:47,  3.44it/s] 58%|█████▊    | 227/390 [01:40<00:47,  3.44it/s] 58%|█████▊    | 228/390 [01:40<00:47,  3.44it/s] 59%|█████▊    | 229/390 [01:40<00:46,  3.44it/s] 59%|█████▉    | 230/390 [01:40<00:46,  3.44it/s] 59%|█████▉    | 231/390 [01:41<00:46,  3.44it/s] 59%|█████▉    | 232/390 [01:41<00:45,  3.44it/s] 60%|█████▉    | 233/390 [01:41<00:45,  3.44it/s] 60%|██████    | 234/390 [01:42<00:45,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 08:35:53,047 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:35:53,047 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 08:35:53,047 >>   Batch size = 8
{'eval_loss': 1.0413392782211304, 'eval_runtime': 11.4491, 'eval_samples_per_second': 352.779, 'eval_steps_per_second': 44.108, 'epoch': 1.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.18it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.68it/s][A
  3%|▎         | 17/505 [00:00<00:10, 45.96it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.23it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.81it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.59it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.47it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.20it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.41it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.44it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.27it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.06it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.15it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.20it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.12it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.19it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.16it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.21it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.29it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.22it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.15it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.18it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.10it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.13it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.15it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.17it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.14it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.25it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.15it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.17it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.08it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.12it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.16it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.18it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 43.99it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.03it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.12it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.21it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.17it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.13it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.09it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.19it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.17it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.16it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.16it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.05it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.17it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.14it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.19it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.15it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.18it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.10it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.20it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.15it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.06it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.08it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.11it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.15it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.27it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.18it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.23it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.24it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.02it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.06it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.20it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.20it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.11it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.23it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.18it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.21it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.20it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.12it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.11it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.19it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.03it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.14it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.14it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.16it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.22it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.23it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.08it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.14it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.09it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.14it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.15it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.17it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.06it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.18it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 42.66it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 43.39it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 43.56it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 43.67it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 43.84it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.01it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.03it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.04it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.03it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.03it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.14it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.20it/s][A                                                 
                                                 [A 60%|██████    | 234/390 [01:53<00:45,  3.44it/s]
100%|██████████| 505/505 [00:11<00:00, 44.20it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 08:36:04,523 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 08:36:04,543 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:36:06,389 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:36:06,406 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:36:06,425 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [01:59<14:09,  5.48s/it] 61%|██████    | 236/390 [01:59<10:04,  3.93s/it] 61%|██████    | 237/390 [02:00<07:13,  2.84s/it] 61%|██████    | 238/390 [02:00<05:15,  2.07s/it] 61%|██████▏   | 239/390 [02:00<03:52,  1.54s/it] 62%|██████▏   | 240/390 [02:01<02:54,  1.17s/it] 62%|██████▏   | 241/390 [02:01<02:14,  1.11it/s] 62%|██████▏   | 242/390 [02:01<01:46,  1.39it/s] 62%|██████▏   | 243/390 [02:02<01:27,  1.69it/s] 63%|██████▎   | 244/390 [02:02<01:13,  1.99it/s] 63%|██████▎   | 245/390 [02:02<01:03,  2.27it/s] 63%|██████▎   | 246/390 [02:02<00:57,  2.52it/s] 63%|██████▎   | 247/390 [02:03<00:52,  2.73it/s] 64%|██████▎   | 248/390 [02:03<00:48,  2.90it/s] 64%|██████▍   | 249/390 [02:03<00:46,  3.03it/s] 64%|██████▍   | 250/390 [02:04<00:44,  3.13it/s] 64%|██████▍   | 251/390 [02:04<00:43,  3.21it/s] 65%|██████▍   | 252/390 [02:04<00:42,  3.26it/s] 65%|██████▍   | 253/390 [02:04<00:41,  3.30it/s] 65%|██████▌   | 254/390 [02:05<00:40,  3.33it/s] 65%|██████▌   | 255/390 [02:05<00:40,  3.35it/s] 66%|██████▌   | 256/390 [02:05<00:39,  3.37it/s] 66%|██████▌   | 257/390 [02:06<00:39,  3.38it/s] 66%|██████▌   | 258/390 [02:06<00:39,  3.38it/s] 66%|██████▋   | 259/390 [02:06<00:38,  3.38it/s] 67%|██████▋   | 260/390 [02:07<00:38,  3.39it/s] 67%|██████▋   | 261/390 [02:07<00:38,  3.39it/s] 67%|██████▋   | 262/390 [02:07<00:37,  3.39it/s] 67%|██████▋   | 263/390 [02:07<00:37,  3.39it/s] 68%|██████▊   | 264/390 [02:08<00:37,  3.40it/s] 68%|██████▊   | 265/390 [02:08<00:36,  3.40it/s] 68%|██████▊   | 266/390 [02:08<00:36,  3.39it/s] 68%|██████▊   | 267/390 [02:09<00:36,  3.40it/s] 69%|██████▊   | 268/390 [02:09<00:35,  3.40it/s] 69%|██████▉   | 269/390 [02:09<00:35,  3.39it/s] 69%|██████▉   | 270/390 [02:09<00:35,  3.39it/s] 69%|██████▉   | 271/390 [02:10<00:35,  3.39it/s] 70%|██████▉   | 272/390 [02:10<00:34,  3.39it/s] 70%|███████   | 273/390 [02:10<00:34,  3.39it/s] 70%|███████   | 274/390 [02:11<00:34,  3.39it/s] 71%|███████   | 275/390 [02:11<00:33,  3.40it/s] 71%|███████   | 276/390 [02:11<00:33,  3.40it/s] 71%|███████   | 277/390 [02:12<00:33,  3.39it/s] 71%|███████▏  | 278/390 [02:12<00:32,  3.39it/s] 72%|███████▏  | 279/390 [02:12<00:32,  3.40it/s] 72%|███████▏  | 280/390 [02:12<00:32,  3.39it/s] 72%|███████▏  | 281/390 [02:13<00:32,  3.39it/s] 72%|███████▏  | 282/390 [02:13<00:31,  3.38it/s] 73%|███████▎  | 283/390 [02:13<00:31,  3.39it/s] 73%|███████▎  | 284/390 [02:14<00:31,  3.39it/s] 73%|███████▎  | 285/390 [02:14<00:30,  3.39it/s] 73%|███████▎  | 286/390 [02:14<00:30,  3.39it/s] 74%|███████▎  | 287/390 [02:14<00:30,  3.40it/s] 74%|███████▍  | 288/390 [02:15<00:30,  3.39it/s] 74%|███████▍  | 289/390 [02:15<00:29,  3.39it/s] 74%|███████▍  | 290/390 [02:15<00:29,  3.39it/s] 75%|███████▍  | 291/390 [02:16<00:29,  3.33it/s] 75%|███████▍  | 292/390 [02:16<00:29,  3.35it/s] 75%|███████▌  | 293/390 [02:16<00:28,  3.36it/s] 75%|███████▌  | 294/390 [02:17<00:28,  3.37it/s] 76%|███████▌  | 295/390 [02:17<00:28,  3.38it/s] 76%|███████▌  | 296/390 [02:17<00:27,  3.38it/s] 76%|███████▌  | 297/390 [02:17<00:27,  3.39it/s] 76%|███████▋  | 298/390 [02:18<00:27,  3.39it/s] 77%|███████▋  | 299/390 [02:18<00:26,  3.39it/s] 77%|███████▋  | 300/390 [02:18<00:26,  3.39it/s] 77%|███████▋  | 301/390 [02:19<00:26,  3.39it/s] 77%|███████▋  | 302/390 [02:19<00:25,  3.39it/s] 78%|███████▊  | 303/390 [02:19<00:25,  3.39it/s] 78%|███████▊  | 304/390 [02:20<00:25,  3.39it/s] 78%|███████▊  | 305/390 [02:20<00:25,  3.39it/s] 78%|███████▊  | 306/390 [02:20<00:24,  3.39it/s] 79%|███████▊  | 307/390 [02:20<00:24,  3.39it/s] 79%|███████▉  | 308/390 [02:21<00:24,  3.39it/s] 79%|███████▉  | 309/390 [02:21<00:23,  3.39it/s] 79%|███████▉  | 310/390 [02:21<00:23,  3.40it/s] 80%|███████▉  | 311/390 [02:22<00:23,  3.39it/s] 80%|████████  | 312/390 [02:22<00:22,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 08:36:33,347 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:36:33,347 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 08:36:33,347 >>   Batch size = 8
{'eval_loss': 1.0433964729309082, 'eval_runtime': 11.461, 'eval_samples_per_second': 352.414, 'eval_steps_per_second': 44.063, 'epoch': 2.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 54.98it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.73it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.15it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.34it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.69it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.42it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.41it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.26it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.24it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.43it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.44it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.27it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.14it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.10it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.98it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.11it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.08it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.20it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.33it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.30it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.21it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.14it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 43.95it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.00it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.07it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.09it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.21it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.24it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.29it/s][A
 30%|███       | 152/505 [00:03<00:12, 28.51it/s][A
 31%|███       | 157/505 [00:03<00:10, 31.91it/s][A
 32%|███▏      | 162/505 [00:03<00:09, 34.91it/s][A
 33%|███▎      | 167/505 [00:03<00:09, 37.29it/s][A
 34%|███▍      | 172/505 [00:04<00:08, 39.32it/s][A
 35%|███▌      | 177/505 [00:04<00:08, 40.83it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 41.91it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 42.56it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 42.57it/s][A
 39%|███▉      | 197/505 [00:04<00:07, 42.76it/s][A
 40%|████      | 202/505 [00:04<00:07, 43.04it/s][A
 41%|████      | 207/505 [00:04<00:06, 43.49it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 43.75it/s][A
 43%|████▎     | 217/505 [00:05<00:06, 44.07it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.25it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.41it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.21it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 43.95it/s][A
 48%|████▊     | 242/505 [00:05<00:06, 43.76it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 43.69it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.00it/s][A
 51%|█████     | 257/505 [00:06<00:05, 44.14it/s][A
 52%|█████▏    | 262/505 [00:06<00:05, 44.32it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.28it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.42it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.25it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.08it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 43.92it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 43.86it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.00it/s][A
 60%|█████▉    | 302/505 [00:07<00:04, 44.14it/s][A
 61%|██████    | 307/505 [00:07<00:04, 44.32it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.37it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.37it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.25it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.05it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 43.90it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 43.84it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.02it/s][A
 69%|██████▊   | 347/505 [00:08<00:03, 44.24it/s][A
 70%|██████▉   | 352/505 [00:08<00:03, 44.33it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.36it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.34it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.19it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.04it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.03it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 43.95it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.14it/s][A
 78%|███████▊  | 392/505 [00:09<00:02, 44.13it/s][A
 79%|███████▊  | 397/505 [00:09<00:02, 44.25it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.32it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.25it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.20it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.02it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.08it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.12it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.21it/s][A
 87%|████████▋ | 437/505 [00:10<00:01, 44.19it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 44.28it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.23it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.20it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.03it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.07it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.10it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.19it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.08it/s][A
 95%|█████████▌| 482/505 [00:11<00:00, 44.19it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.26it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.24it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.18it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.12it/s][A                                                 
                                                 [A 80%|████████  | 312/390 [02:34<00:22,  3.39it/s]
100%|██████████| 505/505 [00:11<00:00, 44.12it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 08:36:45,021 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 08:36:45,037 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:36:46,740 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:36:46,755 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:36:46,764 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:39<06:46,  5.28s/it] 81%|████████  | 314/390 [02:39<04:47,  3.78s/it] 81%|████████  | 315/390 [02:39<03:25,  2.74s/it] 81%|████████  | 316/390 [02:40<02:28,  2.00s/it] 81%|████████▏ | 317/390 [02:40<01:48,  1.49s/it] 82%|████████▏ | 318/390 [02:40<01:21,  1.13s/it] 82%|████████▏ | 319/390 [02:41<01:02,  1.14it/s] 82%|████████▏ | 320/390 [02:41<00:49,  1.42it/s] 82%|████████▏ | 321/390 [02:41<00:39,  1.73it/s] 83%|████████▎ | 322/390 [02:41<00:33,  2.03it/s] 83%|████████▎ | 323/390 [02:42<00:28,  2.32it/s] 83%|████████▎ | 324/390 [02:42<00:25,  2.57it/s] 83%|████████▎ | 325/390 [02:42<00:23,  2.77it/s] 84%|████████▎ | 326/390 [02:43<00:21,  2.94it/s] 84%|████████▍ | 327/390 [02:43<00:20,  3.08it/s] 84%|████████▍ | 328/390 [02:43<00:19,  3.18it/s] 84%|████████▍ | 329/390 [02:43<00:18,  3.25it/s] 85%|████████▍ | 330/390 [02:44<00:18,  3.31it/s] 85%|████████▍ | 331/390 [02:44<00:17,  3.35it/s] 85%|████████▌ | 332/390 [02:44<00:17,  3.38it/s] 85%|████████▌ | 333/390 [02:45<00:16,  3.40it/s] 86%|████████▌ | 334/390 [02:45<00:16,  3.41it/s] 86%|████████▌ | 335/390 [02:45<00:16,  3.42it/s] 86%|████████▌ | 336/390 [02:45<00:15,  3.41it/s] 86%|████████▋ | 337/390 [02:46<00:15,  3.42it/s] 87%|████████▋ | 338/390 [02:46<00:15,  3.43it/s] 87%|████████▋ | 339/390 [02:46<00:14,  3.43it/s] 87%|████████▋ | 340/390 [02:47<00:14,  3.44it/s] 87%|████████▋ | 341/390 [02:47<00:14,  3.44it/s] 88%|████████▊ | 342/390 [02:47<00:13,  3.44it/s] 88%|████████▊ | 343/390 [02:48<00:13,  3.44it/s] 88%|████████▊ | 344/390 [02:48<00:13,  3.44it/s] 88%|████████▊ | 345/390 [02:48<00:13,  3.44it/s] 89%|████████▊ | 346/390 [02:48<00:12,  3.44it/s] 89%|████████▉ | 347/390 [02:49<00:12,  3.42it/s] 89%|████████▉ | 348/390 [02:49<00:12,  3.43it/s] 89%|████████▉ | 349/390 [02:49<00:11,  3.43it/s] 90%|████████▉ | 350/390 [02:50<00:11,  3.43it/s] 90%|█████████ | 351/390 [02:50<00:11,  3.44it/s] 90%|█████████ | 352/390 [02:50<00:11,  3.44it/s] 91%|█████████ | 353/390 [02:50<00:10,  3.44it/s] 91%|█████████ | 354/390 [02:51<00:10,  3.44it/s] 91%|█████████ | 355/390 [02:51<00:10,  3.42it/s] 91%|█████████▏| 356/390 [02:51<00:09,  3.43it/s] 92%|█████████▏| 357/390 [02:52<00:09,  3.44it/s] 92%|█████████▏| 358/390 [02:52<00:09,  3.36it/s] 92%|█████████▏| 359/390 [02:52<00:09,  3.38it/s] 92%|█████████▏| 360/390 [02:52<00:08,  3.41it/s] 93%|█████████▎| 361/390 [02:53<00:08,  3.42it/s] 93%|█████████▎| 362/390 [02:53<00:08,  3.42it/s] 93%|█████████▎| 363/390 [02:53<00:07,  3.43it/s] 93%|█████████▎| 364/390 [02:54<00:07,  3.44it/s] 94%|█████████▎| 365/390 [02:54<00:07,  3.44it/s] 94%|█████████▍| 366/390 [02:54<00:06,  3.44it/s] 94%|█████████▍| 367/390 [02:55<00:06,  3.44it/s] 94%|█████████▍| 368/390 [02:55<00:06,  3.44it/s] 95%|█████████▍| 369/390 [02:55<00:06,  3.42it/s] 95%|█████████▍| 370/390 [02:55<00:05,  3.43it/s] 95%|█████████▌| 371/390 [02:56<00:05,  3.43it/s] 95%|█████████▌| 372/390 [02:56<00:05,  3.43it/s] 96%|█████████▌| 373/390 [02:56<00:04,  3.44it/s] 96%|█████████▌| 374/390 [02:57<00:04,  3.44it/s] 96%|█████████▌| 375/390 [02:57<00:04,  3.44it/s] 96%|█████████▋| 376/390 [02:57<00:04,  3.44it/s] 97%|█████████▋| 377/390 [02:57<00:03,  3.44it/s] 97%|█████████▋| 378/390 [02:58<00:03,  3.44it/s] 97%|█████████▋| 379/390 [02:58<00:03,  3.44it/s] 97%|█████████▋| 380/390 [02:58<00:02,  3.44it/s] 98%|█████████▊| 381/390 [02:59<00:02,  3.44it/s] 98%|█████████▊| 382/390 [02:59<00:02,  3.44it/s] 98%|█████████▊| 383/390 [02:59<00:02,  3.44it/s] 98%|█████████▊| 384/390 [02:59<00:01,  3.43it/s] 99%|█████████▊| 385/390 [03:00<00:01,  3.44it/s] 99%|█████████▉| 386/390 [03:00<00:01,  3.44it/s] 99%|█████████▉| 387/390 [03:00<00:00,  3.44it/s] 99%|█████████▉| 388/390 [03:01<00:00,  3.44it/s]100%|█████████▉| 389/390 [03:01<00:00,  3.44it/s]100%|██████████| 390/390 [03:01<00:00,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 08:37:12,650 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:37:12,650 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 08:37:12,650 >>   Batch size = 8
{'eval_loss': 1.045894980430603, 'eval_runtime': 11.6545, 'eval_samples_per_second': 346.56, 'eval_steps_per_second': 43.331, 'epoch': 3.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.58it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.71it/s][A
  3%|▎         | 17/505 [00:00<00:10, 45.97it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.27it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.91it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.57it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.43it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.35it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.42it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.45it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.36it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.15it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.17it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.11it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.05it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.15it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.20it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.29it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.19it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.23it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.16it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.12it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.05it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.06it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.18it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.28it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.33it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.24it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.21it/s][A
 30%|███       | 152/505 [00:03<00:08, 44.07it/s][A
 31%|███       | 157/505 [00:03<00:07, 43.99it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.01it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.07it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.15it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.24it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.31it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.29it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.22it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.17it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.05it/s][A
 41%|████      | 207/505 [00:04<00:06, 43.94it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.00it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.10it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.23it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.33it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.30it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.27it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.17it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.10it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 43.98it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.02it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.12it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.23it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.33it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.28it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.21it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.14it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.07it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.01it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 43.98it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.13it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.20it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.25it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.29it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.21it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.20it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.08it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 43.98it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.10it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.11it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.22it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.32it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.26it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.12it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.08it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 43.96it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 43.95it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.11it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.10it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.13it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.20it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.24it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.24it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.08it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.00it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 43.99it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.10it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.25it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.25it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.27it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.23it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.17it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.04it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 43.98it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 43.97it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.14it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.25it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.24it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.34it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.27it/s][A                                                 
                                                 [A100%|██████████| 390/390 [03:13<00:00,  3.44it/s]
100%|██████████| 505/505 [00:11<00:00, 44.27it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 08:37:24,101 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 08:37:24,124 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:37:25,820 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:37:25,836 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:37:25,850 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 08:37:29,391 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 08:37:29,394 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-156 (score: 1.0413392782211304).
                                                 100%|██████████| 390/390 [03:20<00:00,  3.44it/s]100%|██████████| 390/390 [03:20<00:00,  1.95it/s]
[INFO|trainer.py:1894] 2023-08-28 08:37:31,373 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 08:37:31,390 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:37:33,243 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:37:33,261 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:37:33,271 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 08:37:33,460 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:37:33,461 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:37:33,461 >>   train_loss               =     0.7566
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:37:33,461 >>   train_runtime            = 0:03:20.42
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:37:33,461 >>   train_samples            =       5002
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:37:33,461 >>   train_samples_per_second =    124.785
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:37:33,461 >>   train_steps_per_second   =      1.946
{'eval_loss': 1.0489449501037598, 'eval_runtime': 11.4371, 'eval_samples_per_second': 353.15, 'eval_steps_per_second': 44.155, 'epoch': 4.99}
{'train_runtime': 200.4248, 'train_samples_per_second': 124.785, 'train_steps_per_second': 1.946, 'train_loss': 0.7565929706280048, 'epoch': 4.99}
08/28/2023 08:37:33 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 08:37:33,547 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:37:33,547 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 08:37:33,547 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:08, 55.59it/s]  2%|▏         | 12/505 [00:00<00:10, 48.74it/s]  3%|▎         | 17/505 [00:00<00:10, 46.98it/s]  4%|▍         | 22/505 [00:00<00:10, 46.28it/s]  5%|▌         | 27/505 [00:00<00:10, 45.86it/s]  6%|▋         | 32/505 [00:00<00:10, 45.61it/s]  7%|▋         | 37/505 [00:00<00:10, 45.34it/s]  8%|▊         | 42/505 [00:00<00:10, 44.79it/s]  9%|▉         | 47/505 [00:01<00:10, 44.18it/s] 10%|█         | 52/505 [00:01<00:10, 44.11it/s] 11%|█▏        | 57/505 [00:01<00:10, 44.25it/s] 12%|█▏        | 62/505 [00:01<00:09, 44.33it/s] 13%|█▎        | 67/505 [00:01<00:09, 44.55it/s] 14%|█▍        | 72/505 [00:01<00:09, 44.60it/s] 15%|█▌        | 77/505 [00:01<00:09, 44.66it/s] 16%|█▌        | 82/505 [00:01<00:09, 44.68it/s] 17%|█▋        | 87/505 [00:01<00:09, 44.40it/s] 18%|█▊        | 92/505 [00:02<00:09, 44.03it/s] 19%|█▉        | 97/505 [00:02<00:09, 44.01it/s] 20%|██        | 102/505 [00:02<00:09, 44.11it/s] 21%|██        | 107/505 [00:02<00:08, 44.32it/s] 22%|██▏       | 112/505 [00:02<00:08, 44.46it/s] 23%|██▎       | 117/505 [00:02<00:08, 44.68it/s] 24%|██▍       | 122/505 [00:02<00:08, 44.69it/s] 25%|██▌       | 127/505 [00:02<00:08, 44.50it/s] 26%|██▌       | 132/505 [00:02<00:08, 44.31it/s] 27%|██▋       | 137/505 [00:03<00:08, 43.86it/s] 28%|██▊       | 142/505 [00:03<00:08, 44.07it/s] 29%|██▉       | 147/505 [00:03<00:08, 44.22it/s] 30%|███       | 152/505 [00:03<00:07, 44.38it/s] 31%|███       | 157/505 [00:03<00:07, 44.51it/s] 32%|███▏      | 162/505 [00:03<00:07, 44.64it/s] 33%|███▎      | 167/505 [00:03<00:07, 44.67it/s] 34%|███▍      | 172/505 [00:03<00:07, 44.52it/s] 35%|███▌      | 177/505 [00:03<00:07, 44.22it/s] 36%|███▌      | 182/505 [00:04<00:07, 44.04it/s] 37%|███▋      | 187/505 [00:04<00:07, 44.11it/s] 38%|███▊      | 192/505 [00:04<00:07, 44.26it/s] 39%|███▉      | 197/505 [00:04<00:06, 44.37it/s] 40%|████      | 202/505 [00:04<00:06, 44.52it/s] 41%|████      | 207/505 [00:04<00:06, 44.41it/s] 42%|████▏     | 212/505 [00:04<00:06, 44.57it/s] 43%|████▎     | 217/505 [00:04<00:06, 44.43it/s] 44%|████▍     | 222/505 [00:04<00:06, 44.20it/s] 45%|████▍     | 227/505 [00:05<00:06, 44.10it/s] 46%|████▌     | 232/505 [00:05<00:06, 44.13it/s] 47%|████▋     | 237/505 [00:05<00:06, 44.19it/s] 48%|████▊     | 242/505 [00:05<00:05, 44.39it/s] 49%|████▉     | 247/505 [00:05<00:05, 44.47it/s] 50%|████▉     | 252/505 [00:05<00:05, 44.56it/s] 51%|█████     | 257/505 [00:05<00:05, 44.56it/s] 52%|█████▏    | 262/505 [00:05<00:05, 44.44it/s] 53%|█████▎    | 267/505 [00:05<00:05, 44.16it/s] 54%|█████▍    | 272/505 [00:06<00:05, 44.01it/s] 55%|█████▍    | 277/505 [00:06<00:05, 44.06it/s] 56%|█████▌    | 282/505 [00:06<00:05, 44.21it/s] 57%|█████▋    | 287/505 [00:06<00:04, 44.39it/s] 58%|█████▊    | 292/505 [00:06<00:04, 44.50it/s] 59%|█████▉    | 297/505 [00:06<00:04, 44.57it/s] 60%|█████▉    | 302/505 [00:06<00:04, 44.59it/s] 61%|██████    | 307/505 [00:06<00:04, 44.36it/s] 62%|██████▏   | 312/505 [00:07<00:04, 44.19it/s] 63%|██████▎   | 317/505 [00:07<00:04, 44.16it/s] 64%|██████▍   | 322/505 [00:07<00:04, 44.17it/s] 65%|██████▍   | 327/505 [00:07<00:04, 44.20it/s] 66%|██████▌   | 332/505 [00:07<00:03, 44.40it/s] 67%|██████▋   | 337/505 [00:07<00:03, 44.51it/s] 68%|██████▊   | 342/505 [00:07<00:03, 44.57it/s] 69%|██████▊   | 347/505 [00:07<00:03, 44.45it/s] 70%|██████▉   | 352/505 [00:07<00:03, 44.31it/s] 71%|███████   | 357/505 [00:08<00:03, 44.12it/s] 72%|███████▏  | 362/505 [00:08<00:03, 44.13it/s] 73%|███████▎  | 367/505 [00:08<00:03, 44.18it/s] 74%|███████▎  | 372/505 [00:08<00:03, 44.19it/s] 75%|███████▍  | 377/505 [00:08<00:02, 44.28it/s] 76%|███████▌  | 382/505 [00:08<00:02, 44.46it/s] 77%|███████▋  | 387/505 [00:08<00:02, 44.59it/s] 78%|███████▊  | 392/505 [00:08<00:02, 44.45it/s] 79%|███████▊  | 397/505 [00:08<00:02, 44.31it/s] 80%|███████▉  | 402/505 [00:09<00:02, 44.13it/s] 81%|████████  | 407/505 [00:09<00:02, 44.11it/s] 82%|████████▏ | 412/505 [00:09<00:02, 44.17it/s] 83%|████████▎ | 417/505 [00:09<00:01, 44.21it/s] 84%|████████▎ | 422/505 [00:09<00:01, 44.30it/s] 85%|████████▍ | 427/505 [00:09<00:01, 44.45it/s] 86%|████████▌ | 432/505 [00:09<00:01, 44.46it/s] 87%|████████▋ | 437/505 [00:09<00:01, 44.45it/s] 88%|████████▊ | 442/505 [00:09<00:01, 44.40it/s] 89%|████████▊ | 447/505 [00:10<00:01, 44.17it/s] 90%|████████▉ | 452/505 [00:10<00:01, 44.14it/s] 90%|█████████ | 457/505 [00:10<00:01, 44.14it/s] 91%|█████████▏| 462/505 [00:10<00:00, 44.14it/s] 92%|█████████▏| 467/505 [00:10<00:00, 44.34it/s] 93%|█████████▎| 472/505 [00:10<00:00, 44.45it/s] 94%|█████████▍| 477/505 [00:10<00:00, 44.47it/s] 95%|█████████▌| 482/505 [00:10<00:00, 44.42it/s] 96%|█████████▋| 487/505 [00:10<00:00, 44.40it/s] 97%|█████████▋| 492/505 [00:11<00:00, 44.14it/s] 98%|█████████▊| 497/505 [00:11<00:00, 44.13it/s] 99%|█████████▉| 502/505 [00:11<00:00, 44.09it/s]100%|██████████| 505/505 [00:11<00:00, 44.40it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 08:37:44,940 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:37:44,940 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:37:44,940 >>   eval_loss               =     1.0413
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:37:44,940 >>   eval_runtime            = 0:00:11.39
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:37:44,940 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:37:44,940 >>   eval_samples_per_second =    354.526
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:37:44,940 >>   eval_steps_per_second   =     44.327
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:37:44,940 >>   perplexity              =      2.833
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:37:49,894 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:37:49,899 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:37:49,899 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:37:49,899 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:37:49,900 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:37:50,623 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:37:50,623 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:37:51,280 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:37:52,315 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:37:52,316 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:37:53,591 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:37:53,595 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:37:53,595 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:37:53,595 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:37:53,595 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:37:53,909 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:37:53,910 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:37:54,177 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:37:54,326 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:37:54,326 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:04,  1.61it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.62it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:08,  1.56it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:10,  1.54it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:13,  1.56it/s]Extractor Predicting: 23it [00:14,  1.52it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:15,  1.56it/s]Extractor Predicting: 26it [00:16,  1.58it/s]Extractor Predicting: 27it [00:17,  1.61it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:18,  1.60it/s]Extractor Predicting: 30it [00:19,  1.57it/s]Extractor Predicting: 31it [00:19,  1.59it/s]Extractor Predicting: 32it [00:20,  1.57it/s]Extractor Predicting: 33it [00:20,  1.56it/s]Extractor Predicting: 34it [00:21,  1.55it/s]Extractor Predicting: 35it [00:22,  1.55it/s]Extractor Predicting: 36it [00:22,  1.60it/s]Extractor Predicting: 37it [00:23,  1.66it/s]Extractor Predicting: 38it [00:24,  1.63it/s]Extractor Predicting: 39it [00:24,  1.65it/s]Extractor Predicting: 40it [00:25,  1.61it/s]Extractor Predicting: 41it [00:25,  1.60it/s]Extractor Predicting: 42it [00:26,  1.58it/s]Extractor Predicting: 43it [00:27,  1.62it/s]Extractor Predicting: 44it [00:27,  1.61it/s]Extractor Predicting: 45it [00:28,  1.64it/s]Extractor Predicting: 46it [00:29,  1.60it/s]Extractor Predicting: 47it [00:29,  1.58it/s]Extractor Predicting: 48it [00:30,  1.51it/s]Extractor Predicting: 49it [00:30,  1.56it/s]Extractor Predicting: 50it [00:31,  1.59it/s]Extractor Predicting: 51it [00:32,  1.61it/s]Extractor Predicting: 52it [00:32,  1.65it/s]Extractor Predicting: 53it [00:33,  1.64it/s]Extractor Predicting: 54it [00:34,  1.64it/s]Extractor Predicting: 55it [00:34,  1.52it/s]Extractor Predicting: 56it [00:35,  1.55it/s]Extractor Predicting: 57it [00:36,  1.56it/s]Extractor Predicting: 58it [00:36,  1.53it/s]Extractor Predicting: 59it [00:37,  1.55it/s]Extractor Predicting: 60it [00:37,  1.55it/s]Extractor Predicting: 61it [00:38,  1.60it/s]Extractor Predicting: 62it [00:39,  1.59it/s]Extractor Predicting: 63it [00:39,  1.57it/s]Extractor Predicting: 64it [00:40,  1.57it/s]Extractor Predicting: 65it [00:41,  1.55it/s]Extractor Predicting: 66it [00:41,  1.58it/s]Extractor Predicting: 67it [00:42,  1.56it/s]Extractor Predicting: 68it [00:43,  1.53it/s]Extractor Predicting: 69it [00:43,  1.51it/s]Extractor Predicting: 70it [00:44,  1.54it/s]Extractor Predicting: 71it [00:45,  1.53it/s]Extractor Predicting: 72it [00:45,  1.50it/s]Extractor Predicting: 73it [00:46,  1.49it/s]Extractor Predicting: 74it [00:47,  1.45it/s]Extractor Predicting: 75it [00:47,  1.47it/s]Extractor Predicting: 76it [00:48,  1.51it/s]Extractor Predicting: 77it [00:49,  1.51it/s]Extractor Predicting: 78it [00:49,  1.52it/s]Extractor Predicting: 79it [00:50,  1.53it/s]Extractor Predicting: 80it [00:51,  1.52it/s]Extractor Predicting: 81it [00:51,  1.53it/s]Extractor Predicting: 82it [00:52,  1.58it/s]Extractor Predicting: 83it [00:52,  1.54it/s]Extractor Predicting: 84it [00:53,  1.56it/s]Extractor Predicting: 85it [00:54,  1.54it/s]Extractor Predicting: 86it [00:54,  1.56it/s]Extractor Predicting: 87it [00:55,  1.59it/s]Extractor Predicting: 88it [00:56,  1.59it/s]Extractor Predicting: 89it [00:56,  1.58it/s]Extractor Predicting: 90it [00:57,  1.59it/s]Extractor Predicting: 91it [00:58,  1.59it/s]Extractor Predicting: 92it [00:58,  1.61it/s]Extractor Predicting: 93it [00:59,  1.59it/s]Extractor Predicting: 94it [00:59,  1.57it/s]Extractor Predicting: 95it [01:00,  1.55it/s]Extractor Predicting: 96it [01:01,  1.54it/s]Extractor Predicting: 97it [01:01,  1.54it/s]Extractor Predicting: 98it [01:02,  1.54it/s]Extractor Predicting: 99it [01:03,  1.54it/s]Extractor Predicting: 100it [01:03,  1.53it/s]Extractor Predicting: 101it [01:04,  1.51it/s]Extractor Predicting: 102it [01:05,  1.51it/s]Extractor Predicting: 103it [01:05,  1.52it/s]Extractor Predicting: 104it [01:06,  1.56it/s]Extractor Predicting: 105it [01:07,  1.55it/s]Extractor Predicting: 106it [01:07,  1.58it/s]Extractor Predicting: 107it [01:08,  1.58it/s]Extractor Predicting: 108it [01:08,  1.59it/s]Extractor Predicting: 109it [01:09,  1.60it/s]Extractor Predicting: 110it [01:10,  1.58it/s]Extractor Predicting: 111it [01:10,  1.54it/s]Extractor Predicting: 112it [01:11,  1.58it/s]Extractor Predicting: 113it [01:12,  1.57it/s]Extractor Predicting: 114it [01:12,  1.55it/s]Extractor Predicting: 115it [01:13,  1.59it/s]Extractor Predicting: 116it [01:14,  1.59it/s]Extractor Predicting: 117it [01:14,  1.58it/s]Extractor Predicting: 118it [01:15,  1.53it/s]Extractor Predicting: 119it [01:16,  1.57it/s]Extractor Predicting: 120it [01:16,  1.57it/s]Extractor Predicting: 121it [01:17,  1.55it/s]Extractor Predicting: 122it [01:17,  1.55it/s]Extractor Predicting: 123it [01:18,  1.57it/s]Extractor Predicting: 124it [01:19,  1.57it/s]Extractor Predicting: 125it [01:20,  1.44it/s]Extractor Predicting: 126it [01:20,  1.49it/s]Extractor Predicting: 127it [01:21,  1.44it/s]Extractor Predicting: 128it [01:22,  1.43it/s]Extractor Predicting: 129it [01:22,  1.44it/s]Extractor Predicting: 130it [01:23,  1.44it/s]Extractor Predicting: 131it [01:24,  1.41it/s]Extractor Predicting: 132it [01:24,  1.42it/s]Extractor Predicting: 133it [01:25,  1.43it/s]Extractor Predicting: 134it [01:26,  1.47it/s]Extractor Predicting: 135it [01:26,  1.47it/s]Extractor Predicting: 136it [01:27,  1.47it/s]Extractor Predicting: 137it [01:28,  1.50it/s]Extractor Predicting: 138it [01:29,  1.34it/s]Extractor Predicting: 139it [01:29,  1.38it/s]Extractor Predicting: 140it [01:30,  1.39it/s]Extractor Predicting: 141it [01:31,  1.41it/s]Extractor Predicting: 142it [01:31,  1.43it/s]Extractor Predicting: 143it [01:32,  1.44it/s]Extractor Predicting: 144it [01:33,  1.45it/s]Extractor Predicting: 145it [01:34,  1.43it/s]Extractor Predicting: 146it [01:34,  1.45it/s]Extractor Predicting: 147it [01:35,  1.46it/s]Extractor Predicting: 148it [01:36,  1.42it/s]Extractor Predicting: 149it [01:36,  1.49it/s]Extractor Predicting: 149it [01:36,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:39:39,184 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:39:39,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:39:39,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:39:39,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:39:39,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:39:39,785 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:39:39,786 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:39:40,351 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:39:41,396 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:39:41,396 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:39:43,160 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:39:43,165 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:39:43,165 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:39:43,165 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:39:43,165 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:39:43,478 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:39:43,479 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:39:44,107 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:39:44,275 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:39:44,275 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.16718266253869968,
  "recall": 0.01336964595196831,
  "score": 0.02475928473177441,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.78it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.68it/s]Extractor Predicting: 9it [00:05,  1.76it/s]Extractor Predicting: 10it [00:05,  1.75it/s]Extractor Predicting: 11it [00:06,  1.74it/s]Extractor Predicting: 12it [00:07,  1.75it/s]Extractor Predicting: 13it [00:07,  1.83it/s]Extractor Predicting: 14it [00:08,  1.81it/s]Extractor Predicting: 15it [00:08,  1.82it/s]Extractor Predicting: 16it [00:09,  1.82it/s]Extractor Predicting: 17it [00:09,  1.81it/s]Extractor Predicting: 18it [00:10,  1.77it/s]Extractor Predicting: 19it [00:11,  1.74it/s]Extractor Predicting: 20it [00:11,  1.74it/s]Extractor Predicting: 21it [00:12,  1.74it/s]Extractor Predicting: 22it [00:12,  1.77it/s]Extractor Predicting: 23it [00:13,  1.79it/s]Extractor Predicting: 24it [00:13,  1.77it/s]Extractor Predicting: 25it [00:14,  1.80it/s]Extractor Predicting: 26it [00:14,  1.75it/s]Extractor Predicting: 27it [00:15,  1.81it/s]Extractor Predicting: 28it [00:16,  1.82it/s]Extractor Predicting: 29it [00:16,  1.80it/s]Extractor Predicting: 30it [00:17,  1.80it/s]Extractor Predicting: 31it [00:17,  1.82it/s]Extractor Predicting: 32it [00:18,  1.81it/s]Extractor Predicting: 33it [00:18,  1.76it/s]Extractor Predicting: 34it [00:19,  1.73it/s]Extractor Predicting: 35it [00:20,  1.71it/s]Extractor Predicting: 36it [00:20,  1.77it/s]Extractor Predicting: 37it [00:21,  1.73it/s]Extractor Predicting: 38it [00:21,  1.71it/s]Extractor Predicting: 39it [00:22,  1.72it/s]Extractor Predicting: 40it [00:22,  1.70it/s]Extractor Predicting: 41it [00:23,  1.68it/s]Extractor Predicting: 42it [00:24,  1.71it/s]Extractor Predicting: 43it [00:24,  1.73it/s]Extractor Predicting: 44it [00:25,  1.73it/s]Extractor Predicting: 45it [00:26,  1.59it/s]Extractor Predicting: 46it [00:26,  1.56it/s]Extractor Predicting: 47it [00:27,  1.59it/s]Extractor Predicting: 48it [00:27,  1.61it/s]Extractor Predicting: 49it [00:28,  1.65it/s]Extractor Predicting: 50it [00:29,  1.67it/s]Extractor Predicting: 51it [00:29,  1.71it/s]Extractor Predicting: 52it [00:30,  1.68it/s]Extractor Predicting: 53it [00:30,  1.70it/s]Extractor Predicting: 54it [00:31,  1.70it/s]Extractor Predicting: 55it [00:31,  1.68it/s]Extractor Predicting: 56it [00:32,  1.62it/s]Extractor Predicting: 57it [00:33,  1.63it/s]Extractor Predicting: 58it [00:33,  1.65it/s]Extractor Predicting: 59it [00:34,  1.63it/s]Extractor Predicting: 60it [00:35,  1.64it/s]Extractor Predicting: 61it [00:35,  1.64it/s]Extractor Predicting: 62it [00:36,  1.66it/s]Extractor Predicting: 63it [00:36,  1.64it/s]Extractor Predicting: 64it [00:37,  1.67it/s]Extractor Predicting: 65it [00:38,  1.70it/s]Extractor Predicting: 66it [00:38,  1.70it/s]Extractor Predicting: 67it [00:39,  1.68it/s]Extractor Predicting: 68it [00:39,  1.67it/s]Extractor Predicting: 69it [00:40,  1.65it/s]Extractor Predicting: 70it [00:41,  1.63it/s]Extractor Predicting: 71it [00:41,  1.62it/s]Extractor Predicting: 72it [00:42,  1.63it/s]Extractor Predicting: 73it [00:42,  1.62it/s]Extractor Predicting: 74it [00:43,  1.64it/s]Extractor Predicting: 75it [00:44,  1.66it/s]Extractor Predicting: 76it [00:44,  1.64it/s]Extractor Predicting: 77it [00:45,  1.65it/s]Extractor Predicting: 78it [00:46,  1.59it/s]Extractor Predicting: 79it [00:46,  1.63it/s]Extractor Predicting: 80it [00:47,  1.67it/s]Extractor Predicting: 81it [00:47,  1.64it/s]Extractor Predicting: 82it [00:48,  1.60it/s]Extractor Predicting: 83it [00:49,  1.61it/s]Extractor Predicting: 84it [00:49,  1.64it/s]Extractor Predicting: 85it [00:50,  1.63it/s]Extractor Predicting: 86it [00:50,  1.60it/s]Extractor Predicting: 87it [00:51,  1.64it/s]Extractor Predicting: 88it [00:52,  1.65it/s]Extractor Predicting: 89it [00:52,  1.69it/s]Extractor Predicting: 90it [00:53,  1.69it/s]Extractor Predicting: 91it [00:53,  1.68it/s]Extractor Predicting: 92it [00:54,  1.68it/s]Extractor Predicting: 93it [00:55,  1.71it/s]Extractor Predicting: 94it [00:55,  1.71it/s]Extractor Predicting: 95it [00:56,  1.69it/s]Extractor Predicting: 96it [00:56,  1.73it/s]Extractor Predicting: 97it [00:57,  1.75it/s]Extractor Predicting: 98it [00:58,  1.57it/s]Extractor Predicting: 99it [00:58,  1.60it/s]Extractor Predicting: 100it [00:59,  1.65it/s]Extractor Predicting: 101it [00:59,  1.66it/s]Extractor Predicting: 102it [01:00,  1.64it/s]Extractor Predicting: 103it [01:01,  1.64it/s]Extractor Predicting: 104it [01:01,  1.69it/s]Extractor Predicting: 105it [01:02,  1.64it/s]Extractor Predicting: 106it [01:02,  1.67it/s]Extractor Predicting: 107it [01:03,  1.68it/s]Extractor Predicting: 108it [01:04,  1.65it/s]Extractor Predicting: 109it [01:04,  1.64it/s]Extractor Predicting: 110it [01:05,  1.67it/s]Extractor Predicting: 111it [01:05,  1.66it/s]Extractor Predicting: 112it [01:06,  1.65it/s]Extractor Predicting: 113it [01:07,  1.69it/s]Extractor Predicting: 114it [01:07,  1.68it/s]Extractor Predicting: 115it [01:08,  1.68it/s]Extractor Predicting: 116it [01:08,  1.71it/s]Extractor Predicting: 117it [01:09,  1.72it/s]Extractor Predicting: 118it [01:09,  1.76it/s]Extractor Predicting: 119it [01:10,  1.78it/s]Extractor Predicting: 120it [01:11,  1.77it/s]Extractor Predicting: 121it [01:11,  1.76it/s]Extractor Predicting: 122it [01:12,  1.74it/s]Extractor Predicting: 123it [01:12,  1.72it/s]Extractor Predicting: 124it [01:13,  1.73it/s]Extractor Predicting: 125it [01:13,  1.73it/s]Extractor Predicting: 126it [01:14,  1.71it/s]Extractor Predicting: 127it [01:15,  1.68it/s]Extractor Predicting: 128it [01:15,  1.65it/s]Extractor Predicting: 129it [01:16,  1.63it/s]Extractor Predicting: 130it [01:17,  1.61it/s]Extractor Predicting: 131it [01:17,  1.58it/s]Extractor Predicting: 132it [01:18,  1.60it/s]Extractor Predicting: 133it [01:18,  1.61it/s]Extractor Predicting: 134it [01:19,  1.65it/s]Extractor Predicting: 135it [01:20,  1.59it/s]Extractor Predicting: 136it [01:20,  1.58it/s]Extractor Predicting: 137it [01:21,  1.57it/s]Extractor Predicting: 138it [01:22,  1.59it/s]Extractor Predicting: 139it [01:22,  1.59it/s]Extractor Predicting: 140it [01:23,  1.56it/s]Extractor Predicting: 141it [01:24,  1.57it/s]Extractor Predicting: 142it [01:24,  1.57it/s]Extractor Predicting: 143it [01:25,  1.59it/s]Extractor Predicting: 144it [01:25,  1.58it/s]Extractor Predicting: 145it [01:26,  1.57it/s]Extractor Predicting: 146it [01:27,  1.56it/s]Extractor Predicting: 147it [01:27,  1.58it/s]Extractor Predicting: 148it [01:28,  1.59it/s]Extractor Predicting: 149it [01:29,  1.62it/s]Extractor Predicting: 150it [01:29,  1.61it/s]Extractor Predicting: 151it [01:30,  1.59it/s]Extractor Predicting: 152it [01:30,  1.60it/s]Extractor Predicting: 153it [01:31,  1.57it/s]Extractor Predicting: 154it [01:32,  1.57it/s]Extractor Predicting: 155it [01:32,  1.58it/s]Extractor Predicting: 156it [01:33,  1.63it/s]Extractor Predicting: 157it [01:33,  1.69it/s]Extractor Predicting: 158it [01:34,  1.70it/s]Extractor Predicting: 159it [01:35,  1.70it/s]Extractor Predicting: 160it [01:35,  1.72it/s]Extractor Predicting: 161it [01:36,  1.76it/s]Extractor Predicting: 162it [01:36,  1.71it/s]Extractor Predicting: 163it [01:37,  1.70it/s]Extractor Predicting: 164it [01:38,  1.71it/s]Extractor Predicting: 165it [01:38,  1.76it/s]Extractor Predicting: 166it [01:39,  1.74it/s]Extractor Predicting: 167it [01:39,  1.80it/s]Extractor Predicting: 168it [01:40,  1.81it/s]Extractor Predicting: 169it [01:40,  1.84it/s]Extractor Predicting: 170it [01:41,  1.78it/s]Extractor Predicting: 171it [01:42,  1.70it/s]Extractor Predicting: 172it [01:42,  1.67it/s]Extractor Predicting: 173it [01:43,  1.72it/s]Extractor Predicting: 173it [01:43,  1.68it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:41:34,707 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:41:34,711 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:41:34,711 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:41:34,711 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:41:34,712 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:41:35,342 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:41:35,343 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:41:35,891 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:41:36,921 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:41:36,921 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:41:39,821 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:41:39,824 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:41:39,824 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:41:39,824 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:41:39,824 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:41:40,453 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:41:40,454 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:41:41,006 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:41:41,158 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:41:41,158 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3148148148148148,
  "recall": 0.016401350699469366,
  "score": 0.031178358551123338,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.83it/s]Extractor Predicting: 2it [00:01,  1.72it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:05,  1.49it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:07,  1.44it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:09,  1.58it/s]Extractor Predicting: 15it [00:09,  1.66it/s]Extractor Predicting: 16it [00:10,  1.75it/s]Extractor Predicting: 17it [00:10,  1.85it/s]Extractor Predicting: 18it [00:11,  1.89it/s]Extractor Predicting: 19it [00:11,  1.90it/s]Extractor Predicting: 20it [00:12,  1.93it/s]Extractor Predicting: 21it [00:12,  1.97it/s]Extractor Predicting: 22it [00:13,  1.93it/s]Extractor Predicting: 23it [00:13,  1.92it/s]Extractor Predicting: 24it [00:14,  1.91it/s]Extractor Predicting: 25it [00:14,  1.91it/s]Extractor Predicting: 26it [00:15,  1.95it/s]Extractor Predicting: 27it [00:15,  1.91it/s]Extractor Predicting: 28it [00:16,  1.93it/s]Extractor Predicting: 29it [00:16,  1.93it/s]Extractor Predicting: 30it [00:17,  1.97it/s]Extractor Predicting: 31it [00:17,  1.99it/s]Extractor Predicting: 32it [00:18,  1.97it/s]Extractor Predicting: 33it [00:18,  1.98it/s]Extractor Predicting: 34it [00:19,  1.98it/s]Extractor Predicting: 35it [00:19,  1.96it/s]Extractor Predicting: 36it [00:20,  1.92it/s]Extractor Predicting: 37it [00:20,  1.87it/s]Extractor Predicting: 38it [00:21,  1.94it/s]Extractor Predicting: 39it [00:21,  1.94it/s]Extractor Predicting: 40it [00:22,  1.94it/s]Extractor Predicting: 41it [00:22,  1.93it/s]Extractor Predicting: 42it [00:23,  1.96it/s]Extractor Predicting: 43it [00:23,  1.92it/s]Extractor Predicting: 44it [00:24,  1.76it/s]Extractor Predicting: 45it [00:25,  1.66it/s]Extractor Predicting: 46it [00:26,  1.59it/s]Extractor Predicting: 47it [00:26,  1.55it/s]Extractor Predicting: 48it [00:27,  1.53it/s]Extractor Predicting: 49it [00:28,  1.53it/s]Extractor Predicting: 50it [00:28,  1.52it/s]Extractor Predicting: 51it [00:29,  1.51it/s]Extractor Predicting: 52it [00:30,  1.49it/s]Extractor Predicting: 53it [00:30,  1.48it/s]Extractor Predicting: 54it [00:31,  1.48it/s]Extractor Predicting: 55it [00:32,  1.50it/s]Extractor Predicting: 56it [00:32,  1.47it/s]Extractor Predicting: 57it [00:33,  1.47it/s]Extractor Predicting: 58it [00:34,  1.47it/s]Extractor Predicting: 59it [00:34,  1.75it/s]Extractor Predicting: 59it [00:34,  1.71it/s]
[INFO|configuration_utils.py:515] 2023-08-28 08:42:16,684 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 08:42:16,685 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 08:42:16,689 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 08:42:16,689 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 08:42:16,694 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 08:42:19,743 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 08:42:19,745 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 08:42:19,761 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 08:42:19,762 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 08:42:19,773 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:42:19,780 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:42:19,780 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:42:19,780 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:42:19,780 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:42:19,780 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:42:19,780 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.391304347826087,
  "recall": 0.008729388942774006,
  "score": 0.017077798861480076,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 08:42:20,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:20,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:21,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:21,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:22,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:23,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:23,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:24,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:25,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:25,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:26,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:27,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:27,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:28,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:28,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:29,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:30,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:30,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:31,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:32,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:32,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:33,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:34,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:14<02:13, 14.79s/it][WARNING|generation_utils.py:914] 2023-08-28 08:42:34,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:35,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:36,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:36,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:37,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:38,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:38,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:39,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:40,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:40,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:41,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:41,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:42,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:43,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:43,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:44,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:44,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:45,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:46,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:47,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:47,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:48,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:48,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:49,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:50,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:51,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:51,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:32<02:10, 16.37s/it][WARNING|generation_utils.py:914] 2023-08-28 08:42:52,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:52,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:53,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:54,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:54,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:55,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:55,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:56,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:57,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:57,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:58,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:59,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:42:59,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:00,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:00,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:01,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:02,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:02,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:03,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:04,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:04,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:05,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:06,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:06,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:07,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:08,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:08,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:09,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:09,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:50<02:00, 17.15s/it][WARNING|generation_utils.py:914] 2023-08-28 08:43:10,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:10,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:11,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:12,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:13,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:13,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:14,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:15,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:15,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:16,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:17,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:17,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:18,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:19,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:19,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:20,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:21,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:22,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:22,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:23,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:24,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:25,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:05<01:38, 16.42s/it][WARNING|generation_utils.py:914] 2023-08-28 08:43:25,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:26,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:26,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:27,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:28,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:28,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:29,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:29,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:30,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:31,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:31,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:32,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:33,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:33,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:34,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:35,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:35,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:36,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:36,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:37,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:38,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:38,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:39,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:19<01:18, 15.66s/it][WARNING|generation_utils.py:914] 2023-08-28 08:43:39,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:40,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:41,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:41,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:42,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:43,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:43,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:44,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:45,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:45,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:46,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:46,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:47,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:48,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:48,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:49,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:50,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:50,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:51,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:51,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:52,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:53,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:53,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:54,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:34<01:01, 15.45s/it][WARNING|generation_utils.py:914] 2023-08-28 08:43:55,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:55,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:56,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:56,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:57,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:57,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:58,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:59,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:43:59,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:00,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:00,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:01,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:02,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:02,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:03,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:03,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:04,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:04,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:05,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:06,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:06,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:07,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:08,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:08,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:49<00:45, 15.26s/it][WARNING|generation_utils.py:914] 2023-08-28 08:44:09,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:10,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:10,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:11,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:11,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:12,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:13,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:13,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:14,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:14,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:15,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:15,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:16,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:17,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:17,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:18,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:18,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:19,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:19,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:20,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:20,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:21,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:01<00:28, 14.25s/it][WARNING|generation_utils.py:914] 2023-08-28 08:44:21,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:22,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:23,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:23,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:24,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:24,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:25,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:26,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:26,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:27,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:27,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:28,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:29,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:29,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:30,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:30,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:31,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:32,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:32,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:33,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:34,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:34,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:15<00:13, 13.99s/it][WARNING|generation_utils.py:914] 2023-08-28 08:44:35,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:36,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:36,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:37,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:37,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:38,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:39,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:39,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:40,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:40,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:41,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:42,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:42,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:43,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:43,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:44,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:44,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:45,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:46,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:46,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:47,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:44:47,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:28<00:00, 13.73s/it]Generating: 100%|██████████| 10/10 [02:28<00:00, 14.85s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:54,627 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:54,631 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:54,631 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:54,631 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:54,631 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:44:55,248 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:44:55,248 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:44:55,827 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:44:56,900 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:44:56,900 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:59,155 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:59,159 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:59,159 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:59,159 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:44:59,159 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:44:59,484 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:44:59,485 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:44:59,751 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:44:59,929 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:44:59,929 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : given name .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : languages spoken, written or signed . Context : The Cintas ( Spanish for The New Cintas ) is a family of six mountain range s of northern Chile , a subsidiary of the Cintar de Cintarán . Head Entity : The New Cintar de Cintarán , Tail Entity : Spanish .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 165, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 254, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 350, 'raw': 480}
{'target': 600, 'success': 366, 'raw': 512}
{'target': 600, 'success': 388, 'raw': 544}
{'target': 600, 'success': 409, 'raw': 576}
{'target': 600, 'success': 431, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 479, 'raw': 672}
{'target': 600, 'success': 497, 'raw': 704}
{'target': 600, 'success': 520, 'raw': 736}
{'target': 600, 'success': 547, 'raw': 768}
{'target': 600, 'success': 566, 'raw': 800}
{'target': 600, 'success': 588, 'raw': 832}
{'target': 600, 'success': 608, 'raw': 864}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.7037037037037037, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Egypt', 'languages spoken, written or signed', '', 'The language of Egypt , spoken by the 1,000 or more people of the Ancient Egypt n nation of Egypt .')"}}
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 33, 'raw': 64}
{'target': 600, 'success': 54, 'raw': 96}
{'target': 600, 'success': 78, 'raw': 128}
{'target': 600, 'success': 99, 'raw': 160}
{'target': 600, 'success': 120, 'raw': 192}
{'target': 600, 'success': 140, 'raw': 224}
{'target': 600, 'success': 166, 'raw': 256}
{'target': 600, 'success': 184, 'raw': 288}
{'target': 600, 'success': 203, 'raw': 320}
{'target': 600, 'success': 225, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 264, 'raw': 416}
{'target': 600, 'success': 283, 'raw': 448}
{'target': 600, 'success': 306, 'raw': 480}
{'target': 600, 'success': 327, 'raw': 512}
{'target': 600, 'success': 351, 'raw': 544}
{'target': 600, 'success': 371, 'raw': 576}
{'target': 600, 'success': 393, 'raw': 608}
{'target': 600, 'success': 414, 'raw': 640}
{'target': 600, 'success': 436, 'raw': 672}
{'target': 600, 'success': 458, 'raw': 704}
{'target': 600, 'success': 481, 'raw': 736}
{'target': 600, 'success': 500, 'raw': 768}
{'target': 600, 'success': 523, 'raw': 800}
{'target': 600, 'success': 545, 'raw': 832}
{'target': 600, 'success': 571, 'raw': 864}
{'target': 600, 'success': 591, 'raw': 896}
{'target': 600, 'success': 617, 'raw': 928}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.6648706896551724, 'errors': {'', "('impact radius', 'lowest point', '', 'Around the time of its last orbit , the Gemini spacecraft moved to another orbit with an impact radius of 1,200 km .')", "('Horzach', 'lowest point', '', 'Hohner ( ) , commonly known as Horzach ( γ Hoh , γ Hohner ) , is a star in the constellation Gemini .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8636363636363636, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 618, 'raw': 736}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.8396739130434783, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : genre . Context : The song reached number one on the RPM RPM Singles & Tracks chart , peaking at number 5 on the Japanese Top Japan Singles & Tracks chart , and number 14 on the Billboard Hot 100 single chart . Head Entity : Hot 100 single , Tail Entity : singles .\n']
['Relation : genre . Context : The song reached number one on the RPM RPM Singles & Tracks chart , peaking at number 5 on the Japanese Top Japan Singles & Tracks chart , and number 14 on the Billboard Hot 100 single chart . Head Entity : Hot 100 single , Tail Entity : singles .\n', 'Relation : genre . Context : After he gained steam as a member of his idol band , the band was released on the Hot Spots label in 1999 by Sony Records . Head Entity : Hot Spots , Tail Entity : band version .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 559, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : genre .', 'success_rate': 0.7955729166666666, 'errors': {'', "('Jack Kerouac', 'genre', '', 'It was in the works of Robert Anton Wilson , Roger Bacon , Henry David Thoreau , and Jack Kerouac .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 584, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.7981770833333334, 'errors': {'', "('Armenia', 'is a list of', '', 'These are the official language of Armenia n government s.')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8707386363636364, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 630, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8948863636363636, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : member of .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/1_ext.jsonl'}}
estimate vocab size: 11275
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11375, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.69it/s]Extractor Estimating: 2it [00:01,  1.53it/s]Extractor Estimating: 3it [00:01,  1.56it/s]Extractor Estimating: 4it [00:02,  1.54it/s]Extractor Estimating: 5it [00:03,  1.57it/s]Extractor Estimating: 6it [00:03,  1.57it/s]Extractor Estimating: 7it [00:04,  1.57it/s]Extractor Estimating: 8it [00:05,  1.53it/s]Extractor Estimating: 9it [00:05,  1.50it/s]Extractor Estimating: 10it [00:06,  1.56it/s]Extractor Estimating: 11it [00:07,  1.58it/s]Extractor Estimating: 12it [00:07,  1.61it/s]Extractor Estimating: 13it [00:08,  1.59it/s]Extractor Estimating: 14it [00:08,  1.57it/s]Extractor Estimating: 15it [00:09,  1.57it/s]Extractor Estimating: 16it [00:10,  1.53it/s]Extractor Estimating: 17it [00:10,  1.55it/s]Extractor Estimating: 18it [00:11,  1.57it/s]Extractor Estimating: 19it [00:12,  1.55it/s]Extractor Estimating: 20it [00:12,  1.58it/s]Extractor Estimating: 21it [00:13,  1.56it/s]Extractor Estimating: 22it [00:14,  1.60it/s]Extractor Estimating: 23it [00:14,  1.59it/s]Extractor Estimating: 24it [00:15,  1.59it/s]Extractor Estimating: 25it [00:15,  1.60it/s]Extractor Estimating: 26it [00:16,  1.64it/s]Extractor Estimating: 27it [00:17,  1.66it/s]Extractor Estimating: 28it [00:17,  1.67it/s]Extractor Estimating: 29it [00:18,  1.64it/s]Extractor Estimating: 30it [00:18,  1.63it/s]Extractor Estimating: 31it [00:19,  1.57it/s]Extractor Estimating: 32it [00:20,  1.61it/s]Extractor Estimating: 33it [00:20,  1.63it/s]Extractor Estimating: 34it [00:21,  1.65it/s]Extractor Estimating: 35it [00:22,  1.63it/s]Extractor Estimating: 36it [00:22,  1.70it/s]Extractor Estimating: 37it [00:23,  1.65it/s]Extractor Estimating: 38it [00:23,  1.69it/s]Extractor Estimating: 39it [00:24,  1.73it/s]Extractor Estimating: 40it [00:24,  1.73it/s]Extractor Estimating: 41it [00:25,  1.75it/s]Extractor Estimating: 42it [00:26,  1.74it/s]Extractor Estimating: 43it [00:26,  1.68it/s]Extractor Estimating: 44it [00:27,  1.66it/s]Extractor Estimating: 45it [00:27,  1.74it/s]Extractor Estimating: 46it [00:28,  1.78it/s]Extractor Estimating: 47it [00:28,  1.83it/s]Extractor Estimating: 48it [00:29,  1.75it/s]Extractor Estimating: 49it [00:30,  1.71it/s]Extractor Estimating: 50it [00:30,  1.73it/s]Extractor Estimating: 51it [00:31,  1.69it/s]Extractor Estimating: 52it [00:31,  1.73it/s]Extractor Estimating: 53it [00:32,  1.69it/s]Extractor Estimating: 54it [00:33,  1.67it/s]Extractor Estimating: 55it [00:33,  1.65it/s]Extractor Estimating: 56it [00:34,  1.63it/s]Extractor Estimating: 57it [00:35,  1.49it/s]Extractor Estimating: 58it [00:35,  1.53it/s]Extractor Estimating: 59it [00:36,  1.49it/s]Extractor Estimating: 60it [00:37,  1.48it/s]Extractor Estimating: 61it [00:37,  1.48it/s]Extractor Estimating: 62it [00:38,  1.51it/s]Extractor Estimating: 63it [00:39,  1.54it/s]Extractor Estimating: 64it [00:39,  1.56it/s]Extractor Estimating: 65it [00:40,  1.54it/s]Extractor Estimating: 66it [00:40,  1.59it/s]Extractor Estimating: 67it [00:41,  1.56it/s]Extractor Estimating: 68it [00:42,  1.51it/s]Extractor Estimating: 69it [00:42,  1.55it/s]Extractor Estimating: 70it [00:43,  1.53it/s]Extractor Estimating: 71it [00:44,  1.56it/s]Extractor Estimating: 72it [00:44,  1.56it/s]Extractor Estimating: 73it [00:45,  1.58it/s]Extractor Estimating: 74it [00:45,  1.64it/s]Extractor Estimating: 75it [00:46,  1.63it/s]Extractor Estimating: 76it [00:47,  1.62it/s]Extractor Estimating: 77it [00:47,  1.58it/s]Extractor Estimating: 78it [00:48,  1.57it/s]Extractor Estimating: 79it [00:49,  1.50it/s]Extractor Estimating: 80it [00:50,  1.46it/s]Extractor Estimating: 81it [00:50,  1.43it/s]Extractor Estimating: 82it [00:51,  1.45it/s]Extractor Estimating: 83it [00:52,  1.43it/s]Extractor Estimating: 84it [00:52,  1.50it/s]Extractor Estimating: 85it [00:53,  1.49it/s]Extractor Estimating: 86it [00:54,  1.46it/s]Extractor Estimating: 87it [00:54,  1.43it/s]Extractor Estimating: 88it [00:55,  1.41it/s]Extractor Estimating: 89it [00:56,  1.43it/s]Extractor Estimating: 90it [00:56,  1.50it/s]Extractor Estimating: 91it [00:57,  1.46it/s]Extractor Estimating: 92it [00:58,  1.45it/s]Extractor Estimating: 93it [00:58,  1.47it/s]Extractor Estimating: 94it [00:59,  1.46it/s]Extractor Estimating: 95it [01:00,  1.47it/s]Extractor Estimating: 96it [01:01,  1.40it/s]Extractor Estimating: 97it [01:01,  1.39it/s]Extractor Estimating: 98it [01:02,  1.41it/s]Extractor Estimating: 99it [01:03,  1.43it/s]Extractor Estimating: 100it [01:03,  1.45it/s]Extractor Estimating: 101it [01:04,  1.49it/s]Extractor Estimating: 102it [01:05,  1.54it/s]Extractor Estimating: 103it [01:05,  1.57it/s]Extractor Estimating: 104it [01:06,  1.59it/s]Extractor Estimating: 105it [01:06,  1.60it/s]Extractor Estimating: 106it [01:07,  1.64it/s]Extractor Estimating: 107it [01:08,  1.62it/s]Extractor Estimating: 108it [01:08,  1.64it/s]Extractor Estimating: 109it [01:09,  1.67it/s]Extractor Estimating: 110it [01:09,  1.74it/s]Extractor Estimating: 111it [01:10,  1.69it/s]Extractor Estimating: 112it [01:11,  1.69it/s]Extractor Estimating: 113it [01:11,  1.71it/s]Extractor Estimating: 114it [01:12,  1.71it/s]Extractor Estimating: 115it [01:13,  1.53it/s]Extractor Estimating: 116it [01:13,  1.57it/s]Extractor Estimating: 117it [01:14,  1.62it/s]Extractor Estimating: 118it [01:14,  1.60it/s]Extractor Estimating: 119it [01:15,  1.65it/s]Extractor Estimating: 120it [01:15,  1.68it/s]Extractor Estimating: 121it [01:16,  1.71it/s]Extractor Estimating: 122it [01:17,  1.68it/s]Extractor Estimating: 123it [01:17,  1.66it/s]Extractor Estimating: 124it [01:18,  1.63it/s]Extractor Estimating: 125it [01:18,  1.65it/s]Extractor Estimating: 126it [01:19,  1.61it/s]Extractor Estimating: 127it [01:20,  1.57it/s]Extractor Estimating: 128it [01:20,  1.60it/s]Extractor Estimating: 129it [01:21,  1.59it/s]Extractor Estimating: 130it [01:22,  1.61it/s]Extractor Estimating: 131it [01:22,  1.64it/s]Extractor Estimating: 132it [01:23,  1.58it/s]Extractor Estimating: 133it [01:24,  1.55it/s]Extractor Estimating: 134it [01:24,  1.52it/s]Extractor Estimating: 135it [01:25,  1.51it/s]Extractor Estimating: 136it [01:26,  1.54it/s]Extractor Estimating: 137it [01:26,  1.50it/s]Extractor Estimating: 138it [01:27,  1.55it/s]Extractor Estimating: 139it [01:27,  1.60it/s]Extractor Estimating: 140it [01:28,  1.60it/s]Extractor Estimating: 141it [01:29,  1.64it/s]Extractor Estimating: 142it [01:29,  1.61it/s]Extractor Estimating: 143it [01:30,  1.61it/s]Extractor Estimating: 144it [01:31,  1.64it/s]Extractor Estimating: 145it [01:31,  1.58it/s]Extractor Estimating: 146it [01:32,  1.58it/s]Extractor Estimating: 147it [01:32,  1.61it/s]Extractor Estimating: 148it [01:33,  1.59it/s]Extractor Estimating: 149it [01:34,  1.57it/s]Extractor Estimating: 150it [01:34,  1.61it/s]Extractor Estimating: 151it [01:35,  1.69it/s]Extractor Estimating: 152it [01:35,  1.69it/s]Extractor Estimating: 153it [01:36,  1.78it/s]Extractor Estimating: 154it [01:37,  1.72it/s]Extractor Estimating: 155it [01:37,  1.72it/s]Extractor Estimating: 156it [01:38,  1.73it/s]Extractor Estimating: 157it [01:38,  1.73it/s]Extractor Estimating: 158it [01:39,  1.76it/s]Extractor Estimating: 159it [01:39,  1.75it/s]Extractor Estimating: 160it [01:40,  1.74it/s]Extractor Estimating: 161it [01:41,  1.72it/s]Extractor Estimating: 162it [01:41,  1.77it/s]Extractor Estimating: 163it [01:42,  1.74it/s]Extractor Estimating: 164it [01:42,  1.74it/s]Extractor Estimating: 165it [01:43,  1.77it/s]Extractor Estimating: 166it [01:43,  1.76it/s]Extractor Estimating: 167it [01:44,  1.71it/s]Extractor Estimating: 168it [01:45,  1.59it/s]Extractor Estimating: 169it [01:45,  1.64it/s]Extractor Estimating: 170it [01:46,  1.71it/s]Extractor Estimating: 171it [01:46,  1.70it/s]Extractor Estimating: 172it [01:47,  1.77it/s]Extractor Estimating: 173it [01:48,  1.66it/s]Extractor Estimating: 174it [01:48,  1.69it/s]Extractor Estimating: 175it [01:49,  1.71it/s]Extractor Estimating: 176it [01:49,  1.72it/s]Extractor Estimating: 177it [01:50,  1.74it/s]Extractor Estimating: 178it [01:51,  1.72it/s]Extractor Estimating: 179it [01:51,  1.74it/s]Extractor Estimating: 180it [01:52,  1.69it/s]Extractor Estimating: 181it [01:52,  1.69it/s]Extractor Estimating: 182it [01:53,  1.73it/s]Extractor Estimating: 183it [01:53,  1.66it/s]Extractor Estimating: 184it [01:54,  1.67it/s]Extractor Estimating: 185it [01:55,  1.56it/s]Extractor Estimating: 186it [01:55,  1.62it/s]Extractor Estimating: 187it [01:56,  1.60it/s]Extractor Estimating: 188it [01:57,  1.60it/s]Extractor Estimating: 189it [01:57,  1.61it/s]Extractor Estimating: 190it [01:58,  1.64it/s]Extractor Estimating: 191it [01:58,  1.67it/s]Extractor Estimating: 192it [01:59,  1.72it/s]Extractor Estimating: 193it [02:00,  1.65it/s]Extractor Estimating: 194it [02:00,  1.70it/s]Extractor Estimating: 195it [02:01,  1.67it/s]Extractor Estimating: 196it [02:01,  1.65it/s]Extractor Estimating: 197it [02:02,  1.68it/s]Extractor Estimating: 198it [02:03,  1.72it/s]Extractor Estimating: 199it [02:03,  1.72it/s]Extractor Estimating: 200it [02:04,  1.72it/s]Extractor Estimating: 201it [02:04,  1.74it/s]Extractor Estimating: 202it [02:05,  1.74it/s]Extractor Estimating: 203it [02:05,  1.78it/s]Extractor Estimating: 204it [02:06,  1.78it/s]Extractor Estimating: 205it [02:07,  1.74it/s]Extractor Estimating: 206it [02:07,  1.69it/s]Extractor Estimating: 207it [02:08,  1.66it/s]Extractor Estimating: 208it [02:08,  1.71it/s]Extractor Estimating: 209it [02:09,  1.64it/s]Extractor Estimating: 210it [02:10,  1.67it/s]Extractor Estimating: 211it [02:10,  1.67it/s]Extractor Estimating: 212it [02:11,  1.72it/s]Extractor Estimating: 213it [02:11,  1.75it/s]Extractor Estimating: 214it [02:12,  1.76it/s]Extractor Estimating: 215it [02:12,  1.72it/s]Extractor Estimating: 216it [02:13,  1.69it/s]Extractor Estimating: 217it [02:14,  1.64it/s]Extractor Estimating: 218it [02:14,  1.64it/s]Extractor Estimating: 219it [02:15,  1.68it/s]Extractor Estimating: 220it [02:15,  1.74it/s]Extractor Estimating: 221it [02:16,  1.75it/s]Extractor Estimating: 222it [02:17,  1.72it/s]Extractor Estimating: 223it [02:17,  1.60it/s]Extractor Estimating: 224it [02:18,  1.61it/s]Extractor Estimating: 225it [02:19,  1.55it/s]Extractor Estimating: 226it [02:19,  1.56it/s]Extractor Estimating: 227it [02:20,  1.58it/s]Extractor Estimating: 228it [02:20,  1.59it/s]Extractor Estimating: 229it [02:21,  1.62it/s]Extractor Estimating: 230it [02:22,  1.61it/s]Extractor Estimating: 231it [02:22,  1.61it/s]Extractor Estimating: 232it [02:23,  1.59it/s]Extractor Estimating: 233it [02:24,  1.58it/s]Extractor Estimating: 234it [02:24,  1.60it/s]Extractor Estimating: 235it [02:25,  1.63it/s]Extractor Estimating: 236it [02:25,  1.62it/s]Extractor Estimating: 237it [02:26,  1.60it/s]Extractor Estimating: 238it [02:27,  1.58it/s]Extractor Estimating: 239it [02:27,  1.58it/s]Extractor Estimating: 240it [02:28,  1.62it/s]Extractor Estimating: 241it [02:29,  1.61it/s]Extractor Estimating: 242it [02:29,  1.67it/s]Extractor Estimating: 243it [02:30,  1.66it/s]Extractor Estimating: 244it [02:30,  1.67it/s]Extractor Estimating: 245it [02:31,  1.63it/s]Extractor Estimating: 246it [02:32,  1.48it/s]Extractor Estimating: 247it [02:32,  1.50it/s]Extractor Estimating: 248it [02:33,  1.55it/s]Extractor Estimating: 249it [02:34,  1.60it/s]Extractor Estimating: 250it [02:34,  1.58it/s]Extractor Estimating: 250it [02:34,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:51,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:51,050 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:51,050 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:51,050 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:51,050 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:47:51,663 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:47:51,664 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:47:52,228 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:47:53,270 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:47:53,270 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:56,101 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:56,107 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:56,108 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:56,108 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:47:56,108 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:47:56,743 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:47:56,745 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:47:57,300 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:47:57,476 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:47:57,477 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 10:14:15,484 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 10:14:15,508 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 4965 mean pseudo reward: 0.9119085031067388
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 22719
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22819, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22819, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.995, loss:780.0848
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.982, loss:746.6039
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 93, avg_time 1.003, loss:715.9095
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 193, avg_time 0.995, loss:737.3236
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 86, avg_time 0.998, loss:717.5875
>> valid entity prec:0.5108, rec:0.5175, f1:0.5141
>> valid relation prec:0.0357, rec:0.0012, f1:0.0024
>> valid relation with NER prec:0.0357, rec:0.0012, f1:0.0024
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 186, avg_time 2.311, loss:714.7537
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 79, avg_time 0.985, loss:699.1200
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 179, avg_time 1.009, loss:724.1420
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 72, avg_time 0.997, loss:688.3115
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 172, avg_time 0.991, loss:735.2829
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4982, rec:0.4428, f1:0.4689
>> valid relation prec:0.0622, rec:0.0032, f1:0.0061
>> valid relation with NER prec:0.0622, rec:0.0032, f1:0.0061
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 65, avg_time 2.313, loss:680.7295
g_step 1200, step 165, avg_time 0.988, loss:680.8954
g_step 1300, step 58, avg_time 1.005, loss:688.5180
g_step 1400, step 158, avg_time 1.002, loss:680.1319
g_step 1500, step 51, avg_time 1.004, loss:641.4609
>> valid entity prec:0.5042, rec:0.4867, f1:0.4953
>> valid relation prec:0.1056, rec:0.0079, f1:0.0148
>> valid relation with NER prec:0.1056, rec:0.0079, f1:0.0148
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 151, avg_time 2.311, loss:643.6116
g_step 1700, step 44, avg_time 1.000, loss:631.7061
g_step 1800, step 144, avg_time 0.994, loss:632.0565
g_step 1900, step 37, avg_time 0.992, loss:644.5570
g_step 2000, step 137, avg_time 1.001, loss:615.4838
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5077, rec:0.5805, f1:0.5417
>> valid relation prec:0.0749, rec:0.0079, f1:0.0143
>> valid relation with NER prec:0.0749, rec:0.0079, f1:0.0143
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 30, avg_time 2.325, loss:592.6346
g_step 2200, step 130, avg_time 0.989, loss:576.5505
g_step 2300, step 23, avg_time 1.002, loss:596.9752
g_step 2400, step 123, avg_time 1.004, loss:551.8190
g_step 2500, step 16, avg_time 0.987, loss:553.5240
>> valid entity prec:0.5080, rec:0.3731, f1:0.4302
>> valid relation prec:0.0727, rec:0.0112, f1:0.0193
>> valid relation with NER prec:0.0727, rec:0.0112, f1:0.0193
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 116, avg_time 2.307, loss:534.8899
g_step 2700, step 9, avg_time 1.005, loss:560.1686
g_step 2800, step 109, avg_time 0.991, loss:512.0286
g_step 2900, step 2, avg_time 0.998, loss:548.3298
g_step 3000, step 102, avg_time 1.003, loss:494.3039
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5338, rec:0.4171, f1:0.4683
>> valid relation prec:0.0994, rec:0.0235, f1:0.0381
>> valid relation with NER prec:0.0994, rec:0.0235, f1:0.0381
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 202, avg_time 2.312, loss:512.3894
g_step 3200, step 95, avg_time 0.990, loss:486.9541
g_step 3300, step 195, avg_time 0.999, loss:498.1615
g_step 3400, step 88, avg_time 0.983, loss:457.6871
g_step 3500, step 188, avg_time 1.004, loss:491.6174
>> valid entity prec:0.5123, rec:0.4005, f1:0.4496
>> valid relation prec:0.1197, rec:0.0134, f1:0.0241
>> valid relation with NER prec:0.1197, rec:0.0134, f1:0.0241
g_step 3600, step 81, avg_time 2.300, loss:458.8122
g_step 3700, step 181, avg_time 0.994, loss:476.1866
g_step 3800, step 74, avg_time 0.989, loss:442.4021
g_step 3900, step 174, avg_time 0.996, loss:451.8557
g_step 4000, step 67, avg_time 0.992, loss:423.0983
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5122, rec:0.3996, f1:0.4489
>> valid relation prec:0.0719, rec:0.0102, f1:0.0178
>> valid relation with NER prec:0.0719, rec:0.0102, f1:0.0178
g_step 4100, step 167, avg_time 2.313, loss:447.5526
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 10:14:15 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 10:14:15 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_10-14-15_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 10:14:16 - WARNING - datasets.builder -   Using custom data configuration default-3f87f8b76db6712c
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-3f87f8b76db6712c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 10:14:16,777 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:14:16,778 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 10:14:16,778 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:14:16,779 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 10:14:16,790 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:14:16,795 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:14:16,795 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:14:16,795 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:14:16,795 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:14:16,795 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:14:16,795 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 10:14:16,942 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 10:14:20,016 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 10:14:20,020 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-3f87f8b76db6712c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.32ba/s] 40%|████      | 2/5 [00:00<00:00,  4.11ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.35ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.47ba/s]100%|██████████| 5/5 [00:01<00:00,  4.55ba/s]100%|██████████| 5/5 [00:01<00:00,  4.37ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.11ba/s] 40%|████      | 2/5 [00:00<00:00,  4.32ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.37ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.66ba/s]100%|██████████| 5/5 [00:01<00:00,  4.77ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.19ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.41ba/s]100%|██████████| 5/5 [00:00<00:00, 10.78ba/s]100%|██████████| 5/5 [00:00<00:00, 10.60ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.18ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.41ba/s]100%|██████████| 5/5 [00:00<00:00, 12.88ba/s]
[INFO|trainer.py:414] 2023-08-28 10:14:23,467 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 10:14:23,485 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 10:14:23,485 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 10:14:23,485 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 10:14:23,485 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 10:14:23,485 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 10:14:23,485 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 10:14:23,485 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:55,  3.37it/s]  1%|          | 2/390 [00:00<01:53,  3.42it/s]  1%|          | 3/390 [00:00<01:52,  3.44it/s]  1%|          | 4/390 [00:01<01:51,  3.45it/s]  1%|▏         | 5/390 [00:01<01:51,  3.45it/s]  2%|▏         | 6/390 [00:01<01:51,  3.44it/s]  2%|▏         | 7/390 [00:02<01:51,  3.43it/s]  2%|▏         | 8/390 [00:02<01:51,  3.42it/s]  2%|▏         | 9/390 [00:02<01:51,  3.42it/s]  3%|▎         | 10/390 [00:02<01:51,  3.42it/s]  3%|▎         | 11/390 [00:03<01:51,  3.40it/s]  3%|▎         | 12/390 [00:03<01:51,  3.40it/s]  3%|▎         | 13/390 [00:03<01:50,  3.41it/s]  4%|▎         | 14/390 [00:04<01:50,  3.41it/s]  4%|▍         | 15/390 [00:04<01:50,  3.40it/s]  4%|▍         | 16/390 [00:04<01:49,  3.41it/s]  4%|▍         | 17/390 [00:04<01:49,  3.41it/s]  5%|▍         | 18/390 [00:05<01:49,  3.41it/s]  5%|▍         | 19/390 [00:05<01:48,  3.41it/s]  5%|▌         | 20/390 [00:05<01:48,  3.41it/s]  5%|▌         | 21/390 [00:06<01:48,  3.40it/s]  6%|▌         | 22/390 [00:06<01:48,  3.39it/s]  6%|▌         | 23/390 [00:06<01:47,  3.40it/s]  6%|▌         | 24/390 [00:07<01:47,  3.40it/s]  6%|▋         | 25/390 [00:07<01:47,  3.40it/s]  7%|▋         | 26/390 [00:07<01:46,  3.41it/s]  7%|▋         | 27/390 [00:07<01:47,  3.39it/s]  7%|▋         | 28/390 [00:08<01:46,  3.40it/s]  7%|▋         | 29/390 [00:08<01:46,  3.40it/s]  8%|▊         | 30/390 [00:08<01:45,  3.40it/s]  8%|▊         | 31/390 [00:09<01:45,  3.40it/s]  8%|▊         | 32/390 [00:09<01:45,  3.40it/s]  8%|▊         | 33/390 [00:09<01:45,  3.40it/s]  9%|▊         | 34/390 [00:09<01:44,  3.40it/s]  9%|▉         | 35/390 [00:10<01:44,  3.40it/s]  9%|▉         | 36/390 [00:10<01:44,  3.40it/s]  9%|▉         | 37/390 [00:10<01:43,  3.41it/s] 10%|▉         | 38/390 [00:11<01:43,  3.40it/s] 10%|█         | 39/390 [00:11<01:43,  3.40it/s] 10%|█         | 40/390 [00:11<01:42,  3.40it/s] 11%|█         | 41/390 [00:12<01:42,  3.40it/s] 11%|█         | 42/390 [00:12<01:42,  3.40it/s] 11%|█         | 43/390 [00:12<01:42,  3.40it/s] 11%|█▏        | 44/390 [00:12<01:41,  3.39it/s] 12%|█▏        | 45/390 [00:13<01:41,  3.40it/s] 12%|█▏        | 46/390 [00:13<01:41,  3.40it/s] 12%|█▏        | 47/390 [00:13<01:40,  3.40it/s] 12%|█▏        | 48/390 [00:14<01:40,  3.40it/s] 13%|█▎        | 49/390 [00:14<01:39,  3.42it/s] 13%|█▎        | 50/390 [00:14<01:39,  3.43it/s] 13%|█▎        | 51/390 [00:14<01:38,  3.43it/s] 13%|█▎        | 52/390 [00:15<01:38,  3.44it/s] 14%|█▎        | 53/390 [00:15<01:37,  3.45it/s] 14%|█▍        | 54/390 [00:15<01:37,  3.45it/s] 14%|█▍        | 55/390 [00:16<01:37,  3.43it/s] 14%|█▍        | 56/390 [00:16<01:37,  3.44it/s] 15%|█▍        | 57/390 [00:16<01:36,  3.44it/s] 15%|█▍        | 58/390 [00:16<01:36,  3.45it/s] 15%|█▌        | 59/390 [00:17<01:36,  3.44it/s] 15%|█▌        | 60/390 [00:17<01:35,  3.45it/s] 16%|█▌        | 61/390 [00:17<01:35,  3.45it/s] 16%|█▌        | 62/390 [00:18<01:35,  3.45it/s] 16%|█▌        | 63/390 [00:18<01:34,  3.45it/s] 16%|█▋        | 64/390 [00:18<01:34,  3.45it/s] 17%|█▋        | 65/390 [00:19<01:34,  3.45it/s] 17%|█▋        | 66/390 [00:19<01:34,  3.44it/s] 17%|█▋        | 67/390 [00:19<01:33,  3.44it/s] 17%|█▋        | 68/390 [00:19<01:33,  3.44it/s] 18%|█▊        | 69/390 [00:20<01:33,  3.44it/s] 18%|█▊        | 70/390 [00:20<01:32,  3.44it/s] 18%|█▊        | 71/390 [00:20<01:32,  3.44it/s] 18%|█▊        | 72/390 [00:21<01:32,  3.45it/s] 19%|█▊        | 73/390 [00:21<01:31,  3.45it/s] 19%|█▉        | 74/390 [00:21<01:31,  3.45it/s] 19%|█▉        | 75/390 [00:21<01:31,  3.45it/s] 19%|█▉        | 76/390 [00:22<01:31,  3.45it/s] 20%|█▉        | 77/390 [00:22<01:31,  3.43it/s] 20%|██        | 78/390 [00:22<01:30,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 10:14:46,326 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:14:46,326 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 10:14:46,326 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.16it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.77it/s][A
  3%|▎         | 17/505 [00:00<00:10, 45.86it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.26it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.84it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.59it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.59it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.44it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.40it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.45it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 43.16it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 43.51it/s][A
 13%|█▎        | 67/505 [00:01<00:10, 43.60it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 43.86it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.91it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.89it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.11it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.27it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.15it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.35it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.32it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.26it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.22it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.20it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.19it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.28it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.27it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.28it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.42it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.39it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.27it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.29it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.25it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.31it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.33it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.34it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.26it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.37it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.38it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.29it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.25it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.27it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.31it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.35it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.27it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.24it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.25it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.25it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.32it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.24it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.34it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.28it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.31it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.32it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.29it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.30it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.25it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.25it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.31it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.34it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.28it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.20it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.27it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.32it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.35it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.30it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.23it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.24it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.33it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.32it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.24it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.28it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.32it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.32it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.30it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.31it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.25it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.29it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.26it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.27it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.30it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.32it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.24it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.30it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.30it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.28it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.23it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.28it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.26it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.32it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.35it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.29it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.29it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.29it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.28it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.21it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.26it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.26it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.29it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.30it/s][A                                                
                                                 [A 20%|██        | 78/390 [00:34<01:30,  3.44it/s]
100%|██████████| 505/505 [00:11<00:00, 44.30it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:14:57,763 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 10:14:57,786 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:15:00,009 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:15:00,029 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:15:00,043 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:40<29:13,  5.64s/it] 21%|██        | 80/390 [00:41<20:50,  4.03s/it] 21%|██        | 81/390 [00:41<14:59,  2.91s/it] 21%|██        | 82/390 [00:41<10:55,  2.13s/it] 21%|██▏       | 83/390 [00:42<08:04,  1.58s/it] 22%|██▏       | 84/390 [00:42<06:04,  1.19s/it] 22%|██▏       | 85/390 [00:42<04:41,  1.08it/s] 22%|██▏       | 86/390 [00:42<03:42,  1.36it/s] 22%|██▏       | 87/390 [00:43<03:02,  1.66it/s] 23%|██▎       | 88/390 [00:43<02:33,  1.96it/s] 23%|██▎       | 89/390 [00:43<02:13,  2.25it/s] 23%|██▎       | 90/390 [00:44<01:59,  2.50it/s] 23%|██▎       | 91/390 [00:44<01:50,  2.71it/s] 24%|██▎       | 92/390 [00:44<01:43,  2.89it/s] 24%|██▍       | 93/390 [00:45<01:38,  3.03it/s] 24%|██▍       | 94/390 [00:45<01:34,  3.13it/s] 24%|██▍       | 95/390 [00:45<01:31,  3.21it/s] 25%|██▍       | 96/390 [00:45<01:30,  3.26it/s] 25%|██▍       | 97/390 [00:46<01:28,  3.31it/s] 25%|██▌       | 98/390 [00:46<01:27,  3.33it/s] 25%|██▌       | 99/390 [00:46<01:26,  3.35it/s] 26%|██▌       | 100/390 [00:47<01:26,  3.37it/s] 26%|██▌       | 101/390 [00:47<01:25,  3.38it/s] 26%|██▌       | 102/390 [00:47<01:25,  3.37it/s] 26%|██▋       | 103/390 [00:47<01:24,  3.38it/s] 27%|██▋       | 104/390 [00:48<01:24,  3.39it/s] 27%|██▋       | 105/390 [00:48<01:24,  3.39it/s] 27%|██▋       | 106/390 [00:48<01:23,  3.39it/s] 27%|██▋       | 107/390 [00:49<01:23,  3.40it/s] 28%|██▊       | 108/390 [00:49<01:23,  3.40it/s] 28%|██▊       | 109/390 [00:49<01:22,  3.39it/s] 28%|██▊       | 110/390 [00:50<01:22,  3.39it/s] 28%|██▊       | 111/390 [00:50<01:22,  3.39it/s] 29%|██▊       | 112/390 [00:50<01:21,  3.39it/s] 29%|██▉       | 113/390 [00:50<01:21,  3.39it/s] 29%|██▉       | 114/390 [00:51<01:21,  3.37it/s] 29%|██▉       | 115/390 [00:51<01:21,  3.38it/s] 30%|██▉       | 116/390 [00:51<01:20,  3.39it/s] 30%|███       | 117/390 [00:52<01:22,  3.32it/s] 30%|███       | 118/390 [00:52<01:21,  3.34it/s] 31%|███       | 119/390 [00:52<01:20,  3.36it/s] 31%|███       | 120/390 [00:53<01:20,  3.37it/s] 31%|███       | 121/390 [00:53<01:19,  3.38it/s] 31%|███▏      | 122/390 [00:53<01:19,  3.38it/s] 32%|███▏      | 123/390 [00:53<01:18,  3.40it/s] 32%|███▏      | 124/390 [00:54<01:18,  3.40it/s] 32%|███▏      | 125/390 [00:54<01:17,  3.42it/s] 32%|███▏      | 126/390 [00:54<01:17,  3.42it/s] 33%|███▎      | 127/390 [00:55<01:16,  3.43it/s] 33%|███▎      | 128/390 [00:55<01:16,  3.44it/s] 33%|███▎      | 129/390 [00:55<01:15,  3.44it/s] 33%|███▎      | 130/390 [00:55<01:15,  3.44it/s] 34%|███▎      | 131/390 [00:56<01:15,  3.44it/s] 34%|███▍      | 132/390 [00:56<01:14,  3.44it/s] 34%|███▍      | 133/390 [00:56<01:14,  3.44it/s] 34%|███▍      | 134/390 [00:57<01:14,  3.44it/s] 35%|███▍      | 135/390 [00:57<01:14,  3.44it/s] 35%|███▍      | 136/390 [00:57<01:13,  3.44it/s] 35%|███▌      | 137/390 [00:57<01:13,  3.44it/s] 35%|███▌      | 138/390 [00:58<01:13,  3.44it/s] 36%|███▌      | 139/390 [00:58<01:12,  3.44it/s] 36%|███▌      | 140/390 [00:58<01:12,  3.44it/s] 36%|███▌      | 141/390 [00:59<01:12,  3.44it/s] 36%|███▋      | 142/390 [00:59<01:11,  3.45it/s] 37%|███▋      | 143/390 [00:59<01:11,  3.44it/s] 37%|███▋      | 144/390 [00:59<01:11,  3.45it/s] 37%|███▋      | 145/390 [01:00<01:11,  3.45it/s] 37%|███▋      | 146/390 [01:00<01:10,  3.45it/s] 38%|███▊      | 147/390 [01:00<01:10,  3.45it/s] 38%|███▊      | 148/390 [01:01<01:10,  3.45it/s] 38%|███▊      | 149/390 [01:01<01:10,  3.44it/s] 38%|███▊      | 150/390 [01:01<01:09,  3.44it/s] 39%|███▊      | 151/390 [01:02<01:09,  3.44it/s] 39%|███▉      | 152/390 [01:02<01:09,  3.45it/s] 39%|███▉      | 153/390 [01:02<01:08,  3.45it/s] 39%|███▉      | 154/390 [01:02<01:08,  3.45it/s] 40%|███▉      | 155/390 [01:03<01:08,  3.45it/s] 40%|████      | 156/390 [01:03<01:07,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 10:15:26,986 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:15:26,986 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 10:15:26,986 >>   Batch size = 8
{'eval_loss': 1.0413262844085693, 'eval_runtime': 11.4214, 'eval_samples_per_second': 353.635, 'eval_steps_per_second': 44.215, 'epoch': 0.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.02it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.72it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.08it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.33it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.89it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.64it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.51it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.24it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.37it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.44it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.38it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.22it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.17it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.12it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.14it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.19it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.17it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.30it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.38it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.32it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.23it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.10it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.09it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.18it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.23it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.18it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.22it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.32it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.29it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.30it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.17it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.10it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.23it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.18it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.19it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.25it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.27it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.29it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.23it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.18it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.10it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.18it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.20it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.27it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.20it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.30it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.29it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.23it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.29it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.22it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.23it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.09it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.20it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.22it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.18it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.24it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.13it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.17it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.12it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.20it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.09it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 43.97it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.09it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.20it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.17it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.09it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.09it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.16it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.24it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.26it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.18it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.22it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.18it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.19it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.14it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.17it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.19it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.21it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.24it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.30it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.23it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.27it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.17it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 43.99it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.16it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.22it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.16it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.13it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.24it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.23it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.29it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.24it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.16it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.14it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.19it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.19it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.21it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.14it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.20it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.24it/s][A                                                 
                                                 [A 40%|████      | 156/390 [01:14<01:07,  3.45it/s]
100%|██████████| 505/505 [00:11<00:00, 44.24it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:15:38,439 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 10:15:38,463 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:15:40,317 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:15:40,330 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:15:40,341 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:21<21:52,  5.64s/it] 41%|████      | 158/390 [01:21<15:35,  4.03s/it] 41%|████      | 159/390 [01:22<11:12,  2.91s/it] 41%|████      | 160/390 [01:22<08:09,  2.13s/it] 41%|████▏     | 161/390 [01:22<06:01,  1.58s/it] 42%|████▏     | 162/390 [01:23<04:31,  1.19s/it] 42%|████▏     | 163/390 [01:23<03:29,  1.08it/s] 42%|████▏     | 164/390 [01:23<02:45,  1.36it/s] 42%|████▏     | 165/390 [01:23<02:15,  1.66it/s] 43%|████▎     | 166/390 [01:24<01:53,  1.97it/s] 43%|████▎     | 167/390 [01:24<01:38,  2.26it/s] 43%|████▎     | 168/390 [01:24<01:28,  2.52it/s] 43%|████▎     | 169/390 [01:25<01:20,  2.74it/s] 44%|████▎     | 170/390 [01:25<01:15,  2.92it/s] 44%|████▍     | 171/390 [01:25<01:11,  3.06it/s] 44%|████▍     | 172/390 [01:25<01:08,  3.17it/s] 44%|████▍     | 173/390 [01:26<01:06,  3.24it/s] 45%|████▍     | 174/390 [01:26<01:05,  3.31it/s] 45%|████▍     | 175/390 [01:26<01:04,  3.35it/s] 45%|████▌     | 176/390 [01:27<01:03,  3.38it/s] 45%|████▌     | 177/390 [01:27<01:02,  3.40it/s] 46%|████▌     | 178/390 [01:27<01:02,  3.41it/s] 46%|████▌     | 179/390 [01:27<01:01,  3.42it/s] 46%|████▌     | 180/390 [01:28<01:01,  3.42it/s] 46%|████▋     | 181/390 [01:28<01:00,  3.43it/s] 47%|████▋     | 182/390 [01:28<01:00,  3.44it/s] 47%|████▋     | 183/390 [01:29<01:00,  3.44it/s] 47%|████▋     | 184/390 [01:29<00:59,  3.44it/s] 47%|████▋     | 185/390 [01:29<00:59,  3.45it/s] 48%|████▊     | 186/390 [01:30<00:59,  3.45it/s] 48%|████▊     | 187/390 [01:30<00:58,  3.45it/s] 48%|████▊     | 188/390 [01:30<00:58,  3.45it/s] 48%|████▊     | 189/390 [01:30<00:58,  3.45it/s] 49%|████▊     | 190/390 [01:31<00:57,  3.45it/s] 49%|████▉     | 191/390 [01:31<00:57,  3.44it/s] 49%|████▉     | 192/390 [01:31<00:57,  3.44it/s] 49%|████▉     | 193/390 [01:32<00:57,  3.44it/s] 50%|████▉     | 194/390 [01:32<00:56,  3.44it/s] 50%|█████     | 195/390 [01:32<00:56,  3.44it/s] 50%|█████     | 196/390 [01:32<00:56,  3.44it/s] 51%|█████     | 197/390 [01:33<00:56,  3.45it/s] 51%|█████     | 198/390 [01:33<00:55,  3.44it/s] 51%|█████     | 199/390 [01:33<00:55,  3.45it/s] 51%|█████▏    | 200/390 [01:34<00:55,  3.45it/s] 52%|█████▏    | 201/390 [01:34<00:54,  3.45it/s] 52%|█████▏    | 202/390 [01:34<00:55,  3.42it/s] 52%|█████▏    | 203/390 [01:34<00:54,  3.42it/s] 52%|█████▏    | 204/390 [01:35<00:54,  3.43it/s] 53%|█████▎    | 205/390 [01:35<00:53,  3.44it/s] 53%|█████▎    | 206/390 [01:35<00:53,  3.44it/s] 53%|█████▎    | 207/390 [01:36<00:53,  3.44it/s] 53%|█████▎    | 208/390 [01:36<00:52,  3.44it/s] 54%|█████▎    | 209/390 [01:36<00:52,  3.44it/s] 54%|█████▍    | 210/390 [01:36<00:52,  3.44it/s] 54%|█████▍    | 211/390 [01:37<00:51,  3.44it/s] 54%|█████▍    | 212/390 [01:37<00:51,  3.44it/s] 55%|█████▍    | 213/390 [01:37<00:51,  3.42it/s] 55%|█████▍    | 214/390 [01:38<00:51,  3.43it/s] 55%|█████▌    | 215/390 [01:38<00:51,  3.43it/s] 55%|█████▌    | 216/390 [01:38<00:50,  3.44it/s] 56%|█████▌    | 217/390 [01:39<00:50,  3.44it/s] 56%|█████▌    | 218/390 [01:39<00:49,  3.44it/s] 56%|█████▌    | 219/390 [01:39<00:49,  3.44it/s] 56%|█████▋    | 220/390 [01:39<00:49,  3.44it/s] 57%|█████▋    | 221/390 [01:40<00:49,  3.44it/s] 57%|█████▋    | 222/390 [01:40<00:48,  3.44it/s] 57%|█████▋    | 223/390 [01:40<00:48,  3.44it/s] 57%|█████▋    | 224/390 [01:41<00:48,  3.42it/s] 58%|█████▊    | 225/390 [01:41<00:48,  3.43it/s] 58%|█████▊    | 226/390 [01:41<00:47,  3.43it/s] 58%|█████▊    | 227/390 [01:41<00:47,  3.44it/s] 58%|█████▊    | 228/390 [01:42<00:47,  3.44it/s] 59%|█████▊    | 229/390 [01:42<00:46,  3.44it/s] 59%|█████▉    | 230/390 [01:42<00:46,  3.44it/s] 59%|█████▉    | 231/390 [01:43<00:46,  3.43it/s] 59%|█████▉    | 232/390 [01:43<00:45,  3.44it/s] 60%|█████▉    | 233/390 [01:43<00:45,  3.44it/s] 60%|██████    | 234/390 [01:43<00:45,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 10:16:07,502 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:16:07,502 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 10:16:07,502 >>   Batch size = 8
{'eval_loss': 1.0475019216537476, 'eval_runtime': 11.4331, 'eval_samples_per_second': 353.274, 'eval_steps_per_second': 44.17, 'epoch': 1.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.40it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.58it/s][A
  3%|▎         | 17/505 [00:00<00:10, 45.95it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.06it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.74it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.58it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.56it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.30it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.41it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.46it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.42it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.30it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.13it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 43.99it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.14it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.17it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.27it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.23it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.32it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.29it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.18it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.05it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 43.97it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 43.98it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.17it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.23it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.23it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.39it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.25it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.18it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.11it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.04it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.17it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.12it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.20it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.30it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.32it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.33it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.18it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.07it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.10it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.26it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.29it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.16it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.28it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.30it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.24it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.15it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 43.98it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.04it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.16it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.25it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.26it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.31it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.32it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.22it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.12it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.11it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.09it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.11it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.21it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.30it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.33it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.16it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.15it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.11it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.23it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.27it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.07it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.18it/s][A
 71%|███████   | 357/505 [00:08<00:03, 41.67it/s][A
 72%|███████▏  | 363/505 [00:08<00:03, 43.89it/s][A
 73%|███████▎  | 368/505 [00:08<00:03, 43.94it/s][A
 74%|███████▍  | 373/505 [00:08<00:03, 43.92it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 44.08it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 44.10it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 44.16it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 44.21it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 44.21it/s][A
 80%|███████▉  | 403/505 [00:09<00:02, 44.10it/s][A
 81%|████████  | 408/505 [00:09<00:02, 44.18it/s][A
 82%|████████▏ | 413/505 [00:09<00:02, 44.22it/s][A
 83%|████████▎ | 418/505 [00:09<00:01, 44.26it/s][A
 84%|████████▍ | 423/505 [00:09<00:01, 44.23it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 44.17it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 44.20it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 44.23it/s][A
 88%|████████▊ | 443/505 [00:10<00:01, 44.16it/s][A
 89%|████████▊ | 448/505 [00:10<00:01, 44.22it/s][A
 90%|████████▉ | 453/505 [00:10<00:01, 44.21it/s][A
 91%|█████████ | 458/505 [00:10<00:01, 44.14it/s][A
 92%|█████████▏| 463/505 [00:10<00:00, 44.23it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 44.25it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 44.22it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 44.16it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 44.21it/s][A
 97%|█████████▋| 488/505 [00:11<00:00, 44.15it/s][A
 98%|█████████▊| 493/505 [00:11<00:00, 44.23it/s][A
 99%|█████████▊| 498/505 [00:11<00:00, 44.24it/s][A
100%|█████████▉| 503/505 [00:11<00:00, 44.06it/s][A                                                 
                                                 [A 60%|██████    | 234/390 [01:55<00:45,  3.44it/s]
100%|██████████| 505/505 [00:11<00:00, 44.06it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:16:18,962 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 10:16:18,983 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:16:20,853 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:16:20,870 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:16:20,882 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:01<14:17,  5.53s/it] 61%|██████    | 236/390 [02:02<10:10,  3.96s/it] 61%|██████    | 237/390 [02:02<07:17,  2.86s/it] 61%|██████    | 238/390 [02:02<05:17,  2.09s/it] 61%|██████▏   | 239/390 [02:02<03:54,  1.55s/it] 62%|██████▏   | 240/390 [02:03<02:56,  1.18s/it] 62%|██████▏   | 241/390 [02:03<02:15,  1.10it/s] 62%|██████▏   | 242/390 [02:03<01:47,  1.38it/s] 62%|██████▏   | 243/390 [02:04<01:27,  1.68it/s] 63%|██████▎   | 244/390 [02:04<01:13,  1.98it/s] 63%|██████▎   | 245/390 [02:04<01:04,  2.26it/s] 63%|██████▎   | 246/390 [02:04<00:57,  2.51it/s] 63%|██████▎   | 247/390 [02:05<00:52,  2.72it/s] 64%|██████▎   | 248/390 [02:05<00:49,  2.89it/s] 64%|██████▍   | 249/390 [02:05<00:46,  3.03it/s] 64%|██████▍   | 250/390 [02:06<00:44,  3.14it/s] 64%|██████▍   | 251/390 [02:06<00:43,  3.21it/s] 65%|██████▍   | 252/390 [02:06<00:42,  3.27it/s] 65%|██████▍   | 253/390 [02:07<00:41,  3.31it/s] 65%|██████▌   | 254/390 [02:07<00:40,  3.34it/s] 65%|██████▌   | 255/390 [02:07<00:40,  3.35it/s] 66%|██████▌   | 256/390 [02:07<00:39,  3.37it/s] 66%|██████▌   | 257/390 [02:08<00:39,  3.38it/s] 66%|██████▌   | 258/390 [02:08<00:39,  3.38it/s] 66%|██████▋   | 259/390 [02:08<00:38,  3.38it/s] 67%|██████▋   | 260/390 [02:09<00:38,  3.39it/s] 67%|██████▋   | 261/390 [02:09<00:38,  3.39it/s] 67%|██████▋   | 262/390 [02:09<00:37,  3.39it/s] 67%|██████▋   | 263/390 [02:09<00:37,  3.39it/s] 68%|██████▊   | 264/390 [02:10<00:37,  3.40it/s] 68%|██████▊   | 265/390 [02:10<00:36,  3.40it/s] 68%|██████▊   | 266/390 [02:10<00:36,  3.40it/s] 68%|██████▊   | 267/390 [02:11<00:36,  3.40it/s] 69%|██████▊   | 268/390 [02:11<00:35,  3.40it/s] 69%|██████▉   | 269/390 [02:11<00:35,  3.39it/s] 69%|██████▉   | 270/390 [02:12<00:35,  3.39it/s] 69%|██████▉   | 271/390 [02:12<00:35,  3.40it/s] 70%|██████▉   | 272/390 [02:12<00:34,  3.40it/s] 70%|███████   | 273/390 [02:12<00:34,  3.40it/s] 70%|███████   | 274/390 [02:13<00:34,  3.40it/s] 71%|███████   | 275/390 [02:13<00:33,  3.40it/s] 71%|███████   | 276/390 [02:13<00:33,  3.40it/s] 71%|███████   | 277/390 [02:14<00:33,  3.40it/s] 71%|███████▏  | 278/390 [02:14<00:32,  3.40it/s] 72%|███████▏  | 279/390 [02:14<00:32,  3.41it/s] 72%|███████▏  | 280/390 [02:14<00:32,  3.41it/s] 72%|███████▏  | 281/390 [02:15<00:31,  3.42it/s] 72%|███████▏  | 282/390 [02:15<00:31,  3.43it/s] 73%|███████▎  | 283/390 [02:15<00:31,  3.44it/s] 73%|███████▎  | 284/390 [02:16<00:30,  3.44it/s] 73%|███████▎  | 285/390 [02:16<00:30,  3.44it/s] 73%|███████▎  | 286/390 [02:16<00:30,  3.44it/s] 74%|███████▎  | 287/390 [02:17<00:29,  3.44it/s] 74%|███████▍  | 288/390 [02:17<00:29,  3.44it/s] 74%|███████▍  | 289/390 [02:17<00:29,  3.44it/s] 74%|███████▍  | 290/390 [02:17<00:29,  3.45it/s] 75%|███████▍  | 291/390 [02:18<00:28,  3.45it/s] 75%|███████▍  | 292/390 [02:18<00:28,  3.45it/s] 75%|███████▌  | 293/390 [02:18<00:28,  3.44it/s] 75%|███████▌  | 294/390 [02:19<00:27,  3.45it/s] 76%|███████▌  | 295/390 [02:19<00:27,  3.45it/s] 76%|███████▌  | 296/390 [02:19<00:27,  3.45it/s] 76%|███████▌  | 297/390 [02:19<00:26,  3.45it/s] 76%|███████▋  | 298/390 [02:20<00:26,  3.44it/s] 77%|███████▋  | 299/390 [02:20<00:26,  3.44it/s] 77%|███████▋  | 300/390 [02:20<00:26,  3.44it/s] 77%|███████▋  | 301/390 [02:21<00:25,  3.44it/s] 77%|███████▋  | 302/390 [02:21<00:25,  3.44it/s] 78%|███████▊  | 303/390 [02:21<00:25,  3.45it/s] 78%|███████▊  | 304/390 [02:21<00:24,  3.44it/s] 78%|███████▊  | 305/390 [02:22<00:24,  3.44it/s] 78%|███████▊  | 306/390 [02:22<00:24,  3.45it/s] 79%|███████▊  | 307/390 [02:22<00:24,  3.44it/s] 79%|███████▉  | 308/390 [02:23<00:23,  3.45it/s] 79%|███████▉  | 309/390 [02:23<00:23,  3.43it/s] 79%|███████▉  | 310/390 [02:23<00:23,  3.44it/s] 80%|███████▉  | 311/390 [02:23<00:22,  3.44it/s] 80%|████████  | 312/390 [02:24<00:22,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 10:16:47,799 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:16:47,799 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 10:16:47,799 >>   Batch size = 8
{'eval_loss': 1.0559324026107788, 'eval_runtime': 11.445, 'eval_samples_per_second': 352.905, 'eval_steps_per_second': 44.124, 'epoch': 2.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.24it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.80it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.05it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.34it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.91it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.53it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.49it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.30it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.47it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.51it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.30it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.25it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.26it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.24it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.16it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.07it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.13it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.33it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.38it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.20it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.18it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.20it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.23it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.06it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.04it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.20it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.23it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.30it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.28it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.22it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.22it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.12it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.00it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.04it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.20it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.23it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.09it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.24it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.27it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.22it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.24it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.04it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.07it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.18it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.26it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.30it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.18it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.23it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.23it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.25it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.20it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.08it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.08it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.22it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.21it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.21it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.11it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.15it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.20it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.18it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.12it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.16it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.21it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.21it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.21it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.21it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.15it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.23it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.22it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.20it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.13it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.13it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.20it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.19it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.21it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.18it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.22it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.24it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.21it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.16it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.11it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.17it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.18it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.13it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.18it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.22it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.20it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.21it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.21it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.20it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.18it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.16it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.07it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.21it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.29it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.21it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.14it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.16it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.23it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.22it/s][A                                                 
                                                 [A 80%|████████  | 312/390 [02:35<00:22,  3.44it/s]
100%|██████████| 505/505 [00:11<00:00, 44.22it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:16:59,253 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 10:16:59,273 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:17:01,378 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:17:01,398 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:17:01,408 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:42<07:05,  5.52s/it] 81%|████████  | 314/390 [02:42<05:00,  3.96s/it] 81%|████████  | 315/390 [02:42<03:34,  2.86s/it] 81%|████████  | 316/390 [02:42<02:34,  2.09s/it] 81%|████████▏ | 317/390 [02:43<01:53,  1.55s/it] 82%|████████▏ | 318/390 [02:43<01:24,  1.17s/it] 82%|████████▏ | 319/390 [02:43<01:04,  1.10it/s] 82%|████████▏ | 320/390 [02:44<00:50,  1.38it/s] 82%|████████▏ | 321/390 [02:44<00:41,  1.68it/s] 83%|████████▎ | 322/390 [02:44<00:34,  1.98it/s] 83%|████████▎ | 323/390 [02:44<00:29,  2.26it/s] 83%|████████▎ | 324/390 [02:45<00:26,  2.52it/s] 83%|████████▎ | 325/390 [02:45<00:23,  2.72it/s] 84%|████████▎ | 326/390 [02:45<00:22,  2.90it/s] 84%|████████▍ | 327/390 [02:46<00:20,  3.04it/s] 84%|████████▍ | 328/390 [02:46<00:19,  3.14it/s] 84%|████████▍ | 329/390 [02:46<00:18,  3.21it/s] 85%|████████▍ | 330/390 [02:47<00:18,  3.27it/s] 85%|████████▍ | 331/390 [02:47<00:17,  3.31it/s] 85%|████████▌ | 332/390 [02:47<00:17,  3.34it/s] 85%|████████▌ | 333/390 [02:47<00:16,  3.36it/s] 86%|████████▌ | 334/390 [02:48<00:16,  3.37it/s] 86%|████████▌ | 335/390 [02:48<00:16,  3.38it/s] 86%|████████▌ | 336/390 [02:48<00:15,  3.38it/s] 86%|████████▋ | 337/390 [02:49<00:15,  3.39it/s] 87%|████████▋ | 338/390 [02:49<00:15,  3.39it/s] 87%|████████▋ | 339/390 [02:49<00:15,  3.39it/s] 87%|████████▋ | 340/390 [02:49<00:14,  3.39it/s] 87%|████████▋ | 341/390 [02:50<00:14,  3.40it/s] 88%|████████▊ | 342/390 [02:50<00:14,  3.40it/s] 88%|████████▊ | 343/390 [02:50<00:13,  3.39it/s] 88%|████████▊ | 344/390 [02:51<00:13,  3.39it/s] 88%|████████▊ | 345/390 [02:51<00:13,  3.39it/s] 89%|████████▊ | 346/390 [02:51<00:12,  3.40it/s] 89%|████████▉ | 347/390 [02:52<00:12,  3.40it/s] 89%|████████▉ | 348/390 [02:52<00:12,  3.31it/s] 89%|████████▉ | 349/390 [02:52<00:12,  3.34it/s] 90%|████████▉ | 350/390 [02:52<00:11,  3.36it/s] 90%|█████████ | 351/390 [02:53<00:11,  3.37it/s] 90%|█████████ | 352/390 [02:53<00:11,  3.38it/s] 91%|█████████ | 353/390 [02:53<00:10,  3.39it/s] 91%|█████████ | 354/390 [02:54<00:10,  3.38it/s] 91%|█████████ | 355/390 [02:54<00:10,  3.39it/s] 91%|█████████▏| 356/390 [02:54<00:10,  3.40it/s] 92%|█████████▏| 357/390 [02:54<00:09,  3.40it/s] 92%|█████████▏| 358/390 [02:55<00:09,  3.40it/s] 92%|█████████▏| 359/390 [02:55<00:09,  3.40it/s] 92%|█████████▏| 360/390 [02:55<00:08,  3.40it/s] 93%|█████████▎| 361/390 [02:56<00:08,  3.40it/s] 93%|█████████▎| 362/390 [02:56<00:08,  3.42it/s] 93%|█████████▎| 363/390 [02:56<00:07,  3.42it/s] 93%|█████████▎| 364/390 [02:57<00:07,  3.43it/s] 94%|█████████▎| 365/390 [02:57<00:07,  3.42it/s] 94%|█████████▍| 366/390 [02:57<00:06,  3.43it/s] 94%|█████████▍| 367/390 [02:57<00:06,  3.43it/s] 94%|█████████▍| 368/390 [02:58<00:06,  3.44it/s] 95%|█████████▍| 369/390 [02:58<00:06,  3.44it/s] 95%|█████████▍| 370/390 [02:58<00:05,  3.44it/s] 95%|█████████▌| 371/390 [02:59<00:05,  3.45it/s] 95%|█████████▌| 372/390 [02:59<00:05,  3.45it/s] 96%|█████████▌| 373/390 [02:59<00:04,  3.45it/s] 96%|█████████▌| 374/390 [02:59<00:04,  3.45it/s] 96%|█████████▌| 375/390 [03:00<00:04,  3.45it/s] 96%|█████████▋| 376/390 [03:00<00:04,  3.44it/s] 97%|█████████▋| 377/390 [03:00<00:03,  3.45it/s] 97%|█████████▋| 378/390 [03:01<00:03,  3.45it/s] 97%|█████████▋| 379/390 [03:01<00:03,  3.45it/s] 97%|█████████▋| 380/390 [03:01<00:02,  3.45it/s] 98%|█████████▊| 381/390 [03:01<00:02,  3.45it/s] 98%|█████████▊| 382/390 [03:02<00:02,  3.45it/s] 98%|█████████▊| 383/390 [03:02<00:02,  3.45it/s] 98%|█████████▊| 384/390 [03:02<00:01,  3.44it/s] 99%|█████████▊| 385/390 [03:03<00:01,  3.45it/s] 99%|█████████▉| 386/390 [03:03<00:01,  3.45it/s] 99%|█████████▉| 387/390 [03:03<00:00,  3.44it/s] 99%|█████████▉| 388/390 [03:03<00:00,  3.44it/s]100%|█████████▉| 389/390 [03:04<00:00,  3.45it/s]100%|██████████| 390/390 [03:04<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 10:17:28,062 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:17:28,062 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 10:17:28,062 >>   Batch size = 8
{'eval_loss': 1.0596261024475098, 'eval_runtime': 11.4357, 'eval_samples_per_second': 353.191, 'eval_steps_per_second': 44.16, 'epoch': 3.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.74it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.79it/s][A
  3%|▎         | 17/505 [00:00<00:10, 45.98it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.27it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.02it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.71it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.47it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.37it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.30it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.45it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.38it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.24it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.18it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.25it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.11it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.17it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.19it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.26it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.25it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.18it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.21it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.19it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.10it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.16it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.20it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.14it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.17it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.26it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.28it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.23it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.20it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.09it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.17it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.12it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.19it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.20it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.25it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.23it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.20it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.21it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.17it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.09it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.12it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.16it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.18it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.28it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.20it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.26it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.16it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.19it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.13it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.20it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.12it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.21it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.21it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.19it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.17it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.24it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.18it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.22it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.13it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.08it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.14it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.20it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.01it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.38it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.20it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.30it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.25it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.21it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.18it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.12it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.16it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.29it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.16it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.22it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.19it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 43.98it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.09it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.19it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.13it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.21it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.21it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.24it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.23it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.23it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.22it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.18it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.12it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.13it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.01it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.08it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.15it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.07it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.10it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.15it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.17it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.12it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.13it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.18it/s][A                                                 
                                                 [A100%|██████████| 390/390 [03:16<00:00,  3.45it/s]
100%|██████████| 505/505 [00:11<00:00, 44.18it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:17:39,507 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 10:17:39,525 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:17:41,111 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:17:41,123 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:17:41,136 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 10:17:44,633 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 10:17:44,635 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78 (score: 1.0413262844085693).
                                                 100%|██████████| 390/390 [03:22<00:00,  3.45it/s]100%|██████████| 390/390 [03:22<00:00,  1.92it/s]
[INFO|trainer.py:1894] 2023-08-28 10:17:46,385 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 10:17:46,396 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:17:48,171 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:17:48,191 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:17:48,199 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:17:48,381 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:17:48,382 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:17:48,382 >>   train_loss               =     0.6739
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:17:48,382 >>   train_runtime            = 0:03:22.88
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:17:48,382 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:17:48,382 >>   train_samples_per_second =     123.22
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:17:48,382 >>   train_steps_per_second   =      1.922
{'eval_loss': 1.0639376640319824, 'eval_runtime': 11.4305, 'eval_samples_per_second': 353.354, 'eval_steps_per_second': 44.18, 'epoch': 4.99}
{'train_runtime': 202.8898, 'train_samples_per_second': 123.22, 'train_steps_per_second': 1.922, 'train_loss': 0.6739199907351763, 'epoch': 4.99}
08/28/2023 10:17:48 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 10:17:48,413 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:17:48,413 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 10:17:48,413 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:08, 55.84it/s]  2%|▏         | 12/505 [00:00<00:10, 48.79it/s]  3%|▎         | 17/505 [00:00<00:10, 47.11it/s]  4%|▍         | 22/505 [00:00<00:10, 46.32it/s]  5%|▌         | 27/505 [00:00<00:10, 45.92it/s]  6%|▋         | 32/505 [00:00<00:10, 45.65it/s]  7%|▋         | 37/505 [00:00<00:10, 45.41it/s]  8%|▊         | 42/505 [00:00<00:10, 44.92it/s]  9%|▉         | 47/505 [00:01<00:10, 44.36it/s] 10%|█         | 52/505 [00:01<00:10, 43.93it/s] 11%|█▏        | 57/505 [00:01<00:10, 44.04it/s] 12%|█▏        | 62/505 [00:01<00:10, 44.27it/s] 13%|█▎        | 67/505 [00:01<00:09, 44.44it/s] 14%|█▍        | 72/505 [00:01<00:09, 44.72it/s] 15%|█▌        | 77/505 [00:01<00:09, 44.85it/s] 16%|█▌        | 82/505 [00:01<00:09, 44.80it/s] 17%|█▋        | 87/505 [00:01<00:09, 44.62it/s] 18%|█▊        | 92/505 [00:02<00:09, 44.16it/s] 19%|█▉        | 97/505 [00:02<00:09, 44.00it/s] 20%|██        | 102/505 [00:02<00:09, 44.14it/s] 21%|██        | 107/505 [00:02<00:08, 44.24it/s] 22%|██▏       | 112/505 [00:02<00:08, 44.47it/s] 23%|██▎       | 117/505 [00:02<00:08, 44.51it/s] 24%|██▍       | 122/505 [00:02<00:08, 44.62it/s] 25%|██▌       | 127/505 [00:02<00:08, 44.67it/s] 26%|██▌       | 132/505 [00:02<00:08, 44.46it/s] 27%|██▋       | 137/505 [00:03<00:08, 44.15it/s] 28%|██▊       | 142/505 [00:03<00:08, 44.00it/s] 29%|██▉       | 147/505 [00:03<00:08, 44.15it/s] 30%|███       | 152/505 [00:03<00:07, 44.14it/s] 31%|███       | 157/505 [00:03<00:07, 44.39it/s] 32%|███▏      | 162/505 [00:03<00:07, 44.63it/s] 33%|███▎      | 167/505 [00:03<00:07, 44.76it/s] 34%|███▍      | 172/505 [00:03<00:07, 44.71it/s] 35%|███▌      | 177/505 [00:03<00:07, 44.50it/s] 36%|███▌      | 182/505 [00:04<00:07, 44.29it/s] 37%|███▋      | 187/505 [00:04<00:07, 44.14it/s] 38%|███▊      | 192/505 [00:04<00:07, 44.13it/s] 39%|███▉      | 197/505 [00:04<00:06, 44.28it/s] 40%|████      | 202/505 [00:04<00:06, 44.48it/s] 41%|████      | 207/505 [00:04<00:06, 44.68it/s] 42%|████▏     | 212/505 [00:04<00:06, 44.57it/s] 43%|████▎     | 217/505 [00:04<00:06, 44.58it/s] 44%|████▍     | 222/505 [00:04<00:06, 44.34it/s] 45%|████▍     | 227/505 [00:05<00:06, 44.22it/s] 46%|████▌     | 232/505 [00:05<00:06, 44.07it/s] 47%|████▋     | 237/505 [00:05<00:06, 44.17it/s] 48%|████▊     | 242/505 [00:05<00:05, 44.28it/s] 49%|████▉     | 247/505 [00:05<00:05, 44.47it/s] 50%|████▉     | 252/505 [00:05<00:05, 44.61it/s] 51%|█████     | 257/505 [00:05<00:05, 44.67it/s] 52%|█████▏    | 262/505 [00:05<00:05, 44.54it/s] 53%|█████▎    | 267/505 [00:05<00:05, 44.32it/s] 54%|█████▍    | 272/505 [00:06<00:05, 44.23it/s] 55%|█████▍    | 277/505 [00:06<00:05, 44.19it/s] 56%|█████▌    | 282/505 [00:06<00:05, 44.25it/s] 57%|█████▋    | 287/505 [00:06<00:04, 44.29it/s] 58%|█████▊    | 292/505 [00:06<00:04, 44.44it/s] 59%|█████▉    | 297/505 [00:06<00:04, 44.54it/s] 60%|█████▉    | 302/505 [00:06<00:04, 44.57it/s] 61%|██████    | 307/505 [00:06<00:04, 44.43it/s] 62%|██████▏   | 312/505 [00:06<00:04, 44.26it/s] 63%|██████▎   | 317/505 [00:07<00:04, 44.24it/s] 64%|██████▍   | 322/505 [00:07<00:04, 44.27it/s] 65%|██████▍   | 327/505 [00:07<00:04, 44.29it/s] 66%|██████▌   | 332/505 [00:07<00:03, 44.37it/s] 67%|██████▋   | 337/505 [00:07<00:03, 44.44it/s] 68%|██████▊   | 342/505 [00:07<00:03, 44.59it/s] 69%|██████▊   | 347/505 [00:07<00:03, 44.46it/s] 70%|██████▉   | 352/505 [00:07<00:03, 44.47it/s] 71%|███████   | 357/505 [00:08<00:03, 44.31it/s] 72%|███████▏  | 362/505 [00:08<00:03, 44.27it/s] 73%|███████▎  | 367/505 [00:08<00:03, 44.19it/s] 74%|███████▎  | 372/505 [00:08<00:03, 44.30it/s] 75%|███████▍  | 377/505 [00:08<00:02, 44.31it/s] 76%|███████▌  | 382/505 [00:08<00:02, 44.37it/s] 77%|███████▋  | 387/505 [00:08<00:02, 44.52it/s] 78%|███████▊  | 392/505 [00:08<00:02, 44.50it/s] 79%|███████▊  | 397/505 [00:08<00:02, 44.40it/s] 80%|███████▉  | 402/505 [00:09<00:02, 44.39it/s] 81%|████████  | 407/505 [00:09<00:02, 44.23it/s] 82%|████████▏ | 412/505 [00:09<00:02, 44.22it/s] 83%|████████▎ | 417/505 [00:09<00:01, 44.17it/s] 84%|████████▎ | 422/505 [00:09<00:01, 44.29it/s] 85%|████████▍ | 427/505 [00:09<00:01, 44.45it/s] 86%|████████▌ | 432/505 [00:09<00:01, 44.53it/s] 87%|████████▋ | 437/505 [00:09<00:01, 44.35it/s] 88%|████████▊ | 442/505 [00:09<00:01, 44.31it/s] 89%|████████▊ | 447/505 [00:10<00:01, 44.32it/s] 90%|████████▉ | 452/505 [00:10<00:01, 44.22it/s] 90%|█████████ | 457/505 [00:10<00:01, 44.21it/s] 91%|█████████▏| 462/505 [00:10<00:00, 44.15it/s] 92%|█████████▏| 467/505 [00:10<00:00, 44.14it/s] 93%|█████████▎| 472/505 [00:10<00:00, 44.40it/s] 94%|█████████▍| 477/505 [00:10<00:00, 44.48it/s] 95%|█████████▌| 482/505 [00:10<00:00, 44.42it/s] 96%|█████████▋| 487/505 [00:10<00:00, 44.47it/s] 97%|█████████▋| 492/505 [00:11<00:00, 44.24it/s] 98%|█████████▊| 497/505 [00:11<00:00, 44.29it/s] 99%|█████████▉| 502/505 [00:11<00:00, 44.18it/s]100%|██████████| 505/505 [00:11<00:00, 44.44it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:17:59,793 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:17:59,793 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:17:59,793 >>   eval_loss               =     1.0413
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:17:59,793 >>   eval_runtime            = 0:00:11.37
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:17:59,793 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:17:59,793 >>   eval_samples_per_second =     354.93
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:17:59,793 >>   eval_steps_per_second   =     44.377
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:17:59,793 >>   perplexity              =      2.833
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:18:06,329 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:18:06,334 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:18:06,334 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:18:06,334 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:18:06,334 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:18:06,922 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:18:06,923 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:18:07,509 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:18:08,572 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:18:08,572 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:18:10,855 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:18:10,861 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:18:10,861 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:18:10,861 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:18:10,861 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:18:11,180 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:18:11,181 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:18:11,434 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:18:11,608 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:18:11,609 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:08,  1.54it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:10,  1.52it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:14,  1.51it/s]Extractor Predicting: 24it [00:15,  1.52it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:16,  1.57it/s]Extractor Predicting: 27it [00:17,  1.59it/s]Extractor Predicting: 28it [00:17,  1.57it/s]Extractor Predicting: 29it [00:18,  1.59it/s]Extractor Predicting: 30it [00:19,  1.56it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:20,  1.56it/s]Extractor Predicting: 33it [00:21,  1.55it/s]Extractor Predicting: 34it [00:21,  1.54it/s]Extractor Predicting: 35it [00:22,  1.44it/s]Extractor Predicting: 36it [00:23,  1.51it/s]Extractor Predicting: 37it [00:23,  1.58it/s]Extractor Predicting: 38it [00:24,  1.56it/s]Extractor Predicting: 39it [00:25,  1.60it/s]Extractor Predicting: 40it [00:25,  1.57it/s]Extractor Predicting: 41it [00:26,  1.56it/s]Extractor Predicting: 42it [00:27,  1.56it/s]Extractor Predicting: 43it [00:27,  1.60it/s]Extractor Predicting: 44it [00:28,  1.59it/s]Extractor Predicting: 45it [00:28,  1.61it/s]Extractor Predicting: 46it [00:29,  1.57it/s]Extractor Predicting: 47it [00:30,  1.56it/s]Extractor Predicting: 48it [00:30,  1.49it/s]Extractor Predicting: 49it [00:31,  1.54it/s]Extractor Predicting: 50it [00:32,  1.57it/s]Extractor Predicting: 51it [00:32,  1.60it/s]Extractor Predicting: 52it [00:33,  1.64it/s]Extractor Predicting: 53it [00:33,  1.62it/s]Extractor Predicting: 54it [00:34,  1.62it/s]Extractor Predicting: 55it [00:35,  1.61it/s]Extractor Predicting: 56it [00:35,  1.61it/s]Extractor Predicting: 57it [00:36,  1.62it/s]Extractor Predicting: 58it [00:37,  1.56it/s]Extractor Predicting: 59it [00:37,  1.57it/s]Extractor Predicting: 60it [00:38,  1.56it/s]Extractor Predicting: 61it [00:38,  1.60it/s]Extractor Predicting: 62it [00:39,  1.60it/s]Extractor Predicting: 63it [00:40,  1.56it/s]Extractor Predicting: 64it [00:40,  1.56it/s]Extractor Predicting: 65it [00:41,  1.55it/s]Extractor Predicting: 66it [00:42,  1.58it/s]Extractor Predicting: 67it [00:42,  1.56it/s]Extractor Predicting: 68it [00:43,  1.53it/s]Extractor Predicting: 69it [00:44,  1.50it/s]Extractor Predicting: 70it [00:44,  1.53it/s]Extractor Predicting: 71it [00:45,  1.52it/s]Extractor Predicting: 72it [00:46,  1.48it/s]Extractor Predicting: 73it [00:46,  1.48it/s]Extractor Predicting: 74it [00:47,  1.43it/s]Extractor Predicting: 75it [00:48,  1.46it/s]Extractor Predicting: 76it [00:48,  1.49it/s]Extractor Predicting: 77it [00:49,  1.49it/s]Extractor Predicting: 78it [00:50,  1.50it/s]Extractor Predicting: 79it [00:50,  1.51it/s]Extractor Predicting: 80it [00:51,  1.51it/s]Extractor Predicting: 81it [00:52,  1.52it/s]Extractor Predicting: 82it [00:52,  1.56it/s]Extractor Predicting: 83it [00:53,  1.52it/s]Extractor Predicting: 84it [00:54,  1.54it/s]Extractor Predicting: 85it [00:54,  1.52it/s]Extractor Predicting: 86it [00:55,  1.54it/s]Extractor Predicting: 87it [00:56,  1.57it/s]Extractor Predicting: 88it [00:56,  1.58it/s]Extractor Predicting: 89it [00:57,  1.56it/s]Extractor Predicting: 90it [00:57,  1.58it/s]Extractor Predicting: 91it [00:58,  1.58it/s]Extractor Predicting: 92it [00:59,  1.60it/s]Extractor Predicting: 93it [00:59,  1.59it/s]Extractor Predicting: 94it [01:00,  1.56it/s]Extractor Predicting: 95it [01:01,  1.55it/s]Extractor Predicting: 96it [01:01,  1.53it/s]Extractor Predicting: 97it [01:02,  1.53it/s]Extractor Predicting: 98it [01:03,  1.53it/s]Extractor Predicting: 99it [01:03,  1.53it/s]Extractor Predicting: 100it [01:04,  1.53it/s]Extractor Predicting: 101it [01:05,  1.49it/s]Extractor Predicting: 102it [01:05,  1.50it/s]Extractor Predicting: 103it [01:06,  1.52it/s]Extractor Predicting: 104it [01:07,  1.55it/s]Extractor Predicting: 105it [01:07,  1.55it/s]Extractor Predicting: 106it [01:08,  1.57it/s]Extractor Predicting: 107it [01:08,  1.57it/s]Extractor Predicting: 108it [01:09,  1.59it/s]Extractor Predicting: 109it [01:10,  1.59it/s]Extractor Predicting: 110it [01:10,  1.57it/s]Extractor Predicting: 111it [01:11,  1.53it/s]Extractor Predicting: 112it [01:12,  1.58it/s]Extractor Predicting: 113it [01:12,  1.56it/s]Extractor Predicting: 114it [01:13,  1.54it/s]Extractor Predicting: 115it [01:14,  1.58it/s]Extractor Predicting: 116it [01:14,  1.54it/s]Extractor Predicting: 117it [01:15,  1.53it/s]Extractor Predicting: 118it [01:16,  1.36it/s]Extractor Predicting: 119it [01:16,  1.43it/s]Extractor Predicting: 120it [01:17,  1.46it/s]Extractor Predicting: 121it [01:18,  1.46it/s]Extractor Predicting: 122it [01:18,  1.48it/s]Extractor Predicting: 123it [01:19,  1.53it/s]Extractor Predicting: 124it [01:20,  1.53it/s]Extractor Predicting: 125it [01:21,  1.41it/s]Extractor Predicting: 126it [01:21,  1.46it/s]Extractor Predicting: 127it [01:22,  1.42it/s]Extractor Predicting: 128it [01:23,  1.42it/s]Extractor Predicting: 129it [01:23,  1.42it/s]Extractor Predicting: 130it [01:24,  1.43it/s]Extractor Predicting: 131it [01:25,  1.40it/s]Extractor Predicting: 132it [01:25,  1.40it/s]Extractor Predicting: 133it [01:26,  1.41it/s]Extractor Predicting: 134it [01:27,  1.46it/s]Extractor Predicting: 135it [01:27,  1.47it/s]Extractor Predicting: 136it [01:28,  1.47it/s]Extractor Predicting: 137it [01:29,  1.49it/s]Extractor Predicting: 138it [01:30,  1.45it/s]Extractor Predicting: 139it [01:30,  1.45it/s]Extractor Predicting: 140it [01:31,  1.44it/s]Extractor Predicting: 141it [01:32,  1.44it/s]Extractor Predicting: 142it [01:32,  1.45it/s]Extractor Predicting: 143it [01:33,  1.45it/s]Extractor Predicting: 144it [01:34,  1.46it/s]Extractor Predicting: 145it [01:34,  1.44it/s]Extractor Predicting: 146it [01:35,  1.45it/s]Extractor Predicting: 147it [01:36,  1.46it/s]Extractor Predicting: 148it [01:36,  1.42it/s]Extractor Predicting: 149it [01:37,  1.47it/s]Extractor Predicting: 149it [01:37,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:19:56,234 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:19:56,243 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:19:56,244 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:19:56,244 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:19:56,244 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:19:56,553 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:19:56,554 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:19:56,813 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:19:57,845 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:19:57,846 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:20:00,695 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:20:00,698 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:20:00,699 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:20:00,699 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:20:00,699 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:20:01,400 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:20:01,401 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:20:01,991 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:20:02,165 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:20:02,165 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.1310344827586207,
  "recall": 0.009408269373607328,
  "score": 0.017556017556017555,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.73it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.68it/s]Extractor Predicting: 9it [00:05,  1.75it/s]Extractor Predicting: 10it [00:05,  1.73it/s]Extractor Predicting: 11it [00:06,  1.73it/s]Extractor Predicting: 12it [00:07,  1.74it/s]Extractor Predicting: 13it [00:07,  1.82it/s]Extractor Predicting: 14it [00:08,  1.80it/s]Extractor Predicting: 15it [00:08,  1.81it/s]Extractor Predicting: 16it [00:09,  1.80it/s]Extractor Predicting: 17it [00:09,  1.79it/s]Extractor Predicting: 18it [00:10,  1.75it/s]Extractor Predicting: 19it [00:11,  1.71it/s]Extractor Predicting: 20it [00:11,  1.72it/s]Extractor Predicting: 21it [00:12,  1.73it/s]Extractor Predicting: 22it [00:12,  1.76it/s]Extractor Predicting: 23it [00:13,  1.78it/s]Extractor Predicting: 24it [00:13,  1.76it/s]Extractor Predicting: 25it [00:14,  1.78it/s]Extractor Predicting: 26it [00:15,  1.74it/s]Extractor Predicting: 27it [00:15,  1.79it/s]Extractor Predicting: 28it [00:16,  1.81it/s]Extractor Predicting: 29it [00:16,  1.79it/s]Extractor Predicting: 30it [00:17,  1.79it/s]Extractor Predicting: 31it [00:17,  1.81it/s]Extractor Predicting: 32it [00:18,  1.80it/s]Extractor Predicting: 33it [00:18,  1.75it/s]Extractor Predicting: 34it [00:19,  1.72it/s]Extractor Predicting: 35it [00:20,  1.70it/s]Extractor Predicting: 36it [00:20,  1.75it/s]Extractor Predicting: 37it [00:21,  1.72it/s]Extractor Predicting: 38it [00:21,  1.71it/s]Extractor Predicting: 39it [00:22,  1.71it/s]Extractor Predicting: 40it [00:23,  1.69it/s]Extractor Predicting: 41it [00:23,  1.67it/s]Extractor Predicting: 42it [00:24,  1.70it/s]Extractor Predicting: 43it [00:24,  1.73it/s]Extractor Predicting: 44it [00:25,  1.72it/s]Extractor Predicting: 45it [00:26,  1.57it/s]Extractor Predicting: 46it [00:26,  1.56it/s]Extractor Predicting: 47it [00:27,  1.59it/s]Extractor Predicting: 48it [00:27,  1.61it/s]Extractor Predicting: 49it [00:28,  1.65it/s]Extractor Predicting: 50it [00:29,  1.67it/s]Extractor Predicting: 51it [00:29,  1.71it/s]Extractor Predicting: 52it [00:30,  1.68it/s]Extractor Predicting: 53it [00:30,  1.69it/s]Extractor Predicting: 54it [00:31,  1.70it/s]Extractor Predicting: 55it [00:32,  1.68it/s]Extractor Predicting: 56it [00:32,  1.63it/s]Extractor Predicting: 57it [00:33,  1.63it/s]Extractor Predicting: 58it [00:33,  1.65it/s]Extractor Predicting: 59it [00:34,  1.63it/s]Extractor Predicting: 60it [00:35,  1.64it/s]Extractor Predicting: 61it [00:35,  1.64it/s]Extractor Predicting: 62it [00:36,  1.68it/s]Extractor Predicting: 63it [00:36,  1.65it/s]Extractor Predicting: 64it [00:37,  1.68it/s]Extractor Predicting: 65it [00:38,  1.71it/s]Extractor Predicting: 66it [00:38,  1.70it/s]Extractor Predicting: 67it [00:39,  1.68it/s]Extractor Predicting: 68it [00:40,  1.55it/s]Extractor Predicting: 69it [00:40,  1.55it/s]Extractor Predicting: 70it [00:41,  1.56it/s]Extractor Predicting: 71it [00:41,  1.58it/s]Extractor Predicting: 72it [00:42,  1.60it/s]Extractor Predicting: 73it [00:43,  1.60it/s]Extractor Predicting: 74it [00:43,  1.62it/s]Extractor Predicting: 75it [00:44,  1.65it/s]Extractor Predicting: 76it [00:45,  1.62it/s]Extractor Predicting: 77it [00:45,  1.64it/s]Extractor Predicting: 78it [00:46,  1.58it/s]Extractor Predicting: 79it [00:46,  1.62it/s]Extractor Predicting: 80it [00:47,  1.66it/s]Extractor Predicting: 81it [00:48,  1.62it/s]Extractor Predicting: 82it [00:48,  1.60it/s]Extractor Predicting: 83it [00:49,  1.60it/s]Extractor Predicting: 84it [00:49,  1.64it/s]Extractor Predicting: 85it [00:50,  1.63it/s]Extractor Predicting: 86it [00:51,  1.61it/s]Extractor Predicting: 87it [00:51,  1.66it/s]Extractor Predicting: 88it [00:52,  1.68it/s]Extractor Predicting: 89it [00:52,  1.71it/s]Extractor Predicting: 90it [00:53,  1.69it/s]Extractor Predicting: 91it [00:54,  1.68it/s]Extractor Predicting: 92it [00:54,  1.68it/s]Extractor Predicting: 93it [00:55,  1.72it/s]Extractor Predicting: 94it [00:55,  1.71it/s]Extractor Predicting: 95it [00:56,  1.68it/s]Extractor Predicting: 96it [00:57,  1.71it/s]Extractor Predicting: 97it [00:57,  1.73it/s]Extractor Predicting: 98it [00:58,  1.71it/s]Extractor Predicting: 99it [00:58,  1.70it/s]Extractor Predicting: 100it [00:59,  1.72it/s]Extractor Predicting: 101it [00:59,  1.70it/s]Extractor Predicting: 102it [01:00,  1.66it/s]Extractor Predicting: 103it [01:01,  1.66it/s]Extractor Predicting: 104it [01:01,  1.70it/s]Extractor Predicting: 105it [01:02,  1.65it/s]Extractor Predicting: 106it [01:02,  1.68it/s]Extractor Predicting: 107it [01:03,  1.68it/s]Extractor Predicting: 108it [01:04,  1.64it/s]Extractor Predicting: 109it [01:04,  1.63it/s]Extractor Predicting: 110it [01:05,  1.66it/s]Extractor Predicting: 111it [01:06,  1.65it/s]Extractor Predicting: 112it [01:06,  1.65it/s]Extractor Predicting: 113it [01:07,  1.68it/s]Extractor Predicting: 114it [01:07,  1.68it/s]Extractor Predicting: 115it [01:08,  1.68it/s]Extractor Predicting: 116it [01:08,  1.71it/s]Extractor Predicting: 117it [01:09,  1.71it/s]Extractor Predicting: 118it [01:10,  1.76it/s]Extractor Predicting: 119it [01:10,  1.77it/s]Extractor Predicting: 120it [01:11,  1.77it/s]Extractor Predicting: 121it [01:11,  1.76it/s]Extractor Predicting: 122it [01:12,  1.74it/s]Extractor Predicting: 123it [01:12,  1.72it/s]Extractor Predicting: 124it [01:13,  1.73it/s]Extractor Predicting: 125it [01:14,  1.72it/s]Extractor Predicting: 126it [01:14,  1.70it/s]Extractor Predicting: 127it [01:15,  1.67it/s]Extractor Predicting: 128it [01:15,  1.64it/s]Extractor Predicting: 129it [01:16,  1.62it/s]Extractor Predicting: 130it [01:17,  1.60it/s]Extractor Predicting: 131it [01:17,  1.58it/s]Extractor Predicting: 132it [01:18,  1.59it/s]Extractor Predicting: 133it [01:19,  1.60it/s]Extractor Predicting: 134it [01:19,  1.64it/s]Extractor Predicting: 135it [01:20,  1.58it/s]Extractor Predicting: 136it [01:21,  1.57it/s]Extractor Predicting: 137it [01:21,  1.57it/s]Extractor Predicting: 138it [01:22,  1.59it/s]Extractor Predicting: 139it [01:22,  1.59it/s]Extractor Predicting: 140it [01:23,  1.56it/s]Extractor Predicting: 141it [01:24,  1.56it/s]Extractor Predicting: 142it [01:24,  1.57it/s]Extractor Predicting: 143it [01:25,  1.60it/s]Extractor Predicting: 144it [01:26,  1.58it/s]Extractor Predicting: 145it [01:26,  1.43it/s]Extractor Predicting: 146it [01:27,  1.47it/s]Extractor Predicting: 147it [01:28,  1.51it/s]Extractor Predicting: 148it [01:28,  1.54it/s]Extractor Predicting: 149it [01:29,  1.57it/s]Extractor Predicting: 150it [01:30,  1.57it/s]Extractor Predicting: 151it [01:30,  1.57it/s]Extractor Predicting: 152it [01:31,  1.57it/s]Extractor Predicting: 153it [01:32,  1.55it/s]Extractor Predicting: 154it [01:32,  1.56it/s]Extractor Predicting: 155it [01:33,  1.57it/s]Extractor Predicting: 156it [01:33,  1.62it/s]Extractor Predicting: 157it [01:34,  1.68it/s]Extractor Predicting: 158it [01:34,  1.69it/s]Extractor Predicting: 159it [01:35,  1.69it/s]Extractor Predicting: 160it [01:36,  1.70it/s]Extractor Predicting: 161it [01:36,  1.74it/s]Extractor Predicting: 162it [01:37,  1.70it/s]Extractor Predicting: 163it [01:37,  1.68it/s]Extractor Predicting: 164it [01:38,  1.70it/s]Extractor Predicting: 165it [01:39,  1.73it/s]Extractor Predicting: 166it [01:39,  1.73it/s]Extractor Predicting: 167it [01:40,  1.79it/s]Extractor Predicting: 168it [01:40,  1.80it/s]Extractor Predicting: 169it [01:41,  1.83it/s]Extractor Predicting: 170it [01:41,  1.77it/s]Extractor Predicting: 171it [01:42,  1.69it/s]Extractor Predicting: 172it [01:43,  1.66it/s]Extractor Predicting: 173it [01:43,  1.72it/s]Extractor Predicting: 173it [01:43,  1.67it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:21:51,658 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:21:51,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:21:51,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:21:51,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:21:51,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:21:52,271 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:21:52,272 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:21:52,835 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:21:53,879 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:21:53,879 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:21:56,754 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:21:56,761 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:21:56,761 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:21:56,761 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:21:56,761 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:21:57,390 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:21:57,391 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:21:57,960 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:21:58,129 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:21:58,129 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3806146572104019,
  "recall": 0.038832609744331885,
  "score": 0.07047493981177501,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.81it/s]Extractor Predicting: 2it [00:01,  1.70it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:05,  1.47it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.42it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.57it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:10,  1.74it/s]Extractor Predicting: 17it [00:10,  1.83it/s]Extractor Predicting: 18it [00:11,  1.87it/s]Extractor Predicting: 19it [00:11,  1.87it/s]Extractor Predicting: 20it [00:12,  1.91it/s]Extractor Predicting: 21it [00:12,  1.95it/s]Extractor Predicting: 22it [00:13,  1.91it/s]Extractor Predicting: 23it [00:13,  1.90it/s]Extractor Predicting: 24it [00:14,  1.89it/s]Extractor Predicting: 25it [00:14,  1.89it/s]Extractor Predicting: 26it [00:15,  1.93it/s]Extractor Predicting: 27it [00:15,  1.89it/s]Extractor Predicting: 28it [00:16,  1.90it/s]Extractor Predicting: 29it [00:16,  1.91it/s]Extractor Predicting: 30it [00:17,  1.95it/s]Extractor Predicting: 31it [00:17,  1.96it/s]Extractor Predicting: 32it [00:18,  1.96it/s]Extractor Predicting: 33it [00:18,  1.96it/s]Extractor Predicting: 34it [00:19,  1.95it/s]Extractor Predicting: 35it [00:19,  1.95it/s]Extractor Predicting: 36it [00:20,  1.90it/s]Extractor Predicting: 37it [00:21,  1.86it/s]Extractor Predicting: 38it [00:21,  1.92it/s]Extractor Predicting: 39it [00:22,  1.92it/s]Extractor Predicting: 40it [00:22,  1.92it/s]Extractor Predicting: 41it [00:23,  1.90it/s]Extractor Predicting: 42it [00:23,  1.94it/s]Extractor Predicting: 43it [00:24,  1.76it/s]Extractor Predicting: 44it [00:25,  1.66it/s]Extractor Predicting: 45it [00:25,  1.59it/s]Extractor Predicting: 46it [00:26,  1.54it/s]Extractor Predicting: 47it [00:27,  1.52it/s]Extractor Predicting: 48it [00:27,  1.51it/s]Extractor Predicting: 49it [00:28,  1.51it/s]Extractor Predicting: 50it [00:29,  1.50it/s]Extractor Predicting: 51it [00:29,  1.49it/s]Extractor Predicting: 52it [00:30,  1.48it/s]Extractor Predicting: 53it [00:31,  1.47it/s]Extractor Predicting: 54it [00:31,  1.47it/s]Extractor Predicting: 55it [00:32,  1.49it/s]Extractor Predicting: 56it [00:33,  1.46it/s]Extractor Predicting: 57it [00:33,  1.46it/s]Extractor Predicting: 58it [00:34,  1.47it/s]Extractor Predicting: 59it [00:34,  1.75it/s]Extractor Predicting: 59it [00:34,  1.69it/s]
[INFO|configuration_utils.py:515] 2023-08-28 10:22:33,931 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:22:33,932 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 10:22:33,935 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:22:33,936 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 10:22:33,939 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 10:22:37,066 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 10:22:37,075 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 10:22:37,089 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:22:37,090 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 10:22:37,094 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:22:37,105 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:22:37,105 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:22:37,105 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:22:37,105 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:22:37,105 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:22:37,105 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.8275401069518716,
  "recall": 0.20012932428063368,
  "score": 0.32231189794324394,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 10:22:37,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:38,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:38,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:39,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:39,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:40,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:40,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:41,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:42,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:42,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:43,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:43,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:44,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:45,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:45,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:46,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:46,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:47,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:48,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:48,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:49,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:50,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:13<02:00, 13.35s/it][WARNING|generation_utils.py:914] 2023-08-28 10:22:50,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:51,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:52,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:52,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:53,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:53,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:54,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:55,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:55,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:56,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:56,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:57,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:58,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:58,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:59,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:00,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:00,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:01,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:01,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:02,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:03,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:03,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:04,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:04,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:05,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:28<01:56, 14.59s/it][WARNING|generation_utils.py:914] 2023-08-28 10:23:06,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:06,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:07,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:08,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:08,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:09,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:10,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:10,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:11,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:12,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:12,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:13,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:13,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:14,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:15,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:15,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:16,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:17,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:17,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:18,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:19,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:19,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:20,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:21,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:44<01:45, 15.01s/it][WARNING|generation_utils.py:914] 2023-08-28 10:23:21,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:22,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:22,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:23,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:24,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:24,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:25,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:26,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:27,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:27,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:28,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:29,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:29,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:30,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:31,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:31,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:32,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:33,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:33,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:34,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:35,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:35,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:36,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:59<01:30, 15.16s/it][WARNING|generation_utils.py:914] 2023-08-28 10:23:37,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:37,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:38,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:38,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:39,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:39,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:40,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:41,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:41,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:42,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:42,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:43,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:43,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:44,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:45,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:45,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:46,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:46,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:47,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:47,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:48,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:49,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:12<01:11, 14.24s/it][WARNING|generation_utils.py:914] 2023-08-28 10:23:49,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:50,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:50,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:51,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:52,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:52,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:53,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:54,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:54,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:55,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:55,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:56,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:57,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:57,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:58,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:58,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:59,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:00,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:00,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:01,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:01,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:02,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:02,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:03,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:26<00:57, 14.35s/it][WARNING|generation_utils.py:914] 2023-08-28 10:24:04,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:04,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:05,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:05,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:06,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:07,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:07,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:08,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:09,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:10,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:10,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:11,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:12,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:12,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:13,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:13,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:14,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:15,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:16,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:16,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:17,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:18,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:19,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:42<00:44, 14.81s/it][WARNING|generation_utils.py:914] 2023-08-28 10:24:19,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:20,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:21,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:21,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:22,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:22,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:23,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:24,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:24,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:25,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:25,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:26,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:27,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:27,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:28,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:28,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:29,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:29,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:30,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:30,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:31,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:54<00:27, 13.90s/it][WARNING|generation_utils.py:914] 2023-08-28 10:24:31,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:32,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:33,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:33,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:34,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:35,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:35,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:36,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:36,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:37,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:38,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:38,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:39,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:39,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:40,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:40,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:41,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:41,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:42,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:43,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:43,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:06<00:13, 13.40s/it][WARNING|generation_utils.py:914] 2023-08-28 10:24:44,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:44,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:45,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:46,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:46,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:47,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:47,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:48,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:49,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:49,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:50,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:50,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:51,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:52,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:52,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:53,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:53,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:54,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:55,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:55,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:56,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:56,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:20<00:00, 13.31s/it]Generating: 100%|██████████| 10/10 [02:20<00:00, 14.00s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:25:03,366 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:25:03,372 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:25:03,372 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:25:03,372 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:25:03,372 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:25:03,956 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:25:03,957 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:25:04,533 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:25:05,601 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:25:05,601 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:25:08,636 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:25:08,641 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:25:08,641 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:25:08,641 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:25:08,641 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:25:09,278 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:25:09,279 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:25:09,848 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:25:10,018 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:25:10,018 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : given name .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('I', 'given name', '', 'The first born as of the son of the first chancellor of Austria I ( 1271 1178 ) , he was born in Brest , near the town of Mecklenburg .')", 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 220, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 410, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 457, 'raw': 608}
{'target': 600, 'success': 481, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 556, 'raw': 736}
{'target': 600, 'success': 577, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.7525, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : lowest point . Context : Later in the year ( 11 July 1920 ) , a battle between Hitler and the Luftwaffe occurred between the two factions . Head Entity : Hitler , Tail Entity : lowest point .\n']
['Relation : lowest point . Context : Later in the year ( 11 July 1920 ) , a battle between Hitler and the Luftwaffe occurred between the two factions . Head Entity : Hitler , Tail Entity : lowest point .\n', 'Relation : lowest point . Context : Eta Aquilae is the lowest point on the ecliptic of the equator , at 0 ° 47 ° 05 15 N , forming the north pole of the equatorial sky . Head Entity : Eta Aquilae , Tail Entity : highest point .\n']
['Relation : lowest point . Context : Later in the year ( 11 July 1920 ) , a battle between Hitler and the Luftwaffe occurred between the two factions . Head Entity : Hitler , Tail Entity : lowest point .\n', 'Relation : lowest point . Context : Eta Aquilae is the lowest point on the ecliptic of the equator , at 0 ° 47 ° 05 15 N , forming the north pole of the equatorial sky . Head Entity : Eta Aquilae , Tail Entity : highest point .\n', 'Relation : lowest point . Context : This was the point at which the Atlantic Ocean reached its greatest temperature in at least the past 250 years and the lowest point in the last 200 million years . Head Entity : Atlantic Ocean , Tail Entity : ocean .\n']
['Relation : lowest point . Context : Later in the year ( 11 July 1920 ) , a battle between Hitler and the Luftwaffe occurred between the two factions . Head Entity : Hitler , Tail Entity : lowest point .\n', 'Relation : lowest point . Context : Eta Aquilae is the lowest point on the ecliptic of the equator , at 0 ° 47 ° 05 15 N , forming the north pole of the equatorial sky . Head Entity : Eta Aquilae , Tail Entity : highest point .\n', 'Relation : lowest point . Context : This was the point at which the Atlantic Ocean reached its greatest temperature in at least the past 250 years and the lowest point in the last 200 million years . Head Entity : Atlantic Ocean , Tail Entity : ocean .\n', 'Relation : lowest point . Context : It is located , with the lowest point ever , on the north side of the Barents Sea , one of six countries on the Antarctic Peninsula , but not on the west side of the equator . Head Entity : Antarctica Peninsula , Tail Entity : equator .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 565, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.8098958333333334, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : mother .', 'success_rate': 0.8491847826086957, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.8821022727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : genre . Context : The song reached number one on the RPM RPM Singles & Tracks chart , peaking at number 5 on the Japanese Top Japan Singles & Tracks chart , and number 14 on the Billboard Hot 100 single chart . Head Entity : Hot 100 single , Tail Entity : singles .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 462, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 592, 'raw': 736}
{'target': 600, 'success': 617, 'raw': 768}
{'prompt': 'Relation : genre .', 'success_rate': 0.8033854166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.8260869565217391, 'errors': {'', "('Its original members', 'is a list of', '', 'Its original members are . #')", 'not enough values to unpack (expected 2, got 1)', "('National Archives', 'is a list of', '', 'It is a collection of catalogues associated with the United States National Archives in its collections of preserved and undamaged American literature and art .')", 'too many values to unpack (expected 2)', "('general manager', 'is a list of', '', 'For each franchise , the top 31 are the teams represented by the head coach and general manager .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.9241071428571429, 'errors': {'', "('', 'located on astronomical body', 'Earth', 'The Earth is tilted by 18 degrees at the equator to the plane of the ecliptic , and has an eccentricity of 0 . 05 .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9241071428571429, 'errors': {'', "('Lexus', 'manufacturer', '', 'The MPX7 was introduced in 2006 and features the Lexus RC6 engine as well as the same ECU as the S.')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : member of .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/2_ext.jsonl'}}
estimate vocab size: 10070
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10170, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.53it/s]Extractor Estimating: 2it [00:01,  1.47it/s]Extractor Estimating: 3it [00:01,  1.53it/s]Extractor Estimating: 4it [00:02,  1.53it/s]Extractor Estimating: 5it [00:03,  1.57it/s]Extractor Estimating: 6it [00:03,  1.56it/s]Extractor Estimating: 7it [00:04,  1.60it/s]Extractor Estimating: 8it [00:05,  1.63it/s]Extractor Estimating: 9it [00:05,  1.60it/s]Extractor Estimating: 10it [00:06,  1.56it/s]Extractor Estimating: 11it [00:06,  1.60it/s]Extractor Estimating: 12it [00:07,  1.56it/s]Extractor Estimating: 13it [00:08,  1.59it/s]Extractor Estimating: 14it [00:08,  1.59it/s]Extractor Estimating: 15it [00:09,  1.64it/s]Extractor Estimating: 16it [00:10,  1.61it/s]Extractor Estimating: 17it [00:10,  1.58it/s]Extractor Estimating: 18it [00:11,  1.58it/s]Extractor Estimating: 19it [00:11,  1.62it/s]Extractor Estimating: 20it [00:12,  1.50it/s]Extractor Estimating: 21it [00:13,  1.52it/s]Extractor Estimating: 22it [00:14,  1.50it/s]Extractor Estimating: 23it [00:14,  1.53it/s]Extractor Estimating: 24it [00:15,  1.55it/s]Extractor Estimating: 25it [00:15,  1.56it/s]Extractor Estimating: 26it [00:16,  1.63it/s]Extractor Estimating: 27it [00:17,  1.60it/s]Extractor Estimating: 28it [00:17,  1.61it/s]Extractor Estimating: 29it [00:18,  1.62it/s]Extractor Estimating: 30it [00:18,  1.68it/s]Extractor Estimating: 31it [00:19,  1.67it/s]Extractor Estimating: 32it [00:20,  1.68it/s]Extractor Estimating: 33it [00:20,  1.66it/s]Extractor Estimating: 34it [00:21,  1.67it/s]Extractor Estimating: 35it [00:21,  1.71it/s]Extractor Estimating: 36it [00:22,  1.71it/s]Extractor Estimating: 37it [00:23,  1.65it/s]Extractor Estimating: 38it [00:23,  1.61it/s]Extractor Estimating: 39it [00:24,  1.61it/s]Extractor Estimating: 40it [00:24,  1.68it/s]Extractor Estimating: 41it [00:25,  1.72it/s]Extractor Estimating: 42it [00:26,  1.75it/s]Extractor Estimating: 43it [00:26,  1.73it/s]Extractor Estimating: 44it [00:27,  1.74it/s]Extractor Estimating: 45it [00:27,  1.74it/s]Extractor Estimating: 46it [00:28,  1.72it/s]Extractor Estimating: 47it [00:28,  1.74it/s]Extractor Estimating: 48it [00:29,  1.73it/s]Extractor Estimating: 49it [00:30,  1.71it/s]Extractor Estimating: 50it [00:30,  1.71it/s]Extractor Estimating: 51it [00:31,  1.67it/s]Extractor Estimating: 52it [00:31,  1.62it/s]Extractor Estimating: 53it [00:32,  1.55it/s]Extractor Estimating: 54it [00:33,  1.56it/s]Extractor Estimating: 55it [00:33,  1.56it/s]Extractor Estimating: 56it [00:34,  1.57it/s]Extractor Estimating: 57it [00:35,  1.59it/s]Extractor Estimating: 58it [00:35,  1.62it/s]Extractor Estimating: 59it [00:36,  1.54it/s]Extractor Estimating: 60it [00:37,  1.49it/s]Extractor Estimating: 61it [00:37,  1.56it/s]Extractor Estimating: 62it [00:38,  1.55it/s]Extractor Estimating: 63it [00:39,  1.55it/s]Extractor Estimating: 64it [00:39,  1.49it/s]Extractor Estimating: 65it [00:40,  1.54it/s]Extractor Estimating: 66it [00:41,  1.47it/s]Extractor Estimating: 67it [00:41,  1.51it/s]Extractor Estimating: 68it [00:42,  1.50it/s]Extractor Estimating: 69it [00:43,  1.53it/s]Extractor Estimating: 70it [00:43,  1.58it/s]Extractor Estimating: 71it [00:44,  1.59it/s]Extractor Estimating: 72it [00:44,  1.56it/s]Extractor Estimating: 73it [00:45,  1.57it/s]Extractor Estimating: 74it [00:46,  1.59it/s]Extractor Estimating: 75it [00:46,  1.65it/s]Extractor Estimating: 76it [00:47,  1.61it/s]Extractor Estimating: 77it [00:48,  1.58it/s]Extractor Estimating: 78it [00:48,  1.56it/s]Extractor Estimating: 79it [00:49,  1.54it/s]Extractor Estimating: 80it [00:50,  1.48it/s]Extractor Estimating: 81it [00:50,  1.46it/s]Extractor Estimating: 82it [00:51,  1.50it/s]Extractor Estimating: 83it [00:52,  1.48it/s]Extractor Estimating: 84it [00:52,  1.46it/s]Extractor Estimating: 85it [00:53,  1.45it/s]Extractor Estimating: 86it [00:54,  1.47it/s]Extractor Estimating: 87it [00:55,  1.39it/s]Extractor Estimating: 88it [00:55,  1.38it/s]Extractor Estimating: 89it [00:56,  1.39it/s]Extractor Estimating: 90it [00:57,  1.40it/s]Extractor Estimating: 91it [00:57,  1.41it/s]Extractor Estimating: 92it [00:58,  1.40it/s]Extractor Estimating: 93it [00:59,  1.41it/s]Extractor Estimating: 94it [01:00,  1.42it/s]Extractor Estimating: 95it [01:00,  1.42it/s]Extractor Estimating: 96it [01:01,  1.45it/s]Extractor Estimating: 97it [01:02,  1.48it/s]Extractor Estimating: 98it [01:02,  1.48it/s]Extractor Estimating: 99it [01:03,  1.46it/s]Extractor Estimating: 100it [01:04,  1.46it/s]Extractor Estimating: 101it [01:04,  1.55it/s]Extractor Estimating: 102it [01:05,  1.60it/s]Extractor Estimating: 103it [01:05,  1.65it/s]Extractor Estimating: 104it [01:06,  1.67it/s]Extractor Estimating: 105it [01:06,  1.67it/s]Extractor Estimating: 106it [01:07,  1.64it/s]Extractor Estimating: 107it [01:08,  1.63it/s]Extractor Estimating: 108it [01:08,  1.68it/s]Extractor Estimating: 109it [01:09,  1.74it/s]Extractor Estimating: 110it [01:09,  1.77it/s]Extractor Estimating: 111it [01:10,  1.76it/s]Extractor Estimating: 112it [01:11,  1.72it/s]Extractor Estimating: 113it [01:11,  1.72it/s]Extractor Estimating: 114it [01:12,  1.73it/s]Extractor Estimating: 115it [01:12,  1.74it/s]Extractor Estimating: 116it [01:13,  1.67it/s]Extractor Estimating: 117it [01:14,  1.66it/s]Extractor Estimating: 118it [01:14,  1.69it/s]Extractor Estimating: 119it [01:15,  1.76it/s]Extractor Estimating: 120it [01:15,  1.71it/s]Extractor Estimating: 121it [01:16,  1.70it/s]Extractor Estimating: 122it [01:16,  1.71it/s]Extractor Estimating: 123it [01:17,  1.69it/s]Extractor Estimating: 124it [01:18,  1.70it/s]Extractor Estimating: 125it [01:18,  1.75it/s]Extractor Estimating: 126it [01:19,  1.73it/s]Extractor Estimating: 127it [01:19,  1.69it/s]Extractor Estimating: 128it [01:20,  1.70it/s]Extractor Estimating: 129it [01:23,  1.35s/it]Extractor Estimating: 130it [01:24,  1.14s/it]Extractor Estimating: 131it [01:24,  1.01it/s]Extractor Estimating: 132it [01:25,  1.12it/s]Extractor Estimating: 133it [01:26,  1.25it/s]Extractor Estimating: 134it [01:26,  1.32it/s]Extractor Estimating: 135it [01:27,  1.39it/s]Extractor Estimating: 136it [01:27,  1.45it/s]Extractor Estimating: 137it [01:28,  1.49it/s]Extractor Estimating: 138it [01:29,  1.52it/s]Extractor Estimating: 139it [01:29,  1.51it/s]Extractor Estimating: 140it [01:30,  1.56it/s]Extractor Estimating: 141it [01:31,  1.61it/s]Extractor Estimating: 142it [01:31,  1.58it/s]Extractor Estimating: 143it [01:32,  1.60it/s]Extractor Estimating: 144it [01:32,  1.61it/s]Extractor Estimating: 145it [01:33,  1.65it/s]Extractor Estimating: 146it [01:34,  1.61it/s]Extractor Estimating: 147it [01:34,  1.61it/s]Extractor Estimating: 148it [01:35,  1.64it/s]Extractor Estimating: 149it [01:36,  1.58it/s]Extractor Estimating: 150it [01:36,  1.56it/s]Extractor Estimating: 151it [01:37,  1.46it/s]Extractor Estimating: 152it [01:38,  1.52it/s]Extractor Estimating: 153it [01:38,  1.56it/s]Extractor Estimating: 154it [01:39,  1.64it/s]Extractor Estimating: 155it [01:39,  1.66it/s]Extractor Estimating: 156it [01:40,  1.75it/s]Extractor Estimating: 157it [01:40,  1.69it/s]Extractor Estimating: 158it [01:41,  1.69it/s]Extractor Estimating: 159it [01:42,  1.65it/s]Extractor Estimating: 160it [01:42,  1.74it/s]Extractor Estimating: 161it [01:43,  1.69it/s]Extractor Estimating: 162it [01:43,  1.74it/s]Extractor Estimating: 163it [01:44,  1.79it/s]Extractor Estimating: 164it [01:44,  1.77it/s]Extractor Estimating: 165it [01:45,  1.80it/s]Extractor Estimating: 166it [01:46,  1.83it/s]Extractor Estimating: 167it [01:46,  1.83it/s]Extractor Estimating: 168it [01:47,  1.81it/s]Extractor Estimating: 169it [01:47,  1.87it/s]Extractor Estimating: 170it [01:48,  1.86it/s]Extractor Estimating: 171it [01:48,  1.79it/s]Extractor Estimating: 172it [01:49,  1.70it/s]Extractor Estimating: 173it [01:50,  1.71it/s]Extractor Estimating: 174it [01:50,  1.59it/s]Extractor Estimating: 175it [01:51,  1.65it/s]Extractor Estimating: 176it [01:51,  1.74it/s]Extractor Estimating: 177it [01:52,  1.68it/s]Extractor Estimating: 178it [01:53,  1.70it/s]Extractor Estimating: 179it [01:53,  1.66it/s]Extractor Estimating: 180it [01:54,  1.67it/s]Extractor Estimating: 181it [01:54,  1.74it/s]Extractor Estimating: 182it [01:55,  1.75it/s]Extractor Estimating: 183it [01:56,  1.68it/s]Extractor Estimating: 184it [01:56,  1.72it/s]Extractor Estimating: 185it [01:57,  1.71it/s]Extractor Estimating: 186it [01:57,  1.65it/s]Extractor Estimating: 187it [01:58,  1.62it/s]Extractor Estimating: 188it [01:59,  1.62it/s]Extractor Estimating: 189it [01:59,  1.64it/s]Extractor Estimating: 190it [02:00,  1.66it/s]Extractor Estimating: 191it [02:00,  1.63it/s]Extractor Estimating: 192it [02:01,  1.67it/s]Extractor Estimating: 193it [02:02,  1.70it/s]Extractor Estimating: 194it [02:02,  1.75it/s]Extractor Estimating: 195it [02:03,  1.75it/s]Extractor Estimating: 196it [02:03,  1.72it/s]Extractor Estimating: 197it [02:04,  1.69it/s]Extractor Estimating: 198it [02:04,  1.69it/s]Extractor Estimating: 199it [02:05,  1.73it/s]Extractor Estimating: 200it [02:06,  1.74it/s]Extractor Estimating: 201it [02:06,  1.69it/s]Extractor Estimating: 202it [02:07,  1.67it/s]Extractor Estimating: 203it [02:07,  1.63it/s]Extractor Estimating: 204it [02:08,  1.56it/s]Extractor Estimating: 205it [02:09,  1.62it/s]Extractor Estimating: 206it [02:09,  1.61it/s]Extractor Estimating: 207it [02:10,  1.61it/s]Extractor Estimating: 208it [02:11,  1.64it/s]Extractor Estimating: 209it [02:11,  1.64it/s]Extractor Estimating: 210it [02:12,  1.69it/s]Extractor Estimating: 211it [02:12,  1.71it/s]Extractor Estimating: 212it [02:13,  1.61it/s]Extractor Estimating: 213it [02:14,  1.66it/s]Extractor Estimating: 214it [02:14,  1.71it/s]Extractor Estimating: 215it [02:15,  1.72it/s]Extractor Estimating: 216it [02:15,  1.73it/s]Extractor Estimating: 217it [02:16,  1.78it/s]Extractor Estimating: 218it [02:16,  1.71it/s]Extractor Estimating: 219it [02:17,  1.73it/s]Extractor Estimating: 220it [02:18,  1.68it/s]Extractor Estimating: 221it [02:18,  1.71it/s]Extractor Estimating: 222it [02:19,  1.72it/s]Extractor Estimating: 223it [02:19,  1.71it/s]Extractor Estimating: 224it [02:20,  1.68it/s]Extractor Estimating: 225it [02:20,  1.74it/s]Extractor Estimating: 226it [02:21,  1.72it/s]Extractor Estimating: 227it [02:22,  1.68it/s]Extractor Estimating: 228it [02:22,  1.51it/s]Extractor Estimating: 229it [02:23,  1.54it/s]Extractor Estimating: 230it [02:24,  1.59it/s]Extractor Estimating: 231it [02:24,  1.60it/s]Extractor Estimating: 232it [02:25,  1.59it/s]Extractor Estimating: 233it [02:26,  1.62it/s]Extractor Estimating: 234it [02:26,  1.62it/s]Extractor Estimating: 235it [02:27,  1.64it/s]Extractor Estimating: 236it [02:27,  1.64it/s]Extractor Estimating: 237it [02:28,  1.58it/s]Extractor Estimating: 238it [02:29,  1.62it/s]Extractor Estimating: 239it [02:29,  1.61it/s]Extractor Estimating: 240it [02:30,  1.63it/s]Extractor Estimating: 241it [02:30,  1.62it/s]Extractor Estimating: 242it [02:31,  1.66it/s]Extractor Estimating: 243it [02:32,  1.72it/s]Extractor Estimating: 244it [02:32,  1.63it/s]Extractor Estimating: 245it [02:33,  1.66it/s]Extractor Estimating: 246it [02:33,  1.67it/s]Extractor Estimating: 247it [02:34,  1.67it/s]Extractor Estimating: 248it [02:35,  1.68it/s]Extractor Estimating: 249it [02:35,  1.67it/s]Extractor Estimating: 250it [02:36,  1.77it/s]Extractor Estimating: 250it [02:36,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:28:02,963 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:28:02,969 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:28:02,969 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:28:02,969 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:28:02,969 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:28:03,583 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:28:03,584 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:28:04,171 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:28:05,235 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:28:05,235 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:28:08,076 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:28:08,080 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:28:08,081 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:28:08,081 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:28:08,081 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:28:08,728 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:28:08,728 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:28:09,281 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:28:09,455 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:28:09,455 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 11:55:27,484 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 11:55:27,511 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 4985 mean pseudo reward: 0.9340817218390706
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 20874
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20974, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20974, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.017, loss:659.8507
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.990, loss:626.1596
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 92, avg_time 1.011, loss:594.3151
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 192, avg_time 0.991, loss:606.1692
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.005, loss:568.7279
>> valid entity prec:0.5034, rec:0.4672, f1:0.4846
>> valid relation prec:0.1321, rec:0.0092, f1:0.0172
>> valid relation with NER prec:0.1321, rec:0.0092, f1:0.0172
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.315, loss:595.8419
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 0.999, loss:566.6695
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.003, loss:592.2081
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.001, loss:566.6010
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.005, loss:588.5728
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4941, rec:0.5412, f1:0.5166
>> valid relation prec:0.1220, rec:0.0186, f1:0.0323
>> valid relation with NER prec:0.1220, rec:0.0186, f1:0.0323
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 60, avg_time 2.327, loss:588.1435
g_step 1200, step 160, avg_time 1.006, loss:575.0585
g_step 1300, step 52, avg_time 1.007, loss:573.8639
g_step 1400, step 152, avg_time 1.000, loss:534.4992
g_step 1500, step 44, avg_time 0.999, loss:551.4243
>> valid entity prec:0.5057, rec:0.5419, f1:0.5231
>> valid relation prec:0.0910, rec:0.0159, f1:0.0270
>> valid relation with NER prec:0.0910, rec:0.0159, f1:0.0270
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 144, avg_time 2.331, loss:529.1200
g_step 1700, step 36, avg_time 1.022, loss:515.6575
g_step 1800, step 136, avg_time 0.996, loss:524.6318
g_step 1900, step 28, avg_time 1.007, loss:518.7491
g_step 2000, step 128, avg_time 1.008, loss:498.7297
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5356, rec:0.4767, f1:0.5045
>> valid relation prec:0.0929, rec:0.0250, f1:0.0394
>> valid relation with NER prec:0.0929, rec:0.0250, f1:0.0394
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 20, avg_time 2.331, loss:496.8127
g_step 2200, step 120, avg_time 0.998, loss:463.8589
g_step 2300, step 12, avg_time 1.006, loss:490.2844
g_step 2400, step 112, avg_time 0.990, loss:459.0614
g_step 2500, step 4, avg_time 1.020, loss:462.8509
>> valid entity prec:0.5070, rec:0.5048, f1:0.5059
>> valid relation prec:0.1268, rec:0.0260, f1:0.0432
>> valid relation with NER prec:0.1268, rec:0.0260, f1:0.0432
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 104, avg_time 2.334, loss:420.2819
g_step 2700, step 204, avg_time 0.995, loss:466.8404
g_step 2800, step 96, avg_time 0.998, loss:403.8578
g_step 2900, step 196, avg_time 1.010, loss:457.9482
g_step 3000, step 88, avg_time 0.990, loss:418.3656
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4914, rec:0.4067, f1:0.4450
>> valid relation prec:0.0964, rec:0.0154, f1:0.0265
>> valid relation with NER prec:0.0964, rec:0.0154, f1:0.0265
g_step 3100, step 188, avg_time 2.331, loss:439.8220
g_step 3200, step 80, avg_time 1.007, loss:400.5350
g_step 3300, step 180, avg_time 1.009, loss:415.3510
g_step 3400, step 72, avg_time 1.007, loss:400.0261
g_step 3500, step 172, avg_time 1.009, loss:398.2366
>> valid entity prec:0.4879, rec:0.4610, f1:0.4741
>> valid relation prec:0.0615, rec:0.0141, f1:0.0230
>> valid relation with NER prec:0.0615, rec:0.0141, f1:0.0230
g_step 3600, step 64, avg_time 2.336, loss:389.4399
g_step 3700, step 164, avg_time 1.007, loss:386.4235
g_step 3800, step 56, avg_time 0.999, loss:363.6955
g_step 3900, step 156, avg_time 1.004, loss:378.2212
g_step 4000, step 48, avg_time 1.002, loss:359.8552
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5039, rec:0.4526, f1:0.4769
>> valid relation prec:0.0802, rec:0.0141, f1:0.0240
>> valid relation with NER prec:0.0802, rec:0.0141, f1:0.0240
g_step 4100, step 148, avg_time 2.325, loss:376.9120
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 11:55:27 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 11:55:27 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_11-55-27_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 11:55:28 - WARNING - datasets.builder -   Using custom data configuration default-9354f1608ec2ef15
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-9354f1608ec2ef15/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 11:55:28,786 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:55:28,787 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 11:55:28,788 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:55:28,789 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 11:55:28,797 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:55:28,803 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:55:28,803 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:55:28,803 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:55:28,803 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:55:28,803 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:55:28,803 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 11:55:28,937 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 11:55:32,235 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 11:55:32,236 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-9354f1608ec2ef15/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.48ba/s] 40%|████      | 2/5 [00:00<00:00,  4.20ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.49ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.56ba/s]100%|██████████| 5/5 [00:01<00:00,  4.64ba/s]100%|██████████| 5/5 [00:01<00:00,  4.47ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.98ba/s] 40%|████      | 2/5 [00:00<00:00,  4.23ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.31ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.35ba/s]100%|██████████| 5/5 [00:00<00:00,  5.29ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.32ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.46ba/s]100%|██████████| 5/5 [00:00<00:00, 10.76ba/s]100%|██████████| 5/5 [00:00<00:00, 10.61ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.88ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.18ba/s]100%|██████████| 5/5 [00:00<00:00, 12.59ba/s]
[INFO|trainer.py:414] 2023-08-28 11:55:35,637 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 11:55:35,650 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 11:55:35,650 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 11:55:35,650 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 11:55:35,650 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 11:55:35,650 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 11:55:35,650 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 11:55:35,650 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:54,  3.38it/s]  1%|          | 2/390 [00:00<01:53,  3.42it/s]  1%|          | 3/390 [00:00<01:52,  3.43it/s]  1%|          | 4/390 [00:01<01:52,  3.44it/s]  1%|▏         | 5/390 [00:01<01:51,  3.44it/s]  2%|▏         | 6/390 [00:01<01:51,  3.44it/s]  2%|▏         | 7/390 [00:02<01:51,  3.44it/s]  2%|▏         | 8/390 [00:02<01:50,  3.44it/s]  2%|▏         | 9/390 [00:02<01:50,  3.44it/s]  3%|▎         | 10/390 [00:02<01:50,  3.44it/s]  3%|▎         | 11/390 [00:03<01:50,  3.44it/s]  3%|▎         | 12/390 [00:03<01:49,  3.44it/s]  3%|▎         | 13/390 [00:03<01:49,  3.44it/s]  4%|▎         | 14/390 [00:04<01:49,  3.44it/s]  4%|▍         | 15/390 [00:04<01:48,  3.44it/s]  4%|▍         | 16/390 [00:04<01:48,  3.45it/s]  4%|▍         | 17/390 [00:04<01:48,  3.45it/s]  5%|▍         | 18/390 [00:05<01:47,  3.44it/s]  5%|▍         | 19/390 [00:05<01:47,  3.44it/s]  5%|▌         | 20/390 [00:05<01:47,  3.44it/s]  5%|▌         | 21/390 [00:06<01:47,  3.44it/s]  6%|▌         | 22/390 [00:06<01:46,  3.44it/s]  6%|▌         | 23/390 [00:06<01:46,  3.44it/s]  6%|▌         | 24/390 [00:06<01:46,  3.44it/s]  6%|▋         | 25/390 [00:07<01:46,  3.44it/s]  7%|▋         | 26/390 [00:07<01:45,  3.44it/s]  7%|▋         | 27/390 [00:07<01:45,  3.44it/s]  7%|▋         | 28/390 [00:08<01:45,  3.44it/s]  7%|▋         | 29/390 [00:08<01:44,  3.44it/s]  8%|▊         | 30/390 [00:08<01:44,  3.44it/s]  8%|▊         | 31/390 [00:09<01:44,  3.44it/s]  8%|▊         | 32/390 [00:09<01:44,  3.44it/s]  8%|▊         | 33/390 [00:09<01:43,  3.44it/s]  9%|▊         | 34/390 [00:09<01:43,  3.44it/s]  9%|▉         | 35/390 [00:10<01:43,  3.44it/s]  9%|▉         | 36/390 [00:10<01:42,  3.44it/s]  9%|▉         | 37/390 [00:10<01:42,  3.44it/s] 10%|▉         | 38/390 [00:11<01:42,  3.44it/s] 10%|█         | 39/390 [00:11<01:42,  3.44it/s] 10%|█         | 40/390 [00:11<01:41,  3.44it/s] 11%|█         | 41/390 [00:11<01:41,  3.44it/s] 11%|█         | 42/390 [00:12<01:41,  3.44it/s] 11%|█         | 43/390 [00:12<01:40,  3.44it/s] 11%|█▏        | 44/390 [00:12<01:40,  3.44it/s] 12%|█▏        | 45/390 [00:13<01:40,  3.44it/s] 12%|█▏        | 46/390 [00:13<01:39,  3.44it/s] 12%|█▏        | 47/390 [00:13<01:39,  3.44it/s] 12%|█▏        | 48/390 [00:13<01:39,  3.44it/s] 13%|█▎        | 49/390 [00:14<01:39,  3.44it/s] 13%|█▎        | 50/390 [00:14<01:38,  3.44it/s] 13%|█▎        | 51/390 [00:14<01:38,  3.44it/s] 13%|█▎        | 52/390 [00:15<01:38,  3.43it/s] 14%|█▎        | 53/390 [00:15<01:38,  3.43it/s] 14%|█▍        | 54/390 [00:15<01:37,  3.43it/s] 14%|█▍        | 55/390 [00:15<01:37,  3.44it/s] 14%|█▍        | 56/390 [00:16<01:37,  3.44it/s] 15%|█▍        | 57/390 [00:16<01:36,  3.44it/s] 15%|█▍        | 58/390 [00:16<01:36,  3.44it/s] 15%|█▌        | 59/390 [00:17<01:36,  3.44it/s] 15%|█▌        | 60/390 [00:17<01:35,  3.44it/s] 16%|█▌        | 61/390 [00:17<01:35,  3.44it/s] 16%|█▌        | 62/390 [00:18<01:35,  3.44it/s] 16%|█▌        | 63/390 [00:18<01:35,  3.44it/s] 16%|█▋        | 64/390 [00:18<01:34,  3.44it/s] 17%|█▋        | 65/390 [00:18<01:34,  3.44it/s] 17%|█▋        | 66/390 [00:19<01:34,  3.44it/s] 17%|█▋        | 67/390 [00:19<01:33,  3.44it/s] 17%|█▋        | 68/390 [00:19<01:33,  3.44it/s] 18%|█▊        | 69/390 [00:20<01:33,  3.44it/s] 18%|█▊        | 70/390 [00:20<01:33,  3.43it/s] 18%|█▊        | 71/390 [00:20<01:32,  3.44it/s] 18%|█▊        | 72/390 [00:20<01:32,  3.44it/s] 19%|█▊        | 73/390 [00:21<01:32,  3.44it/s] 19%|█▉        | 74/390 [00:21<01:31,  3.44it/s] 19%|█▉        | 75/390 [00:21<01:31,  3.44it/s] 19%|█▉        | 76/390 [00:22<01:31,  3.44it/s] 20%|█▉        | 77/390 [00:22<01:31,  3.43it/s] 20%|██        | 78/390 [00:22<01:30,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 11:55:58,372 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:55:58,372 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 11:55:58,372 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.12it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.84it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.09it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.21it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.85it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.58it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.50it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.30it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.40it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.51it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.45it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.32it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.22it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.19it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.07it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.12it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.12it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.32it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.37it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.28it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.27it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.22it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.20it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.16it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.11it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.10it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.32it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.34it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.25it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.24it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.22it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.23it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.14it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.01it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.15it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.33it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.34it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.32it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.20it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.26it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.15it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.08it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.14it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.14it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.29it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.28it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.33it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.24it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.16it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.17it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.18it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.08it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.19it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.28it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.27it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.34it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.23it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.22it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.07it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.11it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.16it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.19it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.24it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.33it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.28it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.27it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.15it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.13it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.13it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.10it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.11it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.09it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.19it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.27it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.07it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.19it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.18it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.03it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.13it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.18it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.29it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.28it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.19it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.12it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.18it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.19it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.13it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.04it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.16it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.20it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.32it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.21it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.09it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.17it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.18it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.14it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.19it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.15it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.11it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.19it/s][A
                                                 [A                                                
100%|██████████| 505/505 [00:11<00:00, 44.19it/s][A 20%|██        | 78/390 [00:34<01:30,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:56:09,822 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 11:56:09,845 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:56:11,507 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:56:11,516 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:56:11,525 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:39<27:37,  5.33s/it] 21%|██        | 80/390 [00:40<19:44,  3.82s/it] 21%|██        | 81/390 [00:40<14:13,  2.76s/it] 21%|██        | 82/390 [00:40<10:22,  2.02s/it] 21%|██▏       | 83/390 [00:40<07:41,  1.50s/it] 22%|██▏       | 84/390 [00:41<05:49,  1.14s/it] 22%|██▏       | 85/390 [00:41<04:30,  1.13it/s] 22%|██▏       | 86/390 [00:41<03:35,  1.41it/s] 22%|██▏       | 87/390 [00:42<02:57,  1.71it/s] 23%|██▎       | 88/390 [00:42<02:30,  2.01it/s] 23%|██▎       | 89/390 [00:42<02:11,  2.29it/s] 23%|██▎       | 90/390 [00:43<01:58,  2.54it/s] 23%|██▎       | 91/390 [00:43<01:49,  2.74it/s] 24%|██▎       | 92/390 [00:43<01:42,  2.91it/s] 24%|██▍       | 93/390 [00:43<01:37,  3.04it/s] 24%|██▍       | 94/390 [00:44<01:34,  3.14it/s] 24%|██▍       | 95/390 [00:44<01:32,  3.20it/s] 25%|██▍       | 96/390 [00:44<01:30,  3.25it/s] 25%|██▍       | 97/390 [00:45<01:28,  3.29it/s] 25%|██▌       | 98/390 [00:45<01:27,  3.32it/s] 25%|██▌       | 99/390 [00:45<01:27,  3.34it/s] 26%|██▌       | 100/390 [00:45<01:26,  3.36it/s] 26%|██▌       | 101/390 [00:46<01:25,  3.38it/s] 26%|██▌       | 102/390 [00:46<01:24,  3.40it/s] 26%|██▋       | 103/390 [00:46<01:24,  3.40it/s] 27%|██▋       | 104/390 [00:47<01:23,  3.41it/s] 27%|██▋       | 105/390 [00:47<01:23,  3.42it/s] 27%|██▋       | 106/390 [00:47<01:22,  3.42it/s] 27%|██▋       | 107/390 [00:48<01:22,  3.43it/s] 28%|██▊       | 108/390 [00:48<01:22,  3.43it/s] 28%|██▊       | 109/390 [00:48<01:22,  3.43it/s] 28%|██▊       | 110/390 [00:48<01:21,  3.43it/s] 28%|██▊       | 111/390 [00:49<01:21,  3.43it/s] 29%|██▊       | 112/390 [00:49<01:21,  3.43it/s] 29%|██▉       | 113/390 [00:49<01:20,  3.43it/s] 29%|██▉       | 114/390 [00:50<01:21,  3.40it/s] 29%|██▉       | 115/390 [00:50<01:20,  3.41it/s] 30%|██▉       | 116/390 [00:50<01:20,  3.42it/s] 30%|███       | 117/390 [00:50<01:20,  3.41it/s] 30%|███       | 118/390 [00:51<01:19,  3.42it/s] 31%|███       | 119/390 [00:51<01:19,  3.42it/s] 31%|███       | 120/390 [00:51<01:18,  3.43it/s] 31%|███       | 121/390 [00:52<01:20,  3.35it/s] 31%|███▏      | 122/390 [00:52<01:19,  3.38it/s] 32%|███▏      | 123/390 [00:52<01:18,  3.39it/s] 32%|███▏      | 124/390 [00:52<01:18,  3.41it/s] 32%|███▏      | 125/390 [00:53<01:18,  3.39it/s] 32%|███▏      | 126/390 [00:53<01:17,  3.40it/s] 33%|███▎      | 127/390 [00:53<01:17,  3.41it/s] 33%|███▎      | 128/390 [00:54<01:16,  3.42it/s] 33%|███▎      | 129/390 [00:54<01:16,  3.42it/s] 33%|███▎      | 130/390 [00:54<01:16,  3.41it/s] 34%|███▎      | 131/390 [00:55<01:15,  3.41it/s] 34%|███▍      | 132/390 [00:55<01:15,  3.42it/s] 34%|███▍      | 133/390 [00:55<01:15,  3.43it/s] 34%|███▍      | 134/390 [00:55<01:14,  3.42it/s] 35%|███▍      | 135/390 [00:56<01:14,  3.43it/s] 35%|███▍      | 136/390 [00:56<01:14,  3.41it/s] 35%|███▌      | 137/390 [00:56<01:14,  3.41it/s] 35%|███▌      | 138/390 [00:57<01:13,  3.42it/s] 36%|███▌      | 139/390 [00:57<01:13,  3.42it/s] 36%|███▌      | 140/390 [00:57<01:12,  3.43it/s] 36%|███▌      | 141/390 [00:57<01:12,  3.43it/s] 36%|███▋      | 142/390 [00:58<01:12,  3.43it/s] 37%|███▋      | 143/390 [00:58<01:12,  3.43it/s] 37%|███▋      | 144/390 [00:58<01:11,  3.43it/s] 37%|███▋      | 145/390 [00:59<01:11,  3.43it/s] 37%|███▋      | 146/390 [00:59<01:11,  3.42it/s] 38%|███▊      | 147/390 [00:59<01:11,  3.41it/s] 38%|███▊      | 148/390 [01:00<01:10,  3.42it/s] 38%|███▊      | 149/390 [01:00<01:10,  3.42it/s] 38%|███▊      | 150/390 [01:00<01:10,  3.42it/s] 39%|███▊      | 151/390 [01:00<01:09,  3.42it/s] 39%|███▉      | 152/390 [01:01<01:09,  3.43it/s] 39%|███▉      | 153/390 [01:01<01:09,  3.43it/s] 39%|███▉      | 154/390 [01:01<01:08,  3.43it/s] 40%|███▉      | 155/390 [01:02<01:08,  3.43it/s] 40%|████      | 156/390 [01:02<01:08,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 11:56:38,035 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:56:38,035 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 11:56:38,035 >>   Batch size = 8
{'eval_loss': 1.0459165573120117, 'eval_runtime': 11.4309, 'eval_samples_per_second': 353.341, 'eval_steps_per_second': 44.179, 'epoch': 0.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.08it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.54it/s][A
  3%|▎         | 17/505 [00:00<00:10, 45.79it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.08it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.69it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.56it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.35it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.13it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.28it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.41it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.24it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.01it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.09it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.08it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.12it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.10it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.11it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.26it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.19it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.07it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.14it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.05it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.01it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.08it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.04it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.11it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.22it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.12it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.09it/s][A
 30%|███       | 152/505 [00:03<00:08, 44.06it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.04it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.07it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.12it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.11it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.10it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.20it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.16it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.05it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.08it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.08it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.13it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.13it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.11it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.09it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.14it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.01it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 43.95it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.09it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.16it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.08it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.10it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.09it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.20it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.13it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.05it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.07it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.10it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.19it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.17it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.15it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.25it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.08it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.06it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.02it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.12it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.16it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.10it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.14it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.19it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.12it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.04it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.03it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 43.96it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.07it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.11it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.13it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.10it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.23it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.11it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.07it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.03it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.00it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.10it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 43.78it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.18it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.20it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.28it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.23it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.03it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 43.97it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.11it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.11it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.22it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.17it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.22it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.21it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.24it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.11it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 43.98it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 43.96it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 43.96it/s][A 40%|████      | 156/390 [01:13<01:08,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:56:49,528 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 11:56:49,554 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:56:51,583 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:56:51,608 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:56:51,623 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:20<21:26,  5.52s/it] 41%|████      | 158/390 [01:20<15:17,  3.96s/it] 41%|████      | 159/390 [01:20<11:00,  2.86s/it] 41%|████      | 160/390 [01:20<08:00,  2.09s/it] 41%|████▏     | 161/390 [01:21<05:55,  1.55s/it] 42%|████▏     | 162/390 [01:21<04:27,  1.17s/it] 42%|████▏     | 163/390 [01:21<03:26,  1.10it/s] 42%|████▏     | 164/390 [01:22<02:43,  1.38it/s] 42%|████▏     | 165/390 [01:22<02:14,  1.68it/s] 43%|████▎     | 166/390 [01:22<01:53,  1.97it/s] 43%|████▎     | 167/390 [01:23<01:38,  2.26it/s] 43%|████▎     | 168/390 [01:23<01:28,  2.51it/s] 43%|████▎     | 169/390 [01:23<01:21,  2.72it/s] 44%|████▎     | 170/390 [01:23<01:16,  2.89it/s] 44%|████▍     | 171/390 [01:24<01:12,  3.02it/s] 44%|████▍     | 172/390 [01:24<01:09,  3.12it/s] 44%|████▍     | 173/390 [01:24<01:07,  3.20it/s] 45%|████▍     | 174/390 [01:25<01:06,  3.25it/s] 45%|████▍     | 175/390 [01:25<01:05,  3.29it/s] 45%|████▌     | 176/390 [01:25<01:04,  3.32it/s] 45%|████▌     | 177/390 [01:25<01:03,  3.34it/s] 46%|████▌     | 178/390 [01:26<01:03,  3.35it/s] 46%|████▌     | 179/390 [01:26<01:02,  3.36it/s] 46%|████▌     | 180/390 [01:26<01:02,  3.37it/s] 46%|████▋     | 181/390 [01:27<01:02,  3.37it/s] 47%|████▋     | 182/390 [01:27<01:01,  3.37it/s] 47%|████▋     | 183/390 [01:27<01:01,  3.38it/s] 47%|████▋     | 184/390 [01:28<01:00,  3.38it/s] 47%|████▋     | 185/390 [01:28<01:00,  3.39it/s] 48%|████▊     | 186/390 [01:28<01:00,  3.39it/s] 48%|████▊     | 187/390 [01:28<00:59,  3.39it/s] 48%|████▊     | 188/390 [01:29<00:59,  3.39it/s] 48%|████▊     | 189/390 [01:29<00:59,  3.39it/s] 49%|████▊     | 190/390 [01:29<00:59,  3.39it/s] 49%|████▉     | 191/390 [01:30<00:58,  3.39it/s] 49%|████▉     | 192/390 [01:30<00:58,  3.38it/s] 49%|████▉     | 193/390 [01:30<00:58,  3.38it/s] 50%|████▉     | 194/390 [01:31<00:57,  3.38it/s] 50%|█████     | 195/390 [01:31<00:57,  3.40it/s] 50%|█████     | 196/390 [01:31<00:56,  3.41it/s] 51%|█████     | 197/390 [01:31<00:56,  3.42it/s] 51%|█████     | 198/390 [01:32<00:56,  3.42it/s] 51%|█████     | 199/390 [01:32<00:55,  3.43it/s] 51%|█████▏    | 200/390 [01:32<00:55,  3.43it/s] 52%|█████▏    | 201/390 [01:33<00:55,  3.43it/s] 52%|█████▏    | 202/390 [01:33<00:54,  3.43it/s] 52%|█████▏    | 203/390 [01:33<00:54,  3.41it/s] 52%|█████▏    | 204/390 [01:33<00:54,  3.42it/s] 53%|█████▎    | 205/390 [01:34<00:54,  3.42it/s] 53%|█████▎    | 206/390 [01:34<00:53,  3.43it/s] 53%|█████▎    | 207/390 [01:34<00:53,  3.43it/s] 53%|█████▎    | 208/390 [01:35<00:53,  3.43it/s] 54%|█████▎    | 209/390 [01:35<00:52,  3.43it/s] 54%|█████▍    | 210/390 [01:35<00:52,  3.43it/s] 54%|█████▍    | 211/390 [01:35<00:52,  3.44it/s] 54%|█████▍    | 212/390 [01:36<00:51,  3.43it/s] 55%|█████▍    | 213/390 [01:36<00:51,  3.44it/s] 55%|█████▍    | 214/390 [01:36<00:51,  3.42it/s] 55%|█████▌    | 215/390 [01:37<00:51,  3.42it/s] 55%|█████▌    | 216/390 [01:37<00:50,  3.42it/s] 56%|█████▌    | 217/390 [01:37<00:50,  3.43it/s] 56%|█████▌    | 218/390 [01:38<00:50,  3.43it/s] 56%|█████▌    | 219/390 [01:38<00:49,  3.43it/s] 56%|█████▋    | 220/390 [01:38<00:49,  3.43it/s] 57%|█████▋    | 221/390 [01:38<00:49,  3.43it/s] 57%|█████▋    | 222/390 [01:39<00:48,  3.43it/s] 57%|█████▋    | 223/390 [01:39<00:48,  3.43it/s] 57%|█████▋    | 224/390 [01:39<00:48,  3.43it/s] 58%|█████▊    | 225/390 [01:40<00:48,  3.43it/s] 58%|█████▊    | 226/390 [01:40<00:47,  3.43it/s] 58%|█████▊    | 227/390 [01:40<00:47,  3.43it/s] 58%|█████▊    | 228/390 [01:40<00:47,  3.43it/s] 59%|█████▊    | 229/390 [01:41<00:46,  3.43it/s] 59%|█████▉    | 230/390 [01:41<00:46,  3.43it/s] 59%|█████▉    | 231/390 [01:41<00:46,  3.43it/s] 59%|█████▉    | 232/390 [01:42<00:46,  3.43it/s] 60%|█████▉    | 233/390 [01:42<00:45,  3.43it/s] 60%|██████    | 234/390 [01:42<00:45,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 11:57:18,358 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:57:18,359 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 11:57:18,359 >>   Batch size = 8
{'eval_loss': 1.063737392425537, 'eval_runtime': 11.458, 'eval_samples_per_second': 352.506, 'eval_steps_per_second': 44.074, 'epoch': 1.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.31it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.84it/s][A
  3%|▎         | 17/505 [00:00<00:10, 45.94it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.23it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.79it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.66it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.48it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.23it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.31it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.36it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.27it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.21it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.14it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.18it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.14it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.08it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.07it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.17it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.23it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.20it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.16it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.07it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.13it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.18it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.05it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.96it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.14it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.11it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.19it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.20it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.16it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.10it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.07it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.00it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.13it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.10it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.22it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.19it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.20it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.21it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.16it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.04it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.03it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.14it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.23it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.21it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.22it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.24it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.16it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.12it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.03it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.07it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.10it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.15it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.15it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.21it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.21it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.22it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.08it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.06it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.03it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.08it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.18it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.18it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.24it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.24it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.19it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.11it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.09it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 43.97it/s][A
 71%|███████   | 357/505 [00:08<00:03, 43.77it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.17it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.25it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.27it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.25it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.18it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.19it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.03it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.05it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.19it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.18it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.21it/s][A
 83%|████████▎ | 417/505 [00:09<00:02, 43.51it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 43.86it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 43.92it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 43.90it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 43.88it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 43.94it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.14it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.21it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.10it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.16it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.08it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.14it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.21it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.15it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.06it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.18it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.17it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.10it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.10it/s][A 60%|██████    | 234/390 [01:54<00:45,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:57:29,824 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 11:57:29,849 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:57:31,349 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:57:31,368 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:57:31,378 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [01:59<13:36,  5.27s/it] 61%|██████    | 236/390 [01:59<09:41,  3.78s/it] 61%|██████    | 237/390 [02:00<06:58,  2.73s/it] 61%|██████    | 238/390 [02:00<05:04,  2.00s/it] 61%|██████▏   | 239/390 [02:00<03:44,  1.49s/it] 62%|██████▏   | 240/390 [02:01<02:49,  1.13s/it] 62%|██████▏   | 241/390 [02:01<02:11,  1.14it/s] 62%|██████▏   | 242/390 [02:01<01:44,  1.42it/s] 62%|██████▏   | 243/390 [02:01<01:25,  1.72it/s] 63%|██████▎   | 244/390 [02:02<01:12,  2.02it/s] 63%|██████▎   | 245/390 [02:02<01:03,  2.30it/s] 63%|██████▎   | 246/390 [02:02<00:56,  2.54it/s] 63%|██████▎   | 247/390 [02:03<00:52,  2.74it/s] 64%|██████▎   | 248/390 [02:03<00:48,  2.91it/s] 64%|██████▍   | 249/390 [02:03<00:46,  3.04it/s] 64%|██████▍   | 250/390 [02:03<00:44,  3.13it/s] 64%|██████▍   | 251/390 [02:04<00:43,  3.21it/s] 65%|██████▍   | 252/390 [02:04<00:42,  3.26it/s] 65%|██████▍   | 253/390 [02:04<00:41,  3.30it/s] 65%|██████▌   | 254/390 [02:05<00:40,  3.32it/s] 65%|██████▌   | 255/390 [02:05<00:40,  3.34it/s] 66%|██████▌   | 256/390 [02:05<00:39,  3.35it/s] 66%|██████▌   | 257/390 [02:06<00:39,  3.36it/s] 66%|██████▌   | 258/390 [02:06<00:39,  3.35it/s] 66%|██████▋   | 259/390 [02:06<00:38,  3.36it/s] 67%|██████▋   | 260/390 [02:06<00:38,  3.37it/s] 67%|██████▋   | 261/390 [02:07<00:38,  3.37it/s] 67%|██████▋   | 262/390 [02:07<00:37,  3.38it/s] 67%|██████▋   | 263/390 [02:07<00:37,  3.38it/s] 68%|██████▊   | 264/390 [02:08<00:37,  3.38it/s] 68%|██████▊   | 265/390 [02:08<00:36,  3.38it/s] 68%|██████▊   | 266/390 [02:08<00:36,  3.38it/s] 68%|██████▊   | 267/390 [02:09<00:36,  3.39it/s] 69%|██████▊   | 268/390 [02:09<00:36,  3.39it/s] 69%|██████▉   | 269/390 [02:09<00:35,  3.37it/s] 69%|██████▉   | 270/390 [02:09<00:35,  3.38it/s] 69%|██████▉   | 271/390 [02:10<00:35,  3.38it/s] 70%|██████▉   | 272/390 [02:10<00:34,  3.39it/s] 70%|███████   | 273/390 [02:10<00:34,  3.39it/s] 70%|███████   | 274/390 [02:11<00:34,  3.39it/s] 71%|███████   | 275/390 [02:11<00:33,  3.39it/s] 71%|███████   | 276/390 [02:11<00:33,  3.39it/s] 71%|███████   | 277/390 [02:11<00:33,  3.39it/s] 71%|███████▏  | 278/390 [02:12<00:33,  3.39it/s] 72%|███████▏  | 279/390 [02:12<00:32,  3.39it/s] 72%|███████▏  | 280/390 [02:12<00:32,  3.36it/s] 72%|███████▏  | 281/390 [02:13<00:32,  3.37it/s] 72%|███████▏  | 282/390 [02:13<00:32,  3.37it/s] 73%|███████▎  | 283/390 [02:13<00:31,  3.38it/s] 73%|███████▎  | 284/390 [02:14<00:31,  3.38it/s] 73%|███████▎  | 285/390 [02:14<00:31,  3.38it/s] 73%|███████▎  | 286/390 [02:14<00:30,  3.38it/s] 74%|███████▎  | 287/390 [02:14<00:30,  3.39it/s] 74%|███████▍  | 288/390 [02:15<00:30,  3.39it/s] 74%|███████▍  | 289/390 [02:15<00:29,  3.39it/s] 74%|███████▍  | 290/390 [02:15<00:29,  3.38it/s] 75%|███████▍  | 291/390 [02:16<00:29,  3.37it/s] 75%|███████▍  | 292/390 [02:16<00:29,  3.38it/s] 75%|███████▌  | 293/390 [02:16<00:28,  3.38it/s] 75%|███████▌  | 294/390 [02:16<00:28,  3.38it/s] 76%|███████▌  | 295/390 [02:17<00:28,  3.38it/s] 76%|███████▌  | 296/390 [02:17<00:27,  3.38it/s] 76%|███████▌  | 297/390 [02:17<00:27,  3.38it/s] 76%|███████▋  | 298/390 [02:18<00:27,  3.38it/s] 77%|███████▋  | 299/390 [02:18<00:26,  3.39it/s] 77%|███████▋  | 300/390 [02:18<00:26,  3.39it/s] 77%|███████▋  | 301/390 [02:19<00:26,  3.38it/s] 77%|███████▋  | 302/390 [02:19<00:26,  3.37it/s] 78%|███████▊  | 303/390 [02:19<00:25,  3.37it/s] 78%|███████▊  | 304/390 [02:19<00:25,  3.38it/s] 78%|███████▊  | 305/390 [02:20<00:25,  3.38it/s] 78%|███████▊  | 306/390 [02:20<00:24,  3.40it/s] 79%|███████▊  | 307/390 [02:20<00:24,  3.41it/s] 79%|███████▉  | 308/390 [02:21<00:23,  3.42it/s] 79%|███████▉  | 309/390 [02:21<00:23,  3.42it/s] 79%|███████▉  | 310/390 [02:21<00:23,  3.43it/s] 80%|███████▉  | 311/390 [02:21<00:23,  3.43it/s] 80%|████████  | 312/390 [02:22<00:22,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 11:57:57,982 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:57:57,982 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 11:57:57,982 >>   Batch size = 8
{'eval_loss': 1.0729706287384033, 'eval_runtime': 11.4526, 'eval_samples_per_second': 352.671, 'eval_steps_per_second': 44.095, 'epoch': 2.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.38it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.62it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.08it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.22it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.79it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.51it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.48it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.24it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.34it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.36it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.30it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.13it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.23it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.12it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.17it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.14it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.06it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.16it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.17it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.18it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.18it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.22it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.12it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.07it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.07it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.19it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.19it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.25it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.15it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.23it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.26it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.12it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.11it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.08it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.15it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.25it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.17it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.18it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.18it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.11it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.15it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.14it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.11it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.15it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.18it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.19it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.12it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.13it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.16it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.06it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.15it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.15it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.01it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.07it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.07it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.08it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.07it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.17it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.14it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.13it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.16it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.14it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.12it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.20it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.08it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.06it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.09it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.17it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.24it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.16it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.15it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.14it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.19it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.15it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.10it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 43.93it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.11it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.15it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.11it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.15it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.15it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.12it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.18it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.09it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.10it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.18it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.16it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.12it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.13it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.24it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.28it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.15it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.18it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.10it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.21it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.11it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.18it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.12it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.05it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.14it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.14it/s][A 80%|████████  | 312/390 [02:33<00:22,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:58:09,463 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 11:58:09,486 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:58:11,399 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:58:11,421 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:58:11,434 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:39<06:53,  5.37s/it] 81%|████████  | 314/390 [02:39<04:52,  3.85s/it] 81%|████████  | 315/390 [02:40<03:28,  2.78s/it] 81%|████████  | 316/390 [02:40<02:30,  2.04s/it] 81%|████████▏ | 317/390 [02:40<01:50,  1.51s/it] 82%|████████▏ | 318/390 [02:41<01:22,  1.15s/it] 82%|████████▏ | 319/390 [02:41<01:03,  1.12it/s] 82%|████████▏ | 320/390 [02:41<00:49,  1.40it/s] 82%|████████▏ | 321/390 [02:41<00:40,  1.70it/s] 83%|████████▎ | 322/390 [02:42<00:33,  2.00it/s] 83%|████████▎ | 323/390 [02:42<00:29,  2.28it/s] 83%|████████▎ | 324/390 [02:42<00:26,  2.53it/s] 83%|████████▎ | 325/390 [02:43<00:23,  2.73it/s] 84%|████████▎ | 326/390 [02:43<00:22,  2.90it/s] 84%|████████▍ | 327/390 [02:43<00:20,  3.03it/s] 84%|████████▍ | 328/390 [02:43<00:19,  3.13it/s] 84%|████████▍ | 329/390 [02:44<00:19,  3.20it/s] 85%|████████▍ | 330/390 [02:44<00:18,  3.26it/s] 85%|████████▍ | 331/390 [02:44<00:17,  3.29it/s] 85%|████████▌ | 332/390 [02:45<00:17,  3.32it/s] 85%|████████▌ | 333/390 [02:45<00:17,  3.34it/s] 86%|████████▌ | 334/390 [02:45<00:16,  3.36it/s] 86%|████████▌ | 335/390 [02:46<00:16,  3.37it/s] 86%|████████▌ | 336/390 [02:46<00:16,  3.36it/s] 86%|████████▋ | 337/390 [02:46<00:15,  3.37it/s] 87%|████████▋ | 338/390 [02:46<00:15,  3.37it/s] 87%|████████▋ | 339/390 [02:47<00:15,  3.38it/s] 87%|████████▋ | 340/390 [02:47<00:14,  3.38it/s] 87%|████████▋ | 341/390 [02:47<00:14,  3.38it/s] 88%|████████▊ | 342/390 [02:48<00:14,  3.38it/s] 88%|████████▊ | 343/390 [02:48<00:13,  3.38it/s] 88%|████████▊ | 344/390 [02:48<00:13,  3.38it/s] 88%|████████▊ | 345/390 [02:48<00:13,  3.39it/s] 89%|████████▊ | 346/390 [02:49<00:12,  3.39it/s] 89%|████████▉ | 347/390 [02:49<00:12,  3.36it/s] 89%|████████▉ | 348/390 [02:49<00:12,  3.37it/s] 89%|████████▉ | 349/390 [02:50<00:12,  3.38it/s] 90%|████████▉ | 350/390 [02:50<00:11,  3.38it/s] 90%|█████████ | 351/390 [02:50<00:11,  3.38it/s] 90%|█████████ | 352/390 [02:51<00:11,  3.37it/s] 91%|█████████ | 353/390 [02:51<00:10,  3.38it/s] 91%|█████████ | 354/390 [02:51<00:10,  3.38it/s] 91%|█████████ | 355/390 [02:51<00:10,  3.39it/s] 91%|█████████▏| 356/390 [02:52<00:10,  3.29it/s] 92%|█████████▏| 357/390 [02:52<00:09,  3.32it/s] 92%|█████████▏| 358/390 [02:52<00:09,  3.31it/s] 92%|█████████▏| 359/390 [02:53<00:09,  3.35it/s] 92%|█████████▏| 360/390 [02:53<00:08,  3.38it/s] 93%|█████████▎| 361/390 [02:53<00:08,  3.39it/s] 93%|█████████▎| 362/390 [02:54<00:08,  3.41it/s] 93%|█████████▎| 363/390 [02:54<00:07,  3.42it/s] 93%|█████████▎| 364/390 [02:54<00:07,  3.42it/s] 94%|█████████▎| 365/390 [02:54<00:07,  3.43it/s] 94%|█████████▍| 366/390 [02:55<00:06,  3.43it/s] 94%|█████████▍| 367/390 [02:55<00:06,  3.43it/s] 94%|█████████▍| 368/390 [02:55<00:06,  3.43it/s] 95%|█████████▍| 369/390 [02:56<00:06,  3.43it/s] 95%|█████████▍| 370/390 [02:56<00:05,  3.43it/s] 95%|█████████▌| 371/390 [02:56<00:05,  3.43it/s] 95%|█████████▌| 372/390 [02:56<00:05,  3.43it/s] 96%|█████████▌| 373/390 [02:57<00:04,  3.43it/s] 96%|█████████▌| 374/390 [02:57<00:04,  3.43it/s] 96%|█████████▌| 375/390 [02:57<00:04,  3.43it/s] 96%|█████████▋| 376/390 [02:58<00:04,  3.43it/s] 97%|█████████▋| 377/390 [02:58<00:03,  3.43it/s] 97%|█████████▋| 378/390 [02:58<00:03,  3.43it/s] 97%|█████████▋| 379/390 [02:58<00:03,  3.43it/s] 97%|█████████▋| 380/390 [02:59<00:02,  3.43it/s] 98%|█████████▊| 381/390 [02:59<00:02,  3.43it/s] 98%|█████████▊| 382/390 [02:59<00:02,  3.43it/s] 98%|█████████▊| 383/390 [03:00<00:02,  3.43it/s] 98%|█████████▊| 384/390 [03:00<00:01,  3.43it/s] 99%|█████████▊| 385/390 [03:00<00:01,  3.43it/s] 99%|█████████▉| 386/390 [03:01<00:01,  3.43it/s] 99%|█████████▉| 387/390 [03:01<00:00,  3.43it/s] 99%|█████████▉| 388/390 [03:01<00:00,  3.43it/s]100%|█████████▉| 389/390 [03:01<00:00,  3.43it/s]100%|██████████| 390/390 [03:02<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 11:58:37,841 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:58:37,842 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 11:58:37,842 >>   Batch size = 8
{'eval_loss': 1.084326982498169, 'eval_runtime': 11.4464, 'eval_samples_per_second': 352.861, 'eval_steps_per_second': 44.119, 'epoch': 3.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.46it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.97it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.16it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.29it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.94it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.74it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.48it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.28it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.40it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.37it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.30it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.15it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.07it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.15it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.26it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.22it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.18it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.21it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.22it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.18it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.15it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.07it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.13it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.20it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.19it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.08it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.14it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.21it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.23it/s][A
 30%|███       | 152/505 [00:03<00:08, 44.11it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.05it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.21it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.25it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.18it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.22it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.19it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.28it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.24it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.16it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.10it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.04it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.11it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.04it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.11it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.23it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.21it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.24it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.24it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.08it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.12it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.14it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.04it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.22it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.28it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.23it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.22it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.21it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.15it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.12it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.06it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.11it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.17it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.19it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.36it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.24it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.21it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.21it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.12it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.02it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.09it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.25it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.28it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.30it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.16it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.16it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.19it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.17it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.12it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.11it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.24it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.27it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.30it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.23it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.10it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.15it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.11it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.13it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.04it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.27it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.29it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.21it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.20it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.13it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.14it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.12it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.06it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.06it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.20it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.34it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.23it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.23it/s][A100%|██████████| 390/390 [03:13<00:00,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 11:58:49,283 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 11:58:49,310 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:58:51,076 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:58:51,092 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:58:51,103 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 11:58:54,986 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 11:58:54,989 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78 (score: 1.0459165573120117).
                                                 100%|██████████| 390/390 [03:21<00:00,  3.43it/s]100%|██████████| 390/390 [03:21<00:00,  1.94it/s]
[INFO|trainer.py:1894] 2023-08-28 11:58:56,843 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 11:58:56,860 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 11:58:58,758 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 11:58:58,776 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 11:58:58,785 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 11:58:58,998 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:58:58,999 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:58:58,999 >>   train_loss               =     0.5973
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:58:58,999 >>   train_runtime            = 0:03:21.18
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:58:58,999 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:58:58,999 >>   train_samples_per_second =    124.264
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:58:58,999 >>   train_steps_per_second   =      1.939
{'eval_loss': 1.0858248472213745, 'eval_runtime': 11.4321, 'eval_samples_per_second': 353.304, 'eval_steps_per_second': 44.174, 'epoch': 4.99}
{'train_runtime': 201.185, 'train_samples_per_second': 124.264, 'train_steps_per_second': 1.939, 'train_loss': 0.5973208305163261, 'epoch': 4.99}
08/28/2023 11:58:59 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 11:58:59,033 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 11:58:59,034 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 11:58:59,034 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:09, 54.75it/s]  2%|▏         | 12/505 [00:00<00:10, 48.51it/s]  3%|▎         | 17/505 [00:00<00:10, 46.91it/s]  4%|▍         | 22/505 [00:00<00:10, 46.16it/s]  5%|▌         | 27/505 [00:00<00:10, 45.69it/s]  6%|▋         | 32/505 [00:00<00:10, 45.27it/s]  7%|▋         | 37/505 [00:00<00:10, 45.15it/s]  8%|▊         | 42/505 [00:00<00:10, 44.63it/s]  9%|▉         | 47/505 [00:01<00:10, 44.09it/s] 10%|█         | 52/505 [00:01<00:10, 43.86it/s] 11%|█▏        | 57/505 [00:01<00:10, 43.97it/s] 12%|█▏        | 62/505 [00:01<00:10, 44.10it/s] 13%|█▎        | 67/505 [00:01<00:09, 44.34it/s] 14%|█▍        | 72/505 [00:01<00:09, 44.48it/s] 15%|█▌        | 77/505 [00:01<00:09, 44.47it/s] 16%|█▌        | 82/505 [00:01<00:09, 44.49it/s] 17%|█▋        | 87/505 [00:01<00:09, 44.23it/s] 18%|█▊        | 92/505 [00:02<00:09, 43.91it/s] 19%|█▉        | 97/505 [00:02<00:09, 43.94it/s] 20%|██        | 102/505 [00:02<00:09, 43.93it/s] 21%|██        | 107/505 [00:02<00:09, 44.07it/s] 22%|██▏       | 112/505 [00:02<00:08, 44.18it/s] 23%|██▎       | 117/505 [00:02<00:08, 44.38it/s] 24%|██▍       | 122/505 [00:02<00:08, 44.45it/s] 25%|██▌       | 127/505 [00:02<00:08, 44.12it/s] 26%|██▌       | 132/505 [00:02<00:08, 44.11it/s] 27%|██▋       | 137/505 [00:03<00:08, 43.97it/s] 28%|██▊       | 142/505 [00:03<00:08, 43.89it/s] 29%|██▉       | 147/505 [00:03<00:08, 43.95it/s] 30%|███       | 152/505 [00:03<00:08, 44.09it/s] 31%|███       | 157/505 [00:03<00:07, 44.19it/s] 32%|███▏      | 162/505 [00:03<00:07, 44.37it/s] 33%|███▎      | 167/505 [00:03<00:07, 44.40it/s] 34%|███▍      | 172/505 [00:03<00:07, 44.26it/s] 35%|███▌      | 177/505 [00:03<00:07, 44.11it/s] 36%|███▌      | 182/505 [00:04<00:07, 43.90it/s] 37%|███▋      | 187/505 [00:04<00:07, 43.94it/s] 38%|███▊      | 192/505 [00:04<00:07, 44.06it/s] 39%|███▉      | 197/505 [00:04<00:06, 44.13it/s] 40%|████      | 202/505 [00:04<00:06, 44.27it/s] 41%|████      | 207/505 [00:04<00:06, 44.39it/s] 42%|████▏     | 212/505 [00:04<00:06, 44.39it/s] 43%|████▎     | 217/505 [00:04<00:06, 44.24it/s] 44%|████▍     | 222/505 [00:04<00:06, 44.07it/s] 45%|████▍     | 227/505 [00:05<00:06, 43.98it/s] 46%|████▌     | 232/505 [00:05<00:06, 43.89it/s] 47%|████▋     | 237/505 [00:05<00:06, 43.95it/s] 48%|████▊     | 242/505 [00:05<00:05, 44.09it/s] 49%|████▉     | 247/505 [00:05<00:05, 44.25it/s] 50%|████▉     | 252/505 [00:05<00:05, 44.30it/s] 51%|█████     | 257/505 [00:05<00:05, 44.25it/s] 52%|█████▏    | 262/505 [00:05<00:05, 44.23it/s] 53%|█████▎    | 267/505 [00:06<00:05, 44.10it/s] 54%|█████▍    | 272/505 [00:06<00:05, 43.93it/s] 55%|█████▍    | 277/505 [00:06<00:05, 44.02it/s] 56%|█████▌    | 282/505 [00:06<00:05, 44.02it/s] 57%|█████▋    | 287/505 [00:06<00:04, 44.06it/s] 58%|█████▊    | 292/505 [00:06<00:04, 44.24it/s] 59%|█████▉    | 297/505 [00:06<00:04, 44.26it/s] 60%|█████▉    | 302/505 [00:06<00:04, 44.27it/s] 61%|██████    | 307/505 [00:06<00:04, 44.17it/s] 62%|██████▏   | 312/505 [00:07<00:04, 44.03it/s] 63%|██████▎   | 317/505 [00:07<00:04, 43.99it/s] 64%|██████▍   | 322/505 [00:07<00:04, 44.07it/s] 65%|██████▍   | 327/505 [00:07<00:04, 44.04it/s] 66%|██████▌   | 332/505 [00:07<00:03, 43.94it/s] 67%|██████▋   | 337/505 [00:07<00:03, 44.13it/s] 68%|██████▊   | 342/505 [00:07<00:03, 44.26it/s] 69%|██████▊   | 347/505 [00:07<00:03, 44.18it/s] 70%|██████▉   | 352/505 [00:07<00:03, 44.15it/s] 71%|███████   | 357/505 [00:08<00:03, 44.09it/s] 72%|███████▏  | 362/505 [00:08<00:03, 44.07it/s] 73%|███████▎  | 367/505 [00:08<00:03, 43.99it/s] 74%|███████▎  | 372/505 [00:08<00:03, 44.09it/s] 75%|███████▍  | 377/505 [00:08<00:02, 44.06it/s] 76%|███████▌  | 382/505 [00:08<00:02, 43.91it/s] 77%|███████▋  | 387/505 [00:08<00:02, 44.09it/s] 78%|███████▊  | 392/505 [00:08<00:02, 44.08it/s] 79%|███████▊  | 397/505 [00:08<00:02, 44.00it/s] 80%|███████▉  | 402/505 [00:09<00:02, 44.01it/s] 81%|████████  | 407/505 [00:09<00:02, 44.07it/s] 82%|████████▏ | 412/505 [00:09<00:02, 44.02it/s] 83%|████████▎ | 417/505 [00:09<00:01, 44.06it/s] 84%|████████▎ | 422/505 [00:09<00:01, 44.12it/s] 85%|████████▍ | 427/505 [00:09<00:01, 44.17it/s] 86%|████████▌ | 432/505 [00:09<00:01, 44.19it/s] 87%|████████▋ | 437/505 [00:09<00:01, 44.14it/s] 88%|████████▊ | 442/505 [00:09<00:01, 44.05it/s] 89%|████████▊ | 447/505 [00:10<00:01, 44.09it/s] 90%|████████▉ | 452/505 [00:10<00:01, 44.04it/s] 90%|█████████ | 457/505 [00:10<00:01, 44.00it/s] 91%|█████████▏| 462/505 [00:10<00:00, 44.07it/s] 92%|█████████▏| 467/505 [00:10<00:00, 44.16it/s] 93%|█████████▎| 472/505 [00:10<00:00, 44.12it/s] 94%|█████████▍| 477/505 [00:10<00:00, 44.13it/s] 95%|█████████▌| 482/505 [00:10<00:00, 44.11it/s] 96%|█████████▋| 487/505 [00:11<00:00, 44.09it/s] 97%|█████████▋| 492/505 [00:11<00:00, 44.06it/s] 98%|█████████▊| 497/505 [00:11<00:00, 44.04it/s] 99%|█████████▉| 502/505 [00:11<00:00, 44.02it/s]100%|██████████| 505/505 [00:11<00:00, 44.20it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 11:59:10,477 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:59:10,477 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:59:10,477 >>   eval_loss               =     1.0459
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:59:10,477 >>   eval_runtime            = 0:00:11.44
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:59:10,477 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:59:10,477 >>   eval_samples_per_second =    352.955
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:59:10,477 >>   eval_steps_per_second   =      44.13
[INFO|trainer_pt_utils.py:913] 2023-08-28 11:59:10,477 >>   perplexity              =      2.846
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:59:17,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:59:17,206 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:59:17,207 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:59:17,207 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:59:17,207 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 11:59:17,836 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 11:59:17,837 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:59:18,383 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 11:59:19,423 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:59:19,423 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:59:22,275 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:59:22,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:59:22,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:59:22,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:59:22,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 11:59:22,927 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 11:59:22,928 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:59:23,523 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 11:59:23,693 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:59:23,693 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-156
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.51it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:12,  1.53it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:14,  1.50it/s]Extractor Predicting: 24it [00:15,  1.50it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:17,  1.58it/s]Extractor Predicting: 28it [00:18,  1.56it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:20,  1.56it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:21,  1.55it/s]Extractor Predicting: 34it [00:21,  1.54it/s]Extractor Predicting: 35it [00:22,  1.53it/s]Extractor Predicting: 36it [00:23,  1.49it/s]Extractor Predicting: 37it [00:23,  1.56it/s]Extractor Predicting: 38it [00:24,  1.55it/s]Extractor Predicting: 39it [00:25,  1.59it/s]Extractor Predicting: 40it [00:25,  1.56it/s]Extractor Predicting: 41it [00:26,  1.55it/s]Extractor Predicting: 42it [00:27,  1.55it/s]Extractor Predicting: 43it [00:27,  1.59it/s]Extractor Predicting: 44it [00:28,  1.58it/s]Extractor Predicting: 45it [00:29,  1.59it/s]Extractor Predicting: 46it [00:29,  1.56it/s]Extractor Predicting: 47it [00:30,  1.55it/s]Extractor Predicting: 48it [00:31,  1.49it/s]Extractor Predicting: 49it [00:31,  1.54it/s]Extractor Predicting: 50it [00:32,  1.57it/s]Extractor Predicting: 51it [00:32,  1.59it/s]Extractor Predicting: 52it [00:33,  1.63it/s]Extractor Predicting: 53it [00:34,  1.62it/s]Extractor Predicting: 54it [00:34,  1.62it/s]Extractor Predicting: 55it [00:35,  1.61it/s]Extractor Predicting: 56it [00:35,  1.60it/s]Extractor Predicting: 57it [00:36,  1.61it/s]Extractor Predicting: 58it [00:37,  1.56it/s]Extractor Predicting: 59it [00:37,  1.57it/s]Extractor Predicting: 60it [00:38,  1.56it/s]Extractor Predicting: 61it [00:39,  1.61it/s]Extractor Predicting: 62it [00:39,  1.60it/s]Extractor Predicting: 63it [00:40,  1.57it/s]Extractor Predicting: 64it [00:41,  1.56it/s]Extractor Predicting: 65it [00:41,  1.55it/s]Extractor Predicting: 66it [00:42,  1.57it/s]Extractor Predicting: 67it [00:43,  1.56it/s]Extractor Predicting: 68it [00:43,  1.52it/s]Extractor Predicting: 69it [00:44,  1.50it/s]Extractor Predicting: 70it [00:45,  1.53it/s]Extractor Predicting: 71it [00:45,  1.53it/s]Extractor Predicting: 72it [00:46,  1.49it/s]Extractor Predicting: 73it [00:47,  1.48it/s]Extractor Predicting: 74it [00:47,  1.43it/s]Extractor Predicting: 75it [00:48,  1.46it/s]Extractor Predicting: 76it [00:49,  1.49it/s]Extractor Predicting: 77it [00:49,  1.48it/s]Extractor Predicting: 78it [00:50,  1.49it/s]Extractor Predicting: 79it [00:51,  1.50it/s]Extractor Predicting: 80it [00:51,  1.51it/s]Extractor Predicting: 81it [00:52,  1.52it/s]Extractor Predicting: 82it [00:53,  1.56it/s]Extractor Predicting: 83it [00:53,  1.52it/s]Extractor Predicting: 84it [00:54,  1.53it/s]Extractor Predicting: 85it [00:55,  1.52it/s]Extractor Predicting: 86it [00:55,  1.54it/s]Extractor Predicting: 87it [00:56,  1.57it/s]Extractor Predicting: 88it [00:56,  1.58it/s]Extractor Predicting: 89it [00:57,  1.56it/s]Extractor Predicting: 90it [00:58,  1.58it/s]Extractor Predicting: 91it [00:58,  1.58it/s]Extractor Predicting: 92it [00:59,  1.60it/s]Extractor Predicting: 93it [01:00,  1.59it/s]Extractor Predicting: 94it [01:00,  1.57it/s]Extractor Predicting: 95it [01:01,  1.55it/s]Extractor Predicting: 96it [01:02,  1.53it/s]Extractor Predicting: 97it [01:02,  1.53it/s]Extractor Predicting: 98it [01:03,  1.53it/s]Extractor Predicting: 99it [01:03,  1.53it/s]Extractor Predicting: 100it [01:04,  1.53it/s]Extractor Predicting: 101it [01:05,  1.49it/s]Extractor Predicting: 102it [01:05,  1.50it/s]Extractor Predicting: 103it [01:06,  1.51it/s]Extractor Predicting: 104it [01:07,  1.55it/s]Extractor Predicting: 105it [01:07,  1.54it/s]Extractor Predicting: 106it [01:08,  1.57it/s]Extractor Predicting: 107it [01:09,  1.57it/s]Extractor Predicting: 108it [01:09,  1.59it/s]Extractor Predicting: 109it [01:10,  1.45it/s]Extractor Predicting: 110it [01:11,  1.47it/s]Extractor Predicting: 111it [01:11,  1.46it/s]Extractor Predicting: 112it [01:12,  1.52it/s]Extractor Predicting: 113it [01:13,  1.52it/s]Extractor Predicting: 114it [01:13,  1.50it/s]Extractor Predicting: 115it [01:14,  1.55it/s]Extractor Predicting: 116it [01:15,  1.56it/s]Extractor Predicting: 117it [01:15,  1.55it/s]Extractor Predicting: 118it [01:16,  1.50it/s]Extractor Predicting: 119it [01:17,  1.54it/s]Extractor Predicting: 120it [01:17,  1.54it/s]Extractor Predicting: 121it [01:18,  1.52it/s]Extractor Predicting: 122it [01:19,  1.52it/s]Extractor Predicting: 123it [01:19,  1.55it/s]Extractor Predicting: 124it [01:20,  1.54it/s]Extractor Predicting: 125it [01:21,  1.42it/s]Extractor Predicting: 126it [01:21,  1.47it/s]Extractor Predicting: 127it [01:22,  1.42it/s]Extractor Predicting: 128it [01:23,  1.41it/s]Extractor Predicting: 129it [01:24,  1.41it/s]Extractor Predicting: 130it [01:24,  1.41it/s]Extractor Predicting: 131it [01:25,  1.39it/s]Extractor Predicting: 132it [01:26,  1.39it/s]Extractor Predicting: 133it [01:26,  1.40it/s]Extractor Predicting: 134it [01:27,  1.44it/s]Extractor Predicting: 135it [01:28,  1.45it/s]Extractor Predicting: 136it [01:28,  1.45it/s]Extractor Predicting: 137it [01:29,  1.48it/s]Extractor Predicting: 138it [01:30,  1.44it/s]Extractor Predicting: 139it [01:30,  1.43it/s]Extractor Predicting: 140it [01:31,  1.43it/s]Extractor Predicting: 141it [01:32,  1.44it/s]Extractor Predicting: 142it [01:33,  1.45it/s]Extractor Predicting: 143it [01:33,  1.44it/s]Extractor Predicting: 144it [01:34,  1.46it/s]Extractor Predicting: 145it [01:35,  1.44it/s]Extractor Predicting: 146it [01:35,  1.45it/s]Extractor Predicting: 147it [01:36,  1.46it/s]Extractor Predicting: 148it [01:37,  1.42it/s]Extractor Predicting: 149it [01:37,  1.46it/s]Extractor Predicting: 149it [01:37,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:01:08,243 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:01:08,249 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:01:08,249 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:01:08,249 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:01:08,249 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:01:08,553 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:01:08,553 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:01:08,819 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:01:09,860 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:01:09,861 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:01:11,144 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:01:11,146 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:01:11,146 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:01:11,146 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:01:11,146 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:01:11,472 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:01:11,478 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:01:11,755 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:01:11,935 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:01:11,935 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.16083916083916083,
  "recall": 0.017083436494181727,
  "score": 0.030886302596239926,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.75it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.68it/s]Extractor Predicting: 9it [00:05,  1.76it/s]Extractor Predicting: 10it [00:05,  1.74it/s]Extractor Predicting: 11it [00:06,  1.73it/s]Extractor Predicting: 12it [00:07,  1.73it/s]Extractor Predicting: 13it [00:07,  1.81it/s]Extractor Predicting: 14it [00:08,  1.79it/s]Extractor Predicting: 15it [00:08,  1.80it/s]Extractor Predicting: 16it [00:09,  1.80it/s]Extractor Predicting: 17it [00:09,  1.79it/s]Extractor Predicting: 18it [00:10,  1.75it/s]Extractor Predicting: 19it [00:11,  1.71it/s]Extractor Predicting: 20it [00:11,  1.72it/s]Extractor Predicting: 21it [00:12,  1.73it/s]Extractor Predicting: 22it [00:12,  1.74it/s]Extractor Predicting: 23it [00:13,  1.77it/s]Extractor Predicting: 24it [00:13,  1.75it/s]Extractor Predicting: 25it [00:14,  1.77it/s]Extractor Predicting: 26it [00:15,  1.73it/s]Extractor Predicting: 27it [00:15,  1.79it/s]Extractor Predicting: 28it [00:16,  1.79it/s]Extractor Predicting: 29it [00:16,  1.78it/s]Extractor Predicting: 30it [00:17,  1.78it/s]Extractor Predicting: 31it [00:17,  1.80it/s]Extractor Predicting: 32it [00:18,  1.80it/s]Extractor Predicting: 33it [00:18,  1.74it/s]Extractor Predicting: 34it [00:19,  1.72it/s]Extractor Predicting: 35it [00:20,  1.69it/s]Extractor Predicting: 36it [00:20,  1.75it/s]Extractor Predicting: 37it [00:21,  1.72it/s]Extractor Predicting: 38it [00:21,  1.71it/s]Extractor Predicting: 39it [00:22,  1.72it/s]Extractor Predicting: 40it [00:23,  1.69it/s]Extractor Predicting: 41it [00:23,  1.67it/s]Extractor Predicting: 42it [00:24,  1.70it/s]Extractor Predicting: 43it [00:24,  1.72it/s]Extractor Predicting: 44it [00:25,  1.72it/s]Extractor Predicting: 45it [00:26,  1.59it/s]Extractor Predicting: 46it [00:26,  1.56it/s]Extractor Predicting: 47it [00:27,  1.59it/s]Extractor Predicting: 48it [00:27,  1.62it/s]Extractor Predicting: 49it [00:28,  1.65it/s]Extractor Predicting: 50it [00:29,  1.66it/s]Extractor Predicting: 51it [00:29,  1.71it/s]Extractor Predicting: 52it [00:30,  1.68it/s]Extractor Predicting: 53it [00:30,  1.69it/s]Extractor Predicting: 54it [00:31,  1.70it/s]Extractor Predicting: 55it [00:32,  1.68it/s]Extractor Predicting: 56it [00:32,  1.63it/s]Extractor Predicting: 57it [00:33,  1.63it/s]Extractor Predicting: 58it [00:33,  1.65it/s]Extractor Predicting: 59it [00:34,  1.63it/s]Extractor Predicting: 60it [00:35,  1.64it/s]Extractor Predicting: 61it [00:35,  1.63it/s]Extractor Predicting: 62it [00:36,  1.67it/s]Extractor Predicting: 63it [00:37,  1.64it/s]Extractor Predicting: 64it [00:37,  1.67it/s]Extractor Predicting: 65it [00:38,  1.70it/s]Extractor Predicting: 66it [00:38,  1.70it/s]Extractor Predicting: 67it [00:39,  1.68it/s]Extractor Predicting: 68it [00:39,  1.68it/s]Extractor Predicting: 69it [00:40,  1.64it/s]Extractor Predicting: 70it [00:41,  1.63it/s]Extractor Predicting: 71it [00:41,  1.62it/s]Extractor Predicting: 72it [00:42,  1.64it/s]Extractor Predicting: 73it [00:43,  1.63it/s]Extractor Predicting: 74it [00:43,  1.64it/s]Extractor Predicting: 75it [00:44,  1.66it/s]Extractor Predicting: 76it [00:44,  1.63it/s]Extractor Predicting: 77it [00:45,  1.64it/s]Extractor Predicting: 78it [00:46,  1.59it/s]Extractor Predicting: 79it [00:46,  1.62it/s]Extractor Predicting: 80it [00:47,  1.66it/s]Extractor Predicting: 81it [00:47,  1.63it/s]Extractor Predicting: 82it [00:48,  1.48it/s]Extractor Predicting: 83it [00:49,  1.51it/s]Extractor Predicting: 84it [00:49,  1.56it/s]Extractor Predicting: 85it [00:50,  1.56it/s]Extractor Predicting: 86it [00:51,  1.56it/s]Extractor Predicting: 87it [00:51,  1.61it/s]Extractor Predicting: 88it [00:52,  1.62it/s]Extractor Predicting: 89it [00:53,  1.67it/s]Extractor Predicting: 90it [00:53,  1.66it/s]Extractor Predicting: 91it [00:54,  1.66it/s]Extractor Predicting: 92it [00:54,  1.66it/s]Extractor Predicting: 93it [00:55,  1.69it/s]Extractor Predicting: 94it [00:55,  1.69it/s]Extractor Predicting: 95it [00:56,  1.68it/s]Extractor Predicting: 96it [00:57,  1.71it/s]Extractor Predicting: 97it [00:57,  1.74it/s]Extractor Predicting: 98it [00:58,  1.71it/s]Extractor Predicting: 99it [00:58,  1.69it/s]Extractor Predicting: 100it [00:59,  1.72it/s]Extractor Predicting: 101it [01:00,  1.70it/s]Extractor Predicting: 102it [01:00,  1.66it/s]Extractor Predicting: 103it [01:01,  1.64it/s]Extractor Predicting: 104it [01:01,  1.69it/s]Extractor Predicting: 105it [01:02,  1.63it/s]Extractor Predicting: 106it [01:03,  1.66it/s]Extractor Predicting: 107it [01:03,  1.67it/s]Extractor Predicting: 108it [01:04,  1.63it/s]Extractor Predicting: 109it [01:04,  1.62it/s]Extractor Predicting: 110it [01:05,  1.65it/s]Extractor Predicting: 111it [01:06,  1.64it/s]Extractor Predicting: 112it [01:06,  1.64it/s]Extractor Predicting: 113it [01:07,  1.67it/s]Extractor Predicting: 114it [01:07,  1.67it/s]Extractor Predicting: 115it [01:08,  1.67it/s]Extractor Predicting: 116it [01:09,  1.70it/s]Extractor Predicting: 117it [01:09,  1.71it/s]Extractor Predicting: 118it [01:10,  1.75it/s]Extractor Predicting: 119it [01:10,  1.76it/s]Extractor Predicting: 120it [01:11,  1.75it/s]Extractor Predicting: 121it [01:11,  1.75it/s]Extractor Predicting: 122it [01:12,  1.74it/s]Extractor Predicting: 123it [01:13,  1.71it/s]Extractor Predicting: 124it [01:13,  1.73it/s]Extractor Predicting: 125it [01:14,  1.71it/s]Extractor Predicting: 126it [01:14,  1.70it/s]Extractor Predicting: 127it [01:15,  1.67it/s]Extractor Predicting: 128it [01:16,  1.64it/s]Extractor Predicting: 129it [01:16,  1.62it/s]Extractor Predicting: 130it [01:17,  1.60it/s]Extractor Predicting: 131it [01:18,  1.58it/s]Extractor Predicting: 132it [01:18,  1.59it/s]Extractor Predicting: 133it [01:19,  1.60it/s]Extractor Predicting: 134it [01:19,  1.64it/s]Extractor Predicting: 135it [01:20,  1.58it/s]Extractor Predicting: 136it [01:21,  1.58it/s]Extractor Predicting: 137it [01:21,  1.56it/s]Extractor Predicting: 138it [01:22,  1.58it/s]Extractor Predicting: 139it [01:23,  1.59it/s]Extractor Predicting: 140it [01:23,  1.55it/s]Extractor Predicting: 141it [01:24,  1.56it/s]Extractor Predicting: 142it [01:25,  1.56it/s]Extractor Predicting: 143it [01:25,  1.59it/s]Extractor Predicting: 144it [01:26,  1.58it/s]Extractor Predicting: 145it [01:26,  1.57it/s]Extractor Predicting: 146it [01:27,  1.57it/s]Extractor Predicting: 147it [01:28,  1.57it/s]Extractor Predicting: 148it [01:28,  1.59it/s]Extractor Predicting: 149it [01:29,  1.62it/s]Extractor Predicting: 150it [01:30,  1.61it/s]Extractor Predicting: 151it [01:30,  1.59it/s]Extractor Predicting: 152it [01:31,  1.59it/s]Extractor Predicting: 153it [01:32,  1.57it/s]Extractor Predicting: 154it [01:32,  1.57it/s]Extractor Predicting: 155it [01:33,  1.58it/s]Extractor Predicting: 156it [01:33,  1.63it/s]Extractor Predicting: 157it [01:34,  1.69it/s]Extractor Predicting: 158it [01:34,  1.70it/s]Extractor Predicting: 159it [01:35,  1.70it/s]Extractor Predicting: 160it [01:36,  1.71it/s]Extractor Predicting: 161it [01:36,  1.75it/s]Extractor Predicting: 162it [01:37,  1.70it/s]Extractor Predicting: 163it [01:37,  1.68it/s]Extractor Predicting: 164it [01:38,  1.70it/s]Extractor Predicting: 165it [01:39,  1.73it/s]Extractor Predicting: 166it [01:39,  1.73it/s]Extractor Predicting: 167it [01:40,  1.79it/s]Extractor Predicting: 168it [01:40,  1.80it/s]Extractor Predicting: 169it [01:41,  1.83it/s]Extractor Predicting: 170it [01:41,  1.77it/s]Extractor Predicting: 171it [01:42,  1.70it/s]Extractor Predicting: 172it [01:43,  1.67it/s]Extractor Predicting: 173it [01:43,  1.71it/s]Extractor Predicting: 173it [01:43,  1.67it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:03:03,299 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:03:03,304 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:03:03,304 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:03:03,304 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:03:03,304 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:03:03,955 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:03:03,956 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:03:04,533 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:03:05,570 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:03:05,571 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:03:08,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:03:08,385 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:03:08,385 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:03:08,385 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:03:08,385 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:03:09,022 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:03:09,023 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:03:09,610 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:03:09,783 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:03:09,783 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.29518072289156627,
  "recall": 0.05909310178485287,
  "score": 0.09847266881028939,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.80it/s]Extractor Predicting: 2it [00:01,  1.70it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.47it/s]Extractor Predicting: 9it [00:05,  1.47it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.46it/s]Extractor Predicting: 12it [00:08,  1.42it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.56it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:10,  1.73it/s]Extractor Predicting: 17it [00:10,  1.82it/s]Extractor Predicting: 18it [00:11,  1.86it/s]Extractor Predicting: 19it [00:11,  1.86it/s]Extractor Predicting: 20it [00:12,  1.89it/s]Extractor Predicting: 21it [00:12,  1.93it/s]Extractor Predicting: 22it [00:13,  1.89it/s]Extractor Predicting: 23it [00:13,  1.89it/s]Extractor Predicting: 24it [00:14,  1.88it/s]Extractor Predicting: 25it [00:14,  1.88it/s]Extractor Predicting: 26it [00:15,  1.92it/s]Extractor Predicting: 27it [00:15,  1.88it/s]Extractor Predicting: 28it [00:16,  1.90it/s]Extractor Predicting: 29it [00:16,  1.90it/s]Extractor Predicting: 30it [00:17,  1.94it/s]Extractor Predicting: 31it [00:17,  1.95it/s]Extractor Predicting: 32it [00:18,  1.94it/s]Extractor Predicting: 33it [00:19,  1.95it/s]Extractor Predicting: 34it [00:19,  1.94it/s]Extractor Predicting: 35it [00:20,  1.95it/s]Extractor Predicting: 36it [00:20,  1.89it/s]Extractor Predicting: 37it [00:21,  1.85it/s]Extractor Predicting: 38it [00:21,  1.91it/s]Extractor Predicting: 39it [00:22,  1.91it/s]Extractor Predicting: 40it [00:22,  1.92it/s]Extractor Predicting: 41it [00:23,  1.91it/s]Extractor Predicting: 42it [00:23,  1.94it/s]Extractor Predicting: 43it [00:24,  1.90it/s]Extractor Predicting: 44it [00:24,  1.74it/s]Extractor Predicting: 45it [00:25,  1.65it/s]Extractor Predicting: 46it [00:26,  1.58it/s]Extractor Predicting: 47it [00:27,  1.55it/s]Extractor Predicting: 48it [00:27,  1.52it/s]Extractor Predicting: 49it [00:28,  1.53it/s]Extractor Predicting: 50it [00:29,  1.50it/s]Extractor Predicting: 51it [00:29,  1.49it/s]Extractor Predicting: 52it [00:30,  1.48it/s]Extractor Predicting: 53it [00:31,  1.47it/s]Extractor Predicting: 54it [00:31,  1.47it/s]Extractor Predicting: 55it [00:32,  1.49it/s]Extractor Predicting: 56it [00:33,  1.38it/s]Extractor Predicting: 57it [00:33,  1.40it/s]Extractor Predicting: 58it [00:34,  1.42it/s]Extractor Predicting: 59it [00:34,  1.69it/s]Extractor Predicting: 59it [00:34,  1.69it/s]
[INFO|configuration_utils.py:515] 2023-08-28 12:03:45,656 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:03:45,657 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 12:03:45,661 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:03:45,662 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 12:03:45,664 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 12:03:48,778 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 12:03:48,782 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 12:03:48,805 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:03:48,806 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 12:03:48,811 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:03:48,817 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:03:48,817 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:03:48,817 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:03:48,817 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:03:48,817 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:03:48,817 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.8442367601246106,
  "recall": 0.17523440025864856,
  "score": 0.2902275769745649,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 12:03:49,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:03:49,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:03:50,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:03:50,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:03:51,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:03:52,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:03:52,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:03:53,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:03:54,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:03:54,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:03:55,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:03:56,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:03:57,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:03:57,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:03:58,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:03:59,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:03:59,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:00,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:00,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:01,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:02,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:02,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:14<02:09, 14.37s/it][WARNING|generation_utils.py:914] 2023-08-28 12:04:03,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:04,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:04,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:05,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:05,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:06,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:07,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:07,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:08,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:08,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:09,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:10,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:10,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:11,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:11,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:12,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:13,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:13,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:14,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:15,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:15,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:16,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:16,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:17,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:28<01:54, 14.30s/it][WARNING|generation_utils.py:914] 2023-08-28 12:04:17,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:18,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:18,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:19,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:20,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:20,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:21,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:22,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:22,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:23,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:24,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:24,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:25,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:25,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:26,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:27,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:28,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:28,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:29,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:30,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:30,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:42<01:38, 14.06s/it][WARNING|generation_utils.py:914] 2023-08-28 12:04:31,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:32,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:32,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:33,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:34,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:34,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:35,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:36,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:36,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:37,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:38,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:39,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:39,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:40,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:41,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:41,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:42,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:42,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:43,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:44,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:45,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:56<01:25, 14.27s/it][WARNING|generation_utils.py:914] 2023-08-28 12:04:46,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:46,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:47,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:47,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:48,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:49,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:49,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:50,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:51,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:51,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:52,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:52,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:53,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:54,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:54,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:55,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:55,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:56,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:57,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:57,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:58,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:10<01:09, 13.85s/it][WARNING|generation_utils.py:914] 2023-08-28 12:04:59,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:04:59,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:00,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:01,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:01,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:02,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:02,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:03,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:04,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:04,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:05,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:05,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:06,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:07,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:07,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:08,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:09,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:09,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:10,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:10,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:11,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:12,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:12,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:13,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:24<00:56, 14.18s/it][WARNING|generation_utils.py:914] 2023-08-28 12:05:13,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:14,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:15,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:15,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:16,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:16,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:17,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:18,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:18,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:19,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:19,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:20,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:20,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:21,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:22,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:22,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:23,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:23,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:24,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:24,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:25,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:26,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:38<00:41, 13.85s/it][WARNING|generation_utils.py:914] 2023-08-28 12:05:27,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:27,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:28,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:28,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:29,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:29,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:30,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:30,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:31,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:32,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:32,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:33,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:33,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:34,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:35,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:35,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:36,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:36,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:37,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:37,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:38,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:49<00:26, 13.15s/it][WARNING|generation_utils.py:914] 2023-08-28 12:05:38,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:39,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:40,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:41,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:41,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:42,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:42,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:43,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:43,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:44,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:44,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:45,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:46,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:46,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:47,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:47,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:48,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:48,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:49,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:50,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:50,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:02<00:12, 12.95s/it][WARNING|generation_utils.py:914] 2023-08-28 12:05:51,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:51,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:52,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:53,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:53,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:54,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:54,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:55,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:55,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:56,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:57,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:57,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:58,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:58,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:05:59,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:06:00,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:06:00,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:06:01,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:06:01,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:06:02,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:06:03,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:06:03,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:15<00:00, 13.00s/it]Generating: 100%|██████████| 10/10 [02:15<00:00, 13.54s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:06:08,820 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:06:08,824 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:06:08,824 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:06:08,824 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:06:08,824 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:06:09,130 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:06:09,131 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:06:09,399 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:06:10,482 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:06:10,482 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:06:13,324 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:06:13,326 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:06:13,326 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:06:13,327 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:06:13,327 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:06:13,953 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:06:13,954 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:06:14,543 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:06:14,717 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:06:14,717 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : given name .', 'success_rate': 0.8693181818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 419, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 473, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 554, 'raw': 704}
{'target': 600, 'success': 579, 'raw': 736}
{'target': 600, 'success': 607, 'raw': 768}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.7903645833333334, 'errors': {'', "('Estonia', 'languages spoken, written or signed', '', 'It was adopted as a standard in Estonia , Croatia and Turkey , during the Soviet era .')", "('Democratic Republic of the Congo', 'languages spoken, written or signed', '', 'They are an official of the Democratic Republic of the Congo .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.9092261904761905, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : mother .', 'success_rate': 0.8958333333333334, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : mouth of the watercourse . Context : Later in the year ( 1141 ) , he purchased from Charles of Saxony at Tours the river Thames between Oxford and Oxfordshire ; in the autumn , he sold to the Lord Mayor of Leicestershire , Margaret of Salford . Head Entity : Oxford , Tail Entity : the Thames .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9122023809523809, 'errors': {''}}
['Relation : genre . Context : The song reached number one on the RPM RPM Singles & Tracks chart in 2002 at number 17 on the Billboard Japanese Top Singles chart , and number 5 on the Billboard Japanese Bodegas Japan Singles & Tracks chart . Head Entity : Billboard Japan Top Singles , Tail Entity : Japanese music .\n']
['Relation : genre . Context : The song reached number one on the RPM RPM Singles & Tracks chart in 2002 at number 17 on the Billboard Japanese Top Singles chart , and number 5 on the Billboard Japanese Bodegas Japan Singles & Tracks chart . Head Entity : Billboard Japan Top Singles , Tail Entity : Japanese music .\n', 'Relation : genre . Context : Eileen McClelland s song , One Step Beyond , was the first single from South Park to feature one or two characters who are more or less like characters from another series . Head Entity : South Park , Tail Entity : video game .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 462, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 575, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 627, 'raw': 768}
{'prompt': 'Relation : genre .', 'success_rate': 0.81640625, 'errors': {'', "('Google Play', 'genre', '', 'It was released on February 26 , 2014 on the Apple Music Store in the Google Play store , for iOS devices .')", "('Love Me', 'genre', '', 'Love Me is a 1985 American drama film directed by David S. Goyer and written by George C. Stuck , starring Will McComb and Catherine Zeta Dix .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.8579545454545454, 'errors': {'', "('database', 'is a list of', '', 'It is a collaborative database of .')", "('list', 'is a list of', '', 'It is a list of .')", 'not enough values to unpack (expected 2, got 1)', "('operator', 'is a list of', '', 'In programming languages , it is represented by the operators , followed by , and operators , followed by , and operators , respectively .')", 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.9270833333333334, 'errors': {''}}
['Relation : manufacturer . Context : The ZX Spectrum ( ZX Spectrum ) is a compact integrated digital audio system manufactured by the Hewlett Packard Company for use by the Hewlett Packard Company , Inc. ( HP ) microprocessor manufacturing division . Head Entity : Hewlett Packard Company , Tail Entity : Hewlett Packard Corporation .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9330357142857143, 'errors': {''}}
['Relation : member of . Context : Later in the year he came to Westminster , where he represented the Conservative Party of Scotland . Head Entity : George Osborne , Tail Entity : Conservative Party .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : member of .', 'success_rate': 0.8821022727272727, 'errors': {'', "('Martin Schubert', 'member of', '', 'During the 1930s and 1940s he was a regular contributor to a list of influential scholars , including Herbert Marcuse , Martin Schubert and Louis Lerman .')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/3_ext.jsonl'}}
estimate vocab size: 8833
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8933, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.50it/s]Extractor Estimating: 2it [00:01,  1.51it/s]Extractor Estimating: 3it [00:02,  1.46it/s]Extractor Estimating: 4it [00:02,  1.49it/s]Extractor Estimating: 5it [00:03,  1.54it/s]Extractor Estimating: 6it [00:03,  1.57it/s]Extractor Estimating: 7it [00:04,  1.53it/s]Extractor Estimating: 8it [00:05,  1.52it/s]Extractor Estimating: 9it [00:05,  1.54it/s]Extractor Estimating: 10it [00:06,  1.54it/s]Extractor Estimating: 11it [00:07,  1.57it/s]Extractor Estimating: 12it [00:07,  1.52it/s]Extractor Estimating: 13it [00:08,  1.57it/s]Extractor Estimating: 14it [00:09,  1.55it/s]Extractor Estimating: 15it [00:09,  1.57it/s]Extractor Estimating: 16it [00:10,  1.58it/s]Extractor Estimating: 17it [00:11,  1.57it/s]Extractor Estimating: 18it [00:11,  1.56it/s]Extractor Estimating: 19it [00:12,  1.60it/s]Extractor Estimating: 20it [00:12,  1.59it/s]Extractor Estimating: 21it [00:13,  1.56it/s]Extractor Estimating: 22it [00:14,  1.57it/s]Extractor Estimating: 23it [00:14,  1.56it/s]Extractor Estimating: 24it [00:15,  1.56it/s]Extractor Estimating: 25it [00:16,  1.61it/s]Extractor Estimating: 26it [00:16,  1.67it/s]Extractor Estimating: 27it [00:17,  1.68it/s]Extractor Estimating: 28it [00:17,  1.54it/s]Extractor Estimating: 29it [00:18,  1.57it/s]Extractor Estimating: 30it [00:19,  1.64it/s]Extractor Estimating: 31it [00:19,  1.64it/s]Extractor Estimating: 32it [00:20,  1.66it/s]Extractor Estimating: 33it [00:20,  1.68it/s]Extractor Estimating: 34it [00:21,  1.68it/s]Extractor Estimating: 35it [00:22,  1.62it/s]Extractor Estimating: 36it [00:22,  1.63it/s]Extractor Estimating: 37it [00:23,  1.67it/s]Extractor Estimating: 38it [00:23,  1.66it/s]Extractor Estimating: 39it [00:24,  1.65it/s]Extractor Estimating: 40it [00:25,  1.68it/s]Extractor Estimating: 41it [00:25,  1.66it/s]Extractor Estimating: 42it [00:26,  1.71it/s]Extractor Estimating: 43it [00:27,  1.57it/s]Extractor Estimating: 44it [00:27,  1.66it/s]Extractor Estimating: 45it [00:28,  1.68it/s]Extractor Estimating: 46it [00:28,  1.66it/s]Extractor Estimating: 47it [00:29,  1.72it/s]Extractor Estimating: 48it [00:29,  1.74it/s]Extractor Estimating: 49it [00:30,  1.80it/s]Extractor Estimating: 50it [00:30,  1.84it/s]Extractor Estimating: 51it [00:31,  1.70it/s]Extractor Estimating: 52it [00:32,  1.65it/s]Extractor Estimating: 53it [00:32,  1.61it/s]Extractor Estimating: 54it [00:33,  1.63it/s]Extractor Estimating: 55it [00:34,  1.66it/s]Extractor Estimating: 56it [00:34,  1.62it/s]Extractor Estimating: 57it [00:35,  1.55it/s]Extractor Estimating: 58it [00:36,  1.56it/s]Extractor Estimating: 59it [00:36,  1.58it/s]Extractor Estimating: 60it [00:37,  1.58it/s]Extractor Estimating: 61it [00:37,  1.58it/s]Extractor Estimating: 62it [00:38,  1.58it/s]Extractor Estimating: 63it [00:39,  1.57it/s]Extractor Estimating: 64it [00:39,  1.57it/s]Extractor Estimating: 65it [00:40,  1.55it/s]Extractor Estimating: 66it [00:41,  1.55it/s]Extractor Estimating: 67it [00:41,  1.59it/s]Extractor Estimating: 68it [00:42,  1.54it/s]Extractor Estimating: 69it [00:42,  1.61it/s]Extractor Estimating: 70it [00:43,  1.50it/s]Extractor Estimating: 71it [00:44,  1.47it/s]Extractor Estimating: 72it [00:45,  1.48it/s]Extractor Estimating: 73it [00:45,  1.52it/s]Extractor Estimating: 74it [00:46,  1.49it/s]Extractor Estimating: 75it [00:47,  1.53it/s]Extractor Estimating: 76it [00:47,  1.53it/s]Extractor Estimating: 77it [00:48,  1.53it/s]Extractor Estimating: 78it [00:49,  1.51it/s]Extractor Estimating: 79it [00:49,  1.47it/s]Extractor Estimating: 80it [00:50,  1.50it/s]Extractor Estimating: 81it [00:51,  1.51it/s]Extractor Estimating: 82it [00:51,  1.51it/s]Extractor Estimating: 83it [00:52,  1.47it/s]Extractor Estimating: 84it [00:53,  1.43it/s]Extractor Estimating: 85it [00:54,  1.35it/s]Extractor Estimating: 86it [00:54,  1.34it/s]Extractor Estimating: 87it [00:55,  1.35it/s]Extractor Estimating: 88it [00:56,  1.37it/s]Extractor Estimating: 89it [00:56,  1.35it/s]Extractor Estimating: 90it [00:57,  1.43it/s]Extractor Estimating: 91it [00:58,  1.47it/s]Extractor Estimating: 92it [00:58,  1.43it/s]Extractor Estimating: 93it [00:59,  1.47it/s]Extractor Estimating: 94it [01:00,  1.46it/s]Extractor Estimating: 95it [01:00,  1.52it/s]Extractor Estimating: 96it [01:01,  1.53it/s]Extractor Estimating: 97it [01:02,  1.49it/s]Extractor Estimating: 98it [01:02,  1.45it/s]Extractor Estimating: 99it [01:03,  1.44it/s]Extractor Estimating: 100it [01:04,  1.41it/s]Extractor Estimating: 101it [01:05,  1.48it/s]Extractor Estimating: 102it [01:05,  1.54it/s]Extractor Estimating: 103it [01:06,  1.56it/s]Extractor Estimating: 104it [01:06,  1.56it/s]Extractor Estimating: 105it [01:07,  1.57it/s]Extractor Estimating: 106it [01:08,  1.58it/s]Extractor Estimating: 107it [01:08,  1.63it/s]Extractor Estimating: 108it [01:09,  1.70it/s]Extractor Estimating: 109it [01:09,  1.70it/s]Extractor Estimating: 110it [01:10,  1.69it/s]Extractor Estimating: 111it [01:11,  1.68it/s]Extractor Estimating: 112it [01:11,  1.67it/s]Extractor Estimating: 113it [01:12,  1.65it/s]Extractor Estimating: 114it [01:12,  1.67it/s]Extractor Estimating: 115it [01:13,  1.65it/s]Extractor Estimating: 116it [01:14,  1.66it/s]Extractor Estimating: 117it [01:14,  1.60it/s]Extractor Estimating: 118it [01:15,  1.64it/s]Extractor Estimating: 119it [01:15,  1.62it/s]Extractor Estimating: 120it [01:16,  1.64it/s]Extractor Estimating: 121it [01:17,  1.69it/s]Extractor Estimating: 122it [01:17,  1.65it/s]Extractor Estimating: 123it [01:18,  1.65it/s]Extractor Estimating: 124it [01:18,  1.64it/s]Extractor Estimating: 125it [01:19,  1.63it/s]Extractor Estimating: 126it [01:20,  1.63it/s]Extractor Estimating: 127it [01:20,  1.64it/s]Extractor Estimating: 128it [01:21,  1.61it/s]Extractor Estimating: 129it [01:22,  1.62it/s]Extractor Estimating: 130it [01:22,  1.68it/s]Extractor Estimating: 131it [01:23,  1.63it/s]Extractor Estimating: 132it [01:23,  1.57it/s]Extractor Estimating: 133it [01:24,  1.60it/s]Extractor Estimating: 134it [01:25,  1.56it/s]Extractor Estimating: 135it [01:25,  1.60it/s]Extractor Estimating: 136it [01:26,  1.59it/s]Extractor Estimating: 137it [01:27,  1.59it/s]Extractor Estimating: 138it [01:27,  1.58it/s]Extractor Estimating: 139it [01:28,  1.59it/s]Extractor Estimating: 140it [01:28,  1.56it/s]Extractor Estimating: 141it [01:29,  1.53it/s]Extractor Estimating: 142it [01:30,  1.54it/s]Extractor Estimating: 143it [01:30,  1.60it/s]Extractor Estimating: 144it [01:31,  1.57it/s]Extractor Estimating: 145it [01:32,  1.59it/s]Extractor Estimating: 146it [01:32,  1.54it/s]Extractor Estimating: 147it [01:33,  1.52it/s]Extractor Estimating: 148it [01:34,  1.46it/s]Extractor Estimating: 149it [01:34,  1.52it/s]Extractor Estimating: 150it [01:35,  1.55it/s]Extractor Estimating: 151it [01:36,  1.60it/s]Extractor Estimating: 152it [01:36,  1.67it/s]Extractor Estimating: 153it [01:37,  1.69it/s]Extractor Estimating: 154it [01:37,  1.71it/s]Extractor Estimating: 155it [01:38,  1.75it/s]Extractor Estimating: 156it [01:38,  1.82it/s]Extractor Estimating: 157it [01:39,  1.82it/s]Extractor Estimating: 158it [01:39,  1.76it/s]Extractor Estimating: 159it [01:40,  1.78it/s]Extractor Estimating: 160it [01:41,  1.73it/s]Extractor Estimating: 161it [01:41,  1.79it/s]Extractor Estimating: 162it [01:42,  1.80it/s]Extractor Estimating: 163it [01:42,  1.78it/s]Extractor Estimating: 164it [01:43,  1.83it/s]Extractor Estimating: 165it [01:43,  1.75it/s]Extractor Estimating: 166it [01:44,  1.81it/s]Extractor Estimating: 167it [01:44,  1.84it/s]Extractor Estimating: 168it [01:45,  1.83it/s]Extractor Estimating: 169it [01:46,  1.83it/s]Extractor Estimating: 170it [01:46,  1.84it/s]Extractor Estimating: 171it [01:47,  1.86it/s]Extractor Estimating: 172it [01:47,  1.84it/s]Extractor Estimating: 173it [01:48,  1.92it/s]Extractor Estimating: 174it [01:48,  1.79it/s]Extractor Estimating: 175it [01:49,  1.75it/s]Extractor Estimating: 176it [01:49,  1.84it/s]Extractor Estimating: 177it [01:50,  1.83it/s]Extractor Estimating: 178it [01:50,  1.79it/s]Extractor Estimating: 179it [01:51,  1.73it/s]Extractor Estimating: 180it [01:52,  1.74it/s]Extractor Estimating: 181it [01:52,  1.69it/s]Extractor Estimating: 182it [01:53,  1.69it/s]Extractor Estimating: 183it [01:54,  1.66it/s]Extractor Estimating: 184it [01:54,  1.67it/s]Extractor Estimating: 185it [01:55,  1.64it/s]Extractor Estimating: 186it [01:55,  1.67it/s]Extractor Estimating: 187it [01:56,  1.72it/s]Extractor Estimating: 188it [01:56,  1.73it/s]Extractor Estimating: 189it [01:57,  1.74it/s]Extractor Estimating: 190it [01:58,  1.67it/s]Extractor Estimating: 191it [01:58,  1.64it/s]Extractor Estimating: 192it [01:59,  1.67it/s]Extractor Estimating: 193it [01:59,  1.75it/s]Extractor Estimating: 194it [02:00,  1.71it/s]Extractor Estimating: 195it [02:01,  1.71it/s]Extractor Estimating: 196it [02:01,  1.71it/s]Extractor Estimating: 197it [02:02,  1.80it/s]Extractor Estimating: 198it [02:02,  1.76it/s]Extractor Estimating: 199it [02:03,  1.72it/s]Extractor Estimating: 200it [02:03,  1.75it/s]Extractor Estimating: 201it [02:04,  1.68it/s]Extractor Estimating: 202it [02:05,  1.70it/s]Extractor Estimating: 203it [02:05,  1.56it/s]Extractor Estimating: 204it [02:06,  1.60it/s]Extractor Estimating: 205it [02:07,  1.63it/s]Extractor Estimating: 206it [02:07,  1.70it/s]Extractor Estimating: 207it [02:08,  1.70it/s]Extractor Estimating: 208it [02:08,  1.67it/s]Extractor Estimating: 209it [02:09,  1.70it/s]Extractor Estimating: 210it [02:09,  1.76it/s]Extractor Estimating: 211it [02:10,  1.78it/s]Extractor Estimating: 212it [02:11,  1.75it/s]Extractor Estimating: 213it [02:11,  1.72it/s]Extractor Estimating: 214it [02:12,  1.76it/s]Extractor Estimating: 215it [02:12,  1.80it/s]Extractor Estimating: 216it [02:13,  1.73it/s]Extractor Estimating: 217it [02:13,  1.76it/s]Extractor Estimating: 218it [02:14,  1.74it/s]Extractor Estimating: 219it [02:15,  1.70it/s]Extractor Estimating: 220it [02:15,  1.70it/s]Extractor Estimating: 221it [02:16,  1.72it/s]Extractor Estimating: 222it [02:16,  1.68it/s]Extractor Estimating: 223it [02:17,  1.73it/s]Extractor Estimating: 224it [02:17,  1.74it/s]Extractor Estimating: 225it [02:18,  1.71it/s]Extractor Estimating: 226it [02:19,  1.70it/s]Extractor Estimating: 227it [02:19,  1.73it/s]Extractor Estimating: 228it [02:20,  1.56it/s]Extractor Estimating: 229it [02:21,  1.64it/s]Extractor Estimating: 230it [02:21,  1.65it/s]Extractor Estimating: 231it [02:22,  1.66it/s]Extractor Estimating: 232it [02:22,  1.66it/s]Extractor Estimating: 233it [02:23,  1.67it/s]Extractor Estimating: 234it [02:23,  1.70it/s]Extractor Estimating: 235it [02:24,  1.67it/s]Extractor Estimating: 236it [02:25,  1.64it/s]Extractor Estimating: 237it [02:25,  1.66it/s]Extractor Estimating: 238it [02:26,  1.66it/s]Extractor Estimating: 239it [02:27,  1.67it/s]Extractor Estimating: 240it [02:27,  1.59it/s]Extractor Estimating: 241it [02:28,  1.49it/s]Extractor Estimating: 242it [02:29,  1.54it/s]Extractor Estimating: 243it [02:29,  1.58it/s]Extractor Estimating: 244it [02:30,  1.63it/s]Extractor Estimating: 245it [02:30,  1.61it/s]Extractor Estimating: 246it [02:31,  1.60it/s]Extractor Estimating: 247it [02:32,  1.63it/s]Extractor Estimating: 248it [02:32,  1.66it/s]Extractor Estimating: 249it [02:33,  1.57it/s]Extractor Estimating: 250it [02:33,  1.63it/s]Extractor Estimating: 250it [02:33,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:09:05,341 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:09:05,345 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:09:05,345 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:09:05,345 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:09:05,345 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:09:05,951 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:09:05,953 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:09:06,525 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:09:07,555 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:09:07,555 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:09:10,376 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:09:10,380 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:09:10,380 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:09:10,380 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:09:10,380 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:09:11,018 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:09:11,020 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:09:11,599 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:09:11,763 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:09:11,763 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 13:38:38,905 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 13:38:38,932 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 4998 mean pseudo reward: 0.9231088668380989
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 18976
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19076, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=19076, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.018, loss:610.1751
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.030, loss:568.7053
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 1.019, loss:549.8322
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 1.032, loss:548.0118
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 1.021, loss:533.5462
>> valid entity prec:0.5178, rec:0.5112, f1:0.5145
>> valid relation prec:0.1307, rec:0.0178, f1:0.0314
>> valid relation with NER prec:0.1307, rec:0.0178, f1:0.0314
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 2.369, loss:525.4222
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 1.019, loss:512.7222
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 1.038, loss:523.3232
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 1.036, loss:503.6827
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 1.031, loss:527.3418
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5222, rec:0.4105, f1:0.4596
>> valid relation prec:0.1653, rec:0.0099, f1:0.0187
>> valid relation with NER prec:0.1653, rec:0.0099, f1:0.0187
g_step 1100, step 55, avg_time 2.356, loss:510.9578
g_step 1200, step 155, avg_time 1.030, loss:518.6821
g_step 1300, step 46, avg_time 1.039, loss:483.1056
g_step 1400, step 146, avg_time 1.038, loss:490.3643
g_step 1500, step 37, avg_time 1.017, loss:482.3826
>> valid entity prec:0.5350, rec:0.4529, f1:0.4905
>> valid relation prec:0.1459, rec:0.0288, f1:0.0480
>> valid relation with NER prec:0.1459, rec:0.0288, f1:0.0480
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 137, avg_time 2.371, loss:470.2194
g_step 1700, step 28, avg_time 1.026, loss:456.3400
g_step 1800, step 128, avg_time 1.028, loss:460.1057
g_step 1900, step 19, avg_time 1.025, loss:468.6225
g_step 2000, step 119, avg_time 1.030, loss:433.1042
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4925, rec:0.4749, f1:0.4835
>> valid relation prec:0.1073, rec:0.0231, f1:0.0380
>> valid relation with NER prec:0.1073, rec:0.0231, f1:0.0380
g_step 2100, step 10, avg_time 2.374, loss:453.1308
g_step 2200, step 110, avg_time 1.023, loss:416.8551
g_step 2300, step 1, avg_time 1.025, loss:421.2608
g_step 2400, step 101, avg_time 1.033, loss:406.9522
g_step 2500, step 201, avg_time 1.027, loss:432.0247
>> valid entity prec:0.5022, rec:0.3975, f1:0.4437
>> valid relation prec:0.1337, rec:0.0166, f1:0.0295
>> valid relation with NER prec:0.1337, rec:0.0166, f1:0.0295
g_step 2600, step 92, avg_time 2.351, loss:378.9102
g_step 2700, step 192, avg_time 1.037, loss:405.5667
g_step 2800, step 83, avg_time 1.024, loss:368.8262
g_step 2900, step 183, avg_time 1.031, loss:382.0045
g_step 3000, step 74, avg_time 1.028, loss:383.7961
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5091, rec:0.3856, f1:0.4388
>> valid relation prec:0.1324, rec:0.0255, f1:0.0428
>> valid relation with NER prec:0.1324, rec:0.0255, f1:0.0428
g_step 3100, step 174, avg_time 2.353, loss:357.0345
g_step 3200, step 65, avg_time 1.024, loss:353.1880
g_step 3300, step 165, avg_time 1.032, loss:357.7333
g_step 3400, step 56, avg_time 1.020, loss:334.1714
g_step 3500, step 156, avg_time 1.034, loss:347.7506
>> valid entity prec:0.5052, rec:0.4933, f1:0.4992
>> valid relation prec:0.1310, rec:0.0285, f1:0.0468
>> valid relation with NER prec:0.1310, rec:0.0285, f1:0.0468
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 47, avg_time 2.364, loss:361.0549
g_step 3700, step 147, avg_time 1.035, loss:336.5983
g_step 3800, step 38, avg_time 1.024, loss:329.6712
g_step 3900, step 138, avg_time 1.032, loss:343.4428
g_step 4000, step 29, avg_time 1.020, loss:322.3252
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5321, rec:0.3706, f1:0.4369
>> valid relation prec:0.1351, rec:0.0211, f1:0.0365
>> valid relation with NER prec:0.1351, rec:0.0211, f1:0.0365
g_step 4100, step 129, avg_time 2.360, loss:323.5811
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 13:38:38 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 13:38:38 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_13-38-38_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 13:38:39 - WARNING - datasets.builder -   Using custom data configuration default-12f0d986529c3974
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-12f0d986529c3974/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 13:38:40,210 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:38:40,211 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:38:40,212 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:38:40,213 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:38:40,220 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:38:40,223 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:38:40,223 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:38:40,223 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:38:40,223 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:38:40,223 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:38:40,223 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 13:38:40,370 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:38:43,518 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 13:38:43,521 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-12f0d986529c3974/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.22ba/s] 40%|████      | 2/5 [00:00<00:00,  3.13ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.75ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.14ba/s]100%|██████████| 5/5 [00:01<00:00,  4.35ba/s]100%|██████████| 5/5 [00:01<00:00,  3.99ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.02ba/s] 40%|████      | 2/5 [00:00<00:00,  4.25ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.30ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.35ba/s]100%|██████████| 5/5 [00:00<00:00,  5.29ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.97ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.14ba/s]100%|██████████| 5/5 [00:00<00:00, 10.55ba/s]100%|██████████| 5/5 [00:00<00:00, 10.37ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.03ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.29ba/s]100%|██████████| 5/5 [00:00<00:00, 13.36ba/s]100%|██████████| 5/5 [00:00<00:00, 12.40ba/s]
[INFO|trainer.py:414] 2023-08-28 13:38:46,963 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 13:38:46,975 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 13:38:46,975 >>   Num examples = 4999
[INFO|trainer.py:1149] 2023-08-28 13:38:46,975 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 13:38:46,975 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 13:38:46,975 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 13:38:46,975 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 13:38:46,975 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:57,  3.32it/s]  1%|          | 2/390 [00:00<01:54,  3.39it/s]  1%|          | 3/390 [00:00<01:53,  3.42it/s]  1%|          | 4/390 [00:01<01:52,  3.43it/s]  1%|▏         | 5/390 [00:01<01:52,  3.43it/s]  2%|▏         | 6/390 [00:01<01:51,  3.44it/s]  2%|▏         | 7/390 [00:02<01:51,  3.44it/s]  2%|▏         | 8/390 [00:02<01:51,  3.44it/s]  2%|▏         | 9/390 [00:02<01:50,  3.44it/s]  3%|▎         | 10/390 [00:02<01:50,  3.44it/s]  3%|▎         | 11/390 [00:03<01:50,  3.42it/s]  3%|▎         | 12/390 [00:03<01:50,  3.43it/s]  3%|▎         | 13/390 [00:03<01:49,  3.43it/s]  4%|▎         | 14/390 [00:04<01:49,  3.44it/s]  4%|▍         | 15/390 [00:04<01:49,  3.44it/s]  4%|▍         | 16/390 [00:04<01:48,  3.44it/s]  4%|▍         | 17/390 [00:04<01:48,  3.44it/s]  5%|▍         | 18/390 [00:05<01:48,  3.44it/s]  5%|▍         | 19/390 [00:05<01:47,  3.44it/s]  5%|▌         | 20/390 [00:05<01:47,  3.44it/s]  5%|▌         | 21/390 [00:06<01:47,  3.44it/s]  6%|▌         | 22/390 [00:06<01:47,  3.44it/s]  6%|▌         | 23/390 [00:06<01:46,  3.44it/s]  6%|▌         | 24/390 [00:06<01:46,  3.44it/s]  6%|▋         | 25/390 [00:07<01:46,  3.44it/s]  7%|▋         | 26/390 [00:07<01:45,  3.44it/s]  7%|▋         | 27/390 [00:07<01:45,  3.45it/s]  7%|▋         | 28/390 [00:08<01:45,  3.44it/s]  7%|▋         | 29/390 [00:08<01:44,  3.45it/s]  8%|▊         | 30/390 [00:08<01:44,  3.44it/s]  8%|▊         | 31/390 [00:09<01:44,  3.44it/s]  8%|▊         | 32/390 [00:09<01:43,  3.44it/s]  8%|▊         | 33/390 [00:09<01:44,  3.43it/s]  9%|▊         | 34/390 [00:09<01:43,  3.43it/s]  9%|▉         | 35/390 [00:10<01:43,  3.43it/s]  9%|▉         | 36/390 [00:10<01:42,  3.44it/s]  9%|▉         | 37/390 [00:10<01:42,  3.44it/s] 10%|▉         | 38/390 [00:11<01:42,  3.44it/s] 10%|█         | 39/390 [00:11<01:42,  3.44it/s] 10%|█         | 40/390 [00:11<01:41,  3.44it/s] 11%|█         | 41/390 [00:11<01:41,  3.44it/s] 11%|█         | 42/390 [00:12<01:41,  3.44it/s] 11%|█         | 43/390 [00:12<01:40,  3.44it/s] 11%|█▏        | 44/390 [00:12<01:40,  3.43it/s] 12%|█▏        | 45/390 [00:13<01:40,  3.44it/s] 12%|█▏        | 46/390 [00:13<01:40,  3.44it/s] 12%|█▏        | 47/390 [00:13<01:39,  3.44it/s] 12%|█▏        | 48/390 [00:13<01:39,  3.44it/s] 13%|█▎        | 49/390 [00:14<01:39,  3.44it/s] 13%|█▎        | 50/390 [00:14<01:38,  3.44it/s] 13%|█▎        | 51/390 [00:14<01:38,  3.44it/s] 13%|█▎        | 52/390 [00:15<01:38,  3.44it/s] 14%|█▎        | 53/390 [00:15<01:37,  3.44it/s] 14%|█▍        | 54/390 [00:15<01:37,  3.44it/s] 14%|█▍        | 55/390 [00:15<01:37,  3.43it/s] 14%|█▍        | 56/390 [00:16<01:37,  3.43it/s] 15%|█▍        | 57/390 [00:16<01:36,  3.43it/s] 15%|█▍        | 58/390 [00:16<01:36,  3.44it/s] 15%|█▌        | 59/390 [00:17<01:36,  3.44it/s] 15%|█▌        | 60/390 [00:17<01:35,  3.44it/s] 16%|█▌        | 61/390 [00:17<01:35,  3.44it/s] 16%|█▌        | 62/390 [00:18<01:35,  3.44it/s] 16%|█▌        | 63/390 [00:18<01:35,  3.44it/s] 16%|█▋        | 64/390 [00:18<01:34,  3.44it/s] 17%|█▋        | 65/390 [00:18<01:34,  3.44it/s] 17%|█▋        | 66/390 [00:19<01:35,  3.41it/s] 17%|█▋        | 67/390 [00:19<01:34,  3.42it/s] 17%|█▋        | 68/390 [00:19<01:33,  3.43it/s] 18%|█▊        | 69/390 [00:20<01:33,  3.43it/s] 18%|█▊        | 70/390 [00:20<01:33,  3.43it/s] 18%|█▊        | 71/390 [00:20<01:32,  3.44it/s] 18%|█▊        | 72/390 [00:20<01:32,  3.44it/s] 19%|█▊        | 73/390 [00:21<01:32,  3.44it/s] 19%|█▉        | 74/390 [00:21<01:31,  3.44it/s] 19%|█▉        | 75/390 [00:21<01:31,  3.44it/s] 19%|█▉        | 76/390 [00:22<01:31,  3.44it/s] 20%|█▉        | 77/390 [00:22<01:31,  3.43it/s] 20%|██        | 78/390 [00:22<01:30,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 13:39:09,717 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:39:09,717 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 13:39:09,717 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.83it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.92it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.07it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.31it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.79it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.56it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.48it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.39it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.44it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.47it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.41it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.25it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.30it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.10it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.12it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.12it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.22it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.31it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.36it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.27it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.31it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.17it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.21it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.20it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.13it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.13it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.27it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.25it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.06it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.25it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.31it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.23it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.15it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.14it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.18it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.28it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.24it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.21it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.23it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.27it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.18it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.21it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.23it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.20it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.21it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.18it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.28it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.24it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.21it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.13it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.19it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.17it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.24it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.06it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 43.98it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.17it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.20it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.12it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.18it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.20it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.11it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.11it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.31it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.17it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.29it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.34it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.23it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.11it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.15it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.18it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.23it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.14it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.15it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.33it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.27it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.22it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.10it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.15it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.23it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.15it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.19it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.15it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.24it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.14it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.20it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.11it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.16it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.16it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.21it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.14it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.19it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.22it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.18it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.20it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.17it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.06it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.18it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.10it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.21it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.26it/s][A
                                                 [A                                                
100%|██████████| 505/505 [00:11<00:00, 44.26it/s][A 20%|██        | 78/390 [00:34<01:30,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:39:21,163 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 13:39:21,186 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:39:23,076 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:39:23,087 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:39:23,097 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:40<28:10,  5.44s/it] 21%|██        | 80/390 [00:40<20:07,  3.90s/it] 21%|██        | 81/390 [00:40<14:29,  2.82s/it] 21%|██        | 82/390 [00:41<10:34,  2.06s/it] 21%|██▏       | 83/390 [00:41<07:49,  1.53s/it] 22%|██▏       | 84/390 [00:41<05:54,  1.16s/it] 22%|██▏       | 85/390 [00:41<04:34,  1.11it/s] 22%|██▏       | 86/390 [00:42<03:38,  1.39it/s] 22%|██▏       | 87/390 [00:42<02:59,  1.69it/s] 23%|██▎       | 88/390 [00:42<02:31,  1.99it/s] 23%|██▎       | 89/390 [00:43<02:12,  2.27it/s] 23%|██▎       | 90/390 [00:43<01:58,  2.52it/s] 23%|██▎       | 91/390 [00:43<01:49,  2.72it/s] 24%|██▎       | 92/390 [00:43<01:43,  2.89it/s] 24%|██▍       | 93/390 [00:44<01:38,  3.02it/s] 24%|██▍       | 94/390 [00:44<01:34,  3.13it/s] 24%|██▍       | 95/390 [00:44<01:32,  3.20it/s] 25%|██▍       | 96/390 [00:45<01:30,  3.26it/s] 25%|██▍       | 97/390 [00:45<01:29,  3.29it/s] 25%|██▌       | 98/390 [00:45<01:27,  3.33it/s] 25%|██▌       | 99/390 [00:46<01:26,  3.36it/s] 26%|██▌       | 100/390 [00:46<01:25,  3.38it/s] 26%|██▌       | 101/390 [00:46<01:25,  3.40it/s] 26%|██▌       | 102/390 [00:46<01:24,  3.40it/s] 26%|██▋       | 103/390 [00:47<01:24,  3.41it/s] 27%|██▋       | 104/390 [00:47<01:23,  3.42it/s] 27%|██▋       | 105/390 [00:47<01:23,  3.43it/s] 27%|██▋       | 106/390 [00:48<01:22,  3.43it/s] 27%|██▋       | 107/390 [00:48<01:22,  3.43it/s] 28%|██▊       | 108/390 [00:48<01:22,  3.43it/s] 28%|██▊       | 109/390 [00:48<01:21,  3.43it/s] 28%|██▊       | 110/390 [00:49<01:21,  3.44it/s] 28%|██▊       | 111/390 [00:49<01:21,  3.44it/s] 29%|██▊       | 112/390 [00:49<01:20,  3.44it/s] 29%|██▉       | 113/390 [00:50<01:20,  3.43it/s] 29%|██▉       | 114/390 [00:50<01:20,  3.43it/s] 29%|██▉       | 115/390 [00:50<01:20,  3.43it/s] 30%|██▉       | 116/390 [00:50<01:20,  3.42it/s] 30%|███       | 117/390 [00:51<01:19,  3.43it/s] 30%|███       | 118/390 [00:51<01:19,  3.43it/s] 31%|███       | 119/390 [00:51<01:18,  3.43it/s] 31%|███       | 120/390 [00:52<01:20,  3.34it/s] 31%|███       | 121/390 [00:52<01:19,  3.37it/s] 31%|███▏      | 122/390 [00:52<01:19,  3.39it/s] 32%|███▏      | 123/390 [00:53<01:18,  3.41it/s] 32%|███▏      | 124/390 [00:53<01:18,  3.40it/s] 32%|███▏      | 125/390 [00:53<01:17,  3.41it/s] 32%|███▏      | 126/390 [00:53<01:17,  3.41it/s] 33%|███▎      | 127/390 [00:54<01:16,  3.42it/s] 33%|███▎      | 128/390 [00:54<01:16,  3.43it/s] 33%|███▎      | 129/390 [00:54<01:16,  3.43it/s] 33%|███▎      | 130/390 [00:55<01:15,  3.43it/s] 34%|███▎      | 131/390 [00:55<01:15,  3.43it/s] 34%|███▍      | 132/390 [00:55<01:15,  3.44it/s] 34%|███▍      | 133/390 [00:55<01:14,  3.44it/s] 34%|███▍      | 134/390 [00:56<01:14,  3.44it/s] 35%|███▍      | 135/390 [00:56<01:14,  3.44it/s] 35%|███▍      | 136/390 [00:56<01:14,  3.42it/s] 35%|███▌      | 137/390 [00:57<01:13,  3.43it/s] 35%|███▌      | 138/390 [00:57<01:13,  3.43it/s] 36%|███▌      | 139/390 [00:57<01:13,  3.42it/s] 36%|███▌      | 140/390 [00:58<01:13,  3.42it/s] 36%|███▌      | 141/390 [00:58<01:12,  3.43it/s] 36%|███▋      | 142/390 [00:58<01:12,  3.43it/s] 37%|███▋      | 143/390 [00:58<01:12,  3.43it/s] 37%|███▋      | 144/390 [00:59<01:11,  3.43it/s] 37%|███▋      | 145/390 [00:59<01:11,  3.43it/s] 37%|███▋      | 146/390 [00:59<01:11,  3.43it/s] 38%|███▊      | 147/390 [01:00<01:11,  3.42it/s] 38%|███▊      | 148/390 [01:00<01:10,  3.42it/s] 38%|███▊      | 149/390 [01:00<01:10,  3.43it/s] 38%|███▊      | 150/390 [01:00<01:10,  3.43it/s] 39%|███▊      | 151/390 [01:01<01:09,  3.43it/s] 39%|███▉      | 152/390 [01:01<01:09,  3.43it/s] 39%|███▉      | 153/390 [01:01<01:09,  3.43it/s] 39%|███▉      | 154/390 [01:02<01:08,  3.43it/s] 40%|███▉      | 155/390 [01:02<01:08,  3.43it/s] 40%|████      | 156/390 [01:02<01:08,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 13:39:49,695 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:39:49,695 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 13:39:49,695 >>   Batch size = 8
{'eval_loss': 1.0727628469467163, 'eval_runtime': 11.4278, 'eval_samples_per_second': 353.437, 'eval_steps_per_second': 44.191, 'epoch': 0.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.33it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.75it/s][A
  3%|▎         | 17/505 [00:00<00:10, 45.83it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.17it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.78it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.57it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.46it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.19it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.34it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.44it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.21it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.17it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.12it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.08it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.13it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.78it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.95it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.10it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.36it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.19it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.17it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.14it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.15it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.07it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.07it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.95it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.10it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.22it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.27it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.15it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.04it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.00it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.11it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.05it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.02it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.22it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.26it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.19it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.16it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.16it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.04it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.12it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 43.98it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.05it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.16it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.21it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.24it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.13it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.09it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.09it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.11it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 43.98it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.12it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.18it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.17it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.11it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.18it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.20it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.16it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.01it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.05it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.17it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.19it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.13it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.18it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.19it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.14it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.10it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 43.99it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.05it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.10it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.23it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.09it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.20it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.19it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.18it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.09it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 43.98it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.02it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.02it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.19it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.19it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.07it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.11it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.12it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.16it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.11it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 43.96it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.03it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.22it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.26it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.06it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.24it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.17it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.24it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 43.97it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.06it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.16it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.28it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.15it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.15it/s][A 40%|████      | 156/390 [01:14<01:08,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:40:01,170 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 13:40:01,199 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:40:02,778 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:40:02,793 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:40:02,803 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:20<21:24,  5.51s/it] 41%|████      | 158/390 [01:20<15:16,  3.95s/it] 41%|████      | 159/390 [01:20<10:58,  2.85s/it] 41%|████      | 160/390 [01:21<07:59,  2.08s/it] 41%|████▏     | 161/390 [01:21<05:54,  1.55s/it] 42%|████▏     | 162/390 [01:21<04:27,  1.17s/it] 42%|████▏     | 163/390 [01:22<03:26,  1.10it/s] 42%|████▏     | 164/390 [01:22<02:43,  1.38it/s] 42%|████▏     | 165/390 [01:22<02:14,  1.68it/s] 43%|████▎     | 166/390 [01:23<01:53,  1.98it/s] 43%|████▎     | 167/390 [01:23<01:38,  2.26it/s] 43%|████▎     | 168/390 [01:23<01:28,  2.51it/s] 43%|████▎     | 169/390 [01:23<01:21,  2.72it/s] 44%|████▎     | 170/390 [01:24<01:16,  2.89it/s] 44%|████▍     | 171/390 [01:24<01:12,  3.02it/s] 44%|████▍     | 172/390 [01:24<01:09,  3.12it/s] 44%|████▍     | 173/390 [01:25<01:07,  3.20it/s] 45%|████▍     | 174/390 [01:25<01:06,  3.26it/s] 45%|████▍     | 175/390 [01:25<01:05,  3.29it/s] 45%|████▌     | 176/390 [01:25<01:04,  3.32it/s] 45%|████▌     | 177/390 [01:26<01:03,  3.34it/s] 46%|████▌     | 178/390 [01:26<01:03,  3.36it/s] 46%|████▌     | 179/390 [01:26<01:02,  3.36it/s] 46%|████▌     | 180/390 [01:27<01:02,  3.37it/s] 46%|████▋     | 181/390 [01:27<01:02,  3.36it/s] 47%|████▋     | 182/390 [01:27<01:01,  3.37it/s] 47%|████▋     | 183/390 [01:28<01:01,  3.38it/s] 47%|████▋     | 184/390 [01:28<01:00,  3.38it/s] 47%|████▋     | 185/390 [01:28<01:00,  3.38it/s] 48%|████▊     | 186/390 [01:28<01:00,  3.39it/s] 48%|████▊     | 187/390 [01:29<00:59,  3.39it/s] 48%|████▊     | 188/390 [01:29<00:59,  3.39it/s] 48%|████▊     | 189/390 [01:29<00:59,  3.38it/s] 49%|████▊     | 190/390 [01:30<00:59,  3.39it/s] 49%|████▉     | 191/390 [01:30<00:58,  3.39it/s] 49%|████▉     | 192/390 [01:30<00:58,  3.39it/s] 49%|████▉     | 193/390 [01:31<00:58,  3.39it/s] 50%|████▉     | 194/390 [01:31<00:57,  3.39it/s] 50%|█████     | 195/390 [01:31<00:57,  3.39it/s] 50%|█████     | 196/390 [01:31<00:57,  3.39it/s] 51%|█████     | 197/390 [01:32<00:56,  3.39it/s] 51%|█████     | 198/390 [01:32<00:56,  3.39it/s] 51%|█████     | 199/390 [01:32<00:56,  3.39it/s] 51%|█████▏    | 200/390 [01:33<00:56,  3.39it/s] 52%|█████▏    | 201/390 [01:33<00:55,  3.39it/s] 52%|█████▏    | 202/390 [01:33<00:55,  3.39it/s] 52%|█████▏    | 203/390 [01:33<00:55,  3.38it/s] 52%|█████▏    | 204/390 [01:34<00:54,  3.39it/s] 53%|█████▎    | 205/390 [01:34<00:54,  3.39it/s] 53%|█████▎    | 206/390 [01:34<00:54,  3.38it/s] 53%|█████▎    | 207/390 [01:35<00:54,  3.38it/s] 53%|█████▎    | 208/390 [01:35<00:53,  3.39it/s] 54%|█████▎    | 209/390 [01:35<00:53,  3.39it/s] 54%|█████▍    | 210/390 [01:36<00:53,  3.39it/s] 54%|█████▍    | 211/390 [01:36<00:52,  3.39it/s] 54%|█████▍    | 212/390 [01:36<00:52,  3.39it/s] 55%|█████▍    | 213/390 [01:36<00:52,  3.39it/s] 55%|█████▍    | 214/390 [01:37<00:52,  3.38it/s] 55%|█████▌    | 215/390 [01:37<00:51,  3.38it/s] 55%|█████▌    | 216/390 [01:37<00:51,  3.38it/s] 56%|█████▌    | 217/390 [01:38<00:51,  3.38it/s] 56%|█████▌    | 218/390 [01:38<00:50,  3.39it/s] 56%|█████▌    | 219/390 [01:38<00:50,  3.39it/s] 56%|█████▋    | 220/390 [01:38<00:50,  3.39it/s] 57%|█████▋    | 221/390 [01:39<00:49,  3.39it/s] 57%|█████▋    | 222/390 [01:39<00:49,  3.39it/s] 57%|█████▋    | 223/390 [01:39<00:49,  3.39it/s] 57%|█████▋    | 224/390 [01:40<00:49,  3.39it/s] 58%|█████▊    | 225/390 [01:40<00:48,  3.38it/s] 58%|█████▊    | 226/390 [01:40<00:48,  3.38it/s] 58%|█████▊    | 227/390 [01:41<00:48,  3.38it/s] 58%|█████▊    | 228/390 [01:41<00:47,  3.38it/s] 59%|█████▊    | 229/390 [01:41<00:47,  3.39it/s] 59%|█████▉    | 230/390 [01:41<00:47,  3.39it/s] 59%|█████▉    | 231/390 [01:42<00:46,  3.39it/s] 59%|█████▉    | 232/390 [01:42<00:46,  3.39it/s] 60%|█████▉    | 233/390 [01:42<00:46,  3.38it/s] 60%|██████    | 234/390 [01:43<00:46,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 13:40:30,132 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:40:30,132 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 13:40:30,132 >>   Batch size = 8
{'eval_loss': 1.0859452486038208, 'eval_runtime': 11.4486, 'eval_samples_per_second': 352.795, 'eval_steps_per_second': 44.11, 'epoch': 1.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.88it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.36it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.34it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.48it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.98it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.62it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.51it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.19it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.21it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.38it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.43it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.32it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.25it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.20it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.03it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.05it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.10it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.14it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.24it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.25it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.24it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.19it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.08it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 43.96it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.13it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.12it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.17it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.27it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.33it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.19it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.11it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 43.98it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.07it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.14it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.15it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.23it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.35it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.26it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.20it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.02it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.00it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.04it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.16it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.15it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.13it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.17it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.29it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.19it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.08it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.07it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.10it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.12it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.18it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.18it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.23it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.16it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.06it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 43.93it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.05it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.10it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.13it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.12it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.26it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.26it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.26it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.17it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.08it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.12it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.10it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.12it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.08it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.26it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.23it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.23it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.10it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.13it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.14it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.13it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.09it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 42.52it/s][A
 81%|████████  | 407/505 [00:09<00:02, 43.21it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 43.54it/s][A
 83%|████████▎ | 417/505 [00:09<00:02, 43.61it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 43.72it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 43.90it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 43.99it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.00it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 43.82it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 43.98it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.13it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.29it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.13it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.19it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.23it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.14it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.01it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 43.93it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 43.95it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.14it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.26it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.26it/s][A 60%|██████    | 234/390 [01:54<00:46,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:40:41,606 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 13:40:41,623 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:40:43,268 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:40:43,280 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:40:43,296 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:00<14:06,  5.46s/it] 61%|██████    | 236/390 [02:00<10:02,  3.91s/it] 61%|██████    | 237/390 [02:01<07:12,  2.83s/it] 61%|██████    | 238/390 [02:01<05:14,  2.07s/it] 61%|██████▏   | 239/390 [02:01<03:51,  1.54s/it] 62%|██████▏   | 240/390 [02:02<02:54,  1.16s/it] 62%|██████▏   | 241/390 [02:02<02:14,  1.11it/s] 62%|██████▏   | 242/390 [02:02<01:46,  1.39it/s] 62%|██████▏   | 243/390 [02:03<01:27,  1.69it/s] 63%|██████▎   | 244/390 [02:03<01:13,  1.99it/s] 63%|██████▎   | 245/390 [02:03<01:03,  2.27it/s] 63%|██████▎   | 246/390 [02:03<00:57,  2.52it/s] 63%|██████▎   | 247/390 [02:04<00:52,  2.72it/s] 64%|██████▎   | 248/390 [02:04<00:49,  2.89it/s] 64%|██████▍   | 249/390 [02:04<00:46,  3.02it/s] 64%|██████▍   | 250/390 [02:05<00:44,  3.13it/s] 64%|██████▍   | 251/390 [02:05<00:43,  3.20it/s] 65%|██████▍   | 252/390 [02:05<00:42,  3.25it/s] 65%|██████▍   | 253/390 [02:05<00:41,  3.29it/s] 65%|██████▌   | 254/390 [02:06<00:40,  3.32it/s] 65%|██████▌   | 255/390 [02:06<00:40,  3.34it/s] 66%|██████▌   | 256/390 [02:06<00:39,  3.35it/s] 66%|██████▌   | 257/390 [02:07<00:39,  3.36it/s] 66%|██████▌   | 258/390 [02:07<00:39,  3.36it/s] 66%|██████▋   | 259/390 [02:07<00:38,  3.37it/s] 67%|██████▋   | 260/390 [02:08<00:38,  3.37it/s] 67%|██████▋   | 261/390 [02:08<00:38,  3.38it/s] 67%|██████▋   | 262/390 [02:08<00:37,  3.38it/s] 67%|██████▋   | 263/390 [02:08<00:37,  3.39it/s] 68%|██████▊   | 264/390 [02:09<00:36,  3.41it/s] 68%|██████▊   | 265/390 [02:09<00:36,  3.41it/s] 68%|██████▊   | 266/390 [02:09<00:36,  3.42it/s] 68%|██████▊   | 267/390 [02:10<00:35,  3.43it/s] 69%|██████▊   | 268/390 [02:10<00:35,  3.42it/s] 69%|██████▉   | 269/390 [02:10<00:35,  3.40it/s] 69%|██████▉   | 270/390 [02:10<00:35,  3.41it/s] 69%|██████▉   | 271/390 [02:11<00:34,  3.42it/s] 70%|██████▉   | 272/390 [02:11<00:34,  3.42it/s] 70%|███████   | 273/390 [02:11<00:34,  3.43it/s] 70%|███████   | 274/390 [02:12<00:33,  3.43it/s] 71%|███████   | 275/390 [02:12<00:33,  3.43it/s] 71%|███████   | 276/390 [02:12<00:33,  3.43it/s] 71%|███████   | 277/390 [02:12<00:32,  3.43it/s] 71%|███████▏  | 278/390 [02:13<00:32,  3.44it/s] 72%|███████▏  | 279/390 [02:13<00:32,  3.44it/s] 72%|███████▏  | 280/390 [02:13<00:32,  3.43it/s] 72%|███████▏  | 281/390 [02:14<00:31,  3.43it/s] 72%|███████▏  | 282/390 [02:14<00:31,  3.43it/s] 73%|███████▎  | 283/390 [02:14<00:31,  3.44it/s] 73%|███████▎  | 284/390 [02:15<00:30,  3.43it/s] 73%|███████▎  | 285/390 [02:15<00:30,  3.44it/s] 73%|███████▎  | 286/390 [02:15<00:30,  3.44it/s] 74%|███████▎  | 287/390 [02:15<00:29,  3.44it/s] 74%|███████▍  | 288/390 [02:16<00:29,  3.44it/s] 74%|███████▍  | 289/390 [02:16<00:29,  3.44it/s] 74%|███████▍  | 290/390 [02:16<00:29,  3.44it/s] 75%|███████▍  | 291/390 [02:17<00:28,  3.43it/s] 75%|███████▍  | 292/390 [02:17<00:28,  3.43it/s] 75%|███████▌  | 293/390 [02:17<00:28,  3.43it/s] 75%|███████▌  | 294/390 [02:17<00:27,  3.43it/s] 76%|███████▌  | 295/390 [02:18<00:27,  3.44it/s] 76%|███████▌  | 296/390 [02:18<00:27,  3.44it/s] 76%|███████▌  | 297/390 [02:18<00:27,  3.43it/s] 76%|███████▋  | 298/390 [02:19<00:26,  3.43it/s] 77%|███████▋  | 299/390 [02:19<00:26,  3.44it/s] 77%|███████▋  | 300/390 [02:19<00:26,  3.44it/s] 77%|███████▋  | 301/390 [02:19<00:25,  3.44it/s] 77%|███████▋  | 302/390 [02:20<00:25,  3.43it/s] 78%|███████▊  | 303/390 [02:20<00:25,  3.43it/s] 78%|███████▊  | 304/390 [02:20<00:25,  3.43it/s] 78%|███████▊  | 305/390 [02:21<00:24,  3.43it/s] 78%|███████▊  | 306/390 [02:21<00:24,  3.43it/s] 79%|███████▊  | 307/390 [02:21<00:24,  3.43it/s] 79%|███████▉  | 308/390 [02:22<00:23,  3.44it/s] 79%|███████▉  | 309/390 [02:22<00:23,  3.44it/s] 79%|███████▉  | 310/390 [02:22<00:23,  3.43it/s] 80%|███████▉  | 311/390 [02:22<00:22,  3.44it/s] 80%|████████  | 312/390 [02:23<00:22,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 13:41:10,200 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:41:10,200 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 13:41:10,200 >>   Batch size = 8
{'eval_loss': 1.1035047769546509, 'eval_runtime': 11.4539, 'eval_samples_per_second': 352.63, 'eval_steps_per_second': 44.09, 'epoch': 2.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.26it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.65it/s][A
  3%|▎         | 17/505 [00:00<00:10, 45.93it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.26it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.88it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.71it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.48it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.20it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.19it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.41it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.28it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.19it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.08it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.19it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.21it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.09it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.09it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.08it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.12it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.22it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.20it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.15it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.18it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.08it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.03it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.05it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.11it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.11it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.15it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.22it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.12it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.19it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.03it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.04it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.10it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.13it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.18it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.26it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.17it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.17it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.03it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.11it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.02it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.10it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.17it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.27it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.17it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.11it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.15it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.10it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.01it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.13it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.22it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.12it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.09it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.10it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.13it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 43.96it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.02it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 43.95it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.24it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.29it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.16it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.25it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.13it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.18it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.18it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.00it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.08it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.18it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.22it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.24it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.19it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.11it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.14it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.11it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.10it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.04it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.09it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.10it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.18it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.21it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.18it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.18it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.10it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.09it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.08it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.10it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.12it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.23it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.23it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.23it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.06it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.10it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.04it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.11it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.15it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.22it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.21it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.06it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.06it/s][A 80%|████████  | 312/390 [02:34<00:22,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:41:21,668 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 13:41:21,701 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:41:23,661 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:41:23,675 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:41:23,691 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:40<07:01,  5.47s/it] 81%|████████  | 314/390 [02:41<04:57,  3.92s/it] 81%|████████  | 315/390 [02:41<03:32,  2.83s/it] 81%|████████  | 316/390 [02:41<02:33,  2.07s/it] 81%|████████▏ | 317/390 [02:41<01:52,  1.54s/it] 82%|████████▏ | 318/390 [02:42<01:23,  1.16s/it] 82%|████████▏ | 319/390 [02:42<01:04,  1.11it/s] 82%|████████▏ | 320/390 [02:42<00:50,  1.39it/s] 82%|████████▏ | 321/390 [02:43<00:40,  1.69it/s] 83%|████████▎ | 322/390 [02:43<00:34,  1.99it/s] 83%|████████▎ | 323/390 [02:43<00:29,  2.27it/s] 83%|████████▎ | 324/390 [02:43<00:26,  2.52it/s] 83%|████████▎ | 325/390 [02:44<00:23,  2.72it/s] 84%|████████▎ | 326/390 [02:44<00:22,  2.89it/s] 84%|████████▍ | 327/390 [02:44<00:20,  3.03it/s] 84%|████████▍ | 328/390 [02:45<00:19,  3.13it/s] 84%|████████▍ | 329/390 [02:45<00:19,  3.20it/s] 85%|████████▍ | 330/390 [02:45<00:18,  3.26it/s] 85%|████████▍ | 331/390 [02:46<00:17,  3.30it/s] 85%|████████▌ | 332/390 [02:46<00:17,  3.32it/s] 85%|████████▌ | 333/390 [02:46<00:17,  3.34it/s] 86%|████████▌ | 334/390 [02:46<00:16,  3.36it/s] 86%|████████▌ | 335/390 [02:47<00:16,  3.37it/s] 86%|████████▌ | 336/390 [02:47<00:16,  3.36it/s] 86%|████████▋ | 337/390 [02:47<00:15,  3.37it/s] 87%|████████▋ | 338/390 [02:48<00:15,  3.38it/s] 87%|████████▋ | 339/390 [02:48<00:15,  3.38it/s] 87%|████████▋ | 340/390 [02:48<00:14,  3.39it/s] 87%|████████▋ | 341/390 [02:48<00:14,  3.39it/s] 88%|████████▊ | 342/390 [02:49<00:14,  3.39it/s] 88%|████████▊ | 343/390 [02:49<00:13,  3.39it/s] 88%|████████▊ | 344/390 [02:49<00:13,  3.39it/s] 88%|████████▊ | 345/390 [02:50<00:13,  3.39it/s] 89%|████████▊ | 346/390 [02:50<00:12,  3.39it/s] 89%|████████▉ | 347/390 [02:50<00:12,  3.33it/s] 89%|████████▉ | 348/390 [02:51<00:12,  3.34it/s] 89%|████████▉ | 349/390 [02:51<00:12,  3.36it/s] 90%|████████▉ | 350/390 [02:51<00:11,  3.37it/s] 90%|█████████ | 351/390 [02:51<00:11,  3.38it/s] 90%|█████████ | 352/390 [02:52<00:11,  3.30it/s] 91%|█████████ | 353/390 [02:52<00:11,  3.33it/s] 91%|█████████ | 354/390 [02:52<00:10,  3.35it/s] 91%|█████████ | 355/390 [02:53<00:10,  3.36it/s] 91%|█████████▏| 356/390 [02:53<00:10,  3.37it/s] 92%|█████████▏| 357/390 [02:53<00:09,  3.38it/s] 92%|█████████▏| 358/390 [02:54<00:09,  3.36it/s] 92%|█████████▏| 359/390 [02:54<00:09,  3.37it/s] 92%|█████████▏| 360/390 [02:54<00:08,  3.38it/s] 93%|█████████▎| 361/390 [02:54<00:08,  3.38it/s] 93%|█████████▎| 362/390 [02:55<00:08,  3.38it/s] 93%|█████████▎| 363/390 [02:55<00:07,  3.38it/s] 93%|█████████▎| 364/390 [02:55<00:07,  3.39it/s] 94%|█████████▎| 365/390 [02:56<00:07,  3.39it/s] 94%|█████████▍| 366/390 [02:56<00:07,  3.39it/s] 94%|█████████▍| 367/390 [02:56<00:06,  3.39it/s] 94%|█████████▍| 368/390 [02:57<00:06,  3.39it/s] 95%|█████████▍| 369/390 [02:57<00:06,  3.39it/s] 95%|█████████▍| 370/390 [02:57<00:05,  3.39it/s] 95%|█████████▌| 371/390 [02:57<00:05,  3.39it/s] 95%|█████████▌| 372/390 [02:58<00:05,  3.39it/s] 96%|█████████▌| 373/390 [02:58<00:05,  3.39it/s] 96%|█████████▌| 374/390 [02:58<00:04,  3.39it/s] 96%|█████████▌| 375/390 [02:59<00:04,  3.39it/s] 96%|█████████▋| 376/390 [02:59<00:04,  3.39it/s] 97%|█████████▋| 377/390 [02:59<00:03,  3.38it/s] 97%|█████████▋| 378/390 [02:59<00:03,  3.38it/s] 97%|█████████▋| 379/390 [03:00<00:03,  3.38it/s] 97%|█████████▋| 380/390 [03:00<00:02,  3.38it/s] 98%|█████████▊| 381/390 [03:00<00:02,  3.39it/s] 98%|█████████▊| 382/390 [03:01<00:02,  3.39it/s] 98%|█████████▊| 383/390 [03:01<00:02,  3.39it/s] 98%|█████████▊| 384/390 [03:01<00:01,  3.39it/s] 99%|█████████▊| 385/390 [03:02<00:01,  3.39it/s] 99%|█████████▉| 386/390 [03:02<00:01,  3.39it/s] 99%|█████████▉| 387/390 [03:02<00:00,  3.39it/s] 99%|█████████▉| 388/390 [03:02<00:00,  3.37it/s]100%|█████████▉| 389/390 [03:03<00:00,  3.38it/s]100%|██████████| 390/390 [03:03<00:00,  3.38it/s][INFO|trainer.py:2140] 2023-08-28 13:41:50,485 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:41:50,485 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 13:41:50,485 >>   Batch size = 8
{'eval_loss': 1.1099940538406372, 'eval_runtime': 11.4447, 'eval_samples_per_second': 352.914, 'eval_steps_per_second': 44.125, 'epoch': 3.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.31it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.64it/s][A
  3%|▎         | 17/505 [00:00<00:10, 45.90it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.27it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.89it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.67it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.46it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.34it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.21it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.47it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.19it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.08it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.12it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.16it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.18it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.13it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.22it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.28it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.18it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.02it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.05it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.08it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.05it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.07it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.20it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.23it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.20it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.23it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.10it/s][A
 30%|███       | 152/505 [00:03<00:08, 44.05it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.02it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.06it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.10it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.24it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.23it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.28it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.14it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.19it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.10it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.10it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.13it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.23it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.12it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.10it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.23it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.22it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.13it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.08it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.13it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.06it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.19it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.10it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.17it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.18it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.20it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.08it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.10it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.15it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.22it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.15it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.28it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.22it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.22it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.13it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.02it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.04it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.17it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.21it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.10it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.20it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.25it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.23it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.11it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.02it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 43.90it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.10it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.20it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.17it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.16it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.18it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.14it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.08it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.09it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.12it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.11it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.13it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.23it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.22it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.15it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.12it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.09it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 43.94it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.03it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.07it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.23it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.22it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.31it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.20it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.24it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.11it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.11it/s][A100%|██████████| 390/390 [03:14<00:00,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:42:01,943 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 13:42:01,963 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:42:03,789 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:42:03,800 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:42:03,808 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 13:42:07,484 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 13:42:07,486 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78 (score: 1.0727628469467163).
                                                 100%|██████████| 390/390 [03:22<00:00,  3.38it/s]100%|██████████| 390/390 [03:22<00:00,  1.93it/s]
[INFO|trainer.py:1894] 2023-08-28 13:42:09,229 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 13:42:09,248 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:42:11,180 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:42:11,196 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:42:11,204 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:42:11,412 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:42:11,412 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:42:11,412 >>   train_loss               =     0.5219
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:42:11,412 >>   train_runtime            = 0:03:22.24
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:42:11,412 >>   train_samples            =       4999
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:42:11,412 >>   train_samples_per_second =    123.588
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:42:11,412 >>   train_steps_per_second   =      1.928
{'eval_loss': 1.114742636680603, 'eval_runtime': 11.4428, 'eval_samples_per_second': 352.974, 'eval_steps_per_second': 44.133, 'epoch': 4.99}
{'train_runtime': 202.2451, 'train_samples_per_second': 123.588, 'train_steps_per_second': 1.928, 'train_loss': 0.5219244541266026, 'epoch': 4.99}
08/28/2023 13:42:11 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 13:42:11,455 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:42:11,455 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 13:42:11,455 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:09, 54.87it/s]  2%|▏         | 12/505 [00:00<00:10, 48.30it/s]  3%|▎         | 17/505 [00:00<00:10, 46.70it/s]  4%|▍         | 22/505 [00:00<00:10, 46.09it/s]  5%|▌         | 27/505 [00:00<00:10, 45.59it/s]  6%|▋         | 32/505 [00:00<00:10, 45.22it/s]  7%|▋         | 37/505 [00:00<00:10, 45.13it/s]  8%|▊         | 42/505 [00:00<00:10, 44.60it/s]  9%|▉         | 47/505 [00:01<00:10, 44.03it/s] 10%|█         | 52/505 [00:01<00:10, 43.80it/s] 11%|█▏        | 57/505 [00:01<00:10, 43.87it/s] 12%|█▏        | 62/505 [00:01<00:10, 44.15it/s] 13%|█▎        | 67/505 [00:01<00:09, 44.27it/s] 14%|█▍        | 72/505 [00:01<00:09, 44.33it/s] 15%|█▌        | 77/505 [00:01<00:09, 44.47it/s] 16%|█▌        | 82/505 [00:01<00:09, 44.42it/s] 17%|█▋        | 87/505 [00:01<00:09, 44.26it/s] 18%|█▊        | 92/505 [00:02<00:09, 43.97it/s] 19%|█▉        | 97/505 [00:02<00:09, 43.76it/s] 20%|██        | 102/505 [00:02<00:09, 43.84it/s] 21%|██        | 107/505 [00:02<00:09, 43.96it/s] 22%|██▏       | 112/505 [00:02<00:08, 44.07it/s] 23%|██▎       | 117/505 [00:02<00:08, 44.16it/s] 24%|██▍       | 122/505 [00:02<00:08, 44.34it/s] 25%|██▌       | 127/505 [00:02<00:08, 44.25it/s] 26%|██▌       | 132/505 [00:02<00:08, 44.08it/s] 27%|██▋       | 137/505 [00:03<00:08, 43.95it/s] 28%|██▊       | 142/505 [00:03<00:08, 43.86it/s] 29%|██▉       | 147/505 [00:03<00:08, 44.02it/s] 30%|███       | 152/505 [00:03<00:07, 44.15it/s] 31%|███       | 157/505 [00:03<00:07, 44.17it/s] 32%|███▏      | 162/505 [00:03<00:07, 44.27it/s] 33%|███▎      | 167/505 [00:03<00:07, 44.28it/s] 34%|███▍      | 172/505 [00:03<00:07, 44.24it/s] 35%|███▌      | 177/505 [00:03<00:07, 43.98it/s] 36%|███▌      | 182/505 [00:04<00:07, 43.91it/s] 37%|███▋      | 187/505 [00:04<00:07, 43.88it/s] 38%|███▊      | 192/505 [00:04<00:07, 43.90it/s] 39%|███▉      | 197/505 [00:04<00:06, 44.10it/s] 40%|████      | 202/505 [00:04<00:06, 44.25it/s] 41%|████      | 207/505 [00:04<00:06, 44.32it/s] 42%|████▏     | 212/505 [00:04<00:06, 44.25it/s] 43%|████▎     | 217/505 [00:04<00:06, 44.11it/s] 44%|████▍     | 222/505 [00:05<00:06, 43.87it/s] 45%|████▍     | 227/505 [00:05<00:06, 43.91it/s] 46%|████▌     | 232/505 [00:05<00:06, 43.88it/s] 47%|████▋     | 237/505 [00:05<00:06, 43.90it/s] 48%|████▊     | 242/505 [00:05<00:05, 44.03it/s] 49%|████▉     | 247/505 [00:05<00:05, 44.27it/s] 50%|████▉     | 252/505 [00:05<00:05, 44.38it/s] 51%|█████     | 257/505 [00:05<00:05, 44.25it/s] 52%|█████▏    | 262/505 [00:05<00:05, 44.08it/s] 53%|█████▎    | 267/505 [00:06<00:05, 43.91it/s] 54%|█████▍    | 272/505 [00:06<00:05, 43.92it/s] 55%|█████▍    | 277/505 [00:06<00:05, 43.85it/s] 56%|█████▌    | 282/505 [00:06<00:05, 43.95it/s] 57%|█████▋    | 287/505 [00:06<00:04, 44.03it/s] 58%|█████▊    | 292/505 [00:06<00:04, 44.18it/s] 59%|█████▉    | 297/505 [00:06<00:04, 44.30it/s] 60%|█████▉    | 302/505 [00:06<00:04, 44.31it/s] 61%|██████    | 307/505 [00:06<00:04, 44.14it/s] 62%|██████▏   | 312/505 [00:07<00:04, 43.96it/s] 63%|██████▎   | 317/505 [00:07<00:04, 43.77it/s] 64%|██████▍   | 322/505 [00:07<00:04, 43.89it/s] 65%|██████▍   | 327/505 [00:07<00:04, 43.97it/s] 66%|██████▌   | 332/505 [00:07<00:03, 43.99it/s] 67%|██████▋   | 337/505 [00:07<00:03, 44.20it/s] 68%|██████▊   | 342/505 [00:07<00:03, 44.31it/s] 69%|██████▊   | 347/505 [00:07<00:03, 44.26it/s] 70%|██████▉   | 352/505 [00:07<00:03, 44.06it/s] 71%|███████   | 357/505 [00:08<00:03, 43.92it/s] 72%|███████▏  | 362/505 [00:08<00:03, 43.80it/s] 73%|███████▎  | 367/505 [00:08<00:03, 43.94it/s] 74%|███████▎  | 372/505 [00:08<00:03, 43.97it/s] 75%|███████▍  | 377/505 [00:08<00:02, 44.09it/s] 76%|███████▌  | 382/505 [00:08<00:02, 44.21it/s] 77%|███████▋  | 387/505 [00:08<00:02, 44.33it/s] 78%|███████▊  | 392/505 [00:08<00:02, 44.17it/s] 79%|███████▊  | 397/505 [00:08<00:02, 44.01it/s] 80%|███████▉  | 402/505 [00:09<00:02, 43.91it/s] 81%|████████  | 407/505 [00:09<00:02, 43.86it/s] 82%|████████▏ | 412/505 [00:09<00:02, 43.83it/s] 83%|████████▎ | 417/505 [00:09<00:01, 44.01it/s] 84%|████████▎ | 422/505 [00:09<00:01, 44.09it/s] 85%|████████▍ | 427/505 [00:09<00:01, 44.14it/s] 86%|████████▌ | 432/505 [00:09<00:01, 44.35it/s] 87%|████████▋ | 437/505 [00:09<00:01, 44.15it/s] 88%|████████▊ | 442/505 [00:09<00:01, 44.10it/s] 89%|████████▊ | 447/505 [00:10<00:01, 43.97it/s] 90%|████████▉ | 452/505 [00:10<00:01, 43.94it/s] 90%|█████████ | 457/505 [00:10<00:01, 43.93it/s] 91%|█████████▏| 462/505 [00:10<00:00, 43.87it/s] 92%|█████████▏| 467/505 [00:10<00:00, 43.93it/s] 93%|█████████▎| 472/505 [00:10<00:00, 44.13it/s] 94%|█████████▍| 477/505 [00:10<00:00, 44.21it/s] 95%|█████████▌| 482/505 [00:10<00:00, 44.32it/s] 96%|█████████▋| 487/505 [00:11<00:00, 44.31it/s] 97%|█████████▋| 492/505 [00:11<00:00, 44.18it/s] 98%|█████████▊| 497/505 [00:11<00:00, 44.19it/s] 99%|█████████▉| 502/505 [00:11<00:00, 44.08it/s]100%|██████████| 505/505 [00:11<00:00, 44.16it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:42:22,909 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:42:22,909 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:42:22,909 >>   eval_loss               =     1.0728
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:42:22,909 >>   eval_runtime            = 0:00:11.45
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:42:22,909 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:42:22,909 >>   eval_samples_per_second =    352.647
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:42:22,909 >>   eval_steps_per_second   =     44.092
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:42:22,909 >>   perplexity              =     2.9234
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:42:29,890 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:42:29,895 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:42:29,895 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:42:29,895 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:42:29,895 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:42:30,541 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:42:30,542 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:42:31,111 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:42:32,153 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:42:32,154 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:42:34,990 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:42:34,995 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:42:34,995 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:42:34,995 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:42:34,995 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:42:35,645 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:42:35,647 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:42:36,212 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:42:36,385 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:42:36,385 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:12,  1.52it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:15,  1.48it/s]Extractor Predicting: 24it [00:15,  1.49it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.57it/s]Extractor Predicting: 28it [00:18,  1.55it/s]Extractor Predicting: 29it [00:18,  1.57it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:20,  1.55it/s]Extractor Predicting: 32it [00:20,  1.54it/s]Extractor Predicting: 33it [00:21,  1.53it/s]Extractor Predicting: 34it [00:22,  1.53it/s]Extractor Predicting: 35it [00:22,  1.52it/s]Extractor Predicting: 36it [00:23,  1.58it/s]Extractor Predicting: 37it [00:23,  1.63it/s]Extractor Predicting: 38it [00:24,  1.60it/s]Extractor Predicting: 39it [00:25,  1.63it/s]Extractor Predicting: 40it [00:25,  1.60it/s]Extractor Predicting: 41it [00:26,  1.58it/s]Extractor Predicting: 42it [00:27,  1.57it/s]Extractor Predicting: 43it [00:27,  1.61it/s]Extractor Predicting: 44it [00:28,  1.60it/s]Extractor Predicting: 45it [00:28,  1.62it/s]Extractor Predicting: 46it [00:29,  1.59it/s]Extractor Predicting: 47it [00:30,  1.57it/s]Extractor Predicting: 48it [00:30,  1.50it/s]Extractor Predicting: 49it [00:31,  1.55it/s]Extractor Predicting: 50it [00:32,  1.58it/s]Extractor Predicting: 51it [00:32,  1.61it/s]Extractor Predicting: 52it [00:33,  1.65it/s]Extractor Predicting: 53it [00:33,  1.63it/s]Extractor Predicting: 54it [00:34,  1.63it/s]Extractor Predicting: 55it [00:35,  1.62it/s]Extractor Predicting: 56it [00:35,  1.62it/s]Extractor Predicting: 57it [00:36,  1.62it/s]Extractor Predicting: 58it [00:37,  1.57it/s]Extractor Predicting: 59it [00:37,  1.59it/s]Extractor Predicting: 60it [00:38,  1.57it/s]Extractor Predicting: 61it [00:38,  1.62it/s]Extractor Predicting: 62it [00:39,  1.61it/s]Extractor Predicting: 63it [00:40,  1.58it/s]Extractor Predicting: 64it [00:40,  1.57it/s]Extractor Predicting: 65it [00:41,  1.56it/s]Extractor Predicting: 66it [00:42,  1.44it/s]Extractor Predicting: 67it [00:43,  1.47it/s]Extractor Predicting: 68it [00:43,  1.46it/s]Extractor Predicting: 69it [00:44,  1.44it/s]Extractor Predicting: 70it [00:45,  1.48it/s]Extractor Predicting: 71it [00:45,  1.48it/s]Extractor Predicting: 72it [00:46,  1.45it/s]Extractor Predicting: 73it [00:47,  1.45it/s]Extractor Predicting: 74it [00:47,  1.40it/s]Extractor Predicting: 75it [00:48,  1.44it/s]Extractor Predicting: 76it [00:49,  1.47it/s]Extractor Predicting: 77it [00:49,  1.47it/s]Extractor Predicting: 78it [00:50,  1.48it/s]Extractor Predicting: 79it [00:51,  1.49it/s]Extractor Predicting: 80it [00:51,  1.49it/s]Extractor Predicting: 81it [00:52,  1.50it/s]Extractor Predicting: 82it [00:53,  1.55it/s]Extractor Predicting: 83it [00:53,  1.51it/s]Extractor Predicting: 84it [00:54,  1.52it/s]Extractor Predicting: 85it [00:55,  1.51it/s]Extractor Predicting: 86it [00:55,  1.54it/s]Extractor Predicting: 87it [00:56,  1.57it/s]Extractor Predicting: 88it [00:57,  1.57it/s]Extractor Predicting: 89it [00:57,  1.57it/s]Extractor Predicting: 90it [00:58,  1.57it/s]Extractor Predicting: 91it [00:58,  1.57it/s]Extractor Predicting: 92it [00:59,  1.60it/s]Extractor Predicting: 93it [01:00,  1.59it/s]Extractor Predicting: 94it [01:00,  1.57it/s]Extractor Predicting: 95it [01:01,  1.55it/s]Extractor Predicting: 96it [01:02,  1.53it/s]Extractor Predicting: 97it [01:02,  1.54it/s]Extractor Predicting: 98it [01:03,  1.54it/s]Extractor Predicting: 99it [01:04,  1.53it/s]Extractor Predicting: 100it [01:04,  1.53it/s]Extractor Predicting: 101it [01:05,  1.50it/s]Extractor Predicting: 102it [01:06,  1.51it/s]Extractor Predicting: 103it [01:06,  1.52it/s]Extractor Predicting: 104it [01:07,  1.56it/s]Extractor Predicting: 105it [01:08,  1.55it/s]Extractor Predicting: 106it [01:08,  1.58it/s]Extractor Predicting: 107it [01:09,  1.58it/s]Extractor Predicting: 108it [01:09,  1.60it/s]Extractor Predicting: 109it [01:10,  1.60it/s]Extractor Predicting: 110it [01:11,  1.57it/s]Extractor Predicting: 111it [01:11,  1.53it/s]Extractor Predicting: 112it [01:12,  1.58it/s]Extractor Predicting: 113it [01:13,  1.56it/s]Extractor Predicting: 114it [01:13,  1.55it/s]Extractor Predicting: 115it [01:14,  1.59it/s]Extractor Predicting: 116it [01:14,  1.59it/s]Extractor Predicting: 117it [01:15,  1.58it/s]Extractor Predicting: 118it [01:16,  1.52it/s]Extractor Predicting: 119it [01:16,  1.56it/s]Extractor Predicting: 120it [01:17,  1.56it/s]Extractor Predicting: 121it [01:18,  1.54it/s]Extractor Predicting: 122it [01:18,  1.55it/s]Extractor Predicting: 123it [01:19,  1.57it/s]Extractor Predicting: 124it [01:20,  1.57it/s]Extractor Predicting: 125it [01:20,  1.43it/s]Extractor Predicting: 126it [01:21,  1.48it/s]Extractor Predicting: 127it [01:22,  1.43it/s]Extractor Predicting: 128it [01:23,  1.42it/s]Extractor Predicting: 129it [01:23,  1.43it/s]Extractor Predicting: 130it [01:24,  1.43it/s]Extractor Predicting: 131it [01:25,  1.40it/s]Extractor Predicting: 132it [01:25,  1.41it/s]Extractor Predicting: 133it [01:26,  1.41it/s]Extractor Predicting: 134it [01:27,  1.46it/s]Extractor Predicting: 135it [01:28,  1.34it/s]Extractor Predicting: 136it [01:28,  1.36it/s]Extractor Predicting: 137it [01:29,  1.38it/s]Extractor Predicting: 138it [01:30,  1.36it/s]Extractor Predicting: 139it [01:31,  1.38it/s]Extractor Predicting: 140it [01:31,  1.38it/s]Extractor Predicting: 141it [01:32,  1.40it/s]Extractor Predicting: 142it [01:33,  1.42it/s]Extractor Predicting: 143it [01:33,  1.42it/s]Extractor Predicting: 144it [01:34,  1.43it/s]Extractor Predicting: 145it [01:35,  1.42it/s]Extractor Predicting: 146it [01:35,  1.43it/s]Extractor Predicting: 147it [01:36,  1.44it/s]Extractor Predicting: 148it [01:37,  1.40it/s]Extractor Predicting: 149it [01:37,  1.44it/s]Extractor Predicting: 149it [01:37,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:44:22,249 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:44:22,253 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:44:22,253 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:44:22,254 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:44:22,254 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:44:22,885 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:44:22,886 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:44:23,450 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:44:24,508 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:44:24,508 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:44:27,527 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:44:27,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:44:27,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:44:27,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:44:27,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:44:28,180 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:44:28,180 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:44:28,763 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:44:28,929 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:44:28,929 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.17024320457796852,
  "recall": 0.029462738301559793,
  "score": 0.050232165470662726,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.72it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.75it/s]Extractor Predicting: 10it [00:05,  1.74it/s]Extractor Predicting: 11it [00:06,  1.73it/s]Extractor Predicting: 12it [00:07,  1.73it/s]Extractor Predicting: 13it [00:07,  1.81it/s]Extractor Predicting: 14it [00:08,  1.78it/s]Extractor Predicting: 15it [00:08,  1.80it/s]Extractor Predicting: 16it [00:09,  1.80it/s]Extractor Predicting: 17it [00:09,  1.79it/s]Extractor Predicting: 18it [00:10,  1.75it/s]Extractor Predicting: 19it [00:11,  1.71it/s]Extractor Predicting: 20it [00:11,  1.72it/s]Extractor Predicting: 21it [00:12,  1.72it/s]Extractor Predicting: 22it [00:12,  1.75it/s]Extractor Predicting: 23it [00:13,  1.77it/s]Extractor Predicting: 24it [00:13,  1.75it/s]Extractor Predicting: 25it [00:14,  1.77it/s]Extractor Predicting: 26it [00:15,  1.73it/s]Extractor Predicting: 27it [00:15,  1.79it/s]Extractor Predicting: 28it [00:16,  1.78it/s]Extractor Predicting: 29it [00:16,  1.76it/s]Extractor Predicting: 30it [00:17,  1.76it/s]Extractor Predicting: 31it [00:17,  1.78it/s]Extractor Predicting: 32it [00:18,  1.78it/s]Extractor Predicting: 33it [00:19,  1.72it/s]Extractor Predicting: 34it [00:19,  1.69it/s]Extractor Predicting: 35it [00:20,  1.66it/s]Extractor Predicting: 36it [00:20,  1.71it/s]Extractor Predicting: 37it [00:21,  1.69it/s]Extractor Predicting: 38it [00:22,  1.68it/s]Extractor Predicting: 39it [00:22,  1.70it/s]Extractor Predicting: 40it [00:23,  1.68it/s]Extractor Predicting: 41it [00:23,  1.66it/s]Extractor Predicting: 42it [00:24,  1.69it/s]Extractor Predicting: 43it [00:24,  1.72it/s]Extractor Predicting: 44it [00:25,  1.72it/s]Extractor Predicting: 45it [00:26,  1.58it/s]Extractor Predicting: 46it [00:26,  1.56it/s]Extractor Predicting: 47it [00:27,  1.59it/s]Extractor Predicting: 48it [00:28,  1.62it/s]Extractor Predicting: 49it [00:28,  1.65it/s]Extractor Predicting: 50it [00:29,  1.67it/s]Extractor Predicting: 51it [00:29,  1.71it/s]Extractor Predicting: 52it [00:30,  1.69it/s]Extractor Predicting: 53it [00:31,  1.69it/s]Extractor Predicting: 54it [00:31,  1.70it/s]Extractor Predicting: 55it [00:32,  1.67it/s]Extractor Predicting: 56it [00:32,  1.62it/s]Extractor Predicting: 57it [00:33,  1.62it/s]Extractor Predicting: 58it [00:34,  1.65it/s]Extractor Predicting: 59it [00:34,  1.62it/s]Extractor Predicting: 60it [00:35,  1.63it/s]Extractor Predicting: 61it [00:35,  1.63it/s]Extractor Predicting: 62it [00:36,  1.67it/s]Extractor Predicting: 63it [00:37,  1.64it/s]Extractor Predicting: 64it [00:37,  1.52it/s]Extractor Predicting: 65it [00:38,  1.58it/s]Extractor Predicting: 66it [00:39,  1.61it/s]Extractor Predicting: 67it [00:39,  1.61it/s]Extractor Predicting: 68it [00:40,  1.61it/s]Extractor Predicting: 69it [00:40,  1.60it/s]Extractor Predicting: 70it [00:41,  1.58it/s]Extractor Predicting: 71it [00:42,  1.58it/s]Extractor Predicting: 72it [00:42,  1.60it/s]Extractor Predicting: 73it [00:43,  1.59it/s]Extractor Predicting: 74it [00:44,  1.60it/s]Extractor Predicting: 75it [00:44,  1.63it/s]Extractor Predicting: 76it [00:45,  1.61it/s]Extractor Predicting: 77it [00:45,  1.62it/s]Extractor Predicting: 78it [00:46,  1.57it/s]Extractor Predicting: 79it [00:47,  1.60it/s]Extractor Predicting: 80it [00:47,  1.64it/s]Extractor Predicting: 81it [00:48,  1.61it/s]Extractor Predicting: 82it [00:49,  1.59it/s]Extractor Predicting: 83it [00:49,  1.59it/s]Extractor Predicting: 84it [00:50,  1.62it/s]Extractor Predicting: 85it [00:50,  1.61it/s]Extractor Predicting: 86it [00:51,  1.58it/s]Extractor Predicting: 87it [00:52,  1.62it/s]Extractor Predicting: 88it [00:52,  1.64it/s]Extractor Predicting: 89it [00:53,  1.68it/s]Extractor Predicting: 90it [00:53,  1.67it/s]Extractor Predicting: 91it [00:54,  1.66it/s]Extractor Predicting: 92it [00:55,  1.66it/s]Extractor Predicting: 93it [00:55,  1.69it/s]Extractor Predicting: 94it [00:56,  1.70it/s]Extractor Predicting: 95it [00:56,  1.68it/s]Extractor Predicting: 96it [00:57,  1.72it/s]Extractor Predicting: 97it [00:58,  1.74it/s]Extractor Predicting: 98it [00:58,  1.70it/s]Extractor Predicting: 99it [00:59,  1.67it/s]Extractor Predicting: 100it [00:59,  1.70it/s]Extractor Predicting: 101it [01:00,  1.69it/s]Extractor Predicting: 102it [01:01,  1.65it/s]Extractor Predicting: 103it [01:01,  1.63it/s]Extractor Predicting: 104it [01:02,  1.65it/s]Extractor Predicting: 105it [01:02,  1.60it/s]Extractor Predicting: 106it [01:03,  1.63it/s]Extractor Predicting: 107it [01:04,  1.65it/s]Extractor Predicting: 108it [01:04,  1.62it/s]Extractor Predicting: 109it [01:05,  1.62it/s]Extractor Predicting: 110it [01:05,  1.65it/s]Extractor Predicting: 111it [01:06,  1.64it/s]Extractor Predicting: 112it [01:07,  1.64it/s]Extractor Predicting: 113it [01:07,  1.67it/s]Extractor Predicting: 114it [01:08,  1.67it/s]Extractor Predicting: 115it [01:08,  1.67it/s]Extractor Predicting: 116it [01:09,  1.70it/s]Extractor Predicting: 117it [01:10,  1.71it/s]Extractor Predicting: 118it [01:10,  1.75it/s]Extractor Predicting: 119it [01:11,  1.77it/s]Extractor Predicting: 120it [01:11,  1.76it/s]Extractor Predicting: 121it [01:12,  1.75it/s]Extractor Predicting: 122it [01:12,  1.74it/s]Extractor Predicting: 123it [01:13,  1.71it/s]Extractor Predicting: 124it [01:14,  1.73it/s]Extractor Predicting: 125it [01:14,  1.72it/s]Extractor Predicting: 126it [01:15,  1.70it/s]Extractor Predicting: 127it [01:15,  1.66it/s]Extractor Predicting: 128it [01:16,  1.64it/s]Extractor Predicting: 129it [01:17,  1.62it/s]Extractor Predicting: 130it [01:17,  1.60it/s]Extractor Predicting: 131it [01:18,  1.57it/s]Extractor Predicting: 132it [01:19,  1.59it/s]Extractor Predicting: 133it [01:19,  1.60it/s]Extractor Predicting: 134it [01:20,  1.64it/s]Extractor Predicting: 135it [01:21,  1.58it/s]Extractor Predicting: 136it [01:21,  1.56it/s]Extractor Predicting: 137it [01:22,  1.56it/s]Extractor Predicting: 138it [01:22,  1.58it/s]Extractor Predicting: 139it [01:23,  1.58it/s]Extractor Predicting: 140it [01:24,  1.55it/s]Extractor Predicting: 141it [01:24,  1.56it/s]Extractor Predicting: 142it [01:25,  1.56it/s]Extractor Predicting: 143it [01:26,  1.59it/s]Extractor Predicting: 144it [01:26,  1.57it/s]Extractor Predicting: 145it [01:27,  1.56it/s]Extractor Predicting: 146it [01:28,  1.56it/s]Extractor Predicting: 147it [01:28,  1.57it/s]Extractor Predicting: 148it [01:29,  1.58it/s]Extractor Predicting: 149it [01:29,  1.61it/s]Extractor Predicting: 150it [01:30,  1.45it/s]Extractor Predicting: 151it [01:31,  1.48it/s]Extractor Predicting: 152it [01:32,  1.51it/s]Extractor Predicting: 153it [01:32,  1.50it/s]Extractor Predicting: 154it [01:33,  1.51it/s]Extractor Predicting: 155it [01:33,  1.54it/s]Extractor Predicting: 156it [01:34,  1.59it/s]Extractor Predicting: 157it [01:35,  1.66it/s]Extractor Predicting: 158it [01:35,  1.67it/s]Extractor Predicting: 159it [01:36,  1.67it/s]Extractor Predicting: 160it [01:36,  1.68it/s]Extractor Predicting: 161it [01:37,  1.72it/s]Extractor Predicting: 162it [01:38,  1.68it/s]Extractor Predicting: 163it [01:38,  1.66it/s]Extractor Predicting: 164it [01:39,  1.68it/s]Extractor Predicting: 165it [01:39,  1.72it/s]Extractor Predicting: 166it [01:40,  1.71it/s]Extractor Predicting: 167it [01:40,  1.76it/s]Extractor Predicting: 168it [01:41,  1.77it/s]Extractor Predicting: 169it [01:42,  1.80it/s]Extractor Predicting: 170it [01:42,  1.74it/s]Extractor Predicting: 171it [01:43,  1.65it/s]Extractor Predicting: 172it [01:43,  1.60it/s]Extractor Predicting: 173it [01:44,  1.63it/s]Extractor Predicting: 173it [01:44,  1.65it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:46:21,115 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:46:21,121 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:46:21,121 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:46:21,121 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:46:21,122 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:46:21,750 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:46:21,752 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:46:22,323 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:46:23,349 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:46:23,349 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:46:26,310 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:46:26,315 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:46:26,315 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:46:26,315 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:46:26,315 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:46:27,003 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:46:27,004 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:46:27,578 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:46:27,740 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:46:27,740 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.24736842105263157,
  "recall": 0.07935359382537385,
  "score": 0.12016070124178231,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.79it/s]Extractor Predicting: 2it [00:01,  1.70it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:05,  1.46it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.46it/s]Extractor Predicting: 12it [00:08,  1.42it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.56it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:10,  1.73it/s]Extractor Predicting: 17it [00:10,  1.82it/s]Extractor Predicting: 18it [00:11,  1.86it/s]Extractor Predicting: 19it [00:11,  1.87it/s]Extractor Predicting: 20it [00:12,  1.90it/s]Extractor Predicting: 21it [00:12,  1.94it/s]Extractor Predicting: 22it [00:13,  1.90it/s]Extractor Predicting: 23it [00:13,  1.89it/s]Extractor Predicting: 24it [00:14,  1.88it/s]Extractor Predicting: 25it [00:14,  1.88it/s]Extractor Predicting: 26it [00:15,  1.92it/s]Extractor Predicting: 27it [00:15,  1.87it/s]Extractor Predicting: 28it [00:16,  1.90it/s]Extractor Predicting: 29it [00:17,  1.90it/s]Extractor Predicting: 30it [00:17,  1.93it/s]Extractor Predicting: 31it [00:18,  1.96it/s]Extractor Predicting: 32it [00:18,  1.94it/s]Extractor Predicting: 33it [00:19,  1.95it/s]Extractor Predicting: 34it [00:19,  1.95it/s]Extractor Predicting: 35it [00:20,  1.94it/s]Extractor Predicting: 36it [00:20,  1.89it/s]Extractor Predicting: 37it [00:21,  1.86it/s]Extractor Predicting: 38it [00:21,  1.92it/s]Extractor Predicting: 39it [00:22,  1.92it/s]Extractor Predicting: 40it [00:22,  1.91it/s]Extractor Predicting: 41it [00:23,  1.77it/s]Extractor Predicting: 42it [00:23,  1.84it/s]Extractor Predicting: 43it [00:24,  1.83it/s]Extractor Predicting: 44it [00:25,  1.67it/s]Extractor Predicting: 45it [00:25,  1.57it/s]Extractor Predicting: 46it [00:26,  1.51it/s]Extractor Predicting: 47it [00:27,  1.49it/s]Extractor Predicting: 48it [00:27,  1.47it/s]Extractor Predicting: 49it [00:28,  1.46it/s]Extractor Predicting: 50it [00:29,  1.46it/s]Extractor Predicting: 51it [00:30,  1.44it/s]Extractor Predicting: 52it [00:30,  1.42it/s]Extractor Predicting: 53it [00:31,  1.41it/s]Extractor Predicting: 54it [00:32,  1.41it/s]Extractor Predicting: 55it [00:32,  1.44it/s]Extractor Predicting: 56it [00:33,  1.42it/s]Extractor Predicting: 57it [00:34,  1.43it/s]Extractor Predicting: 58it [00:35,  1.44it/s]Extractor Predicting: 59it [00:35,  1.70it/s]Extractor Predicting: 59it [00:35,  1.67it/s]
[INFO|configuration_utils.py:515] 2023-08-28 13:47:04,009 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:47:04,011 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:47:04,015 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:47:04,016 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 13:47:04,019 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:47:07,163 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 13:47:07,166 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 13:47:07,177 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:47:07,178 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:47:07,185 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:47:07,188 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:47:07,188 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:47:07,188 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:47:07,188 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:47:07,188 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:47:07,189 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.776500638569604,
  "recall": 0.19657290656320725,
  "score": 0.3137254901960784,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 13:47:07,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:08,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:08,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:09,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:09,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:10,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:11,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:11,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:12,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:13,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:14,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:14,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:15,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:16,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:16,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:17,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:17,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:18,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:19,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:19,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:20,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:21,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:14<02:08, 14.27s/it][WARNING|generation_utils.py:914] 2023-08-28 13:47:21,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:22,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:22,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:23,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:24,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:24,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:25,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:25,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:26,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:27,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:28,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:28,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:29,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:30,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:30,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:31,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:32,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:33,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:33,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:34,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:34,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:35,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:28<01:54, 14.29s/it][WARNING|generation_utils.py:914] 2023-08-28 13:47:36,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:36,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:37,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:38,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:38,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:39,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:39,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:40,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:41,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:41,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:42,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:43,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:43,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:44,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:45,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:45,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:46,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:47,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:47,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:48,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:41<01:36, 13.75s/it][WARNING|generation_utils.py:914] 2023-08-28 13:47:49,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:49,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:50,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:51,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:51,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:52,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:52,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:53,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:54,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:54,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:55,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:55,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:56,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:57,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:57,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:58,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:59,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:47:59,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:00,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:01,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:02,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:55<01:21, 13.65s/it][WARNING|generation_utils.py:914] 2023-08-28 13:48:02,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:03,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:03,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:04,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:04,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:05,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:06,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:06,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:07,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:08,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:08,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:09,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:09,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:10,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:11,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:11,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:12,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:12,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:13,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:14,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:14,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:08<01:07, 13.45s/it][WARNING|generation_utils.py:914] 2023-08-28 13:48:15,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:16,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:17,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:17,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:18,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:19,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:19,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:20,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:20,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:21,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:21,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:22,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:23,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:23,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:24,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:25,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:25,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:26,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:26,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:27,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:27,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:28,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:29,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:22<00:54, 13.65s/it][WARNING|generation_utils.py:914] 2023-08-28 13:48:29,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:30,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:30,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:31,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:31,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:32,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:33,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:33,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:34,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:35,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:35,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:36,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:36,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:37,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:37,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:38,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:38,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:39,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:40,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:40,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:41,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:41,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:34<00:39, 13.28s/it][WARNING|generation_utils.py:914] 2023-08-28 13:48:42,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:42,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:43,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:44,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:44,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:45,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:45,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:46,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:46,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:47,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:48,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:48,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:49,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:49,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:50,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:51,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:51,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:52,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:53,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:53,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:46<00:25, 12.83s/it][WARNING|generation_utils.py:914] 2023-08-28 13:48:54,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:54,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:55,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:55,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:56,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:57,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:58,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:58,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:59,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:48:59,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:00,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:01,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:01,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:02,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:02,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:03,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:04,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:04,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:05,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:06,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:06,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [01:59<00:12, 12.98s/it][WARNING|generation_utils.py:914] 2023-08-28 13:49:07,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:08,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:08,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:09,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:09,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:10,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:11,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:12,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:12,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:13,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:13,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:14,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:14,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:15,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:16,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:16,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:17,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:17,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:18,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:18,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:19,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:49:20,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:13<00:00, 13.04s/it]Generating: 100%|██████████| 10/10 [02:13<00:00, 13.32s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:26,591 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:26,599 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:26,599 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:26,599 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:26,599 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:49:27,195 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:49:27,195 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:49:27,780 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:49:28,840 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:49:28,840 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:31,716 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:31,722 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:31,722 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:31,722 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:49:31,723 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:49:32,385 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:49:32,386 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:49:32,955 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:49:33,127 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:49:33,127 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : given name . Context : Later in life he was the son of the late Thomas P. P. P. and the mother of George P. P. P. , a friend of Henry P. Bode and Richard R. Bode . Head Entity : Thomas P. P. P. , Tail Entity : George P. P. P. P. .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : given name .', 'success_rate': 0.875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.8565340909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.9453125, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : mother .', 'success_rate': 0.9047619047619048, 'errors': {'', "('first king', 'mother', '', 'After she married the daughter of the first king ( d.')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9270833333333334, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : genre .', 'success_rate': 0.8505434782608695, 'errors': {'', "('Murder', 'genre', '', 'She played a supporting role on the hit sitcom , Murder ( starring Gene Kelly and Jon Voight ) on the cable network KFI in 1999 based on her role as Susan Kaye .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.8792613636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('compilation', 'is a list of', '', 'It is a compilation of all of the .')", "('a nation', 'is a list of', '', 'These are alphabetical order of the official languages of a nation or state .')", "('PHP', 'is a list of', '', 'The database contains information on languages used in the web ( PHP , Ruby , Python , Python 2 . 3 , Python 3. 3.4 and the Linux kernel ) .')"}}
['Relation : located on astronomical body . Context : The Moon is located in the southern hemisphere , and a binary star system , located at the equator , is named after it . Head Entity : Moon , Tail Entity : Sun .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.9515625, 'errors': {''}}
['Relation : manufacturer . Context : The ZX Spectrum ( ZX Spectrum ) is a compact integrated digital audio system manufactured by the Hewlett Packard Company for use by the Hewlett Packard Company , Inc. ( HP ) microprocessor division . Head Entity : Hewlett Packard Company , Tail Entity : Hewlett Packard Corporation .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9196428571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : member of .', 'success_rate': 0.859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/4_ext.jsonl'}}
estimate vocab size: 7594
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7694, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.63it/s]Extractor Estimating: 2it [00:01,  1.57it/s]Extractor Estimating: 3it [00:01,  1.65it/s]Extractor Estimating: 4it [00:02,  1.57it/s]Extractor Estimating: 5it [00:03,  1.61it/s]Extractor Estimating: 6it [00:03,  1.60it/s]Extractor Estimating: 7it [00:04,  1.61it/s]Extractor Estimating: 8it [00:04,  1.61it/s]Extractor Estimating: 9it [00:05,  1.59it/s]Extractor Estimating: 10it [00:06,  1.58it/s]Extractor Estimating: 11it [00:06,  1.60it/s]Extractor Estimating: 12it [00:07,  1.59it/s]Extractor Estimating: 13it [00:08,  1.64it/s]Extractor Estimating: 14it [00:08,  1.53it/s]Extractor Estimating: 15it [00:09,  1.53it/s]Extractor Estimating: 16it [00:10,  1.57it/s]Extractor Estimating: 17it [00:10,  1.59it/s]Extractor Estimating: 18it [00:11,  1.57it/s]Extractor Estimating: 19it [00:12,  1.55it/s]Extractor Estimating: 20it [00:12,  1.56it/s]Extractor Estimating: 21it [00:13,  1.54it/s]Extractor Estimating: 22it [00:13,  1.53it/s]Extractor Estimating: 23it [00:14,  1.57it/s]Extractor Estimating: 24it [00:15,  1.60it/s]Extractor Estimating: 25it [00:15,  1.61it/s]Extractor Estimating: 26it [00:16,  1.67it/s]Extractor Estimating: 27it [00:16,  1.66it/s]Extractor Estimating: 28it [00:17,  1.68it/s]Extractor Estimating: 29it [00:18,  1.72it/s]Extractor Estimating: 30it [00:18,  1.65it/s]Extractor Estimating: 31it [00:19,  1.64it/s]Extractor Estimating: 32it [00:19,  1.70it/s]Extractor Estimating: 33it [00:20,  1.75it/s]Extractor Estimating: 34it [00:20,  1.83it/s]Extractor Estimating: 35it [00:21,  1.84it/s]Extractor Estimating: 36it [00:22,  1.81it/s]Extractor Estimating: 37it [00:22,  1.56it/s]Extractor Estimating: 38it [00:23,  1.68it/s]Extractor Estimating: 39it [00:23,  1.65it/s]Extractor Estimating: 40it [00:24,  1.72it/s]Extractor Estimating: 41it [00:25,  1.70it/s]Extractor Estimating: 42it [00:25,  1.74it/s]Extractor Estimating: 43it [00:26,  1.73it/s]Extractor Estimating: 44it [00:26,  1.80it/s]Extractor Estimating: 45it [00:27,  1.73it/s]Extractor Estimating: 46it [00:27,  1.74it/s]Extractor Estimating: 47it [00:28,  1.71it/s]Extractor Estimating: 48it [00:29,  1.76it/s]Extractor Estimating: 49it [00:29,  1.73it/s]Extractor Estimating: 50it [00:30,  1.78it/s]Extractor Estimating: 51it [00:30,  1.72it/s]Extractor Estimating: 52it [00:31,  1.73it/s]Extractor Estimating: 53it [00:32,  1.67it/s]Extractor Estimating: 54it [00:32,  1.64it/s]Extractor Estimating: 55it [00:33,  1.62it/s]Extractor Estimating: 56it [00:34,  1.58it/s]Extractor Estimating: 57it [00:34,  1.62it/s]Extractor Estimating: 58it [00:35,  1.61it/s]Extractor Estimating: 59it [00:35,  1.58it/s]Extractor Estimating: 60it [00:36,  1.58it/s]Extractor Estimating: 61it [00:37,  1.58it/s]Extractor Estimating: 62it [00:37,  1.61it/s]Extractor Estimating: 63it [00:38,  1.57it/s]Extractor Estimating: 64it [00:39,  1.51it/s]Extractor Estimating: 65it [00:39,  1.53it/s]Extractor Estimating: 66it [00:40,  1.61it/s]Extractor Estimating: 67it [00:40,  1.61it/s]Extractor Estimating: 68it [00:41,  1.56it/s]Extractor Estimating: 69it [00:42,  1.55it/s]Extractor Estimating: 70it [00:42,  1.55it/s]Extractor Estimating: 71it [00:43,  1.53it/s]Extractor Estimating: 72it [00:44,  1.57it/s]Extractor Estimating: 73it [00:44,  1.59it/s]Extractor Estimating: 74it [00:45,  1.50it/s]Extractor Estimating: 75it [00:46,  1.51it/s]Extractor Estimating: 76it [00:46,  1.56it/s]Extractor Estimating: 77it [00:47,  1.54it/s]Extractor Estimating: 78it [00:48,  1.53it/s]Extractor Estimating: 79it [00:48,  1.50it/s]Extractor Estimating: 80it [00:49,  1.56it/s]Extractor Estimating: 81it [00:49,  1.59it/s]Extractor Estimating: 82it [00:50,  1.59it/s]Extractor Estimating: 83it [00:51,  1.45it/s]Extractor Estimating: 84it [00:52,  1.48it/s]Extractor Estimating: 85it [00:52,  1.52it/s]Extractor Estimating: 86it [00:53,  1.56it/s]Extractor Estimating: 87it [00:53,  1.59it/s]Extractor Estimating: 88it [00:54,  1.62it/s]Extractor Estimating: 89it [00:55,  1.63it/s]Extractor Estimating: 90it [00:55,  1.59it/s]Extractor Estimating: 91it [00:56,  1.56it/s]Extractor Estimating: 92it [00:57,  1.53it/s]Extractor Estimating: 93it [00:57,  1.54it/s]Extractor Estimating: 94it [00:58,  1.54it/s]Extractor Estimating: 95it [00:59,  1.52it/s]Extractor Estimating: 96it [00:59,  1.49it/s]Extractor Estimating: 97it [01:00,  1.52it/s]Extractor Estimating: 98it [01:01,  1.55it/s]Extractor Estimating: 99it [01:01,  1.60it/s]Extractor Estimating: 100it [01:02,  1.61it/s]Extractor Estimating: 101it [01:02,  1.61it/s]Extractor Estimating: 102it [01:03,  1.63it/s]Extractor Estimating: 103it [01:04,  1.63it/s]Extractor Estimating: 104it [01:04,  1.67it/s]Extractor Estimating: 105it [01:05,  1.66it/s]Extractor Estimating: 106it [01:05,  1.66it/s]Extractor Estimating: 107it [01:06,  1.64it/s]Extractor Estimating: 108it [01:07,  1.63it/s]Extractor Estimating: 109it [01:07,  1.67it/s]Extractor Estimating: 110it [01:08,  1.67it/s]Extractor Estimating: 111it [01:08,  1.67it/s]Extractor Estimating: 112it [01:09,  1.68it/s]Extractor Estimating: 113it [01:10,  1.72it/s]Extractor Estimating: 114it [01:10,  1.71it/s]Extractor Estimating: 115it [01:11,  1.71it/s]Extractor Estimating: 116it [01:11,  1.66it/s]Extractor Estimating: 117it [01:12,  1.70it/s]Extractor Estimating: 118it [01:12,  1.67it/s]Extractor Estimating: 119it [01:13,  1.69it/s]Extractor Estimating: 120it [01:14,  1.71it/s]Extractor Estimating: 121it [01:14,  1.68it/s]Extractor Estimating: 122it [01:15,  1.67it/s]Extractor Estimating: 123it [01:16,  1.61it/s]Extractor Estimating: 124it [01:16,  1.63it/s]Extractor Estimating: 125it [01:17,  1.64it/s]Extractor Estimating: 126it [01:17,  1.63it/s]Extractor Estimating: 127it [01:18,  1.63it/s]Extractor Estimating: 128it [01:19,  1.60it/s]Extractor Estimating: 129it [01:19,  1.64it/s]Extractor Estimating: 130it [01:20,  1.65it/s]Extractor Estimating: 131it [01:20,  1.63it/s]Extractor Estimating: 132it [01:21,  1.63it/s]Extractor Estimating: 133it [01:22,  1.59it/s]Extractor Estimating: 134it [01:22,  1.60it/s]Extractor Estimating: 135it [01:23,  1.58it/s]Extractor Estimating: 136it [01:24,  1.60it/s]Extractor Estimating: 137it [01:24,  1.61it/s]Extractor Estimating: 138it [01:25,  1.64it/s]Extractor Estimating: 139it [01:25,  1.65it/s]Extractor Estimating: 140it [01:26,  1.58it/s]Extractor Estimating: 141it [01:27,  1.59it/s]Extractor Estimating: 142it [01:27,  1.59it/s]Extractor Estimating: 143it [01:28,  1.57it/s]Extractor Estimating: 144it [01:29,  1.58it/s]Extractor Estimating: 145it [01:29,  1.55it/s]Extractor Estimating: 146it [01:30,  1.56it/s]Extractor Estimating: 147it [01:31,  1.55it/s]Extractor Estimating: 148it [01:31,  1.55it/s]Extractor Estimating: 149it [01:32,  1.62it/s]Extractor Estimating: 150it [01:32,  1.60it/s]Extractor Estimating: 151it [01:33,  1.69it/s]Extractor Estimating: 152it [01:33,  1.74it/s]Extractor Estimating: 153it [01:34,  1.81it/s]Extractor Estimating: 154it [01:34,  1.86it/s]Extractor Estimating: 155it [01:35,  1.82it/s]Extractor Estimating: 156it [01:36,  1.83it/s]Extractor Estimating: 157it [01:36,  1.79it/s]Extractor Estimating: 158it [01:37,  1.88it/s]Extractor Estimating: 159it [01:37,  1.78it/s]Extractor Estimating: 160it [01:38,  1.79it/s]Extractor Estimating: 161it [01:38,  1.84it/s]Extractor Estimating: 162it [01:39,  1.66it/s]Extractor Estimating: 163it [01:40,  1.70it/s]Extractor Estimating: 164it [01:40,  1.77it/s]Extractor Estimating: 165it [01:41,  1.77it/s]Extractor Estimating: 166it [01:41,  1.78it/s]Extractor Estimating: 167it [01:42,  1.85it/s]Extractor Estimating: 168it [01:42,  1.83it/s]Extractor Estimating: 169it [01:43,  1.86it/s]Extractor Estimating: 170it [01:43,  1.80it/s]Extractor Estimating: 171it [01:44,  1.79it/s]Extractor Estimating: 172it [01:44,  1.84it/s]Extractor Estimating: 173it [01:45,  1.91it/s]Extractor Estimating: 174it [01:45,  1.92it/s]Extractor Estimating: 175it [01:46,  1.97it/s]Extractor Estimating: 176it [01:47,  1.89it/s]Extractor Estimating: 177it [01:47,  1.83it/s]Extractor Estimating: 178it [01:48,  1.76it/s]Extractor Estimating: 179it [01:48,  1.75it/s]Extractor Estimating: 180it [01:49,  1.71it/s]Extractor Estimating: 181it [01:50,  1.71it/s]Extractor Estimating: 182it [01:50,  1.72it/s]Extractor Estimating: 183it [01:51,  1.73it/s]Extractor Estimating: 184it [01:51,  1.69it/s]Extractor Estimating: 185it [01:52,  1.71it/s]Extractor Estimating: 186it [01:52,  1.73it/s]Extractor Estimating: 187it [01:53,  1.65it/s]Extractor Estimating: 188it [01:54,  1.66it/s]Extractor Estimating: 189it [01:54,  1.67it/s]Extractor Estimating: 190it [01:55,  1.69it/s]Extractor Estimating: 191it [01:55,  1.72it/s]Extractor Estimating: 192it [01:56,  1.73it/s]Extractor Estimating: 193it [01:57,  1.65it/s]Extractor Estimating: 194it [01:57,  1.69it/s]Extractor Estimating: 195it [01:58,  1.70it/s]Extractor Estimating: 196it [01:58,  1.73it/s]Extractor Estimating: 197it [01:59,  1.70it/s]Extractor Estimating: 198it [02:00,  1.67it/s]Extractor Estimating: 199it [02:00,  1.70it/s]Extractor Estimating: 200it [02:01,  1.69it/s]Extractor Estimating: 201it [02:01,  1.66it/s]Extractor Estimating: 202it [02:02,  1.72it/s]Extractor Estimating: 203it [02:02,  1.71it/s]Extractor Estimating: 204it [02:03,  1.76it/s]Extractor Estimating: 205it [02:04,  1.69it/s]Extractor Estimating: 206it [02:04,  1.72it/s]Extractor Estimating: 207it [02:05,  1.75it/s]Extractor Estimating: 208it [02:05,  1.76it/s]Extractor Estimating: 209it [02:06,  1.81it/s]Extractor Estimating: 210it [02:06,  1.75it/s]Extractor Estimating: 211it [02:07,  1.74it/s]Extractor Estimating: 212it [02:08,  1.73it/s]Extractor Estimating: 213it [02:08,  1.74it/s]Extractor Estimating: 214it [02:09,  1.74it/s]Extractor Estimating: 215it [02:09,  1.75it/s]Extractor Estimating: 216it [02:10,  1.79it/s]Extractor Estimating: 217it [02:10,  1.82it/s]Extractor Estimating: 218it [02:11,  1.74it/s]Extractor Estimating: 219it [02:12,  1.75it/s]Extractor Estimating: 220it [02:12,  1.72it/s]Extractor Estimating: 221it [02:13,  1.76it/s]Extractor Estimating: 222it [02:13,  1.64it/s]Extractor Estimating: 223it [02:14,  1.68it/s]Extractor Estimating: 224it [02:15,  1.64it/s]Extractor Estimating: 225it [02:15,  1.66it/s]Extractor Estimating: 226it [02:16,  1.58it/s]Extractor Estimating: 227it [02:16,  1.64it/s]Extractor Estimating: 228it [02:17,  1.67it/s]Extractor Estimating: 229it [02:18,  1.64it/s]Extractor Estimating: 230it [02:18,  1.66it/s]Extractor Estimating: 231it [02:19,  1.68it/s]Extractor Estimating: 232it [02:19,  1.68it/s]Extractor Estimating: 233it [02:20,  1.67it/s]Extractor Estimating: 234it [02:21,  1.71it/s]Extractor Estimating: 235it [02:21,  1.72it/s]Extractor Estimating: 236it [02:22,  1.71it/s]Extractor Estimating: 237it [02:22,  1.71it/s]Extractor Estimating: 238it [02:23,  1.74it/s]Extractor Estimating: 239it [02:23,  1.74it/s]Extractor Estimating: 240it [02:24,  1.73it/s]Extractor Estimating: 241it [02:25,  1.72it/s]Extractor Estimating: 242it [02:25,  1.72it/s]Extractor Estimating: 243it [02:26,  1.72it/s]Extractor Estimating: 244it [02:26,  1.74it/s]Extractor Estimating: 245it [02:27,  1.73it/s]Extractor Estimating: 246it [02:28,  1.74it/s]Extractor Estimating: 247it [02:28,  1.72it/s]Extractor Estimating: 248it [02:29,  1.74it/s]Extractor Estimating: 249it [02:29,  1.70it/s]Extractor Estimating: 250it [02:30,  1.72it/s]Extractor Estimating: 250it [02:30,  1.66it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:52:18,618 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:52:18,620 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:52:18,621 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:52:18,621 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:52:18,621 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:52:18,917 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:52:18,918 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:52:19,278 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:52:20,309 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:52:20,309 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:52:22,030 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:52:22,036 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:52:22,036 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:52:22,036 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:52:22,036 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:52:22,700 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:52:22,701 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:52:23,288 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:52:23,449 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:52:23,450 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 15:20:48,382 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 15:20:48,419 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 4995 mean pseudo reward: 0.9191389217509037
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 16694
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16794, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16794, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.014, loss:562.7665
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.021, loss:522.1276
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 1.008, loss:471.6665
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 1.013, loss:483.9535
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 1.011, loss:445.0048
>> valid entity prec:0.5444, rec:0.4602, f1:0.4988
>> valid relation prec:0.2089, rec:0.0327, f1:0.0566
>> valid relation with NER prec:0.2089, rec:0.0327, f1:0.0566
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 2.339, loss:451.2496
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 1.009, loss:438.7471
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 1.018, loss:428.0266
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 1.016, loss:418.0863
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 1.012, loss:438.4315
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5163, rec:0.4782, f1:0.4965
>> valid relation prec:0.1907, rec:0.0285, f1:0.0496
>> valid relation with NER prec:0.1907, rec:0.0285, f1:0.0496
g_step 1100, step 55, avg_time 2.355, loss:390.3063
g_step 1200, step 155, avg_time 1.020, loss:429.0527
g_step 1300, step 46, avg_time 1.003, loss:395.3606
g_step 1400, step 146, avg_time 1.025, loss:371.8362
g_step 1500, step 37, avg_time 1.014, loss:395.1455
>> valid entity prec:0.4821, rec:0.4829, f1:0.4825
>> valid relation prec:0.1283, rec:0.0488, f1:0.0707
>> valid relation with NER prec:0.1283, rec:0.0488, f1:0.0707
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 137, avg_time 2.349, loss:378.5598
g_step 1700, step 28, avg_time 1.004, loss:369.9194
g_step 1800, step 128, avg_time 1.009, loss:363.5327
g_step 1900, step 19, avg_time 1.018, loss:366.5061
g_step 2000, step 119, avg_time 1.026, loss:344.8662
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5286, rec:0.3954, f1:0.4524
>> valid relation prec:0.1529, rec:0.0330, f1:0.0542
>> valid relation with NER prec:0.1529, rec:0.0330, f1:0.0542
g_step 2100, step 10, avg_time 2.338, loss:364.3808
g_step 2200, step 110, avg_time 1.018, loss:325.4153
g_step 2300, step 1, avg_time 1.026, loss:349.9218
g_step 2400, step 101, avg_time 1.007, loss:310.1046
g_step 2500, step 201, avg_time 1.023, loss:341.5239
>> valid entity prec:0.5378, rec:0.4124, f1:0.4668
>> valid relation prec:0.1555, rec:0.0345, f1:0.0564
>> valid relation with NER prec:0.1555, rec:0.0345, f1:0.0564
g_step 2600, step 92, avg_time 2.338, loss:308.4800
g_step 2700, step 192, avg_time 1.017, loss:316.3330
g_step 2800, step 83, avg_time 1.012, loss:307.0578
g_step 2900, step 183, avg_time 1.019, loss:306.8360
g_step 3000, step 74, avg_time 1.006, loss:289.9125
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5109, rec:0.4297, f1:0.4668
>> valid relation prec:0.1657, rec:0.0558, f1:0.0835
>> valid relation with NER prec:0.1657, rec:0.0558, f1:0.0835
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 174, avg_time 2.348, loss:320.3293
g_step 3200, step 65, avg_time 1.001, loss:287.3711
g_step 3300, step 165, avg_time 1.017, loss:294.6464
g_step 3400, step 56, avg_time 1.004, loss:285.3993
g_step 3500, step 156, avg_time 1.021, loss:291.3925
>> valid entity prec:0.5460, rec:0.3654, f1:0.4378
>> valid relation prec:0.1346, rec:0.0273, f1:0.0454
>> valid relation with NER prec:0.1346, rec:0.0273, f1:0.0454
g_step 3600, step 47, avg_time 2.352, loss:285.3104
g_step 3700, step 147, avg_time 1.005, loss:270.0840
g_step 3800, step 38, avg_time 1.005, loss:266.1129
g_step 3900, step 138, avg_time 1.015, loss:269.6862
g_step 4000, step 29, avg_time 1.016, loss:268.7262
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5178, rec:0.3676, f1:0.4300
>> valid relation prec:0.1219, rec:0.0332, f1:0.0522
>> valid relation with NER prec:0.1219, rec:0.0332, f1:0.0522
g_step 4100, step 129, avg_time 2.355, loss:248.0954
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 15:20:48 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 15:20:48 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_15-20-48_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 15:20:49 - WARNING - datasets.builder -   Using custom data configuration default-2219f0ed54922568
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-2219f0ed54922568/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 15:20:49,708 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:20:49,709 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 15:20:49,709 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:20:49,710 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 15:20:49,724 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:20:49,730 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:20:49,730 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:20:49,730 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:20:49,730 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:20:49,730 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:20:49,730 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 15:20:49,889 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 15:20:52,987 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 15:20:52,992 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-2219f0ed54922568/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.28ba/s] 40%|████      | 2/5 [00:00<00:00,  3.19ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.80ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.21ba/s]100%|██████████| 5/5 [00:01<00:00,  4.46ba/s]100%|██████████| 5/5 [00:01<00:00,  4.07ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.20ba/s] 40%|████      | 2/5 [00:00<00:00,  4.36ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.41ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.43ba/s]100%|██████████| 5/5 [00:00<00:00,  5.42ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.86ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.17ba/s]100%|██████████| 5/5 [00:00<00:00, 10.58ba/s]100%|██████████| 5/5 [00:00<00:00, 10.39ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.49ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.61ba/s]100%|██████████| 5/5 [00:00<00:00, 13.86ba/s]100%|██████████| 5/5 [00:00<00:00, 12.86ba/s]
[INFO|trainer.py:414] 2023-08-28 15:20:56,410 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 15:20:56,423 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 15:20:56,423 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 15:20:56,423 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 15:20:56,423 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 15:20:56,423 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 15:20:56,423 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 15:20:56,423 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:56,  3.33it/s]  1%|          | 2/390 [00:00<01:54,  3.40it/s]  1%|          | 3/390 [00:00<01:53,  3.42it/s]  1%|          | 4/390 [00:01<01:52,  3.43it/s]  1%|▏         | 5/390 [00:01<01:52,  3.44it/s]  2%|▏         | 6/390 [00:01<01:51,  3.44it/s]  2%|▏         | 7/390 [00:02<01:51,  3.44it/s]  2%|▏         | 8/390 [00:02<01:50,  3.44it/s]  2%|▏         | 9/390 [00:02<01:50,  3.44it/s]  3%|▎         | 10/390 [00:02<01:50,  3.45it/s]  3%|▎         | 11/390 [00:03<01:51,  3.41it/s]  3%|▎         | 12/390 [00:03<01:50,  3.42it/s]  3%|▎         | 13/390 [00:03<01:49,  3.43it/s]  4%|▎         | 14/390 [00:04<01:49,  3.43it/s]  4%|▍         | 15/390 [00:04<01:49,  3.43it/s]  4%|▍         | 16/390 [00:04<01:48,  3.44it/s]  4%|▍         | 17/390 [00:04<01:48,  3.44it/s]  5%|▍         | 18/390 [00:05<01:48,  3.44it/s]  5%|▍         | 19/390 [00:05<01:47,  3.44it/s]  5%|▌         | 20/390 [00:05<01:47,  3.44it/s]  5%|▌         | 21/390 [00:06<01:47,  3.44it/s]  6%|▌         | 22/390 [00:06<01:47,  3.42it/s]  6%|▌         | 23/390 [00:06<01:46,  3.43it/s]  6%|▌         | 24/390 [00:06<01:46,  3.43it/s]  6%|▋         | 25/390 [00:07<01:46,  3.44it/s]  7%|▋         | 26/390 [00:07<01:45,  3.44it/s]  7%|▋         | 27/390 [00:07<01:45,  3.44it/s]  7%|▋         | 28/390 [00:08<01:45,  3.44it/s]  7%|▋         | 29/390 [00:08<01:44,  3.44it/s]  8%|▊         | 30/390 [00:08<01:44,  3.44it/s]  8%|▊         | 31/390 [00:09<01:44,  3.44it/s]  8%|▊         | 32/390 [00:09<01:44,  3.43it/s]  8%|▊         | 33/390 [00:09<01:44,  3.42it/s]  9%|▊         | 34/390 [00:09<01:43,  3.43it/s]  9%|▉         | 35/390 [00:10<01:43,  3.43it/s]  9%|▉         | 36/390 [00:10<01:43,  3.44it/s]  9%|▉         | 37/390 [00:10<01:42,  3.44it/s] 10%|▉         | 38/390 [00:11<01:42,  3.44it/s] 10%|█         | 39/390 [00:11<01:42,  3.44it/s] 10%|█         | 40/390 [00:11<01:41,  3.44it/s] 11%|█         | 41/390 [00:11<01:41,  3.44it/s] 11%|█         | 42/390 [00:12<01:41,  3.44it/s] 11%|█         | 43/390 [00:12<01:40,  3.44it/s] 11%|█▏        | 44/390 [00:12<01:57,  2.96it/s] 12%|█▏        | 45/390 [00:13<01:51,  3.09it/s] 12%|█▏        | 46/390 [00:13<01:48,  3.18it/s] 12%|█▏        | 47/390 [00:13<01:45,  3.26it/s] 12%|█▏        | 48/390 [00:14<01:43,  3.31it/s] 13%|█▎        | 49/390 [00:14<01:41,  3.35it/s] 13%|█▎        | 50/390 [00:14<01:40,  3.37it/s] 13%|█▎        | 51/390 [00:15<01:39,  3.40it/s] 13%|█▎        | 52/390 [00:15<01:39,  3.41it/s] 14%|█▎        | 53/390 [00:15<01:38,  3.42it/s] 14%|█▍        | 54/390 [00:15<01:38,  3.40it/s] 14%|█▍        | 55/390 [00:16<01:38,  3.41it/s] 14%|█▍        | 56/390 [00:16<01:37,  3.42it/s] 15%|█▍        | 57/390 [00:16<01:37,  3.42it/s] 15%|█▍        | 58/390 [00:17<01:36,  3.43it/s] 15%|█▌        | 59/390 [00:17<01:36,  3.43it/s] 15%|█▌        | 60/390 [00:17<01:36,  3.43it/s] 16%|█▌        | 61/390 [00:17<01:35,  3.44it/s] 16%|█▌        | 62/390 [00:18<01:35,  3.44it/s] 16%|█▌        | 63/390 [00:18<01:35,  3.44it/s] 16%|█▋        | 64/390 [00:18<01:34,  3.44it/s] 17%|█▋        | 65/390 [00:19<01:34,  3.42it/s] 17%|█▋        | 66/390 [00:19<01:34,  3.43it/s] 17%|█▋        | 67/390 [00:19<01:34,  3.43it/s] 17%|█▋        | 68/390 [00:19<01:33,  3.43it/s] 18%|█▊        | 69/390 [00:20<01:33,  3.43it/s] 18%|█▊        | 70/390 [00:20<01:33,  3.43it/s] 18%|█▊        | 71/390 [00:20<01:32,  3.43it/s] 18%|█▊        | 72/390 [00:21<01:32,  3.43it/s] 19%|█▊        | 73/390 [00:21<01:32,  3.44it/s] 19%|█▉        | 74/390 [00:21<01:31,  3.44it/s] 19%|█▉        | 75/390 [00:21<01:31,  3.44it/s] 19%|█▉        | 76/390 [00:22<01:32,  3.41it/s] 20%|█▉        | 77/390 [00:22<01:31,  3.42it/s] 20%|██        | 78/390 [00:22<01:31,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 15:21:19,337 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:21:19,337 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 15:21:19,337 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.40it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.81it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.11it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.35it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.97it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.72it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.49it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.33it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.39it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.57it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.43it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.30it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.24it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.24it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.28it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.07it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.28it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.30it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.47it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.19it/s][A
 21%|██        | 107/505 [00:02<00:09, 43.99it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.10it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.17it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.14it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.17it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.26it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.29it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.39it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.34it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.26it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.19it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.22it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.16it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.18it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.30it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.27it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.31it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.36it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.18it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.21it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.16it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.17it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.30it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.27it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.32it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.27it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.20it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.24it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.24it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.12it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.09it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.23it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.29it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.23it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.29it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.31it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.23it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.22it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.16it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.13it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.22it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.20it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.33it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.25it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.26it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.21it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.23it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.16it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.22it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.19it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.19it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.31it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.22it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.28it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.21it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.28it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.21it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.29it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.31it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.08it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.24it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.24it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.24it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.13it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.14it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.29it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.28it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.25it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.15it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.24it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.29it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.25it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.16it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.05it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.22it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.22it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.26it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.25it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.28it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.21it/s][A
                                                 [A                                                
100%|██████████| 505/505 [00:11<00:00, 44.21it/s][A 20%|██        | 78/390 [00:34<01:31,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:21:30,776 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 15:21:30,799 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:21:32,801 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:21:32,821 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:21:32,831 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:41<29:38,  5.72s/it] 21%|██        | 80/390 [00:41<21:09,  4.10s/it] 21%|██        | 81/390 [00:41<15:13,  2.96s/it] 21%|██        | 82/390 [00:42<11:04,  2.16s/it] 21%|██▏       | 83/390 [00:42<08:10,  1.60s/it] 22%|██▏       | 84/390 [00:42<06:09,  1.21s/it] 22%|██▏       | 85/390 [00:43<04:44,  1.07it/s] 22%|██▏       | 86/390 [00:43<03:45,  1.35it/s] 22%|██▏       | 87/390 [00:43<03:04,  1.65it/s] 23%|██▎       | 88/390 [00:43<02:35,  1.95it/s] 23%|██▎       | 89/390 [00:44<02:14,  2.23it/s] 23%|██▎       | 90/390 [00:44<02:00,  2.49it/s] 23%|██▎       | 91/390 [00:44<01:50,  2.70it/s] 24%|██▎       | 92/390 [00:45<01:43,  2.87it/s] 24%|██▍       | 93/390 [00:45<01:38,  3.01it/s] 24%|██▍       | 94/390 [00:45<01:35,  3.11it/s] 24%|██▍       | 95/390 [00:45<01:32,  3.19it/s] 25%|██▍       | 96/390 [00:46<01:30,  3.25it/s] 25%|██▍       | 97/390 [00:46<01:29,  3.29it/s] 25%|██▌       | 98/390 [00:46<01:27,  3.32it/s] 25%|██▌       | 99/390 [00:47<01:27,  3.34it/s] 26%|██▌       | 100/390 [00:47<01:26,  3.35it/s] 26%|██▌       | 101/390 [00:47<01:25,  3.37it/s] 26%|██▌       | 102/390 [00:48<01:25,  3.38it/s] 26%|██▋       | 103/390 [00:48<01:24,  3.39it/s] 27%|██▋       | 104/390 [00:48<01:23,  3.41it/s] 27%|██▋       | 105/390 [00:48<01:23,  3.42it/s] 27%|██▋       | 106/390 [00:49<01:22,  3.42it/s] 27%|██▋       | 107/390 [00:49<01:22,  3.43it/s] 28%|██▊       | 108/390 [00:49<01:22,  3.43it/s] 28%|██▊       | 109/390 [00:50<01:21,  3.43it/s] 28%|██▊       | 110/390 [00:50<01:21,  3.43it/s] 28%|██▊       | 111/390 [00:50<01:21,  3.43it/s] 29%|██▊       | 112/390 [00:50<01:20,  3.43it/s] 29%|██▉       | 113/390 [00:51<01:21,  3.40it/s] 29%|██▉       | 114/390 [00:51<01:20,  3.42it/s] 29%|██▉       | 115/390 [00:51<01:20,  3.42it/s] 30%|██▉       | 116/390 [00:52<01:21,  3.35it/s] 30%|███       | 117/390 [00:52<01:20,  3.38it/s] 30%|███       | 118/390 [00:52<01:20,  3.39it/s] 31%|███       | 119/390 [00:53<01:19,  3.41it/s] 31%|███       | 120/390 [00:53<01:19,  3.42it/s] 31%|███       | 121/390 [00:53<01:18,  3.42it/s] 31%|███▏      | 122/390 [00:53<01:18,  3.42it/s] 32%|███▏      | 123/390 [00:54<01:17,  3.43it/s] 32%|███▏      | 124/390 [00:54<01:17,  3.42it/s] 32%|███▏      | 125/390 [00:54<01:17,  3.42it/s] 32%|███▏      | 126/390 [00:55<01:17,  3.43it/s] 33%|███▎      | 127/390 [00:55<01:16,  3.43it/s] 33%|███▎      | 128/390 [00:55<01:16,  3.43it/s] 33%|███▎      | 129/390 [00:55<01:16,  3.43it/s] 33%|███▎      | 130/390 [00:56<01:15,  3.43it/s] 34%|███▎      | 131/390 [00:56<01:15,  3.43it/s] 34%|███▍      | 132/390 [00:56<01:15,  3.43it/s] 34%|███▍      | 133/390 [00:57<01:14,  3.43it/s] 34%|███▍      | 134/390 [00:57<01:14,  3.43it/s] 35%|███▍      | 135/390 [00:57<01:14,  3.43it/s] 35%|███▍      | 136/390 [00:57<01:13,  3.44it/s] 35%|███▌      | 137/390 [00:58<01:13,  3.44it/s] 35%|███▌      | 138/390 [00:58<01:13,  3.44it/s] 36%|███▌      | 139/390 [00:58<01:13,  3.43it/s] 36%|███▌      | 140/390 [00:59<01:12,  3.43it/s] 36%|███▌      | 141/390 [00:59<01:12,  3.43it/s] 36%|███▋      | 142/390 [00:59<01:12,  3.43it/s] 37%|███▋      | 143/390 [01:00<01:11,  3.43it/s] 37%|███▋      | 144/390 [01:00<01:11,  3.42it/s] 37%|███▋      | 145/390 [01:00<01:11,  3.42it/s] 37%|███▋      | 146/390 [01:00<01:11,  3.43it/s] 38%|███▊      | 147/390 [01:01<01:10,  3.43it/s] 38%|███▊      | 148/390 [01:01<01:10,  3.43it/s] 38%|███▊      | 149/390 [01:01<01:10,  3.43it/s] 38%|███▊      | 150/390 [01:02<01:09,  3.43it/s] 39%|███▊      | 151/390 [01:02<01:09,  3.43it/s] 39%|███▉      | 152/390 [01:02<01:09,  3.43it/s] 39%|███▉      | 153/390 [01:02<01:09,  3.43it/s] 39%|███▉      | 154/390 [01:03<01:08,  3.43it/s] 40%|███▉      | 155/390 [01:03<01:10,  3.35it/s] 40%|████      | 156/390 [01:03<01:09,  3.37it/s][INFO|trainer.py:2140] 2023-08-28 15:22:00,303 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:22:00,303 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 15:22:00,304 >>   Batch size = 8
{'eval_loss': 1.1094993352890015, 'eval_runtime': 11.4246, 'eval_samples_per_second': 353.535, 'eval_steps_per_second': 44.203, 'epoch': 0.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.03it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.57it/s][A
  3%|▎         | 17/505 [00:00<00:10, 45.97it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.08it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.65it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.58it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.49it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.26it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.28it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.64it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.50it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.38it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.06it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.13it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.17it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.16it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.24it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.23it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.43it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.38it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.16it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.01it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.08it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.06it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.18it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.16it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.25it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.35it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.35it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.23it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.17it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.16it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.17it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.16it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.17it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.28it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.32it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.30it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.19it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.20it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.19it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.08it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.18it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.23it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.25it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.29it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.24it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.21it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.23it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.14it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.20it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.13it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.17it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.26it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.24it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 43.98it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.10it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.04it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 43.93it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.10it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.21it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.24it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.18it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.29it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.31it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.26it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.07it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.11it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.14it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.21it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.17it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.21it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.17it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.19it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.11it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 41.80it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 42.60it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 43.16it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 43.49it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 43.70it/s][A
 81%|████████  | 407/505 [00:09<00:02, 43.88it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 43.96it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.05it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 43.73it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 43.92it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.08it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.23it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.29it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.31it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.36it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.28it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.12it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 43.97it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.03it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.15it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.29it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.31it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.33it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.39it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.22it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.22it/s][A 40%|████      | 156/390 [01:15<01:09,  3.37it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:22:11,777 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 15:22:11,805 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:22:14,744 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:22:14,762 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:22:14,775 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:24<24:48,  6.39s/it] 41%|████      | 158/390 [01:24<17:38,  4.56s/it] 41%|████      | 159/390 [01:25<12:38,  3.28s/it] 41%|████      | 160/390 [01:25<09:08,  2.39s/it] 41%|████▏     | 161/390 [01:25<06:42,  1.76s/it] 42%|████▏     | 162/390 [01:25<05:00,  1.32s/it] 42%|████▏     | 163/390 [01:26<03:49,  1.01s/it] 42%|████▏     | 164/390 [01:26<03:00,  1.25it/s] 42%|████▏     | 165/390 [01:26<02:25,  1.55it/s] 43%|████▎     | 166/390 [01:27<02:01,  1.85it/s] 43%|████▎     | 167/390 [01:27<01:44,  2.14it/s] 43%|████▎     | 168/390 [01:27<01:32,  2.41it/s] 43%|████▎     | 169/390 [01:27<01:23,  2.64it/s] 44%|████▎     | 170/390 [01:28<01:17,  2.82it/s] 44%|████▍     | 171/390 [01:28<01:13,  2.97it/s] 44%|████▍     | 172/390 [01:28<01:10,  3.09it/s] 44%|████▍     | 173/390 [01:29<01:08,  3.17it/s] 45%|████▍     | 174/390 [01:29<01:06,  3.23it/s] 45%|████▍     | 175/390 [01:29<01:05,  3.28it/s] 45%|████▌     | 176/390 [01:30<01:04,  3.31it/s] 45%|████▌     | 177/390 [01:30<01:03,  3.33it/s] 46%|████▌     | 178/390 [01:30<01:03,  3.35it/s] 46%|████▌     | 179/390 [01:30<01:02,  3.35it/s] 46%|████▌     | 180/390 [01:31<01:02,  3.36it/s] 46%|████▋     | 181/390 [01:31<01:02,  3.37it/s] 47%|████▋     | 182/390 [01:31<01:01,  3.38it/s] 47%|████▋     | 183/390 [01:32<01:01,  3.38it/s] 47%|████▋     | 184/390 [01:32<01:00,  3.39it/s] 47%|████▋     | 185/390 [01:32<01:00,  3.39it/s] 48%|████▊     | 186/390 [01:33<01:00,  3.39it/s] 48%|████▊     | 187/390 [01:33<00:59,  3.39it/s] 48%|████▊     | 188/390 [01:33<00:59,  3.39it/s] 48%|████▊     | 189/390 [01:33<00:59,  3.39it/s] 49%|████▊     | 190/390 [01:34<00:59,  3.37it/s] 49%|████▉     | 191/390 [01:34<00:58,  3.38it/s] 49%|████▉     | 192/390 [01:34<00:58,  3.38it/s] 49%|████▉     | 193/390 [01:35<00:58,  3.38it/s] 50%|████▉     | 194/390 [01:35<00:57,  3.39it/s] 50%|█████     | 195/390 [01:35<00:57,  3.39it/s] 50%|█████     | 196/390 [01:35<00:57,  3.39it/s] 51%|█████     | 197/390 [01:36<00:56,  3.39it/s] 51%|█████     | 198/390 [01:36<00:56,  3.39it/s] 51%|█████     | 199/390 [01:36<00:56,  3.39it/s] 51%|█████▏    | 200/390 [01:37<00:56,  3.39it/s] 52%|█████▏    | 201/390 [01:37<00:56,  3.36it/s] 52%|█████▏    | 202/390 [01:37<00:56,  3.36it/s] 52%|█████▏    | 203/390 [01:38<00:55,  3.36it/s] 52%|█████▏    | 204/390 [01:38<00:55,  3.37it/s] 53%|█████▎    | 205/390 [01:38<00:54,  3.37it/s] 53%|█████▎    | 206/390 [01:38<00:54,  3.38it/s] 53%|█████▎    | 207/390 [01:39<00:54,  3.38it/s] 53%|█████▎    | 208/390 [01:39<00:53,  3.38it/s] 54%|█████▎    | 209/390 [01:39<00:53,  3.38it/s] 54%|█████▍    | 210/390 [01:40<00:53,  3.38it/s] 54%|█████▍    | 211/390 [01:40<00:52,  3.38it/s] 54%|█████▍    | 212/390 [01:40<00:52,  3.37it/s] 55%|█████▍    | 213/390 [01:41<00:52,  3.37it/s] 55%|█████▍    | 214/390 [01:41<00:51,  3.39it/s] 55%|█████▌    | 215/390 [01:41<00:51,  3.40it/s] 55%|█████▌    | 216/390 [01:41<00:51,  3.41it/s] 56%|█████▌    | 217/390 [01:42<00:50,  3.42it/s] 56%|█████▌    | 218/390 [01:42<00:50,  3.42it/s] 56%|█████▌    | 219/390 [01:42<00:49,  3.42it/s] 56%|█████▋    | 220/390 [01:43<00:49,  3.43it/s] 57%|█████▋    | 221/390 [01:43<00:49,  3.43it/s] 57%|█████▋    | 222/390 [01:43<00:48,  3.43it/s] 57%|█████▋    | 223/390 [01:43<00:48,  3.41it/s] 57%|█████▋    | 224/390 [01:44<00:48,  3.42it/s] 58%|█████▊    | 225/390 [01:44<00:48,  3.43it/s] 58%|█████▊    | 226/390 [01:44<00:47,  3.43it/s] 58%|█████▊    | 227/390 [01:45<00:47,  3.43it/s] 58%|█████▊    | 228/390 [01:45<00:47,  3.43it/s] 59%|█████▊    | 229/390 [01:45<00:46,  3.43it/s] 59%|█████▉    | 230/390 [01:45<00:46,  3.43it/s] 59%|█████▉    | 231/390 [01:46<00:46,  3.43it/s] 59%|█████▉    | 232/390 [01:46<00:46,  3.43it/s] 60%|█████▉    | 233/390 [01:46<00:45,  3.43it/s] 60%|██████    | 234/390 [01:47<00:45,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 15:22:43,594 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:22:43,594 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 15:22:43,594 >>   Batch size = 8
{'eval_loss': 1.1356555223464966, 'eval_runtime': 11.4514, 'eval_samples_per_second': 352.708, 'eval_steps_per_second': 44.099, 'epoch': 1.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.18it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.56it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.03it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.33it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.80it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.72it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.59it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.28it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.46it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.38it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.31it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.31it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.22it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.22it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.25it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.16it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.27it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.23it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.25it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.13it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.10it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.09it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.16it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.05it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.12it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.27it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.22it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.27it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.25it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.20it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.06it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.15it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.18it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.16it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.34it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.12it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.22it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.17it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.26it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.22it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.15it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.10it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.13it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 40.99it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 42.33it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 42.91it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 43.35it/s][A
 48%|████▊     | 242/505 [00:05<00:06, 43.66it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 43.87it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 43.90it/s][A
 51%|█████     | 257/505 [00:05<00:05, 43.99it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 43.84it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 43.74it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.02it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.15it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.25it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.29it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.26it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.23it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.18it/s][A
 61%|██████    | 307/505 [00:06<00:04, 43.92it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.06it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.06it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.22it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.31it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.31it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.25it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.28it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.13it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 43.95it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.04it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.09it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.02it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.28it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.27it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.20it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.21it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.16it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 43.99it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.08it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.03it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.22it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.37it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.37it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.25it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.24it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.06it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 44.01it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.09it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.10it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.16it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.34it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.30it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.25it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.19it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.12it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 43.97it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.07it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.16it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.23it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.23it/s][A 60%|██████    | 234/390 [01:58<00:45,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:22:55,083 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 15:22:55,123 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:22:57,562 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:22:57,593 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:22:57,608 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:07<16:11,  6.27s/it] 61%|██████    | 236/390 [02:07<11:29,  4.48s/it] 61%|██████    | 237/390 [02:07<08:13,  3.22s/it] 61%|██████    | 238/390 [02:08<05:56,  2.35s/it] 61%|██████▏   | 239/390 [02:08<04:21,  1.73s/it] 62%|██████▏   | 240/390 [02:08<03:14,  1.30s/it] 62%|██████▏   | 241/390 [02:09<02:28,  1.00it/s] 62%|██████▏   | 242/390 [02:09<01:56,  1.27it/s] 62%|██████▏   | 243/390 [02:09<01:34,  1.56it/s] 63%|██████▎   | 244/390 [02:10<01:18,  1.86it/s] 63%|██████▎   | 245/390 [02:10<01:07,  2.16it/s] 63%|██████▎   | 246/390 [02:10<00:59,  2.42it/s] 63%|██████▎   | 247/390 [02:10<00:55,  2.59it/s] 64%|██████▎   | 248/390 [02:11<00:50,  2.79it/s] 64%|██████▍   | 249/390 [02:11<00:47,  2.95it/s] 64%|██████▍   | 250/390 [02:11<00:45,  3.07it/s] 64%|██████▍   | 251/390 [02:12<00:44,  3.16it/s] 65%|██████▍   | 252/390 [02:12<00:42,  3.22it/s] 65%|██████▍   | 253/390 [02:12<00:41,  3.27it/s] 65%|██████▌   | 254/390 [02:12<00:41,  3.31it/s] 65%|██████▌   | 255/390 [02:13<00:40,  3.33it/s] 66%|██████▌   | 256/390 [02:13<00:40,  3.35it/s] 66%|██████▌   | 257/390 [02:13<00:39,  3.36it/s] 66%|██████▌   | 258/390 [02:14<00:39,  3.36it/s] 66%|██████▋   | 259/390 [02:14<00:38,  3.37it/s] 67%|██████▋   | 260/390 [02:14<00:38,  3.37it/s] 67%|██████▋   | 261/390 [02:15<00:38,  3.38it/s] 67%|██████▋   | 262/390 [02:15<00:37,  3.38it/s] 67%|██████▋   | 263/390 [02:15<00:37,  3.39it/s] 68%|██████▊   | 264/390 [02:15<00:37,  3.39it/s] 68%|██████▊   | 265/390 [02:16<00:36,  3.39it/s] 68%|██████▊   | 266/390 [02:16<00:36,  3.39it/s] 68%|██████▊   | 267/390 [02:16<00:36,  3.39it/s] 69%|██████▊   | 268/390 [02:17<00:36,  3.39it/s] 69%|██████▉   | 269/390 [02:17<00:35,  3.37it/s] 69%|██████▉   | 270/390 [02:17<00:35,  3.38it/s] 69%|██████▉   | 271/390 [02:18<00:35,  3.38it/s] 70%|██████▉   | 272/390 [02:18<00:34,  3.39it/s] 70%|███████   | 273/390 [02:18<00:34,  3.38it/s] 70%|███████   | 274/390 [02:18<00:34,  3.38it/s] 71%|███████   | 275/390 [02:19<00:33,  3.39it/s] 71%|███████   | 276/390 [02:19<00:33,  3.39it/s] 71%|███████   | 277/390 [02:19<00:33,  3.39it/s] 71%|███████▏  | 278/390 [02:20<00:33,  3.38it/s] 72%|███████▏  | 279/390 [02:20<00:32,  3.38it/s] 72%|███████▏  | 280/390 [02:20<00:32,  3.37it/s] 72%|███████▏  | 281/390 [02:20<00:32,  3.38it/s] 72%|███████▏  | 282/390 [02:21<00:31,  3.38it/s] 73%|███████▎  | 283/390 [02:21<00:31,  3.38it/s] 73%|███████▎  | 284/390 [02:21<00:31,  3.38it/s] 73%|███████▎  | 285/390 [02:22<00:31,  3.39it/s] 73%|███████▎  | 286/390 [02:22<00:30,  3.39it/s] 74%|███████▎  | 287/390 [02:22<00:30,  3.38it/s] 74%|███████▍  | 288/390 [02:23<00:30,  3.39it/s] 74%|███████▍  | 289/390 [02:23<00:29,  3.39it/s] 74%|███████▍  | 290/390 [02:23<00:29,  3.39it/s] 75%|███████▍  | 291/390 [02:23<00:29,  3.37it/s] 75%|███████▍  | 292/390 [02:24<00:29,  3.37it/s] 75%|███████▌  | 293/390 [02:24<00:28,  3.38it/s] 75%|███████▌  | 294/390 [02:24<00:28,  3.40it/s] 76%|███████▌  | 295/390 [02:25<00:27,  3.41it/s] 76%|███████▌  | 296/390 [02:25<00:27,  3.42it/s] 76%|███████▌  | 297/390 [02:25<00:27,  3.42it/s] 76%|███████▋  | 298/390 [02:25<00:26,  3.42it/s] 77%|███████▋  | 299/390 [02:26<00:26,  3.43it/s] 77%|███████▋  | 300/390 [02:26<00:26,  3.43it/s] 77%|███████▋  | 301/390 [02:26<00:25,  3.43it/s] 77%|███████▋  | 302/390 [02:27<00:25,  3.42it/s] 78%|███████▊  | 303/390 [02:27<00:25,  3.42it/s] 78%|███████▊  | 304/390 [02:27<00:25,  3.43it/s] 78%|███████▊  | 305/390 [02:28<00:24,  3.43it/s] 78%|███████▊  | 306/390 [02:28<00:24,  3.43it/s] 79%|███████▊  | 307/390 [02:28<00:24,  3.43it/s] 79%|███████▉  | 308/390 [02:28<00:23,  3.43it/s] 79%|███████▉  | 309/390 [02:29<00:23,  3.43it/s] 79%|███████▉  | 310/390 [02:29<00:31,  2.51it/s] 80%|███████▉  | 311/390 [02:30<00:28,  2.73it/s] 80%|████████  | 312/390 [02:30<00:26,  2.91it/s][INFO|trainer.py:2140] 2023-08-28 15:23:26,869 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:23:26,869 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 15:23:26,869 >>   Batch size = 8
{'eval_loss': 1.1506845951080322, 'eval_runtime': 11.4605, 'eval_samples_per_second': 352.426, 'eval_steps_per_second': 44.064, 'epoch': 2.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.35it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.73it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.11it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.31it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.79it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.58it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.55it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.33it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.41it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.37it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.26it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.23it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.26it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.14it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.21it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.22it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.22it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.29it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.32it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.17it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.17it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.14it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.14it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.19it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.19it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.27it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.27it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.13it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.09it/s][A
 30%|███       | 152/505 [00:03<00:08, 44.12it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.15it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.11it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.03it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.24it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.21it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.30it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.16it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.10it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.21it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.23it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.08it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.17it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.12it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.18it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.21it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.15it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.16it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.20it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.09it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.12it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.21it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.31it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.12it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.13it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.20it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.25it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.14it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.13it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.10it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.25it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.35it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.20it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.10it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.10it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.13it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.19it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.13it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.26it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.28it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.21it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.14it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.25it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.20it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.11it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.14it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.11it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.22it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.21it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.19it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.15it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.28it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.24it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.22it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.14it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 43.83it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.11it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.18it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.12it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.21it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.26it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.27it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.27it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.23it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.10it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.10it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.09it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.11it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.18it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.12it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.22it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.22it/s][A 80%|████████  | 312/390 [02:41<00:26,  2.91it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:23:38,340 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 15:23:38,363 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:23:40,976 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:23:40,999 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:23:41,019 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [02:50<08:03,  6.27s/it] 81%|████████  | 314/390 [02:50<05:40,  4.48s/it] 81%|████████  | 315/390 [02:51<04:01,  3.23s/it] 81%|████████  | 316/390 [02:51<02:53,  2.35s/it] 81%|████████▏ | 317/390 [02:51<02:06,  1.73s/it] 82%|████████▏ | 318/390 [02:51<01:33,  1.30s/it] 82%|████████▏ | 319/390 [02:52<01:11,  1.01s/it] 82%|████████▏ | 320/390 [02:52<00:55,  1.26it/s] 82%|████████▏ | 321/390 [02:52<00:44,  1.55it/s] 83%|████████▎ | 322/390 [02:53<00:36,  1.86it/s] 83%|████████▎ | 323/390 [02:53<00:31,  2.16it/s] 83%|████████▎ | 324/390 [02:53<00:27,  2.43it/s] 83%|████████▎ | 325/390 [02:54<00:24,  2.65it/s] 84%|████████▎ | 326/390 [02:54<00:22,  2.85it/s] 84%|████████▍ | 327/390 [02:54<00:20,  3.00it/s] 84%|████████▍ | 328/390 [02:54<00:19,  3.12it/s] 84%|████████▍ | 329/390 [02:55<00:19,  3.21it/s] 85%|████████▍ | 330/390 [02:55<00:18,  3.28it/s] 85%|████████▍ | 331/390 [02:55<00:17,  3.32it/s] 85%|████████▌ | 332/390 [02:56<00:17,  3.36it/s] 85%|████████▌ | 333/390 [02:56<00:16,  3.38it/s] 86%|████████▌ | 334/390 [02:56<00:16,  3.40it/s] 86%|████████▌ | 335/390 [02:56<00:16,  3.41it/s] 86%|████████▌ | 336/390 [02:57<00:15,  3.41it/s] 86%|████████▋ | 337/390 [02:57<00:15,  3.42it/s] 87%|████████▋ | 338/390 [02:57<00:15,  3.42it/s] 87%|████████▋ | 339/390 [02:58<00:14,  3.43it/s] 87%|████████▋ | 340/390 [02:58<00:14,  3.43it/s] 87%|████████▋ | 341/390 [02:58<00:14,  3.43it/s] 88%|████████▊ | 342/390 [02:59<00:13,  3.43it/s] 88%|████████▊ | 343/390 [02:59<00:13,  3.43it/s] 88%|████████▊ | 344/390 [02:59<00:13,  3.43it/s] 88%|████████▊ | 345/390 [02:59<00:13,  3.43it/s] 89%|████████▊ | 346/390 [03:00<00:12,  3.43it/s] 89%|████████▉ | 347/390 [03:00<00:12,  3.43it/s] 89%|████████▉ | 348/390 [03:00<00:12,  3.43it/s] 89%|████████▉ | 349/390 [03:01<00:11,  3.43it/s] 90%|████████▉ | 350/390 [03:01<00:11,  3.43it/s] 90%|█████████ | 351/390 [03:01<00:11,  3.44it/s] 90%|█████████ | 352/390 [03:01<00:11,  3.44it/s] 91%|█████████ | 353/390 [03:02<00:10,  3.44it/s] 91%|█████████ | 354/390 [03:02<00:10,  3.44it/s] 91%|█████████ | 355/390 [03:02<00:10,  3.44it/s] 91%|█████████▏| 356/390 [03:03<00:09,  3.42it/s] 92%|█████████▏| 357/390 [03:03<00:09,  3.42it/s] 92%|█████████▏| 358/390 [03:03<00:09,  3.43it/s] 92%|█████████▏| 359/390 [03:03<00:09,  3.42it/s] 92%|█████████▏| 360/390 [03:04<00:08,  3.43it/s] 93%|█████████▎| 361/390 [03:04<00:08,  3.42it/s] 93%|█████████▎| 362/390 [03:04<00:08,  3.42it/s] 93%|█████████▎| 363/390 [03:05<00:07,  3.42it/s] 93%|█████████▎| 364/390 [03:05<00:07,  3.42it/s] 94%|█████████▎| 365/390 [03:05<00:07,  3.43it/s] 94%|█████████▍| 366/390 [03:06<00:07,  3.43it/s] 94%|█████████▍| 367/390 [03:06<00:06,  3.41it/s] 94%|█████████▍| 368/390 [03:06<00:06,  3.42it/s] 95%|█████████▍| 369/390 [03:06<00:06,  3.42it/s] 95%|█████████▍| 370/390 [03:07<00:05,  3.42it/s] 95%|█████████▌| 371/390 [03:07<00:05,  3.43it/s] 95%|█████████▌| 372/390 [03:07<00:05,  3.42it/s] 96%|█████████▌| 373/390 [03:08<00:04,  3.43it/s] 96%|█████████▌| 374/390 [03:08<00:04,  3.43it/s] 96%|█████████▌| 375/390 [03:08<00:04,  3.43it/s] 96%|█████████▋| 376/390 [03:08<00:04,  3.43it/s] 97%|█████████▋| 377/390 [03:09<00:03,  3.43it/s] 97%|█████████▋| 378/390 [03:09<00:03,  3.40it/s] 97%|█████████▋| 379/390 [03:09<00:03,  3.41it/s] 97%|█████████▋| 380/390 [03:10<00:02,  3.42it/s] 98%|█████████▊| 381/390 [03:10<00:02,  3.42it/s] 98%|█████████▊| 382/390 [03:10<00:02,  3.42it/s] 98%|█████████▊| 383/390 [03:10<00:02,  3.43it/s] 98%|█████████▊| 384/390 [03:11<00:01,  3.43it/s] 99%|█████████▊| 385/390 [03:11<00:01,  3.43it/s] 99%|█████████▉| 386/390 [03:11<00:01,  3.43it/s] 99%|█████████▉| 387/390 [03:12<00:00,  3.43it/s] 99%|█████████▉| 388/390 [03:12<00:00,  3.43it/s]100%|█████████▉| 389/390 [03:12<00:00,  3.42it/s]100%|██████████| 390/390 [03:13<00:00,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 15:24:09,465 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:24:09,465 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 15:24:09,465 >>   Batch size = 8
{'eval_loss': 1.1666141748428345, 'eval_runtime': 11.4363, 'eval_samples_per_second': 353.173, 'eval_steps_per_second': 44.158, 'epoch': 3.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.56it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.63it/s][A
  3%|▎         | 17/505 [00:00<00:10, 45.56it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.08it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.87it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.64it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.53it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.43it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.40it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.50it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.39it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.10it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.10it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.15it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.18it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.24it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.30it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.21it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.31it/s][A
 20%|██        | 102/505 [00:02<00:09, 43.97it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.17it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.06it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.18it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.22it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.23it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.26it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.26it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.24it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.20it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.19it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.09it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.20it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.24it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.18it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.24it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.18it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.19it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.16it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.16it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.19it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.19it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.16it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.26it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.38it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.17it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.23it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.11it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.07it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.24it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.19it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.16it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.25it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.27it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.21it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.28it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.16it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.11it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.14it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.22it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.16it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.19it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.28it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.22it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.19it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.12it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.20it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.22it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.15it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.21it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.22it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.06it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.28it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.22it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.19it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.18it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.21it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.19it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.18it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.17it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.11it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.18it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.20it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.10it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.20it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.24it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.27it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.22it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.14it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.20it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.12it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.17it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.19it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 43.88it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.33it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.33it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.16it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.15it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.25it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.21it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.17it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.17it/s][A100%|██████████| 390/390 [03:24<00:00,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:24:21,018 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 15:24:21,075 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:24:23,083 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:24:23,101 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:24:23,110 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 15:24:27,220 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 15:24:27,220 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78 (score: 1.1094993352890015).
                                                 100%|██████████| 390/390 [03:33<00:00,  3.42it/s]100%|██████████| 390/390 [03:33<00:00,  1.83it/s]
[INFO|trainer.py:1894] 2023-08-28 15:24:29,665 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 15:24:29,687 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:24:32,031 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:24:32,049 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:24:32,062 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 15:24:32,276 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:24:32,277 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:24:32,277 >>   train_loss               =     0.4208
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:24:32,277 >>   train_runtime            = 0:03:33.23
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:24:32,277 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:24:32,277 >>   train_samples_per_second =    117.241
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:24:32,277 >>   train_steps_per_second   =      1.829
{'eval_loss': 1.1704217195510864, 'eval_runtime': 11.4294, 'eval_samples_per_second': 353.387, 'eval_steps_per_second': 44.184, 'epoch': 4.99}
{'train_runtime': 213.2367, 'train_samples_per_second': 117.241, 'train_steps_per_second': 1.829, 'train_loss': 0.42079350398137016, 'epoch': 4.99}
08/28/2023 15:24:32 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 15:24:32,312 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:24:32,312 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-28 15:24:32,312 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:08, 56.15it/s]  2%|▏         | 12/505 [00:00<00:10, 48.57it/s]  3%|▎         | 17/505 [00:00<00:10, 46.87it/s]  4%|▍         | 22/505 [00:00<00:10, 46.24it/s]  5%|▌         | 27/505 [00:00<00:10, 45.87it/s]  6%|▋         | 32/505 [00:00<00:10, 45.54it/s]  7%|▋         | 37/505 [00:00<00:10, 45.11it/s]  8%|▊         | 42/505 [00:00<00:10, 44.57it/s]  9%|▉         | 47/505 [00:01<00:10, 44.09it/s] 10%|█         | 52/505 [00:01<00:10, 43.92it/s] 11%|█▏        | 57/505 [00:01<00:10, 43.99it/s] 12%|█▏        | 62/505 [00:01<00:10, 44.23it/s] 13%|█▎        | 67/505 [00:01<00:09, 44.36it/s] 14%|█▍        | 72/505 [00:01<00:09, 44.52it/s] 15%|█▌        | 77/505 [00:01<00:09, 44.51it/s] 16%|█▌        | 82/505 [00:01<00:09, 44.43it/s] 17%|█▋        | 87/505 [00:01<00:09, 44.21it/s] 18%|█▊        | 92/505 [00:02<00:09, 43.92it/s] 19%|█▉        | 97/505 [00:02<00:09, 43.89it/s] 20%|██        | 102/505 [00:02<00:09, 43.91it/s] 21%|██        | 107/505 [00:02<00:09, 44.20it/s] 22%|██▏       | 112/505 [00:02<00:08, 44.28it/s] 23%|██▎       | 117/505 [00:02<00:08, 44.13it/s] 24%|██▍       | 122/505 [00:02<00:08, 44.36it/s] 25%|██▌       | 127/505 [00:02<00:08, 44.25it/s] 26%|██▌       | 132/505 [00:02<00:08, 44.03it/s] 27%|██▋       | 137/505 [00:03<00:08, 43.96it/s] 28%|██▊       | 142/505 [00:03<00:08, 43.88it/s] 29%|██▉       | 147/505 [00:03<00:08, 43.94it/s] 30%|███       | 152/505 [00:03<00:07, 44.22it/s] 31%|███       | 157/505 [00:03<00:07, 44.29it/s] 32%|███▏      | 162/505 [00:03<00:07, 44.34it/s] 33%|███▎      | 167/505 [00:03<00:07, 44.33it/s] 34%|███▍      | 172/505 [00:03<00:07, 44.26it/s] 35%|███▌      | 177/505 [00:03<00:07, 44.13it/s] 36%|███▌      | 182/505 [00:04<00:07, 43.98it/s] 37%|███▋      | 187/505 [00:04<00:07, 43.58it/s] 38%|███▊      | 192/505 [00:04<00:07, 43.94it/s] 39%|███▉      | 197/505 [00:04<00:06, 44.23it/s] 40%|████      | 202/505 [00:04<00:06, 44.24it/s] 41%|████      | 207/505 [00:04<00:06, 44.27it/s] 42%|████▏     | 212/505 [00:04<00:06, 44.33it/s] 43%|████▎     | 217/505 [00:04<00:06, 44.29it/s] 44%|████▍     | 222/505 [00:04<00:06, 44.18it/s] 45%|████▍     | 227/505 [00:05<00:06, 43.90it/s] 46%|████▌     | 232/505 [00:05<00:06, 43.99it/s] 47%|████▋     | 237/505 [00:05<00:06, 44.10it/s] 48%|████▊     | 242/505 [00:05<00:05, 44.27it/s] 49%|████▉     | 247/505 [00:05<00:05, 44.25it/s] 50%|████▉     | 252/505 [00:05<00:05, 44.31it/s] 51%|█████     | 257/505 [00:05<00:05, 44.26it/s] 52%|█████▏    | 262/505 [00:05<00:05, 44.06it/s] 53%|█████▎    | 267/505 [00:06<00:05, 43.96it/s] 54%|█████▍    | 272/505 [00:06<00:05, 43.95it/s] 55%|█████▍    | 277/505 [00:06<00:05, 44.05it/s] 56%|█████▌    | 282/505 [00:06<00:05, 44.17it/s] 57%|█████▋    | 287/505 [00:06<00:04, 44.25it/s] 58%|█████▊    | 292/505 [00:06<00:04, 44.23it/s] 59%|█████▉    | 297/505 [00:06<00:04, 44.29it/s] 60%|█████▉    | 302/505 [00:06<00:04, 44.27it/s] 61%|██████    | 307/505 [00:06<00:04, 44.05it/s] 62%|██████▏   | 312/505 [00:07<00:04, 43.89it/s] 63%|██████▎   | 317/505 [00:07<00:04, 43.97it/s] 64%|██████▍   | 322/505 [00:07<00:04, 44.05it/s] 65%|██████▍   | 327/505 [00:07<00:04, 44.07it/s] 66%|██████▌   | 332/505 [00:07<00:03, 44.27it/s] 67%|██████▋   | 337/505 [00:07<00:03, 44.24it/s] 68%|██████▊   | 342/505 [00:07<00:03, 44.26it/s] 69%|██████▊   | 347/505 [00:07<00:03, 44.23it/s] 70%|██████▉   | 352/505 [00:07<00:03, 44.09it/s] 71%|███████   | 357/505 [00:08<00:03, 43.95it/s] 72%|███████▏  | 362/505 [00:08<00:03, 43.96it/s] 73%|███████▎  | 367/505 [00:08<00:03, 44.07it/s] 74%|███████▎  | 372/505 [00:08<00:03, 44.15it/s] 75%|███████▍  | 377/505 [00:08<00:02, 44.15it/s] 76%|███████▌  | 382/505 [00:08<00:02, 44.20it/s] 77%|███████▋  | 387/505 [00:08<00:02, 44.21it/s] 78%|███████▊  | 392/505 [00:08<00:02, 44.17it/s] 79%|███████▊  | 397/505 [00:08<00:02, 43.91it/s] 80%|███████▉  | 402/505 [00:09<00:02, 43.90it/s] 81%|████████  | 407/505 [00:09<00:02, 43.86it/s] 82%|████████▏ | 412/505 [00:09<00:02, 44.09it/s] 83%|████████▎ | 417/505 [00:09<00:01, 44.22it/s] 84%|████████▎ | 422/505 [00:09<00:01, 44.15it/s] 85%|████████▍ | 427/505 [00:09<00:01, 44.23it/s] 86%|████████▌ | 432/505 [00:09<00:01, 44.27it/s] 87%|████████▋ | 437/505 [00:09<00:01, 44.01it/s] 88%|████████▊ | 442/505 [00:09<00:01, 43.99it/s] 89%|████████▊ | 447/505 [00:10<00:01, 43.83it/s] 90%|████████▉ | 452/505 [00:10<00:01, 43.95it/s] 90%|█████████ | 457/505 [00:10<00:01, 44.08it/s] 91%|█████████▏| 462/505 [00:10<00:00, 44.22it/s] 92%|█████████▏| 467/505 [00:10<00:00, 44.18it/s] 93%|█████████▎| 472/505 [00:10<00:00, 44.16it/s] 94%|█████████▍| 477/505 [00:10<00:00, 44.25it/s] 95%|█████████▌| 482/505 [00:10<00:00, 44.13it/s] 96%|█████████▋| 487/505 [00:11<00:00, 44.02it/s] 97%|█████████▋| 492/505 [00:11<00:00, 43.92it/s] 98%|█████████▊| 497/505 [00:11<00:00, 43.93it/s] 99%|█████████▉| 502/505 [00:11<00:00, 44.07it/s]100%|██████████| 505/505 [00:11<00:00, 44.21it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 15:24:43,753 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:24:43,753 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:24:43,753 >>   eval_loss               =     1.1095
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:24:43,753 >>   eval_runtime            = 0:00:11.44
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:24:43,753 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:24:43,753 >>   eval_samples_per_second =    353.044
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:24:43,753 >>   eval_steps_per_second   =     44.141
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:24:43,753 >>   perplexity              =     3.0328
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:24:50,670 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:24:50,678 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:24:50,679 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:24:50,679 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:24:50,679 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:24:51,312 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:24:51,313 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:24:51,879 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:24:52,897 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:24:52,897 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:24:55,726 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:24:55,731 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:24:55,731 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:24:55,731 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:24:55,731 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:24:56,386 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:24:56,387 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:24:56,981 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:24:57,138 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:24:57,138 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:12,  1.53it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:15,  1.47it/s]Extractor Predicting: 24it [00:15,  1.49it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.57it/s]Extractor Predicting: 28it [00:18,  1.55it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:20,  1.55it/s]Extractor Predicting: 32it [00:20,  1.54it/s]Extractor Predicting: 33it [00:21,  1.53it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:22,  1.51it/s]Extractor Predicting: 36it [00:23,  1.57it/s]Extractor Predicting: 37it [00:23,  1.63it/s]Extractor Predicting: 38it [00:24,  1.61it/s]Extractor Predicting: 39it [00:25,  1.64it/s]Extractor Predicting: 40it [00:25,  1.60it/s]Extractor Predicting: 41it [00:26,  1.58it/s]Extractor Predicting: 42it [00:27,  1.58it/s]Extractor Predicting: 43it [00:27,  1.62it/s]Extractor Predicting: 44it [00:28,  1.60it/s]Extractor Predicting: 45it [00:28,  1.62it/s]Extractor Predicting: 46it [00:29,  1.59it/s]Extractor Predicting: 47it [00:30,  1.57it/s]Extractor Predicting: 48it [00:30,  1.50it/s]Extractor Predicting: 49it [00:31,  1.54it/s]Extractor Predicting: 50it [00:32,  1.58it/s]Extractor Predicting: 51it [00:32,  1.51it/s]Extractor Predicting: 52it [00:33,  1.56it/s]Extractor Predicting: 53it [00:34,  1.57it/s]Extractor Predicting: 54it [00:34,  1.58it/s]Extractor Predicting: 55it [00:35,  1.58it/s]Extractor Predicting: 56it [00:36,  1.59it/s]Extractor Predicting: 57it [00:36,  1.38it/s]Extractor Predicting: 58it [00:37,  1.40it/s]Extractor Predicting: 59it [00:38,  1.45it/s]Extractor Predicting: 60it [00:38,  1.48it/s]Extractor Predicting: 61it [00:39,  1.54it/s]Extractor Predicting: 62it [00:40,  1.55it/s]Extractor Predicting: 63it [00:40,  1.53it/s]Extractor Predicting: 64it [00:41,  1.54it/s]Extractor Predicting: 65it [00:42,  1.54it/s]Extractor Predicting: 66it [00:42,  1.56it/s]Extractor Predicting: 67it [00:43,  1.55it/s]Extractor Predicting: 68it [00:44,  1.52it/s]Extractor Predicting: 69it [00:44,  1.49it/s]Extractor Predicting: 70it [00:45,  1.53it/s]Extractor Predicting: 71it [00:46,  1.52it/s]Extractor Predicting: 72it [00:46,  1.48it/s]Extractor Predicting: 73it [00:47,  1.48it/s]Extractor Predicting: 74it [00:48,  1.43it/s]Extractor Predicting: 75it [00:48,  1.46it/s]Extractor Predicting: 76it [00:49,  1.50it/s]Extractor Predicting: 77it [00:50,  1.49it/s]Extractor Predicting: 78it [00:50,  1.50it/s]Extractor Predicting: 79it [00:51,  1.51it/s]Extractor Predicting: 80it [00:52,  1.51it/s]Extractor Predicting: 81it [00:52,  1.52it/s]Extractor Predicting: 82it [00:53,  1.57it/s]Extractor Predicting: 83it [00:54,  1.53it/s]Extractor Predicting: 84it [00:54,  1.54it/s]Extractor Predicting: 85it [00:55,  1.53it/s]Extractor Predicting: 86it [00:55,  1.55it/s]Extractor Predicting: 87it [00:56,  1.58it/s]Extractor Predicting: 88it [00:57,  1.59it/s]Extractor Predicting: 89it [00:57,  1.58it/s]Extractor Predicting: 90it [00:58,  1.59it/s]Extractor Predicting: 91it [00:59,  1.59it/s]Extractor Predicting: 92it [00:59,  1.61it/s]Extractor Predicting: 93it [01:00,  1.60it/s]Extractor Predicting: 94it [01:00,  1.57it/s]Extractor Predicting: 95it [01:01,  1.55it/s]Extractor Predicting: 96it [01:02,  1.53it/s]Extractor Predicting: 97it [01:02,  1.54it/s]Extractor Predicting: 98it [01:03,  1.54it/s]Extractor Predicting: 99it [01:04,  1.53it/s]Extractor Predicting: 100it [01:04,  1.53it/s]Extractor Predicting: 101it [01:05,  1.51it/s]Extractor Predicting: 102it [01:06,  1.51it/s]Extractor Predicting: 103it [01:06,  1.53it/s]Extractor Predicting: 104it [01:07,  1.56it/s]Extractor Predicting: 105it [01:08,  1.55it/s]Extractor Predicting: 106it [01:08,  1.58it/s]Extractor Predicting: 107it [01:09,  1.58it/s]Extractor Predicting: 108it [01:10,  1.59it/s]Extractor Predicting: 109it [01:10,  1.59it/s]Extractor Predicting: 110it [01:11,  1.57it/s]Extractor Predicting: 111it [01:12,  1.54it/s]Extractor Predicting: 112it [01:12,  1.58it/s]Extractor Predicting: 113it [01:13,  1.57it/s]Extractor Predicting: 114it [01:13,  1.55it/s]Extractor Predicting: 115it [01:14,  1.59it/s]Extractor Predicting: 116it [01:15,  1.59it/s]Extractor Predicting: 117it [01:15,  1.58it/s]Extractor Predicting: 118it [01:16,  1.53it/s]Extractor Predicting: 119it [01:17,  1.56it/s]Extractor Predicting: 120it [01:17,  1.57it/s]Extractor Predicting: 121it [01:18,  1.54it/s]Extractor Predicting: 122it [01:19,  1.55it/s]Extractor Predicting: 123it [01:19,  1.58it/s]Extractor Predicting: 124it [01:20,  1.57it/s]Extractor Predicting: 125it [01:21,  1.42it/s]Extractor Predicting: 126it [01:21,  1.47it/s]Extractor Predicting: 127it [01:22,  1.31it/s]Extractor Predicting: 128it [01:23,  1.33it/s]Extractor Predicting: 129it [01:24,  1.36it/s]Extractor Predicting: 130it [01:24,  1.38it/s]Extractor Predicting: 131it [01:25,  1.36it/s]Extractor Predicting: 132it [01:26,  1.37it/s]Extractor Predicting: 133it [01:27,  1.39it/s]Extractor Predicting: 134it [01:27,  1.43it/s]Extractor Predicting: 135it [01:28,  1.45it/s]Extractor Predicting: 136it [01:29,  1.45it/s]Extractor Predicting: 137it [01:29,  1.48it/s]Extractor Predicting: 138it [01:30,  1.42it/s]Extractor Predicting: 139it [01:31,  1.43it/s]Extractor Predicting: 140it [01:31,  1.42it/s]Extractor Predicting: 141it [01:32,  1.43it/s]Extractor Predicting: 142it [01:33,  1.44it/s]Extractor Predicting: 143it [01:33,  1.44it/s]Extractor Predicting: 144it [01:34,  1.45it/s]Extractor Predicting: 145it [01:35,  1.43it/s]Extractor Predicting: 146it [01:35,  1.45it/s]Extractor Predicting: 147it [01:36,  1.46it/s]Extractor Predicting: 148it [01:37,  1.42it/s]Extractor Predicting: 149it [01:38,  1.45it/s]Extractor Predicting: 149it [01:38,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:26:43,481 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:26:43,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:26:43,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:26:43,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:26:43,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:26:44,095 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:26:44,096 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:26:44,674 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:26:45,696 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:26:45,697 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:26:48,508 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:26:48,512 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:26:48,512 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:26:48,512 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:26:48,512 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:26:49,136 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:26:49,137 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:26:49,706 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:26:49,862 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:26:49,862 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.31220657276995306,
  "recall": 0.03292894280762565,
  "score": 0.059574468085106386,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.74it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.75it/s]Extractor Predicting: 10it [00:05,  1.73it/s]Extractor Predicting: 11it [00:06,  1.72it/s]Extractor Predicting: 12it [00:07,  1.73it/s]Extractor Predicting: 13it [00:07,  1.80it/s]Extractor Predicting: 14it [00:08,  1.78it/s]Extractor Predicting: 15it [00:08,  1.80it/s]Extractor Predicting: 16it [00:09,  1.79it/s]Extractor Predicting: 17it [00:09,  1.79it/s]Extractor Predicting: 18it [00:10,  1.75it/s]Extractor Predicting: 19it [00:11,  1.72it/s]Extractor Predicting: 20it [00:11,  1.72it/s]Extractor Predicting: 21it [00:12,  1.72it/s]Extractor Predicting: 22it [00:12,  1.74it/s]Extractor Predicting: 23it [00:13,  1.75it/s]Extractor Predicting: 24it [00:13,  1.74it/s]Extractor Predicting: 25it [00:14,  1.77it/s]Extractor Predicting: 26it [00:15,  1.73it/s]Extractor Predicting: 27it [00:15,  1.78it/s]Extractor Predicting: 28it [00:16,  1.79it/s]Extractor Predicting: 29it [00:16,  1.77it/s]Extractor Predicting: 30it [00:17,  1.78it/s]Extractor Predicting: 31it [00:17,  1.80it/s]Extractor Predicting: 32it [00:18,  1.79it/s]Extractor Predicting: 33it [00:18,  1.75it/s]Extractor Predicting: 34it [00:19,  1.71it/s]Extractor Predicting: 35it [00:20,  1.69it/s]Extractor Predicting: 36it [00:20,  1.75it/s]Extractor Predicting: 37it [00:21,  1.72it/s]Extractor Predicting: 38it [00:21,  1.70it/s]Extractor Predicting: 39it [00:22,  1.71it/s]Extractor Predicting: 40it [00:23,  1.69it/s]Extractor Predicting: 41it [00:23,  1.67it/s]Extractor Predicting: 42it [00:24,  1.70it/s]Extractor Predicting: 43it [00:24,  1.72it/s]Extractor Predicting: 44it [00:25,  1.72it/s]Extractor Predicting: 45it [00:26,  1.57it/s]Extractor Predicting: 46it [00:26,  1.55it/s]Extractor Predicting: 47it [00:27,  1.58it/s]Extractor Predicting: 48it [00:28,  1.61it/s]Extractor Predicting: 49it [00:28,  1.65it/s]Extractor Predicting: 50it [00:29,  1.67it/s]Extractor Predicting: 51it [00:29,  1.71it/s]Extractor Predicting: 52it [00:30,  1.68it/s]Extractor Predicting: 53it [00:31,  1.67it/s]Extractor Predicting: 54it [00:31,  1.68it/s]Extractor Predicting: 55it [00:32,  1.66it/s]Extractor Predicting: 56it [00:32,  1.62it/s]Extractor Predicting: 57it [00:33,  1.62it/s]Extractor Predicting: 58it [00:34,  1.64it/s]Extractor Predicting: 59it [00:34,  1.62it/s]Extractor Predicting: 60it [00:35,  1.64it/s]Extractor Predicting: 61it [00:35,  1.64it/s]Extractor Predicting: 62it [00:36,  1.67it/s]Extractor Predicting: 63it [00:37,  1.64it/s]Extractor Predicting: 64it [00:37,  1.67it/s]Extractor Predicting: 65it [00:38,  1.70it/s]Extractor Predicting: 66it [00:38,  1.70it/s]Extractor Predicting: 67it [00:39,  1.67it/s]Extractor Predicting: 68it [00:40,  1.67it/s]Extractor Predicting: 69it [00:40,  1.65it/s]Extractor Predicting: 70it [00:41,  1.63it/s]Extractor Predicting: 71it [00:41,  1.63it/s]Extractor Predicting: 72it [00:42,  1.64it/s]Extractor Predicting: 73it [00:43,  1.63it/s]Extractor Predicting: 74it [00:43,  1.64it/s]Extractor Predicting: 75it [00:44,  1.66it/s]Extractor Predicting: 76it [00:44,  1.64it/s]Extractor Predicting: 77it [00:45,  1.65it/s]Extractor Predicting: 78it [00:46,  1.59it/s]Extractor Predicting: 79it [00:46,  1.63it/s]Extractor Predicting: 80it [00:47,  1.52it/s]Extractor Predicting: 81it [00:48,  1.53it/s]Extractor Predicting: 82it [00:48,  1.54it/s]Extractor Predicting: 83it [00:49,  1.56it/s]Extractor Predicting: 84it [00:50,  1.59it/s]Extractor Predicting: 85it [00:50,  1.59it/s]Extractor Predicting: 86it [00:51,  1.58it/s]Extractor Predicting: 87it [00:51,  1.62it/s]Extractor Predicting: 88it [00:52,  1.63it/s]Extractor Predicting: 89it [00:53,  1.68it/s]Extractor Predicting: 90it [00:53,  1.67it/s]Extractor Predicting: 91it [00:54,  1.66it/s]Extractor Predicting: 92it [00:54,  1.66it/s]Extractor Predicting: 93it [00:55,  1.69it/s]Extractor Predicting: 94it [00:56,  1.70it/s]Extractor Predicting: 95it [00:56,  1.68it/s]Extractor Predicting: 96it [00:57,  1.72it/s]Extractor Predicting: 97it [00:57,  1.74it/s]Extractor Predicting: 98it [00:58,  1.71it/s]Extractor Predicting: 99it [00:58,  1.70it/s]Extractor Predicting: 100it [00:59,  1.73it/s]Extractor Predicting: 101it [01:00,  1.71it/s]Extractor Predicting: 102it [01:00,  1.66it/s]Extractor Predicting: 103it [01:01,  1.65it/s]Extractor Predicting: 104it [01:01,  1.70it/s]Extractor Predicting: 105it [01:02,  1.64it/s]Extractor Predicting: 106it [01:03,  1.67it/s]Extractor Predicting: 107it [01:03,  1.68it/s]Extractor Predicting: 108it [01:04,  1.65it/s]Extractor Predicting: 109it [01:05,  1.64it/s]Extractor Predicting: 110it [01:05,  1.66it/s]Extractor Predicting: 111it [01:06,  1.65it/s]Extractor Predicting: 112it [01:06,  1.65it/s]Extractor Predicting: 113it [01:07,  1.68it/s]Extractor Predicting: 114it [01:07,  1.68it/s]Extractor Predicting: 115it [01:08,  1.68it/s]Extractor Predicting: 116it [01:09,  1.71it/s]Extractor Predicting: 117it [01:09,  1.72it/s]Extractor Predicting: 118it [01:10,  1.76it/s]Extractor Predicting: 119it [01:10,  1.77it/s]Extractor Predicting: 120it [01:11,  1.76it/s]Extractor Predicting: 121it [01:11,  1.75it/s]Extractor Predicting: 122it [01:12,  1.74it/s]Extractor Predicting: 123it [01:13,  1.72it/s]Extractor Predicting: 124it [01:13,  1.73it/s]Extractor Predicting: 125it [01:14,  1.72it/s]Extractor Predicting: 126it [01:14,  1.70it/s]Extractor Predicting: 127it [01:15,  1.66it/s]Extractor Predicting: 128it [01:16,  1.64it/s]Extractor Predicting: 129it [01:16,  1.62it/s]Extractor Predicting: 130it [01:17,  1.60it/s]Extractor Predicting: 131it [01:18,  1.58it/s]Extractor Predicting: 132it [01:18,  1.59it/s]Extractor Predicting: 133it [01:19,  1.61it/s]Extractor Predicting: 134it [01:19,  1.63it/s]Extractor Predicting: 135it [01:20,  1.57it/s]Extractor Predicting: 136it [01:21,  1.56it/s]Extractor Predicting: 137it [01:21,  1.56it/s]Extractor Predicting: 138it [01:22,  1.58it/s]Extractor Predicting: 139it [01:23,  1.58it/s]Extractor Predicting: 140it [01:23,  1.55it/s]Extractor Predicting: 141it [01:24,  1.55it/s]Extractor Predicting: 142it [01:25,  1.56it/s]Extractor Predicting: 143it [01:25,  1.59it/s]Extractor Predicting: 144it [01:26,  1.57it/s]Extractor Predicting: 145it [01:27,  1.57it/s]Extractor Predicting: 146it [01:27,  1.56it/s]Extractor Predicting: 147it [01:28,  1.57it/s]Extractor Predicting: 148it [01:28,  1.58it/s]Extractor Predicting: 149it [01:29,  1.62it/s]Extractor Predicting: 150it [01:30,  1.61it/s]Extractor Predicting: 151it [01:30,  1.59it/s]Extractor Predicting: 152it [01:31,  1.59it/s]Extractor Predicting: 153it [01:32,  1.57it/s]Extractor Predicting: 154it [01:32,  1.56it/s]Extractor Predicting: 155it [01:33,  1.58it/s]Extractor Predicting: 156it [01:33,  1.63it/s]Extractor Predicting: 157it [01:34,  1.69it/s]Extractor Predicting: 158it [01:34,  1.70it/s]Extractor Predicting: 159it [01:35,  1.70it/s]Extractor Predicting: 160it [01:36,  1.71it/s]Extractor Predicting: 161it [01:36,  1.75it/s]Extractor Predicting: 162it [01:37,  1.70it/s]Extractor Predicting: 163it [01:37,  1.68it/s]Extractor Predicting: 164it [01:38,  1.70it/s]Extractor Predicting: 165it [01:39,  1.74it/s]Extractor Predicting: 166it [01:39,  1.74it/s]Extractor Predicting: 167it [01:40,  1.61it/s]Extractor Predicting: 168it [01:40,  1.65it/s]Extractor Predicting: 169it [01:41,  1.71it/s]Extractor Predicting: 170it [01:42,  1.68it/s]Extractor Predicting: 171it [01:42,  1.63it/s]Extractor Predicting: 172it [01:43,  1.62it/s]Extractor Predicting: 173it [01:43,  1.66it/s]Extractor Predicting: 173it [01:43,  1.66it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:28:41,507 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:28:41,513 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:28:41,513 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:28:41,513 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:28:41,513 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:28:42,141 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:28:42,142 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:28:42,701 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:28:43,830 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:28:43,830 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:28:46,779 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:28:46,785 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:28:46,785 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:28:46,785 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:28:46,785 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:28:47,443 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:28:47,444 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:28:48,036 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:28:48,191 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:28:48,191 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2758834469931804,
  "recall": 0.10733236854799807,
  "score": 0.15454071887480467,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.81it/s]Extractor Predicting: 2it [00:01,  1.70it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.56it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.45it/s]Extractor Predicting: 9it [00:05,  1.46it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.46it/s]Extractor Predicting: 12it [00:08,  1.42it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.56it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:10,  1.73it/s]Extractor Predicting: 17it [00:10,  1.82it/s]Extractor Predicting: 18it [00:11,  1.85it/s]Extractor Predicting: 19it [00:11,  1.86it/s]Extractor Predicting: 20it [00:12,  1.89it/s]Extractor Predicting: 21it [00:12,  1.93it/s]Extractor Predicting: 22it [00:13,  1.88it/s]Extractor Predicting: 23it [00:13,  1.88it/s]Extractor Predicting: 24it [00:14,  1.87it/s]Extractor Predicting: 25it [00:14,  1.88it/s]Extractor Predicting: 26it [00:15,  1.90it/s]Extractor Predicting: 27it [00:16,  1.86it/s]Extractor Predicting: 28it [00:16,  1.88it/s]Extractor Predicting: 29it [00:17,  1.89it/s]Extractor Predicting: 30it [00:17,  1.93it/s]Extractor Predicting: 31it [00:18,  1.94it/s]Extractor Predicting: 32it [00:18,  1.94it/s]Extractor Predicting: 33it [00:19,  1.94it/s]Extractor Predicting: 34it [00:19,  1.93it/s]Extractor Predicting: 35it [00:20,  1.93it/s]Extractor Predicting: 36it [00:20,  1.88it/s]Extractor Predicting: 37it [00:21,  1.84it/s]Extractor Predicting: 38it [00:21,  1.90it/s]Extractor Predicting: 39it [00:22,  1.90it/s]Extractor Predicting: 40it [00:22,  1.91it/s]Extractor Predicting: 41it [00:23,  1.90it/s]Extractor Predicting: 42it [00:23,  1.94it/s]Extractor Predicting: 43it [00:24,  1.90it/s]Extractor Predicting: 44it [00:25,  1.74it/s]Extractor Predicting: 45it [00:25,  1.65it/s]Extractor Predicting: 46it [00:26,  1.58it/s]Extractor Predicting: 47it [00:27,  1.55it/s]Extractor Predicting: 48it [00:27,  1.53it/s]Extractor Predicting: 49it [00:28,  1.53it/s]Extractor Predicting: 50it [00:29,  1.51it/s]Extractor Predicting: 51it [00:29,  1.51it/s]Extractor Predicting: 52it [00:30,  1.40it/s]Extractor Predicting: 53it [00:31,  1.42it/s]Extractor Predicting: 54it [00:32,  1.43it/s]Extractor Predicting: 55it [00:32,  1.46it/s]Extractor Predicting: 56it [00:33,  1.44it/s]Extractor Predicting: 57it [00:34,  1.44it/s]Extractor Predicting: 58it [00:34,  1.45it/s]Extractor Predicting: 59it [00:35,  1.69it/s]Extractor Predicting: 59it [00:35,  1.68it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7715877437325905,
  "recall": 0.17911412867765922,
  "score": 0.2907373392810286,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/results_single_is_eval_True_limit5000.json'
