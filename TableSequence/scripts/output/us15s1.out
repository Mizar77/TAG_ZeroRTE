/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_15_seed_1', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Predicting: 1it [00:12, 12.87s/it]Extractor Predicting: 2it [00:15,  6.63s/it]Extractor Predicting: 3it [00:16,  4.12s/it]Extractor Predicting: 4it [00:16,  2.74s/it]Extractor Predicting: 5it [00:17,  1.98s/it]Extractor Predicting: 6it [00:18,  1.52s/it]Extractor Predicting: 7it [00:18,  1.24s/it]Extractor Predicting: 8it [00:19,  1.09s/it]Extractor Predicting: 9it [00:20,  1.06it/s]Extractor Predicting: 10it [00:20,  1.16it/s]Extractor Predicting: 11it [00:21,  1.25it/s]Extractor Predicting: 12it [00:22,  1.32it/s]Extractor Predicting: 13it [00:22,  1.41it/s]Extractor Predicting: 14it [00:23,  1.42it/s]Extractor Predicting: 15it [00:24,  1.47it/s]Extractor Predicting: 16it [00:24,  1.49it/s]Extractor Predicting: 17it [00:25,  1.54it/s]Extractor Predicting: 18it [00:25,  1.55it/s]Extractor Predicting: 19it [00:26,  1.57it/s]Extractor Predicting: 20it [00:27,  1.56it/s]Extractor Predicting: 21it [00:27,  1.56it/s]Extractor Predicting: 22it [00:28,  1.57it/s]Extractor Predicting: 23it [00:29,  1.60it/s]Extractor Predicting: 24it [00:29,  1.61it/s]Extractor Predicting: 25it [00:30,  1.57it/s]Extractor Predicting: 26it [00:31,  1.55it/s]Extractor Predicting: 27it [00:31,  1.57it/s]Extractor Predicting: 28it [00:32,  1.54it/s]Extractor Predicting: 29it [00:33,  1.52it/s]Extractor Predicting: 30it [00:33,  1.53it/s]Extractor Predicting: 31it [00:34,  1.53it/s]Extractor Predicting: 32it [00:34,  1.54it/s]Extractor Predicting: 33it [00:35,  1.51it/s]Extractor Predicting: 34it [00:36,  1.52it/s]Extractor Predicting: 35it [00:36,  1.56it/s]Extractor Predicting: 36it [00:37,  1.58it/s]Extractor Predicting: 37it [00:38,  1.57it/s]Extractor Predicting: 38it [00:38,  1.51it/s]Extractor Predicting: 39it [00:39,  1.54it/s]Extractor Predicting: 40it [00:40,  1.54it/s]Extractor Predicting: 41it [00:40,  1.58it/s]Extractor Predicting: 42it [00:41,  1.58it/s]Extractor Predicting: 43it [00:42,  1.56it/s]Extractor Predicting: 44it [00:42,  1.56it/s]Extractor Predicting: 45it [00:43,  1.54it/s]Extractor Predicting: 46it [00:43,  1.56it/s]Extractor Predicting: 47it [00:44,  1.58it/s]Extractor Predicting: 48it [00:45,  1.58it/s]Extractor Predicting: 49it [00:45,  1.60it/s]Extractor Predicting: 50it [00:46,  1.64it/s]Extractor Predicting: 51it [00:47,  1.63it/s]Extractor Predicting: 52it [00:47,  1.58it/s]Extractor Predicting: 53it [00:48,  1.57it/s]Extractor Predicting: 54it [00:48,  1.58it/s]Extractor Predicting: 55it [00:49,  1.56it/s]Extractor Predicting: 56it [00:50,  1.54it/s]Extractor Predicting: 57it [00:50,  1.53it/s]Extractor Predicting: 58it [00:51,  1.56it/s]Extractor Predicting: 59it [00:52,  1.60it/s]Extractor Predicting: 60it [00:52,  1.61it/s]Extractor Predicting: 61it [00:53,  1.61it/s]Extractor Predicting: 62it [00:54,  1.63it/s]Extractor Predicting: 63it [00:54,  1.61it/s]Extractor Predicting: 64it [00:55,  1.61it/s]Extractor Predicting: 65it [00:55,  1.62it/s]Extractor Predicting: 66it [00:56,  1.59it/s]Extractor Predicting: 67it [00:57,  1.60it/s]Extractor Predicting: 68it [00:57,  1.59it/s]Extractor Predicting: 69it [00:58,  1.60it/s]Extractor Predicting: 70it [00:59,  1.58it/s]Extractor Predicting: 71it [00:59,  1.57it/s]Extractor Predicting: 72it [01:00,  1.59it/s]Extractor Predicting: 73it [01:00,  1.59it/s]Extractor Predicting: 74it [01:01,  1.61it/s]Extractor Predicting: 75it [01:02,  1.64it/s]Extractor Predicting: 76it [01:02,  1.61it/s]Extractor Predicting: 77it [01:03,  1.57it/s]Extractor Predicting: 78it [01:04,  1.57it/s]Extractor Predicting: 79it [01:04,  1.54it/s]Extractor Predicting: 80it [01:05,  1.54it/s]Extractor Predicting: 81it [01:06,  1.52it/s]Extractor Predicting: 82it [01:06,  1.55it/s]Extractor Predicting: 83it [01:07,  1.56it/s]Extractor Predicting: 84it [01:07,  1.56it/s]Extractor Predicting: 85it [01:08,  1.55it/s]Extractor Predicting: 86it [01:09,  1.55it/s]Extractor Predicting: 87it [01:09,  1.55it/s]Extractor Predicting: 88it [01:10,  1.55it/s]Extractor Predicting: 89it [01:11,  1.58it/s]Extractor Predicting: 90it [01:11,  1.58it/s]Extractor Predicting: 91it [01:12,  1.62it/s]Extractor Predicting: 92it [01:12,  1.65it/s]Extractor Predicting: 93it [01:13,  1.66it/s]Extractor Predicting: 94it [01:14,  1.61it/s]Extractor Predicting: 95it [01:14,  1.65it/s]Extractor Predicting: 96it [01:15,  1.62it/s]Extractor Predicting: 97it [01:16,  1.62it/s]Extractor Predicting: 98it [01:16,  1.62it/s]Extractor Predicting: 99it [01:17,  1.58it/s]Extractor Predicting: 100it [01:18,  1.54it/s]Extractor Predicting: 101it [01:18,  1.43it/s]Extractor Predicting: 102it [01:19,  1.53it/s]Extractor Predicting: 103it [01:19,  1.57it/s]Extractor Predicting: 104it [01:20,  1.58it/s]Extractor Predicting: 105it [01:21,  1.59it/s]Extractor Predicting: 106it [01:21,  1.60it/s]Extractor Predicting: 107it [01:22,  1.62it/s]Extractor Predicting: 108it [01:23,  1.61it/s]Extractor Predicting: 109it [01:23,  1.61it/s]Extractor Predicting: 110it [01:24,  1.62it/s]Extractor Predicting: 111it [01:24,  1.63it/s]Extractor Predicting: 112it [01:25,  1.62it/s]Extractor Predicting: 113it [01:26,  1.67it/s]Extractor Predicting: 114it [01:26,  1.70it/s]Extractor Predicting: 115it [01:27,  1.68it/s]Extractor Predicting: 116it [01:27,  1.67it/s]Extractor Predicting: 117it [01:28,  1.68it/s]Extractor Predicting: 118it [01:29,  1.65it/s]Extractor Predicting: 119it [01:29,  1.66it/s]Extractor Predicting: 120it [01:30,  1.62it/s]Extractor Predicting: 121it [01:31,  1.59it/s]Extractor Predicting: 122it [01:31,  1.59it/s]Extractor Predicting: 123it [01:32,  1.60it/s]Extractor Predicting: 124it [01:32,  1.58it/s]Extractor Predicting: 125it [01:33,  1.59it/s]Extractor Predicting: 126it [01:34,  1.60it/s]Extractor Predicting: 127it [01:34,  1.59it/s]Extractor Predicting: 128it [01:35,  1.59it/s]Extractor Predicting: 129it [01:36,  1.59it/s]Extractor Predicting: 130it [01:36,  1.57it/s]Extractor Predicting: 131it [01:37,  1.58it/s]Extractor Predicting: 132it [01:37,  1.56it/s]Extractor Predicting: 133it [01:38,  1.58it/s]Extractor Predicting: 134it [01:39,  1.61it/s]Extractor Predicting: 135it [01:39,  1.60it/s]Extractor Predicting: 136it [01:40,  1.57it/s]Extractor Predicting: 137it [01:41,  1.60it/s]Extractor Predicting: 138it [01:41,  1.59it/s]Extractor Predicting: 139it [01:42,  1.57it/s]Extractor Predicting: 140it [01:42,  1.58it/s]Extractor Predicting: 141it [01:43,  1.60it/s]Extractor Predicting: 142it [01:44,  1.58it/s]Extractor Predicting: 143it [01:44,  1.60it/s]Extractor Predicting: 144it [01:45,  1.64it/s]Extractor Predicting: 144it [01:45,  1.37it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.47746478873239434,
  "recall": 0.09749784296807593,
  "score": 0.1619297826606162,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.56it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.66it/s]Extractor Predicting: 8it [00:04,  1.69it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.66it/s]Extractor Predicting: 11it [00:06,  1.66it/s]Extractor Predicting: 12it [00:07,  1.64it/s]Extractor Predicting: 13it [00:07,  1.62it/s]Extractor Predicting: 14it [00:08,  1.64it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:11,  1.64it/s]Extractor Predicting: 19it [00:11,  1.68it/s]Extractor Predicting: 20it [00:12,  1.64it/s]Extractor Predicting: 21it [00:12,  1.64it/s]Extractor Predicting: 22it [00:13,  1.66it/s]Extractor Predicting: 23it [00:14,  1.66it/s]Extractor Predicting: 24it [00:14,  1.63it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:15,  1.63it/s]Extractor Predicting: 27it [00:16,  1.61it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:17,  1.63it/s]Extractor Predicting: 30it [00:18,  1.66it/s]Extractor Predicting: 31it [00:18,  1.63it/s]Extractor Predicting: 32it [00:19,  1.69it/s]Extractor Predicting: 33it [00:20,  1.65it/s]Extractor Predicting: 34it [00:20,  1.62it/s]Extractor Predicting: 35it [00:21,  1.62it/s]Extractor Predicting: 36it [00:21,  1.64it/s]Extractor Predicting: 37it [00:22,  1.66it/s]Extractor Predicting: 38it [00:23,  1.68it/s]Extractor Predicting: 39it [00:23,  1.67it/s]Extractor Predicting: 40it [00:24,  1.65it/s]Extractor Predicting: 41it [00:24,  1.65it/s]Extractor Predicting: 42it [00:25,  1.68it/s]Extractor Predicting: 43it [00:26,  1.63it/s]Extractor Predicting: 44it [00:26,  1.63it/s]Extractor Predicting: 45it [00:27,  1.60it/s]Extractor Predicting: 46it [00:28,  1.59it/s]Extractor Predicting: 47it [00:28,  1.62it/s]Extractor Predicting: 48it [00:29,  1.60it/s]Extractor Predicting: 49it [00:29,  1.63it/s]Extractor Predicting: 50it [00:30,  1.62it/s]Extractor Predicting: 51it [00:31,  1.61it/s]Extractor Predicting: 52it [00:31,  1.59it/s]Extractor Predicting: 53it [00:32,  1.62it/s]Extractor Predicting: 54it [00:33,  1.59it/s]Extractor Predicting: 55it [00:33,  1.64it/s]Extractor Predicting: 56it [00:34,  1.62it/s]Extractor Predicting: 57it [00:34,  1.60it/s]Extractor Predicting: 58it [00:35,  1.57it/s]Extractor Predicting: 59it [00:36,  1.55it/s]Extractor Predicting: 60it [00:36,  1.58it/s]Extractor Predicting: 61it [00:37,  1.56it/s]Extractor Predicting: 62it [00:38,  1.57it/s]Extractor Predicting: 63it [00:38,  1.59it/s]Extractor Predicting: 64it [00:39,  1.58it/s]Extractor Predicting: 65it [00:39,  1.63it/s]Extractor Predicting: 66it [00:40,  1.59it/s]Extractor Predicting: 67it [00:41,  1.57it/s]Extractor Predicting: 68it [00:41,  1.56it/s]Extractor Predicting: 69it [00:42,  1.57it/s]Extractor Predicting: 70it [00:43,  1.54it/s]Extractor Predicting: 71it [00:43,  1.56it/s]Extractor Predicting: 72it [00:44,  1.55it/s]Extractor Predicting: 73it [00:45,  1.54it/s]Extractor Predicting: 74it [00:45,  1.55it/s]Extractor Predicting: 75it [00:46,  1.56it/s]Extractor Predicting: 76it [00:47,  1.57it/s]Extractor Predicting: 77it [00:47,  1.58it/s]Extractor Predicting: 78it [00:48,  1.60it/s]Extractor Predicting: 79it [00:48,  1.59it/s]Extractor Predicting: 80it [00:49,  1.65it/s]Extractor Predicting: 81it [00:50,  1.65it/s]Extractor Predicting: 82it [00:50,  1.64it/s]Extractor Predicting: 83it [00:51,  1.63it/s]Extractor Predicting: 84it [00:51,  1.62it/s]Extractor Predicting: 85it [00:52,  1.60it/s]Extractor Predicting: 86it [00:53,  1.54it/s]Extractor Predicting: 87it [00:53,  1.52it/s]Extractor Predicting: 88it [00:54,  1.55it/s]Extractor Predicting: 89it [00:55,  1.41it/s]Extractor Predicting: 90it [00:56,  1.42it/s]Extractor Predicting: 91it [00:56,  1.45it/s]Extractor Predicting: 92it [00:57,  1.48it/s]Extractor Predicting: 93it [00:58,  1.51it/s]Extractor Predicting: 94it [00:58,  1.54it/s]Extractor Predicting: 95it [00:59,  1.54it/s]Extractor Predicting: 96it [01:00,  1.55it/s]Extractor Predicting: 97it [01:00,  1.53it/s]Extractor Predicting: 98it [01:01,  1.50it/s]Extractor Predicting: 99it [01:02,  1.51it/s]Extractor Predicting: 100it [01:02,  1.54it/s]Extractor Predicting: 101it [01:03,  1.55it/s]Extractor Predicting: 102it [01:03,  1.57it/s]Extractor Predicting: 103it [01:04,  1.54it/s]Extractor Predicting: 104it [01:05,  1.53it/s]Extractor Predicting: 105it [01:05,  1.56it/s]Extractor Predicting: 106it [01:06,  1.55it/s]Extractor Predicting: 107it [01:07,  1.56it/s]Extractor Predicting: 108it [01:07,  1.57it/s]Extractor Predicting: 109it [01:08,  1.55it/s]Extractor Predicting: 110it [01:09,  1.54it/s]Extractor Predicting: 111it [01:09,  1.55it/s]Extractor Predicting: 112it [01:10,  1.54it/s]Extractor Predicting: 113it [01:11,  1.55it/s]Extractor Predicting: 114it [01:11,  1.57it/s]Extractor Predicting: 115it [01:12,  1.56it/s]Extractor Predicting: 116it [01:12,  1.52it/s]Extractor Predicting: 117it [01:13,  1.52it/s]Extractor Predicting: 118it [01:14,  1.54it/s]Extractor Predicting: 119it [01:14,  1.53it/s]Extractor Predicting: 120it [01:15,  1.53it/s]Extractor Predicting: 121it [01:16,  1.54it/s]Extractor Predicting: 122it [01:16,  1.57it/s]Extractor Predicting: 123it [01:17,  1.58it/s]Extractor Predicting: 124it [01:18,  1.58it/s]Extractor Predicting: 125it [01:18,  1.57it/s]Extractor Predicting: 126it [01:19,  1.64it/s]Extractor Predicting: 127it [01:19,  1.63it/s]Extractor Predicting: 128it [01:20,  1.60it/s]Extractor Predicting: 129it [01:21,  1.64it/s]Extractor Predicting: 130it [01:21,  1.63it/s]Extractor Predicting: 131it [01:22,  1.61it/s]Extractor Predicting: 132it [01:23,  1.62it/s]Extractor Predicting: 133it [01:23,  1.67it/s]Extractor Predicting: 134it [01:24,  1.68it/s]Extractor Predicting: 135it [01:24,  1.62it/s]Extractor Predicting: 136it [01:25,  1.63it/s]Extractor Predicting: 137it [01:26,  1.65it/s]Extractor Predicting: 138it [01:26,  1.69it/s]Extractor Predicting: 139it [01:27,  1.67it/s]Extractor Predicting: 140it [01:27,  1.65it/s]Extractor Predicting: 141it [01:28,  1.63it/s]Extractor Predicting: 142it [01:29,  1.65it/s]Extractor Predicting: 143it [01:29,  1.62it/s]Extractor Predicting: 144it [01:30,  1.60it/s]Extractor Predicting: 145it [01:30,  1.60it/s]Extractor Predicting: 146it [01:31,  1.62it/s]Extractor Predicting: 147it [01:32,  1.67it/s]Extractor Predicting: 148it [01:32,  1.67it/s]Extractor Predicting: 149it [01:33,  1.69it/s]Extractor Predicting: 150it [01:33,  1.72it/s]Extractor Predicting: 151it [01:34,  1.72it/s]Extractor Predicting: 152it [01:34,  1.72it/s]Extractor Predicting: 153it [01:35,  1.71it/s]Extractor Predicting: 154it [01:36,  1.74it/s]Extractor Predicting: 155it [01:36,  1.71it/s]Extractor Predicting: 156it [01:37,  1.73it/s]Extractor Predicting: 157it [01:37,  1.76it/s]Extractor Predicting: 158it [01:38,  1.75it/s]Extractor Predicting: 159it [01:38,  1.80it/s]Extractor Predicting: 160it [01:39,  1.84it/s]Extractor Predicting: 161it [01:40,  1.78it/s]Extractor Predicting: 162it [01:40,  1.73it/s]Extractor Predicting: 163it [01:41,  1.72it/s]Extractor Predicting: 164it [01:41,  1.72it/s]Extractor Predicting: 165it [01:42,  1.75it/s]Extractor Predicting: 166it [01:42,  1.77it/s]Extractor Predicting: 167it [01:43,  1.76it/s]Extractor Predicting: 168it [01:44,  1.72it/s]Extractor Predicting: 169it [01:44,  1.76it/s]Extractor Predicting: 170it [01:45,  1.74it/s]Extractor Predicting: 171it [01:45,  1.78it/s]Extractor Predicting: 172it [01:46,  1.77it/s]Extractor Predicting: 173it [01:46,  1.77it/s]Extractor Predicting: 174it [01:47,  1.66it/s]Extractor Predicting: 175it [01:48,  1.66it/s]Extractor Predicting: 176it [01:48,  1.63it/s]Extractor Predicting: 177it [01:49,  1.62it/s]Extractor Predicting: 178it [01:50,  1.57it/s]Extractor Predicting: 179it [01:50,  1.56it/s]Extractor Predicting: 180it [01:51,  1.59it/s]Extractor Predicting: 181it [01:52,  1.57it/s]Extractor Predicting: 182it [01:52,  1.58it/s]Extractor Predicting: 183it [01:53,  1.60it/s]Extractor Predicting: 184it [01:53,  1.59it/s]Extractor Predicting: 185it [01:54,  1.59it/s]Extractor Predicting: 186it [01:55,  1.61it/s]Extractor Predicting: 187it [01:55,  1.60it/s]Extractor Predicting: 188it [01:56,  1.57it/s]Extractor Predicting: 189it [01:57,  1.59it/s]Extractor Predicting: 190it [01:57,  1.57it/s]Extractor Predicting: 191it [01:58,  1.55it/s]Extractor Predicting: 192it [01:59,  1.54it/s]Extractor Predicting: 193it [01:59,  1.53it/s]Extractor Predicting: 194it [02:00,  1.52it/s]Extractor Predicting: 195it [02:01,  1.54it/s]Extractor Predicting: 196it [02:01,  1.57it/s]Extractor Predicting: 197it [02:02,  1.58it/s]Extractor Predicting: 198it [02:02,  1.57it/s]Extractor Predicting: 199it [02:03,  1.58it/s]Extractor Predicting: 200it [02:04,  1.54it/s]Extractor Predicting: 201it [02:04,  1.52it/s]Extractor Predicting: 202it [02:05,  1.50it/s]Extractor Predicting: 203it [02:06,  1.47it/s]Extractor Predicting: 204it [02:06,  1.47it/s]Extractor Predicting: 205it [02:07,  1.47it/s]Extractor Predicting: 206it [02:08,  1.47it/s]Extractor Predicting: 207it [02:08,  1.49it/s]Extractor Predicting: 208it [02:09,  1.48it/s]Extractor Predicting: 209it [02:10,  1.31it/s]Extractor Predicting: 210it [02:11,  1.35it/s]Extractor Predicting: 211it [02:12,  1.38it/s]Extractor Predicting: 212it [02:12,  1.42it/s]Extractor Predicting: 213it [02:13,  1.44it/s]Extractor Predicting: 214it [02:13,  1.47it/s]Extractor Predicting: 215it [02:14,  1.45it/s]Extractor Predicting: 216it [02:15,  1.46it/s]Extractor Predicting: 217it [02:16,  1.48it/s]Extractor Predicting: 218it [02:16,  1.47it/s]Extractor Predicting: 219it [02:17,  1.47it/s]Extractor Predicting: 220it [02:18,  1.43it/s]Extractor Predicting: 221it [02:18,  1.43it/s]Extractor Predicting: 222it [02:19,  1.42it/s]Extractor Predicting: 223it [02:20,  1.46it/s]Extractor Predicting: 224it [02:20,  1.48it/s]Extractor Predicting: 225it [02:21,  1.48it/s]Extractor Predicting: 226it [02:22,  1.46it/s]Extractor Predicting: 227it [02:22,  1.47it/s]Extractor Predicting: 228it [02:23,  1.48it/s]Extractor Predicting: 229it [02:24,  1.50it/s]Extractor Predicting: 230it [02:24,  1.52it/s]Extractor Predicting: 231it [02:25,  1.56it/s]Extractor Predicting: 232it [02:26,  1.60it/s]Extractor Predicting: 233it [02:26,  1.62it/s]Extractor Predicting: 234it [02:27,  1.57it/s]Extractor Predicting: 235it [02:27,  1.59it/s]Extractor Predicting: 236it [02:28,  1.59it/s]Extractor Predicting: 237it [02:29,  1.59it/s]Extractor Predicting: 238it [02:29,  1.60it/s]Extractor Predicting: 239it [02:30,  1.57it/s]Extractor Predicting: 240it [02:31,  1.61it/s]Extractor Predicting: 241it [02:31,  1.62it/s]Extractor Predicting: 242it [02:32,  1.64it/s]Extractor Predicting: 243it [02:32,  1.63it/s]Extractor Predicting: 244it [02:33,  1.68it/s]Extractor Predicting: 245it [02:34,  1.64it/s]Extractor Predicting: 246it [02:34,  1.61it/s]Extractor Predicting: 247it [02:35,  1.58it/s]Extractor Predicting: 248it [02:36,  1.56it/s]Extractor Predicting: 249it [02:36,  1.59it/s]Extractor Predicting: 250it [02:37,  1.59it/s]Extractor Predicting: 251it [02:37,  1.61it/s]Extractor Predicting: 252it [02:38,  1.64it/s]Extractor Predicting: 253it [02:39,  1.62it/s]Extractor Predicting: 254it [02:39,  1.59it/s]Extractor Predicting: 255it [02:40,  1.59it/s]Extractor Predicting: 256it [02:41,  1.57it/s]Extractor Predicting: 257it [02:41,  1.54it/s]Extractor Predicting: 258it [02:42,  1.54it/s]Extractor Predicting: 259it [02:42,  1.55it/s]Extractor Predicting: 260it [02:43,  1.54it/s]Extractor Predicting: 261it [02:44,  1.54it/s]Extractor Predicting: 262it [02:44,  1.55it/s]Extractor Predicting: 263it [02:45,  1.58it/s]Extractor Predicting: 264it [02:46,  1.56it/s]Extractor Predicting: 265it [02:46,  1.56it/s]Extractor Predicting: 266it [02:47,  1.54it/s]Extractor Predicting: 267it [02:48,  1.53it/s]Extractor Predicting: 268it [02:48,  1.51it/s]Extractor Predicting: 269it [02:49,  1.52it/s]Extractor Predicting: 270it [02:50,  1.53it/s]Extractor Predicting: 271it [02:50,  1.53it/s]Extractor Predicting: 272it [02:51,  1.56it/s]Extractor Predicting: 273it [02:52,  1.52it/s]Extractor Predicting: 274it [02:52,  1.50it/s]Extractor Predicting: 275it [02:53,  1.51it/s]Extractor Predicting: 276it [02:54,  1.51it/s]Extractor Predicting: 277it [02:54,  1.52it/s]Extractor Predicting: 278it [02:55,  1.53it/s]Extractor Predicting: 279it [02:56,  1.51it/s]Extractor Predicting: 280it [02:56,  1.52it/s]Extractor Predicting: 281it [02:57,  1.55it/s]Extractor Predicting: 282it [02:58,  1.53it/s]Extractor Predicting: 283it [02:58,  1.50it/s]Extractor Predicting: 284it [02:59,  1.56it/s]Extractor Predicting: 285it [02:59,  1.55it/s]Extractor Predicting: 286it [03:00,  1.57it/s]Extractor Predicting: 287it [03:01,  1.57it/s]Extractor Predicting: 288it [03:01,  1.55it/s]Extractor Predicting: 289it [03:02,  1.55it/s]Extractor Predicting: 290it [03:03,  1.53it/s]Extractor Predicting: 291it [03:03,  1.48it/s]Extractor Predicting: 292it [03:04,  1.47it/s]Extractor Predicting: 293it [03:05,  1.51it/s]Extractor Predicting: 294it [03:05,  1.48it/s]Extractor Predicting: 295it [03:06,  1.50it/s]Extractor Predicting: 296it [03:07,  1.49it/s]Extractor Predicting: 297it [03:07,  1.51it/s]Extractor Predicting: 298it [03:08,  1.49it/s]Extractor Predicting: 299it [03:09,  1.47it/s]Extractor Predicting: 300it [03:09,  1.50it/s]Extractor Predicting: 301it [03:10,  1.49it/s]Extractor Predicting: 302it [03:11,  1.52it/s]Extractor Predicting: 303it [03:12,  1.33it/s]Extractor Predicting: 304it [03:12,  1.37it/s]Extractor Predicting: 305it [03:13,  1.41it/s]Extractor Predicting: 306it [03:14,  1.46it/s]Extractor Predicting: 307it [03:14,  1.46it/s]Extractor Predicting: 308it [03:15,  1.49it/s]Extractor Predicting: 309it [03:16,  1.51it/s]Extractor Predicting: 310it [03:16,  1.53it/s]Extractor Predicting: 311it [03:17,  1.52it/s]Extractor Predicting: 312it [03:18,  1.57it/s]Extractor Predicting: 313it [03:18,  1.61it/s]Extractor Predicting: 314it [03:19,  1.63it/s]Extractor Predicting: 315it [03:19,  1.64it/s]Extractor Predicting: 316it [03:20,  1.61it/s]Extractor Predicting: 317it [03:21,  1.58it/s]Extractor Predicting: 318it [03:21,  1.57it/s]Extractor Predicting: 319it [03:22,  1.56it/s]Extractor Predicting: 320it [03:23,  1.55it/s]Extractor Predicting: 321it [03:23,  1.55it/s]Extractor Predicting: 322it [03:24,  1.55it/s]Extractor Predicting: 323it [03:25,  1.51it/s]Extractor Predicting: 324it [03:25,  1.52it/s]Extractor Predicting: 325it [03:26,  1.52it/s]Extractor Predicting: 326it [03:27,  1.53it/s]Extractor Predicting: 327it [03:27,  1.56it/s]Extractor Predicting: 328it [03:28,  1.54it/s]Extractor Predicting: 329it [03:28,  1.56it/s]Extractor Predicting: 330it [03:29,  1.55it/s]Extractor Predicting: 331it [03:30,  1.54it/s]Extractor Predicting: 332it [03:30,  1.53it/s]Extractor Predicting: 333it [03:31,  1.56it/s]Extractor Predicting: 334it [03:32,  1.56it/s]Extractor Predicting: 335it [03:32,  1.57it/s]Extractor Predicting: 336it [03:33,  1.54it/s]Extractor Predicting: 337it [03:34,  1.55it/s]Extractor Predicting: 338it [03:34,  1.56it/s]Extractor Predicting: 339it [03:35,  1.60it/s]Extractor Predicting: 340it [03:35,  1.60it/s]Extractor Predicting: 341it [03:36,  1.60it/s]Extractor Predicting: 342it [03:37,  1.58it/s]Extractor Predicting: 343it [03:37,  1.56it/s]Extractor Predicting: 344it [03:38,  1.58it/s]Extractor Predicting: 345it [03:39,  1.54it/s]Extractor Predicting: 346it [03:39,  1.55it/s]Extractor Predicting: 347it [03:40,  1.55it/s]Extractor Predicting: 348it [03:41,  1.55it/s]Extractor Predicting: 349it [03:41,  1.57it/s]Extractor Predicting: 350it [03:42,  1.58it/s]Extractor Predicting: 351it [03:42,  1.59it/s]Extractor Predicting: 352it [03:43,  1.55it/s]Extractor Predicting: 353it [03:44,  1.57it/s]Extractor Predicting: 354it [03:44,  1.60it/s]Extractor Predicting: 355it [03:45,  1.62it/s]Extractor Predicting: 356it [03:46,  1.60it/s]Extractor Predicting: 357it [03:46,  1.59it/s]Extractor Predicting: 358it [03:47,  1.59it/s]Extractor Predicting: 359it [03:48,  1.57it/s]Extractor Predicting: 360it [03:48,  1.57it/s]Extractor Predicting: 361it [03:49,  1.58it/s]Extractor Predicting: 362it [03:49,  1.59it/s]Extractor Predicting: 363it [03:50,  1.60it/s]Extractor Predicting: 364it [03:51,  1.61it/s]Extractor Predicting: 365it [03:51,  1.63it/s]Extractor Predicting: 366it [03:52,  1.61it/s]Extractor Predicting: 367it [03:53,  1.56it/s]Extractor Predicting: 368it [03:53,  1.56it/s]Extractor Predicting: 369it [03:54,  1.57it/s]Extractor Predicting: 370it [03:54,  1.62it/s]Extractor Predicting: 371it [03:55,  1.63it/s]Extractor Predicting: 372it [03:56,  1.62it/s]Extractor Predicting: 373it [03:56,  1.59it/s]Extractor Predicting: 374it [03:57,  1.61it/s]Extractor Predicting: 375it [03:57,  1.61it/s]Extractor Predicting: 376it [03:58,  1.62it/s]Extractor Predicting: 377it [03:59,  1.63it/s]Extractor Predicting: 378it [03:59,  1.65it/s]Extractor Predicting: 379it [04:00,  1.60it/s]Extractor Predicting: 380it [04:01,  1.60it/s]Extractor Predicting: 381it [04:01,  1.61it/s]Extractor Predicting: 382it [04:02,  1.61it/s]Extractor Predicting: 383it [04:02,  1.64it/s]Extractor Predicting: 384it [04:03,  1.67it/s]Extractor Predicting: 385it [04:04,  1.63it/s]Extractor Predicting: 386it [04:04,  1.63it/s]Extractor Predicting: 387it [04:05,  1.62it/s]Extractor Predicting: 388it [04:05,  1.61it/s]Extractor Predicting: 389it [04:06,  1.61it/s]Extractor Predicting: 390it [04:07,  1.60it/s]Extractor Predicting: 391it [04:07,  1.58it/s]Extractor Predicting: 392it [04:08,  1.59it/s]Extractor Predicting: 393it [04:09,  1.62it/s]Extractor Predicting: 394it [04:09,  1.55it/s]Extractor Predicting: 395it [04:10,  1.51it/s]Extractor Predicting: 396it [04:11,  1.50it/s]Extractor Predicting: 397it [04:11,  1.49it/s]Extractor Predicting: 398it [04:12,  1.49it/s]Extractor Predicting: 399it [04:13,  1.48it/s]Extractor Predicting: 400it [04:13,  1.53it/s]Extractor Predicting: 401it [04:14,  1.50it/s]Extractor Predicting: 402it [04:15,  1.32it/s]Extractor Predicting: 403it [04:16,  1.37it/s]Extractor Predicting: 404it [04:16,  1.36it/s]Extractor Predicting: 405it [04:17,  1.39it/s]Extractor Predicting: 406it [04:18,  1.41it/s]Extractor Predicting: 407it [04:18,  1.41it/s]Extractor Predicting: 408it [04:19,  1.45it/s]Extractor Predicting: 409it [04:20,  1.47it/s]Extractor Predicting: 410it [04:20,  1.45it/s]Extractor Predicting: 411it [04:21,  1.46it/s]Extractor Predicting: 412it [04:22,  1.48it/s]Extractor Predicting: 413it [04:22,  1.50it/s]Extractor Predicting: 414it [04:23,  1.53it/s]Extractor Predicting: 415it [04:24,  1.56it/s]Extractor Predicting: 416it [04:24,  1.56it/s]Extractor Predicting: 417it [04:25,  1.56it/s]Extractor Predicting: 418it [04:26,  1.54it/s]Extractor Predicting: 419it [04:26,  1.49it/s]Extractor Predicting: 420it [04:27,  1.50it/s]Extractor Predicting: 421it [04:28,  1.60it/s]Extractor Predicting: 421it [04:28,  1.57it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.30120481927710846,
  "recall": 0.05943536404160475,
  "score": 0.0992802184164805,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.43it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.46it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:04,  1.50it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:06,  1.30it/s]Extractor Predicting: 9it [00:06,  1.42it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3333333333333333,
  "recall": 0.01728395061728395,
  "score": 0.03286384976525822,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:19<06:14, 19.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:38<05:49, 19.41s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:55<05:08, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:11<04:35, 17.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:30<04:30, 18.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:45<03:57, 16.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [02:05<03:51, 17.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:24<03:40, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:42<03:19, 18.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [03:00<03:02, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:17<02:40, 17.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:34<02:20, 17.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:53<02:05, 18.00s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [04:09<01:43, 17.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:27<01:27, 17.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:44<01:09, 17.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:59<00:50, 16.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [05:18<00:34, 17.32s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:33<00:16, 16.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:51<00:00, 17.04s/it]Generating: 100%|██████████| 20/20 [05:51<00:00, 17.57s/it]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 174, 'raw': 256}
{'target': 600, 'success': 199, 'raw': 288}
{'target': 600, 'success': 221, 'raw': 320}
{'target': 600, 'success': 245, 'raw': 352}
{'target': 600, 'success': 265, 'raw': 384}
{'target': 600, 'success': 292, 'raw': 416}
{'target': 600, 'success': 318, 'raw': 448}
{'target': 600, 'success': 342, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 442, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 492, 'raw': 672}
{'target': 600, 'success': 517, 'raw': 704}
{'target': 600, 'success': 540, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 584, 'raw': 800}
{'target': 600, 'success': 612, 'raw': 832}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7355769230769231, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 245, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 359, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 433, 'raw': 576}
{'target': 600, 'success': 460, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 510, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 575, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 627, 'raw': 832}
{'prompt': 'Relation : genre .', 'success_rate': 0.7536057692307693, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : head of government .', 'success_rate': 0.8342391304347826, 'errors': {''}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : military branch .', 'success_rate': 0.8536931818181818, 'errors': {''}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 446, 'raw': 576}
{'target': 600, 'success': 469, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 587, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : winner .', 'success_rate': 0.76625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.8988095238095238, 'errors': {'', '(\'Chiba no Naihyuu\', \'characters\', \'\', \'According to the popular Japanese fiction novel , " Chiba no Naihyuu " , who had been kidnapped by a group of mysterious men with " " near " powers , has escaped as soon as her power becomes absorbed .\')'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 487, 'raw': 608}
{'target': 600, 'success': 512, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 618, 'raw': 768}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8046875, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : crosses .', 'success_rate': 0.7994791666666666, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8707386363636364, 'errors': {'', 'too many values to unpack (expected 2)', '(\'How is that a game ?\', \'distributed by\', \'\', \'" How is that a game ? "\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 628, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8532608695652174, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8735795454545454, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.90625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8098958333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 533, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8678977272727273, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 508, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 556, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : participant .', 'success_rate': 0.75375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : platform . Context : The game is a spin on " The Legend of Zelda - Twilight Princess " . Head Entity : The Legend of Zelda - Twilight Princess , Tail Entity : Wii U .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : platform .', 'success_rate': 0.8233695652173914, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 369, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 588, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.76375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8988095238095238, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8636363636363636, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/1_ext.jsonl'}}
estimate vocab size: 15385
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15485, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.64it/s]Extractor Estimating: 2it [00:05,  3.24s/it]Extractor Estimating: 3it [00:06,  2.02s/it]Extractor Estimating: 4it [00:06,  1.44s/it]Extractor Estimating: 5it [00:07,  1.17s/it]Extractor Estimating: 6it [00:08,  1.02it/s]Extractor Estimating: 7it [00:08,  1.14it/s]Extractor Estimating: 8it [00:09,  1.24it/s]Extractor Estimating: 9it [00:10,  1.32it/s]Extractor Estimating: 10it [00:10,  1.39it/s]Extractor Estimating: 11it [00:11,  1.40it/s]Extractor Estimating: 12it [00:12,  1.46it/s]Extractor Estimating: 13it [00:12,  1.52it/s]Extractor Estimating: 14it [00:13,  1.53it/s]Extractor Estimating: 15it [00:14,  1.48it/s]Extractor Estimating: 16it [00:14,  1.55it/s]Extractor Estimating: 17it [00:15,  1.53it/s]Extractor Estimating: 18it [00:15,  1.55it/s]Extractor Estimating: 19it [00:16,  1.56it/s]Extractor Estimating: 20it [00:17,  1.62it/s]Extractor Estimating: 21it [00:17,  1.65it/s]Extractor Estimating: 22it [00:18,  1.53it/s]Extractor Estimating: 23it [00:19,  1.53it/s]Extractor Estimating: 24it [00:19,  1.55it/s]Extractor Estimating: 25it [00:20,  1.57it/s]Extractor Estimating: 26it [00:22,  1.01s/it]Extractor Estimating: 27it [00:22,  1.08it/s]Extractor Estimating: 28it [00:23,  1.17it/s]Extractor Estimating: 29it [00:24,  1.19it/s]Extractor Estimating: 30it [00:25,  1.27it/s]Extractor Estimating: 31it [00:25,  1.37it/s]Extractor Estimating: 32it [00:26,  1.39it/s]Extractor Estimating: 33it [00:27,  1.37it/s]Extractor Estimating: 34it [00:27,  1.43it/s]Extractor Estimating: 35it [00:28,  1.47it/s]Extractor Estimating: 36it [00:29,  1.47it/s]Extractor Estimating: 37it [00:29,  1.49it/s]Extractor Estimating: 38it [00:30,  1.52it/s]Extractor Estimating: 39it [00:30,  1.52it/s]Extractor Estimating: 40it [00:31,  1.50it/s]Extractor Estimating: 41it [00:32,  1.53it/s]Extractor Estimating: 42it [00:32,  1.55it/s]Extractor Estimating: 43it [00:33,  1.53it/s]Extractor Estimating: 44it [00:34,  1.48it/s]Extractor Estimating: 45it [00:34,  1.50it/s]Extractor Estimating: 46it [00:35,  1.40it/s]Extractor Estimating: 47it [00:36,  1.40it/s]Extractor Estimating: 48it [00:37,  1.41it/s]Extractor Estimating: 49it [00:37,  1.48it/s]Extractor Estimating: 50it [00:38,  1.49it/s]Extractor Estimating: 51it [00:39,  1.55it/s]Extractor Estimating: 52it [00:39,  1.59it/s]Extractor Estimating: 53it [00:40,  1.52it/s]Extractor Estimating: 54it [00:40,  1.58it/s]Extractor Estimating: 55it [00:41,  1.60it/s]Extractor Estimating: 56it [00:42,  1.60it/s]Extractor Estimating: 57it [00:42,  1.60it/s]Extractor Estimating: 58it [00:43,  1.56it/s]Extractor Estimating: 59it [00:44,  1.59it/s]Extractor Estimating: 60it [00:44,  1.58it/s]Extractor Estimating: 61it [00:45,  1.60it/s]Extractor Estimating: 62it [00:45,  1.60it/s]Extractor Estimating: 63it [00:46,  1.64it/s]Extractor Estimating: 64it [00:47,  1.71it/s]Extractor Estimating: 65it [00:47,  1.63it/s]Extractor Estimating: 66it [00:48,  1.61it/s]Extractor Estimating: 67it [00:49,  1.61it/s]Extractor Estimating: 68it [00:49,  1.65it/s]Extractor Estimating: 69it [00:50,  1.64it/s]Extractor Estimating: 70it [00:50,  1.61it/s]Extractor Estimating: 71it [00:51,  1.61it/s]Extractor Estimating: 72it [00:52,  1.63it/s]Extractor Estimating: 73it [00:52,  1.57it/s]Extractor Estimating: 74it [00:53,  1.51it/s]Extractor Estimating: 75it [00:54,  1.54it/s]Extractor Estimating: 76it [00:54,  1.54it/s]Extractor Estimating: 77it [00:55,  1.51it/s]Extractor Estimating: 78it [00:56,  1.54it/s]Extractor Estimating: 79it [00:56,  1.53it/s]Extractor Estimating: 80it [00:57,  1.53it/s]Extractor Estimating: 81it [00:58,  1.50it/s]Extractor Estimating: 82it [00:58,  1.51it/s]Extractor Estimating: 83it [00:59,  1.54it/s]Extractor Estimating: 84it [00:59,  1.53it/s]Extractor Estimating: 85it [01:00,  1.47it/s]Extractor Estimating: 86it [01:01,  1.49it/s]Extractor Estimating: 87it [01:02,  1.50it/s]Extractor Estimating: 88it [01:02,  1.51it/s]Extractor Estimating: 89it [01:03,  1.53it/s]Extractor Estimating: 90it [01:03,  1.52it/s]Extractor Estimating: 91it [01:04,  1.50it/s]Extractor Estimating: 92it [01:05,  1.46it/s]Extractor Estimating: 93it [01:06,  1.48it/s]Extractor Estimating: 94it [01:06,  1.46it/s]Extractor Estimating: 95it [01:07,  1.46it/s]Extractor Estimating: 96it [01:08,  1.48it/s]Extractor Estimating: 97it [01:08,  1.48it/s]Extractor Estimating: 98it [01:09,  1.47it/s]Extractor Estimating: 99it [01:10,  1.47it/s]Extractor Estimating: 100it [01:10,  1.47it/s]Extractor Estimating: 101it [01:11,  1.48it/s]Extractor Estimating: 102it [01:12,  1.50it/s]Extractor Estimating: 103it [01:12,  1.52it/s]Extractor Estimating: 104it [01:13,  1.56it/s]Extractor Estimating: 105it [01:14,  1.56it/s]Extractor Estimating: 106it [01:14,  1.53it/s]Extractor Estimating: 107it [01:15,  1.54it/s]Extractor Estimating: 108it [01:16,  1.51it/s]Extractor Estimating: 109it [01:16,  1.55it/s]Extractor Estimating: 110it [01:17,  1.60it/s]Extractor Estimating: 111it [01:17,  1.58it/s]Extractor Estimating: 112it [01:18,  1.57it/s]Extractor Estimating: 113it [01:19,  1.58it/s]Extractor Estimating: 114it [01:19,  1.58it/s]Extractor Estimating: 115it [01:20,  1.56it/s]Extractor Estimating: 116it [01:21,  1.58it/s]Extractor Estimating: 117it [01:21,  1.50it/s]Extractor Estimating: 118it [01:22,  1.51it/s]Extractor Estimating: 119it [01:23,  1.56it/s]Extractor Estimating: 120it [01:23,  1.59it/s]Extractor Estimating: 121it [01:24,  1.52it/s]Extractor Estimating: 122it [01:24,  1.55it/s]Extractor Estimating: 123it [01:25,  1.57it/s]Extractor Estimating: 124it [01:26,  1.54it/s]Extractor Estimating: 125it [01:26,  1.56it/s]Extractor Estimating: 126it [01:27,  1.50it/s]Extractor Estimating: 127it [01:28,  1.47it/s]Extractor Estimating: 128it [01:29,  1.32it/s]Extractor Estimating: 129it [01:29,  1.38it/s]Extractor Estimating: 130it [01:30,  1.38it/s]Extractor Estimating: 131it [01:31,  1.38it/s]Extractor Estimating: 132it [01:32,  1.41it/s]Extractor Estimating: 133it [01:32,  1.43it/s]Extractor Estimating: 134it [01:33,  1.47it/s]Extractor Estimating: 135it [01:34,  1.46it/s]Extractor Estimating: 136it [01:34,  1.44it/s]Extractor Estimating: 137it [01:35,  1.50it/s]Extractor Estimating: 138it [01:36,  1.48it/s]Extractor Estimating: 139it [01:36,  1.52it/s]Extractor Estimating: 140it [01:37,  1.47it/s]Extractor Estimating: 141it [01:38,  1.44it/s]Extractor Estimating: 142it [01:38,  1.40it/s]Extractor Estimating: 143it [01:39,  1.42it/s]Extractor Estimating: 144it [01:40,  1.45it/s]Extractor Estimating: 145it [01:40,  1.46it/s]Extractor Estimating: 146it [01:41,  1.48it/s]Extractor Estimating: 147it [01:42,  1.47it/s]Extractor Estimating: 148it [01:43,  1.43it/s]Extractor Estimating: 149it [01:43,  1.46it/s]Extractor Estimating: 150it [01:44,  1.49it/s]Extractor Estimating: 151it [01:44,  1.52it/s]Extractor Estimating: 152it [01:45,  1.56it/s]Extractor Estimating: 153it [01:46,  1.59it/s]Extractor Estimating: 154it [01:46,  1.65it/s]Extractor Estimating: 155it [01:47,  1.65it/s]Extractor Estimating: 156it [01:47,  1.64it/s]Extractor Estimating: 157it [01:48,  1.63it/s]Extractor Estimating: 158it [01:49,  1.63it/s]Extractor Estimating: 159it [01:49,  1.63it/s]Extractor Estimating: 160it [01:50,  1.58it/s]Extractor Estimating: 161it [01:51,  1.58it/s]Extractor Estimating: 162it [01:51,  1.56it/s]Extractor Estimating: 163it [01:52,  1.60it/s]Extractor Estimating: 164it [01:52,  1.59it/s]Extractor Estimating: 165it [01:53,  1.59it/s]Extractor Estimating: 166it [01:54,  1.51it/s]Extractor Estimating: 167it [01:54,  1.57it/s]Extractor Estimating: 168it [01:55,  1.57it/s]Extractor Estimating: 169it [01:56,  1.59it/s]Extractor Estimating: 170it [01:56,  1.57it/s]Extractor Estimating: 171it [01:57,  1.54it/s]Extractor Estimating: 172it [01:58,  1.55it/s]Extractor Estimating: 173it [01:58,  1.51it/s]Extractor Estimating: 174it [01:59,  1.53it/s]Extractor Estimating: 175it [02:00,  1.52it/s]Extractor Estimating: 176it [02:00,  1.48it/s]Extractor Estimating: 177it [02:01,  1.46it/s]Extractor Estimating: 178it [02:02,  1.49it/s]Extractor Estimating: 179it [02:02,  1.50it/s]Extractor Estimating: 180it [02:03,  1.49it/s]Extractor Estimating: 181it [02:04,  1.50it/s]Extractor Estimating: 182it [02:04,  1.50it/s]Extractor Estimating: 183it [02:05,  1.49it/s]Extractor Estimating: 184it [02:06,  1.48it/s]Extractor Estimating: 185it [02:06,  1.48it/s]Extractor Estimating: 186it [02:07,  1.48it/s]Extractor Estimating: 187it [02:08,  1.12it/s]Extractor Estimating: 188it [02:09,  1.19it/s]Extractor Estimating: 189it [02:10,  1.24it/s]Extractor Estimating: 190it [02:11,  1.28it/s]Extractor Estimating: 191it [02:11,  1.32it/s]Extractor Estimating: 192it [02:12,  1.36it/s]Extractor Estimating: 193it [02:13,  1.36it/s]Extractor Estimating: 194it [02:13,  1.40it/s]Extractor Estimating: 195it [02:14,  1.35it/s]Extractor Estimating: 196it [02:15,  1.38it/s]Extractor Estimating: 197it [02:16,  1.42it/s]Extractor Estimating: 198it [02:16,  1.44it/s]Extractor Estimating: 199it [02:17,  1.45it/s]Extractor Estimating: 200it [02:18,  1.44it/s]Extractor Estimating: 201it [02:18,  1.46it/s]Extractor Estimating: 202it [02:19,  1.45it/s]Extractor Estimating: 203it [02:20,  1.45it/s]Extractor Estimating: 204it [02:21,  1.28it/s]Extractor Estimating: 205it [02:21,  1.35it/s]Extractor Estimating: 206it [02:22,  1.32it/s]Extractor Estimating: 207it [02:23,  1.34it/s]Extractor Estimating: 208it [02:24,  1.37it/s]Extractor Estimating: 209it [02:24,  1.41it/s]Extractor Estimating: 210it [02:25,  1.42it/s]Extractor Estimating: 211it [02:26,  1.42it/s]Extractor Estimating: 212it [02:26,  1.41it/s]Extractor Estimating: 213it [02:27,  1.41it/s]Extractor Estimating: 214it [02:28,  1.38it/s]Extractor Estimating: 215it [02:28,  1.41it/s]Extractor Estimating: 216it [02:29,  1.41it/s]Extractor Estimating: 217it [02:30,  1.43it/s]Extractor Estimating: 218it [02:31,  1.39it/s]Extractor Estimating: 219it [02:31,  1.35it/s]Extractor Estimating: 220it [02:32,  1.41it/s]Extractor Estimating: 221it [02:33,  1.35it/s]Extractor Estimating: 222it [02:34,  1.38it/s]Extractor Estimating: 223it [02:34,  1.45it/s]Extractor Estimating: 224it [02:35,  1.46it/s]Extractor Estimating: 225it [02:35,  1.52it/s]Extractor Estimating: 226it [02:36,  1.52it/s]Extractor Estimating: 227it [02:37,  1.45it/s]Extractor Estimating: 228it [02:38,  1.43it/s]Extractor Estimating: 229it [02:38,  1.48it/s]Extractor Estimating: 230it [02:39,  1.49it/s]Extractor Estimating: 231it [02:40,  1.45it/s]Extractor Estimating: 232it [02:40,  1.37it/s]Extractor Estimating: 233it [02:41,  1.35it/s]Extractor Estimating: 234it [02:42,  1.40it/s]Extractor Estimating: 235it [02:43,  1.38it/s]Extractor Estimating: 236it [02:43,  1.39it/s]Extractor Estimating: 237it [02:44,  1.36it/s]Extractor Estimating: 238it [02:45,  1.38it/s]Extractor Estimating: 239it [02:45,  1.38it/s]Extractor Estimating: 240it [02:46,  1.42it/s]Extractor Estimating: 241it [02:47,  1.41it/s]Extractor Estimating: 242it [02:47,  1.47it/s]Extractor Estimating: 243it [02:48,  1.45it/s]Extractor Estimating: 244it [02:49,  1.39it/s]Extractor Estimating: 245it [02:50,  1.37it/s]Extractor Estimating: 246it [02:50,  1.36it/s]Extractor Estimating: 247it [02:51,  1.38it/s]Extractor Estimating: 248it [02:52,  1.39it/s]Extractor Estimating: 249it [02:53,  1.36it/s]Extractor Estimating: 250it [02:53,  1.40it/s]Extractor Estimating: 251it [02:54,  1.43it/s]Extractor Estimating: 252it [02:55,  1.46it/s]Extractor Estimating: 253it [02:55,  1.48it/s]Extractor Estimating: 254it [02:56,  1.42it/s]Extractor Estimating: 255it [02:57,  1.46it/s]Extractor Estimating: 256it [02:57,  1.46it/s]Extractor Estimating: 257it [02:58,  1.51it/s]Extractor Estimating: 258it [02:59,  1.50it/s]Extractor Estimating: 259it [02:59,  1.43it/s]Extractor Estimating: 260it [03:00,  1.39it/s]Extractor Estimating: 261it [03:01,  1.38it/s]Extractor Estimating: 262it [03:02,  1.36it/s]Extractor Estimating: 263it [03:02,  1.39it/s]Extractor Estimating: 264it [03:03,  1.42it/s]Extractor Estimating: 265it [03:04,  1.40it/s]Extractor Estimating: 266it [03:04,  1.45it/s]Extractor Estimating: 267it [03:05,  1.47it/s]Extractor Estimating: 268it [03:06,  1.49it/s]Extractor Estimating: 269it [03:06,  1.46it/s]Extractor Estimating: 270it [03:07,  1.49it/s]Extractor Estimating: 271it [03:08,  1.45it/s]Extractor Estimating: 272it [03:08,  1.48it/s]Extractor Estimating: 273it [03:09,  1.47it/s]Extractor Estimating: 274it [03:10,  1.46it/s]Extractor Estimating: 275it [03:11,  1.46it/s]Extractor Estimating: 276it [03:11,  1.47it/s]Extractor Estimating: 277it [03:12,  1.53it/s]Extractor Estimating: 278it [03:13,  1.48it/s]Extractor Estimating: 279it [03:13,  1.52it/s]Extractor Estimating: 280it [03:14,  1.39it/s]Extractor Estimating: 281it [03:15,  1.45it/s]Extractor Estimating: 282it [03:15,  1.40it/s]Extractor Estimating: 283it [03:16,  1.42it/s]Extractor Estimating: 284it [03:17,  1.45it/s]Extractor Estimating: 285it [03:17,  1.42it/s]Extractor Estimating: 286it [03:18,  1.43it/s]Extractor Estimating: 287it [03:19,  1.47it/s]Extractor Estimating: 288it [03:20,  1.44it/s]Extractor Estimating: 289it [03:20,  1.46it/s]Extractor Estimating: 290it [03:21,  1.48it/s]Extractor Estimating: 291it [03:21,  1.51it/s]Extractor Estimating: 292it [03:22,  1.48it/s]Extractor Estimating: 293it [03:23,  1.51it/s]Extractor Estimating: 294it [03:23,  1.53it/s]Extractor Estimating: 295it [03:24,  1.51it/s]Extractor Estimating: 296it [03:25,  1.52it/s]Extractor Estimating: 297it [03:25,  1.55it/s]Extractor Estimating: 298it [03:26,  1.55it/s]Extractor Estimating: 299it [03:27,  1.55it/s]Extractor Estimating: 300it [03:27,  1.53it/s]Extractor Estimating: 301it [03:28,  1.46it/s]Extractor Estimating: 302it [03:29,  1.48it/s]Extractor Estimating: 303it [03:29,  1.47it/s]Extractor Estimating: 304it [03:30,  1.43it/s]Extractor Estimating: 305it [03:31,  1.40it/s]Extractor Estimating: 306it [03:32,  1.35it/s]Extractor Estimating: 307it [03:32,  1.41it/s]Extractor Estimating: 308it [03:33,  1.40it/s]Extractor Estimating: 309it [03:34,  1.45it/s]Extractor Estimating: 310it [03:34,  1.46it/s]Extractor Estimating: 311it [03:35,  1.46it/s]Extractor Estimating: 312it [03:36,  1.46it/s]Extractor Estimating: 313it [03:37,  1.39it/s]Extractor Estimating: 314it [03:37,  1.44it/s]Extractor Estimating: 315it [03:38,  1.48it/s]Extractor Estimating: 316it [03:39,  1.42it/s]Extractor Estimating: 317it [03:39,  1.45it/s]Extractor Estimating: 318it [03:40,  1.43it/s]Extractor Estimating: 319it [03:41,  1.46it/s]Extractor Estimating: 320it [03:41,  1.45it/s]Extractor Estimating: 321it [03:42,  1.51it/s]Extractor Estimating: 322it [03:43,  1.50it/s]Extractor Estimating: 323it [03:43,  1.51it/s]Extractor Estimating: 324it [03:44,  1.51it/s]Extractor Estimating: 325it [03:45,  1.56it/s]Extractor Estimating: 326it [03:45,  1.62it/s]Extractor Estimating: 327it [03:46,  1.63it/s]Extractor Estimating: 328it [03:46,  1.66it/s]Extractor Estimating: 329it [03:47,  1.65it/s]Extractor Estimating: 330it [03:47,  1.66it/s]Extractor Estimating: 331it [03:48,  1.72it/s]Extractor Estimating: 332it [03:49,  1.71it/s]Extractor Estimating: 333it [03:49,  1.67it/s]Extractor Estimating: 334it [03:50,  1.70it/s]Extractor Estimating: 335it [03:50,  1.69it/s]Extractor Estimating: 336it [03:51,  1.73it/s]Extractor Estimating: 337it [03:52,  1.74it/s]Extractor Estimating: 338it [03:52,  1.70it/s]Extractor Estimating: 339it [03:53,  1.76it/s]Extractor Estimating: 340it [03:53,  1.78it/s]Extractor Estimating: 341it [03:54,  1.71it/s]Extractor Estimating: 342it [03:55,  1.63it/s]Extractor Estimating: 343it [03:55,  1.68it/s]Extractor Estimating: 344it [03:56,  1.73it/s]Extractor Estimating: 345it [03:56,  1.66it/s]Extractor Estimating: 346it [03:57,  1.60it/s]Extractor Estimating: 347it [03:58,  1.16it/s]Extractor Estimating: 348it [03:59,  1.33it/s]Extractor Estimating: 349it [03:59,  1.45it/s]Extractor Estimating: 350it [04:00,  1.56it/s]Extractor Estimating: 351it [04:01,  1.54it/s]Extractor Estimating: 352it [04:01,  1.61it/s]Extractor Estimating: 353it [04:02,  1.67it/s]Extractor Estimating: 354it [04:02,  1.71it/s]Extractor Estimating: 355it [04:03,  1.68it/s]Extractor Estimating: 356it [04:04,  1.64it/s]Extractor Estimating: 357it [04:04,  1.70it/s]Extractor Estimating: 358it [04:05,  1.73it/s]Extractor Estimating: 359it [04:05,  1.68it/s]Extractor Estimating: 360it [04:06,  1.49it/s]Extractor Estimating: 361it [04:07,  1.52it/s]Extractor Estimating: 362it [04:07,  1.53it/s]Extractor Estimating: 363it [04:08,  1.57it/s]Extractor Estimating: 364it [04:09,  1.62it/s]Extractor Estimating: 365it [04:09,  1.61it/s]Extractor Estimating: 366it [04:10,  1.59it/s]Extractor Estimating: 367it [04:10,  1.61it/s]Extractor Estimating: 368it [04:11,  1.63it/s]Extractor Estimating: 369it [04:12,  1.62it/s]Extractor Estimating: 370it [04:12,  1.60it/s]Extractor Estimating: 371it [04:13,  1.63it/s]Extractor Estimating: 372it [04:13,  1.61it/s]Extractor Estimating: 373it [04:14,  1.63it/s]Extractor Estimating: 374it [04:15,  1.63it/s]Extractor Estimating: 375it [04:15,  1.60it/s]Extractor Estimating: 376it [04:16,  1.62it/s]Extractor Estimating: 377it [04:17,  1.65it/s]Extractor Estimating: 378it [04:17,  1.63it/s]Extractor Estimating: 379it [04:18,  1.58it/s]Extractor Estimating: 380it [04:18,  1.58it/s]Extractor Estimating: 381it [04:19,  1.61it/s]Extractor Estimating: 382it [04:20,  1.61it/s]Extractor Estimating: 383it [04:20,  1.57it/s]Extractor Estimating: 384it [04:21,  1.50it/s]Extractor Estimating: 385it [04:22,  1.55it/s]Extractor Estimating: 386it [04:22,  1.61it/s]Extractor Estimating: 387it [04:23,  1.61it/s]Extractor Estimating: 388it [04:24,  1.59it/s]Extractor Estimating: 389it [04:24,  1.57it/s]Extractor Estimating: 390it [04:25,  1.55it/s]Extractor Estimating: 391it [04:25,  1.55it/s]Extractor Estimating: 392it [04:26,  1.51it/s]Extractor Estimating: 393it [04:27,  1.55it/s]Extractor Estimating: 394it [04:27,  1.61it/s]Extractor Estimating: 395it [04:28,  1.62it/s]Extractor Estimating: 396it [04:29,  1.59it/s]Extractor Estimating: 397it [04:29,  1.59it/s]Extractor Estimating: 398it [04:30,  1.62it/s]Extractor Estimating: 399it [04:30,  1.63it/s]Extractor Estimating: 400it [04:31,  1.60it/s]Extractor Estimating: 401it [04:32,  1.55it/s]Extractor Estimating: 402it [04:32,  1.55it/s]Extractor Estimating: 403it [04:33,  1.58it/s]Extractor Estimating: 404it [04:34,  1.57it/s]Extractor Estimating: 405it [04:34,  1.59it/s]Extractor Estimating: 406it [04:35,  1.57it/s]Extractor Estimating: 407it [04:36,  1.58it/s]Extractor Estimating: 408it [04:36,  1.56it/s]Extractor Estimating: 409it [04:37,  1.63it/s]Extractor Estimating: 410it [04:37,  1.62it/s]Extractor Estimating: 411it [04:38,  1.57it/s]Extractor Estimating: 412it [04:39,  1.59it/s]Extractor Estimating: 413it [04:39,  1.58it/s]Extractor Estimating: 414it [04:40,  1.61it/s]Extractor Estimating: 415it [04:41,  1.63it/s]Extractor Estimating: 416it [04:41,  1.63it/s]Extractor Estimating: 417it [04:42,  1.49it/s]Extractor Estimating: 418it [04:43,  1.56it/s]Extractor Estimating: 419it [04:43,  1.61it/s]Extractor Estimating: 420it [04:44,  1.65it/s]Extractor Estimating: 421it [04:44,  1.65it/s]Extractor Estimating: 422it [04:45,  1.65it/s]Extractor Estimating: 423it [04:46,  1.63it/s]Extractor Estimating: 424it [04:46,  1.62it/s]Extractor Estimating: 425it [04:47,  1.55it/s]Extractor Estimating: 426it [04:47,  1.60it/s]Extractor Estimating: 427it [04:48,  1.61it/s]Extractor Estimating: 428it [04:49,  1.59it/s]Extractor Estimating: 429it [04:49,  1.63it/s]Extractor Estimating: 430it [04:50,  1.62it/s]Extractor Estimating: 431it [04:51,  1.61it/s]Extractor Estimating: 432it [04:51,  1.64it/s]Extractor Estimating: 433it [04:52,  1.67it/s]Extractor Estimating: 434it [04:52,  1.70it/s]Extractor Estimating: 435it [04:53,  1.68it/s]Extractor Estimating: 436it [04:53,  1.68it/s]Extractor Estimating: 437it [04:54,  1.61it/s]Extractor Estimating: 438it [04:55,  1.64it/s]Extractor Estimating: 439it [04:55,  1.63it/s]Extractor Estimating: 440it [04:56,  1.64it/s]Extractor Estimating: 441it [04:57,  1.51it/s]Extractor Estimating: 442it [04:57,  1.56it/s]Extractor Estimating: 443it [04:58,  1.59it/s]Extractor Estimating: 444it [04:59,  1.60it/s]Extractor Estimating: 445it [04:59,  1.61it/s]Extractor Estimating: 446it [05:00,  1.59it/s]Extractor Estimating: 447it [05:00,  1.59it/s]Extractor Estimating: 448it [05:01,  1.57it/s]Extractor Estimating: 449it [05:02,  1.57it/s]Extractor Estimating: 450it [05:02,  1.55it/s]Extractor Estimating: 451it [05:03,  1.50it/s]Extractor Estimating: 452it [05:04,  1.53it/s]Extractor Estimating: 453it [05:04,  1.50it/s]Extractor Estimating: 454it [05:05,  1.45it/s]Extractor Estimating: 455it [05:06,  1.44it/s]Extractor Estimating: 456it [05:07,  1.42it/s]Extractor Estimating: 457it [05:07,  1.42it/s]Extractor Estimating: 458it [05:08,  1.43it/s]Extractor Estimating: 459it [05:09,  1.47it/s]Extractor Estimating: 460it [05:09,  1.42it/s]Extractor Estimating: 461it [05:10,  1.44it/s]Extractor Estimating: 462it [05:11,  1.45it/s]Extractor Estimating: 463it [05:12,  1.37it/s]Extractor Estimating: 464it [05:12,  1.38it/s]Extractor Estimating: 465it [05:13,  1.46it/s]Extractor Estimating: 466it [05:14,  1.49it/s]Extractor Estimating: 467it [05:14,  1.53it/s]Extractor Estimating: 468it [05:15,  1.57it/s]Extractor Estimating: 469it [05:15,  1.58it/s]Extractor Estimating: 470it [05:16,  1.55it/s]Extractor Estimating: 471it [05:17,  1.49it/s]Extractor Estimating: 472it [05:17,  1.49it/s]Extractor Estimating: 473it [05:18,  1.48it/s]Extractor Estimating: 474it [05:19,  1.49it/s]Extractor Estimating: 475it [05:19,  1.47it/s]Extractor Estimating: 476it [05:20,  1.52it/s]Extractor Estimating: 477it [05:21,  1.55it/s]Extractor Estimating: 478it [05:21,  1.50it/s]Extractor Estimating: 479it [05:22,  1.52it/s]Extractor Estimating: 480it [05:23,  1.49it/s]Extractor Estimating: 481it [05:23,  1.48it/s]Extractor Estimating: 482it [05:24,  1.47it/s]Extractor Estimating: 483it [05:25,  1.49it/s]Extractor Estimating: 484it [05:25,  1.49it/s]Extractor Estimating: 485it [05:26,  1.50it/s]Extractor Estimating: 486it [05:27,  1.53it/s]Extractor Estimating: 487it [05:27,  1.52it/s]Extractor Estimating: 488it [05:28,  1.54it/s]Extractor Estimating: 489it [05:29,  1.55it/s]Extractor Estimating: 490it [05:29,  1.57it/s]Extractor Estimating: 491it [05:30,  1.50it/s]Extractor Estimating: 492it [05:31,  1.44it/s]Extractor Estimating: 493it [05:31,  1.42it/s]Extractor Estimating: 494it [05:32,  1.44it/s]Extractor Estimating: 495it [05:33,  1.48it/s]Extractor Estimating: 496it [05:33,  1.48it/s]Extractor Estimating: 497it [05:34,  1.53it/s]Extractor Estimating: 498it [05:35,  1.51it/s]Extractor Estimating: 499it [05:35,  1.54it/s]Extractor Estimating: 500it [05:36,  1.59it/s]Extractor Estimating: 500it [05:36,  1.49it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 9991 mean pseudo reward: 0.9168773966885282
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 28422
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28522, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=28522, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.378, loss:1388.9513
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.119, loss:1267.7322
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.092, loss:1287.3998
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.119, loss:1253.4644
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 1.086, loss:1199.7377
>> valid entity prec:0.6236, rec:0.5127, f1:0.5628
>> valid relation prec:0.3187, rec:0.0900, f1:0.1404
>> valid relation with NER prec:0.3187, rec:0.0900, f1:0.1404
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.479, loss:1147.2406
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 1.101, loss:1154.7374
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 1.106, loss:1083.2698
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 1.072, loss:1011.6825
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 1.086, loss:1021.8899
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5991, rec:0.5833, f1:0.5911
>> valid relation prec:0.2971, rec:0.1015, f1:0.1513
>> valid relation with NER prec:0.2971, rec:0.1015, f1:0.1513
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 266, avg_time 2.482, loss:999.9342
g_step 1200, step 366, avg_time 1.091, loss:1036.8709
g_step 1300, step 49, avg_time 1.092, loss:968.5969
g_step 1400, step 149, avg_time 1.104, loss:960.8363
g_step 1500, step 249, avg_time 1.095, loss:935.5842
>> valid entity prec:0.5531, rec:0.5941, f1:0.5729
>> valid relation prec:0.2868, rec:0.1133, f1:0.1624
>> valid relation with NER prec:0.2868, rec:0.1133, f1:0.1624
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 349, avg_time 2.464, loss:900.7346
g_step 1700, step 32, avg_time 1.100, loss:918.9098
g_step 1800, step 132, avg_time 1.101, loss:883.4961
g_step 1900, step 232, avg_time 1.098, loss:868.6118
g_step 2000, step 332, avg_time 1.082, loss:835.8179
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5735, rec:0.6126, f1:0.5924
>> valid relation prec:0.2517, rec:0.0981, f1:0.1411
>> valid relation with NER prec:0.2517, rec:0.0981, f1:0.1411
new max entity f1 on valid!
g_step 2100, step 15, avg_time 2.464, loss:875.0745
g_step 2200, step 115, avg_time 1.097, loss:804.8654
g_step 2300, step 215, avg_time 1.102, loss:836.1598
g_step 2400, step 315, avg_time 1.103, loss:831.8093
g_step 2500, step 415, avg_time 1.099, loss:797.5987
>> valid entity prec:0.5949, rec:0.5576, f1:0.5756
>> valid relation prec:0.2618, rec:0.0989, f1:0.1436
>> valid relation with NER prec:0.2618, rec:0.0989, f1:0.1436
g_step 2600, step 98, avg_time 2.474, loss:755.8478
g_step 2700, step 198, avg_time 1.097, loss:781.0579
g_step 2800, step 298, avg_time 1.093, loss:761.4687
g_step 2900, step 398, avg_time 1.095, loss:789.2027
g_step 3000, step 81, avg_time 1.082, loss:722.0216
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5642, rec:0.5656, f1:0.5649
>> valid relation prec:0.2285, rec:0.1070, f1:0.1457
>> valid relation with NER prec:0.2285, rec:0.1070, f1:0.1457
g_step 3100, step 181, avg_time 2.473, loss:734.7002
g_step 3200, step 281, avg_time 1.089, loss:744.9762
g_step 3300, step 381, avg_time 1.104, loss:762.8568
g_step 3400, step 64, avg_time 1.097, loss:690.0479
g_step 3500, step 164, avg_time 1.091, loss:683.5690
>> valid entity prec:0.5872, rec:0.5400, f1:0.5626
>> valid relation prec:0.2388, rec:0.0984, f1:0.1393
>> valid relation with NER prec:0.2388, rec:0.0984, f1:0.1393
g_step 3600, step 264, avg_time 2.483, loss:730.1639
g_step 3700, step 364, avg_time 1.095, loss:690.6203
g_step 3800, step 47, avg_time 1.089, loss:681.1882
g_step 3900, step 147, avg_time 1.086, loss:668.9579
g_step 4000, step 247, avg_time 1.095, loss:696.9468
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5877, rec:0.5609, f1:0.5740
>> valid relation prec:0.2444, rec:0.0912, f1:0.1328
>> valid relation with NER prec:0.2444, rec:0.0912, f1:0.1328
g_step 4100, step 347, avg_time 2.491, loss:672.9716
g_step 4200, step 30, avg_time 1.100, loss:676.2109
g_step 4300, step 130, avg_time 1.089, loss:613.0685
g_step 4400, step 230, avg_time 1.096, loss:621.9364
g_step 4500, step 330, avg_time 1.102, loss:650.3510
>> valid entity prec:0.6211, rec:0.5736, f1:0.5964
>> valid relation prec:0.2077, rec:0.0791, f1:0.1146
>> valid relation with NER prec:0.2077, rec:0.0791, f1:0.1146
new max entity f1 on valid!
g_step 4600, step 13, avg_time 2.458, loss:690.7044
g_step 4700, step 113, avg_time 1.100, loss:597.7161
g_step 4800, step 213, avg_time 1.100, loss:616.8930
g_step 4900, step 313, avg_time 1.098, loss:654.1017
g_step 5000, step 413, avg_time 1.097, loss:667.2243
learning rate was adjusted to 0.0008
>> valid entity prec:0.5871, rec:0.5195, f1:0.5512
>> valid relation prec:0.1758, rec:0.0638, f1:0.0937
>> valid relation with NER prec:0.1758, rec:0.0638, f1:0.0937
g_step 5100, step 96, avg_time 2.453, loss:588.8903
g_step 5200, step 196, avg_time 1.105, loss:605.2179
g_step 5300, step 296, avg_time 1.107, loss:585.6923
g_step 5400, step 396, avg_time 1.091, loss:621.1874
g_step 5500, step 79, avg_time 1.099, loss:568.1296
>> valid entity prec:0.5684, rec:0.5560, f1:0.5621
>> valid relation prec:0.1478, rec:0.0851, f1:0.1080
>> valid relation with NER prec:0.1478, rec:0.0851, f1:0.1080
g_step 5600, step 179, avg_time 2.477, loss:562.5670
g_step 5700, step 279, avg_time 1.100, loss:581.6220
g_step 5800, step 379, avg_time 1.098, loss:586.1205
g_step 5900, step 62, avg_time 1.086, loss:552.4674
g_step 6000, step 162, avg_time 1.086, loss:543.1118
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.6442, rec:0.5118, f1:0.5704
>> valid relation prec:0.2306, rec:0.0992, f1:0.1387
>> valid relation with NER prec:0.2306, rec:0.0992, f1:0.1387
g_step 6100, step 262, avg_time 2.480, loss:554.8696
g_step 6200, step 362, avg_time 1.100, loss:560.7016
g_step 6300, step 45, avg_time 1.083, loss:558.7302
g_step 6400, step 145, avg_time 1.106, loss:511.6173
g_step 6500, step 245, avg_time 1.093, loss:542.9131
>> valid entity prec:0.5911, rec:0.5394, f1:0.5641
>> valid relation prec:0.1659, rec:0.0797, f1:0.1076
>> valid relation with NER prec:0.1659, rec:0.0797, f1:0.1076
g_step 6600, step 345, avg_time 2.463, loss:540.0224
g_step 6700, step 28, avg_time 1.093, loss:511.7459
g_step 6800, step 128, avg_time 1.095, loss:489.8400
g_step 6900, step 228, avg_time 1.094, loss:525.7013
g_step 7000, step 328, avg_time 1.098, loss:533.6656
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.6074, rec:0.5394, f1:0.5714
>> valid relation prec:0.1843, rec:0.0923, f1:0.1230
>> valid relation with NER prec:0.1843, rec:0.0923, f1:0.1230
g_step 7100, step 11, avg_time 2.459, loss:515.7518
g_step 7200, step 111, avg_time 1.091, loss:476.5734
g_step 7300, step 211, avg_time 1.096, loss:490.7982
g_step 7400, step 311, avg_time 1.110, loss:490.7664
g_step 7500, step 411, avg_time 1.097, loss:504.5359
>> valid entity prec:0.5782, rec:0.5419, f1:0.5594
>> valid relation prec:0.1891, rec:0.0929, f1:0.1246
>> valid relation with NER prec:0.1891, rec:0.0929, f1:0.1246
g_step 7600, step 94, avg_time 2.464, loss:463.8629
g_step 7700, step 194, avg_time 1.086, loss:472.2577
g_step 7800, step 294, avg_time 1.103, loss:475.2427
g_step 7900, step 394, avg_time 1.111, loss:495.5271
g_step 8000, step 77, avg_time 1.085, loss:455.7606
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5833, rec:0.5400, f1:0.5608
>> valid relation prec:0.2041, rec:0.1079, f1:0.1411
>> valid relation with NER prec:0.2041, rec:0.1079, f1:0.1411
g_step 8100, step 177, avg_time 2.466, loss:449.9691
g_step 8200, step 277, avg_time 1.097, loss:453.5781
g_step 8300, step 377, avg_time 1.110, loss:478.4593
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 14:17:50 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 14:17:50 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_14-17-50_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 14:17:51 - WARNING - datasets.builder -   Using custom data configuration default-8307959dbcf4f490
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-8307959dbcf4f490/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 14:17:51,881 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:17:51,882 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:17:51,882 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:17:51,883 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:17:51,896 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:17:51,903 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:17:51,903 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:17:51,903 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:17:51,903 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:17:51,903 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:17:51,903 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 14:17:52,035 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:17:55,348 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 14:17:55,389 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-8307959dbcf4f490/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 14:17:55 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14d251ff30e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:06,  1.44ba/s] 18%|█▊        | 2/11 [00:00<00:03,  2.44ba/s] 27%|██▋       | 3/11 [00:01<00:02,  3.10ba/s] 36%|███▋      | 4/11 [00:01<00:01,  3.55ba/s] 45%|████▌     | 5/11 [00:01<00:01,  3.87ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.07ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.22ba/s] 73%|███████▎  | 8/11 [00:02<00:00,  4.33ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.38ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.41ba/s]100%|██████████| 11/11 [00:02<00:00,  4.11ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.93ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.17ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.33ba/s]100%|██████████| 4/4 [00:00<00:00,  5.45ba/s]100%|██████████| 4/4 [00:00<00:00,  4.91ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:02,  3.77ba/s] 27%|██▋       | 3/11 [00:00<00:01,  7.15ba/s] 45%|████▌     | 5/11 [00:00<00:00,  8.39ba/s] 64%|██████▎   | 7/11 [00:00<00:00,  9.05ba/s] 73%|███████▎  | 8/11 [00:00<00:00,  9.24ba/s] 82%|████████▏ | 9/11 [00:01<00:00,  9.38ba/s]100%|██████████| 11/11 [00:01<00:00, 11.97ba/s]100%|██████████| 11/11 [00:01<00:00,  9.49ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.88ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  7.16ba/s]100%|██████████| 4/4 [00:00<00:00,  7.73ba/s]100%|██████████| 4/4 [00:00<00:00,  7.08ba/s]
[INFO|trainer.py:414] 2023-08-28 14:18:02,162 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 14:18:02,220 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 14:18:02,220 >>   Num examples = 10032
[INFO|trainer.py:1149] 2023-08-28 14:18:02,221 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 14:18:02,221 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 14:18:02,221 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 14:18:02,221 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 14:18:02,221 >>   Total optimization steps = 785
  0%|          | 0/785 [00:00<?, ?it/s]  0%|          | 1/785 [00:00<03:59,  3.27it/s]  0%|          | 2/785 [00:00<03:50,  3.39it/s]  0%|          | 3/785 [00:00<03:47,  3.43it/s]  1%|          | 4/785 [00:01<03:46,  3.45it/s]  1%|          | 5/785 [00:01<03:45,  3.47it/s]  1%|          | 6/785 [00:01<03:44,  3.47it/s]  1%|          | 7/785 [00:02<03:43,  3.48it/s]  1%|          | 8/785 [00:02<03:43,  3.48it/s]  1%|          | 9/785 [00:02<03:42,  3.48it/s]  1%|▏         | 10/785 [00:02<03:42,  3.48it/s]  1%|▏         | 11/785 [00:03<03:42,  3.48it/s]  2%|▏         | 12/785 [00:03<03:42,  3.47it/s]  2%|▏         | 13/785 [00:03<03:42,  3.47it/s]  2%|▏         | 14/785 [00:04<03:42,  3.47it/s]  2%|▏         | 15/785 [00:04<03:41,  3.47it/s]  2%|▏         | 16/785 [00:04<03:41,  3.48it/s]  2%|▏         | 17/785 [00:04<03:40,  3.48it/s]  2%|▏         | 18/785 [00:05<03:40,  3.48it/s]  2%|▏         | 19/785 [00:05<03:40,  3.48it/s]  3%|▎         | 20/785 [00:05<03:40,  3.48it/s]  3%|▎         | 21/785 [00:06<03:39,  3.48it/s]  3%|▎         | 22/785 [00:06<03:39,  3.48it/s]  3%|▎         | 23/785 [00:06<03:39,  3.48it/s]  3%|▎         | 24/785 [00:06<03:38,  3.48it/s]  3%|▎         | 25/785 [00:07<03:38,  3.48it/s]  3%|▎         | 26/785 [00:07<03:38,  3.48it/s]  3%|▎         | 27/785 [00:07<03:37,  3.48it/s]  4%|▎         | 28/785 [00:08<03:37,  3.48it/s]  4%|▎         | 29/785 [00:08<03:37,  3.48it/s]  4%|▍         | 30/785 [00:08<03:37,  3.48it/s]  4%|▍         | 31/785 [00:08<03:36,  3.48it/s]  4%|▍         | 32/785 [00:09<03:36,  3.47it/s]  4%|▍         | 33/785 [00:09<03:36,  3.47it/s]  4%|▍         | 34/785 [00:09<03:36,  3.47it/s]  4%|▍         | 35/785 [00:10<03:35,  3.47it/s]  5%|▍         | 36/785 [00:10<03:35,  3.47it/s]  5%|▍         | 37/785 [00:10<03:35,  3.47it/s]  5%|▍         | 38/785 [00:10<03:35,  3.47it/s]  5%|▍         | 39/785 [00:11<03:34,  3.47it/s]  5%|▌         | 40/785 [00:11<03:35,  3.46it/s]  5%|▌         | 41/785 [00:11<03:34,  3.46it/s]  5%|▌         | 42/785 [00:12<03:34,  3.47it/s]  5%|▌         | 43/785 [00:12<03:33,  3.47it/s]  6%|▌         | 44/785 [00:12<03:33,  3.47it/s]  6%|▌         | 45/785 [00:12<03:33,  3.47it/s]  6%|▌         | 46/785 [00:13<03:32,  3.47it/s]  6%|▌         | 47/785 [00:13<03:32,  3.48it/s]  6%|▌         | 48/785 [00:13<03:32,  3.47it/s]  6%|▌         | 49/785 [00:14<03:31,  3.48it/s]  6%|▋         | 50/785 [00:14<03:31,  3.47it/s]  6%|▋         | 51/785 [00:14<03:31,  3.47it/s]  7%|▋         | 52/785 [00:14<03:30,  3.48it/s]  7%|▋         | 53/785 [00:15<03:30,  3.48it/s]  7%|▋         | 54/785 [00:15<03:30,  3.48it/s]  7%|▋         | 55/785 [00:15<03:30,  3.48it/s]  7%|▋         | 56/785 [00:16<03:29,  3.48it/s]  7%|▋         | 57/785 [00:16<03:29,  3.48it/s]  7%|▋         | 58/785 [00:16<03:29,  3.48it/s]  8%|▊         | 59/785 [00:16<03:28,  3.47it/s]  8%|▊         | 60/785 [00:17<03:28,  3.47it/s]  8%|▊         | 61/785 [00:17<03:28,  3.47it/s]  8%|▊         | 62/785 [00:17<03:28,  3.47it/s]  8%|▊         | 63/785 [00:18<03:28,  3.47it/s]  8%|▊         | 64/785 [00:18<03:27,  3.47it/s]  8%|▊         | 65/785 [00:18<03:27,  3.47it/s]  8%|▊         | 66/785 [00:19<03:27,  3.47it/s]  9%|▊         | 67/785 [00:19<03:27,  3.47it/s]  9%|▊         | 68/785 [00:19<03:26,  3.47it/s]  9%|▉         | 69/785 [00:19<03:26,  3.47it/s]  9%|▉         | 70/785 [00:20<03:26,  3.47it/s]  9%|▉         | 71/785 [00:20<03:25,  3.47it/s]  9%|▉         | 72/785 [00:20<03:25,  3.47it/s]  9%|▉         | 73/785 [00:21<03:25,  3.47it/s]  9%|▉         | 74/785 [00:21<03:24,  3.47it/s] 10%|▉         | 75/785 [00:21<03:24,  3.47it/s] 10%|▉         | 76/785 [00:21<03:24,  3.47it/s] 10%|▉         | 77/785 [00:22<03:24,  3.47it/s] 10%|▉         | 78/785 [00:22<03:23,  3.47it/s] 10%|█         | 79/785 [00:22<03:23,  3.47it/s] 10%|█         | 80/785 [00:23<03:23,  3.47it/s] 10%|█         | 81/785 [00:23<03:22,  3.47it/s] 10%|█         | 82/785 [00:23<03:22,  3.47it/s] 11%|█         | 83/785 [00:23<03:22,  3.47it/s] 11%|█         | 84/785 [00:24<03:22,  3.47it/s] 11%|█         | 85/785 [00:24<03:21,  3.47it/s] 11%|█         | 86/785 [00:24<03:21,  3.47it/s] 11%|█         | 87/785 [00:25<03:21,  3.47it/s] 11%|█         | 88/785 [00:25<03:21,  3.47it/s] 11%|█▏        | 89/785 [00:25<03:20,  3.47it/s] 11%|█▏        | 90/785 [00:25<03:20,  3.47it/s] 12%|█▏        | 91/785 [00:26<03:20,  3.46it/s] 12%|█▏        | 92/785 [00:26<03:19,  3.47it/s] 12%|█▏        | 93/785 [00:26<03:19,  3.47it/s] 12%|█▏        | 94/785 [00:27<03:19,  3.47it/s] 12%|█▏        | 95/785 [00:27<03:18,  3.47it/s] 12%|█▏        | 96/785 [00:27<03:18,  3.47it/s] 12%|█▏        | 97/785 [00:27<03:18,  3.47it/s] 12%|█▏        | 98/785 [00:28<03:18,  3.46it/s] 13%|█▎        | 99/785 [00:28<03:17,  3.47it/s] 13%|█▎        | 100/785 [00:28<03:17,  3.47it/s] 13%|█▎        | 101/785 [00:29<03:17,  3.46it/s] 13%|█▎        | 102/785 [00:29<03:17,  3.47it/s] 13%|█▎        | 103/785 [00:29<03:16,  3.47it/s] 13%|█▎        | 104/785 [00:29<03:16,  3.47it/s] 13%|█▎        | 105/785 [00:30<03:16,  3.47it/s] 14%|█▎        | 106/785 [00:30<03:15,  3.47it/s] 14%|█▎        | 107/785 [00:30<03:15,  3.46it/s] 14%|█▍        | 108/785 [00:31<03:15,  3.46it/s] 14%|█▍        | 109/785 [00:31<03:15,  3.46it/s] 14%|█▍        | 110/785 [00:31<03:14,  3.46it/s] 14%|█▍        | 111/785 [00:31<03:14,  3.46it/s] 14%|█▍        | 112/785 [00:32<03:14,  3.46it/s] 14%|█▍        | 113/785 [00:32<03:14,  3.46it/s] 15%|█▍        | 114/785 [00:32<03:13,  3.46it/s] 15%|█▍        | 115/785 [00:33<03:13,  3.46it/s] 15%|█▍        | 116/785 [00:33<03:13,  3.46it/s] 15%|█▍        | 117/785 [00:33<03:12,  3.46it/s] 15%|█▌        | 118/785 [00:34<03:12,  3.46it/s] 15%|█▌        | 119/785 [00:34<03:12,  3.46it/s] 15%|█▌        | 120/785 [00:34<03:12,  3.46it/s] 15%|█▌        | 121/785 [00:34<03:11,  3.46it/s] 16%|█▌        | 122/785 [00:35<03:11,  3.46it/s] 16%|█▌        | 123/785 [00:35<03:11,  3.46it/s] 16%|█▌        | 124/785 [00:35<03:11,  3.46it/s] 16%|█▌        | 125/785 [00:36<03:10,  3.46it/s] 16%|█▌        | 126/785 [00:36<03:10,  3.46it/s] 16%|█▌        | 127/785 [00:36<03:10,  3.45it/s] 16%|█▋        | 128/785 [00:36<03:09,  3.46it/s] 16%|█▋        | 129/785 [00:37<03:09,  3.46it/s] 17%|█▋        | 130/785 [00:37<03:09,  3.46it/s] 17%|█▋        | 131/785 [00:37<03:08,  3.46it/s] 17%|█▋        | 132/785 [00:38<03:08,  3.46it/s] 17%|█▋        | 133/785 [00:38<03:08,  3.46it/s] 17%|█▋        | 134/785 [00:38<03:08,  3.46it/s] 17%|█▋        | 135/785 [00:38<03:07,  3.46it/s] 17%|█▋        | 136/785 [00:39<03:07,  3.46it/s] 17%|█▋        | 137/785 [00:39<03:07,  3.46it/s] 18%|█▊        | 138/785 [00:39<03:06,  3.46it/s] 18%|█▊        | 139/785 [00:40<03:06,  3.46it/s] 18%|█▊        | 140/785 [00:40<03:06,  3.47it/s] 18%|█▊        | 141/785 [00:40<03:05,  3.47it/s] 18%|█▊        | 142/785 [00:40<03:05,  3.47it/s] 18%|█▊        | 143/785 [00:41<03:05,  3.46it/s] 18%|█▊        | 144/785 [00:41<03:05,  3.46it/s] 18%|█▊        | 145/785 [00:41<03:04,  3.46it/s] 19%|█▊        | 146/785 [00:42<03:04,  3.46it/s] 19%|█▊        | 147/785 [00:42<03:04,  3.47it/s] 19%|█▉        | 148/785 [00:42<03:03,  3.46it/s] 19%|█▉        | 149/785 [00:42<03:03,  3.46it/s] 19%|█▉        | 150/785 [00:43<03:03,  3.46it/s] 19%|█▉        | 151/785 [00:43<03:02,  3.47it/s] 19%|█▉        | 152/785 [00:43<03:02,  3.47it/s] 19%|█▉        | 153/785 [00:44<03:02,  3.46it/s] 20%|█▉        | 154/785 [00:44<03:02,  3.46it/s] 20%|█▉        | 155/785 [00:44<03:02,  3.46it/s] 20%|█▉        | 156/785 [00:44<03:01,  3.46it/s] 20%|██        | 157/785 [00:45<02:49,  3.70it/s][INFO|trainer.py:2140] 2023-08-28 14:18:47,447 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:18:47,447 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 14:18:47,447 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.73it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.62it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.91it/s][A
  5%|▌         | 23/435 [00:00<00:08, 48.18it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.60it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.31it/s][A
  9%|▊         | 38/435 [00:00<00:08, 47.10it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.75it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.77it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.68it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.75it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.84it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.79it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.83it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.68it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.58it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.59it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 45.96it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.21it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.30it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.46it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.58it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.66it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.68it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.63it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.61it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.57it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.60it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.62it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.67it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.66it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.67it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.74it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.74it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.55it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.59it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.58it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.64it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.63it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.63it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.67it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.65it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.66it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.70it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.62it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.62it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.59it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.61it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.61it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.65it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.66it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.64it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.62it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.53it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.63it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.62it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.65it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.68it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.65it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.64it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.60it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.64it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.62it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.52it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.55it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.62it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.65it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.60it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.58it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.56it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.58it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.65it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.59it/s][A
 86%|████████▌ | 373/435 [00:07<00:01, 46.55it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.59it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.62it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.60it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.55it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.54it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.61it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.61it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.60it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.55it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.59it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.62it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.60it/s][A                                                 
                                                 [A 20%|██        | 157/785 [00:54<02:49,  3.70it/s]
100%|██████████| 435/435 [00:09<00:00, 46.60it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:18:56,808 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-157
[INFO|configuration_utils.py:351] 2023-08-28 14:18:56,844 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-157/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:18:59,277 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-157/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:18:59,294 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-157/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:18:59,303 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-157/special_tokens_map.json
 20%|██        | 158/785 [01:03<1:00:38,  5.80s/it] 20%|██        | 159/785 [01:04<43:19,  4.15s/it]   20%|██        | 160/785 [01:04<31:10,  2.99s/it] 21%|██        | 161/785 [01:04<22:41,  2.18s/it] 21%|██        | 162/785 [01:05<16:45,  1.61s/it] 21%|██        | 163/785 [01:05<12:36,  1.22s/it] 21%|██        | 164/785 [01:05<09:42,  1.07it/s] 21%|██        | 165/785 [01:05<07:40,  1.35it/s] 21%|██        | 166/785 [01:06<06:15,  1.65it/s] 21%|██▏       | 167/785 [01:06<05:15,  1.96it/s] 21%|██▏       | 168/785 [01:06<04:34,  2.25it/s] 22%|██▏       | 169/785 [01:07<04:04,  2.52it/s] 22%|██▏       | 170/785 [01:07<03:52,  2.65it/s] 22%|██▏       | 171/785 [01:07<03:35,  2.85it/s] 22%|██▏       | 172/785 [01:08<03:23,  3.01it/s] 22%|██▏       | 173/785 [01:08<03:15,  3.13it/s] 22%|██▏       | 174/785 [01:08<03:09,  3.22it/s] 22%|██▏       | 175/785 [01:08<03:05,  3.29it/s] 22%|██▏       | 176/785 [01:09<03:02,  3.34it/s] 23%|██▎       | 177/785 [01:09<03:00,  3.37it/s] 23%|██▎       | 178/785 [01:09<02:59,  3.39it/s] 23%|██▎       | 179/785 [01:10<02:57,  3.41it/s] 23%|██▎       | 180/785 [01:10<02:56,  3.42it/s] 23%|██▎       | 181/785 [01:10<03:02,  3.31it/s] 23%|██▎       | 182/785 [01:10<02:59,  3.36it/s] 23%|██▎       | 183/785 [01:11<02:57,  3.39it/s] 23%|██▎       | 184/785 [01:11<02:56,  3.41it/s] 24%|██▎       | 185/785 [01:11<02:55,  3.42it/s] 24%|██▎       | 186/785 [01:12<02:54,  3.43it/s] 24%|██▍       | 187/785 [01:12<02:53,  3.44it/s] 24%|██▍       | 188/785 [01:12<02:53,  3.44it/s] 24%|██▍       | 189/785 [01:12<02:52,  3.45it/s] 24%|██▍       | 190/785 [01:13<02:52,  3.44it/s] 24%|██▍       | 191/785 [01:13<02:52,  3.45it/s] 24%|██▍       | 192/785 [01:13<02:52,  3.43it/s] 25%|██▍       | 193/785 [01:14<02:52,  3.44it/s] 25%|██▍       | 194/785 [01:14<02:51,  3.45it/s] 25%|██▍       | 195/785 [01:14<02:51,  3.45it/s] 25%|██▍       | 196/785 [01:15<02:50,  3.45it/s] 25%|██▌       | 197/785 [01:15<02:50,  3.45it/s] 25%|██▌       | 198/785 [01:15<02:49,  3.45it/s] 25%|██▌       | 199/785 [01:15<02:49,  3.46it/s] 25%|██▌       | 200/785 [01:16<02:49,  3.46it/s] 26%|██▌       | 201/785 [01:16<02:48,  3.46it/s] 26%|██▌       | 202/785 [01:16<02:48,  3.45it/s] 26%|██▌       | 203/785 [01:17<02:48,  3.45it/s] 26%|██▌       | 204/785 [01:17<02:48,  3.45it/s] 26%|██▌       | 205/785 [01:17<02:48,  3.45it/s] 26%|██▌       | 206/785 [01:17<02:47,  3.45it/s] 26%|██▋       | 207/785 [01:18<02:47,  3.46it/s] 26%|██▋       | 208/785 [01:18<02:46,  3.46it/s] 27%|██▋       | 209/785 [01:18<02:46,  3.46it/s] 27%|██▋       | 210/785 [01:19<02:46,  3.45it/s] 27%|██▋       | 211/785 [01:19<02:46,  3.46it/s] 27%|██▋       | 212/785 [01:19<02:45,  3.46it/s] 27%|██▋       | 213/785 [01:19<02:45,  3.46it/s] 27%|██▋       | 214/785 [01:20<02:45,  3.45it/s] 27%|██▋       | 215/785 [01:20<02:45,  3.45it/s] 28%|██▊       | 216/785 [01:20<02:44,  3.45it/s] 28%|██▊       | 217/785 [01:21<02:44,  3.45it/s] 28%|██▊       | 218/785 [01:21<02:44,  3.46it/s] 28%|██▊       | 219/785 [01:21<02:43,  3.46it/s] 28%|██▊       | 220/785 [01:21<02:43,  3.46it/s] 28%|██▊       | 221/785 [01:22<02:43,  3.46it/s] 28%|██▊       | 222/785 [01:22<02:42,  3.46it/s] 28%|██▊       | 223/785 [01:22<02:42,  3.45it/s] 29%|██▊       | 224/785 [01:23<02:42,  3.46it/s] 29%|██▊       | 225/785 [01:23<02:42,  3.45it/s] 29%|██▉       | 226/785 [01:23<02:41,  3.45it/s] 29%|██▉       | 227/785 [01:23<02:41,  3.45it/s] 29%|██▉       | 228/785 [01:24<02:41,  3.46it/s] 29%|██▉       | 229/785 [01:24<02:41,  3.44it/s] 29%|██▉       | 230/785 [01:24<02:40,  3.45it/s] 29%|██▉       | 231/785 [01:25<02:40,  3.45it/s] 30%|██▉       | 232/785 [01:25<02:40,  3.45it/s] 30%|██▉       | 233/785 [01:25<02:39,  3.45it/s] 30%|██▉       | 234/785 [01:26<02:39,  3.46it/s] 30%|██▉       | 235/785 [01:26<02:39,  3.45it/s] 30%|███       | 236/785 [01:26<02:38,  3.46it/s] 30%|███       | 237/785 [01:26<02:38,  3.46it/s] 30%|███       | 238/785 [01:27<02:38,  3.46it/s] 30%|███       | 239/785 [01:27<02:37,  3.46it/s] 31%|███       | 240/785 [01:27<02:38,  3.43it/s] 31%|███       | 241/785 [01:28<02:38,  3.44it/s] 31%|███       | 242/785 [01:28<02:37,  3.44it/s] 31%|███       | 243/785 [01:28<02:37,  3.45it/s] 31%|███       | 244/785 [01:28<02:36,  3.45it/s] 31%|███       | 245/785 [01:29<02:36,  3.45it/s] 31%|███▏      | 246/785 [01:29<02:36,  3.45it/s] 31%|███▏      | 247/785 [01:29<02:35,  3.45it/s] 32%|███▏      | 248/785 [01:30<02:35,  3.46it/s] 32%|███▏      | 249/785 [01:30<02:35,  3.46it/s] 32%|███▏      | 250/785 [01:30<02:34,  3.46it/s] 32%|███▏      | 251/785 [01:30<02:35,  3.43it/s] 32%|███▏      | 252/785 [01:31<02:34,  3.44it/s] 32%|███▏      | 253/785 [01:31<02:34,  3.44it/s] 32%|███▏      | 254/785 [01:31<02:33,  3.45it/s] 32%|███▏      | 255/785 [01:32<02:33,  3.45it/s] 33%|███▎      | 256/785 [01:32<02:33,  3.45it/s] 33%|███▎      | 257/785 [01:32<02:33,  3.45it/s] 33%|███▎      | 258/785 [01:32<02:32,  3.45it/s] 33%|███▎      | 259/785 [01:33<02:32,  3.45it/s] 33%|███▎      | 260/785 [01:33<02:32,  3.45it/s] 33%|███▎      | 261/785 [01:33<02:31,  3.45it/s] 33%|███▎      | 262/785 [01:34<02:31,  3.44it/s] 34%|███▎      | 263/785 [01:34<02:31,  3.45it/s] 34%|███▎      | 264/785 [01:34<02:30,  3.45it/s] 34%|███▍      | 265/785 [01:35<02:30,  3.45it/s] 34%|███▍      | 266/785 [01:35<02:30,  3.45it/s] 34%|███▍      | 267/785 [01:35<02:29,  3.45it/s] 34%|███▍      | 268/785 [01:35<02:29,  3.46it/s] 34%|███▍      | 269/785 [01:36<02:29,  3.46it/s] 34%|███▍      | 270/785 [01:36<02:29,  3.46it/s] 35%|███▍      | 271/785 [01:36<02:28,  3.46it/s] 35%|███▍      | 272/785 [01:37<02:28,  3.46it/s] 35%|███▍      | 273/785 [01:37<02:28,  3.44it/s] 35%|███▍      | 274/785 [01:37<02:28,  3.45it/s] 35%|███▌      | 275/785 [01:37<02:27,  3.45it/s] 35%|███▌      | 276/785 [01:38<02:27,  3.45it/s] 35%|███▌      | 277/785 [01:38<02:27,  3.45it/s] 35%|███▌      | 278/785 [01:38<02:26,  3.45it/s] 36%|███▌      | 279/785 [01:39<02:26,  3.46it/s] 36%|███▌      | 280/785 [01:39<02:26,  3.46it/s] 36%|███▌      | 281/785 [01:39<02:25,  3.46it/s] 36%|███▌      | 282/785 [01:39<02:25,  3.46it/s] 36%|███▌      | 283/785 [01:40<02:25,  3.46it/s] 36%|███▌      | 284/785 [01:40<02:25,  3.43it/s] 36%|███▋      | 285/785 [01:40<02:25,  3.44it/s] 36%|███▋      | 286/785 [01:41<02:24,  3.45it/s] 37%|███▋      | 287/785 [01:41<02:24,  3.45it/s] 37%|███▋      | 288/785 [01:41<02:24,  3.45it/s] 37%|███▋      | 289/785 [01:41<02:23,  3.45it/s] 37%|███▋      | 290/785 [01:42<02:23,  3.45it/s] 37%|███▋      | 291/785 [01:42<02:23,  3.45it/s] 37%|███▋      | 292/785 [01:42<02:22,  3.45it/s] 37%|███▋      | 293/785 [01:43<02:22,  3.45it/s] 37%|███▋      | 294/785 [01:43<02:22,  3.45it/s] 38%|███▊      | 295/785 [01:43<02:22,  3.44it/s] 38%|███▊      | 296/785 [01:43<02:21,  3.45it/s] 38%|███▊      | 297/785 [01:44<02:21,  3.45it/s] 38%|███▊      | 298/785 [01:44<02:21,  3.45it/s] 38%|███▊      | 299/785 [01:44<02:20,  3.45it/s] 38%|███▊      | 300/785 [01:45<02:20,  3.45it/s] 38%|███▊      | 301/785 [01:45<02:20,  3.45it/s] 38%|███▊      | 302/785 [01:45<02:20,  3.45it/s] 39%|███▊      | 303/785 [01:46<02:19,  3.45it/s] 39%|███▊      | 304/785 [01:46<02:19,  3.45it/s] 39%|███▉      | 305/785 [01:46<02:19,  3.45it/s] 39%|███▉      | 306/785 [01:46<02:19,  3.44it/s] 39%|███▉      | 307/785 [01:47<02:24,  3.31it/s] 39%|███▉      | 308/785 [01:47<02:22,  3.34it/s] 39%|███▉      | 309/785 [01:47<02:21,  3.37it/s] 39%|███▉      | 310/785 [01:48<02:19,  3.40it/s] 40%|███▉      | 311/785 [01:48<02:18,  3.41it/s] 40%|███▉      | 312/785 [01:48<02:18,  3.43it/s] 40%|███▉      | 313/785 [01:48<02:17,  3.43it/s] 40%|████      | 314/785 [01:49<02:08,  3.67it/s][INFO|trainer.py:2140] 2023-08-28 14:19:51,413 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:19:51,413 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 14:19:51,413 >>   Batch size = 8
{'eval_loss': 0.9368933439254761, 'eval_runtime': 9.3275, 'eval_samples_per_second': 372.77, 'eval_steps_per_second': 46.636, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.68it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.27it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.40it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.80it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.41it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.16it/s][A
  9%|▊         | 38/435 [00:00<00:08, 47.05it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.81it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.81it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.81it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.64it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.57it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.63it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.67it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.58it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.65it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.60it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.57it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.64it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.60it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.56it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.57it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.63it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.63it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.55it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.58it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.54it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.57it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.55it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.57it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.59it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.59it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.52it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.60it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.57it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.52it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.59it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.65it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.49it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.58it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.60it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.56it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.63it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.63it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.50it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.58it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.62it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.60it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.55it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.52it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.56it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.60it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.57it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.56it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.46it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.54it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.62it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.60it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.57it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.56it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.59it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.65it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.65it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.53it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.54it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.59it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.59it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.58it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.55it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.58it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.57it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.60it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.59it/s][A
 86%|████████▌ | 373/435 [00:07<00:01, 46.62it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.51it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.57it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.57it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.46it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.55it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.58it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.57it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.51it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.56it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.56it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.51it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.57it/s][A                                                 
                                                 [A 40%|████      | 314/785 [01:58<02:08,  3.67it/s]
100%|██████████| 435/435 [00:09<00:00, 46.57it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:20:00,780 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-314
[INFO|configuration_utils.py:351] 2023-08-28 14:20:00,801 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-314/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:20:03,305 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-314/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:20:03,321 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-314/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:20:03,333 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-314/special_tokens_map.json
 40%|████      | 315/785 [02:06<42:15,  5.39s/it] 40%|████      | 316/785 [02:06<30:18,  3.88s/it] 40%|████      | 317/785 [02:07<21:50,  2.80s/it] 41%|████      | 318/785 [02:07<15:56,  2.05s/it] 41%|████      | 319/785 [02:07<11:48,  1.52s/it] 41%|████      | 320/785 [02:08<08:55,  1.15s/it] 41%|████      | 321/785 [02:08<06:53,  1.12it/s] 41%|████      | 322/785 [02:08<05:29,  1.41it/s] 41%|████      | 323/785 [02:08<04:30,  1.71it/s] 41%|████▏     | 324/785 [02:09<03:49,  2.01it/s] 41%|████▏     | 325/785 [02:09<03:20,  2.30it/s] 42%|████▏     | 326/785 [02:09<02:59,  2.55it/s] 42%|████▏     | 327/785 [02:10<02:51,  2.67it/s] 42%|████▏     | 328/785 [02:10<02:39,  2.87it/s] 42%|████▏     | 329/785 [02:10<02:30,  3.02it/s] 42%|████▏     | 330/785 [02:10<02:25,  3.14it/s] 42%|████▏     | 331/785 [02:11<02:20,  3.23it/s] 42%|████▏     | 332/785 [02:11<02:17,  3.29it/s] 42%|████▏     | 333/785 [02:11<02:15,  3.34it/s] 43%|████▎     | 334/785 [02:12<02:13,  3.38it/s] 43%|████▎     | 335/785 [02:12<02:12,  3.40it/s] 43%|████▎     | 336/785 [02:12<02:11,  3.42it/s] 43%|████▎     | 337/785 [02:12<02:10,  3.43it/s] 43%|████▎     | 338/785 [02:13<02:16,  3.28it/s] 43%|████▎     | 339/785 [02:13<02:14,  3.33it/s] 43%|████▎     | 340/785 [02:13<02:12,  3.36it/s] 43%|████▎     | 341/785 [02:14<02:11,  3.39it/s] 44%|████▎     | 342/785 [02:14<02:10,  3.40it/s] 44%|████▎     | 343/785 [02:14<02:09,  3.42it/s] 44%|████▍     | 344/785 [02:15<02:08,  3.43it/s] 44%|████▍     | 345/785 [02:15<02:07,  3.44it/s] 44%|████▍     | 346/785 [02:15<02:07,  3.44it/s] 44%|████▍     | 347/785 [02:15<02:07,  3.45it/s] 44%|████▍     | 348/785 [02:16<02:06,  3.45it/s] 44%|████▍     | 349/785 [02:16<02:11,  3.31it/s] 45%|████▍     | 350/785 [02:16<02:09,  3.35it/s] 45%|████▍     | 351/785 [02:17<02:08,  3.38it/s] 45%|████▍     | 352/785 [02:17<02:07,  3.40it/s] 45%|████▍     | 353/785 [02:17<02:06,  3.42it/s] 45%|████▌     | 354/785 [02:18<02:05,  3.43it/s] 45%|████▌     | 355/785 [02:18<02:05,  3.44it/s] 45%|████▌     | 356/785 [02:18<02:04,  3.44it/s] 45%|████▌     | 357/785 [02:18<02:04,  3.45it/s] 46%|████▌     | 358/785 [02:19<02:03,  3.45it/s] 46%|████▌     | 359/785 [02:19<02:03,  3.45it/s] 46%|████▌     | 360/785 [02:19<02:03,  3.44it/s] 46%|████▌     | 361/785 [02:20<02:02,  3.45it/s] 46%|████▌     | 362/785 [02:20<02:02,  3.45it/s] 46%|████▌     | 363/785 [02:20<02:02,  3.45it/s] 46%|████▋     | 364/785 [02:20<02:01,  3.45it/s] 46%|████▋     | 365/785 [02:21<02:01,  3.45it/s] 47%|████▋     | 366/785 [02:21<02:01,  3.45it/s] 47%|████▋     | 367/785 [02:21<02:01,  3.45it/s] 47%|████▋     | 368/785 [02:22<02:00,  3.46it/s] 47%|████▋     | 369/785 [02:22<02:00,  3.45it/s] 47%|████▋     | 370/785 [02:22<02:00,  3.46it/s] 47%|████▋     | 371/785 [02:22<02:00,  3.45it/s] 47%|████▋     | 372/785 [02:23<01:59,  3.45it/s] 48%|████▊     | 373/785 [02:23<01:59,  3.45it/s] 48%|████▊     | 374/785 [02:23<01:59,  3.45it/s] 48%|████▊     | 375/785 [02:24<01:58,  3.45it/s] 48%|████▊     | 376/785 [02:24<01:58,  3.46it/s] 48%|████▊     | 377/785 [02:24<01:58,  3.46it/s] 48%|████▊     | 378/785 [02:24<01:57,  3.46it/s] 48%|████▊     | 379/785 [02:25<01:57,  3.46it/s] 48%|████▊     | 380/785 [02:25<01:57,  3.46it/s] 49%|████▊     | 381/785 [02:25<01:56,  3.46it/s] 49%|████▊     | 382/785 [02:26<01:57,  3.44it/s] 49%|████▉     | 383/785 [02:26<01:56,  3.45it/s] 49%|████▉     | 384/785 [02:26<01:56,  3.45it/s] 49%|████▉     | 385/785 [02:26<01:55,  3.45it/s] 49%|████▉     | 386/785 [02:27<01:55,  3.45it/s] 49%|████▉     | 387/785 [02:27<01:55,  3.45it/s] 49%|████▉     | 388/785 [02:27<01:54,  3.45it/s] 50%|████▉     | 389/785 [02:28<01:54,  3.45it/s] 50%|████▉     | 390/785 [02:28<01:54,  3.46it/s] 50%|████▉     | 391/785 [02:28<01:54,  3.46it/s] 50%|████▉     | 392/785 [02:29<01:53,  3.46it/s] 50%|█████     | 393/785 [02:29<01:53,  3.45it/s] 50%|█████     | 394/785 [02:29<01:53,  3.45it/s] 50%|█████     | 395/785 [02:29<01:53,  3.45it/s] 50%|█████     | 396/785 [02:30<01:52,  3.45it/s] 51%|█████     | 397/785 [02:30<01:52,  3.45it/s] 51%|█████     | 398/785 [02:30<01:52,  3.45it/s] 51%|█████     | 399/785 [02:31<01:51,  3.45it/s] 51%|█████     | 400/785 [02:31<01:51,  3.45it/s] 51%|█████     | 401/785 [02:31<01:51,  3.46it/s] 51%|█████     | 402/785 [02:31<01:50,  3.45it/s] 51%|█████▏    | 403/785 [02:32<01:50,  3.46it/s] 51%|█████▏    | 404/785 [02:32<01:50,  3.45it/s] 52%|█████▏    | 405/785 [02:32<01:50,  3.45it/s] 52%|█████▏    | 406/785 [02:33<01:49,  3.45it/s] 52%|█████▏    | 407/785 [02:33<01:49,  3.45it/s] 52%|█████▏    | 408/785 [02:33<01:49,  3.45it/s] 52%|█████▏    | 409/785 [02:33<01:48,  3.45it/s] 52%|█████▏    | 410/785 [02:34<01:48,  3.45it/s] 52%|█████▏    | 411/785 [02:34<01:48,  3.45it/s] 52%|█████▏    | 412/785 [02:34<01:48,  3.45it/s] 53%|█████▎    | 413/785 [02:35<01:47,  3.45it/s] 53%|█████▎    | 414/785 [02:35<01:47,  3.45it/s] 53%|█████▎    | 415/785 [02:35<01:47,  3.44it/s] 53%|█████▎    | 416/785 [02:35<01:47,  3.44it/s] 53%|█████▎    | 417/785 [02:36<01:46,  3.45it/s] 53%|█████▎    | 418/785 [02:36<01:46,  3.45it/s] 53%|█████▎    | 419/785 [02:36<01:46,  3.45it/s] 54%|█████▎    | 420/785 [02:37<01:45,  3.45it/s] 54%|█████▎    | 421/785 [02:37<01:45,  3.45it/s] 54%|█████▍    | 422/785 [02:37<01:45,  3.45it/s] 54%|█████▍    | 423/785 [02:37<01:44,  3.45it/s] 54%|█████▍    | 424/785 [02:38<01:44,  3.45it/s] 54%|█████▍    | 425/785 [02:38<01:44,  3.45it/s] 54%|█████▍    | 426/785 [02:38<01:44,  3.44it/s] 54%|█████▍    | 427/785 [02:39<01:43,  3.44it/s] 55%|█████▍    | 428/785 [02:39<01:43,  3.45it/s] 55%|█████▍    | 429/785 [02:39<01:43,  3.45it/s] 55%|█████▍    | 430/785 [02:40<01:42,  3.45it/s] 55%|█████▍    | 431/785 [02:40<01:42,  3.45it/s] 55%|█████▌    | 432/785 [02:40<01:42,  3.45it/s] 55%|█████▌    | 433/785 [02:40<01:42,  3.45it/s] 55%|█████▌    | 434/785 [02:41<01:41,  3.45it/s] 55%|█████▌    | 435/785 [02:41<01:41,  3.45it/s] 56%|█████▌    | 436/785 [02:41<01:41,  3.45it/s] 56%|█████▌    | 437/785 [02:42<01:41,  3.44it/s] 56%|█████▌    | 438/785 [02:42<01:40,  3.44it/s] 56%|█████▌    | 439/785 [02:42<01:40,  3.44it/s] 56%|█████▌    | 440/785 [02:42<01:40,  3.45it/s] 56%|█████▌    | 441/785 [02:43<01:39,  3.45it/s] 56%|█████▋    | 442/785 [02:43<01:39,  3.45it/s] 56%|█████▋    | 443/785 [02:43<01:39,  3.45it/s] 57%|█████▋    | 444/785 [02:44<01:38,  3.45it/s] 57%|█████▋    | 445/785 [02:44<01:38,  3.45it/s] 57%|█████▋    | 446/785 [02:44<01:38,  3.45it/s] 57%|█████▋    | 447/785 [02:44<01:37,  3.46it/s] 57%|█████▋    | 448/785 [02:45<01:37,  3.44it/s] 57%|█████▋    | 449/785 [02:45<01:37,  3.44it/s] 57%|█████▋    | 450/785 [02:45<01:37,  3.45it/s] 57%|█████▋    | 451/785 [02:46<01:36,  3.45it/s] 58%|█████▊    | 452/785 [02:46<01:36,  3.45it/s] 58%|█████▊    | 453/785 [02:46<01:36,  3.45it/s] 58%|█████▊    | 454/785 [02:46<01:35,  3.45it/s] 58%|█████▊    | 455/785 [02:47<01:35,  3.45it/s] 58%|█████▊    | 456/785 [02:47<01:35,  3.45it/s] 58%|█████▊    | 457/785 [02:47<01:35,  3.45it/s] 58%|█████▊    | 458/785 [02:48<01:34,  3.45it/s] 58%|█████▊    | 459/785 [02:48<01:34,  3.45it/s] 59%|█████▊    | 460/785 [02:48<01:34,  3.45it/s] 59%|█████▊    | 461/785 [02:49<01:33,  3.45it/s] 59%|█████▉    | 462/785 [02:49<01:33,  3.45it/s] 59%|█████▉    | 463/785 [02:49<01:33,  3.45it/s] 59%|█████▉    | 464/785 [02:49<01:32,  3.45it/s] 59%|█████▉    | 465/785 [02:50<01:32,  3.45it/s] 59%|█████▉    | 466/785 [02:50<01:32,  3.45it/s] 59%|█████▉    | 467/785 [02:50<01:32,  3.45it/s] 60%|█████▉    | 468/785 [02:51<01:31,  3.45it/s] 60%|█████▉    | 469/785 [02:51<01:31,  3.45it/s] 60%|█████▉    | 470/785 [02:51<01:31,  3.44it/s] 60%|██████    | 471/785 [02:51<01:25,  3.68it/s][INFO|trainer.py:2140] 2023-08-28 14:20:54,080 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:20:54,080 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 14:20:54,080 >>   Batch size = 8
{'eval_loss': 0.9451273679733276, 'eval_runtime': 9.3353, 'eval_samples_per_second': 372.455, 'eval_steps_per_second': 46.597, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.96it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.19it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.30it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.56it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.17it/s][A
  8%|▊         | 33/435 [00:00<00:08, 46.90it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.72it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.61it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.57it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.54it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.59it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.65it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.58it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.64it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.67it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.53it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.52it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.51it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.49it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.60it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.57it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.60it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.62it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.66it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.59it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.54it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.49it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.44it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.52it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.60it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.59it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.61it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.62it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.55it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.56it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.58it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.52it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.55it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.58it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.47it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.63it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.58it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.53it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.52it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.52it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.51it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.53it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.52it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.52it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.63it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.54it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.49it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.50it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.53it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.50it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.49it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.54it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.62it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.57it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.63it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.52it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.53it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.54it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.50it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.44it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.48it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.52it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.57it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.54it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.53it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.56it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.47it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.41it/s][A
 86%|████████▌ | 373/435 [00:07<00:01, 46.35it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.38it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.50it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.56it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.61it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.63it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.50it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.56it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.51it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.49it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.45it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.50it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.58it/s][A                                                 
                                                 [A 60%|██████    | 471/785 [03:01<01:25,  3.68it/s]
100%|██████████| 435/435 [00:09<00:00, 46.58it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:21:03,448 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-471
[INFO|configuration_utils.py:351] 2023-08-28 14:21:03,469 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-471/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:21:05,862 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-471/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:21:05,875 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-471/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:21:05,883 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-471/special_tokens_map.json
 60%|██████    | 472/785 [03:08<27:40,  5.31s/it] 60%|██████    | 473/785 [03:09<19:46,  3.80s/it] 60%|██████    | 474/785 [03:09<14:14,  2.75s/it] 61%|██████    | 475/785 [03:09<10:23,  2.01s/it] 61%|██████    | 476/785 [03:10<07:41,  1.49s/it] 61%|██████    | 477/785 [03:10<05:48,  1.13s/it] 61%|██████    | 478/785 [03:10<04:29,  1.14it/s] 61%|██████    | 479/785 [03:10<03:34,  1.42it/s] 61%|██████    | 480/785 [03:11<02:56,  1.73it/s] 61%|██████▏   | 481/785 [03:11<02:29,  2.04it/s] 61%|██████▏   | 482/785 [03:11<02:10,  2.32it/s] 62%|██████▏   | 483/785 [03:12<01:57,  2.58it/s] 62%|██████▏   | 484/785 [03:12<01:51,  2.71it/s] 62%|██████▏   | 485/785 [03:12<01:43,  2.90it/s] 62%|██████▏   | 486/785 [03:12<01:38,  3.04it/s] 62%|██████▏   | 487/785 [03:13<01:34,  3.16it/s] 62%|██████▏   | 488/785 [03:13<01:31,  3.25it/s] 62%|██████▏   | 489/785 [03:13<01:29,  3.31it/s] 62%|██████▏   | 490/785 [03:14<01:28,  3.35it/s] 63%|██████▎   | 491/785 [03:14<01:27,  3.37it/s] 63%|██████▎   | 492/785 [03:14<01:26,  3.39it/s] 63%|██████▎   | 493/785 [03:15<01:25,  3.41it/s] 63%|██████▎   | 494/785 [03:15<01:25,  3.42it/s] 63%|██████▎   | 495/785 [03:15<01:28,  3.28it/s] 63%|██████▎   | 496/785 [03:15<01:26,  3.33it/s] 63%|██████▎   | 497/785 [03:16<01:25,  3.37it/s] 63%|██████▎   | 498/785 [03:16<01:24,  3.40it/s] 64%|██████▎   | 499/785 [03:16<01:23,  3.42it/s] 64%|██████▎   | 500/785 [03:17<01:23,  3.43it/s]                                                  64%|██████▎   | 500/785 [03:17<01:23,  3.43it/s] 64%|██████▍   | 501/785 [03:17<01:22,  3.44it/s] 64%|██████▍   | 502/785 [03:17<01:22,  3.44it/s] 64%|██████▍   | 503/785 [03:17<01:21,  3.45it/s] 64%|██████▍   | 504/785 [03:18<01:21,  3.45it/s] 64%|██████▍   | 505/785 [03:18<01:21,  3.45it/s] 64%|██████▍   | 506/785 [03:18<01:24,  3.30it/s] 65%|██████▍   | 507/785 [03:19<01:23,  3.35it/s] 65%|██████▍   | 508/785 [03:19<01:22,  3.38it/s] 65%|██████▍   | 509/785 [03:19<01:21,  3.40it/s] 65%|██████▍   | 510/785 [03:20<01:20,  3.41it/s] 65%|██████▌   | 511/785 [03:20<01:20,  3.42it/s] 65%|██████▌   | 512/785 [03:20<01:19,  3.43it/s] 65%|██████▌   | 513/785 [03:20<01:19,  3.44it/s] 65%|██████▌   | 514/785 [03:21<01:18,  3.45it/s] 66%|██████▌   | 515/785 [03:21<01:18,  3.45it/s] 66%|██████▌   | 516/785 [03:21<01:18,  3.45it/s] 66%|██████▌   | 517/785 [03:22<01:23,  3.20it/s] 66%|██████▌   | 518/785 [03:22<01:21,  3.27it/s] 66%|██████▌   | 519/785 [03:22<01:19,  3.33it/s] 66%|██████▌   | 520/785 [03:22<01:18,  3.36it/s] 66%|██████▋   | 521/785 [03:23<01:17,  3.39it/s] 66%|██████▋   | 522/785 [03:23<01:17,  3.41it/s] 67%|██████▋   | 523/785 [03:23<01:16,  3.42it/s] 67%|██████▋   | 524/785 [03:24<01:16,  3.43it/s] 67%|██████▋   | 525/785 [03:24<01:15,  3.44it/s] 67%|██████▋   | 526/785 [03:24<01:15,  3.44it/s] 67%|██████▋   | 527/785 [03:25<01:14,  3.44it/s] 67%|██████▋   | 528/785 [03:25<01:14,  3.45it/s] 67%|██████▋   | 529/785 [03:25<01:14,  3.45it/s] 68%|██████▊   | 530/785 [03:25<01:13,  3.45it/s] 68%|██████▊   | 531/785 [03:26<01:13,  3.45it/s] 68%|██████▊   | 532/785 [03:26<01:13,  3.45it/s] 68%|██████▊   | 533/785 [03:26<01:12,  3.45it/s] 68%|██████▊   | 534/785 [03:27<01:12,  3.45it/s] 68%|██████▊   | 535/785 [03:27<01:12,  3.46it/s] 68%|██████▊   | 536/785 [03:27<01:12,  3.43it/s] 68%|██████▊   | 537/785 [03:27<01:12,  3.44it/s] 69%|██████▊   | 538/785 [03:28<01:11,  3.44it/s] 69%|██████▊   | 539/785 [03:28<01:11,  3.45it/s] 69%|██████▉   | 540/785 [03:28<01:11,  3.45it/s] 69%|██████▉   | 541/785 [03:29<01:10,  3.45it/s] 69%|██████▉   | 542/785 [03:29<01:10,  3.45it/s] 69%|██████▉   | 543/785 [03:29<01:10,  3.46it/s] 69%|██████▉   | 544/785 [03:29<01:09,  3.45it/s] 69%|██████▉   | 545/785 [03:30<01:09,  3.45it/s] 70%|██████▉   | 546/785 [03:30<01:09,  3.45it/s] 70%|██████▉   | 547/785 [03:30<01:09,  3.43it/s] 70%|██████▉   | 548/785 [03:31<01:08,  3.44it/s] 70%|██████▉   | 549/785 [03:31<01:08,  3.44it/s] 70%|███████   | 550/785 [03:31<01:08,  3.45it/s] 70%|███████   | 551/785 [03:31<01:07,  3.45it/s] 70%|███████   | 552/785 [03:32<01:07,  3.45it/s] 70%|███████   | 553/785 [03:32<01:07,  3.45it/s] 71%|███████   | 554/785 [03:32<01:06,  3.45it/s] 71%|███████   | 555/785 [03:33<01:06,  3.45it/s] 71%|███████   | 556/785 [03:33<01:06,  3.45it/s] 71%|███████   | 557/785 [03:33<01:06,  3.45it/s] 71%|███████   | 558/785 [03:34<01:05,  3.44it/s] 71%|███████   | 559/785 [03:34<01:05,  3.44it/s] 71%|███████▏  | 560/785 [03:34<01:05,  3.45it/s] 71%|███████▏  | 561/785 [03:34<01:04,  3.45it/s] 72%|███████▏  | 562/785 [03:35<01:04,  3.45it/s] 72%|███████▏  | 563/785 [03:35<01:04,  3.45it/s] 72%|███████▏  | 564/785 [03:35<01:04,  3.45it/s] 72%|███████▏  | 565/785 [03:36<01:03,  3.45it/s] 72%|███████▏  | 566/785 [03:36<01:03,  3.45it/s] 72%|███████▏  | 567/785 [03:36<01:03,  3.45it/s] 72%|███████▏  | 568/785 [03:36<01:02,  3.45it/s] 72%|███████▏  | 569/785 [03:37<01:02,  3.44it/s] 73%|███████▎  | 570/785 [03:37<01:02,  3.44it/s] 73%|███████▎  | 571/785 [03:37<01:02,  3.45it/s] 73%|███████▎  | 572/785 [03:38<01:01,  3.45it/s] 73%|███████▎  | 573/785 [03:38<01:01,  3.45it/s] 73%|███████▎  | 574/785 [03:38<01:01,  3.45it/s] 73%|███████▎  | 575/785 [03:38<01:00,  3.45it/s] 73%|███████▎  | 576/785 [03:39<01:00,  3.45it/s] 74%|███████▎  | 577/785 [03:39<01:00,  3.45it/s] 74%|███████▎  | 578/785 [03:39<00:59,  3.45it/s] 74%|███████▍  | 579/785 [03:40<00:59,  3.45it/s] 74%|███████▍  | 580/785 [03:40<00:59,  3.44it/s] 74%|███████▍  | 581/785 [03:40<00:59,  3.44it/s] 74%|███████▍  | 582/785 [03:40<00:58,  3.45it/s] 74%|███████▍  | 583/785 [03:41<00:58,  3.45it/s] 74%|███████▍  | 584/785 [03:41<00:58,  3.45it/s] 75%|███████▍  | 585/785 [03:41<00:57,  3.45it/s] 75%|███████▍  | 586/785 [03:42<00:57,  3.45it/s] 75%|███████▍  | 587/785 [03:42<00:57,  3.45it/s] 75%|███████▍  | 588/785 [03:42<00:57,  3.45it/s] 75%|███████▌  | 589/785 [03:43<00:56,  3.45it/s] 75%|███████▌  | 590/785 [03:43<00:56,  3.45it/s] 75%|███████▌  | 591/785 [03:43<00:56,  3.43it/s] 75%|███████▌  | 592/785 [03:43<00:56,  3.44it/s] 76%|███████▌  | 593/785 [03:44<00:55,  3.44it/s] 76%|███████▌  | 594/785 [03:44<00:55,  3.44it/s] 76%|███████▌  | 595/785 [03:44<00:55,  3.45it/s] 76%|███████▌  | 596/785 [03:45<00:54,  3.45it/s] 76%|███████▌  | 597/785 [03:45<00:54,  3.45it/s] 76%|███████▌  | 598/785 [03:45<00:54,  3.45it/s] 76%|███████▋  | 599/785 [03:45<00:54,  3.43it/s] 76%|███████▋  | 600/785 [03:46<00:53,  3.44it/s] 77%|███████▋  | 601/785 [03:46<00:53,  3.44it/s] 77%|███████▋  | 602/785 [03:46<00:53,  3.44it/s] 77%|███████▋  | 603/785 [03:47<00:52,  3.44it/s] 77%|███████▋  | 604/785 [03:47<00:53,  3.38it/s] 77%|███████▋  | 605/785 [03:47<00:52,  3.40it/s] 77%|███████▋  | 606/785 [03:47<00:52,  3.41it/s] 77%|███████▋  | 607/785 [03:48<00:52,  3.42it/s] 77%|███████▋  | 608/785 [03:48<00:51,  3.43it/s] 78%|███████▊  | 609/785 [03:48<00:51,  3.44it/s] 78%|███████▊  | 610/785 [03:49<00:50,  3.44it/s] 78%|███████▊  | 611/785 [03:49<00:50,  3.45it/s] 78%|███████▊  | 612/785 [03:49<00:50,  3.45it/s] 78%|███████▊  | 613/785 [03:49<00:49,  3.44it/s] 78%|███████▊  | 614/785 [03:50<00:49,  3.44it/s] 78%|███████▊  | 615/785 [03:50<00:49,  3.45it/s] 78%|███████▊  | 616/785 [03:50<00:49,  3.45it/s] 79%|███████▊  | 617/785 [03:51<00:48,  3.45it/s] 79%|███████▊  | 618/785 [03:51<00:48,  3.45it/s] 79%|███████▉  | 619/785 [03:51<00:48,  3.45it/s] 79%|███████▉  | 620/785 [03:52<00:47,  3.45it/s] 79%|███████▉  | 621/785 [03:52<00:47,  3.45it/s] 79%|███████▉  | 622/785 [03:52<00:47,  3.45it/s] 79%|███████▉  | 623/785 [03:52<00:46,  3.45it/s] 79%|███████▉  | 624/785 [03:53<00:46,  3.44it/s] 80%|███████▉  | 625/785 [03:53<00:46,  3.44it/s] 80%|███████▉  | 626/785 [03:53<00:46,  3.44it/s] 80%|███████▉  | 627/785 [03:54<00:45,  3.45it/s] 80%|████████  | 628/785 [03:54<00:42,  3.68it/s][INFO|trainer.py:2140] 2023-08-28 14:21:56,503 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:21:56,504 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 14:21:56,504 >>   Batch size = 8
{'eval_loss': 0.9546495079994202, 'eval_runtime': 9.345, 'eval_samples_per_second': 372.071, 'eval_steps_per_second': 46.549, 'epoch': 3.0}
{'loss': 0.7473, 'learning_rate': 1.3614649681528663e-05, 'epoch': 3.18}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.32it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.37it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.51it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.77it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.25it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.04it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.77it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.67it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.66it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.59it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.55it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.62it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.63it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.69it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.65it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.60it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.52it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.53it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.59it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.58it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.48it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.56it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.53it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.57it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.59it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.54it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.53it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.62it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.59it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.60it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.51it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.52it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.62it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.66it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.62it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.68it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.66it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.55it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.56it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.53it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.60it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.62it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.56it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.63it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.48it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.62it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.54it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.49it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.55it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.62it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.60it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.57it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.56it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.59it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.61it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.51it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.50it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.51it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.53it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.57it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.57it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.51it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.57it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.57it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.52it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.55it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.41it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.35it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.49it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.53it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.51it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.57it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.61it/s][A
 86%|████████▌ | 373/435 [00:07<00:01, 46.60it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.50it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.51it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.50it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.56it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.61it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.59it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.68it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.62it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.52it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.58it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.58it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.49it/s][A                                                 
                                                 [A 80%|████████  | 628/785 [04:03<00:42,  3.68it/s]
100%|██████████| 435/435 [00:09<00:00, 46.49it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:22:05,867 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-628
[INFO|configuration_utils.py:351] 2023-08-28 14:22:05,883 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-628/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:22:08,351 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-628/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:22:08,371 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-628/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:22:08,384 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-628/special_tokens_map.json
 80%|████████  | 629/785 [04:11<13:49,  5.32s/it] 80%|████████  | 630/785 [04:11<09:50,  3.81s/it] 80%|████████  | 631/785 [04:11<07:04,  2.75s/it] 81%|████████  | 632/785 [04:12<05:08,  2.01s/it] 81%|████████  | 633/785 [04:12<03:47,  1.50s/it] 81%|████████  | 634/785 [04:12<02:51,  1.13s/it] 81%|████████  | 635/785 [04:13<02:12,  1.14it/s] 81%|████████  | 636/785 [04:13<01:44,  1.42it/s] 81%|████████  | 637/785 [04:13<01:25,  1.73it/s] 81%|████████▏ | 638/785 [04:13<01:12,  2.03it/s] 81%|████████▏ | 639/785 [04:14<01:02,  2.32it/s] 82%|████████▏ | 640/785 [04:14<00:56,  2.58it/s] 82%|████████▏ | 641/785 [04:14<00:51,  2.78it/s] 82%|████████▏ | 642/785 [04:15<00:48,  2.95it/s] 82%|████████▏ | 643/785 [04:15<00:45,  3.09it/s] 82%|████████▏ | 644/785 [04:15<00:44,  3.19it/s] 82%|████████▏ | 645/785 [04:16<00:42,  3.27it/s] 82%|████████▏ | 646/785 [04:16<00:41,  3.32it/s] 82%|████████▏ | 647/785 [04:16<00:41,  3.36it/s] 83%|████████▎ | 648/785 [04:16<00:40,  3.39it/s] 83%|████████▎ | 649/785 [04:17<00:39,  3.42it/s] 83%|████████▎ | 650/785 [04:17<00:39,  3.43it/s] 83%|████████▎ | 651/785 [04:17<00:38,  3.44it/s] 83%|████████▎ | 652/785 [04:18<00:41,  3.21it/s] 83%|████████▎ | 653/785 [04:18<00:40,  3.28it/s] 83%|████████▎ | 654/785 [04:18<00:39,  3.33it/s] 83%|████████▎ | 655/785 [04:18<00:38,  3.37it/s] 84%|████████▎ | 656/785 [04:19<00:38,  3.39it/s] 84%|████████▎ | 657/785 [04:19<00:37,  3.41it/s] 84%|████████▍ | 658/785 [04:19<00:37,  3.41it/s] 84%|████████▍ | 659/785 [04:20<00:36,  3.42it/s] 84%|████████▍ | 660/785 [04:20<00:36,  3.43it/s] 84%|████████▍ | 661/785 [04:20<00:36,  3.44it/s] 84%|████████▍ | 662/785 [04:20<00:35,  3.45it/s] 84%|████████▍ | 663/785 [04:21<00:38,  3.20it/s] 85%|████████▍ | 664/785 [04:21<00:36,  3.27it/s] 85%|████████▍ | 665/785 [04:21<00:36,  3.33it/s] 85%|████████▍ | 666/785 [04:22<00:35,  3.36it/s] 85%|████████▍ | 667/785 [04:22<00:34,  3.39it/s] 85%|████████▌ | 668/785 [04:22<00:34,  3.41it/s] 85%|████████▌ | 669/785 [04:23<00:33,  3.43it/s] 85%|████████▌ | 670/785 [04:23<00:33,  3.44it/s] 85%|████████▌ | 671/785 [04:23<00:33,  3.44it/s] 86%|████████▌ | 672/785 [04:23<00:32,  3.45it/s] 86%|████████▌ | 673/785 [04:24<00:32,  3.45it/s] 86%|████████▌ | 674/785 [04:24<00:33,  3.28it/s] 86%|████████▌ | 675/785 [04:24<00:32,  3.33it/s] 86%|████████▌ | 676/785 [04:25<00:32,  3.37it/s] 86%|████████▌ | 677/785 [04:25<00:31,  3.39it/s] 86%|████████▋ | 678/785 [04:25<00:31,  3.40it/s] 86%|████████▋ | 679/785 [04:26<00:31,  3.42it/s] 87%|████████▋ | 680/785 [04:26<00:30,  3.43it/s] 87%|████████▋ | 681/785 [04:26<00:30,  3.44it/s] 87%|████████▋ | 682/785 [04:26<00:29,  3.44it/s] 87%|████████▋ | 683/785 [04:27<00:29,  3.45it/s] 87%|████████▋ | 684/785 [04:27<00:29,  3.45it/s] 87%|████████▋ | 685/785 [04:27<00:28,  3.45it/s] 87%|████████▋ | 686/785 [04:28<00:28,  3.45it/s] 88%|████████▊ | 687/785 [04:28<00:28,  3.45it/s] 88%|████████▊ | 688/785 [04:28<00:28,  3.45it/s] 88%|████████▊ | 689/785 [04:28<00:29,  3.27it/s] 88%|████████▊ | 690/785 [04:29<00:28,  3.32it/s] 88%|████████▊ | 691/785 [04:29<00:27,  3.36it/s] 88%|████████▊ | 692/785 [04:29<00:27,  3.39it/s] 88%|████████▊ | 693/785 [04:30<00:26,  3.41it/s] 88%|████████▊ | 694/785 [04:30<00:26,  3.42it/s] 89%|████████▊ | 695/785 [04:30<00:26,  3.43it/s] 89%|████████▊ | 696/785 [04:31<00:25,  3.44it/s] 89%|████████▉ | 697/785 [04:31<00:25,  3.44it/s] 89%|████████▉ | 698/785 [04:31<00:25,  3.44it/s] 89%|████████▉ | 699/785 [04:31<00:24,  3.45it/s] 89%|████████▉ | 700/785 [04:32<00:25,  3.38it/s] 89%|████████▉ | 701/785 [04:32<00:24,  3.40it/s] 89%|████████▉ | 702/785 [04:32<00:24,  3.41it/s] 90%|████████▉ | 703/785 [04:33<00:23,  3.42it/s] 90%|████████▉ | 704/785 [04:33<00:23,  3.43it/s] 90%|████████▉ | 705/785 [04:33<00:23,  3.44it/s] 90%|████████▉ | 706/785 [04:33<00:22,  3.45it/s] 90%|█████████ | 707/785 [04:34<00:22,  3.45it/s] 90%|█████████ | 708/785 [04:34<00:22,  3.45it/s] 90%|█████████ | 709/785 [04:34<00:22,  3.45it/s] 90%|█████████ | 710/785 [04:35<00:21,  3.45it/s] 91%|█████████ | 711/785 [04:35<00:22,  3.25it/s] 91%|█████████ | 712/785 [04:35<00:22,  3.30it/s] 91%|█████████ | 713/785 [04:36<00:21,  3.34it/s] 91%|█████████ | 714/785 [04:36<00:21,  3.37it/s] 91%|█████████ | 715/785 [04:36<00:20,  3.39it/s] 91%|█████████ | 716/785 [04:36<00:20,  3.41it/s] 91%|█████████▏| 717/785 [04:37<00:19,  3.42it/s] 91%|█████████▏| 718/785 [04:37<00:19,  3.43it/s] 92%|█████████▏| 719/785 [04:37<00:19,  3.44it/s] 92%|█████████▏| 720/785 [04:38<00:18,  3.44it/s] 92%|█████████▏| 721/785 [04:38<00:18,  3.44it/s] 92%|█████████▏| 722/785 [04:38<00:18,  3.44it/s] 92%|█████████▏| 723/785 [04:38<00:18,  3.44it/s] 92%|█████████▏| 724/785 [04:39<00:17,  3.44it/s] 92%|█████████▏| 725/785 [04:39<00:17,  3.45it/s] 92%|█████████▏| 726/785 [04:39<00:17,  3.45it/s] 93%|█████████▎| 727/785 [04:40<00:16,  3.45it/s] 93%|█████████▎| 728/785 [04:40<00:16,  3.45it/s] 93%|█████████▎| 729/785 [04:40<00:16,  3.45it/s] 93%|█████████▎| 730/785 [04:40<00:15,  3.45it/s] 93%|█████████▎| 731/785 [04:41<00:15,  3.45it/s] 93%|█████████▎| 732/785 [04:41<00:15,  3.45it/s] 93%|█████████▎| 733/785 [04:41<00:15,  3.45it/s] 94%|█████████▎| 734/785 [04:42<00:14,  3.45it/s] 94%|█████████▎| 735/785 [04:42<00:14,  3.45it/s] 94%|█████████▍| 736/785 [04:42<00:14,  3.45it/s] 94%|█████████▍| 737/785 [04:42<00:13,  3.45it/s] 94%|█████████▍| 738/785 [04:43<00:13,  3.45it/s] 94%|█████████▍| 739/785 [04:43<00:13,  3.45it/s] 94%|█████████▍| 740/785 [04:43<00:13,  3.45it/s] 94%|█████████▍| 741/785 [04:44<00:12,  3.46it/s] 95%|█████████▍| 742/785 [04:44<00:12,  3.45it/s] 95%|█████████▍| 743/785 [04:44<00:12,  3.46it/s] 95%|█████████▍| 744/785 [04:45<00:11,  3.44it/s] 95%|█████████▍| 745/785 [04:45<00:11,  3.44it/s] 95%|█████████▌| 746/785 [04:45<00:11,  3.45it/s] 95%|█████████▌| 747/785 [04:45<00:11,  3.45it/s] 95%|█████████▌| 748/785 [04:46<00:10,  3.45it/s] 95%|█████████▌| 749/785 [04:46<00:10,  3.45it/s] 96%|█████████▌| 750/785 [04:46<00:10,  3.45it/s] 96%|█████████▌| 751/785 [04:47<00:09,  3.45it/s] 96%|█████████▌| 752/785 [04:47<00:09,  3.45it/s] 96%|█████████▌| 753/785 [04:47<00:09,  3.45it/s] 96%|█████████▌| 754/785 [04:47<00:08,  3.45it/s] 96%|█████████▌| 755/785 [04:48<00:08,  3.44it/s] 96%|█████████▋| 756/785 [04:48<00:08,  3.45it/s] 96%|█████████▋| 757/785 [04:48<00:08,  3.45it/s] 97%|█████████▋| 758/785 [04:49<00:07,  3.45it/s] 97%|█████████▋| 759/785 [04:49<00:07,  3.45it/s] 97%|█████████▋| 760/785 [04:49<00:07,  3.45it/s] 97%|█████████▋| 761/785 [04:49<00:06,  3.45it/s] 97%|█████████▋| 762/785 [04:50<00:06,  3.45it/s] 97%|█████████▋| 763/785 [04:50<00:06,  3.45it/s] 97%|█████████▋| 764/785 [04:50<00:06,  3.45it/s] 97%|█████████▋| 765/785 [04:51<00:05,  3.45it/s] 98%|█████████▊| 766/785 [04:51<00:05,  3.44it/s] 98%|█████████▊| 767/785 [04:51<00:05,  3.45it/s] 98%|█████████▊| 768/785 [04:51<00:04,  3.45it/s] 98%|█████████▊| 769/785 [04:52<00:04,  3.45it/s] 98%|█████████▊| 770/785 [04:52<00:04,  3.45it/s] 98%|█████████▊| 771/785 [04:52<00:04,  3.45it/s] 98%|█████████▊| 772/785 [04:53<00:03,  3.45it/s] 98%|█████████▊| 773/785 [04:53<00:03,  3.45it/s] 99%|█████████▊| 774/785 [04:53<00:03,  3.45it/s] 99%|█████████▊| 775/785 [04:54<00:02,  3.45it/s] 99%|█████████▉| 776/785 [04:54<00:02,  3.45it/s] 99%|█████████▉| 777/785 [04:54<00:02,  3.42it/s] 99%|█████████▉| 778/785 [04:54<00:02,  3.42it/s] 99%|█████████▉| 779/785 [04:55<00:01,  3.43it/s] 99%|█████████▉| 780/785 [04:55<00:01,  3.44it/s] 99%|█████████▉| 781/785 [04:55<00:01,  3.44it/s]100%|█████████▉| 782/785 [04:56<00:00,  3.44it/s]100%|█████████▉| 783/785 [04:56<00:00,  3.45it/s]100%|█████████▉| 784/785 [04:56<00:00,  3.44it/s]100%|██████████| 785/785 [04:56<00:00,  3.68it/s][INFO|trainer.py:2140] 2023-08-28 14:22:59,073 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:22:59,074 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 14:22:59,074 >>   Batch size = 8
{'eval_loss': 0.9628894925117493, 'eval_runtime': 9.3394, 'eval_samples_per_second': 372.295, 'eval_steps_per_second': 46.577, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.81it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.27it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.57it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.88it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.47it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.20it/s][A
  9%|▊         | 38/435 [00:00<00:08, 47.03it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.82it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.68it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.62it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.58it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.58it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.66it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.72it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.63it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.53it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.50it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.52it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.52it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.49it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.48it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.42it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.43it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.56it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.35it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.34it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.37it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.44it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.48it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.46it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.45it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.41it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.52it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.60it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.60it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.53it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.45it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.35it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.46it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.49it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.47it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.48it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.47it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.52it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.51it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.45it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.35it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.26it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.28it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.40it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.38it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.49it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.45it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.41it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.40it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.39it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.46it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.49it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.46it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.45it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.46it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.33it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.48it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.50it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.58it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.40it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.38it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.44it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.47it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.54it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.57it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.42it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.41it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.55it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.53it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.54it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.47it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.42it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.55it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.42it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.44it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.41it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.50it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.54it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.38it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.42it/s][A                                                 
                                                 [A100%|██████████| 785/785 [05:06<00:00,  3.68it/s]
100%|██████████| 435/435 [00:09<00:00, 46.42it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:23:08,453 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-785
[INFO|configuration_utils.py:351] 2023-08-28 14:23:08,472 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-785/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:23:11,296 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-785/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:23:11,320 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-785/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:23:11,335 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-785/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 14:23:16,059 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 14:23:16,062 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-157 (score: 0.9368933439254761).
                                                 100%|██████████| 785/785 [05:15<00:00,  3.68it/s]100%|██████████| 785/785 [05:15<00:00,  2.49it/s]
[INFO|trainer.py:1894] 2023-08-28 14:23:17,940 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 14:23:17,959 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:23:20,389 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:23:20,413 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:23:20,427 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:23:20,625 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:23:20,625 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:23:20,625 >>   train_loss               =     0.7353
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:23:20,626 >>   train_runtime            = 0:05:15.71
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:23:20,626 >>   train_samples            =      10032
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:23:20,626 >>   train_samples_per_second =    158.877
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:23:20,626 >>   train_steps_per_second   =      2.486
{'eval_loss': 0.9682080745697021, 'eval_runtime': 9.3548, 'eval_samples_per_second': 371.68, 'eval_steps_per_second': 46.5, 'epoch': 5.0}
{'train_runtime': 315.715, 'train_samples_per_second': 158.877, 'train_steps_per_second': 2.486, 'train_loss': 0.7352550530889231, 'epoch': 5.0}
08/28/2023 14:23:20 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 14:23:20,666 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:23:20,666 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 14:23:20,666 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|▏         | 6/435 [00:00<00:07, 57.81it/s]  3%|▎         | 12/435 [00:00<00:08, 50.93it/s]  4%|▍         | 18/435 [00:00<00:08, 49.11it/s]  5%|▌         | 23/435 [00:00<00:08, 48.23it/s]  6%|▋         | 28/435 [00:00<00:08, 47.66it/s]  8%|▊         | 33/435 [00:00<00:08, 47.41it/s]  9%|▊         | 38/435 [00:00<00:08, 47.31it/s] 10%|▉         | 43/435 [00:00<00:08, 47.09it/s] 11%|█         | 48/435 [00:01<00:08, 46.78it/s] 12%|█▏        | 53/435 [00:01<00:08, 46.79it/s] 13%|█▎        | 58/435 [00:01<00:08, 46.75it/s] 14%|█▍        | 63/435 [00:01<00:07, 46.63it/s] 16%|█▌        | 68/435 [00:01<00:07, 46.69it/s] 17%|█▋        | 73/435 [00:01<00:07, 46.79it/s] 18%|█▊        | 78/435 [00:01<00:07, 46.86it/s] 19%|█▉        | 83/435 [00:01<00:07, 46.90it/s] 20%|██        | 88/435 [00:01<00:07, 46.76it/s] 21%|██▏       | 93/435 [00:01<00:07, 46.43it/s] 23%|██▎       | 98/435 [00:02<00:07, 46.55it/s] 24%|██▎       | 103/435 [00:02<00:07, 46.54it/s] 25%|██▍       | 108/435 [00:02<00:07, 46.57it/s] 26%|██▌       | 113/435 [00:02<00:06, 46.64it/s] 27%|██▋       | 118/435 [00:02<00:06, 46.61it/s] 28%|██▊       | 123/435 [00:02<00:06, 46.55it/s] 29%|██▉       | 128/435 [00:02<00:06, 46.69it/s] 31%|███       | 133/435 [00:02<00:06, 46.69it/s] 32%|███▏      | 138/435 [00:02<00:06, 46.75it/s] 33%|███▎      | 143/435 [00:03<00:06, 46.67it/s] 34%|███▍      | 148/435 [00:03<00:06, 46.55it/s] 35%|███▌      | 153/435 [00:03<00:06, 46.60it/s] 36%|███▋      | 158/435 [00:03<00:05, 46.50it/s] 37%|███▋      | 163/435 [00:03<00:05, 46.55it/s] 39%|███▊      | 168/435 [00:03<00:05, 46.68it/s] 40%|███▉      | 173/435 [00:03<00:05, 46.79it/s] 41%|████      | 178/435 [00:03<00:05, 46.84it/s] 42%|████▏     | 183/435 [00:03<00:05, 46.77it/s] 43%|████▎     | 188/435 [00:04<00:05, 46.79it/s] 44%|████▍     | 193/435 [00:04<00:05, 46.71it/s] 46%|████▌     | 198/435 [00:04<00:05, 46.66it/s] 47%|████▋     | 203/435 [00:04<00:04, 46.69it/s] 48%|████▊     | 208/435 [00:04<00:04, 46.80it/s] 49%|████▉     | 213/435 [00:04<00:04, 46.88it/s] 50%|█████     | 218/435 [00:04<00:04, 46.76it/s] 51%|█████▏    | 223/435 [00:04<00:04, 46.78it/s] 52%|█████▏    | 228/435 [00:04<00:04, 46.77it/s] 54%|█████▎    | 233/435 [00:04<00:04, 46.78it/s] 55%|█████▍    | 238/435 [00:05<00:04, 46.84it/s] 56%|█████▌    | 243/435 [00:05<00:04, 46.78it/s] 57%|█████▋    | 248/435 [00:05<00:04, 46.61it/s] 58%|█████▊    | 253/435 [00:05<00:03, 46.63it/s] 59%|█████▉    | 258/435 [00:05<00:03, 46.74it/s] 60%|██████    | 263/435 [00:05<00:03, 46.76it/s] 62%|██████▏   | 268/435 [00:05<00:03, 46.83it/s] 63%|██████▎   | 273/435 [00:05<00:03, 46.86it/s] 64%|██████▍   | 278/435 [00:05<00:03, 46.73it/s] 65%|██████▌   | 283/435 [00:06<00:03, 46.72it/s] 66%|██████▌   | 288/435 [00:06<00:03, 46.74it/s] 67%|██████▋   | 293/435 [00:06<00:03, 46.69it/s] 69%|██████▊   | 298/435 [00:06<00:02, 46.60it/s] 70%|██████▉   | 303/435 [00:06<00:02, 46.68it/s] 71%|███████   | 308/435 [00:06<00:02, 46.65it/s] 72%|███████▏  | 313/435 [00:06<00:02, 46.75it/s] 73%|███████▎  | 318/435 [00:06<00:02, 46.72it/s] 74%|███████▍  | 323/435 [00:06<00:02, 46.80it/s] 75%|███████▌  | 328/435 [00:06<00:02, 46.85it/s] 77%|███████▋  | 333/435 [00:07<00:02, 46.81it/s] 78%|███████▊  | 338/435 [00:07<00:02, 46.72it/s] 79%|███████▉  | 343/435 [00:07<00:01, 46.61it/s] 80%|████████  | 348/435 [00:07<00:01, 46.55it/s] 81%|████████  | 353/435 [00:07<00:01, 46.67it/s] 82%|████████▏ | 358/435 [00:07<00:01, 46.75it/s] 83%|████████▎ | 363/435 [00:07<00:01, 46.79it/s] 85%|████████▍ | 368/435 [00:07<00:01, 46.87it/s] 86%|████████▌ | 373/435 [00:07<00:01, 46.66it/s] 87%|████████▋ | 378/435 [00:08<00:01, 46.64it/s] 88%|████████▊ | 383/435 [00:08<00:01, 46.72it/s] 89%|████████▉ | 388/435 [00:08<00:01, 46.67it/s] 90%|█████████ | 393/435 [00:08<00:00, 46.64it/s] 91%|█████████▏| 398/435 [00:08<00:00, 46.60it/s] 93%|█████████▎| 403/435 [00:08<00:00, 46.68it/s] 94%|█████████▍| 408/435 [00:08<00:00, 46.70it/s] 95%|█████████▍| 413/435 [00:08<00:00, 46.70it/s] 96%|█████████▌| 418/435 [00:08<00:00, 46.74it/s] 97%|█████████▋| 423/435 [00:09<00:00, 46.72it/s] 98%|█████████▊| 428/435 [00:09<00:00, 46.74it/s]100%|█████████▉| 433/435 [00:09<00:00, 46.74it/s]100%|██████████| 435/435 [00:09<00:00, 46.82it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:23:29,978 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:23:29,978 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:23:29,979 >>   eval_loss               =     0.9369
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:23:29,979 >>   eval_runtime            = 0:00:09.31
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:23:29,979 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:23:29,979 >>   eval_samples_per_second =    373.379
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:23:29,979 >>   eval_steps_per_second   =     46.713
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:23:29,979 >>   perplexity              =      2.552
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:36,656 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:36,660 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:36,660 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:36,660 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:36,661 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:23:37,372 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:23:37,373 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:23:37,961 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:23:39,005 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:23:39,005 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:41,960 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:41,965 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:41,966 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:41,966 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:23:41,966 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:23:42,593 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:23:42,594 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:23:43,169 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:23:43,328 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:23:43,328 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-471
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-628
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-785
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-157
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-314
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.70it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:05,  1.51it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:07,  1.46it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.46it/s]Extractor Predicting: 15it [00:09,  1.47it/s]Extractor Predicting: 16it [00:10,  1.47it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.54it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.50it/s]Extractor Predicting: 26it [00:17,  1.48it/s]Extractor Predicting: 27it [00:17,  1.50it/s]Extractor Predicting: 28it [00:18,  1.47it/s]Extractor Predicting: 29it [00:19,  1.45it/s]Extractor Predicting: 30it [00:20,  1.46it/s]Extractor Predicting: 31it [00:20,  1.46it/s]Extractor Predicting: 32it [00:21,  1.46it/s]Extractor Predicting: 33it [00:22,  1.44it/s]Extractor Predicting: 34it [00:22,  1.46it/s]Extractor Predicting: 35it [00:23,  1.50it/s]Extractor Predicting: 36it [00:24,  1.52it/s]Extractor Predicting: 37it [00:24,  1.51it/s]Extractor Predicting: 38it [00:25,  1.46it/s]Extractor Predicting: 39it [00:26,  1.48it/s]Extractor Predicting: 40it [00:26,  1.48it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.40it/s]Extractor Predicting: 43it [00:28,  1.41it/s]Extractor Predicting: 44it [00:29,  1.43it/s]Extractor Predicting: 45it [00:30,  1.42it/s]Extractor Predicting: 46it [00:30,  1.45it/s]Extractor Predicting: 47it [00:31,  1.48it/s]Extractor Predicting: 48it [00:32,  1.48it/s]Extractor Predicting: 49it [00:32,  1.50it/s]Extractor Predicting: 50it [00:33,  1.54it/s]Extractor Predicting: 51it [00:34,  1.53it/s]Extractor Predicting: 52it [00:34,  1.48it/s]Extractor Predicting: 53it [00:35,  1.47it/s]Extractor Predicting: 54it [00:36,  1.50it/s]Extractor Predicting: 55it [00:36,  1.48it/s]Extractor Predicting: 56it [00:37,  1.47it/s]Extractor Predicting: 57it [00:38,  1.46it/s]Extractor Predicting: 58it [00:38,  1.49it/s]Extractor Predicting: 59it [00:39,  1.52it/s]Extractor Predicting: 60it [00:40,  1.53it/s]Extractor Predicting: 61it [00:40,  1.53it/s]Extractor Predicting: 62it [00:41,  1.54it/s]Extractor Predicting: 63it [00:42,  1.53it/s]Extractor Predicting: 64it [00:42,  1.53it/s]Extractor Predicting: 65it [00:43,  1.53it/s]Extractor Predicting: 66it [00:44,  1.51it/s]Extractor Predicting: 67it [00:44,  1.51it/s]Extractor Predicting: 68it [00:45,  1.52it/s]Extractor Predicting: 69it [00:46,  1.52it/s]Extractor Predicting: 70it [00:46,  1.51it/s]Extractor Predicting: 71it [00:47,  1.50it/s]Extractor Predicting: 72it [00:48,  1.52it/s]Extractor Predicting: 73it [00:48,  1.51it/s]Extractor Predicting: 74it [00:49,  1.53it/s]Extractor Predicting: 75it [00:50,  1.55it/s]Extractor Predicting: 76it [00:50,  1.53it/s]Extractor Predicting: 77it [00:51,  1.50it/s]Extractor Predicting: 78it [00:52,  1.49it/s]Extractor Predicting: 79it [00:52,  1.48it/s]Extractor Predicting: 80it [00:53,  1.47it/s]Extractor Predicting: 81it [00:54,  1.45it/s]Extractor Predicting: 82it [00:54,  1.48it/s]Extractor Predicting: 83it [00:55,  1.49it/s]Extractor Predicting: 84it [00:56,  1.50it/s]Extractor Predicting: 85it [00:56,  1.48it/s]Extractor Predicting: 86it [00:57,  1.49it/s]Extractor Predicting: 87it [00:58,  1.49it/s]Extractor Predicting: 88it [00:58,  1.48it/s]Extractor Predicting: 89it [00:59,  1.50it/s]Extractor Predicting: 90it [01:00,  1.50it/s]Extractor Predicting: 91it [01:00,  1.54it/s]Extractor Predicting: 92it [01:01,  1.57it/s]Extractor Predicting: 93it [01:02,  1.58it/s]Extractor Predicting: 94it [01:02,  1.55it/s]Extractor Predicting: 95it [01:03,  1.57it/s]Extractor Predicting: 96it [01:03,  1.56it/s]Extractor Predicting: 97it [01:04,  1.55it/s]Extractor Predicting: 98it [01:05,  1.56it/s]Extractor Predicting: 99it [01:05,  1.52it/s]Extractor Predicting: 100it [01:06,  1.48it/s]Extractor Predicting: 101it [01:07,  1.48it/s]Extractor Predicting: 102it [01:07,  1.55it/s]Extractor Predicting: 103it [01:08,  1.57it/s]Extractor Predicting: 104it [01:09,  1.56it/s]Extractor Predicting: 105it [01:09,  1.55it/s]Extractor Predicting: 106it [01:10,  1.55it/s]Extractor Predicting: 107it [01:11,  1.56it/s]Extractor Predicting: 108it [01:11,  1.54it/s]Extractor Predicting: 109it [01:12,  1.53it/s]Extractor Predicting: 110it [01:13,  1.54it/s]Extractor Predicting: 111it [01:13,  1.54it/s]Extractor Predicting: 112it [01:14,  1.54it/s]Extractor Predicting: 113it [01:15,  1.58it/s]Extractor Predicting: 114it [01:15,  1.61it/s]Extractor Predicting: 115it [01:16,  1.59it/s]Extractor Predicting: 116it [01:16,  1.58it/s]Extractor Predicting: 117it [01:17,  1.58it/s]Extractor Predicting: 118it [01:18,  1.56it/s]Extractor Predicting: 119it [01:18,  1.57it/s]Extractor Predicting: 120it [01:19,  1.53it/s]Extractor Predicting: 121it [01:20,  1.50it/s]Extractor Predicting: 122it [01:20,  1.51it/s]Extractor Predicting: 123it [01:21,  1.51it/s]Extractor Predicting: 124it [01:22,  1.49it/s]Extractor Predicting: 125it [01:22,  1.49it/s]Extractor Predicting: 126it [01:23,  1.51it/s]Extractor Predicting: 127it [01:24,  1.38it/s]Extractor Predicting: 128it [01:25,  1.43it/s]Extractor Predicting: 129it [01:25,  1.44it/s]Extractor Predicting: 130it [01:26,  1.44it/s]Extractor Predicting: 131it [01:27,  1.46it/s]Extractor Predicting: 132it [01:27,  1.45it/s]Extractor Predicting: 133it [01:28,  1.48it/s]Extractor Predicting: 134it [01:29,  1.51it/s]Extractor Predicting: 135it [01:29,  1.50it/s]Extractor Predicting: 136it [01:30,  1.48it/s]Extractor Predicting: 137it [01:31,  1.51it/s]Extractor Predicting: 138it [01:31,  1.50it/s]Extractor Predicting: 139it [01:32,  1.48it/s]Extractor Predicting: 140it [01:33,  1.50it/s]Extractor Predicting: 141it [01:33,  1.52it/s]Extractor Predicting: 142it [01:34,  1.51it/s]Extractor Predicting: 143it [01:34,  1.53it/s]Extractor Predicting: 144it [01:35,  1.56it/s]Extractor Predicting: 144it [01:35,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:27,984 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:27,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:27,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:27,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:27,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:25:28,578 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:25:28,579 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:25:29,161 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:25:30,210 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:25:30,210 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:32,490 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:32,495 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:32,495 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:32,495 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:25:32,495 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:25:32,803 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:25:32,804 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:25:33,067 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:25:33,209 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:25:33,209 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.41443538998835855,
  "recall": 0.10238711532930687,
  "score": 0.16420664206642066,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.60it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:07,  1.58it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.54it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:11,  1.53it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.59it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:14,  1.56it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:19,  1.57it/s]Extractor Predicting: 31it [00:19,  1.56it/s]Extractor Predicting: 32it [00:20,  1.61it/s]Extractor Predicting: 33it [00:21,  1.57it/s]Extractor Predicting: 34it [00:21,  1.55it/s]Extractor Predicting: 35it [00:22,  1.55it/s]Extractor Predicting: 36it [00:23,  1.56it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:24,  1.58it/s]Extractor Predicting: 39it [00:25,  1.45it/s]Extractor Predicting: 40it [00:25,  1.47it/s]Extractor Predicting: 41it [00:26,  1.50it/s]Extractor Predicting: 42it [00:27,  1.54it/s]Extractor Predicting: 43it [00:27,  1.51it/s]Extractor Predicting: 44it [00:28,  1.52it/s]Extractor Predicting: 45it [00:29,  1.51it/s]Extractor Predicting: 46it [00:29,  1.50it/s]Extractor Predicting: 47it [00:30,  1.54it/s]Extractor Predicting: 48it [00:31,  1.52it/s]Extractor Predicting: 49it [00:31,  1.55it/s]Extractor Predicting: 50it [00:32,  1.54it/s]Extractor Predicting: 51it [00:33,  1.53it/s]Extractor Predicting: 52it [00:33,  1.51it/s]Extractor Predicting: 53it [00:34,  1.53it/s]Extractor Predicting: 54it [00:35,  1.50it/s]Extractor Predicting: 55it [00:35,  1.55it/s]Extractor Predicting: 56it [00:36,  1.53it/s]Extractor Predicting: 57it [00:37,  1.51it/s]Extractor Predicting: 58it [00:37,  1.49it/s]Extractor Predicting: 59it [00:38,  1.47it/s]Extractor Predicting: 60it [00:39,  1.50it/s]Extractor Predicting: 61it [00:39,  1.48it/s]Extractor Predicting: 62it [00:40,  1.49it/s]Extractor Predicting: 63it [00:41,  1.51it/s]Extractor Predicting: 64it [00:41,  1.50it/s]Extractor Predicting: 65it [00:42,  1.55it/s]Extractor Predicting: 66it [00:43,  1.52it/s]Extractor Predicting: 67it [00:43,  1.50it/s]Extractor Predicting: 68it [00:44,  1.49it/s]Extractor Predicting: 69it [00:45,  1.49it/s]Extractor Predicting: 70it [00:45,  1.46it/s]Extractor Predicting: 71it [00:46,  1.48it/s]Extractor Predicting: 72it [00:47,  1.48it/s]Extractor Predicting: 73it [00:47,  1.46it/s]Extractor Predicting: 74it [00:48,  1.48it/s]Extractor Predicting: 75it [00:49,  1.49it/s]Extractor Predicting: 76it [00:49,  1.50it/s]Extractor Predicting: 77it [00:50,  1.50it/s]Extractor Predicting: 78it [00:51,  1.52it/s]Extractor Predicting: 79it [00:51,  1.52it/s]Extractor Predicting: 80it [00:52,  1.56it/s]Extractor Predicting: 81it [00:52,  1.57it/s]Extractor Predicting: 82it [00:53,  1.56it/s]Extractor Predicting: 83it [00:54,  1.55it/s]Extractor Predicting: 84it [00:54,  1.54it/s]Extractor Predicting: 85it [00:55,  1.51it/s]Extractor Predicting: 86it [00:56,  1.46it/s]Extractor Predicting: 87it [00:57,  1.44it/s]Extractor Predicting: 88it [00:57,  1.47it/s]Extractor Predicting: 89it [00:58,  1.45it/s]Extractor Predicting: 90it [00:59,  1.43it/s]Extractor Predicting: 91it [00:59,  1.42it/s]Extractor Predicting: 92it [01:00,  1.44it/s]Extractor Predicting: 93it [01:01,  1.45it/s]Extractor Predicting: 94it [01:01,  1.47it/s]Extractor Predicting: 95it [01:02,  1.47it/s]Extractor Predicting: 96it [01:03,  1.47it/s]Extractor Predicting: 97it [01:03,  1.45it/s]Extractor Predicting: 98it [01:04,  1.42it/s]Extractor Predicting: 99it [01:05,  1.43it/s]Extractor Predicting: 100it [01:06,  1.45it/s]Extractor Predicting: 101it [01:06,  1.46it/s]Extractor Predicting: 102it [01:07,  1.47it/s]Extractor Predicting: 103it [01:08,  1.45it/s]Extractor Predicting: 104it [01:08,  1.44it/s]Extractor Predicting: 105it [01:09,  1.46it/s]Extractor Predicting: 106it [01:10,  1.45it/s]Extractor Predicting: 107it [01:10,  1.46it/s]Extractor Predicting: 108it [01:11,  1.48it/s]Extractor Predicting: 109it [01:12,  1.47it/s]Extractor Predicting: 110it [01:12,  1.45it/s]Extractor Predicting: 111it [01:13,  1.47it/s]Extractor Predicting: 112it [01:14,  1.46it/s]Extractor Predicting: 113it [01:14,  1.46it/s]Extractor Predicting: 114it [01:15,  1.48it/s]Extractor Predicting: 115it [01:16,  1.47it/s]Extractor Predicting: 116it [01:17,  1.43it/s]Extractor Predicting: 117it [01:17,  1.43it/s]Extractor Predicting: 118it [01:18,  1.45it/s]Extractor Predicting: 119it [01:19,  1.44it/s]Extractor Predicting: 120it [01:19,  1.44it/s]Extractor Predicting: 121it [01:20,  1.46it/s]Extractor Predicting: 122it [01:21,  1.49it/s]Extractor Predicting: 123it [01:21,  1.49it/s]Extractor Predicting: 124it [01:22,  1.49it/s]Extractor Predicting: 125it [01:23,  1.49it/s]Extractor Predicting: 126it [01:23,  1.55it/s]Extractor Predicting: 127it [01:24,  1.55it/s]Extractor Predicting: 128it [01:25,  1.51it/s]Extractor Predicting: 129it [01:25,  1.55it/s]Extractor Predicting: 130it [01:26,  1.54it/s]Extractor Predicting: 131it [01:26,  1.53it/s]Extractor Predicting: 132it [01:27,  1.53it/s]Extractor Predicting: 133it [01:28,  1.57it/s]Extractor Predicting: 134it [01:28,  1.59it/s]Extractor Predicting: 135it [01:29,  1.54it/s]Extractor Predicting: 136it [01:30,  1.54it/s]Extractor Predicting: 137it [01:30,  1.56it/s]Extractor Predicting: 138it [01:31,  1.60it/s]Extractor Predicting: 139it [01:32,  1.58it/s]Extractor Predicting: 140it [01:32,  1.56it/s]Extractor Predicting: 141it [01:33,  1.54it/s]Extractor Predicting: 142it [01:34,  1.56it/s]Extractor Predicting: 143it [01:34,  1.53it/s]Extractor Predicting: 144it [01:35,  1.51it/s]Extractor Predicting: 145it [01:36,  1.51it/s]Extractor Predicting: 146it [01:36,  1.53it/s]Extractor Predicting: 147it [01:37,  1.58it/s]Extractor Predicting: 148it [01:37,  1.58it/s]Extractor Predicting: 149it [01:38,  1.59it/s]Extractor Predicting: 150it [01:39,  1.63it/s]Extractor Predicting: 151it [01:39,  1.62it/s]Extractor Predicting: 152it [01:40,  1.62it/s]Extractor Predicting: 153it [01:40,  1.62it/s]Extractor Predicting: 154it [01:41,  1.65it/s]Extractor Predicting: 155it [01:42,  1.63it/s]Extractor Predicting: 156it [01:42,  1.64it/s]Extractor Predicting: 157it [01:43,  1.67it/s]Extractor Predicting: 158it [01:43,  1.65it/s]Extractor Predicting: 159it [01:44,  1.70it/s]Extractor Predicting: 160it [01:45,  1.75it/s]Extractor Predicting: 161it [01:45,  1.68it/s]Extractor Predicting: 162it [01:46,  1.64it/s]Extractor Predicting: 163it [01:46,  1.63it/s]Extractor Predicting: 164it [01:47,  1.63it/s]Extractor Predicting: 165it [01:48,  1.67it/s]Extractor Predicting: 166it [01:48,  1.68it/s]Extractor Predicting: 167it [01:49,  1.67it/s]Extractor Predicting: 168it [01:49,  1.64it/s]Extractor Predicting: 169it [01:50,  1.48it/s]Extractor Predicting: 170it [01:51,  1.52it/s]Extractor Predicting: 171it [01:51,  1.59it/s]Extractor Predicting: 172it [01:52,  1.61it/s]Extractor Predicting: 173it [01:53,  1.63it/s]Extractor Predicting: 174it [01:53,  1.56it/s]Extractor Predicting: 175it [01:54,  1.56it/s]Extractor Predicting: 176it [01:55,  1.54it/s]Extractor Predicting: 177it [01:55,  1.52it/s]Extractor Predicting: 178it [01:56,  1.49it/s]Extractor Predicting: 179it [01:57,  1.48it/s]Extractor Predicting: 180it [01:57,  1.50it/s]Extractor Predicting: 181it [01:58,  1.49it/s]Extractor Predicting: 182it [01:59,  1.50it/s]Extractor Predicting: 183it [01:59,  1.51it/s]Extractor Predicting: 184it [02:00,  1.51it/s]Extractor Predicting: 185it [02:01,  1.50it/s]Extractor Predicting: 186it [02:01,  1.52it/s]Extractor Predicting: 187it [02:02,  1.51it/s]Extractor Predicting: 188it [02:03,  1.49it/s]Extractor Predicting: 189it [02:03,  1.50it/s]Extractor Predicting: 190it [02:04,  1.49it/s]Extractor Predicting: 191it [02:05,  1.46it/s]Extractor Predicting: 192it [02:05,  1.46it/s]Extractor Predicting: 193it [02:06,  1.45it/s]Extractor Predicting: 194it [02:07,  1.44it/s]Extractor Predicting: 195it [02:08,  1.45it/s]Extractor Predicting: 196it [02:08,  1.49it/s]Extractor Predicting: 197it [02:09,  1.50it/s]Extractor Predicting: 198it [02:09,  1.50it/s]Extractor Predicting: 199it [02:10,  1.50it/s]Extractor Predicting: 200it [02:11,  1.47it/s]Extractor Predicting: 201it [02:12,  1.46it/s]Extractor Predicting: 202it [02:12,  1.43it/s]Extractor Predicting: 203it [02:13,  1.41it/s]Extractor Predicting: 204it [02:14,  1.40it/s]Extractor Predicting: 205it [02:14,  1.40it/s]Extractor Predicting: 206it [02:15,  1.41it/s]Extractor Predicting: 207it [02:16,  1.43it/s]Extractor Predicting: 208it [02:17,  1.41it/s]Extractor Predicting: 209it [02:17,  1.37it/s]Extractor Predicting: 210it [02:18,  1.38it/s]Extractor Predicting: 211it [02:19,  1.37it/s]Extractor Predicting: 212it [02:19,  1.40it/s]Extractor Predicting: 213it [02:20,  1.40it/s]Extractor Predicting: 214it [02:21,  1.42it/s]Extractor Predicting: 215it [02:22,  1.40it/s]Extractor Predicting: 216it [02:22,  1.40it/s]Extractor Predicting: 217it [02:23,  1.41it/s]Extractor Predicting: 218it [02:24,  1.40it/s]Extractor Predicting: 219it [02:24,  1.39it/s]Extractor Predicting: 220it [02:25,  1.37it/s]Extractor Predicting: 221it [02:26,  1.36it/s]Extractor Predicting: 222it [02:27,  1.36it/s]Extractor Predicting: 223it [02:27,  1.39it/s]Extractor Predicting: 224it [02:28,  1.41it/s]Extractor Predicting: 225it [02:29,  1.41it/s]Extractor Predicting: 226it [02:30,  1.39it/s]Extractor Predicting: 227it [02:30,  1.41it/s]Extractor Predicting: 228it [02:31,  1.41it/s]Extractor Predicting: 229it [02:32,  1.43it/s]Extractor Predicting: 230it [02:32,  1.45it/s]Extractor Predicting: 231it [02:33,  1.48it/s]Extractor Predicting: 232it [02:34,  1.52it/s]Extractor Predicting: 233it [02:34,  1.53it/s]Extractor Predicting: 234it [02:35,  1.48it/s]Extractor Predicting: 235it [02:36,  1.50it/s]Extractor Predicting: 236it [02:36,  1.50it/s]Extractor Predicting: 237it [02:37,  1.50it/s]Extractor Predicting: 238it [02:38,  1.51it/s]Extractor Predicting: 239it [02:38,  1.48it/s]Extractor Predicting: 240it [02:39,  1.52it/s]Extractor Predicting: 241it [02:39,  1.53it/s]Extractor Predicting: 242it [02:40,  1.56it/s]Extractor Predicting: 243it [02:41,  1.55it/s]Extractor Predicting: 244it [02:41,  1.60it/s]Extractor Predicting: 245it [02:42,  1.56it/s]Extractor Predicting: 246it [02:43,  1.53it/s]Extractor Predicting: 247it [02:43,  1.50it/s]Extractor Predicting: 248it [02:44,  1.49it/s]Extractor Predicting: 249it [02:45,  1.51it/s]Extractor Predicting: 250it [02:45,  1.51it/s]Extractor Predicting: 251it [02:46,  1.53it/s]Extractor Predicting: 252it [02:47,  1.56it/s]Extractor Predicting: 253it [02:47,  1.54it/s]Extractor Predicting: 254it [02:48,  1.50it/s]Extractor Predicting: 255it [02:49,  1.51it/s]Extractor Predicting: 256it [02:49,  1.50it/s]Extractor Predicting: 257it [02:50,  1.46it/s]Extractor Predicting: 258it [02:51,  1.46it/s]Extractor Predicting: 259it [02:51,  1.47it/s]Extractor Predicting: 260it [02:52,  1.46it/s]Extractor Predicting: 261it [02:53,  1.46it/s]Extractor Predicting: 262it [02:53,  1.48it/s]Extractor Predicting: 263it [02:54,  1.50it/s]Extractor Predicting: 264it [02:55,  1.48it/s]Extractor Predicting: 265it [02:55,  1.48it/s]Extractor Predicting: 266it [02:56,  1.46it/s]Extractor Predicting: 267it [02:57,  1.45it/s]Extractor Predicting: 268it [02:58,  1.44it/s]Extractor Predicting: 269it [02:58,  1.43it/s]Extractor Predicting: 270it [02:59,  1.44it/s]Extractor Predicting: 271it [03:00,  1.45it/s]Extractor Predicting: 272it [03:00,  1.48it/s]Extractor Predicting: 273it [03:01,  1.43it/s]Extractor Predicting: 274it [03:02,  1.42it/s]Extractor Predicting: 275it [03:02,  1.43it/s]Extractor Predicting: 276it [03:03,  1.43it/s]Extractor Predicting: 277it [03:04,  1.44it/s]Extractor Predicting: 278it [03:05,  1.45it/s]Extractor Predicting: 279it [03:05,  1.43it/s]Extractor Predicting: 280it [03:06,  1.29it/s]Extractor Predicting: 281it [03:07,  1.36it/s]Extractor Predicting: 282it [03:08,  1.37it/s]Extractor Predicting: 283it [03:08,  1.37it/s]Extractor Predicting: 284it [03:09,  1.44it/s]Extractor Predicting: 285it [03:10,  1.44it/s]Extractor Predicting: 286it [03:10,  1.47it/s]Extractor Predicting: 287it [03:11,  1.47it/s]Extractor Predicting: 288it [03:12,  1.46it/s]Extractor Predicting: 289it [03:12,  1.46it/s]Extractor Predicting: 290it [03:13,  1.45it/s]Extractor Predicting: 291it [03:14,  1.41it/s]Extractor Predicting: 292it [03:14,  1.41it/s]Extractor Predicting: 293it [03:15,  1.43it/s]Extractor Predicting: 294it [03:16,  1.40it/s]Extractor Predicting: 295it [03:17,  1.43it/s]Extractor Predicting: 296it [03:17,  1.42it/s]Extractor Predicting: 297it [03:18,  1.44it/s]Extractor Predicting: 298it [03:19,  1.43it/s]Extractor Predicting: 299it [03:19,  1.41it/s]Extractor Predicting: 300it [03:20,  1.44it/s]Extractor Predicting: 301it [03:21,  1.43it/s]Extractor Predicting: 302it [03:21,  1.46it/s]Extractor Predicting: 303it [03:22,  1.42it/s]Extractor Predicting: 304it [03:23,  1.41it/s]Extractor Predicting: 305it [03:24,  1.42it/s]Extractor Predicting: 306it [03:24,  1.45it/s]Extractor Predicting: 307it [03:25,  1.44it/s]Extractor Predicting: 308it [03:26,  1.46it/s]Extractor Predicting: 309it [03:26,  1.46it/s]Extractor Predicting: 310it [03:27,  1.48it/s]Extractor Predicting: 311it [03:28,  1.47it/s]Extractor Predicting: 312it [03:28,  1.51it/s]Extractor Predicting: 313it [03:29,  1.55it/s]Extractor Predicting: 314it [03:29,  1.57it/s]Extractor Predicting: 315it [03:30,  1.58it/s]Extractor Predicting: 316it [03:31,  1.54it/s]Extractor Predicting: 317it [03:31,  1.51it/s]Extractor Predicting: 318it [03:32,  1.50it/s]Extractor Predicting: 319it [03:33,  1.49it/s]Extractor Predicting: 320it [03:34,  1.48it/s]Extractor Predicting: 321it [03:34,  1.48it/s]Extractor Predicting: 322it [03:35,  1.49it/s]Extractor Predicting: 323it [03:36,  1.45it/s]Extractor Predicting: 324it [03:36,  1.45it/s]Extractor Predicting: 325it [03:37,  1.46it/s]Extractor Predicting: 326it [03:38,  1.47it/s]Extractor Predicting: 327it [03:38,  1.50it/s]Extractor Predicting: 328it [03:39,  1.48it/s]Extractor Predicting: 329it [03:40,  1.49it/s]Extractor Predicting: 330it [03:40,  1.49it/s]Extractor Predicting: 331it [03:41,  1.48it/s]Extractor Predicting: 332it [03:42,  1.48it/s]Extractor Predicting: 333it [03:42,  1.50it/s]Extractor Predicting: 334it [03:43,  1.50it/s]Extractor Predicting: 335it [03:44,  1.51it/s]Extractor Predicting: 336it [03:44,  1.48it/s]Extractor Predicting: 337it [03:45,  1.49it/s]Extractor Predicting: 338it [03:46,  1.50it/s]Extractor Predicting: 339it [03:46,  1.53it/s]Extractor Predicting: 340it [03:47,  1.54it/s]Extractor Predicting: 341it [03:48,  1.53it/s]Extractor Predicting: 342it [03:48,  1.50it/s]Extractor Predicting: 343it [03:49,  1.49it/s]Extractor Predicting: 344it [03:50,  1.52it/s]Extractor Predicting: 345it [03:50,  1.48it/s]Extractor Predicting: 346it [03:51,  1.48it/s]Extractor Predicting: 347it [03:52,  1.48it/s]Extractor Predicting: 348it [03:52,  1.48it/s]Extractor Predicting: 349it [03:53,  1.50it/s]Extractor Predicting: 350it [03:54,  1.51it/s]Extractor Predicting: 351it [03:54,  1.52it/s]Extractor Predicting: 352it [03:55,  1.49it/s]Extractor Predicting: 353it [03:56,  1.50it/s]Extractor Predicting: 354it [03:56,  1.53it/s]Extractor Predicting: 355it [03:57,  1.56it/s]Extractor Predicting: 356it [03:58,  1.52it/s]Extractor Predicting: 357it [03:58,  1.52it/s]Extractor Predicting: 358it [03:59,  1.53it/s]Extractor Predicting: 359it [04:00,  1.51it/s]Extractor Predicting: 360it [04:00,  1.51it/s]Extractor Predicting: 361it [04:01,  1.52it/s]Extractor Predicting: 362it [04:01,  1.53it/s]Extractor Predicting: 363it [04:02,  1.54it/s]Extractor Predicting: 364it [04:03,  1.54it/s]Extractor Predicting: 365it [04:03,  1.56it/s]Extractor Predicting: 366it [04:04,  1.54it/s]Extractor Predicting: 367it [04:05,  1.50it/s]Extractor Predicting: 368it [04:05,  1.50it/s]Extractor Predicting: 369it [04:06,  1.50it/s]Extractor Predicting: 370it [04:07,  1.55it/s]Extractor Predicting: 371it [04:07,  1.56it/s]Extractor Predicting: 372it [04:08,  1.55it/s]Extractor Predicting: 373it [04:09,  1.52it/s]Extractor Predicting: 374it [04:09,  1.53it/s]Extractor Predicting: 375it [04:10,  1.54it/s]Extractor Predicting: 376it [04:11,  1.38it/s]Extractor Predicting: 377it [04:11,  1.43it/s]Extractor Predicting: 378it [04:12,  1.49it/s]Extractor Predicting: 379it [04:13,  1.48it/s]Extractor Predicting: 380it [04:13,  1.50it/s]Extractor Predicting: 381it [04:14,  1.52it/s]Extractor Predicting: 382it [04:15,  1.52it/s]Extractor Predicting: 383it [04:15,  1.56it/s]Extractor Predicting: 384it [04:16,  1.59it/s]Extractor Predicting: 385it [04:17,  1.56it/s]Extractor Predicting: 386it [04:17,  1.57it/s]Extractor Predicting: 387it [04:18,  1.56it/s]Extractor Predicting: 388it [04:19,  1.54it/s]Extractor Predicting: 389it [04:19,  1.54it/s]Extractor Predicting: 390it [04:20,  1.53it/s]Extractor Predicting: 391it [04:21,  1.52it/s]Extractor Predicting: 392it [04:21,  1.53it/s]Extractor Predicting: 393it [04:22,  1.55it/s]Extractor Predicting: 394it [04:23,  1.49it/s]Extractor Predicting: 395it [04:23,  1.44it/s]Extractor Predicting: 396it [04:24,  1.43it/s]Extractor Predicting: 397it [04:25,  1.43it/s]Extractor Predicting: 398it [04:25,  1.43it/s]Extractor Predicting: 399it [04:26,  1.42it/s]Extractor Predicting: 400it [04:27,  1.46it/s]Extractor Predicting: 401it [04:27,  1.43it/s]Extractor Predicting: 402it [04:28,  1.42it/s]Extractor Predicting: 403it [04:29,  1.43it/s]Extractor Predicting: 404it [04:30,  1.38it/s]Extractor Predicting: 405it [04:30,  1.39it/s]Extractor Predicting: 406it [04:31,  1.39it/s]Extractor Predicting: 407it [04:32,  1.38it/s]Extractor Predicting: 408it [04:33,  1.41it/s]Extractor Predicting: 409it [04:33,  1.42it/s]Extractor Predicting: 410it [04:34,  1.40it/s]Extractor Predicting: 411it [04:35,  1.41it/s]Extractor Predicting: 412it [04:35,  1.42it/s]Extractor Predicting: 413it [04:36,  1.44it/s]Extractor Predicting: 414it [04:37,  1.46it/s]Extractor Predicting: 415it [04:37,  1.50it/s]Extractor Predicting: 416it [04:38,  1.48it/s]Extractor Predicting: 417it [04:39,  1.49it/s]Extractor Predicting: 418it [04:39,  1.47it/s]Extractor Predicting: 419it [04:40,  1.42it/s]Extractor Predicting: 420it [04:41,  1.43it/s]Extractor Predicting: 421it [04:41,  1.52it/s]Extractor Predicting: 421it [04:41,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:23,987 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:23,992 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:23,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:23,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:23,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:30:24,603 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:30:24,604 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:30:25,179 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:30:26,239 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:30:26,239 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:29,082 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:29,084 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:29,084 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:29,084 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:29,085 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:30:29,714 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:30:29,716 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:30:30,281 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:30:30,434 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:30:30,434 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2892768079800499,
  "recall": 0.06894502228826152,
  "score": 0.11135109191264699,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.37it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:02,  1.40it/s]Extractor Predicting: 4it [00:02,  1.39it/s]Extractor Predicting: 5it [00:03,  1.43it/s]Extractor Predicting: 6it [00:04,  1.42it/s]Extractor Predicting: 7it [00:04,  1.40it/s]Extractor Predicting: 8it [00:05,  1.42it/s]Extractor Predicting: 9it [00:05,  1.84it/s]Extractor Predicting: 9it [00:05,  1.54it/s]
[INFO|configuration_utils.py:515] 2023-08-28 14:30:36,727 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:30:36,728 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:30:36,734 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:30:36,735 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 14:30:36,742 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:30:39,930 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 14:30:39,930 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 14:30:39,945 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:30:39,946 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:30:39,960 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:39,964 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:39,964 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:39,964 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:39,964 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:39,964 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:39,964 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.28125,
  "recall": 0.022222222222222223,
  "score": 0.04118993135011442,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 14:30:40,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:40,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:41,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:42,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:43,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:43,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:44,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:45,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:45,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:46,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:47,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:48,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:48,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:49,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:50,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:50,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:51,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:52,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:53,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:53,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:54,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:55,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:55,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:56,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:57,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:17<05:38, 17.84s/it][WARNING|generation_utils.py:914] 2023-08-28 14:30:58,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:58,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:59,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:00,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:00,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:01,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:02,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:03,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:03,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:04,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:05,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:05,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:06,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:07,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:08,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:09,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:09,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:10,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:11,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:11,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:12,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:13,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:14,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:34<05:11, 17.30s/it][WARNING|generation_utils.py:914] 2023-08-28 14:31:14,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:15,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:16,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:16,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:17,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:18,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:19,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:19,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:20,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:21,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:21,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:22,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:22,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:23,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:24,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:25,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:25,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:26,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:27,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:28,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:28,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:29,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:49<04:37, 16.31s/it][WARNING|generation_utils.py:914] 2023-08-28 14:31:30,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:30,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:31,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:32,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:33,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:33,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:34,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:35,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:36,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:37,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:37,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:38,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:39,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:40,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:40,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:41,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:42,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:43,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:43,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:44,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:45,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:46,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:06<04:25, 16.58s/it][WARNING|generation_utils.py:914] 2023-08-28 14:31:47,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:48,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:48,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:49,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:50,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:51,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:51,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:52,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:53,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:53,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:54,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:55,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:56,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:56,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:57,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:58,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:58,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:59,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:00,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:01,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:02,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:02,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:03,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:23<04:11, 16.75s/it][WARNING|generation_utils.py:914] 2023-08-28 14:32:04,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:04,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:05,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:06,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:07,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:08,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:08,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:09,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:10,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:11,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:11,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:12,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:13,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:14,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:14,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:15,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:16,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:17,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:17,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:18,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:19,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:39<03:50, 16.50s/it][WARNING|generation_utils.py:914] 2023-08-28 14:32:20,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:20,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:21,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:22,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:23,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:23,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:24,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:25,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:26,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:27,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:27,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:28,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:29,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:30,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:30,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:31,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:32,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:33,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:34,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:34,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:35,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:36,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:37,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:57<03:40, 16.98s/it][WARNING|generation_utils.py:914] 2023-08-28 14:32:38,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:38,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:39,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:40,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:41,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:42,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:43,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:44,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:44,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:45,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:46,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:47,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:48,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:49,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:50,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:51,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:52,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:53,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:54,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:55,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:56,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:57,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:58,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:59,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:00,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:20<03:46, 18.87s/it][WARNING|generation_utils.py:914] 2023-08-28 14:33:01,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:01,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:02,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:03,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:03,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:04,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:05,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:06,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:06,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:07,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:08,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:09,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:10,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:11,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:11,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:12,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:13,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:14,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:15,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:15,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:16,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:17,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:38<03:21, 18.34s/it][WARNING|generation_utils.py:914] 2023-08-28 14:33:18,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:18,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:19,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:20,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:21,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:22,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:22,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:23,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:24,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:25,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:26,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:26,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:27,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:28,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:29,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:30,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:30,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:31,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:32,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:33,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:33,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:35,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:36,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:56<03:05, 18.50s/it][WARNING|generation_utils.py:914] 2023-08-28 14:33:37,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:37,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:38,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:39,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:40,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:41,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:41,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:42,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:43,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:44,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:45,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:45,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:46,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:47,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:48,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:48,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:49,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:50,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:51,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:51,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:52,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:13<02:40, 17.80s/it][WARNING|generation_utils.py:914] 2023-08-28 14:33:53,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:54,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:54,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:55,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:56,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:57,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:58,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:59,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:00,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:00,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:01,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:02,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:03,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:04,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:04,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:05,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:06,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:07,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:07,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:08,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:09,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:30<02:20, 17.62s/it][WARNING|generation_utils.py:914] 2023-08-28 14:34:10,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:11,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:12,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:13,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:13,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:14,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:15,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:16,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:16,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:17,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:18,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:19,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:20,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:21,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:21,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:22,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:23,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:24,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:25,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:25,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:26,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:27,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:47<02:03, 17.64s/it][WARNING|generation_utils.py:914] 2023-08-28 14:34:28,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:28,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:29,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:30,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:30,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:31,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:31,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:32,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:33,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:33,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:34,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:35,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:35,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:36,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:37,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:37,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:38,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:38,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:39,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:40,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:40,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [04:01<01:38, 16.39s/it][WARNING|generation_utils.py:914] 2023-08-28 14:34:41,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:42,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:43,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:43,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:44,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:45,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:46,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:47,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:47,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:48,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:49,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:50,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:50,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:51,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:52,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:53,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:53,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:54,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:55,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:55,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:56,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:57,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:58,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:18<01:23, 16.64s/it][WARNING|generation_utils.py:914] 2023-08-28 14:34:58,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:59,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:00,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:00,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:01,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:02,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:03,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:03,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:04,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:05,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:06,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:06,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:07,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:08,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:08,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:09,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:10,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:11,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:11,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:12,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:13,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:33<01:04, 16.21s/it][WARNING|generation_utils.py:914] 2023-08-28 14:35:14,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:14,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:15,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:15,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:16,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:17,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:18,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:18,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:19,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:20,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:20,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:21,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:22,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:22,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:23,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:24,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:24,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:25,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:26,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:26,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:27,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:28,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:28,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:29,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:50<00:48, 16.24s/it][WARNING|generation_utils.py:914] 2023-08-28 14:35:30,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:31,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:31,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:32,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:33,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:33,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:34,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:35,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:36,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:36,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:37,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:38,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:39,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:40,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:40,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:41,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:42,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:43,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:43,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:44,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:45,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:46,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:46,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:47,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [05:08<00:33, 16.77s/it][WARNING|generation_utils.py:914] 2023-08-28 14:35:48,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:49,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:49,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:50,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:51,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:52,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:53,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:53,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:54,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:55,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:56,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:56,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:57,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:58,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:59,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:00,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:01,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:01,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:02,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:03,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:04,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:04,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:25<00:16, 16.89s/it][WARNING|generation_utils.py:914] 2023-08-28 14:36:05,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:06,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:07,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:07,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:08,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:09,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:10,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:11,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:12,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:12,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:13,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:14,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:15,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:16,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:17,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:18,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:18,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:19,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:20,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:21,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:22,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:22,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:43<00:00, 17.27s/it]Generating: 100%|██████████| 20/20 [05:43<00:00, 17.18s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:29,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:29,135 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:29,136 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:29,136 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:29,136 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:36:29,437 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:36:29,438 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:36:29,702 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:36:30,780 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:36:30,780 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:32,198 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:32,202 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:32,202 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:32,202 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:36:32,202 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:36:32,516 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:36:32,517 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:36:33,192 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:36:33,346 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:36:33,346 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 321, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 396, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 470, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 520, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 573, 'raw': 736}
{'target': 600, 'success': 597, 'raw': 768}
{'target': 600, 'success': 622, 'raw': 800}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7775, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : genre .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : head of government .', 'success_rate': 0.8877840909090909, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : military branch .', 'success_rate': 0.8650568181818182, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : winner .', 'success_rate': 0.8369565217391305, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8491847826086957, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 300, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 378, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 548, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 598, 'raw': 768}
{'target': 600, 'success': 628, 'raw': 800}
{'prompt': 'Relation : crosses .', 'success_rate': 0.785, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8650568181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8491847826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9122023809523809, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8522727272727273, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9047619047619048, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : participant .', 'success_rate': 0.8165760869565217, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : participating team .', 'success_rate': 0.9047619047619048, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 324, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 431, 'raw': 544}
{'target': 600, 'success': 452, 'raw': 576}
{'target': 600, 'success': 481, 'raw': 608}
{'target': 600, 'success': 509, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : platform .', 'success_rate': 0.8072916666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 512, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 618, 'raw': 768}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.8046875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8764204545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8863636363636364, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/2_ext.jsonl'}}
estimate vocab size: 13864
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13964, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.64it/s]Extractor Estimating: 2it [00:01,  1.56it/s]Extractor Estimating: 3it [00:01,  1.56it/s]Extractor Estimating: 4it [00:02,  1.63it/s]Extractor Estimating: 5it [00:03,  1.65it/s]Extractor Estimating: 6it [00:03,  1.64it/s]Extractor Estimating: 7it [00:04,  1.65it/s]Extractor Estimating: 8it [00:04,  1.65it/s]Extractor Estimating: 9it [00:05,  1.65it/s]Extractor Estimating: 10it [00:06,  1.59it/s]Extractor Estimating: 11it [00:06,  1.57it/s]Extractor Estimating: 12it [00:07,  1.54it/s]Extractor Estimating: 13it [00:08,  1.58it/s]Extractor Estimating: 14it [00:08,  1.57it/s]Extractor Estimating: 15it [00:09,  1.58it/s]Extractor Estimating: 16it [00:10,  1.58it/s]Extractor Estimating: 17it [00:10,  1.58it/s]Extractor Estimating: 18it [00:11,  1.57it/s]Extractor Estimating: 19it [00:12,  1.48it/s]Extractor Estimating: 20it [00:12,  1.55it/s]Extractor Estimating: 21it [00:13,  1.60it/s]Extractor Estimating: 22it [00:13,  1.63it/s]Extractor Estimating: 23it [00:14,  1.59it/s]Extractor Estimating: 24it [00:15,  1.57it/s]Extractor Estimating: 25it [00:15,  1.57it/s]Extractor Estimating: 26it [00:16,  1.56it/s]Extractor Estimating: 27it [00:17,  1.53it/s]Extractor Estimating: 28it [00:17,  1.53it/s]Extractor Estimating: 29it [00:18,  1.51it/s]Extractor Estimating: 30it [00:19,  1.48it/s]Extractor Estimating: 31it [00:19,  1.44it/s]Extractor Estimating: 32it [00:20,  1.42it/s]Extractor Estimating: 33it [00:21,  1.47it/s]Extractor Estimating: 34it [00:21,  1.42it/s]Extractor Estimating: 35it [00:22,  1.41it/s]Extractor Estimating: 36it [00:23,  1.43it/s]Extractor Estimating: 37it [00:24,  1.48it/s]Extractor Estimating: 38it [00:24,  1.46it/s]Extractor Estimating: 39it [00:25,  1.48it/s]Extractor Estimating: 40it [00:26,  1.49it/s]Extractor Estimating: 41it [00:26,  1.47it/s]Extractor Estimating: 42it [00:27,  1.43it/s]Extractor Estimating: 43it [00:28,  1.47it/s]Extractor Estimating: 44it [00:28,  1.48it/s]Extractor Estimating: 45it [00:29,  1.48it/s]Extractor Estimating: 46it [00:30,  1.49it/s]Extractor Estimating: 47it [00:30,  1.48it/s]Extractor Estimating: 48it [00:31,  1.45it/s]Extractor Estimating: 49it [00:32,  1.41it/s]Extractor Estimating: 50it [00:32,  1.44it/s]Extractor Estimating: 51it [00:33,  1.58it/s]Extractor Estimating: 52it [00:34,  1.59it/s]Extractor Estimating: 53it [00:34,  1.60it/s]Extractor Estimating: 54it [00:35,  1.63it/s]Extractor Estimating: 55it [00:35,  1.65it/s]Extractor Estimating: 56it [00:36,  1.69it/s]Extractor Estimating: 57it [00:36,  1.69it/s]Extractor Estimating: 58it [00:37,  1.64it/s]Extractor Estimating: 59it [00:38,  1.63it/s]Extractor Estimating: 60it [00:38,  1.62it/s]Extractor Estimating: 61it [00:39,  1.65it/s]Extractor Estimating: 62it [00:40,  1.70it/s]Extractor Estimating: 63it [00:40,  1.74it/s]Extractor Estimating: 64it [00:41,  1.73it/s]Extractor Estimating: 65it [00:41,  1.71it/s]Extractor Estimating: 66it [00:42,  1.66it/s]Extractor Estimating: 67it [00:42,  1.65it/s]Extractor Estimating: 68it [00:43,  1.71it/s]Extractor Estimating: 69it [00:44,  1.68it/s]Extractor Estimating: 70it [00:44,  1.67it/s]Extractor Estimating: 71it [00:45,  1.66it/s]Extractor Estimating: 72it [00:45,  1.70it/s]Extractor Estimating: 73it [00:46,  1.69it/s]Extractor Estimating: 74it [00:47,  1.67it/s]Extractor Estimating: 75it [00:47,  1.70it/s]Extractor Estimating: 76it [00:48,  1.62it/s]Extractor Estimating: 77it [00:49,  1.56it/s]Extractor Estimating: 78it [00:49,  1.49it/s]Extractor Estimating: 79it [00:50,  1.51it/s]Extractor Estimating: 80it [00:51,  1.50it/s]Extractor Estimating: 81it [00:51,  1.39it/s]Extractor Estimating: 82it [00:52,  1.45it/s]Extractor Estimating: 83it [00:53,  1.49it/s]Extractor Estimating: 84it [00:53,  1.47it/s]Extractor Estimating: 85it [00:54,  1.50it/s]Extractor Estimating: 86it [00:55,  1.54it/s]Extractor Estimating: 87it [00:55,  1.43it/s]Extractor Estimating: 88it [00:56,  1.43it/s]Extractor Estimating: 89it [00:57,  1.45it/s]Extractor Estimating: 90it [00:58,  1.44it/s]Extractor Estimating: 91it [00:58,  1.50it/s]Extractor Estimating: 92it [00:59,  1.47it/s]Extractor Estimating: 93it [01:00,  1.37it/s]Extractor Estimating: 94it [01:00,  1.42it/s]Extractor Estimating: 95it [01:01,  1.43it/s]Extractor Estimating: 96it [01:02,  1.36it/s]Extractor Estimating: 97it [01:02,  1.43it/s]Extractor Estimating: 98it [01:03,  1.46it/s]Extractor Estimating: 99it [01:04,  1.45it/s]Extractor Estimating: 100it [01:05,  1.44it/s]Extractor Estimating: 101it [01:05,  1.50it/s]Extractor Estimating: 102it [01:06,  1.50it/s]Extractor Estimating: 103it [01:06,  1.59it/s]Extractor Estimating: 104it [01:07,  1.57it/s]Extractor Estimating: 105it [01:08,  1.55it/s]Extractor Estimating: 106it [01:08,  1.54it/s]Extractor Estimating: 107it [01:09,  1.54it/s]Extractor Estimating: 108it [01:10,  1.62it/s]Extractor Estimating: 109it [01:10,  1.60it/s]Extractor Estimating: 110it [01:11,  1.61it/s]Extractor Estimating: 111it [01:11,  1.66it/s]Extractor Estimating: 112it [01:12,  1.62it/s]Extractor Estimating: 113it [01:13,  1.58it/s]Extractor Estimating: 114it [01:13,  1.57it/s]Extractor Estimating: 115it [01:14,  1.58it/s]Extractor Estimating: 116it [01:14,  1.64it/s]Extractor Estimating: 117it [01:15,  1.63it/s]Extractor Estimating: 118it [01:16,  1.55it/s]Extractor Estimating: 119it [01:16,  1.61it/s]Extractor Estimating: 120it [01:17,  1.59it/s]Extractor Estimating: 121it [01:18,  1.57it/s]Extractor Estimating: 122it [01:18,  1.61it/s]Extractor Estimating: 123it [01:19,  1.57it/s]Extractor Estimating: 124it [01:20,  1.59it/s]Extractor Estimating: 125it [01:20,  1.59it/s]Extractor Estimating: 126it [01:21,  1.53it/s]Extractor Estimating: 127it [01:22,  1.49it/s]Extractor Estimating: 128it [01:22,  1.49it/s]Extractor Estimating: 129it [01:23,  1.52it/s]Extractor Estimating: 130it [01:24,  1.42it/s]Extractor Estimating: 131it [01:24,  1.44it/s]Extractor Estimating: 132it [01:25,  1.41it/s]Extractor Estimating: 133it [01:26,  1.40it/s]Extractor Estimating: 134it [01:27,  1.39it/s]Extractor Estimating: 135it [01:27,  1.30it/s]Extractor Estimating: 136it [01:28,  1.35it/s]Extractor Estimating: 137it [01:29,  1.42it/s]Extractor Estimating: 138it [01:29,  1.47it/s]Extractor Estimating: 139it [01:30,  1.48it/s]Extractor Estimating: 140it [01:31,  1.49it/s]Extractor Estimating: 141it [01:31,  1.46it/s]Extractor Estimating: 142it [01:32,  1.42it/s]Extractor Estimating: 143it [01:33,  1.41it/s]Extractor Estimating: 144it [01:34,  1.41it/s]Extractor Estimating: 145it [01:34,  1.42it/s]Extractor Estimating: 146it [01:35,  1.45it/s]Extractor Estimating: 147it [01:36,  1.45it/s]Extractor Estimating: 148it [01:36,  1.40it/s]Extractor Estimating: 149it [01:37,  1.43it/s]Extractor Estimating: 150it [01:38,  1.43it/s]Extractor Estimating: 151it [01:38,  1.48it/s]Extractor Estimating: 152it [01:39,  1.52it/s]Extractor Estimating: 153it [01:40,  1.58it/s]Extractor Estimating: 154it [01:40,  1.54it/s]Extractor Estimating: 155it [01:41,  1.58it/s]Extractor Estimating: 156it [01:42,  1.55it/s]Extractor Estimating: 157it [01:42,  1.57it/s]Extractor Estimating: 158it [01:43,  1.63it/s]Extractor Estimating: 159it [01:43,  1.60it/s]Extractor Estimating: 160it [01:44,  1.64it/s]Extractor Estimating: 161it [01:45,  1.60it/s]Extractor Estimating: 162it [01:45,  1.62it/s]Extractor Estimating: 163it [01:46,  1.45it/s]Extractor Estimating: 164it [01:47,  1.53it/s]Extractor Estimating: 165it [01:47,  1.54it/s]Extractor Estimating: 166it [01:48,  1.51it/s]Extractor Estimating: 167it [01:49,  1.57it/s]Extractor Estimating: 168it [01:49,  1.55it/s]Extractor Estimating: 169it [01:50,  1.55it/s]Extractor Estimating: 170it [01:51,  1.51it/s]Extractor Estimating: 171it [01:51,  1.53it/s]Extractor Estimating: 172it [01:52,  1.57it/s]Extractor Estimating: 173it [01:52,  1.60it/s]Extractor Estimating: 174it [01:53,  1.61it/s]Extractor Estimating: 175it [01:54,  1.65it/s]Extractor Estimating: 176it [01:54,  1.64it/s]Extractor Estimating: 177it [01:55,  1.52it/s]Extractor Estimating: 178it [01:56,  1.46it/s]Extractor Estimating: 179it [01:56,  1.48it/s]Extractor Estimating: 180it [01:57,  1.47it/s]Extractor Estimating: 181it [01:58,  1.46it/s]Extractor Estimating: 182it [01:59,  1.41it/s]Extractor Estimating: 183it [01:59,  1.46it/s]Extractor Estimating: 184it [02:00,  1.50it/s]Extractor Estimating: 185it [02:01,  1.46it/s]Extractor Estimating: 186it [02:01,  1.44it/s]Extractor Estimating: 187it [02:02,  1.41it/s]Extractor Estimating: 188it [02:03,  1.39it/s]Extractor Estimating: 189it [02:03,  1.39it/s]Extractor Estimating: 190it [02:04,  1.39it/s]Extractor Estimating: 191it [02:05,  1.39it/s]Extractor Estimating: 192it [02:05,  1.45it/s]Extractor Estimating: 193it [02:06,  1.52it/s]Extractor Estimating: 194it [02:07,  1.50it/s]Extractor Estimating: 195it [02:08,  1.45it/s]Extractor Estimating: 196it [02:08,  1.46it/s]Extractor Estimating: 197it [02:09,  1.46it/s]Extractor Estimating: 198it [02:10,  1.46it/s]Extractor Estimating: 199it [02:10,  1.39it/s]Extractor Estimating: 200it [02:11,  1.41it/s]Extractor Estimating: 201it [02:12,  1.45it/s]Extractor Estimating: 202it [02:12,  1.48it/s]Extractor Estimating: 203it [02:13,  1.52it/s]Extractor Estimating: 204it [02:14,  1.48it/s]Extractor Estimating: 205it [02:14,  1.45it/s]Extractor Estimating: 206it [02:15,  1.44it/s]Extractor Estimating: 207it [02:16,  1.45it/s]Extractor Estimating: 208it [02:16,  1.48it/s]Extractor Estimating: 209it [02:17,  1.53it/s]Extractor Estimating: 210it [02:18,  1.44it/s]Extractor Estimating: 211it [02:18,  1.49it/s]Extractor Estimating: 212it [02:19,  1.45it/s]Extractor Estimating: 213it [02:20,  1.42it/s]Extractor Estimating: 214it [02:21,  1.39it/s]Extractor Estimating: 215it [02:21,  1.45it/s]Extractor Estimating: 216it [02:22,  1.47it/s]Extractor Estimating: 217it [02:23,  1.44it/s]Extractor Estimating: 218it [02:23,  1.45it/s]Extractor Estimating: 219it [02:24,  1.45it/s]Extractor Estimating: 220it [02:25,  1.44it/s]Extractor Estimating: 221it [02:25,  1.41it/s]Extractor Estimating: 222it [02:26,  1.38it/s]Extractor Estimating: 223it [02:27,  1.38it/s]Extractor Estimating: 224it [02:28,  1.38it/s]Extractor Estimating: 225it [02:28,  1.42it/s]Extractor Estimating: 226it [02:29,  1.44it/s]Extractor Estimating: 227it [02:30,  1.42it/s]Extractor Estimating: 228it [02:30,  1.44it/s]Extractor Estimating: 229it [02:31,  1.48it/s]Extractor Estimating: 230it [02:32,  1.46it/s]Extractor Estimating: 231it [02:32,  1.49it/s]Extractor Estimating: 232it [02:33,  1.50it/s]Extractor Estimating: 233it [02:34,  1.52it/s]Extractor Estimating: 234it [02:34,  1.47it/s]Extractor Estimating: 235it [02:35,  1.44it/s]Extractor Estimating: 236it [02:36,  1.45it/s]Extractor Estimating: 237it [02:36,  1.47it/s]Extractor Estimating: 238it [02:37,  1.48it/s]Extractor Estimating: 239it [02:38,  1.46it/s]Extractor Estimating: 240it [02:39,  1.47it/s]Extractor Estimating: 241it [02:39,  1.37it/s]Extractor Estimating: 242it [02:40,  1.39it/s]Extractor Estimating: 243it [02:41,  1.44it/s]Extractor Estimating: 244it [02:41,  1.45it/s]Extractor Estimating: 245it [02:42,  1.42it/s]Extractor Estimating: 246it [02:43,  1.47it/s]Extractor Estimating: 247it [02:43,  1.54it/s]Extractor Estimating: 248it [02:44,  1.53it/s]Extractor Estimating: 249it [02:45,  1.52it/s]Extractor Estimating: 250it [02:45,  1.49it/s]Extractor Estimating: 251it [02:46,  1.49it/s]Extractor Estimating: 252it [02:47,  1.50it/s]Extractor Estimating: 253it [02:47,  1.50it/s]Extractor Estimating: 254it [02:48,  1.53it/s]Extractor Estimating: 255it [02:49,  1.47it/s]Extractor Estimating: 256it [02:49,  1.46it/s]Extractor Estimating: 257it [02:50,  1.48it/s]Extractor Estimating: 258it [02:51,  1.51it/s]Extractor Estimating: 259it [02:51,  1.51it/s]Extractor Estimating: 260it [02:52,  1.49it/s]Extractor Estimating: 261it [02:53,  1.49it/s]Extractor Estimating: 262it [02:53,  1.47it/s]Extractor Estimating: 263it [02:54,  1.49it/s]Extractor Estimating: 264it [02:55,  1.46it/s]Extractor Estimating: 265it [02:55,  1.44it/s]Extractor Estimating: 266it [02:56,  1.48it/s]Extractor Estimating: 267it [02:57,  1.48it/s]Extractor Estimating: 268it [02:57,  1.50it/s]Extractor Estimating: 269it [02:58,  1.50it/s]Extractor Estimating: 270it [02:59,  1.46it/s]Extractor Estimating: 271it [02:59,  1.48it/s]Extractor Estimating: 272it [03:00,  1.50it/s]Extractor Estimating: 273it [03:01,  1.54it/s]Extractor Estimating: 274it [03:01,  1.53it/s]Extractor Estimating: 275it [03:02,  1.49it/s]Extractor Estimating: 276it [03:03,  1.51it/s]Extractor Estimating: 277it [03:04,  1.46it/s]Extractor Estimating: 278it [03:04,  1.49it/s]Extractor Estimating: 279it [03:05,  1.48it/s]Extractor Estimating: 280it [03:05,  1.50it/s]Extractor Estimating: 281it [03:06,  1.56it/s]Extractor Estimating: 282it [03:07,  1.50it/s]Extractor Estimating: 283it [03:07,  1.53it/s]Extractor Estimating: 284it [03:08,  1.50it/s]Extractor Estimating: 285it [03:09,  1.54it/s]Extractor Estimating: 286it [03:09,  1.49it/s]Extractor Estimating: 287it [03:10,  1.50it/s]Extractor Estimating: 288it [03:11,  1.52it/s]Extractor Estimating: 289it [03:11,  1.49it/s]Extractor Estimating: 290it [03:12,  1.48it/s]Extractor Estimating: 291it [03:13,  1.55it/s]Extractor Estimating: 292it [03:13,  1.53it/s]Extractor Estimating: 293it [03:14,  1.50it/s]Extractor Estimating: 294it [03:15,  1.47it/s]Extractor Estimating: 295it [03:15,  1.49it/s]Extractor Estimating: 296it [03:16,  1.46it/s]Extractor Estimating: 297it [03:17,  1.52it/s]Extractor Estimating: 298it [03:17,  1.46it/s]Extractor Estimating: 299it [03:18,  1.52it/s]Extractor Estimating: 300it [03:19,  1.56it/s]Extractor Estimating: 301it [03:19,  1.54it/s]Extractor Estimating: 302it [03:20,  1.51it/s]Extractor Estimating: 303it [03:21,  1.50it/s]Extractor Estimating: 304it [03:21,  1.55it/s]Extractor Estimating: 305it [03:22,  1.58it/s]Extractor Estimating: 306it [03:23,  1.55it/s]Extractor Estimating: 307it [03:23,  1.54it/s]Extractor Estimating: 308it [03:24,  1.51it/s]Extractor Estimating: 309it [03:25,  1.51it/s]Extractor Estimating: 310it [03:25,  1.47it/s]Extractor Estimating: 311it [03:26,  1.47it/s]Extractor Estimating: 312it [03:27,  1.48it/s]Extractor Estimating: 313it [03:27,  1.48it/s]Extractor Estimating: 314it [03:28,  1.45it/s]Extractor Estimating: 315it [03:29,  1.48it/s]Extractor Estimating: 316it [03:29,  1.50it/s]Extractor Estimating: 317it [03:30,  1.49it/s]Extractor Estimating: 318it [03:31,  1.48it/s]Extractor Estimating: 319it [03:31,  1.50it/s]Extractor Estimating: 320it [03:32,  1.50it/s]Extractor Estimating: 321it [03:33,  1.40it/s]Extractor Estimating: 322it [03:33,  1.49it/s]Extractor Estimating: 323it [03:34,  1.50it/s]Extractor Estimating: 324it [03:35,  1.53it/s]Extractor Estimating: 325it [03:35,  1.48it/s]Extractor Estimating: 326it [03:36,  1.59it/s]Extractor Estimating: 327it [03:37,  1.61it/s]Extractor Estimating: 328it [03:37,  1.73it/s]Extractor Estimating: 329it [03:38,  1.74it/s]Extractor Estimating: 330it [03:38,  1.75it/s]Extractor Estimating: 331it [03:39,  1.78it/s]Extractor Estimating: 332it [03:39,  1.80it/s]Extractor Estimating: 333it [03:40,  1.83it/s]Extractor Estimating: 334it [03:40,  1.83it/s]Extractor Estimating: 335it [03:41,  1.96it/s]Extractor Estimating: 336it [03:41,  1.97it/s]Extractor Estimating: 337it [03:42,  2.02it/s]Extractor Estimating: 338it [03:42,  1.97it/s]Extractor Estimating: 339it [03:43,  1.91it/s]Extractor Estimating: 340it [03:43,  1.92it/s]Extractor Estimating: 341it [03:44,  1.93it/s]Extractor Estimating: 342it [03:44,  1.95it/s]Extractor Estimating: 343it [03:45,  1.93it/s]Extractor Estimating: 344it [03:45,  1.83it/s]Extractor Estimating: 345it [03:46,  1.88it/s]Extractor Estimating: 346it [03:47,  1.88it/s]Extractor Estimating: 347it [03:47,  1.93it/s]Extractor Estimating: 348it [03:48,  1.88it/s]Extractor Estimating: 349it [03:48,  1.84it/s]Extractor Estimating: 350it [03:49,  1.81it/s]Extractor Estimating: 351it [03:49,  1.76it/s]Extractor Estimating: 352it [03:50,  1.76it/s]Extractor Estimating: 353it [03:50,  1.77it/s]Extractor Estimating: 354it [03:51,  1.72it/s]Extractor Estimating: 355it [03:52,  1.72it/s]Extractor Estimating: 356it [03:52,  1.58it/s]Extractor Estimating: 357it [03:53,  1.61it/s]Extractor Estimating: 358it [03:54,  1.59it/s]Extractor Estimating: 359it [03:54,  1.63it/s]Extractor Estimating: 360it [03:55,  1.65it/s]Extractor Estimating: 361it [03:55,  1.63it/s]Extractor Estimating: 362it [03:56,  1.64it/s]Extractor Estimating: 363it [03:57,  1.68it/s]Extractor Estimating: 364it [03:57,  1.61it/s]Extractor Estimating: 365it [03:58,  1.62it/s]Extractor Estimating: 366it [03:58,  1.68it/s]Extractor Estimating: 367it [03:59,  1.69it/s]Extractor Estimating: 368it [04:00,  1.70it/s]Extractor Estimating: 369it [04:00,  1.72it/s]Extractor Estimating: 370it [04:01,  1.71it/s]Extractor Estimating: 371it [04:01,  1.65it/s]Extractor Estimating: 372it [04:02,  1.62it/s]Extractor Estimating: 373it [04:03,  1.63it/s]Extractor Estimating: 374it [04:03,  1.66it/s]Extractor Estimating: 375it [04:04,  1.69it/s]Extractor Estimating: 376it [04:04,  1.68it/s]Extractor Estimating: 377it [04:05,  1.68it/s]Extractor Estimating: 378it [04:06,  1.65it/s]Extractor Estimating: 379it [04:06,  1.68it/s]Extractor Estimating: 380it [04:07,  1.68it/s]Extractor Estimating: 381it [04:07,  1.67it/s]Extractor Estimating: 382it [04:08,  1.63it/s]Extractor Estimating: 383it [04:09,  1.67it/s]Extractor Estimating: 384it [04:09,  1.64it/s]Extractor Estimating: 385it [04:10,  1.66it/s]Extractor Estimating: 386it [04:10,  1.66it/s]Extractor Estimating: 387it [04:11,  1.67it/s]Extractor Estimating: 388it [04:12,  1.67it/s]Extractor Estimating: 389it [04:12,  1.68it/s]Extractor Estimating: 390it [04:13,  1.67it/s]Extractor Estimating: 391it [04:13,  1.66it/s]Extractor Estimating: 392it [04:14,  1.66it/s]Extractor Estimating: 393it [04:15,  1.67it/s]Extractor Estimating: 394it [04:15,  1.69it/s]Extractor Estimating: 395it [04:16,  1.71it/s]Extractor Estimating: 396it [04:16,  1.78it/s]Extractor Estimating: 397it [04:17,  1.78it/s]Extractor Estimating: 398it [04:18,  1.65it/s]Extractor Estimating: 399it [04:18,  1.69it/s]Extractor Estimating: 400it [04:19,  1.68it/s]Extractor Estimating: 401it [04:19,  1.66it/s]Extractor Estimating: 402it [04:20,  1.66it/s]Extractor Estimating: 403it [04:21,  1.48it/s]Extractor Estimating: 404it [04:21,  1.52it/s]Extractor Estimating: 405it [04:22,  1.59it/s]Extractor Estimating: 406it [04:23,  1.64it/s]Extractor Estimating: 407it [04:23,  1.68it/s]Extractor Estimating: 408it [04:24,  1.69it/s]Extractor Estimating: 409it [04:24,  1.67it/s]Extractor Estimating: 410it [04:25,  1.68it/s]Extractor Estimating: 411it [04:26,  1.61it/s]Extractor Estimating: 412it [04:26,  1.66it/s]Extractor Estimating: 413it [04:27,  1.69it/s]Extractor Estimating: 414it [04:27,  1.65it/s]Extractor Estimating: 415it [04:28,  1.67it/s]Extractor Estimating: 416it [04:28,  1.69it/s]Extractor Estimating: 417it [04:29,  1.72it/s]Extractor Estimating: 418it [04:30,  1.71it/s]Extractor Estimating: 419it [04:30,  1.64it/s]Extractor Estimating: 420it [04:31,  1.65it/s]Extractor Estimating: 421it [04:31,  1.67it/s]Extractor Estimating: 422it [04:32,  1.64it/s]Extractor Estimating: 423it [04:33,  1.69it/s]Extractor Estimating: 424it [04:33,  1.67it/s]Extractor Estimating: 425it [04:34,  1.62it/s]Extractor Estimating: 426it [04:35,  1.64it/s]Extractor Estimating: 427it [04:35,  1.64it/s]Extractor Estimating: 428it [04:36,  1.55it/s]Extractor Estimating: 429it [04:36,  1.60it/s]Extractor Estimating: 430it [04:37,  1.65it/s]Extractor Estimating: 431it [04:38,  1.66it/s]Extractor Estimating: 432it [04:38,  1.66it/s]Extractor Estimating: 433it [04:39,  1.60it/s]Extractor Estimating: 434it [04:39,  1.65it/s]Extractor Estimating: 435it [04:40,  1.67it/s]Extractor Estimating: 436it [04:41,  1.66it/s]Extractor Estimating: 437it [04:41,  1.73it/s]Extractor Estimating: 438it [04:42,  1.67it/s]Extractor Estimating: 439it [04:42,  1.67it/s]Extractor Estimating: 440it [04:43,  1.65it/s]Extractor Estimating: 441it [04:44,  1.62it/s]Extractor Estimating: 442it [04:44,  1.64it/s]Extractor Estimating: 443it [04:45,  1.62it/s]Extractor Estimating: 444it [04:45,  1.64it/s]Extractor Estimating: 445it [04:46,  1.62it/s]Extractor Estimating: 446it [04:47,  1.64it/s]Extractor Estimating: 447it [04:47,  1.66it/s]Extractor Estimating: 448it [04:48,  1.65it/s]Extractor Estimating: 449it [04:49,  1.63it/s]Extractor Estimating: 450it [04:49,  1.60it/s]Extractor Estimating: 451it [04:50,  1.60it/s]Extractor Estimating: 452it [04:51,  1.53it/s]Extractor Estimating: 453it [04:51,  1.50it/s]Extractor Estimating: 454it [04:52,  1.48it/s]Extractor Estimating: 455it [04:53,  1.54it/s]Extractor Estimating: 456it [04:53,  1.54it/s]Extractor Estimating: 457it [04:54,  1.46it/s]Extractor Estimating: 458it [04:55,  1.44it/s]Extractor Estimating: 459it [04:55,  1.50it/s]Extractor Estimating: 460it [04:56,  1.52it/s]Extractor Estimating: 461it [04:57,  1.47it/s]Extractor Estimating: 462it [04:57,  1.42it/s]Extractor Estimating: 463it [04:58,  1.44it/s]Extractor Estimating: 464it [04:59,  1.44it/s]Extractor Estimating: 465it [04:59,  1.44it/s]Extractor Estimating: 466it [05:00,  1.46it/s]Extractor Estimating: 467it [05:01,  1.49it/s]Extractor Estimating: 468it [05:02,  1.41it/s]Extractor Estimating: 469it [05:02,  1.40it/s]Extractor Estimating: 470it [05:03,  1.43it/s]Extractor Estimating: 471it [05:04,  1.44it/s]Extractor Estimating: 472it [05:04,  1.42it/s]Extractor Estimating: 473it [05:05,  1.41it/s]Extractor Estimating: 474it [05:06,  1.48it/s]Extractor Estimating: 475it [05:06,  1.47it/s]Extractor Estimating: 476it [05:07,  1.54it/s]Extractor Estimating: 477it [05:08,  1.57it/s]Extractor Estimating: 478it [05:08,  1.51it/s]Extractor Estimating: 479it [05:09,  1.55it/s]Extractor Estimating: 480it [05:10,  1.50it/s]Extractor Estimating: 481it [05:10,  1.50it/s]Extractor Estimating: 482it [05:11,  1.54it/s]Extractor Estimating: 483it [05:11,  1.54it/s]Extractor Estimating: 484it [05:12,  1.57it/s]Extractor Estimating: 485it [05:13,  1.58it/s]Extractor Estimating: 486it [05:13,  1.59it/s]Extractor Estimating: 487it [05:14,  1.61it/s]Extractor Estimating: 488it [05:15,  1.52it/s]Extractor Estimating: 489it [05:15,  1.53it/s]Extractor Estimating: 490it [05:16,  1.52it/s]Extractor Estimating: 491it [05:17,  1.37it/s]Extractor Estimating: 492it [05:18,  1.42it/s]Extractor Estimating: 493it [05:18,  1.44it/s]Extractor Estimating: 494it [05:19,  1.46it/s]Extractor Estimating: 495it [05:19,  1.52it/s]Extractor Estimating: 496it [05:20,  1.48it/s]Extractor Estimating: 497it [05:21,  1.48it/s]Extractor Estimating: 498it [05:22,  1.50it/s]Extractor Estimating: 499it [05:22,  1.50it/s]Extractor Estimating: 500it [05:23,  1.59it/s]Extractor Estimating: 500it [05:23,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:11,035 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:11,040 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:11,040 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:11,041 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:11,041 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:42:11,637 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:42:11,638 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:42:12,689 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:42:13,751 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:42:13,751 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:17,200 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:17,205 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:17,206 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:17,206 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:17,206 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:42:17,844 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:42:17,845 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:42:18,425 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:42:18,578 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:42:18,578 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 17:51:34,968 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 17:51:34,978 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 10017 mean pseudo reward: 0.9315341506676885
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 25475
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25575, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25575, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.081, loss:865.3238
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.112, loss:831.4381
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.095, loss:856.4224
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.101, loss:797.6358
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 1.098, loss:781.6100
>> valid entity prec:0.6033, rec:0.5427, f1:0.5714
>> valid relation prec:0.2922, rec:0.1015, f1:0.1507
>> valid relation with NER prec:0.2922, rec:0.1015, f1:0.1507
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 2.476, loss:811.8822
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 282, avg_time 1.093, loss:818.7066
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 382, avg_time 1.089, loss:822.1626
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 1.099, loss:783.3316
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 1.085, loss:779.3678
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5428, rec:0.5694, f1:0.5558
>> valid relation prec:0.2405, rec:0.1061, f1:0.1473
>> valid relation with NER prec:0.2405, rec:0.1061, f1:0.1473
g_step 1100, step 264, avg_time 2.457, loss:811.8708
g_step 1200, step 364, avg_time 1.101, loss:794.5127
g_step 1300, step 46, avg_time 1.084, loss:769.9595
g_step 1400, step 146, avg_time 1.090, loss:744.0425
g_step 1500, step 246, avg_time 1.100, loss:747.6689
>> valid entity prec:0.6181, rec:0.5380, f1:0.5753
>> valid relation prec:0.2222, rec:0.0794, f1:0.1170
>> valid relation with NER prec:0.2222, rec:0.0794, f1:0.1170
new max entity f1 on valid!
g_step 1600, step 346, avg_time 2.473, loss:749.3566
g_step 1700, step 28, avg_time 1.081, loss:713.5585
g_step 1800, step 128, avg_time 1.088, loss:694.4049
g_step 1900, step 228, avg_time 1.091, loss:742.1193
g_step 2000, step 328, avg_time 1.109, loss:705.8968
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5954, rec:0.5482, f1:0.5708
>> valid relation prec:0.2101, rec:0.0877, f1:0.1238
>> valid relation with NER prec:0.2101, rec:0.0877, f1:0.1238
g_step 2100, step 10, avg_time 2.476, loss:700.6773
g_step 2200, step 110, avg_time 1.082, loss:669.8712
g_step 2300, step 210, avg_time 1.106, loss:657.3021
g_step 2400, step 310, avg_time 1.094, loss:682.8924
g_step 2500, step 410, avg_time 1.105, loss:682.0876
>> valid entity prec:0.6135, rec:0.5609, f1:0.5860
>> valid relation prec:0.1799, rec:0.0716, f1:0.1024
>> valid relation with NER prec:0.1799, rec:0.0716, f1:0.1024
new max entity f1 on valid!
g_step 2600, step 92, avg_time 2.473, loss:626.9589
g_step 2700, step 192, avg_time 1.110, loss:621.9402
g_step 2800, step 292, avg_time 1.085, loss:667.0539
g_step 2900, step 392, avg_time 1.096, loss:636.0611
g_step 3000, step 74, avg_time 1.094, loss:612.8203
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6281, rec:0.5061, f1:0.5605
>> valid relation prec:0.2304, rec:0.0846, f1:0.1237
>> valid relation with NER prec:0.2304, rec:0.0846, f1:0.1237
g_step 3100, step 174, avg_time 2.466, loss:606.3270
g_step 3200, step 274, avg_time 1.103, loss:608.4152
g_step 3300, step 374, avg_time 1.083, loss:616.4916
g_step 3400, step 56, avg_time 1.088, loss:594.5940
g_step 3500, step 156, avg_time 1.102, loss:584.3068
>> valid entity prec:0.6010, rec:0.5887, f1:0.5947
>> valid relation prec:0.2103, rec:0.0825, f1:0.1185
>> valid relation with NER prec:0.2103, rec:0.0825, f1:0.1185
new max entity f1 on valid!
g_step 3600, step 256, avg_time 2.485, loss:590.1730
g_step 3700, step 356, avg_time 1.100, loss:604.1832
g_step 3800, step 38, avg_time 1.090, loss:567.0399
g_step 3900, step 138, avg_time 1.096, loss:551.8515
g_step 4000, step 238, avg_time 1.121, loss:545.1759
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5936, rec:0.5580, f1:0.5753
>> valid relation prec:0.2044, rec:0.1018, f1:0.1359
>> valid relation with NER prec:0.2044, rec:0.1018, f1:0.1359
g_step 4100, step 338, avg_time 2.448, loss:595.8361
g_step 4200, step 20, avg_time 1.094, loss:578.4960
g_step 4300, step 120, avg_time 1.085, loss:540.0939
g_step 4400, step 220, avg_time 1.089, loss:517.3245
g_step 4500, step 320, avg_time 1.106, loss:553.2396
>> valid entity prec:0.5930, rec:0.5195, f1:0.5538
>> valid relation prec:0.2171, rec:0.0840, f1:0.1211
>> valid relation with NER prec:0.2171, rec:0.0840, f1:0.1211
g_step 4600, step 2, avg_time 2.478, loss:550.4575
g_step 4700, step 102, avg_time 1.094, loss:506.4476
g_step 4800, step 202, avg_time 1.109, loss:503.9708
g_step 4900, step 302, avg_time 1.090, loss:529.9300
g_step 5000, step 402, avg_time 1.101, loss:556.4548
learning rate was adjusted to 0.0008
>> valid entity prec:0.5800, rec:0.5303, f1:0.5540
>> valid relation prec:0.1769, rec:0.0900, f1:0.1193
>> valid relation with NER prec:0.1769, rec:0.0900, f1:0.1193
g_step 5100, step 84, avg_time 2.448, loss:482.7328
g_step 5200, step 184, avg_time 1.099, loss:499.5709
g_step 5300, step 284, avg_time 1.095, loss:504.9245
g_step 5400, step 384, avg_time 1.089, loss:518.4106
g_step 5500, step 66, avg_time 1.099, loss:481.2248
>> valid entity prec:0.5998, rec:0.5183, f1:0.5561
>> valid relation prec:0.1501, rec:0.0615, f1:0.0873
>> valid relation with NER prec:0.1501, rec:0.0615, f1:0.0873
g_step 5600, step 166, avg_time 2.477, loss:454.9157
g_step 5700, step 266, avg_time 1.096, loss:478.9751
g_step 5800, step 366, avg_time 1.097, loss:501.5455
g_step 5900, step 48, avg_time 1.081, loss:466.1691
g_step 6000, step 148, avg_time 1.088, loss:442.2048
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5974, rec:0.5423, f1:0.5685
>> valid relation prec:0.1486, rec:0.0759, f1:0.1005
>> valid relation with NER prec:0.1486, rec:0.0759, f1:0.1005
g_step 6100, step 248, avg_time 2.475, loss:455.9617
g_step 6200, step 348, avg_time 1.089, loss:490.8566
g_step 6300, step 30, avg_time 1.093, loss:461.7441
g_step 6400, step 130, avg_time 1.086, loss:424.5730
g_step 6500, step 230, avg_time 1.096, loss:438.1032
>> valid entity prec:0.5530, rec:0.5217, f1:0.5368
>> valid relation prec:0.1592, rec:0.0661, f1:0.0935
>> valid relation with NER prec:0.1592, rec:0.0661, f1:0.0935
g_step 6600, step 330, avg_time 2.475, loss:448.7997
g_step 6700, step 12, avg_time 1.105, loss:456.1974
g_step 6800, step 112, avg_time 1.092, loss:409.6372
g_step 6900, step 212, avg_time 1.093, loss:422.9903
g_step 7000, step 312, avg_time 1.088, loss:428.8121
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.6111, rec:0.4824, f1:0.5392
>> valid relation prec:0.1721, rec:0.0705, f1:0.1000
>> valid relation with NER prec:0.1721, rec:0.0705, f1:0.1000
g_step 7100, step 412, avg_time 2.476, loss:441.2174
g_step 7200, step 94, avg_time 1.086, loss:398.6581
g_step 7300, step 194, avg_time 1.091, loss:399.5317
g_step 7400, step 294, avg_time 1.097, loss:410.8475
g_step 7500, step 394, avg_time 1.106, loss:422.0060
>> valid entity prec:0.5855, rec:0.5212, f1:0.5515
>> valid relation prec:0.1862, rec:0.0900, f1:0.1214
>> valid relation with NER prec:0.1862, rec:0.0900, f1:0.1214
g_step 7600, step 76, avg_time 2.470, loss:384.8209
g_step 7700, step 176, avg_time 1.097, loss:387.7400
g_step 7800, step 276, avg_time 1.090, loss:404.7994
g_step 7900, step 376, avg_time 1.099, loss:419.0709
g_step 8000, step 58, avg_time 1.087, loss:352.9722
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5940, rec:0.5010, f1:0.5435
>> valid relation prec:0.1712, rec:0.0782, f1:0.1074
>> valid relation with NER prec:0.1712, rec:0.0782, f1:0.1074
g_step 8100, step 158, avg_time 2.464, loss:379.0303
g_step 8200, step 258, avg_time 1.107, loss:383.0426
g_step 8300, step 358, avg_time 1.091, loss:400.3592
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 17:51:34 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 17:51:34 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_17-51-34_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 17:51:35 - WARNING - datasets.builder -   Using custom data configuration default-b5a408424f9b38d5
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-b5a408424f9b38d5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 17:51:36,930 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:51:36,931 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:51:36,931 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:51:36,932 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:51:36,940 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:51:36,949 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:51:36,949 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:51:36,949 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:51:36,949 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:51:36,949 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:51:36,949 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 17:51:37,098 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:51:40,250 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 17:51:40,258 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-b5a408424f9b38d5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  3.20ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.98ba/s] 27%|██▋       | 3/11 [00:00<00:02,  3.36ba/s] 36%|███▋      | 4/11 [00:01<00:01,  3.77ba/s] 45%|████▌     | 5/11 [00:01<00:01,  4.07ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.22ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.35ba/s] 73%|███████▎  | 8/11 [00:01<00:00,  4.45ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.51ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.58ba/s]100%|██████████| 11/11 [00:02<00:00,  4.60ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.83ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.19ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.33ba/s]100%|██████████| 4/4 [00:00<00:00,  5.31ba/s]100%|██████████| 4/4 [00:00<00:00,  4.83ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  5.90ba/s] 27%|██▋       | 3/11 [00:00<00:00,  8.51ba/s] 45%|████▌     | 5/11 [00:00<00:00,  9.32ba/s] 64%|██████▎   | 7/11 [00:00<00:00,  9.66ba/s] 73%|███████▎  | 8/11 [00:00<00:00,  9.71ba/s] 82%|████████▏ | 9/11 [00:00<00:00,  9.78ba/s]100%|██████████| 11/11 [00:01<00:00, 12.30ba/s]100%|██████████| 11/11 [00:01<00:00, 10.32ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.67ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.41ba/s]100%|██████████| 4/4 [00:00<00:00, 10.64ba/s]
[INFO|trainer.py:414] 2023-08-28 17:51:45,629 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 17:51:45,635 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 17:51:45,635 >>   Num examples = 10037
[INFO|trainer.py:1149] 2023-08-28 17:51:45,635 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 17:51:45,635 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 17:51:45,635 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 17:51:45,635 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 17:51:45,635 >>   Total optimization steps = 785
  0%|          | 0/785 [00:00<?, ?it/s]  0%|          | 1/785 [00:00<03:51,  3.39it/s]  0%|          | 2/785 [00:00<03:47,  3.44it/s]  0%|          | 3/785 [00:00<03:46,  3.45it/s]  1%|          | 4/785 [00:01<03:45,  3.46it/s]  1%|          | 5/785 [00:01<03:44,  3.47it/s]  1%|          | 6/785 [00:01<03:44,  3.47it/s]  1%|          | 7/785 [00:02<03:44,  3.47it/s]  1%|          | 8/785 [00:02<03:43,  3.47it/s]  1%|          | 9/785 [00:02<03:43,  3.48it/s]  1%|▏         | 10/785 [00:02<03:42,  3.48it/s]  1%|▏         | 11/785 [00:03<03:42,  3.48it/s]  2%|▏         | 12/785 [00:03<03:42,  3.48it/s]  2%|▏         | 13/785 [00:03<03:41,  3.48it/s]  2%|▏         | 14/785 [00:04<03:41,  3.48it/s]  2%|▏         | 15/785 [00:04<03:41,  3.48it/s]  2%|▏         | 16/785 [00:04<03:40,  3.48it/s]  2%|▏         | 17/785 [00:04<03:40,  3.48it/s]  2%|▏         | 18/785 [00:05<03:40,  3.48it/s]  2%|▏         | 19/785 [00:05<03:40,  3.48it/s]  3%|▎         | 20/785 [00:05<03:40,  3.48it/s]  3%|▎         | 21/785 [00:06<03:39,  3.48it/s]  3%|▎         | 22/785 [00:06<03:39,  3.48it/s]  3%|▎         | 23/785 [00:06<03:39,  3.48it/s]  3%|▎         | 24/785 [00:06<03:38,  3.48it/s]  3%|▎         | 25/785 [00:07<03:39,  3.47it/s]  3%|▎         | 26/785 [00:07<03:38,  3.47it/s]  3%|▎         | 27/785 [00:07<03:38,  3.47it/s]  4%|▎         | 28/785 [00:08<03:37,  3.48it/s]  4%|▎         | 29/785 [00:08<03:37,  3.48it/s]  4%|▍         | 30/785 [00:08<03:37,  3.47it/s]  4%|▍         | 31/785 [00:08<03:37,  3.47it/s]  4%|▍         | 32/785 [00:09<03:50,  3.26it/s]  4%|▍         | 33/785 [00:09<03:46,  3.32it/s]  4%|▍         | 34/785 [00:09<03:43,  3.35it/s]  4%|▍         | 35/785 [00:10<03:41,  3.39it/s]  5%|▍         | 36/785 [00:10<03:39,  3.42it/s]  5%|▍         | 37/785 [00:10<03:37,  3.43it/s]  5%|▍         | 38/785 [00:11<03:36,  3.45it/s]  5%|▍         | 39/785 [00:11<03:35,  3.46it/s]  5%|▌         | 40/785 [00:11<03:35,  3.46it/s]  5%|▌         | 41/785 [00:11<03:34,  3.47it/s]  5%|▌         | 42/785 [00:12<03:33,  3.47it/s]  5%|▌         | 43/785 [00:12<03:33,  3.47it/s]  6%|▌         | 44/785 [00:12<03:33,  3.47it/s]  6%|▌         | 45/785 [00:13<03:33,  3.47it/s]  6%|▌         | 46/785 [00:13<03:32,  3.47it/s]  6%|▌         | 47/785 [00:13<03:32,  3.47it/s]  6%|▌         | 48/785 [00:13<03:32,  3.47it/s]  6%|▌         | 49/785 [00:14<03:31,  3.47it/s]  6%|▋         | 50/785 [00:14<03:31,  3.47it/s]  6%|▋         | 51/785 [00:14<03:31,  3.47it/s]  7%|▋         | 52/785 [00:15<03:31,  3.47it/s]  7%|▋         | 53/785 [00:15<03:30,  3.47it/s]  7%|▋         | 54/785 [00:15<03:30,  3.47it/s]  7%|▋         | 55/785 [00:15<03:30,  3.47it/s]  7%|▋         | 56/785 [00:16<03:29,  3.47it/s]  7%|▋         | 57/785 [00:16<03:29,  3.47it/s]  7%|▋         | 58/785 [00:16<03:29,  3.47it/s]  8%|▊         | 59/785 [00:17<03:29,  3.47it/s]  8%|▊         | 60/785 [00:17<03:29,  3.46it/s]  8%|▊         | 61/785 [00:17<03:29,  3.46it/s]  8%|▊         | 62/785 [00:17<03:28,  3.47it/s]  8%|▊         | 63/785 [00:18<03:28,  3.47it/s]  8%|▊         | 64/785 [00:18<03:27,  3.47it/s]  8%|▊         | 65/785 [00:18<03:27,  3.47it/s]  8%|▊         | 66/785 [00:19<03:27,  3.47it/s]  9%|▊         | 67/785 [00:19<03:26,  3.47it/s]  9%|▊         | 68/785 [00:19<03:26,  3.47it/s]  9%|▉         | 69/785 [00:19<03:26,  3.47it/s]  9%|▉         | 70/785 [00:20<03:26,  3.47it/s]  9%|▉         | 71/785 [00:20<03:28,  3.43it/s]  9%|▉         | 72/785 [00:20<03:27,  3.44it/s]  9%|▉         | 73/785 [00:21<03:26,  3.44it/s]  9%|▉         | 74/785 [00:21<03:26,  3.45it/s] 10%|▉         | 75/785 [00:21<03:25,  3.46it/s] 10%|▉         | 76/785 [00:21<03:24,  3.46it/s] 10%|▉         | 77/785 [00:22<03:24,  3.47it/s] 10%|▉         | 78/785 [00:22<03:24,  3.46it/s] 10%|█         | 79/785 [00:22<03:23,  3.47it/s] 10%|█         | 80/785 [00:23<03:23,  3.47it/s] 10%|█         | 81/785 [00:23<03:22,  3.47it/s] 10%|█         | 82/785 [00:23<03:29,  3.35it/s] 11%|█         | 83/785 [00:24<03:27,  3.39it/s] 11%|█         | 84/785 [00:24<03:25,  3.41it/s] 11%|█         | 85/785 [00:24<03:24,  3.43it/s] 11%|█         | 86/785 [00:24<03:22,  3.44it/s] 11%|█         | 87/785 [00:25<03:22,  3.45it/s] 11%|█         | 88/785 [00:25<03:21,  3.46it/s] 11%|█▏        | 89/785 [00:25<03:21,  3.46it/s] 11%|█▏        | 90/785 [00:26<03:20,  3.46it/s] 12%|█▏        | 91/785 [00:26<03:20,  3.46it/s] 12%|█▏        | 92/785 [00:26<03:20,  3.46it/s] 12%|█▏        | 93/785 [00:26<03:26,  3.35it/s] 12%|█▏        | 94/785 [00:27<03:24,  3.38it/s] 12%|█▏        | 95/785 [00:27<03:22,  3.41it/s] 12%|█▏        | 96/785 [00:27<03:20,  3.43it/s] 12%|█▏        | 97/785 [00:28<03:20,  3.44it/s] 12%|█▏        | 98/785 [00:28<03:19,  3.45it/s] 13%|█▎        | 99/785 [00:28<03:18,  3.45it/s] 13%|█▎        | 100/785 [00:28<03:18,  3.45it/s] 13%|█▎        | 101/785 [00:29<03:17,  3.46it/s] 13%|█▎        | 102/785 [00:29<03:17,  3.46it/s] 13%|█▎        | 103/785 [00:29<03:17,  3.46it/s] 13%|█▎        | 104/785 [00:30<03:17,  3.44it/s] 13%|█▎        | 105/785 [00:30<03:17,  3.45it/s] 14%|█▎        | 106/785 [00:30<03:16,  3.45it/s] 14%|█▎        | 107/785 [00:30<03:16,  3.46it/s] 14%|█▍        | 108/785 [00:31<03:15,  3.46it/s] 14%|█▍        | 109/785 [00:31<03:15,  3.46it/s] 14%|█▍        | 110/785 [00:31<03:15,  3.46it/s] 14%|█▍        | 111/785 [00:32<03:14,  3.46it/s] 14%|█▍        | 112/785 [00:32<03:14,  3.46it/s] 14%|█▍        | 113/785 [00:32<03:14,  3.46it/s] 15%|█▍        | 114/785 [00:33<03:14,  3.46it/s] 15%|█▍        | 115/785 [00:33<03:14,  3.45it/s] 15%|█▍        | 116/785 [00:33<03:13,  3.45it/s] 15%|█▍        | 117/785 [00:33<03:13,  3.46it/s] 15%|█▌        | 118/785 [00:34<03:12,  3.46it/s] 15%|█▌        | 119/785 [00:34<03:12,  3.46it/s] 15%|█▌        | 120/785 [00:34<03:12,  3.46it/s] 15%|█▌        | 121/785 [00:35<03:11,  3.46it/s] 16%|█▌        | 122/785 [00:35<03:11,  3.46it/s] 16%|█▌        | 123/785 [00:35<03:11,  3.46it/s] 16%|█▌        | 124/785 [00:35<03:10,  3.46it/s] 16%|█▌        | 125/785 [00:36<03:10,  3.47it/s] 16%|█▌        | 126/785 [00:36<03:18,  3.31it/s] 16%|█▌        | 127/785 [00:36<03:16,  3.36it/s] 16%|█▋        | 128/785 [00:37<03:13,  3.39it/s] 16%|█▋        | 129/785 [00:37<03:12,  3.41it/s] 17%|█▋        | 130/785 [00:37<03:11,  3.43it/s] 17%|█▋        | 131/785 [00:37<03:10,  3.44it/s] 17%|█▋        | 132/785 [00:38<03:09,  3.45it/s] 17%|█▋        | 133/785 [00:38<03:08,  3.45it/s] 17%|█▋        | 134/785 [00:38<03:08,  3.46it/s] 17%|█▋        | 135/785 [00:39<03:07,  3.46it/s] 17%|█▋        | 136/785 [00:39<03:07,  3.46it/s] 17%|█▋        | 137/785 [00:39<03:10,  3.41it/s] 18%|█▊        | 138/785 [00:39<03:08,  3.43it/s] 18%|█▊        | 139/785 [00:40<03:08,  3.44it/s] 18%|█▊        | 140/785 [00:40<03:07,  3.44it/s] 18%|█▊        | 141/785 [00:40<03:06,  3.45it/s] 18%|█▊        | 142/785 [00:41<03:06,  3.45it/s] 18%|█▊        | 143/785 [00:41<03:05,  3.46it/s] 18%|█▊        | 144/785 [00:41<03:05,  3.46it/s] 18%|█▊        | 145/785 [00:42<03:05,  3.46it/s] 19%|█▊        | 146/785 [00:42<03:04,  3.46it/s] 19%|█▊        | 147/785 [00:42<03:04,  3.46it/s] 19%|█▉        | 148/785 [00:42<03:12,  3.31it/s] 19%|█▉        | 149/785 [00:43<03:09,  3.36it/s] 19%|█▉        | 150/785 [00:43<03:07,  3.39it/s] 19%|█▉        | 151/785 [00:43<03:05,  3.41it/s] 19%|█▉        | 152/785 [00:44<03:04,  3.42it/s] 19%|█▉        | 153/785 [00:44<03:03,  3.44it/s] 20%|█▉        | 154/785 [00:44<03:03,  3.44it/s] 20%|█▉        | 155/785 [00:44<03:02,  3.45it/s] 20%|█▉        | 156/785 [00:45<03:02,  3.45it/s] 20%|██        | 157/785 [00:45<02:53,  3.62it/s][INFO|trainer.py:2140] 2023-08-28 17:52:31,119 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:52:31,119 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 17:52:31,119 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.61it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.51it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.67it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.92it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.49it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.30it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.92it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.79it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.73it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.72it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.73it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.74it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.67it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.49it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.41it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.33it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.21it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.16it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.16it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.21it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.39it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.46it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.56it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.57it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.64it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.50it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.56it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.61it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.51it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.52it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.51it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.58it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.59it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.64it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.69it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.67it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.50it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.63it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.65it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.61it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.60it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.58it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.54it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.66it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.62it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.71it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.70it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.65it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.59it/s][A
 58%|█████▊    | 253/435 [00:05<00:04, 43.24it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 44.26it/s][A
 60%|██████    | 263/435 [00:05<00:03, 44.89it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 45.38it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 45.78it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 45.98it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.24it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.42it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.28it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.40it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.52it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.41it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.53it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.60it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.64it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.65it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.63it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.56it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.51it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.48it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.46it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.56it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.61it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.63it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.64it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.59it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.66it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.61it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.08it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.20it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.29it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.45it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.45it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.58it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.54it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.55it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.53it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 46.53it/s][A 20%|██        | 157/785 [00:54<02:53,  3.62it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:52:40,540 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-157
[INFO|configuration_utils.py:351] 2023-08-28 17:52:40,594 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-157/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:52:45,312 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-157/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:52:45,352 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-157/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:52:45,370 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-157/special_tokens_map.json
 20%|██        | 158/785 [01:06<1:08:12,  6.53s/it] 20%|██        | 159/785 [01:06<48:35,  4.66s/it]   20%|██        | 160/785 [01:07<34:51,  3.35s/it] 21%|██        | 161/785 [01:07<25:15,  2.43s/it] 21%|██        | 162/785 [01:07<18:33,  1.79s/it] 21%|██        | 163/785 [01:08<13:52,  1.34s/it] 21%|██        | 164/785 [01:08<10:35,  1.02s/it] 21%|██        | 165/785 [01:08<08:17,  1.25it/s] 21%|██        | 166/785 [01:08<06:41,  1.54it/s] 21%|██▏       | 167/785 [01:09<05:33,  1.85it/s] 21%|██▏       | 168/785 [01:09<04:46,  2.15it/s] 22%|██▏       | 169/785 [01:09<04:15,  2.41it/s] 22%|██▏       | 170/785 [01:10<03:55,  2.61it/s] 22%|██▏       | 171/785 [01:10<03:37,  2.82it/s] 22%|██▏       | 172/785 [01:10<03:25,  2.99it/s] 22%|██▏       | 173/785 [01:10<03:16,  3.12it/s] 22%|██▏       | 174/785 [01:11<03:10,  3.21it/s] 22%|██▏       | 175/785 [01:11<03:05,  3.29it/s] 22%|██▏       | 176/785 [01:11<03:02,  3.34it/s] 23%|██▎       | 177/785 [01:12<02:59,  3.38it/s] 23%|██▎       | 178/785 [01:12<02:58,  3.41it/s] 23%|██▎       | 179/785 [01:12<02:56,  3.42it/s] 23%|██▎       | 180/785 [01:12<02:55,  3.44it/s] 23%|██▎       | 181/785 [01:13<02:59,  3.36it/s] 23%|██▎       | 182/785 [01:13<02:57,  3.39it/s] 23%|██▎       | 183/785 [01:13<02:56,  3.42it/s] 23%|██▎       | 184/785 [01:14<02:55,  3.43it/s] 24%|██▎       | 185/785 [01:14<02:54,  3.44it/s] 24%|██▎       | 186/785 [01:14<02:53,  3.45it/s] 24%|██▍       | 187/785 [01:15<02:52,  3.46it/s] 24%|██▍       | 188/785 [01:15<02:52,  3.46it/s] 24%|██▍       | 189/785 [01:15<02:52,  3.46it/s] 24%|██▍       | 190/785 [01:15<02:51,  3.46it/s] 24%|██▍       | 191/785 [01:16<02:51,  3.47it/s] 24%|██▍       | 192/785 [01:16<02:51,  3.46it/s] 25%|██▍       | 193/785 [01:16<02:50,  3.46it/s] 25%|██▍       | 194/785 [01:17<02:50,  3.46it/s] 25%|██▍       | 195/785 [01:17<02:50,  3.47it/s] 25%|██▍       | 196/785 [01:17<02:50,  3.46it/s] 25%|██▌       | 197/785 [01:17<02:49,  3.47it/s] 25%|██▌       | 198/785 [01:18<02:49,  3.47it/s] 25%|██▌       | 199/785 [01:18<02:48,  3.47it/s] 25%|██▌       | 200/785 [01:18<02:50,  3.42it/s] 26%|██▌       | 201/785 [01:19<02:49,  3.44it/s] 26%|██▌       | 202/785 [01:19<02:49,  3.45it/s] 26%|██▌       | 203/785 [01:19<02:48,  3.45it/s] 26%|██▌       | 204/785 [01:19<02:47,  3.46it/s] 26%|██▌       | 205/785 [01:20<02:47,  3.46it/s] 26%|██▌       | 206/785 [01:20<02:47,  3.46it/s] 26%|██▋       | 207/785 [01:20<02:46,  3.46it/s] 26%|██▋       | 208/785 [01:21<02:46,  3.46it/s] 27%|██▋       | 209/785 [01:21<02:46,  3.47it/s] 27%|██▋       | 210/785 [01:21<02:45,  3.47it/s] 27%|██▋       | 211/785 [01:21<02:51,  3.34it/s] 27%|██▋       | 212/785 [01:22<02:49,  3.38it/s] 27%|██▋       | 213/785 [01:22<02:47,  3.41it/s] 27%|██▋       | 214/785 [01:22<02:46,  3.42it/s] 27%|██▋       | 215/785 [01:23<02:45,  3.44it/s] 28%|██▊       | 216/785 [01:23<02:45,  3.45it/s] 28%|██▊       | 217/785 [01:23<02:44,  3.45it/s] 28%|██▊       | 218/785 [01:24<02:44,  3.45it/s] 28%|██▊       | 219/785 [01:24<02:43,  3.46it/s] 28%|██▊       | 220/785 [01:24<02:43,  3.46it/s] 28%|██▊       | 221/785 [01:24<02:42,  3.46it/s] 28%|██▊       | 222/785 [01:25<03:00,  3.13it/s] 28%|██▊       | 223/785 [01:25<02:54,  3.22it/s] 29%|██▊       | 224/785 [01:25<02:50,  3.29it/s] 29%|██▊       | 225/785 [01:26<02:47,  3.34it/s] 29%|██▉       | 226/785 [01:26<02:45,  3.38it/s] 29%|██▉       | 227/785 [01:26<02:43,  3.40it/s] 29%|██▉       | 228/785 [01:26<02:42,  3.42it/s] 29%|██▉       | 229/785 [01:27<02:41,  3.43it/s] 29%|██▉       | 230/785 [01:27<02:41,  3.44it/s] 29%|██▉       | 231/785 [01:27<02:40,  3.45it/s] 30%|██▉       | 232/785 [01:28<02:40,  3.45it/s] 30%|██▉       | 233/785 [01:28<02:52,  3.20it/s] 30%|██▉       | 234/785 [01:28<02:48,  3.27it/s] 30%|██▉       | 235/785 [01:29<02:45,  3.33it/s] 30%|███       | 236/785 [01:29<02:43,  3.37it/s] 30%|███       | 237/785 [01:29<02:41,  3.39it/s] 30%|███       | 238/785 [01:29<02:40,  3.41it/s] 30%|███       | 239/785 [01:30<02:39,  3.42it/s] 31%|███       | 240/785 [01:30<02:38,  3.43it/s] 31%|███       | 241/785 [01:30<02:38,  3.44it/s] 31%|███       | 242/785 [01:31<02:37,  3.44it/s] 31%|███       | 243/785 [01:31<02:37,  3.44it/s] 31%|███       | 244/785 [01:31<03:06,  2.89it/s] 31%|███       | 245/785 [01:32<02:57,  3.04it/s] 31%|███▏      | 246/785 [01:32<02:50,  3.16it/s] 31%|███▏      | 247/785 [01:32<02:45,  3.25it/s] 32%|███▏      | 248/785 [01:33<02:42,  3.31it/s] 32%|███▏      | 249/785 [01:33<02:39,  3.35it/s] 32%|███▏      | 250/785 [01:33<02:38,  3.38it/s] 32%|███▏      | 251/785 [01:33<02:36,  3.41it/s] 32%|███▏      | 252/785 [01:34<02:35,  3.42it/s] 32%|███▏      | 253/785 [01:34<02:34,  3.43it/s] 32%|███▏      | 254/785 [01:34<02:56,  3.00it/s] 32%|███▏      | 255/785 [01:35<02:49,  3.12it/s] 33%|███▎      | 256/785 [01:35<02:44,  3.22it/s] 33%|███▎      | 257/785 [01:35<02:40,  3.29it/s] 33%|███▎      | 258/785 [01:36<02:37,  3.34it/s] 33%|███▎      | 259/785 [01:36<02:35,  3.37it/s] 33%|███▎      | 260/785 [01:36<02:34,  3.40it/s] 33%|███▎      | 261/785 [01:36<02:33,  3.42it/s] 33%|███▎      | 262/785 [01:37<02:32,  3.43it/s] 34%|███▎      | 263/785 [01:37<02:31,  3.44it/s] 34%|███▎      | 264/785 [01:37<02:51,  3.04it/s] 34%|███▍      | 265/785 [01:38<02:44,  3.16it/s] 34%|███▍      | 266/785 [01:38<02:40,  3.24it/s] 34%|███▍      | 267/785 [01:38<02:36,  3.30it/s] 34%|███▍      | 268/785 [01:39<02:34,  3.35it/s] 34%|███▍      | 269/785 [01:39<02:32,  3.38it/s] 34%|███▍      | 270/785 [01:39<02:31,  3.41it/s] 35%|███▍      | 271/785 [01:39<02:30,  3.42it/s] 35%|███▍      | 272/785 [01:40<02:29,  3.43it/s] 35%|███▍      | 273/785 [01:40<02:28,  3.44it/s] 35%|███▍      | 274/785 [01:40<02:29,  3.42it/s] 35%|███▌      | 275/785 [01:41<02:28,  3.43it/s] 35%|███▌      | 276/785 [01:41<02:27,  3.44it/s] 35%|███▌      | 277/785 [01:41<02:27,  3.44it/s] 35%|███▌      | 278/785 [01:41<02:27,  3.45it/s] 36%|███▌      | 279/785 [01:42<02:26,  3.45it/s] 36%|███▌      | 280/785 [01:42<02:26,  3.45it/s] 36%|███▌      | 281/785 [01:42<02:25,  3.46it/s] 36%|███▌      | 282/785 [01:43<02:25,  3.46it/s] 36%|███▌      | 283/785 [01:43<02:25,  3.46it/s] 36%|███▌      | 284/785 [01:43<02:24,  3.46it/s] 36%|███▋      | 285/785 [01:44<02:26,  3.42it/s] 36%|███▋      | 286/785 [01:44<02:25,  3.43it/s] 37%|███▋      | 287/785 [01:44<02:24,  3.44it/s] 37%|███▋      | 288/785 [01:44<02:24,  3.45it/s] 37%|███▋      | 289/785 [01:45<02:23,  3.45it/s] 37%|███▋      | 290/785 [01:45<02:23,  3.45it/s] 37%|███▋      | 291/785 [01:45<02:23,  3.45it/s] 37%|███▋      | 292/785 [01:46<02:22,  3.45it/s] 37%|███▋      | 293/785 [01:46<02:22,  3.45it/s] 37%|███▋      | 294/785 [01:46<02:22,  3.45it/s] 38%|███▊      | 295/785 [01:46<02:21,  3.46it/s] 38%|███▊      | 296/785 [01:47<02:21,  3.46it/s] 38%|███▊      | 297/785 [01:47<02:21,  3.46it/s] 38%|███▊      | 298/785 [01:47<02:20,  3.46it/s] 38%|███▊      | 299/785 [01:48<02:20,  3.46it/s] 38%|███▊      | 300/785 [01:48<02:20,  3.46it/s] 38%|███▊      | 301/785 [01:48<02:19,  3.46it/s] 38%|███▊      | 302/785 [01:48<02:19,  3.46it/s] 39%|███▊      | 303/785 [01:49<02:19,  3.46it/s] 39%|███▊      | 304/785 [01:49<02:20,  3.43it/s] 39%|███▉      | 305/785 [01:49<02:19,  3.44it/s] 39%|███▉      | 306/785 [01:50<02:19,  3.44it/s] 39%|███▉      | 307/785 [01:50<02:18,  3.45it/s] 39%|███▉      | 308/785 [01:50<02:18,  3.45it/s] 39%|███▉      | 309/785 [01:50<02:17,  3.45it/s] 39%|███▉      | 310/785 [01:51<02:17,  3.45it/s] 40%|███▉      | 311/785 [01:51<02:17,  3.46it/s] 40%|███▉      | 312/785 [01:51<02:16,  3.45it/s] 40%|███▉      | 313/785 [01:52<02:16,  3.46it/s] 40%|████      | 314/785 [01:52<02:10,  3.62it/s][INFO|trainer.py:2140] 2023-08-28 17:53:38,017 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:53:38,017 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 17:53:38,017 >>   Batch size = 8
{'eval_loss': 0.9571003913879395, 'eval_runtime': 9.3672, 'eval_samples_per_second': 371.191, 'eval_steps_per_second': 46.439, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.35it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.59it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.73it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.80it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.47it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.22it/s][A
  9%|▊         | 38/435 [00:00<00:08, 47.03it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.83it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.77it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.71it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.76it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.74it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.64it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.64it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.59it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.61it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.68it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.60it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.55it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.66it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.52it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.70it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.74it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.56it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.60it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.66it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.45it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.61it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.47it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.47it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.57it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.67it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.58it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.59it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.58it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.59it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.59it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.64it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.52it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.66it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.63it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.60it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.66it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.59it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.64it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.61it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.56it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.59it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.59it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.62it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.57it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.59it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.63it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.64it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.63it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.52it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.17it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.32it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.46it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.40it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.48it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.54it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.55it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.58it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.56it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.54it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.58it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.65it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.56it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.47it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.63it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.60it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.60it/s][A
 86%|████████▌ | 373/435 [00:07<00:01, 46.67it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.59it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.56it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.59it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.59it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.62it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.62it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.55it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.58it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.61it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.52it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.57it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.60it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 46.60it/s][A 40%|████      | 314/785 [02:01<02:10,  3.62it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:53:47,372 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-314
[INFO|configuration_utils.py:351] 2023-08-28 17:53:47,385 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-314/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:53:53,060 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-314/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:53:53,165 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-314/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:53:53,191 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-314/special_tokens_map.json
 40%|████      | 315/785 [02:14<54:32,  6.96s/it] 40%|████      | 316/785 [02:15<38:48,  4.97s/it] 40%|████      | 317/785 [02:15<27:47,  3.56s/it] 41%|████      | 318/785 [02:15<20:04,  2.58s/it] 41%|████      | 319/785 [02:16<14:41,  1.89s/it] 41%|████      | 320/785 [02:16<10:56,  1.41s/it] 41%|████      | 321/785 [02:16<08:18,  1.07s/it] 41%|████      | 322/785 [02:16<06:28,  1.19it/s] 41%|████      | 323/785 [02:17<05:11,  1.49it/s] 41%|████▏     | 324/785 [02:17<04:17,  1.79it/s] 41%|████▏     | 325/785 [02:17<03:39,  2.10it/s] 42%|████▏     | 326/785 [02:18<03:12,  2.38it/s] 42%|████▏     | 327/785 [02:18<02:54,  2.63it/s] 42%|████▏     | 328/785 [02:18<02:41,  2.83it/s] 42%|████▏     | 329/785 [02:18<02:32,  3.00it/s] 42%|████▏     | 330/785 [02:19<02:25,  3.13it/s] 42%|████▏     | 331/785 [02:19<02:20,  3.22it/s] 42%|████▏     | 332/785 [02:19<02:17,  3.29it/s] 42%|████▏     | 333/785 [02:20<02:15,  3.34it/s] 43%|████▎     | 334/785 [02:20<02:13,  3.37it/s] 43%|████▎     | 335/785 [02:20<02:12,  3.40it/s] 43%|████▎     | 336/785 [02:21<02:11,  3.41it/s] 43%|████▎     | 337/785 [02:21<02:10,  3.43it/s] 43%|████▎     | 338/785 [02:21<02:09,  3.44it/s] 43%|████▎     | 339/785 [02:21<02:09,  3.45it/s] 43%|████▎     | 340/785 [02:22<02:08,  3.46it/s] 43%|████▎     | 341/785 [02:22<02:08,  3.46it/s] 44%|████▎     | 342/785 [02:22<02:07,  3.46it/s] 44%|████▎     | 343/785 [02:23<02:07,  3.46it/s] 44%|████▍     | 344/785 [02:23<02:07,  3.46it/s] 44%|████▍     | 345/785 [02:23<02:14,  3.27it/s] 44%|████▍     | 346/785 [02:23<02:11,  3.33it/s] 44%|████▍     | 347/785 [02:24<02:09,  3.37it/s] 44%|████▍     | 348/785 [02:24<02:08,  3.40it/s] 44%|████▍     | 349/785 [02:24<02:07,  3.42it/s] 45%|████▍     | 350/785 [02:25<02:06,  3.43it/s] 45%|████▍     | 351/785 [02:25<02:05,  3.45it/s] 45%|████▍     | 352/785 [02:25<02:05,  3.45it/s] 45%|████▍     | 353/785 [02:25<02:04,  3.46it/s] 45%|████▌     | 354/785 [02:26<02:04,  3.46it/s] 45%|████▌     | 355/785 [02:26<02:04,  3.46it/s] 45%|████▌     | 356/785 [02:26<02:04,  3.44it/s] 45%|████▌     | 357/785 [02:27<02:04,  3.45it/s] 46%|████▌     | 358/785 [02:27<02:03,  3.46it/s] 46%|████▌     | 359/785 [02:27<02:03,  3.46it/s] 46%|████▌     | 360/785 [02:27<02:02,  3.46it/s] 46%|████▌     | 361/785 [02:28<02:02,  3.46it/s] 46%|████▌     | 362/785 [02:28<02:01,  3.47it/s] 46%|████▌     | 363/785 [02:28<02:01,  3.47it/s] 46%|████▋     | 364/785 [02:29<02:01,  3.47it/s] 46%|████▋     | 365/785 [02:29<02:01,  3.47it/s] 47%|████▋     | 366/785 [02:29<02:00,  3.47it/s] 47%|████▋     | 367/785 [02:30<02:03,  3.39it/s] 47%|████▋     | 368/785 [02:30<02:02,  3.42it/s] 47%|████▋     | 369/785 [02:30<02:01,  3.43it/s] 47%|████▋     | 370/785 [02:30<02:00,  3.44it/s] 47%|████▋     | 371/785 [02:31<02:00,  3.45it/s] 47%|████▋     | 372/785 [02:31<01:59,  3.46it/s] 48%|████▊     | 373/785 [02:31<01:59,  3.46it/s] 48%|████▊     | 374/785 [02:32<01:58,  3.46it/s] 48%|████▊     | 375/785 [02:32<01:58,  3.46it/s] 48%|████▊     | 376/785 [02:32<01:58,  3.46it/s] 48%|████▊     | 377/785 [02:32<01:57,  3.47it/s] 48%|████▊     | 378/785 [02:33<02:01,  3.35it/s] 48%|████▊     | 379/785 [02:33<01:59,  3.38it/s] 48%|████▊     | 380/785 [02:33<01:58,  3.41it/s] 49%|████▊     | 381/785 [02:34<01:57,  3.42it/s] 49%|████▊     | 382/785 [02:34<01:57,  3.44it/s] 49%|████▉     | 383/785 [02:34<01:56,  3.45it/s] 49%|████▉     | 384/785 [02:34<01:56,  3.45it/s] 49%|████▉     | 385/785 [02:35<01:55,  3.46it/s] 49%|████▉     | 386/785 [02:35<01:55,  3.47it/s] 49%|████▉     | 387/785 [02:35<01:54,  3.47it/s] 49%|████▉     | 388/785 [02:36<01:54,  3.47it/s] 50%|████▉     | 389/785 [02:36<01:57,  3.38it/s] 50%|████▉     | 390/785 [02:36<01:55,  3.41it/s] 50%|████▉     | 391/785 [02:37<01:54,  3.43it/s] 50%|████▉     | 392/785 [02:37<01:54,  3.44it/s] 50%|█████     | 393/785 [02:37<01:53,  3.44it/s] 50%|█████     | 394/785 [02:37<01:53,  3.45it/s] 50%|█████     | 395/785 [02:38<01:52,  3.45it/s] 50%|█████     | 396/785 [02:38<01:52,  3.46it/s] 51%|█████     | 397/785 [02:38<01:52,  3.46it/s] 51%|█████     | 398/785 [02:39<01:51,  3.46it/s] 51%|█████     | 399/785 [02:39<01:51,  3.46it/s] 51%|█████     | 400/785 [02:40<02:46,  2.31it/s] 51%|█████     | 401/785 [02:40<02:29,  2.57it/s] 51%|█████     | 402/785 [02:40<02:17,  2.78it/s] 51%|█████▏    | 403/785 [02:40<02:09,  2.96it/s] 51%|█████▏    | 404/785 [02:41<02:03,  3.09it/s] 52%|█████▏    | 405/785 [02:41<01:58,  3.19it/s] 52%|█████▏    | 406/785 [02:41<01:56,  3.27it/s] 52%|█████▏    | 407/785 [02:42<01:53,  3.32it/s] 52%|█████▏    | 408/785 [02:42<01:52,  3.36it/s] 52%|█████▏    | 409/785 [02:43<03:22,  1.86it/s] 52%|█████▏    | 410/785 [02:43<02:53,  2.16it/s] 52%|█████▏    | 411/785 [02:44<02:33,  2.43it/s] 52%|█████▏    | 412/785 [02:44<02:19,  2.67it/s] 53%|█████▎    | 413/785 [02:44<02:09,  2.86it/s] 53%|█████▎    | 414/785 [02:44<02:03,  3.02it/s] 53%|█████▎    | 415/785 [02:45<01:58,  3.13it/s] 53%|█████▎    | 416/785 [02:45<01:54,  3.22it/s] 53%|█████▎    | 417/785 [02:45<01:52,  3.27it/s] 53%|█████▎    | 418/785 [02:46<01:50,  3.33it/s] 53%|█████▎    | 419/785 [02:46<01:48,  3.36it/s] 54%|█████▎    | 420/785 [02:46<01:47,  3.40it/s] 54%|█████▎    | 421/785 [02:46<01:46,  3.41it/s] 54%|█████▍    | 422/785 [02:47<01:45,  3.43it/s] 54%|█████▍    | 423/785 [02:47<01:45,  3.44it/s] 54%|█████▍    | 424/785 [02:47<01:44,  3.44it/s] 54%|█████▍    | 425/785 [02:48<01:44,  3.45it/s] 54%|█████▍    | 426/785 [02:48<01:44,  3.45it/s] 54%|█████▍    | 427/785 [02:48<01:43,  3.45it/s] 55%|█████▍    | 428/785 [02:49<01:43,  3.46it/s] 55%|█████▍    | 429/785 [02:49<01:43,  3.45it/s] 55%|█████▍    | 430/785 [02:49<01:42,  3.45it/s] 55%|█████▍    | 431/785 [02:49<01:42,  3.46it/s] 55%|█████▌    | 432/785 [02:50<01:42,  3.46it/s] 55%|█████▌    | 433/785 [02:50<01:41,  3.46it/s] 55%|█████▌    | 434/785 [02:50<01:41,  3.46it/s] 55%|█████▌    | 435/785 [02:51<01:46,  3.27it/s] 56%|█████▌    | 436/785 [02:51<01:44,  3.33it/s] 56%|█████▌    | 437/785 [02:51<01:43,  3.36it/s] 56%|█████▌    | 438/785 [02:51<01:42,  3.39it/s] 56%|█████▌    | 439/785 [02:52<01:41,  3.41it/s] 56%|█████▌    | 440/785 [02:52<01:40,  3.43it/s] 56%|█████▌    | 441/785 [02:52<01:40,  3.44it/s] 56%|█████▋    | 442/785 [02:53<01:39,  3.44it/s] 56%|█████▋    | 443/785 [02:53<01:39,  3.45it/s] 57%|█████▋    | 444/785 [02:53<01:38,  3.45it/s] 57%|█████▋    | 445/785 [02:53<01:38,  3.46it/s] 57%|█████▋    | 446/785 [02:54<02:00,  2.80it/s] 57%|█████▋    | 447/785 [02:54<01:53,  2.97it/s] 57%|█████▋    | 448/785 [02:55<01:48,  3.10it/s] 57%|█████▋    | 449/785 [02:55<01:44,  3.20it/s] 57%|█████▋    | 450/785 [02:55<01:42,  3.28it/s] 57%|█████▋    | 451/785 [02:55<01:40,  3.33it/s] 58%|█████▊    | 452/785 [02:56<01:38,  3.37it/s] 58%|█████▊    | 453/785 [02:56<01:37,  3.39it/s] 58%|█████▊    | 454/785 [02:56<01:37,  3.41it/s] 58%|█████▊    | 455/785 [02:57<01:36,  3.43it/s] 58%|█████▊    | 456/785 [02:57<01:36,  3.42it/s] 58%|█████▊    | 457/785 [02:57<01:35,  3.44it/s] 58%|█████▊    | 458/785 [02:57<01:35,  3.44it/s] 58%|█████▊    | 459/785 [02:58<01:34,  3.45it/s] 59%|█████▊    | 460/785 [02:58<01:34,  3.45it/s] 59%|█████▊    | 461/785 [02:58<01:33,  3.45it/s] 59%|█████▉    | 462/785 [02:59<01:33,  3.46it/s] 59%|█████▉    | 463/785 [02:59<01:33,  3.46it/s] 59%|█████▉    | 464/785 [02:59<01:32,  3.46it/s] 59%|█████▉    | 465/785 [02:59<01:32,  3.46it/s] 59%|█████▉    | 466/785 [03:00<01:32,  3.46it/s] 59%|█████▉    | 467/785 [03:00<01:32,  3.42it/s] 60%|█████▉    | 468/785 [03:00<01:32,  3.43it/s] 60%|█████▉    | 469/785 [03:01<01:31,  3.44it/s] 60%|█████▉    | 470/785 [03:01<01:31,  3.45it/s] 60%|██████    | 471/785 [03:01<01:26,  3.61it/s][INFO|trainer.py:2140] 2023-08-28 17:54:47,322 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:54:47,323 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 17:54:47,323 >>   Batch size = 8
{'eval_loss': 0.9756343960762024, 'eval_runtime': 9.3334, 'eval_samples_per_second': 372.532, 'eval_steps_per_second': 46.607, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.06it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.56it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.80it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.85it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.56it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.32it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.96it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.86it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.76it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.63it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.71it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.67it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.69it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.75it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.74it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.60it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.69it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.56it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.65it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.70it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.63it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.71it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.69it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.64it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.70it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.68it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.68it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.72it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.66it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.59it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.66it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.67it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.71it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.71it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.61it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.59it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.65it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.68it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.67it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.71it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.59it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.59it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.65it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.68it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.61it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.66it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.63it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.67it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.70it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.69it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.68it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.70it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.61it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.59it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.60it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.67it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.64it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.70it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.70it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.59it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.64it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.64it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.67it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.73it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.61it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.58it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.66it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.60it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.64it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.69it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.67it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.36it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.49it/s][A
 86%|████████▌ | 373/435 [00:07<00:01, 46.57it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 45.76it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 41.30it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 33.77it/s][A
 90%|█████████ | 393/435 [00:08<00:01, 37.12it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 39.53it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 40.59it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 42.24it/s][A
 95%|█████████▍| 413/435 [00:09<00:00, 38.96it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 37.31it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 39.71it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 41.62it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 43.02it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 43.02it/s][A 60%|██████    | 471/785 [03:11<01:26,  3.61it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:54:56,909 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-471
[INFO|configuration_utils.py:351] 2023-08-28 17:54:56,930 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-471/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:55:00,847 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-471/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:55:00,869 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-471/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:55:00,886 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-471/special_tokens_map.json
 60%|██████    | 472/785 [03:31<47:37,  9.13s/it] 60%|██████    | 473/785 [03:31<33:57,  6.53s/it] 60%|██████    | 474/785 [03:32<24:08,  4.66s/it] 61%|██████    | 475/785 [03:32<17:17,  3.35s/it] 61%|██████    | 476/785 [03:32<12:30,  2.43s/it] 61%|██████    | 477/785 [03:33<09:10,  1.79s/it] 61%|██████    | 478/785 [03:33<06:50,  1.34s/it] 61%|██████    | 479/785 [03:33<05:12,  1.02s/it] 61%|██████    | 480/785 [03:33<04:04,  1.25it/s] 61%|██████▏   | 481/785 [03:34<03:16,  1.54it/s] 61%|██████▏   | 482/785 [03:34<02:43,  1.85it/s] 62%|██████▏   | 483/785 [03:34<02:21,  2.14it/s] 62%|██████▏   | 484/785 [03:35<02:04,  2.42it/s] 62%|██████▏   | 485/785 [03:35<01:52,  2.66it/s] 62%|██████▏   | 486/785 [03:35<01:44,  2.86it/s] 62%|██████▏   | 487/785 [03:35<01:38,  3.02it/s] 62%|██████▏   | 488/785 [03:36<01:34,  3.14it/s] 62%|██████▏   | 489/785 [03:36<01:31,  3.23it/s] 62%|██████▏   | 490/785 [03:36<01:29,  3.30it/s] 63%|██████▎   | 491/785 [03:37<01:27,  3.35it/s] 63%|██████▎   | 492/785 [03:37<01:26,  3.39it/s] 63%|██████▎   | 493/785 [03:37<01:25,  3.41it/s] 63%|██████▎   | 494/785 [03:38<01:29,  3.25it/s] 63%|██████▎   | 495/785 [03:38<01:27,  3.31it/s] 63%|██████▎   | 496/785 [03:38<01:26,  3.36it/s] 63%|██████▎   | 497/785 [03:38<01:24,  3.39it/s] 63%|██████▎   | 498/785 [03:39<01:24,  3.42it/s] 64%|██████▎   | 499/785 [03:39<01:23,  3.43it/s] 64%|██████▎   | 500/785 [03:39<01:22,  3.44it/s]                                                  64%|██████▎   | 500/785 [03:39<01:22,  3.44it/s] 64%|██████▍   | 501/785 [03:40<01:22,  3.45it/s] 64%|██████▍   | 502/785 [03:40<01:21,  3.45it/s] 64%|██████▍   | 503/785 [03:40<01:21,  3.46it/s] 64%|██████▍   | 504/785 [03:40<01:21,  3.46it/s] 64%|██████▍   | 505/785 [03:41<02:04,  2.25it/s] 64%|██████▍   | 506/785 [03:42<01:50,  2.52it/s] 65%|██████▍   | 507/785 [03:42<01:41,  2.75it/s] 65%|██████▍   | 508/785 [03:42<01:34,  2.93it/s] 65%|██████▍   | 509/785 [03:42<01:29,  3.07it/s] 65%|██████▍   | 510/785 [03:43<01:26,  3.18it/s] 65%|██████▌   | 511/785 [03:43<01:23,  3.26it/s] 65%|██████▌   | 512/785 [03:43<01:22,  3.33it/s] 65%|██████▌   | 513/785 [03:44<01:20,  3.37it/s] 65%|██████▌   | 514/785 [03:44<01:27,  3.10it/s] 66%|██████▌   | 515/785 [03:44<01:24,  3.21it/s] 66%|██████▌   | 516/785 [03:44<01:21,  3.28it/s] 66%|██████▌   | 517/785 [03:45<01:20,  3.34it/s] 66%|██████▌   | 518/785 [03:45<01:19,  3.38it/s] 66%|██████▌   | 519/785 [03:45<01:18,  3.41it/s] 66%|██████▌   | 520/785 [03:46<01:17,  3.43it/s] 66%|██████▋   | 521/785 [03:46<01:16,  3.44it/s] 66%|██████▋   | 522/785 [03:46<01:16,  3.45it/s] 67%|██████▋   | 523/785 [03:47<01:15,  3.46it/s] 67%|██████▋   | 524/785 [03:47<01:15,  3.46it/s] 67%|██████▋   | 525/785 [03:47<01:15,  3.44it/s] 67%|██████▋   | 526/785 [03:47<01:15,  3.45it/s] 67%|██████▋   | 527/785 [03:48<01:14,  3.46it/s] 67%|██████▋   | 528/785 [03:48<01:14,  3.46it/s] 67%|██████▋   | 529/785 [03:48<01:13,  3.47it/s] 68%|██████▊   | 530/785 [03:49<01:13,  3.47it/s] 68%|██████▊   | 531/785 [03:49<01:13,  3.47it/s] 68%|██████▊   | 532/785 [03:49<01:12,  3.47it/s] 68%|██████▊   | 533/785 [03:49<01:12,  3.47it/s] 68%|██████▊   | 534/785 [03:50<01:12,  3.47it/s] 68%|██████▊   | 535/785 [03:50<01:12,  3.47it/s] 68%|██████▊   | 536/785 [03:50<01:11,  3.47it/s] 68%|██████▊   | 537/785 [03:51<01:11,  3.47it/s] 69%|██████▊   | 538/785 [03:51<01:11,  3.47it/s] 69%|██████▊   | 539/785 [03:51<01:10,  3.47it/s] 69%|██████▉   | 540/785 [03:51<01:10,  3.47it/s] 69%|██████▉   | 541/785 [03:52<01:10,  3.47it/s] 69%|██████▉   | 542/785 [03:52<01:14,  3.28it/s] 69%|██████▉   | 543/785 [03:52<01:12,  3.34it/s] 69%|██████▉   | 544/785 [03:53<01:11,  3.38it/s] 69%|██████▉   | 545/785 [03:53<01:10,  3.40it/s] 70%|██████▉   | 546/785 [03:53<01:09,  3.42it/s] 70%|██████▉   | 547/785 [03:53<01:09,  3.44it/s] 70%|██████▉   | 548/785 [03:54<01:08,  3.45it/s] 70%|██████▉   | 549/785 [03:54<01:08,  3.45it/s] 70%|███████   | 550/785 [03:54<01:07,  3.46it/s] 70%|███████   | 551/785 [03:55<01:07,  3.46it/s] 70%|███████   | 552/785 [03:55<01:07,  3.47it/s] 70%|███████   | 553/785 [03:55<01:07,  3.45it/s] 71%|███████   | 554/785 [03:56<01:06,  3.45it/s] 71%|███████   | 555/785 [03:56<01:06,  3.46it/s] 71%|███████   | 556/785 [03:56<01:06,  3.46it/s] 71%|███████   | 557/785 [03:56<01:05,  3.46it/s] 71%|███████   | 558/785 [03:57<01:05,  3.46it/s] 71%|███████   | 559/785 [03:57<01:05,  3.46it/s] 71%|███████▏  | 560/785 [03:57<01:04,  3.47it/s] 71%|███████▏  | 561/785 [03:58<01:04,  3.47it/s] 72%|███████▏  | 562/785 [03:58<01:04,  3.47it/s] 72%|███████▏  | 563/785 [03:58<01:03,  3.47it/s] 72%|███████▏  | 564/785 [03:58<01:06,  3.33it/s] 72%|███████▏  | 565/785 [03:59<01:05,  3.37it/s] 72%|███████▏  | 566/785 [03:59<01:04,  3.40it/s] 72%|███████▏  | 567/785 [03:59<01:03,  3.42it/s] 72%|███████▏  | 568/785 [04:00<01:03,  3.44it/s] 72%|███████▏  | 569/785 [04:00<01:02,  3.45it/s] 73%|███████▎  | 570/785 [04:00<01:02,  3.45it/s] 73%|███████▎  | 571/785 [04:00<01:01,  3.46it/s] 73%|███████▎  | 572/785 [04:01<01:01,  3.46it/s] 73%|███████▎  | 573/785 [04:01<01:01,  3.46it/s] 73%|███████▎  | 574/785 [04:01<01:00,  3.47it/s] 73%|███████▎  | 575/785 [04:02<01:04,  3.25it/s] 73%|███████▎  | 576/785 [04:02<01:03,  3.32it/s] 74%|███████▎  | 577/785 [04:02<01:01,  3.36it/s] 74%|███████▎  | 578/785 [04:03<01:01,  3.39it/s] 74%|███████▍  | 579/785 [04:03<01:00,  3.42it/s] 74%|███████▍  | 580/785 [04:03<00:59,  3.43it/s] 74%|███████▍  | 581/785 [04:03<00:59,  3.44it/s] 74%|███████▍  | 582/785 [04:04<00:58,  3.45it/s] 74%|███████▍  | 583/785 [04:04<00:58,  3.46it/s] 74%|███████▍  | 584/785 [04:04<00:58,  3.46it/s] 75%|███████▍  | 585/785 [04:05<00:57,  3.46it/s] 75%|███████▍  | 586/785 [04:05<00:58,  3.41it/s] 75%|███████▍  | 587/785 [04:05<00:57,  3.42it/s] 75%|███████▍  | 588/785 [04:05<00:57,  3.43it/s] 75%|███████▌  | 589/785 [04:06<00:56,  3.44it/s] 75%|███████▌  | 590/785 [04:06<00:56,  3.45it/s] 75%|███████▌  | 591/785 [04:06<00:56,  3.45it/s] 75%|███████▌  | 592/785 [04:07<00:55,  3.45it/s] 76%|███████▌  | 593/785 [04:07<00:55,  3.46it/s] 76%|███████▌  | 594/785 [04:07<00:55,  3.46it/s] 76%|███████▌  | 595/785 [04:07<00:54,  3.46it/s] 76%|███████▌  | 596/785 [04:08<00:54,  3.46it/s] 76%|███████▌  | 597/785 [04:08<00:56,  3.35it/s] 76%|███████▌  | 598/785 [04:08<00:55,  3.39it/s] 76%|███████▋  | 599/785 [04:09<00:54,  3.41it/s] 76%|███████▋  | 600/785 [04:09<00:53,  3.43it/s] 77%|███████▋  | 601/785 [04:09<00:53,  3.44it/s] 77%|███████▋  | 602/785 [04:10<01:41,  1.81it/s] 77%|███████▋  | 603/785 [04:11<01:26,  2.11it/s] 77%|███████▋  | 604/785 [04:11<01:15,  2.39it/s] 77%|███████▋  | 605/785 [04:11<01:08,  2.62it/s] 77%|███████▋  | 606/785 [04:12<01:03,  2.83it/s] 77%|███████▋  | 607/785 [04:12<00:59,  2.99it/s] 77%|███████▋  | 608/785 [04:12<00:56,  3.12it/s] 78%|███████▊  | 609/785 [04:12<00:54,  3.22it/s] 78%|███████▊  | 610/785 [04:13<00:53,  3.29it/s] 78%|███████▊  | 611/785 [04:13<00:52,  3.34it/s] 78%|███████▊  | 612/785 [04:13<00:51,  3.37it/s] 78%|███████▊  | 613/785 [04:14<00:50,  3.40it/s] 78%|███████▊  | 614/785 [04:14<00:50,  3.42it/s] 78%|███████▊  | 615/785 [04:14<00:49,  3.43it/s] 78%|███████▊  | 616/785 [04:14<00:50,  3.33it/s] 79%|███████▊  | 617/785 [04:15<00:49,  3.37it/s] 79%|███████▊  | 618/785 [04:15<00:49,  3.39it/s] 79%|███████▉  | 619/785 [04:15<00:48,  3.41it/s] 79%|███████▉  | 620/785 [04:16<00:48,  3.43it/s] 79%|███████▉  | 621/785 [04:16<00:47,  3.44it/s] 79%|███████▉  | 622/785 [04:16<00:47,  3.44it/s] 79%|███████▉  | 623/785 [04:16<00:46,  3.45it/s] 79%|███████▉  | 624/785 [04:17<00:46,  3.45it/s] 80%|███████▉  | 625/785 [04:17<00:46,  3.45it/s] 80%|███████▉  | 626/785 [04:17<00:46,  3.45it/s] 80%|███████▉  | 627/785 [04:18<00:47,  3.33it/s] 80%|████████  | 628/785 [04:18<00:44,  3.52it/s][INFO|trainer.py:2140] 2023-08-28 17:56:04,061 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:56:04,062 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 17:56:04,062 >>   Batch size = 8
{'eval_loss': 0.9917402267456055, 'eval_runtime': 9.5608, 'eval_samples_per_second': 363.673, 'eval_steps_per_second': 45.498, 'epoch': 3.0}
{'loss': 0.6478, 'learning_rate': 1.3614649681528663e-05, 'epoch': 3.18}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.30it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.43it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.69it/s][A
  5%|▌         | 23/435 [00:00<00:08, 48.03it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.60it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.31it/s][A
  9%|▊         | 38/435 [00:00<00:08, 47.13it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.95it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.90it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.91it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.80it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.77it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.80it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.78it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.69it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.79it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.73it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.73it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.68it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.67it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.70it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.77it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.70it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.70it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.66it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.69it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.74it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.70it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.76it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.69it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.64it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.68it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.73it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.64it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.66it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.74it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.77it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.74it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.70it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.67it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.73it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.77it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.70it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.64it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.63it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.63it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.73it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.75it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.71it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.67it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.61it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.70it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.69it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.70it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.67it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.72it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.62it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.73it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.76it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.79it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.70it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.71it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.61it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.72it/s][A
 75%|███████▌  | 328/435 [00:06<00:02, 46.74it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.74it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.66it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.70it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.63it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.67it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 41.87it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 43.24it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 44.22it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 44.96it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 45.40it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 45.82it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.13it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.29it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.31it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.43it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.49it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.59it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.65it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.61it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.66it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.63it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 46.63it/s][A 80%|████████  | 628/785 [04:27<00:44,  3.52it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:56:13,519 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-628
[INFO|configuration_utils.py:351] 2023-08-28 17:56:13,577 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-628/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:56:18,045 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-628/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:56:18,083 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-628/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:56:18,165 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-628/special_tokens_map.json
 80%|████████  | 629/785 [04:42<19:26,  7.48s/it] 80%|████████  | 630/785 [04:43<13:54,  5.38s/it] 80%|████████  | 631/785 [04:43<09:53,  3.85s/it] 81%|████████  | 632/785 [04:43<07:05,  2.78s/it] 81%|████████  | 633/785 [04:44<05:09,  2.04s/it] 81%|████████  | 634/785 [04:44<03:48,  1.51s/it] 81%|████████  | 635/785 [04:44<02:51,  1.14s/it] 81%|████████  | 636/785 [04:44<02:12,  1.13it/s] 81%|████████  | 637/785 [04:45<01:44,  1.41it/s] 81%|████████▏ | 638/785 [04:45<01:25,  1.72it/s] 81%|████████▏ | 639/785 [04:45<01:12,  2.03it/s] 82%|████████▏ | 640/785 [04:46<01:03,  2.30it/s] 82%|████████▏ | 641/785 [04:46<00:56,  2.56it/s] 82%|████████▏ | 642/785 [04:46<00:51,  2.78it/s] 82%|████████▏ | 643/785 [04:46<00:48,  2.95it/s] 82%|████████▏ | 644/785 [04:47<00:45,  3.09it/s] 82%|████████▏ | 645/785 [04:47<00:43,  3.19it/s] 82%|████████▏ | 646/785 [04:47<00:42,  3.27it/s] 82%|████████▏ | 647/785 [04:48<00:41,  3.33it/s] 83%|████████▎ | 648/785 [04:48<00:40,  3.37it/s] 83%|████████▎ | 649/785 [04:48<00:39,  3.40it/s] 83%|████████▎ | 650/785 [04:48<00:39,  3.42it/s] 83%|████████▎ | 651/785 [04:49<00:41,  3.27it/s] 83%|████████▎ | 652/785 [04:49<00:39,  3.33it/s] 83%|████████▎ | 653/785 [04:49<00:39,  3.37it/s] 83%|████████▎ | 654/785 [04:50<00:38,  3.40it/s] 83%|████████▎ | 655/785 [04:50<00:37,  3.42it/s] 84%|████████▎ | 656/785 [04:50<00:37,  3.44it/s] 84%|████████▎ | 657/785 [04:51<00:37,  3.45it/s] 84%|████████▍ | 658/785 [04:51<00:36,  3.46it/s] 84%|████████▍ | 659/785 [04:51<00:36,  3.46it/s] 84%|████████▍ | 660/785 [04:51<00:36,  3.47it/s] 84%|████████▍ | 661/785 [04:52<00:35,  3.47it/s] 84%|████████▍ | 662/785 [04:52<00:35,  3.47it/s] 84%|████████▍ | 663/785 [04:52<00:35,  3.47it/s] 85%|████████▍ | 664/785 [04:53<00:34,  3.47it/s] 85%|████████▍ | 665/785 [04:53<00:34,  3.47it/s] 85%|████████▍ | 666/785 [04:53<00:34,  3.47it/s] 85%|████████▍ | 667/785 [04:53<00:34,  3.46it/s] 85%|████████▌ | 668/785 [04:54<00:33,  3.46it/s] 85%|████████▌ | 669/785 [04:54<00:33,  3.46it/s] 85%|████████▌ | 670/785 [04:54<00:33,  3.46it/s] 85%|████████▌ | 671/785 [04:55<00:32,  3.47it/s] 86%|████████▌ | 672/785 [04:55<00:32,  3.47it/s] 86%|████████▌ | 673/785 [04:55<00:32,  3.47it/s] 86%|████████▌ | 674/785 [04:55<00:32,  3.47it/s] 86%|████████▌ | 675/785 [04:56<00:31,  3.47it/s] 86%|████████▌ | 676/785 [04:56<00:31,  3.47it/s] 86%|████████▌ | 677/785 [04:56<00:31,  3.47it/s] 86%|████████▋ | 678/785 [04:57<00:30,  3.45it/s] 86%|████████▋ | 679/785 [04:57<00:30,  3.46it/s] 87%|████████▋ | 680/785 [04:57<00:30,  3.46it/s] 87%|████████▋ | 681/785 [04:57<00:29,  3.47it/s] 87%|████████▋ | 682/785 [04:58<00:29,  3.47it/s] 87%|████████▋ | 683/785 [04:58<00:29,  3.47it/s] 87%|████████▋ | 684/785 [04:58<00:29,  3.47it/s] 87%|████████▋ | 685/785 [04:59<00:28,  3.47it/s] 87%|████████▋ | 686/785 [04:59<00:28,  3.47it/s] 88%|████████▊ | 687/785 [04:59<00:28,  3.47it/s] 88%|████████▊ | 688/785 [04:59<00:27,  3.47it/s] 88%|████████▊ | 689/785 [05:00<00:28,  3.43it/s] 88%|████████▊ | 690/785 [05:00<00:27,  3.44it/s] 88%|████████▊ | 691/785 [05:00<00:27,  3.45it/s] 88%|████████▊ | 692/785 [05:01<00:26,  3.45it/s] 88%|████████▊ | 693/785 [05:01<00:26,  3.46it/s] 88%|████████▊ | 694/785 [05:01<00:26,  3.46it/s] 89%|████████▊ | 695/785 [05:01<00:25,  3.46it/s] 89%|████████▊ | 696/785 [05:02<00:25,  3.47it/s] 89%|████████▉ | 697/785 [05:02<00:25,  3.47it/s] 89%|████████▉ | 698/785 [05:02<00:25,  3.47it/s] 89%|████████▉ | 699/785 [05:03<00:24,  3.47it/s] 89%|████████▉ | 700/785 [05:03<00:26,  3.16it/s] 89%|████████▉ | 701/785 [05:03<00:25,  3.25it/s] 89%|████████▉ | 702/785 [05:04<00:25,  3.31it/s] 90%|████████▉ | 703/785 [05:04<00:24,  3.36it/s] 90%|████████▉ | 704/785 [05:04<00:23,  3.39it/s] 90%|████████▉ | 705/785 [05:04<00:23,  3.42it/s] 90%|████████▉ | 706/785 [05:05<00:23,  3.43it/s] 90%|█████████ | 707/785 [05:05<00:22,  3.44it/s] 90%|█████████ | 708/785 [05:05<00:22,  3.45it/s] 90%|█████████ | 709/785 [05:06<00:22,  3.45it/s] 90%|█████████ | 710/785 [05:06<00:21,  3.45it/s] 91%|█████████ | 711/785 [05:06<00:21,  3.45it/s] 91%|█████████ | 712/785 [05:06<00:21,  3.45it/s] 91%|█████████ | 713/785 [05:07<00:20,  3.46it/s] 91%|█████████ | 714/785 [05:07<00:20,  3.46it/s] 91%|█████████ | 715/785 [05:07<00:20,  3.46it/s] 91%|█████████ | 716/785 [05:08<00:19,  3.47it/s] 91%|█████████▏| 717/785 [05:08<00:19,  3.47it/s] 91%|█████████▏| 718/785 [05:08<00:19,  3.47it/s] 92%|█████████▏| 719/785 [05:08<00:19,  3.47it/s] 92%|█████████▏| 720/785 [05:09<00:18,  3.47it/s] 92%|█████████▏| 721/785 [05:09<00:18,  3.47it/s] 92%|█████████▏| 722/785 [05:11<00:44,  1.42it/s] 92%|█████████▏| 723/785 [05:11<00:36,  1.72it/s] 92%|█████████▏| 724/785 [05:11<00:30,  2.02it/s] 92%|█████████▏| 725/785 [05:12<00:25,  2.31it/s] 92%|█████████▏| 726/785 [05:12<00:22,  2.57it/s] 93%|█████████▎| 727/785 [05:12<00:20,  2.79it/s] 93%|█████████▎| 728/785 [05:13<00:21,  2.66it/s] 93%|█████████▎| 729/785 [05:13<00:19,  2.86it/s] 93%|█████████▎| 730/785 [05:13<00:18,  3.02it/s] 93%|█████████▎| 731/785 [05:13<00:17,  3.14it/s] 93%|█████████▎| 732/785 [05:14<00:16,  3.23it/s] 93%|█████████▎| 733/785 [05:14<00:15,  3.30it/s] 94%|█████████▎| 734/785 [05:14<00:15,  3.35it/s] 94%|█████████▎| 735/785 [05:15<00:14,  3.38it/s] 94%|█████████▍| 736/785 [05:15<00:14,  3.40it/s] 94%|█████████▍| 737/785 [05:15<00:14,  3.42it/s] 94%|█████████▍| 738/785 [05:16<00:14,  3.30it/s] 94%|█████████▍| 739/785 [05:16<00:13,  3.35it/s] 94%|█████████▍| 740/785 [05:16<00:13,  3.38it/s] 94%|█████████▍| 741/785 [05:16<00:12,  3.40it/s] 95%|█████████▍| 742/785 [05:17<00:12,  3.42it/s] 95%|█████████▍| 743/785 [05:17<00:12,  3.43it/s] 95%|█████████▍| 744/785 [05:17<00:11,  3.44it/s] 95%|█████████▍| 745/785 [05:18<00:11,  3.44it/s] 95%|█████████▌| 746/785 [05:18<00:11,  3.45it/s] 95%|█████████▌| 747/785 [05:18<00:11,  3.45it/s] 95%|█████████▌| 748/785 [05:18<00:10,  3.45it/s] 95%|█████████▌| 749/785 [05:19<00:10,  3.29it/s] 96%|█████████▌| 750/785 [05:19<00:10,  3.34it/s] 96%|█████████▌| 751/785 [05:19<00:10,  3.38it/s] 96%|█████████▌| 752/785 [05:20<00:09,  3.40it/s] 96%|█████████▌| 753/785 [05:20<00:09,  3.42it/s] 96%|█████████▌| 754/785 [05:20<00:09,  3.43it/s] 96%|█████████▌| 755/785 [05:21<00:08,  3.44it/s] 96%|█████████▋| 756/785 [05:21<00:08,  3.44it/s] 96%|█████████▋| 757/785 [05:21<00:08,  3.31it/s] 97%|█████████▋| 758/785 [05:21<00:08,  3.35it/s] 97%|█████████▋| 759/785 [05:22<00:07,  3.39it/s] 97%|█████████▋| 760/785 [05:22<00:07,  3.41it/s] 97%|█████████▋| 761/785 [05:22<00:07,  3.42it/s] 97%|█████████▋| 762/785 [05:23<00:06,  3.44it/s] 97%|█████████▋| 763/785 [05:23<00:06,  3.44it/s] 97%|█████████▋| 764/785 [05:23<00:06,  3.45it/s] 97%|█████████▋| 765/785 [05:23<00:05,  3.45it/s] 98%|█████████▊| 766/785 [05:24<00:05,  3.46it/s] 98%|█████████▊| 767/785 [05:24<00:05,  3.46it/s] 98%|█████████▊| 768/785 [05:24<00:04,  3.44it/s] 98%|█████████▊| 769/785 [05:25<00:04,  3.45it/s] 98%|█████████▊| 770/785 [05:25<00:04,  3.45it/s] 98%|█████████▊| 771/785 [05:25<00:04,  3.45it/s] 98%|█████████▊| 772/785 [05:25<00:03,  3.46it/s] 98%|█████████▊| 773/785 [05:26<00:03,  3.46it/s] 99%|█████████▊| 774/785 [05:26<00:03,  3.46it/s] 99%|█████████▊| 775/785 [05:26<00:02,  3.46it/s] 99%|█████████▉| 776/785 [05:27<00:02,  3.46it/s] 99%|█████████▉| 777/785 [05:27<00:02,  3.46it/s] 99%|█████████▉| 778/785 [05:27<00:02,  3.46it/s] 99%|█████████▉| 779/785 [05:27<00:01,  3.41it/s] 99%|█████████▉| 780/785 [05:28<00:01,  3.43it/s] 99%|█████████▉| 781/785 [05:28<00:01,  3.44it/s]100%|█████████▉| 782/785 [05:28<00:00,  3.45it/s]100%|█████████▉| 783/785 [05:29<00:00,  3.45it/s]100%|█████████▉| 784/785 [05:29<00:00,  3.45it/s]100%|██████████| 785/785 [05:29<00:00,  3.62it/s][INFO|trainer.py:2140] 2023-08-28 17:57:15,325 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:57:15,325 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 17:57:15,325 >>   Batch size = 8
{'eval_loss': 1.001243233680725, 'eval_runtime': 9.3505, 'eval_samples_per_second': 371.852, 'eval_steps_per_second': 46.522, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.76it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.46it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.72it/s][A
  5%|▌         | 23/435 [00:00<00:08, 48.06it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.56it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.16it/s][A
  9%|▊         | 38/435 [00:00<00:08, 47.13it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.97it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.91it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 45.94it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.06it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 46.20it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.40it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.52it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.61it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.60it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.65it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.61it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.61it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.68it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.70it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.60it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.63it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.61it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.64it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.67it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.72it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.68it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.71it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.65it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.66it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.68it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.62it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.65it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.66it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.70it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.63it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 42.36it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 43.60it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 44.48it/s][A
 48%|████▊     | 208/435 [00:04<00:05, 45.09it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 45.48it/s][A
 50%|█████     | 218/435 [00:04<00:04, 45.90it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.13it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.34it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 46.37it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.32it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.35it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.45it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.53it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.64it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.71it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.60it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.60it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.62it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.64it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.70it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.69it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.65it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.64it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.71it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.73it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.66it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.66it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.62it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.66it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.66it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.69it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.70it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.68it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.64it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.72it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.70it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.66it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.68it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.74it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.75it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.61it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.66it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.68it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.72it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.73it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.64it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.64it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.66it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.58it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 46.58it/s][A100%|██████████| 785/785 [05:39<00:00,  3.62it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:57:24,702 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-785
[INFO|configuration_utils.py:351] 2023-08-28 17:57:24,721 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-785/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:57:28,900 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-785/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:57:28,932 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-785/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:57:28,954 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-785/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 17:57:40,379 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 17:57:40,379 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-157 (score: 0.9571003913879395).
                                                 100%|██████████| 785/785 [06:07<00:00,  3.62it/s]100%|██████████| 785/785 [06:07<00:00,  2.14it/s]
[INFO|trainer.py:1894] 2023-08-28 17:57:52,820 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 17:57:52,838 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:57:58,745 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:57:58,763 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:57:58,774 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:57:58,967 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:58,967 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:58,967 >>   train_loss               =     0.6366
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:58,967 >>   train_runtime            = 0:06:07.17
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:58,967 >>   train_samples            =      10037
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:58,967 >>   train_samples_per_second =    136.678
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:58,967 >>   train_steps_per_second   =      2.138
{'eval_loss': 1.0057296752929688, 'eval_runtime': 9.3597, 'eval_samples_per_second': 371.485, 'eval_steps_per_second': 46.476, 'epoch': 5.0}
{'train_runtime': 367.1781, 'train_samples_per_second': 136.678, 'train_steps_per_second': 2.138, 'train_loss': 0.6366399096835191, 'epoch': 5.0}
08/28/2023 17:57:58 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 17:57:59,003 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:57:59,003 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 17:57:59,003 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|▏         | 6/435 [00:00<00:07, 58.87it/s]  3%|▎         | 12/435 [00:00<00:08, 51.58it/s]  4%|▍         | 18/435 [00:00<00:08, 49.48it/s]  5%|▌         | 23/435 [00:00<00:08, 48.63it/s]  6%|▋         | 28/435 [00:00<00:08, 48.23it/s]  8%|▊         | 33/435 [00:00<00:08, 47.89it/s]  9%|▊         | 38/435 [00:00<00:08, 47.70it/s] 10%|▉         | 43/435 [00:00<00:08, 47.55it/s] 11%|█         | 48/435 [00:00<00:08, 47.30it/s] 12%|█▏        | 53/435 [00:01<00:08, 47.10it/s] 13%|█▎        | 58/435 [00:01<00:07, 47.23it/s] 14%|█▍        | 63/435 [00:01<00:07, 47.22it/s] 16%|█▌        | 68/435 [00:01<00:07, 47.25it/s] 17%|█▋        | 73/435 [00:01<00:07, 47.24it/s] 18%|█▊        | 78/435 [00:01<00:07, 47.28it/s] 19%|█▉        | 83/435 [00:01<00:07, 47.33it/s] 20%|██        | 88/435 [00:01<00:07, 47.17it/s] 21%|██▏       | 93/435 [00:01<00:07, 47.13it/s] 23%|██▎       | 98/435 [00:02<00:07, 47.17it/s] 24%|██▎       | 103/435 [00:02<00:07, 47.19it/s] 25%|██▍       | 108/435 [00:02<00:06, 47.09it/s] 26%|██▌       | 113/435 [00:02<00:06, 47.18it/s] 27%|██▋       | 118/435 [00:02<00:06, 47.12it/s] 28%|██▊       | 123/435 [00:02<00:06, 47.15it/s] 29%|██▉       | 128/435 [00:02<00:06, 47.14it/s] 31%|███       | 133/435 [00:02<00:06, 47.19it/s] 32%|███▏      | 138/435 [00:02<00:06, 47.13it/s] 33%|███▎      | 143/435 [00:03<00:06, 47.19it/s] 34%|███▍      | 148/435 [00:03<00:06, 47.16it/s] 35%|███▌      | 153/435 [00:03<00:05, 47.02it/s] 36%|███▋      | 158/435 [00:03<00:05, 47.11it/s] 37%|███▋      | 163/435 [00:03<00:05, 47.10it/s] 39%|███▊      | 168/435 [00:03<00:05, 47.17it/s] 40%|███▉      | 173/435 [00:03<00:05, 47.21it/s] 41%|████      | 178/435 [00:03<00:05, 47.24it/s] 42%|████▏     | 183/435 [00:03<00:05, 47.03it/s] 43%|████▎     | 188/435 [00:03<00:05, 47.01it/s] 44%|████▍     | 193/435 [00:04<00:05, 47.03it/s] 46%|████▌     | 198/435 [00:04<00:05, 47.04it/s] 47%|████▋     | 203/435 [00:04<00:04, 46.99it/s] 48%|████▊     | 208/435 [00:04<00:04, 47.15it/s] 49%|████▉     | 213/435 [00:04<00:04, 47.10it/s] 50%|█████     | 218/435 [00:04<00:04, 47.07it/s] 51%|█████▏    | 223/435 [00:04<00:04, 47.11it/s] 52%|█████▏    | 228/435 [00:04<00:04, 47.06it/s] 54%|█████▎    | 233/435 [00:04<00:04, 47.10it/s] 55%|█████▍    | 238/435 [00:05<00:04, 47.08it/s] 56%|█████▌    | 243/435 [00:05<00:04, 47.03it/s] 57%|█████▋    | 248/435 [00:05<00:03, 46.98it/s] 58%|█████▊    | 253/435 [00:05<00:03, 47.00it/s] 59%|█████▉    | 258/435 [00:05<00:03, 47.06it/s] 60%|██████    | 263/435 [00:05<00:03, 47.08it/s] 62%|██████▏   | 268/435 [00:05<00:03, 47.05it/s] 63%|██████▎   | 273/435 [00:05<00:03, 47.00it/s] 64%|██████▍   | 278/435 [00:05<00:03, 46.99it/s] 65%|██████▌   | 283/435 [00:05<00:03, 47.00it/s] 66%|██████▌   | 288/435 [00:06<00:03, 46.95it/s] 67%|██████▋   | 293/435 [00:06<00:03, 46.97it/s] 69%|██████▊   | 298/435 [00:06<00:02, 46.94it/s] 70%|██████▉   | 303/435 [00:06<00:02, 47.05it/s] 71%|███████   | 308/435 [00:06<00:02, 47.10it/s] 72%|███████▏  | 313/435 [00:06<00:02, 46.99it/s] 73%|███████▎  | 318/435 [00:06<00:02, 47.00it/s] 74%|███████▍  | 323/435 [00:06<00:02, 47.08it/s] 75%|███████▌  | 328/435 [00:06<00:02, 47.06it/s] 77%|███████▋  | 333/435 [00:07<00:02, 47.09it/s] 78%|███████▊  | 338/435 [00:07<00:02, 47.09it/s] 79%|███████▉  | 343/435 [00:07<00:01, 47.13it/s] 80%|████████  | 348/435 [00:07<00:01, 46.95it/s] 81%|████████  | 353/435 [00:07<00:01, 47.03it/s] 82%|████████▏ | 358/435 [00:07<00:01, 47.08it/s] 83%|████████▎ | 363/435 [00:07<00:01, 47.06it/s] 85%|████████▍ | 368/435 [00:07<00:01, 47.04it/s] 86%|████████▌ | 373/435 [00:07<00:01, 47.04it/s] 87%|████████▋ | 378/435 [00:08<00:01, 47.00it/s] 88%|████████▊ | 383/435 [00:08<00:01, 47.01it/s] 89%|████████▉ | 388/435 [00:08<00:00, 47.01it/s] 90%|█████████ | 393/435 [00:08<00:00, 46.98it/s] 91%|█████████▏| 398/435 [00:08<00:00, 47.06it/s] 93%|█████████▎| 403/435 [00:08<00:00, 47.06it/s] 94%|█████████▍| 408/435 [00:08<00:00, 46.97it/s] 95%|█████████▍| 413/435 [00:08<00:00, 46.98it/s] 96%|█████████▌| 418/435 [00:08<00:00, 47.01it/s] 97%|█████████▋| 423/435 [00:08<00:00, 46.94it/s] 98%|█████████▊| 428/435 [00:09<00:00, 47.03it/s]100%|█████████▉| 433/435 [00:09<00:00, 46.95it/s]100%|██████████| 435/435 [00:09<00:00, 47.20it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:58:08,243 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:58:08,243 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:58:08,243 >>   eval_loss               =     0.9571
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:58:08,243 >>   eval_runtime            = 0:00:09.23
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:58:08,243 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:58:08,243 >>   eval_samples_per_second =    376.311
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:58:08,243 >>   eval_steps_per_second   =     47.079
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:58:08,243 >>   perplexity              =     2.6041
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:16,902 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:16,925 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:16,925 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:16,925 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:16,925 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:58:17,241 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:58:17,242 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:58:17,525 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:58:18,554 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:58:18,554 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:19,934 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:19,940 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:19,940 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:19,940 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:19,940 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:58:20,262 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:58:20,263 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:58:20,525 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:58:20,667 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:58:20,667 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-628
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-785
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-314
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-471
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-157
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:02,  1.45it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:03,  1.49it/s]Extractor Predicting: 6it [00:04,  1.49it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.49it/s]Extractor Predicting: 10it [00:06,  1.46it/s]Extractor Predicting: 11it [00:07,  1.45it/s]Extractor Predicting: 12it [00:08,  1.45it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.45it/s]Extractor Predicting: 15it [00:10,  1.46it/s]Extractor Predicting: 16it [00:10,  1.46it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:12,  1.50it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:14,  1.50it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:15,  1.53it/s]Extractor Predicting: 24it [00:16,  1.54it/s]Extractor Predicting: 25it [00:16,  1.50it/s]Extractor Predicting: 26it [00:17,  1.48it/s]Extractor Predicting: 27it [00:18,  1.49it/s]Extractor Predicting: 28it [00:18,  1.46it/s]Extractor Predicting: 29it [00:19,  1.45it/s]Extractor Predicting: 30it [00:20,  1.43it/s]Extractor Predicting: 31it [00:20,  1.44it/s]Extractor Predicting: 32it [00:21,  1.45it/s]Extractor Predicting: 33it [00:22,  1.43it/s]Extractor Predicting: 34it [00:23,  1.46it/s]Extractor Predicting: 35it [00:23,  1.45it/s]Extractor Predicting: 36it [00:24,  1.49it/s]Extractor Predicting: 37it [00:25,  1.49it/s]Extractor Predicting: 38it [00:25,  1.45it/s]Extractor Predicting: 39it [00:26,  1.47it/s]Extractor Predicting: 40it [00:27,  1.47it/s]Extractor Predicting: 41it [00:27,  1.50it/s]Extractor Predicting: 42it [00:28,  1.50it/s]Extractor Predicting: 43it [00:29,  1.48it/s]Extractor Predicting: 44it [00:29,  1.48it/s]Extractor Predicting: 45it [00:30,  1.43it/s]Extractor Predicting: 46it [00:31,  1.46it/s]Extractor Predicting: 47it [00:31,  1.48it/s]Extractor Predicting: 48it [00:32,  1.49it/s]Extractor Predicting: 49it [00:33,  1.50it/s]Extractor Predicting: 50it [00:33,  1.51it/s]Extractor Predicting: 51it [00:34,  1.52it/s]Extractor Predicting: 52it [00:35,  1.48it/s]Extractor Predicting: 53it [00:35,  1.48it/s]Extractor Predicting: 54it [00:36,  1.50it/s]Extractor Predicting: 55it [00:37,  1.32it/s]Extractor Predicting: 56it [00:38,  1.36it/s]Extractor Predicting: 57it [00:38,  1.39it/s]Extractor Predicting: 58it [00:39,  1.43it/s]Extractor Predicting: 59it [00:40,  1.48it/s]Extractor Predicting: 60it [00:40,  1.40it/s]Extractor Predicting: 61it [00:41,  1.44it/s]Extractor Predicting: 62it [00:42,  1.48it/s]Extractor Predicting: 63it [00:42,  1.49it/s]Extractor Predicting: 64it [00:43,  1.51it/s]Extractor Predicting: 65it [00:44,  1.51it/s]Extractor Predicting: 66it [00:44,  1.50it/s]Extractor Predicting: 67it [00:45,  1.51it/s]Extractor Predicting: 68it [00:46,  1.52it/s]Extractor Predicting: 69it [00:46,  1.52it/s]Extractor Predicting: 70it [00:47,  1.51it/s]Extractor Predicting: 71it [00:48,  1.50it/s]Extractor Predicting: 72it [00:48,  1.52it/s]Extractor Predicting: 73it [00:49,  1.52it/s]Extractor Predicting: 74it [00:50,  1.54it/s]Extractor Predicting: 75it [00:50,  1.54it/s]Extractor Predicting: 76it [00:51,  1.52it/s]Extractor Predicting: 77it [00:52,  1.49it/s]Extractor Predicting: 78it [00:52,  1.49it/s]Extractor Predicting: 79it [00:53,  1.47it/s]Extractor Predicting: 80it [00:54,  1.45it/s]Extractor Predicting: 81it [00:54,  1.44it/s]Extractor Predicting: 82it [00:55,  1.47it/s]Extractor Predicting: 83it [00:56,  1.37it/s]Extractor Predicting: 84it [00:57,  1.41it/s]Extractor Predicting: 85it [00:57,  1.39it/s]Extractor Predicting: 86it [00:58,  1.42it/s]Extractor Predicting: 87it [00:59,  1.43it/s]Extractor Predicting: 88it [00:59,  1.43it/s]Extractor Predicting: 89it [01:00,  1.47it/s]Extractor Predicting: 90it [01:01,  1.39it/s]Extractor Predicting: 91it [01:01,  1.46it/s]Extractor Predicting: 92it [01:02,  1.50it/s]Extractor Predicting: 93it [01:03,  1.53it/s]Extractor Predicting: 94it [01:03,  1.51it/s]Extractor Predicting: 95it [01:04,  1.53it/s]Extractor Predicting: 96it [01:05,  1.53it/s]Extractor Predicting: 97it [01:05,  1.53it/s]Extractor Predicting: 98it [01:06,  1.53it/s]Extractor Predicting: 99it [01:07,  1.50it/s]Extractor Predicting: 100it [01:08,  1.13it/s]Extractor Predicting: 101it [01:09,  1.21it/s]Extractor Predicting: 102it [01:09,  1.34it/s]Extractor Predicting: 103it [01:10,  1.41it/s]Extractor Predicting: 104it [01:11,  1.42it/s]Extractor Predicting: 105it [01:11,  1.46it/s]Extractor Predicting: 106it [01:12,  1.48it/s]Extractor Predicting: 107it [01:12,  1.51it/s]Extractor Predicting: 108it [01:13,  1.51it/s]Extractor Predicting: 109it [01:14,  1.51it/s]Extractor Predicting: 110it [01:14,  1.53it/s]Extractor Predicting: 111it [01:15,  1.54it/s]Extractor Predicting: 112it [01:16,  1.54it/s]Extractor Predicting: 113it [01:16,  1.57it/s]Extractor Predicting: 114it [01:17,  1.54it/s]Extractor Predicting: 115it [01:18,  1.54it/s]Extractor Predicting: 116it [01:18,  1.56it/s]Extractor Predicting: 117it [01:19,  1.57it/s]Extractor Predicting: 118it [01:20,  1.56it/s]Extractor Predicting: 119it [01:20,  1.57it/s]Extractor Predicting: 120it [01:21,  1.51it/s]Extractor Predicting: 121it [01:22,  1.49it/s]Extractor Predicting: 122it [01:22,  1.50it/s]Extractor Predicting: 123it [01:23,  1.51it/s]Extractor Predicting: 124it [01:24,  1.49it/s]Extractor Predicting: 125it [01:24,  1.50it/s]Extractor Predicting: 126it [01:25,  1.52it/s]Extractor Predicting: 127it [01:26,  1.51it/s]Extractor Predicting: 128it [01:26,  1.52it/s]Extractor Predicting: 129it [01:27,  1.52it/s]Extractor Predicting: 130it [01:28,  1.49it/s]Extractor Predicting: 131it [01:28,  1.50it/s]Extractor Predicting: 132it [01:29,  1.48it/s]Extractor Predicting: 133it [01:30,  1.51it/s]Extractor Predicting: 134it [01:30,  1.54it/s]Extractor Predicting: 135it [01:31,  1.50it/s]Extractor Predicting: 136it [01:32,  1.48it/s]Extractor Predicting: 137it [01:32,  1.52it/s]Extractor Predicting: 138it [01:33,  1.51it/s]Extractor Predicting: 139it [01:34,  1.49it/s]Extractor Predicting: 140it [01:34,  1.46it/s]Extractor Predicting: 141it [01:35,  1.50it/s]Extractor Predicting: 142it [01:36,  1.49it/s]Extractor Predicting: 143it [01:36,  1.52it/s]Extractor Predicting: 144it [01:37,  1.55it/s]Extractor Predicting: 144it [01:37,  1.48it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:13,349 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:13,351 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:13,351 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:13,351 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:13,351 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:00:14,091 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:00:14,092 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:00:14,787 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:00:15,860 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:00:15,860 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:18,844 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:18,849 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:18,850 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:18,850 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:00:18,850 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:00:19,485 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:00:19,487 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:00:20,231 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:00:20,386 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:00:20,386 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.40993071593533487,
  "recall": 0.10209951107276388,
  "score": 0.16348146442551234,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.50it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:07,  1.55it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:11,  1.53it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.59it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:19,  1.57it/s]Extractor Predicting: 31it [00:20,  1.56it/s]Extractor Predicting: 32it [00:20,  1.61it/s]Extractor Predicting: 33it [00:21,  1.57it/s]Extractor Predicting: 34it [00:21,  1.55it/s]Extractor Predicting: 35it [00:22,  1.55it/s]Extractor Predicting: 36it [00:23,  1.56it/s]Extractor Predicting: 37it [00:23,  1.55it/s]Extractor Predicting: 38it [00:24,  1.57it/s]Extractor Predicting: 39it [00:25,  1.57it/s]Extractor Predicting: 40it [00:25,  1.56it/s]Extractor Predicting: 41it [00:26,  1.57it/s]Extractor Predicting: 42it [00:27,  1.59it/s]Extractor Predicting: 43it [00:27,  1.54it/s]Extractor Predicting: 44it [00:28,  1.55it/s]Extractor Predicting: 45it [00:29,  1.53it/s]Extractor Predicting: 46it [00:29,  1.51it/s]Extractor Predicting: 47it [00:30,  1.54it/s]Extractor Predicting: 48it [00:31,  1.53it/s]Extractor Predicting: 49it [00:31,  1.55it/s]Extractor Predicting: 50it [00:32,  1.55it/s]Extractor Predicting: 51it [00:32,  1.53it/s]Extractor Predicting: 52it [00:33,  1.45it/s]Extractor Predicting: 53it [00:34,  1.49it/s]Extractor Predicting: 54it [00:35,  1.48it/s]Extractor Predicting: 55it [00:35,  1.53it/s]Extractor Predicting: 56it [00:36,  1.53it/s]Extractor Predicting: 57it [00:37,  1.48it/s]Extractor Predicting: 58it [00:37,  1.47it/s]Extractor Predicting: 59it [00:38,  1.46it/s]Extractor Predicting: 60it [00:39,  1.49it/s]Extractor Predicting: 61it [00:39,  1.49it/s]Extractor Predicting: 62it [00:40,  1.49it/s]Extractor Predicting: 63it [00:41,  1.51it/s]Extractor Predicting: 64it [00:41,  1.50it/s]Extractor Predicting: 65it [00:42,  1.55it/s]Extractor Predicting: 66it [00:43,  1.52it/s]Extractor Predicting: 67it [00:43,  1.50it/s]Extractor Predicting: 68it [00:44,  1.49it/s]Extractor Predicting: 69it [00:45,  1.50it/s]Extractor Predicting: 70it [00:45,  1.47it/s]Extractor Predicting: 71it [00:46,  1.49it/s]Extractor Predicting: 72it [00:47,  1.33it/s]Extractor Predicting: 73it [00:48,  1.35it/s]Extractor Predicting: 74it [00:48,  1.40it/s]Extractor Predicting: 75it [00:49,  1.42it/s]Extractor Predicting: 76it [00:50,  1.45it/s]Extractor Predicting: 77it [00:50,  1.46it/s]Extractor Predicting: 78it [00:51,  1.49it/s]Extractor Predicting: 79it [00:52,  1.49it/s]Extractor Predicting: 80it [00:52,  1.54it/s]Extractor Predicting: 81it [00:53,  1.55it/s]Extractor Predicting: 82it [00:53,  1.54it/s]Extractor Predicting: 83it [00:54,  1.53it/s]Extractor Predicting: 84it [00:55,  1.53it/s]Extractor Predicting: 85it [00:55,  1.50it/s]Extractor Predicting: 86it [00:56,  1.46it/s]Extractor Predicting: 87it [00:57,  1.43it/s]Extractor Predicting: 88it [00:58,  1.46it/s]Extractor Predicting: 89it [00:58,  1.45it/s]Extractor Predicting: 90it [00:59,  1.42it/s]Extractor Predicting: 91it [01:00,  1.42it/s]Extractor Predicting: 92it [01:00,  1.44it/s]Extractor Predicting: 93it [01:01,  1.45it/s]Extractor Predicting: 94it [01:02,  1.47it/s]Extractor Predicting: 95it [01:02,  1.47it/s]Extractor Predicting: 96it [01:03,  1.47it/s]Extractor Predicting: 97it [01:04,  1.45it/s]Extractor Predicting: 98it [01:05,  1.42it/s]Extractor Predicting: 99it [01:05,  1.43it/s]Extractor Predicting: 100it [01:06,  1.46it/s]Extractor Predicting: 101it [01:07,  1.47it/s]Extractor Predicting: 102it [01:07,  1.48it/s]Extractor Predicting: 103it [01:08,  1.46it/s]Extractor Predicting: 104it [01:09,  1.45it/s]Extractor Predicting: 105it [01:09,  1.47it/s]Extractor Predicting: 106it [01:10,  1.46it/s]Extractor Predicting: 107it [01:11,  1.47it/s]Extractor Predicting: 108it [01:11,  1.48it/s]Extractor Predicting: 109it [01:12,  1.47it/s]Extractor Predicting: 110it [01:13,  1.45it/s]Extractor Predicting: 111it [01:13,  1.47it/s]Extractor Predicting: 112it [01:14,  1.46it/s]Extractor Predicting: 113it [01:15,  1.45it/s]Extractor Predicting: 114it [01:15,  1.48it/s]Extractor Predicting: 115it [01:16,  1.47it/s]Extractor Predicting: 116it [01:17,  1.43it/s]Extractor Predicting: 117it [01:18,  1.44it/s]Extractor Predicting: 118it [01:18,  1.45it/s]Extractor Predicting: 119it [01:19,  1.44it/s]Extractor Predicting: 120it [01:20,  1.44it/s]Extractor Predicting: 121it [01:20,  1.46it/s]Extractor Predicting: 122it [01:21,  1.49it/s]Extractor Predicting: 123it [01:22,  1.49it/s]Extractor Predicting: 124it [01:22,  1.49it/s]Extractor Predicting: 125it [01:23,  1.49it/s]Extractor Predicting: 126it [01:23,  1.55it/s]Extractor Predicting: 127it [01:24,  1.54it/s]Extractor Predicting: 128it [01:25,  1.47it/s]Extractor Predicting: 129it [01:26,  1.52it/s]Extractor Predicting: 130it [01:26,  1.52it/s]Extractor Predicting: 131it [01:27,  1.51it/s]Extractor Predicting: 132it [01:27,  1.53it/s]Extractor Predicting: 133it [01:28,  1.56it/s]Extractor Predicting: 134it [01:29,  1.58it/s]Extractor Predicting: 135it [01:29,  1.53it/s]Extractor Predicting: 136it [01:30,  1.54it/s]Extractor Predicting: 137it [01:31,  1.56it/s]Extractor Predicting: 138it [01:31,  1.59it/s]Extractor Predicting: 139it [01:32,  1.58it/s]Extractor Predicting: 140it [01:33,  1.56it/s]Extractor Predicting: 141it [01:33,  1.55it/s]Extractor Predicting: 142it [01:34,  1.56it/s]Extractor Predicting: 143it [01:35,  1.53it/s]Extractor Predicting: 144it [01:35,  1.51it/s]Extractor Predicting: 145it [01:36,  1.51it/s]Extractor Predicting: 146it [01:36,  1.54it/s]Extractor Predicting: 147it [01:37,  1.59it/s]Extractor Predicting: 148it [01:38,  1.57it/s]Extractor Predicting: 149it [01:38,  1.59it/s]Extractor Predicting: 150it [01:39,  1.63it/s]Extractor Predicting: 151it [01:40,  1.62it/s]Extractor Predicting: 152it [01:40,  1.63it/s]Extractor Predicting: 153it [01:41,  1.56it/s]Extractor Predicting: 154it [01:41,  1.60it/s]Extractor Predicting: 155it [01:42,  1.59it/s]Extractor Predicting: 156it [01:43,  1.61it/s]Extractor Predicting: 157it [01:43,  1.65it/s]Extractor Predicting: 158it [01:44,  1.63it/s]Extractor Predicting: 159it [01:44,  1.69it/s]Extractor Predicting: 160it [01:45,  1.74it/s]Extractor Predicting: 161it [01:46,  1.68it/s]Extractor Predicting: 162it [01:46,  1.64it/s]Extractor Predicting: 163it [01:47,  1.62it/s]Extractor Predicting: 164it [01:47,  1.63it/s]Extractor Predicting: 165it [01:48,  1.66it/s]Extractor Predicting: 166it [01:49,  1.68it/s]Extractor Predicting: 167it [01:49,  1.67it/s]Extractor Predicting: 168it [01:50,  1.64it/s]Extractor Predicting: 169it [01:50,  1.67it/s]Extractor Predicting: 170it [01:51,  1.66it/s]Extractor Predicting: 171it [01:52,  1.69it/s]Extractor Predicting: 172it [01:52,  1.66it/s]Extractor Predicting: 173it [01:53,  1.67it/s]Extractor Predicting: 174it [01:54,  1.58it/s]Extractor Predicting: 175it [01:54,  1.58it/s]Extractor Predicting: 176it [01:55,  1.54it/s]Extractor Predicting: 177it [01:56,  1.52it/s]Extractor Predicting: 178it [01:56,  1.49it/s]Extractor Predicting: 179it [01:57,  1.48it/s]Extractor Predicting: 180it [01:58,  1.50it/s]Extractor Predicting: 181it [01:58,  1.50it/s]Extractor Predicting: 182it [01:59,  1.50it/s]Extractor Predicting: 183it [02:00,  1.52it/s]Extractor Predicting: 184it [02:00,  1.52it/s]Extractor Predicting: 185it [02:01,  1.51it/s]Extractor Predicting: 186it [02:02,  1.53it/s]Extractor Predicting: 187it [02:02,  1.52it/s]Extractor Predicting: 188it [02:03,  1.50it/s]Extractor Predicting: 189it [02:04,  1.51it/s]Extractor Predicting: 190it [02:04,  1.49it/s]Extractor Predicting: 191it [02:05,  1.47it/s]Extractor Predicting: 192it [02:06,  1.46it/s]Extractor Predicting: 193it [02:06,  1.45it/s]Extractor Predicting: 194it [02:07,  1.29it/s]Extractor Predicting: 195it [02:08,  1.34it/s]Extractor Predicting: 196it [02:09,  1.40it/s]Extractor Predicting: 197it [02:09,  1.44it/s]Extractor Predicting: 198it [02:10,  1.45it/s]Extractor Predicting: 199it [02:11,  1.47it/s]Extractor Predicting: 200it [02:11,  1.45it/s]Extractor Predicting: 201it [02:12,  1.44it/s]Extractor Predicting: 202it [02:13,  1.41it/s]Extractor Predicting: 203it [02:13,  1.40it/s]Extractor Predicting: 204it [02:14,  1.39it/s]Extractor Predicting: 205it [02:15,  1.40it/s]Extractor Predicting: 206it [02:16,  1.36it/s]Extractor Predicting: 207it [02:16,  1.39it/s]Extractor Predicting: 208it [02:17,  1.39it/s]Extractor Predicting: 209it [02:18,  1.36it/s]Extractor Predicting: 210it [02:19,  1.36it/s]Extractor Predicting: 211it [02:19,  1.33it/s]Extractor Predicting: 212it [02:20,  1.37it/s]Extractor Predicting: 213it [02:21,  1.38it/s]Extractor Predicting: 214it [02:21,  1.41it/s]Extractor Predicting: 215it [02:22,  1.39it/s]Extractor Predicting: 216it [02:23,  1.40it/s]Extractor Predicting: 217it [02:24,  1.41it/s]Extractor Predicting: 218it [02:24,  1.41it/s]Extractor Predicting: 219it [02:25,  1.40it/s]Extractor Predicting: 220it [02:26,  1.37it/s]Extractor Predicting: 221it [02:27,  1.37it/s]Extractor Predicting: 222it [02:27,  1.36it/s]Extractor Predicting: 223it [02:28,  1.40it/s]Extractor Predicting: 224it [02:29,  1.41it/s]Extractor Predicting: 225it [02:29,  1.42it/s]Extractor Predicting: 226it [02:30,  1.40it/s]Extractor Predicting: 227it [02:31,  1.41it/s]Extractor Predicting: 228it [02:31,  1.41it/s]Extractor Predicting: 229it [02:32,  1.43it/s]Extractor Predicting: 230it [02:33,  1.45it/s]Extractor Predicting: 231it [02:33,  1.49it/s]Extractor Predicting: 232it [02:34,  1.51it/s]Extractor Predicting: 233it [02:35,  1.53it/s]Extractor Predicting: 234it [02:35,  1.48it/s]Extractor Predicting: 235it [02:36,  1.51it/s]Extractor Predicting: 236it [02:37,  1.51it/s]Extractor Predicting: 237it [02:37,  1.51it/s]Extractor Predicting: 238it [02:38,  1.51it/s]Extractor Predicting: 239it [02:39,  1.49it/s]Extractor Predicting: 240it [02:39,  1.53it/s]Extractor Predicting: 241it [02:40,  1.54it/s]Extractor Predicting: 242it [02:41,  1.55it/s]Extractor Predicting: 243it [02:41,  1.55it/s]Extractor Predicting: 244it [02:42,  1.60it/s]Extractor Predicting: 245it [02:43,  1.56it/s]Extractor Predicting: 246it [02:43,  1.53it/s]Extractor Predicting: 247it [02:44,  1.51it/s]Extractor Predicting: 248it [02:45,  1.49it/s]Extractor Predicting: 249it [02:45,  1.52it/s]Extractor Predicting: 250it [02:46,  1.51it/s]Extractor Predicting: 251it [02:47,  1.53it/s]Extractor Predicting: 252it [02:47,  1.55it/s]Extractor Predicting: 253it [02:48,  1.54it/s]Extractor Predicting: 254it [02:49,  1.50it/s]Extractor Predicting: 255it [02:49,  1.51it/s]Extractor Predicting: 256it [02:50,  1.50it/s]Extractor Predicting: 257it [02:51,  1.46it/s]Extractor Predicting: 258it [02:51,  1.47it/s]Extractor Predicting: 259it [02:52,  1.48it/s]Extractor Predicting: 260it [02:53,  1.46it/s]Extractor Predicting: 261it [02:53,  1.47it/s]Extractor Predicting: 262it [02:54,  1.46it/s]Extractor Predicting: 263it [02:55,  1.49it/s]Extractor Predicting: 264it [02:55,  1.48it/s]Extractor Predicting: 265it [02:56,  1.48it/s]Extractor Predicting: 266it [02:57,  1.46it/s]Extractor Predicting: 267it [02:57,  1.42it/s]Extractor Predicting: 268it [02:58,  1.42it/s]Extractor Predicting: 269it [02:59,  1.43it/s]Extractor Predicting: 270it [03:00,  1.44it/s]Extractor Predicting: 271it [03:00,  1.45it/s]Extractor Predicting: 272it [03:01,  1.47it/s]Extractor Predicting: 273it [03:02,  1.44it/s]Extractor Predicting: 274it [03:02,  1.42it/s]Extractor Predicting: 275it [03:03,  1.43it/s]Extractor Predicting: 276it [03:04,  1.43it/s]Extractor Predicting: 277it [03:04,  1.44it/s]Extractor Predicting: 278it [03:05,  1.45it/s]Extractor Predicting: 279it [03:06,  1.44it/s]Extractor Predicting: 280it [03:06,  1.45it/s]Extractor Predicting: 281it [03:07,  1.48it/s]Extractor Predicting: 282it [03:08,  1.46it/s]Extractor Predicting: 283it [03:09,  1.43it/s]Extractor Predicting: 284it [03:09,  1.49it/s]Extractor Predicting: 285it [03:10,  1.48it/s]Extractor Predicting: 286it [03:10,  1.50it/s]Extractor Predicting: 287it [03:11,  1.42it/s]Extractor Predicting: 288it [03:12,  1.43it/s]Extractor Predicting: 289it [03:13,  1.44it/s]Extractor Predicting: 290it [03:13,  1.44it/s]Extractor Predicting: 291it [03:14,  1.41it/s]Extractor Predicting: 292it [03:15,  1.24it/s]Extractor Predicting: 293it [03:16,  1.31it/s]Extractor Predicting: 294it [03:17,  1.32it/s]Extractor Predicting: 295it [03:17,  1.37it/s]Extractor Predicting: 296it [03:18,  1.36it/s]Extractor Predicting: 297it [03:19,  1.40it/s]Extractor Predicting: 298it [03:19,  1.40it/s]Extractor Predicting: 299it [03:20,  1.39it/s]Extractor Predicting: 300it [03:21,  1.42it/s]Extractor Predicting: 301it [03:21,  1.41it/s]Extractor Predicting: 302it [03:22,  1.44it/s]Extractor Predicting: 303it [03:23,  1.41it/s]Extractor Predicting: 304it [03:24,  1.40it/s]Extractor Predicting: 305it [03:24,  1.41it/s]Extractor Predicting: 306it [03:25,  1.44it/s]Extractor Predicting: 307it [03:26,  1.43it/s]Extractor Predicting: 308it [03:26,  1.45it/s]Extractor Predicting: 309it [03:27,  1.46it/s]Extractor Predicting: 310it [03:28,  1.48it/s]Extractor Predicting: 311it [03:28,  1.45it/s]Extractor Predicting: 312it [03:29,  1.49it/s]Extractor Predicting: 313it [03:30,  1.54it/s]Extractor Predicting: 314it [03:30,  1.56it/s]Extractor Predicting: 315it [03:31,  1.57it/s]Extractor Predicting: 316it [03:32,  1.51it/s]Extractor Predicting: 317it [03:32,  1.49it/s]Extractor Predicting: 318it [03:33,  1.49it/s]Extractor Predicting: 319it [03:34,  1.48it/s]Extractor Predicting: 320it [03:34,  1.47it/s]Extractor Predicting: 321it [03:35,  1.47it/s]Extractor Predicting: 322it [03:36,  1.48it/s]Extractor Predicting: 323it [03:36,  1.44it/s]Extractor Predicting: 324it [03:37,  1.45it/s]Extractor Predicting: 325it [03:38,  1.45it/s]Extractor Predicting: 326it [03:38,  1.45it/s]Extractor Predicting: 327it [03:39,  1.48it/s]Extractor Predicting: 328it [03:40,  1.47it/s]Extractor Predicting: 329it [03:40,  1.49it/s]Extractor Predicting: 330it [03:41,  1.49it/s]Extractor Predicting: 331it [03:42,  1.46it/s]Extractor Predicting: 332it [03:42,  1.46it/s]Extractor Predicting: 333it [03:43,  1.49it/s]Extractor Predicting: 334it [03:44,  1.49it/s]Extractor Predicting: 335it [03:44,  1.51it/s]Extractor Predicting: 336it [03:45,  1.46it/s]Extractor Predicting: 337it [03:46,  1.47it/s]Extractor Predicting: 338it [03:46,  1.49it/s]Extractor Predicting: 339it [03:47,  1.53it/s]Extractor Predicting: 340it [03:48,  1.53it/s]Extractor Predicting: 341it [03:48,  1.52it/s]Extractor Predicting: 342it [03:49,  1.50it/s]Extractor Predicting: 343it [03:50,  1.50it/s]Extractor Predicting: 344it [03:50,  1.52it/s]Extractor Predicting: 345it [03:51,  1.48it/s]Extractor Predicting: 346it [03:52,  1.48it/s]Extractor Predicting: 347it [03:52,  1.48it/s]Extractor Predicting: 348it [03:53,  1.48it/s]Extractor Predicting: 349it [03:54,  1.51it/s]Extractor Predicting: 350it [03:54,  1.52it/s]Extractor Predicting: 351it [03:55,  1.53it/s]Extractor Predicting: 352it [03:56,  1.43it/s]Extractor Predicting: 353it [03:57,  1.46it/s]Extractor Predicting: 354it [03:57,  1.50it/s]Extractor Predicting: 355it [03:58,  1.54it/s]Extractor Predicting: 356it [03:58,  1.52it/s]Extractor Predicting: 357it [03:59,  1.41it/s]Extractor Predicting: 358it [04:00,  1.45it/s]Extractor Predicting: 359it [04:01,  1.46it/s]Extractor Predicting: 360it [04:01,  1.48it/s]Extractor Predicting: 361it [04:02,  1.50it/s]Extractor Predicting: 362it [04:03,  1.48it/s]Extractor Predicting: 363it [04:03,  1.51it/s]Extractor Predicting: 364it [04:04,  1.53it/s]Extractor Predicting: 365it [04:04,  1.55it/s]Extractor Predicting: 366it [04:05,  1.54it/s]Extractor Predicting: 367it [04:06,  1.50it/s]Extractor Predicting: 368it [04:06,  1.50it/s]Extractor Predicting: 369it [04:07,  1.51it/s]Extractor Predicting: 370it [04:08,  1.55it/s]Extractor Predicting: 371it [04:08,  1.56it/s]Extractor Predicting: 372it [04:09,  1.56it/s]Extractor Predicting: 373it [04:10,  1.53it/s]Extractor Predicting: 374it [04:10,  1.54it/s]Extractor Predicting: 375it [04:11,  1.54it/s]Extractor Predicting: 376it [04:12,  1.55it/s]Extractor Predicting: 377it [04:12,  1.56it/s]Extractor Predicting: 378it [04:13,  1.59it/s]Extractor Predicting: 379it [04:14,  1.54it/s]Extractor Predicting: 380it [04:14,  1.54it/s]Extractor Predicting: 381it [04:15,  1.55it/s]Extractor Predicting: 382it [04:16,  1.53it/s]Extractor Predicting: 383it [04:16,  1.56it/s]Extractor Predicting: 384it [04:17,  1.60it/s]Extractor Predicting: 385it [04:17,  1.56it/s]Extractor Predicting: 386it [04:18,  1.57it/s]Extractor Predicting: 387it [04:19,  1.55it/s]Extractor Predicting: 388it [04:19,  1.54it/s]Extractor Predicting: 389it [04:20,  1.54it/s]Extractor Predicting: 390it [04:21,  1.54it/s]Extractor Predicting: 391it [04:21,  1.52it/s]Extractor Predicting: 392it [04:22,  1.51it/s]Extractor Predicting: 393it [04:23,  1.55it/s]Extractor Predicting: 394it [04:23,  1.49it/s]Extractor Predicting: 395it [04:24,  1.45it/s]Extractor Predicting: 396it [04:25,  1.45it/s]Extractor Predicting: 397it [04:25,  1.44it/s]Extractor Predicting: 398it [04:26,  1.44it/s]Extractor Predicting: 399it [04:27,  1.43it/s]Extractor Predicting: 400it [04:27,  1.47it/s]Extractor Predicting: 401it [04:28,  1.44it/s]Extractor Predicting: 402it [04:29,  1.43it/s]Extractor Predicting: 403it [04:30,  1.27it/s]Extractor Predicting: 404it [04:31,  1.28it/s]Extractor Predicting: 405it [04:31,  1.31it/s]Extractor Predicting: 406it [04:32,  1.34it/s]Extractor Predicting: 407it [04:33,  1.21it/s]Extractor Predicting: 408it [04:34,  1.29it/s]Extractor Predicting: 409it [04:34,  1.33it/s]Extractor Predicting: 410it [04:35,  1.34it/s]Extractor Predicting: 411it [04:36,  1.31it/s]Extractor Predicting: 412it [04:37,  1.35it/s]Extractor Predicting: 413it [04:37,  1.39it/s]Extractor Predicting: 414it [04:38,  1.43it/s]Extractor Predicting: 415it [04:39,  1.47it/s]Extractor Predicting: 416it [04:39,  1.43it/s]Extractor Predicting: 417it [04:40,  1.44it/s]Extractor Predicting: 418it [04:41,  1.44it/s]Extractor Predicting: 419it [04:42,  1.40it/s]Extractor Predicting: 420it [04:42,  1.42it/s]Extractor Predicting: 421it [04:43,  1.47it/s]Extractor Predicting: 421it [04:43,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:16,076 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:16,091 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:16,091 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:16,091 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:16,091 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:05:16,701 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:05:16,702 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:05:17,265 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:05:18,327 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:05:18,327 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:21,346 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:21,348 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:21,348 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:21,348 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:05:21,348 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:05:22,104 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:05:22,121 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:05:22,691 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:05:22,846 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:05:22,846 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.29406815313420276,
  "recall": 0.06924219910846954,
  "score": 0.11209108402822322,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.24it/s]Extractor Predicting: 2it [00:01,  1.33it/s]Extractor Predicting: 3it [00:02,  1.36it/s]Extractor Predicting: 4it [00:02,  1.35it/s]Extractor Predicting: 5it [00:03,  1.41it/s]Extractor Predicting: 6it [00:04,  1.40it/s]Extractor Predicting: 7it [00:05,  1.39it/s]Extractor Predicting: 8it [00:05,  1.42it/s]Extractor Predicting: 9it [00:05,  1.81it/s]Extractor Predicting: 9it [00:05,  1.50it/s]
[INFO|configuration_utils.py:515] 2023-08-28 18:05:29,502 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:05:29,503 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:05:29,511 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:05:29,512 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 18:05:29,527 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:05:36,314 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 18:05:36,318 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 18:05:36,330 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:05:36,330 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:05:36,342 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:05:36,346 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:05:36,346 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:05:36,346 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:05:36,346 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:05:36,346 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:05:36,346 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4827586206896552,
  "recall": 0.0345679012345679,
  "score": 0.06451612903225806,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 18:05:36,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:37,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:37,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:38,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:39,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:40,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:40,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:41,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:42,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:42,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:43,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:44,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:45,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:45,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:46,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:47,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:47,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:48,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:49,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:50,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:50,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:51,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:52,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:16<05:13, 16.51s/it][WARNING|generation_utils.py:914] 2023-08-28 18:05:53,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:53,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:54,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:55,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:56,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:56,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:57,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:58,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:59,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:59,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:00,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:01,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:02,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:02,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:03,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:04,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:05,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:05,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:06,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:07,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:08,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:08,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:09,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:33<05:04, 16.93s/it][WARNING|generation_utils.py:914] 2023-08-28 18:06:10,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:10,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:11,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:12,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:13,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:13,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:14,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:15,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:16,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:16,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:17,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:18,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:18,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:19,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:20,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:21,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:21,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:22,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:23,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:23,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:48<04:29, 15.83s/it][WARNING|generation_utils.py:914] 2023-08-28 18:06:24,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:25,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:26,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:26,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:27,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:28,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:29,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:29,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:30,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:31,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:32,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:33,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:33,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:34,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:35,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:36,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:37,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:37,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:38,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:39,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:40,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:04<04:16, 16.01s/it][WARNING|generation_utils.py:914] 2023-08-28 18:06:41,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:41,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:42,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:43,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:44,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:44,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:45,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:46,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:47,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:47,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:48,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:49,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:49,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:50,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:51,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:52,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:52,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:53,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:54,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:54,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:55,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:56,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:20<04:00, 16.01s/it][WARNING|generation_utils.py:914] 2023-08-28 18:06:57,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:58,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:58,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:59,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:00,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:00,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:01,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:02,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:03,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:03,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:04,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:05,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:06,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:07,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:07,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:08,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:09,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:10,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:11,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:12,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:36<03:42, 15.87s/it][WARNING|generation_utils.py:914] 2023-08-28 18:07:12,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:13,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:14,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:15,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:16,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:16,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:17,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:18,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:18,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:19,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:20,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:21,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:22,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:22,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:23,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:24,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:25,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:26,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:26,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:27,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:28,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:52<03:28, 16.04s/it][WARNING|generation_utils.py:914] 2023-08-28 18:07:29,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:29,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:31,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:32,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:32,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:33,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:34,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:35,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:36,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:37,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:38,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:39,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:40,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:41,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:42,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:43,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:44,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:45,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:46,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:47,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:48,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:49,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:50,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:14<03:36, 18.01s/it][WARNING|generation_utils.py:914] 2023-08-28 18:07:51,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:52,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:52,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:53,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:54,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:55,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:55,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:56,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:57,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:58,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:58,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:59,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:00,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:01,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:01,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:02,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:03,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:04,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:04,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:05,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:06,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:30<03:09, 17.25s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:06,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:07,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:08,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:09,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:10,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:11,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:12,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:12,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:13,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:14,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:15,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:15,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:16,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:17,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:18,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:18,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:19,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:20,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:21,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:22,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:22,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:47<02:51, 17.15s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:23,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:24,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:25,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:26,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:26,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:27,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:28,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:29,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:29,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:30,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:31,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:32,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:33,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:33,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:34,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:35,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:35,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:36,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:37,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:38,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:02<02:28, 16.54s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:39,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:39,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:40,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:41,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:42,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:42,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:43,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:44,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:45,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:46,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:46,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:47,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:48,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:49,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:49,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:50,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:51,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:52,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:53,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:53,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:54,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:18<02:11, 16.46s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:55,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:56,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:56,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:57,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:58,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:59,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:00,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:01,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:02,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:02,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:03,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:04,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:05,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:06,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:06,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:07,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:08,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:09,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:10,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:10,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:11,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:12,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:36<01:58, 16.89s/it][WARNING|generation_utils.py:914] 2023-08-28 18:09:13,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:13,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:14,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:14,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:15,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:16,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:16,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:17,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:17,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:18,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:19,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:19,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:20,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:20,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:21,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:21,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:22,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:23,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:23,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:24,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:48<01:31, 15.29s/it][WARNING|generation_utils.py:914] 2023-08-28 18:09:24,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:25,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:26,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:27,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:27,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:28,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:29,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:29,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:30,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:31,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:31,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:32,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:33,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:33,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:34,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:35,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:36,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:36,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:37,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:38,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:39,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:03<01:16, 15.37s/it][WARNING|generation_utils.py:914] 2023-08-28 18:09:40,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:40,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:41,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:42,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:43,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:43,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:44,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:45,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:46,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:46,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:47,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:48,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:48,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:49,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:50,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:50,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:51,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:52,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:53,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:53,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:54,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:18<01:00, 15.20s/it][WARNING|generation_utils.py:914] 2023-08-28 18:09:55,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:55,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:56,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:57,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:57,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:58,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:58,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:59,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:00,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:00,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:01,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:02,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:02,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:03,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:04,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:05,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:05,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:06,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:07,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:07,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:08,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:32<00:44, 14.80s/it][WARNING|generation_utils.py:914] 2023-08-28 18:10:09,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:09,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:10,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:11,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:11,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:12,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:12,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:13,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:14,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:15,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:16,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:17,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:17,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:18,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:19,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:19,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:20,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:21,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:21,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:22,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:23,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:24,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:48<00:30, 15.09s/it][WARNING|generation_utils.py:914] 2023-08-28 18:10:24,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:25,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:26,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:27,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:28,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:28,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:29,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:30,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:31,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:31,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:32,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:33,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:33,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:34,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:35,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:36,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:36,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:37,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:38,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:39,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:39,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:03<00:15, 15.27s/it][WARNING|generation_utils.py:914] 2023-08-28 18:10:40,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:41,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:42,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:43,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:43,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:44,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:45,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:46,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:46,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:47,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:48,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:49,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:50,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:51,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:52,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:52,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:53,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:54,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:55,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:55,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:56,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:20<00:00, 15.80s/it]Generating: 100%|██████████| 20/20 [05:20<00:00, 16.05s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:05,009 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:05,029 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:05,029 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:05,029 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:05,029 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:11:05,659 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:11:05,660 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:11:06,269 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:11:07,410 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:11:07,410 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:10,569 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:10,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:10,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:10,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:11:10,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:11:10,923 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:11:10,924 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:11:11,183 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:11:11,336 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:11:11,336 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8315217391304348, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : genre .', 'success_rate': 0.8274456521739131, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : head of government .', 'success_rate': 0.940625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : military branch .', 'success_rate': 0.9226190476190477, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : winner .', 'success_rate': 0.8877840909090909, 'errors': {'', "('Anthony Joshua', 'winner', '', 'On February 11 , 2012 , fellow American sprinter and future professional American - based light heavyweight champion Anthony Joshua defeated the undefeated Michael Vick in the 2nd round .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : characters .', 'success_rate': 0.94375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.9047619047619048, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : crosses .', 'success_rate': 0.84375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.9270833333333334, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9032738095238095, 'errors': {''}}
['Relation : instrument . Context : The song \'s most famous example is the single " The Way of the Tiger at the End of the World " by French singer - songwriter Jean - Vincent Van Gogh , first recorded in 1958 . Head Entity : The Way of the Tiger at the End of the World , Tail Entity : Jean - Vincent van Gogh .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : instrument .', 'success_rate': 0.946875, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9181547619047619, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.890625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : participant .', 'success_rate': 0.9196428571428571, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : participating team .', 'success_rate': 0.9255952380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : platform . Context : The game is a spin on " The Legend of Zelda - Twilight Princess " . Head Entity : The Legend of Zelda - Twilight Princess , Tail Entity : Nintendo .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.9002976190476191, 'errors': {'', '(\'Project CARS\', \'platform\', \'\', \'The game uses a proprietary software called " Project CARS " to display detailed and detailed terrain graphics across the latest version of the Project CARS system .\')', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : position played on team / speciality . Context : On 31 March 2014 , the Armenian national team announced a deal to sign former Armenian national team international defender Çaš Ačaak , a goalkeeper who led the team back to the 2004 FIFA World Cup final . Head Entity : Çaš Ačaak , Tail Entity : head of the Armenian national team .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.8821022727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : publisher .', 'success_rate': 0.9255952380952381, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : spouse .', 'success_rate': 0.9211309523809523, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/3_ext.jsonl'}}
estimate vocab size: 12096
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12196, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.52it/s]Extractor Estimating: 2it [00:01,  1.63it/s]Extractor Estimating: 3it [00:01,  1.70it/s]Extractor Estimating: 4it [00:02,  1.71it/s]Extractor Estimating: 5it [00:03,  1.64it/s]Extractor Estimating: 6it [00:03,  1.73it/s]Extractor Estimating: 7it [00:04,  1.67it/s]Extractor Estimating: 8it [00:04,  1.68it/s]Extractor Estimating: 9it [00:05,  1.69it/s]Extractor Estimating: 10it [00:05,  1.68it/s]Extractor Estimating: 11it [00:06,  1.55it/s]Extractor Estimating: 12it [00:07,  1.59it/s]Extractor Estimating: 13it [00:07,  1.63it/s]Extractor Estimating: 14it [00:08,  1.67it/s]Extractor Estimating: 15it [00:09,  1.61it/s]Extractor Estimating: 16it [00:09,  1.65it/s]Extractor Estimating: 17it [00:10,  1.64it/s]Extractor Estimating: 18it [00:10,  1.66it/s]Extractor Estimating: 19it [00:11,  1.60it/s]Extractor Estimating: 20it [00:12,  1.63it/s]Extractor Estimating: 21it [00:12,  1.67it/s]Extractor Estimating: 22it [00:13,  1.64it/s]Extractor Estimating: 23it [00:13,  1.65it/s]Extractor Estimating: 24it [00:14,  1.64it/s]Extractor Estimating: 25it [00:15,  1.69it/s]Extractor Estimating: 26it [00:15,  1.62it/s]Extractor Estimating: 27it [00:16,  1.57it/s]Extractor Estimating: 28it [00:17,  1.49it/s]Extractor Estimating: 29it [00:18,  1.40it/s]Extractor Estimating: 30it [00:18,  1.45it/s]Extractor Estimating: 31it [00:19,  1.50it/s]Extractor Estimating: 32it [00:19,  1.56it/s]Extractor Estimating: 33it [00:20,  1.54it/s]Extractor Estimating: 34it [00:21,  1.55it/s]Extractor Estimating: 35it [00:21,  1.48it/s]Extractor Estimating: 36it [00:22,  1.49it/s]Extractor Estimating: 37it [00:23,  1.44it/s]Extractor Estimating: 38it [00:23,  1.49it/s]Extractor Estimating: 39it [00:24,  1.45it/s]Extractor Estimating: 40it [00:25,  1.45it/s]Extractor Estimating: 41it [00:26,  1.46it/s]Extractor Estimating: 42it [00:26,  1.47it/s]Extractor Estimating: 43it [00:27,  1.48it/s]Extractor Estimating: 44it [00:28,  1.52it/s]Extractor Estimating: 45it [00:28,  1.51it/s]Extractor Estimating: 46it [00:29,  1.48it/s]Extractor Estimating: 47it [00:30,  1.49it/s]Extractor Estimating: 48it [00:30,  1.50it/s]Extractor Estimating: 49it [00:31,  1.46it/s]Extractor Estimating: 50it [00:32,  1.44it/s]Extractor Estimating: 51it [00:32,  1.53it/s]Extractor Estimating: 52it [00:33,  1.56it/s]Extractor Estimating: 53it [00:33,  1.56it/s]Extractor Estimating: 54it [00:34,  1.60it/s]Extractor Estimating: 55it [00:35,  1.65it/s]Extractor Estimating: 56it [00:35,  1.71it/s]Extractor Estimating: 57it [00:36,  1.70it/s]Extractor Estimating: 58it [00:36,  1.76it/s]Extractor Estimating: 59it [00:37,  1.77it/s]Extractor Estimating: 60it [00:37,  1.73it/s]Extractor Estimating: 61it [00:38,  1.73it/s]Extractor Estimating: 62it [00:39,  1.68it/s]Extractor Estimating: 63it [00:39,  1.69it/s]Extractor Estimating: 64it [00:40,  1.71it/s]Extractor Estimating: 65it [00:40,  1.74it/s]Extractor Estimating: 66it [00:41,  1.66it/s]Extractor Estimating: 67it [00:42,  1.64it/s]Extractor Estimating: 68it [00:42,  1.67it/s]Extractor Estimating: 69it [00:43,  1.70it/s]Extractor Estimating: 70it [00:43,  1.81it/s]Extractor Estimating: 71it [00:44,  1.77it/s]Extractor Estimating: 72it [00:44,  1.77it/s]Extractor Estimating: 73it [00:45,  1.80it/s]Extractor Estimating: 74it [00:45,  1.80it/s]Extractor Estimating: 75it [00:46,  1.68it/s]Extractor Estimating: 76it [00:47,  1.63it/s]Extractor Estimating: 77it [00:47,  1.60it/s]Extractor Estimating: 78it [00:48,  1.59it/s]Extractor Estimating: 79it [00:49,  1.60it/s]Extractor Estimating: 80it [00:49,  1.58it/s]Extractor Estimating: 81it [00:50,  1.53it/s]Extractor Estimating: 82it [00:51,  1.48it/s]Extractor Estimating: 83it [00:51,  1.54it/s]Extractor Estimating: 84it [00:52,  1.53it/s]Extractor Estimating: 85it [00:53,  1.53it/s]Extractor Estimating: 86it [00:53,  1.53it/s]Extractor Estimating: 87it [00:54,  1.48it/s]Extractor Estimating: 88it [00:55,  1.50it/s]Extractor Estimating: 89it [00:55,  1.47it/s]Extractor Estimating: 90it [00:56,  1.48it/s]Extractor Estimating: 91it [00:57,  1.51it/s]Extractor Estimating: 92it [00:57,  1.52it/s]Extractor Estimating: 93it [00:58,  1.52it/s]Extractor Estimating: 94it [00:59,  1.55it/s]Extractor Estimating: 95it [00:59,  1.61it/s]Extractor Estimating: 96it [01:00,  1.56it/s]Extractor Estimating: 97it [01:01,  1.51it/s]Extractor Estimating: 98it [01:01,  1.51it/s]Extractor Estimating: 99it [01:02,  1.50it/s]Extractor Estimating: 100it [01:03,  1.51it/s]Extractor Estimating: 101it [01:03,  1.58it/s]Extractor Estimating: 102it [01:04,  1.56it/s]Extractor Estimating: 103it [01:04,  1.59it/s]Extractor Estimating: 104it [01:05,  1.61it/s]Extractor Estimating: 105it [01:06,  1.58it/s]Extractor Estimating: 106it [01:06,  1.62it/s]Extractor Estimating: 107it [01:07,  1.62it/s]Extractor Estimating: 108it [01:08,  1.48it/s]Extractor Estimating: 109it [01:08,  1.51it/s]Extractor Estimating: 110it [01:09,  1.56it/s]Extractor Estimating: 111it [01:10,  1.56it/s]Extractor Estimating: 112it [01:10,  1.56it/s]Extractor Estimating: 113it [01:11,  1.61it/s]Extractor Estimating: 114it [01:11,  1.68it/s]Extractor Estimating: 115it [01:12,  1.72it/s]Extractor Estimating: 116it [01:13,  1.64it/s]Extractor Estimating: 117it [01:13,  1.63it/s]Extractor Estimating: 118it [01:14,  1.68it/s]Extractor Estimating: 119it [01:14,  1.65it/s]Extractor Estimating: 120it [01:15,  1.59it/s]Extractor Estimating: 121it [01:16,  1.60it/s]Extractor Estimating: 122it [01:16,  1.62it/s]Extractor Estimating: 123it [01:17,  1.64it/s]Extractor Estimating: 124it [01:17,  1.67it/s]Extractor Estimating: 125it [01:18,  1.59it/s]Extractor Estimating: 126it [01:19,  1.52it/s]Extractor Estimating: 127it [01:20,  1.50it/s]Extractor Estimating: 128it [01:20,  1.51it/s]Extractor Estimating: 129it [01:21,  1.51it/s]Extractor Estimating: 130it [01:22,  1.51it/s]Extractor Estimating: 131it [01:22,  1.51it/s]Extractor Estimating: 132it [01:23,  1.52it/s]Extractor Estimating: 133it [01:23,  1.54it/s]Extractor Estimating: 134it [01:24,  1.51it/s]Extractor Estimating: 135it [01:25,  1.50it/s]Extractor Estimating: 136it [01:26,  1.48it/s]Extractor Estimating: 137it [01:26,  1.48it/s]Extractor Estimating: 138it [01:27,  1.46it/s]Extractor Estimating: 139it [01:28,  1.43it/s]Extractor Estimating: 140it [01:28,  1.35it/s]Extractor Estimating: 141it [01:29,  1.45it/s]Extractor Estimating: 142it [01:30,  1.45it/s]Extractor Estimating: 143it [01:30,  1.48it/s]Extractor Estimating: 144it [01:31,  1.45it/s]Extractor Estimating: 145it [01:32,  1.45it/s]Extractor Estimating: 146it [01:32,  1.44it/s]Extractor Estimating: 147it [01:33,  1.41it/s]Extractor Estimating: 148it [01:34,  1.35it/s]Extractor Estimating: 149it [01:35,  1.39it/s]Extractor Estimating: 150it [01:35,  1.40it/s]Extractor Estimating: 151it [01:36,  1.43it/s]Extractor Estimating: 152it [01:37,  1.41it/s]Extractor Estimating: 153it [01:37,  1.52it/s]Extractor Estimating: 154it [01:38,  1.52it/s]Extractor Estimating: 155it [01:38,  1.65it/s]Extractor Estimating: 156it [01:39,  1.64it/s]Extractor Estimating: 157it [01:40,  1.64it/s]Extractor Estimating: 158it [01:40,  1.65it/s]Extractor Estimating: 159it [01:41,  1.67it/s]Extractor Estimating: 160it [01:42,  1.64it/s]Extractor Estimating: 161it [01:42,  1.61it/s]Extractor Estimating: 162it [01:43,  1.64it/s]Extractor Estimating: 163it [01:43,  1.66it/s]Extractor Estimating: 164it [01:44,  1.63it/s]Extractor Estimating: 165it [01:45,  1.67it/s]Extractor Estimating: 166it [01:45,  1.68it/s]Extractor Estimating: 167it [01:46,  1.65it/s]Extractor Estimating: 168it [01:46,  1.64it/s]Extractor Estimating: 169it [01:47,  1.69it/s]Extractor Estimating: 170it [01:48,  1.70it/s]Extractor Estimating: 171it [01:48,  1.70it/s]Extractor Estimating: 172it [01:49,  1.73it/s]Extractor Estimating: 173it [01:49,  1.71it/s]Extractor Estimating: 174it [01:50,  1.74it/s]Extractor Estimating: 175it [01:50,  1.79it/s]Extractor Estimating: 176it [01:51,  1.74it/s]Extractor Estimating: 177it [01:52,  1.67it/s]Extractor Estimating: 178it [01:52,  1.57it/s]Extractor Estimating: 179it [01:53,  1.58it/s]Extractor Estimating: 180it [01:54,  1.54it/s]Extractor Estimating: 181it [01:54,  1.51it/s]Extractor Estimating: 182it [01:55,  1.50it/s]Extractor Estimating: 183it [01:56,  1.46it/s]Extractor Estimating: 184it [01:56,  1.51it/s]Extractor Estimating: 185it [01:57,  1.37it/s]Extractor Estimating: 186it [01:58,  1.41it/s]Extractor Estimating: 187it [01:58,  1.48it/s]Extractor Estimating: 188it [01:59,  1.49it/s]Extractor Estimating: 189it [02:00,  1.49it/s]Extractor Estimating: 190it [02:00,  1.52it/s]Extractor Estimating: 191it [02:01,  1.52it/s]Extractor Estimating: 192it [02:02,  1.53it/s]Extractor Estimating: 193it [02:02,  1.54it/s]Extractor Estimating: 194it [02:03,  1.52it/s]Extractor Estimating: 195it [02:04,  1.52it/s]Extractor Estimating: 196it [02:04,  1.55it/s]Extractor Estimating: 197it [02:05,  1.50it/s]Extractor Estimating: 198it [02:06,  1.44it/s]Extractor Estimating: 199it [02:07,  1.38it/s]Extractor Estimating: 200it [02:07,  1.39it/s]Extractor Estimating: 201it [02:08,  1.41it/s]Extractor Estimating: 202it [02:09,  1.37it/s]Extractor Estimating: 203it [02:09,  1.42it/s]Extractor Estimating: 204it [02:10,  1.49it/s]Extractor Estimating: 205it [02:11,  1.51it/s]Extractor Estimating: 206it [02:11,  1.52it/s]Extractor Estimating: 207it [02:12,  1.52it/s]Extractor Estimating: 208it [02:13,  1.51it/s]Extractor Estimating: 209it [02:13,  1.53it/s]Extractor Estimating: 210it [02:14,  1.52it/s]Extractor Estimating: 211it [02:15,  1.52it/s]Extractor Estimating: 212it [02:15,  1.55it/s]Extractor Estimating: 213it [02:16,  1.48it/s]Extractor Estimating: 214it [02:17,  1.45it/s]Extractor Estimating: 215it [02:17,  1.51it/s]Extractor Estimating: 216it [02:18,  1.48it/s]Extractor Estimating: 217it [02:19,  1.49it/s]Extractor Estimating: 218it [02:19,  1.50it/s]Extractor Estimating: 219it [02:20,  1.54it/s]Extractor Estimating: 220it [02:21,  1.54it/s]Extractor Estimating: 221it [02:21,  1.53it/s]Extractor Estimating: 222it [02:22,  1.55it/s]Extractor Estimating: 223it [02:23,  1.52it/s]Extractor Estimating: 224it [02:23,  1.53it/s]Extractor Estimating: 225it [02:24,  1.52it/s]Extractor Estimating: 226it [02:24,  1.57it/s]Extractor Estimating: 227it [02:25,  1.53it/s]Extractor Estimating: 228it [02:26,  1.52it/s]Extractor Estimating: 229it [02:26,  1.53it/s]Extractor Estimating: 230it [02:27,  1.50it/s]Extractor Estimating: 231it [02:28,  1.52it/s]Extractor Estimating: 232it [02:28,  1.52it/s]Extractor Estimating: 233it [02:29,  1.55it/s]Extractor Estimating: 234it [02:30,  1.53it/s]Extractor Estimating: 235it [02:30,  1.49it/s]Extractor Estimating: 236it [02:31,  1.54it/s]Extractor Estimating: 237it [02:32,  1.55it/s]Extractor Estimating: 238it [02:32,  1.59it/s]Extractor Estimating: 239it [02:33,  1.53it/s]Extractor Estimating: 240it [02:34,  1.55it/s]Extractor Estimating: 241it [02:34,  1.56it/s]Extractor Estimating: 242it [02:35,  1.58it/s]Extractor Estimating: 243it [02:36,  1.54it/s]Extractor Estimating: 244it [02:36,  1.51it/s]Extractor Estimating: 245it [02:37,  1.49it/s]Extractor Estimating: 246it [02:38,  1.49it/s]Extractor Estimating: 247it [02:38,  1.55it/s]Extractor Estimating: 248it [02:39,  1.47it/s]Extractor Estimating: 249it [02:40,  1.48it/s]Extractor Estimating: 250it [02:40,  1.51it/s]Extractor Estimating: 251it [02:41,  1.47it/s]Extractor Estimating: 252it [02:42,  1.46it/s]Extractor Estimating: 253it [02:42,  1.47it/s]Extractor Estimating: 254it [02:43,  1.50it/s]Extractor Estimating: 255it [02:44,  1.41it/s]Extractor Estimating: 256it [02:44,  1.45it/s]Extractor Estimating: 257it [02:45,  1.52it/s]Extractor Estimating: 258it [02:46,  1.49it/s]Extractor Estimating: 259it [02:46,  1.41it/s]Extractor Estimating: 260it [02:47,  1.46it/s]Extractor Estimating: 261it [02:48,  1.48it/s]Extractor Estimating: 262it [02:48,  1.46it/s]Extractor Estimating: 263it [02:49,  1.42it/s]Extractor Estimating: 264it [02:50,  1.32it/s]Extractor Estimating: 265it [02:51,  1.36it/s]Extractor Estimating: 266it [02:51,  1.41it/s]Extractor Estimating: 267it [02:52,  1.46it/s]Extractor Estimating: 268it [02:53,  1.44it/s]Extractor Estimating: 269it [02:53,  1.49it/s]Extractor Estimating: 270it [02:54,  1.47it/s]Extractor Estimating: 271it [02:55,  1.45it/s]Extractor Estimating: 272it [02:55,  1.48it/s]Extractor Estimating: 273it [02:56,  1.44it/s]Extractor Estimating: 274it [02:57,  1.48it/s]Extractor Estimating: 275it [02:57,  1.48it/s]Extractor Estimating: 276it [02:58,  1.50it/s]Extractor Estimating: 277it [02:59,  1.48it/s]Extractor Estimating: 278it [02:59,  1.52it/s]Extractor Estimating: 279it [03:00,  1.52it/s]Extractor Estimating: 280it [03:01,  1.54it/s]Extractor Estimating: 281it [03:01,  1.58it/s]Extractor Estimating: 282it [03:02,  1.59it/s]Extractor Estimating: 283it [03:03,  1.63it/s]Extractor Estimating: 284it [03:03,  1.61it/s]Extractor Estimating: 285it [03:04,  1.57it/s]Extractor Estimating: 286it [03:04,  1.61it/s]Extractor Estimating: 287it [03:05,  1.65it/s]Extractor Estimating: 288it [03:06,  1.58it/s]Extractor Estimating: 289it [03:06,  1.57it/s]Extractor Estimating: 290it [03:07,  1.58it/s]Extractor Estimating: 291it [03:08,  1.60it/s]Extractor Estimating: 292it [03:08,  1.57it/s]Extractor Estimating: 293it [03:09,  1.59it/s]Extractor Estimating: 294it [03:09,  1.61it/s]Extractor Estimating: 295it [03:10,  1.57it/s]Extractor Estimating: 296it [03:11,  1.62it/s]Extractor Estimating: 297it [03:11,  1.59it/s]Extractor Estimating: 298it [03:12,  1.60it/s]Extractor Estimating: 299it [03:13,  1.65it/s]Extractor Estimating: 300it [03:13,  1.59it/s]Extractor Estimating: 301it [03:14,  1.61it/s]Extractor Estimating: 302it [03:14,  1.60it/s]Extractor Estimating: 303it [03:15,  1.56it/s]Extractor Estimating: 304it [03:16,  1.43it/s]Extractor Estimating: 305it [03:17,  1.46it/s]Extractor Estimating: 306it [03:17,  1.49it/s]Extractor Estimating: 307it [03:18,  1.48it/s]Extractor Estimating: 308it [03:19,  1.45it/s]Extractor Estimating: 309it [03:19,  1.43it/s]Extractor Estimating: 310it [03:20,  1.46it/s]Extractor Estimating: 311it [03:21,  1.49it/s]Extractor Estimating: 312it [03:21,  1.44it/s]Extractor Estimating: 313it [03:22,  1.44it/s]Extractor Estimating: 314it [03:23,  1.41it/s]Extractor Estimating: 315it [03:24,  1.45it/s]Extractor Estimating: 316it [03:24,  1.42it/s]Extractor Estimating: 317it [03:25,  1.43it/s]Extractor Estimating: 318it [03:26,  1.49it/s]Extractor Estimating: 319it [03:26,  1.49it/s]Extractor Estimating: 320it [03:27,  1.46it/s]Extractor Estimating: 321it [03:28,  1.48it/s]Extractor Estimating: 322it [03:28,  1.39it/s]Extractor Estimating: 323it [03:29,  1.41it/s]Extractor Estimating: 324it [03:30,  1.43it/s]Extractor Estimating: 325it [03:30,  1.47it/s]Extractor Estimating: 326it [03:31,  1.61it/s]Extractor Estimating: 327it [03:31,  1.72it/s]Extractor Estimating: 328it [03:32,  1.85it/s]Extractor Estimating: 329it [03:32,  1.89it/s]Extractor Estimating: 330it [03:33,  1.98it/s]Extractor Estimating: 331it [03:33,  1.95it/s]Extractor Estimating: 332it [03:34,  2.01it/s]Extractor Estimating: 333it [03:34,  1.95it/s]Extractor Estimating: 334it [03:35,  1.95it/s]Extractor Estimating: 335it [03:35,  1.97it/s]Extractor Estimating: 336it [03:36,  1.99it/s]Extractor Estimating: 337it [03:36,  2.04it/s]Extractor Estimating: 338it [03:37,  1.94it/s]Extractor Estimating: 339it [03:37,  2.00it/s]Extractor Estimating: 340it [03:38,  1.98it/s]Extractor Estimating: 341it [03:38,  2.08it/s]Extractor Estimating: 342it [03:39,  2.07it/s]Extractor Estimating: 343it [03:39,  2.02it/s]Extractor Estimating: 344it [03:40,  2.03it/s]Extractor Estimating: 345it [03:40,  1.98it/s]Extractor Estimating: 346it [03:41,  2.05it/s]Extractor Estimating: 347it [03:41,  2.02it/s]Extractor Estimating: 348it [03:42,  2.08it/s]Extractor Estimating: 349it [03:42,  2.06it/s]Extractor Estimating: 350it [03:43,  2.03it/s]Extractor Estimating: 351it [03:44,  1.62it/s]Extractor Estimating: 352it [03:44,  1.67it/s]Extractor Estimating: 353it [03:45,  1.70it/s]Extractor Estimating: 354it [03:45,  1.67it/s]Extractor Estimating: 355it [03:46,  1.61it/s]Extractor Estimating: 356it [03:47,  1.66it/s]Extractor Estimating: 357it [03:47,  1.65it/s]Extractor Estimating: 358it [03:48,  1.69it/s]Extractor Estimating: 359it [03:48,  1.69it/s]Extractor Estimating: 360it [03:49,  1.71it/s]Extractor Estimating: 361it [03:49,  1.73it/s]Extractor Estimating: 362it [03:50,  1.71it/s]Extractor Estimating: 363it [03:51,  1.72it/s]Extractor Estimating: 364it [03:51,  1.75it/s]Extractor Estimating: 365it [03:52,  1.70it/s]Extractor Estimating: 366it [03:52,  1.64it/s]Extractor Estimating: 367it [03:53,  1.67it/s]Extractor Estimating: 368it [03:54,  1.68it/s]Extractor Estimating: 369it [03:54,  1.65it/s]Extractor Estimating: 370it [03:55,  1.60it/s]Extractor Estimating: 371it [03:56,  1.63it/s]Extractor Estimating: 372it [03:56,  1.62it/s]Extractor Estimating: 373it [03:57,  1.57it/s]Extractor Estimating: 374it [03:57,  1.58it/s]Extractor Estimating: 375it [03:58,  1.58it/s]Extractor Estimating: 376it [03:59,  1.62it/s]Extractor Estimating: 377it [03:59,  1.60it/s]Extractor Estimating: 378it [04:00,  1.60it/s]Extractor Estimating: 379it [04:00,  1.66it/s]Extractor Estimating: 380it [04:01,  1.67it/s]Extractor Estimating: 381it [04:02,  1.69it/s]Extractor Estimating: 382it [04:02,  1.66it/s]Extractor Estimating: 383it [04:03,  1.68it/s]Extractor Estimating: 384it [04:03,  1.72it/s]Extractor Estimating: 385it [04:04,  1.67it/s]Extractor Estimating: 386it [04:05,  1.60it/s]Extractor Estimating: 387it [04:05,  1.63it/s]Extractor Estimating: 388it [04:06,  1.67it/s]Extractor Estimating: 389it [04:06,  1.70it/s]Extractor Estimating: 390it [04:07,  1.69it/s]Extractor Estimating: 391it [04:08,  1.73it/s]Extractor Estimating: 392it [04:08,  1.73it/s]Extractor Estimating: 393it [04:09,  1.74it/s]Extractor Estimating: 394it [04:09,  1.77it/s]Extractor Estimating: 395it [04:10,  1.78it/s]Extractor Estimating: 396it [04:10,  1.80it/s]Extractor Estimating: 397it [04:11,  1.75it/s]Extractor Estimating: 398it [04:12,  1.62it/s]Extractor Estimating: 399it [04:12,  1.63it/s]Extractor Estimating: 400it [04:13,  1.65it/s]Extractor Estimating: 401it [04:14,  1.64it/s]Extractor Estimating: 402it [04:14,  1.64it/s]Extractor Estimating: 403it [04:15,  1.63it/s]Extractor Estimating: 404it [04:15,  1.62it/s]Extractor Estimating: 405it [04:16,  1.71it/s]Extractor Estimating: 406it [04:16,  1.78it/s]Extractor Estimating: 407it [04:17,  1.78it/s]Extractor Estimating: 408it [04:18,  1.70it/s]Extractor Estimating: 409it [04:18,  1.64it/s]Extractor Estimating: 410it [04:19,  1.67it/s]Extractor Estimating: 411it [04:19,  1.68it/s]Extractor Estimating: 412it [04:20,  1.70it/s]Extractor Estimating: 413it [04:21,  1.69it/s]Extractor Estimating: 414it [04:21,  1.69it/s]Extractor Estimating: 415it [04:22,  1.60it/s]Extractor Estimating: 416it [04:23,  1.57it/s]Extractor Estimating: 417it [04:23,  1.60it/s]Extractor Estimating: 418it [04:24,  1.59it/s]Extractor Estimating: 419it [04:24,  1.58it/s]Extractor Estimating: 420it [04:25,  1.66it/s]Extractor Estimating: 421it [04:26,  1.66it/s]Extractor Estimating: 422it [04:26,  1.69it/s]Extractor Estimating: 423it [04:27,  1.67it/s]Extractor Estimating: 424it [04:27,  1.65it/s]Extractor Estimating: 425it [04:28,  1.62it/s]Extractor Estimating: 426it [04:29,  1.61it/s]Extractor Estimating: 427it [04:29,  1.63it/s]Extractor Estimating: 428it [04:30,  1.67it/s]Extractor Estimating: 429it [04:30,  1.70it/s]Extractor Estimating: 430it [04:31,  1.70it/s]Extractor Estimating: 431it [04:32,  1.75it/s]Extractor Estimating: 432it [04:32,  1.75it/s]Extractor Estimating: 433it [04:33,  1.74it/s]Extractor Estimating: 434it [04:33,  1.69it/s]Extractor Estimating: 435it [04:34,  1.66it/s]Extractor Estimating: 436it [04:35,  1.64it/s]Extractor Estimating: 437it [04:35,  1.64it/s]Extractor Estimating: 438it [04:36,  1.65it/s]Extractor Estimating: 439it [04:36,  1.65it/s]Extractor Estimating: 440it [04:37,  1.65it/s]Extractor Estimating: 441it [04:38,  1.66it/s]Extractor Estimating: 442it [04:38,  1.69it/s]Extractor Estimating: 443it [04:39,  1.74it/s]Extractor Estimating: 444it [04:39,  1.80it/s]Extractor Estimating: 445it [04:40,  1.79it/s]Extractor Estimating: 446it [04:40,  1.80it/s]Extractor Estimating: 447it [04:41,  1.73it/s]Extractor Estimating: 448it [04:42,  1.67it/s]Extractor Estimating: 449it [04:42,  1.70it/s]Extractor Estimating: 450it [04:43,  1.65it/s]Extractor Estimating: 451it [04:43,  1.64it/s]Extractor Estimating: 452it [04:44,  1.60it/s]Extractor Estimating: 453it [04:45,  1.47it/s]Extractor Estimating: 454it [04:45,  1.52it/s]Extractor Estimating: 455it [04:46,  1.47it/s]Extractor Estimating: 456it [04:47,  1.50it/s]Extractor Estimating: 457it [04:47,  1.54it/s]Extractor Estimating: 458it [04:48,  1.56it/s]Extractor Estimating: 459it [04:49,  1.54it/s]Extractor Estimating: 460it [04:49,  1.48it/s]Extractor Estimating: 461it [04:50,  1.33it/s]Extractor Estimating: 462it [04:51,  1.36it/s]Extractor Estimating: 463it [04:52,  1.41it/s]Extractor Estimating: 464it [04:52,  1.40it/s]Extractor Estimating: 465it [04:53,  1.40it/s]Extractor Estimating: 466it [04:54,  1.43it/s]Extractor Estimating: 467it [04:55,  1.44it/s]Extractor Estimating: 468it [04:55,  1.46it/s]Extractor Estimating: 469it [04:56,  1.45it/s]Extractor Estimating: 470it [04:57,  1.43it/s]Extractor Estimating: 471it [04:57,  1.43it/s]Extractor Estimating: 472it [04:58,  1.47it/s]Extractor Estimating: 473it [04:59,  1.50it/s]Extractor Estimating: 474it [04:59,  1.56it/s]Extractor Estimating: 475it [05:00,  1.59it/s]Extractor Estimating: 476it [05:00,  1.59it/s]Extractor Estimating: 477it [05:01,  1.54it/s]Extractor Estimating: 478it [05:02,  1.50it/s]Extractor Estimating: 479it [05:03,  1.46it/s]Extractor Estimating: 480it [05:03,  1.53it/s]Extractor Estimating: 481it [05:04,  1.54it/s]Extractor Estimating: 482it [05:04,  1.59it/s]Extractor Estimating: 483it [05:05,  1.60it/s]Extractor Estimating: 484it [05:06,  1.65it/s]Extractor Estimating: 485it [05:06,  1.65it/s]Extractor Estimating: 486it [05:07,  1.62it/s]Extractor Estimating: 487it [05:07,  1.58it/s]Extractor Estimating: 488it [05:08,  1.58it/s]Extractor Estimating: 489it [05:09,  1.49it/s]Extractor Estimating: 490it [05:10,  1.42it/s]Extractor Estimating: 491it [05:10,  1.46it/s]Extractor Estimating: 492it [05:11,  1.53it/s]Extractor Estimating: 493it [05:11,  1.58it/s]Extractor Estimating: 494it [05:12,  1.60it/s]Extractor Estimating: 495it [05:13,  1.52it/s]Extractor Estimating: 496it [05:13,  1.56it/s]Extractor Estimating: 497it [05:14,  1.57it/s]Extractor Estimating: 498it [05:15,  1.64it/s]Extractor Estimating: 499it [05:15,  1.59it/s]Extractor Estimating: 500it [05:16,  1.71it/s]Extractor Estimating: 500it [05:16,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:40,643 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:40,680 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:40,680 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:40,680 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:40,680 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:16:41,091 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:16:41,092 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:16:41,776 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:16:42,852 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:16:42,852 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:44,659 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:44,721 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:44,721 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:44,721 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:16:44,721 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:16:45,086 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:16:45,087 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:16:45,365 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:16:45,518 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:16:45,518 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 21:25:05,450 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 21:25:05,459 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 9985 mean pseudo reward: 0.9317742198457679
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 21561
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21661, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=21661, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.090, loss:695.9193
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.099, loss:632.4410
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.097, loss:655.6465
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.088, loss:667.9962
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 1.096, loss:642.9595
>> valid entity prec:0.6351, rec:0.5219, f1:0.5730
>> valid relation prec:0.2529, rec:0.0883, f1:0.1309
>> valid relation with NER prec:0.2529, rec:0.0883, f1:0.1309
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.479, loss:639.1072
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 1.098, loss:659.9580
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 1.095, loss:668.1813
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 1.102, loss:631.0721
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 1.090, loss:604.0343
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5764, rec:0.5534, f1:0.5647
>> valid relation prec:0.1993, rec:0.0811, f1:0.1153
>> valid relation with NER prec:0.1993, rec:0.0811, f1:0.1153
g_step 1100, step 266, avg_time 2.456, loss:657.5457
g_step 1200, step 366, avg_time 1.104, loss:659.8520
g_step 1300, step 49, avg_time 1.101, loss:624.5097
g_step 1400, step 149, avg_time 1.098, loss:604.2905
g_step 1500, step 249, avg_time 1.079, loss:593.8120
>> valid entity prec:0.6496, rec:0.5687, f1:0.6065
>> valid relation prec:0.2677, rec:0.0955, f1:0.1408
>> valid relation with NER prec:0.2677, rec:0.0955, f1:0.1408
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 349, avg_time 2.456, loss:630.0035
g_step 1700, step 32, avg_time 1.078, loss:626.0001
g_step 1800, step 132, avg_time 1.065, loss:560.0587
g_step 1900, step 232, avg_time 1.107, loss:604.0216
g_step 2000, step 332, avg_time 1.090, loss:557.8133
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6005, rec:0.5731, f1:0.5864
>> valid relation prec:0.2321, rec:0.1056, f1:0.1451
>> valid relation with NER prec:0.2321, rec:0.1056, f1:0.1451
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 15, avg_time 2.447, loss:614.4559
g_step 2200, step 115, avg_time 1.101, loss:540.2714
g_step 2300, step 215, avg_time 1.085, loss:552.1505
g_step 2400, step 315, avg_time 1.087, loss:544.4513
g_step 2500, step 415, avg_time 1.095, loss:570.5131
>> valid entity prec:0.6432, rec:0.5604, f1:0.5989
>> valid relation prec:0.2489, rec:0.0843, f1:0.1259
>> valid relation with NER prec:0.2489, rec:0.0843, f1:0.1259
g_step 2600, step 98, avg_time 2.451, loss:504.8521
g_step 2700, step 198, avg_time 1.083, loss:516.3461
g_step 2800, step 298, avg_time 1.089, loss:542.9472
g_step 2900, step 398, avg_time 1.099, loss:534.5609
g_step 3000, step 81, avg_time 1.082, loss:511.1041
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6750, rec:0.5185, f1:0.5865
>> valid relation prec:0.2332, rec:0.0791, f1:0.1181
>> valid relation with NER prec:0.2332, rec:0.0791, f1:0.1181
g_step 3100, step 181, avg_time 2.442, loss:491.9221
g_step 3200, step 281, avg_time 1.115, loss:502.9883
g_step 3300, step 381, avg_time 1.106, loss:519.6729
g_step 3400, step 64, avg_time 1.065, loss:453.1479
g_step 3500, step 164, avg_time 1.100, loss:464.8400
>> valid entity prec:0.6085, rec:0.5827, f1:0.5953
>> valid relation prec:0.1887, rec:0.0797, f1:0.1120
>> valid relation with NER prec:0.1887, rec:0.0797, f1:0.1120
g_step 3600, step 264, avg_time 2.468, loss:496.1084
g_step 3700, step 364, avg_time 1.093, loss:507.9985
g_step 3800, step 47, avg_time 1.098, loss:462.8515
g_step 3900, step 147, avg_time 1.107, loss:449.6702
g_step 4000, step 247, avg_time 1.099, loss:472.2048
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5863, rec:0.5667, f1:0.5763
>> valid relation prec:0.2078, rec:0.0935, f1:0.1289
>> valid relation with NER prec:0.2078, rec:0.0935, f1:0.1289
g_step 4100, step 347, avg_time 2.443, loss:463.4633
g_step 4200, step 30, avg_time 1.077, loss:445.5641
g_step 4300, step 130, avg_time 1.079, loss:428.1795
g_step 4400, step 230, avg_time 1.114, loss:437.8796
g_step 4500, step 330, avg_time 1.084, loss:440.3153
>> valid entity prec:0.5908, rec:0.5944, f1:0.5926
>> valid relation prec:0.1851, rec:0.0851, f1:0.1166
>> valid relation with NER prec:0.1851, rec:0.0851, f1:0.1166
g_step 4600, step 13, avg_time 2.464, loss:444.9946
g_step 4700, step 113, avg_time 1.083, loss:398.5507
g_step 4800, step 213, avg_time 1.091, loss:432.4097
g_step 4900, step 313, avg_time 1.094, loss:436.1141
g_step 5000, step 413, avg_time 1.108, loss:426.0540
learning rate was adjusted to 0.0008
>> valid entity prec:0.5729, rec:0.6081, f1:0.5900
>> valid relation prec:0.1899, rec:0.0981, f1:0.1293
>> valid relation with NER prec:0.1899, rec:0.0981, f1:0.1293
g_step 5100, step 96, avg_time 2.469, loss:386.6659
g_step 5200, step 196, avg_time 1.090, loss:403.6376
g_step 5300, step 296, avg_time 1.086, loss:416.5536
g_step 5400, step 396, avg_time 1.090, loss:415.6463
g_step 5500, step 79, avg_time 1.082, loss:377.1800
>> valid entity prec:0.6440, rec:0.5166, f1:0.5733
>> valid relation prec:0.2236, rec:0.0952, f1:0.1335
>> valid relation with NER prec:0.2236, rec:0.0952, f1:0.1335
g_step 5600, step 179, avg_time 2.454, loss:396.8910
g_step 5700, step 279, avg_time 1.109, loss:398.4728
g_step 5800, step 379, avg_time 1.082, loss:388.9668
g_step 5900, step 62, avg_time 1.097, loss:382.7718
g_step 6000, step 162, avg_time 1.096, loss:358.5162
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.6056, rec:0.5738, f1:0.5893
>> valid relation prec:0.1958, rec:0.0992, f1:0.1317
>> valid relation with NER prec:0.1958, rec:0.0992, f1:0.1317
g_step 6100, step 262, avg_time 2.466, loss:361.8612
g_step 6200, step 362, avg_time 1.088, loss:379.9487
g_step 6300, step 45, avg_time 1.078, loss:373.4791
g_step 6400, step 145, avg_time 1.086, loss:350.8860
g_step 6500, step 245, avg_time 1.108, loss:353.1973
>> valid entity prec:0.6066, rec:0.5510, f1:0.5774
>> valid relation prec:0.2155, rec:0.0794, f1:0.1160
>> valid relation with NER prec:0.2155, rec:0.0794, f1:0.1160
g_step 6600, step 345, avg_time 2.461, loss:366.0444
g_step 6700, step 28, avg_time 1.097, loss:349.6882
g_step 6800, step 128, avg_time 1.093, loss:335.7572
g_step 6900, step 228, avg_time 1.091, loss:346.2801
g_step 7000, step 328, avg_time 1.090, loss:357.1072
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.6020, rec:0.5368, f1:0.5675
>> valid relation prec:0.1398, rec:0.0690, f1:0.0924
>> valid relation with NER prec:0.1398, rec:0.0690, f1:0.0924
g_step 7100, step 11, avg_time 2.466, loss:357.8192
g_step 7200, step 111, avg_time 1.089, loss:316.0754
g_step 7300, step 211, avg_time 1.100, loss:337.2191
g_step 7400, step 311, avg_time 1.089, loss:338.4069
g_step 7500, step 411, avg_time 1.085, loss:358.4042
>> valid entity prec:0.6010, rec:0.5501, f1:0.5744
>> valid relation prec:0.1622, rec:0.0733, f1:0.1010
>> valid relation with NER prec:0.1622, rec:0.0733, f1:0.1010
g_step 7600, step 94, avg_time 2.448, loss:313.9868
g_step 7700, step 194, avg_time 1.100, loss:316.6171
g_step 7800, step 294, avg_time 1.089, loss:334.1064
g_step 7900, step 394, avg_time 1.094, loss:346.5478
g_step 8000, step 77, avg_time 1.076, loss:303.2432
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5722, rec:0.5180, f1:0.5438
>> valid relation prec:0.1300, rec:0.0492, f1:0.0714
>> valid relation with NER prec:0.1300, rec:0.0492, f1:0.0714
g_step 8100, step 177, avg_time 2.460, loss:303.9506
g_step 8200, step 277, avg_time 1.107, loss:315.6032
g_step 8300, step 377, avg_time 1.093, loss:312.4152
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 21:25:05 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 21:25:05 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_21-25-05_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 21:25:06 - WARNING - datasets.builder -   Using custom data configuration default-c297f6728094b3f8
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-c297f6728094b3f8/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 21:25:06,926 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:25:06,927 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:25:06,928 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:25:06,929 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:25:06,950 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:25:06,958 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:25:06,959 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:25:06,959 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:25:06,959 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:25:06,959 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:25:06,959 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 21:25:07,198 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:25:10,372 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 21:25:10,396 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-c297f6728094b3f8/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  3.20ba/s] 20%|██        | 2/10 [00:00<00:01,  4.01ba/s] 30%|███       | 3/10 [00:00<00:01,  4.35ba/s] 40%|████      | 4/10 [00:01<00:01,  3.64ba/s] 50%|█████     | 5/10 [00:01<00:01,  3.97ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.20ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.38ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.50ba/s] 90%|█████████ | 9/10 [00:02<00:00,  4.59ba/s]100%|██████████| 10/10 [00:02<00:00,  4.65ba/s]100%|██████████| 10/10 [00:02<00:00,  4.30ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.99ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.28ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.38ba/s]100%|██████████| 4/4 [00:00<00:00,  5.45ba/s]100%|██████████| 4/4 [00:00<00:00,  4.94ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:01,  5.73ba/s] 30%|███       | 3/10 [00:00<00:00,  8.35ba/s] 50%|█████     | 5/10 [00:00<00:00,  9.22ba/s] 60%|██████    | 6/10 [00:00<00:00,  9.40ba/s] 80%|████████  | 8/10 [00:00<00:00,  9.73ba/s] 90%|█████████ | 9/10 [00:00<00:00,  9.76ba/s]100%|██████████| 10/10 [00:01<00:00,  9.37ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.11ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.56ba/s]100%|██████████| 4/4 [00:00<00:00, 10.84ba/s]
[INFO|trainer.py:414] 2023-08-28 21:25:16,403 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 21:25:16,451 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 21:25:16,451 >>   Num examples = 9999
[INFO|trainer.py:1149] 2023-08-28 21:25:16,451 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 21:25:16,451 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 21:25:16,451 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 21:25:16,451 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 21:25:16,451 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<05:18,  2.45it/s]  0%|          | 2/780 [00:00<04:43,  2.74it/s]  0%|          | 3/780 [00:01<04:25,  2.93it/s]  1%|          | 4/780 [00:01<04:20,  2.98it/s]  1%|          | 5/780 [00:01<04:07,  3.13it/s]  1%|          | 6/780 [00:01<04:04,  3.16it/s]  1%|          | 7/780 [00:02<04:06,  3.14it/s]  1%|          | 8/780 [00:02<04:16,  3.01it/s]  1%|          | 9/780 [00:02<04:06,  3.13it/s]  1%|▏         | 10/780 [00:03<04:00,  3.20it/s]  1%|▏         | 11/780 [00:03<04:25,  2.89it/s]  2%|▏         | 12/780 [00:04<05:45,  2.22it/s]  2%|▏         | 13/780 [00:04<05:07,  2.49it/s]  2%|▏         | 14/780 [00:04<04:41,  2.73it/s]  2%|▏         | 15/780 [00:05<04:22,  2.91it/s]  2%|▏         | 16/780 [00:05<04:09,  3.06it/s]  2%|▏         | 17/780 [00:05<04:00,  3.17it/s]  2%|▏         | 18/780 [00:06<03:54,  3.25it/s]  2%|▏         | 19/780 [00:06<03:49,  3.32it/s]  3%|▎         | 20/780 [00:06<03:45,  3.36it/s]  3%|▎         | 21/780 [00:06<03:43,  3.39it/s]  3%|▎         | 22/780 [00:07<03:52,  3.26it/s]  3%|▎         | 23/780 [00:07<03:48,  3.32it/s]  3%|▎         | 24/780 [00:07<03:45,  3.36it/s]  3%|▎         | 25/780 [00:08<03:42,  3.39it/s]  3%|▎         | 26/780 [00:08<03:41,  3.41it/s]  3%|▎         | 27/780 [00:08<03:40,  3.42it/s]  4%|▎         | 28/780 [00:09<03:39,  3.43it/s]  4%|▎         | 29/780 [00:09<03:37,  3.45it/s]  4%|▍         | 30/780 [00:09<03:37,  3.45it/s]  4%|▍         | 31/780 [00:09<03:36,  3.46it/s]  4%|▍         | 32/780 [00:10<03:36,  3.46it/s]  4%|▍         | 33/780 [00:10<03:43,  3.35it/s]  4%|▍         | 34/780 [00:10<03:40,  3.39it/s]  4%|▍         | 35/780 [00:11<03:38,  3.41it/s]  5%|▍         | 36/780 [00:11<03:37,  3.43it/s]  5%|▍         | 37/780 [00:11<03:35,  3.44it/s]  5%|▍         | 38/780 [00:11<03:35,  3.45it/s]  5%|▌         | 39/780 [00:12<05:41,  2.17it/s]  5%|▌         | 40/780 [00:13<05:02,  2.44it/s]  5%|▌         | 41/780 [00:13<04:35,  2.68it/s]  5%|▌         | 42/780 [00:14<05:35,  2.20it/s]  6%|▌         | 43/780 [00:14<04:58,  2.47it/s]  6%|▌         | 44/780 [00:14<04:32,  2.70it/s]  6%|▌         | 45/780 [00:14<04:13,  2.90it/s]  6%|▌         | 46/780 [00:15<04:00,  3.05it/s]  6%|▌         | 47/780 [00:15<03:51,  3.16it/s]  6%|▌         | 48/780 [00:15<03:45,  3.25it/s]  6%|▋         | 49/780 [00:16<03:40,  3.31it/s]  6%|▋         | 50/780 [00:16<03:37,  3.36it/s]  7%|▋         | 51/780 [00:16<03:35,  3.39it/s]  7%|▋         | 52/780 [00:16<03:42,  3.27it/s]  7%|▋         | 53/780 [00:17<03:38,  3.32it/s]  7%|▋         | 54/780 [00:17<03:35,  3.37it/s]  7%|▋         | 55/780 [00:17<03:33,  3.39it/s]  7%|▋         | 56/780 [00:18<03:32,  3.41it/s]  7%|▋         | 57/780 [00:18<03:30,  3.43it/s]  7%|▋         | 58/780 [00:18<03:30,  3.43it/s]  8%|▊         | 59/780 [00:18<03:29,  3.44it/s]  8%|▊         | 60/780 [00:19<03:29,  3.44it/s]  8%|▊         | 61/780 [00:19<03:28,  3.45it/s]  8%|▊         | 62/780 [00:19<03:28,  3.45it/s]  8%|▊         | 63/780 [00:20<03:29,  3.42it/s]  8%|▊         | 64/780 [00:20<03:28,  3.43it/s]  8%|▊         | 65/780 [00:20<03:27,  3.45it/s]  8%|▊         | 66/780 [00:21<03:27,  3.45it/s]  9%|▊         | 67/780 [00:21<03:26,  3.45it/s]  9%|▊         | 68/780 [00:21<03:26,  3.45it/s]  9%|▉         | 69/780 [00:21<03:25,  3.46it/s]  9%|▉         | 70/780 [00:22<03:25,  3.46it/s]  9%|▉         | 71/780 [00:22<03:25,  3.46it/s]  9%|▉         | 72/780 [00:22<03:25,  3.45it/s]  9%|▉         | 73/780 [00:23<03:24,  3.45it/s]  9%|▉         | 74/780 [00:23<03:25,  3.44it/s] 10%|▉         | 75/780 [00:23<03:24,  3.45it/s] 10%|▉         | 76/780 [00:23<03:23,  3.45it/s] 10%|▉         | 77/780 [00:24<03:23,  3.45it/s] 10%|█         | 78/780 [00:24<03:23,  3.45it/s] 10%|█         | 79/780 [00:24<03:22,  3.45it/s] 10%|█         | 80/780 [00:25<03:22,  3.45it/s] 10%|█         | 81/780 [00:25<03:22,  3.45it/s] 11%|█         | 82/780 [00:25<03:22,  3.45it/s] 11%|█         | 83/780 [00:25<03:21,  3.45it/s] 11%|█         | 84/780 [00:26<03:21,  3.45it/s] 11%|█         | 85/780 [00:26<03:24,  3.40it/s] 11%|█         | 86/780 [00:26<03:23,  3.41it/s] 11%|█         | 87/780 [00:27<03:22,  3.43it/s] 11%|█▏        | 88/780 [00:27<03:21,  3.43it/s] 11%|█▏        | 89/780 [00:27<03:20,  3.44it/s] 12%|█▏        | 90/780 [00:27<03:20,  3.44it/s] 12%|█▏        | 91/780 [00:28<03:20,  3.44it/s] 12%|█▏        | 92/780 [00:28<03:19,  3.44it/s] 12%|█▏        | 93/780 [00:28<03:19,  3.45it/s] 12%|█▏        | 94/780 [00:29<03:18,  3.45it/s] 12%|█▏        | 95/780 [00:29<03:18,  3.45it/s] 12%|█▏        | 96/780 [00:29<03:19,  3.43it/s] 12%|█▏        | 97/780 [00:30<03:18,  3.44it/s] 13%|█▎        | 98/780 [00:30<03:18,  3.44it/s] 13%|█▎        | 99/780 [00:30<03:17,  3.45it/s] 13%|█▎        | 100/780 [00:30<03:17,  3.45it/s] 13%|█▎        | 101/780 [00:31<03:16,  3.45it/s] 13%|█▎        | 102/780 [00:31<03:16,  3.45it/s] 13%|█▎        | 103/780 [00:31<03:16,  3.45it/s] 13%|█▎        | 104/780 [00:32<03:15,  3.45it/s] 13%|█▎        | 105/780 [00:32<03:15,  3.45it/s] 14%|█▎        | 106/780 [00:32<03:15,  3.45it/s] 14%|█▎        | 107/780 [00:32<03:18,  3.38it/s] 14%|█▍        | 108/780 [00:33<03:17,  3.40it/s] 14%|█▍        | 109/780 [00:33<03:16,  3.41it/s] 14%|█▍        | 110/780 [00:33<03:15,  3.42it/s] 14%|█▍        | 111/780 [00:34<03:15,  3.43it/s] 14%|█▍        | 112/780 [00:34<03:14,  3.43it/s] 14%|█▍        | 113/780 [00:34<03:13,  3.44it/s] 15%|█▍        | 114/780 [00:34<03:13,  3.44it/s] 15%|█▍        | 115/780 [00:35<03:12,  3.45it/s] 15%|█▍        | 116/780 [00:35<03:12,  3.44it/s] 15%|█▌        | 117/780 [00:35<03:12,  3.45it/s] 15%|█▌        | 118/780 [00:36<03:12,  3.44it/s] 15%|█▌        | 119/780 [00:36<03:12,  3.44it/s] 15%|█▌        | 120/780 [00:36<03:11,  3.44it/s] 16%|█▌        | 121/780 [00:36<03:11,  3.45it/s] 16%|█▌        | 122/780 [00:37<03:10,  3.45it/s] 16%|█▌        | 123/780 [00:37<03:10,  3.45it/s] 16%|█▌        | 124/780 [00:37<03:10,  3.45it/s] 16%|█▌        | 125/780 [00:38<03:09,  3.45it/s] 16%|█▌        | 126/780 [00:38<03:09,  3.45it/s] 16%|█▋        | 127/780 [00:38<03:09,  3.45it/s] 16%|█▋        | 128/780 [00:39<03:09,  3.45it/s] 17%|█▋        | 129/780 [00:39<03:13,  3.37it/s] 17%|█▋        | 130/780 [00:39<03:11,  3.40it/s] 17%|█▋        | 131/780 [00:39<03:10,  3.41it/s] 17%|█▋        | 132/780 [00:40<03:09,  3.42it/s] 17%|█▋        | 133/780 [00:40<03:08,  3.43it/s] 17%|█▋        | 134/780 [00:40<03:08,  3.43it/s] 17%|█▋        | 135/780 [00:41<03:07,  3.44it/s] 17%|█▋        | 136/780 [00:41<03:06,  3.44it/s] 18%|█▊        | 137/780 [00:41<03:06,  3.44it/s] 18%|█▊        | 138/780 [00:41<03:06,  3.44it/s] 18%|█▊        | 139/780 [00:42<03:06,  3.45it/s] 18%|█▊        | 140/780 [00:42<03:11,  3.34it/s] 18%|█▊        | 141/780 [00:42<03:09,  3.37it/s] 18%|█▊        | 142/780 [00:43<03:07,  3.40it/s] 18%|█▊        | 143/780 [00:43<03:06,  3.41it/s] 18%|█▊        | 144/780 [00:43<03:12,  3.30it/s] 19%|█▊        | 145/780 [00:44<03:10,  3.33it/s] 19%|█▊        | 146/780 [00:44<03:08,  3.36it/s] 19%|█▉        | 147/780 [00:44<03:06,  3.39it/s] 19%|█▉        | 148/780 [00:44<03:05,  3.41it/s] 19%|█▉        | 149/780 [00:45<03:04,  3.42it/s] 19%|█▉        | 150/780 [00:45<03:03,  3.43it/s] 19%|█▉        | 151/780 [00:45<03:12,  3.26it/s] 19%|█▉        | 152/780 [00:46<03:09,  3.32it/s] 20%|█▉        | 153/780 [00:46<03:06,  3.35it/s] 20%|█▉        | 154/780 [00:46<03:04,  3.39it/s] 20%|█▉        | 155/780 [00:46<03:03,  3.40it/s] 20%|██        | 156/780 [00:47<03:02,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 21:26:03,789 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:26:03,789 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 21:26:03,789 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.97it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.33it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.76it/s][A
  5%|▌         | 23/435 [00:00<00:08, 48.19it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.83it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.49it/s][A
  9%|▊         | 38/435 [00:00<00:08, 47.11it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.64it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.65it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 44.99it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 45.63it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 46.02it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.29it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.49it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.65it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.57it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.49it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.34it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.33it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.48it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.49it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.57it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.62it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.63it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.75it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.72it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.68it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.64it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.54it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.67it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.74it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.61it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.58it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.66it/s][A
 41%|████      | 178/435 [00:03<00:05, 45.16it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 45.70it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.03it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.19it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.42it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.55it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.58it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.63it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.51it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.37it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.56it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.70it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.81it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.70it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.67it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.73it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.67it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.60it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.56it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.61it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.55it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.53it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.70it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.75it/s][A
 69%|██████▊   | 298/435 [00:06<00:03, 44.59it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 45.54it/s][A
 71%|███████   | 308/435 [00:06<00:02, 45.95it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.02it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.33it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.45it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.52it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.57it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.58it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.47it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.58it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.58it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.63it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.70it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.63it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.65it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.73it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.56it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.67it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.58it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.62it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.61it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.65it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.57it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.75it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.69it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.68it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.55it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 46.55it/s][A 20%|██        | 156/780 [00:56<03:02,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:26:13,227 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 21:26:13,347 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:26:19,077 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:26:19,106 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:26:19,116 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:12<1:21:15,  7.83s/it] 20%|██        | 158/780 [01:12<57:43,  5.57s/it]   20%|██        | 159/780 [01:13<41:14,  3.98s/it] 21%|██        | 160/780 [01:13<29:42,  2.88s/it] 21%|██        | 161/780 [01:13<21:39,  2.10s/it] 21%|██        | 162/780 [01:14<16:02,  1.56s/it] 21%|██        | 163/780 [01:14<12:05,  1.18s/it] 21%|██        | 164/780 [01:14<09:20,  1.10it/s] 21%|██        | 165/780 [01:15<07:25,  1.38it/s] 21%|██▏       | 166/780 [01:15<06:04,  1.69it/s] 21%|██▏       | 167/780 [01:15<05:07,  1.99it/s] 22%|██▏       | 168/780 [01:15<04:28,  2.28it/s] 22%|██▏       | 169/780 [01:16<04:03,  2.50it/s] 22%|██▏       | 170/780 [01:16<03:43,  2.73it/s] 22%|██▏       | 171/780 [01:16<03:28,  2.92it/s] 22%|██▏       | 172/780 [01:17<03:18,  3.06it/s] 22%|██▏       | 173/780 [01:17<03:11,  3.17it/s] 22%|██▏       | 174/780 [01:17<03:06,  3.26it/s] 22%|██▏       | 175/780 [01:17<03:02,  3.32it/s] 23%|██▎       | 176/780 [01:18<02:59,  3.36it/s] 23%|██▎       | 177/780 [01:18<02:57,  3.39it/s] 23%|██▎       | 178/780 [01:18<02:56,  3.41it/s] 23%|██▎       | 179/780 [01:19<02:55,  3.43it/s] 23%|██▎       | 180/780 [01:20<04:50,  2.07it/s] 23%|██▎       | 181/780 [01:20<04:14,  2.35it/s] 23%|██▎       | 182/780 [01:20<03:49,  2.60it/s] 23%|██▎       | 183/780 [01:20<03:32,  2.81it/s] 24%|██▎       | 184/780 [01:21<03:19,  2.98it/s] 24%|██▎       | 185/780 [01:21<03:11,  3.11it/s] 24%|██▍       | 186/780 [01:21<03:05,  3.20it/s] 24%|██▍       | 187/780 [01:22<03:12,  3.09it/s] 24%|██▍       | 188/780 [01:22<03:06,  3.18it/s] 24%|██▍       | 189/780 [01:22<03:01,  3.25it/s] 24%|██▍       | 190/780 [01:22<02:58,  3.31it/s] 24%|██▍       | 191/780 [01:23<02:55,  3.36it/s] 25%|██▍       | 192/780 [01:23<02:53,  3.39it/s] 25%|██▍       | 193/780 [01:23<02:52,  3.41it/s] 25%|██▍       | 194/780 [01:24<02:51,  3.42it/s] 25%|██▌       | 195/780 [01:24<02:50,  3.43it/s] 25%|██▌       | 196/780 [01:24<03:19,  2.92it/s] 25%|██▌       | 197/780 [01:25<03:10,  3.06it/s] 25%|██▌       | 198/780 [01:25<03:03,  3.17it/s] 26%|██▌       | 199/780 [01:25<02:58,  3.25it/s] 26%|██▌       | 200/780 [01:26<02:55,  3.31it/s] 26%|██▌       | 201/780 [01:26<02:52,  3.35it/s] 26%|██▌       | 202/780 [01:26<02:50,  3.38it/s] 26%|██▌       | 203/780 [01:26<02:49,  3.40it/s] 26%|██▌       | 204/780 [01:27<02:48,  3.42it/s] 26%|██▋       | 205/780 [01:27<02:47,  3.43it/s] 26%|██▋       | 206/780 [01:27<03:15,  2.94it/s] 27%|██▋       | 207/780 [01:28<03:06,  3.08it/s] 27%|██▋       | 208/780 [01:28<03:21,  2.84it/s] 27%|██▋       | 209/780 [01:28<03:10,  2.99it/s] 27%|██▋       | 210/780 [01:29<03:02,  3.12it/s] 27%|██▋       | 211/780 [01:29<02:57,  3.21it/s] 27%|██▋       | 212/780 [01:29<02:52,  3.29it/s] 27%|██▋       | 213/780 [01:30<02:49,  3.34it/s] 27%|██▋       | 214/780 [01:30<02:47,  3.37it/s] 28%|██▊       | 215/780 [01:30<02:46,  3.40it/s] 28%|██▊       | 216/780 [01:30<02:45,  3.41it/s] 28%|██▊       | 217/780 [01:31<02:44,  3.42it/s] 28%|██▊       | 218/780 [01:31<02:43,  3.43it/s] 28%|██▊       | 219/780 [01:31<02:43,  3.44it/s] 28%|██▊       | 220/780 [01:32<02:42,  3.45it/s] 28%|██▊       | 221/780 [01:32<02:42,  3.45it/s] 28%|██▊       | 222/780 [01:32<02:41,  3.45it/s] 29%|██▊       | 223/780 [01:32<02:41,  3.45it/s] 29%|██▊       | 224/780 [01:33<02:40,  3.46it/s] 29%|██▉       | 225/780 [01:33<02:40,  3.45it/s] 29%|██▉       | 226/780 [01:33<02:40,  3.45it/s] 29%|██▉       | 227/780 [01:34<02:41,  3.42it/s] 29%|██▉       | 228/780 [01:34<02:40,  3.43it/s] 29%|██▉       | 229/780 [01:34<02:40,  3.44it/s] 29%|██▉       | 230/780 [01:35<02:39,  3.44it/s] 30%|██▉       | 231/780 [01:35<02:39,  3.45it/s] 30%|██▉       | 232/780 [01:35<02:38,  3.45it/s] 30%|██▉       | 233/780 [01:35<02:38,  3.45it/s] 30%|███       | 234/780 [01:36<02:37,  3.46it/s] 30%|███       | 235/780 [01:36<02:37,  3.46it/s] 30%|███       | 236/780 [01:36<02:37,  3.46it/s] 30%|███       | 237/780 [01:37<02:36,  3.46it/s] 31%|███       | 238/780 [01:37<02:39,  3.41it/s] 31%|███       | 239/780 [01:37<02:38,  3.42it/s] 31%|███       | 240/780 [01:37<02:37,  3.43it/s] 31%|███       | 241/780 [01:38<02:36,  3.44it/s] 31%|███       | 242/780 [01:38<02:36,  3.45it/s] 31%|███       | 243/780 [01:38<02:35,  3.45it/s] 31%|███▏      | 244/780 [01:39<02:35,  3.45it/s] 31%|███▏      | 245/780 [01:39<02:34,  3.45it/s] 32%|███▏      | 246/780 [01:39<02:34,  3.45it/s] 32%|███▏      | 247/780 [01:39<02:34,  3.46it/s] 32%|███▏      | 248/780 [01:40<02:33,  3.46it/s] 32%|███▏      | 249/780 [01:40<02:33,  3.45it/s] 32%|███▏      | 250/780 [01:40<02:33,  3.45it/s] 32%|███▏      | 251/780 [01:41<02:33,  3.45it/s] 32%|███▏      | 252/780 [01:41<02:32,  3.46it/s] 32%|███▏      | 253/780 [01:41<02:32,  3.46it/s] 33%|███▎      | 254/780 [01:41<02:32,  3.46it/s] 33%|███▎      | 255/780 [01:42<02:31,  3.46it/s] 33%|███▎      | 256/780 [01:42<02:31,  3.46it/s] 33%|███▎      | 257/780 [01:42<02:31,  3.45it/s] 33%|███▎      | 258/780 [01:43<02:31,  3.45it/s] 33%|███▎      | 259/780 [01:43<02:30,  3.46it/s] 33%|███▎      | 260/780 [01:43<02:31,  3.44it/s] 33%|███▎      | 261/780 [01:44<02:30,  3.44it/s] 34%|███▎      | 262/780 [01:44<02:30,  3.45it/s] 34%|███▎      | 263/780 [01:44<02:29,  3.45it/s] 34%|███▍      | 264/780 [01:44<02:29,  3.45it/s] 34%|███▍      | 265/780 [01:45<02:29,  3.45it/s] 34%|███▍      | 266/780 [01:45<02:28,  3.46it/s] 34%|███▍      | 267/780 [01:45<02:28,  3.46it/s] 34%|███▍      | 268/780 [01:46<02:28,  3.46it/s] 34%|███▍      | 269/780 [01:46<02:27,  3.46it/s] 35%|███▍      | 270/780 [01:46<02:27,  3.46it/s] 35%|███▍      | 271/780 [01:46<02:27,  3.44it/s] 35%|███▍      | 272/780 [01:47<02:27,  3.45it/s] 35%|███▌      | 273/780 [01:47<02:27,  3.44it/s] 35%|███▌      | 274/780 [01:47<02:26,  3.45it/s] 35%|███▌      | 275/780 [01:48<02:26,  3.45it/s] 35%|███▌      | 276/780 [01:48<02:26,  3.45it/s] 36%|███▌      | 277/780 [01:48<02:25,  3.45it/s] 36%|███▌      | 278/780 [01:48<02:25,  3.45it/s] 36%|███▌      | 279/780 [01:49<02:25,  3.45it/s] 36%|███▌      | 280/780 [01:49<02:24,  3.45it/s] 36%|███▌      | 281/780 [01:49<02:24,  3.45it/s] 36%|███▌      | 282/780 [01:50<02:29,  3.33it/s] 36%|███▋      | 283/780 [01:50<02:27,  3.37it/s] 36%|███▋      | 284/780 [01:50<02:26,  3.39it/s] 37%|███▋      | 285/780 [01:50<02:25,  3.41it/s] 37%|███▋      | 286/780 [01:51<02:24,  3.42it/s] 37%|███▋      | 287/780 [01:51<02:23,  3.43it/s] 37%|███▋      | 288/780 [01:51<02:23,  3.44it/s] 37%|███▋      | 289/780 [01:52<02:22,  3.44it/s] 37%|███▋      | 290/780 [01:52<02:22,  3.45it/s] 37%|███▋      | 291/780 [01:52<02:21,  3.45it/s] 37%|███▋      | 292/780 [01:53<02:21,  3.45it/s] 38%|███▊      | 293/780 [01:53<02:21,  3.45it/s] 38%|███▊      | 294/780 [01:53<02:20,  3.45it/s] 38%|███▊      | 295/780 [01:53<02:20,  3.45it/s] 38%|███▊      | 296/780 [01:54<02:20,  3.45it/s] 38%|███▊      | 297/780 [01:54<02:19,  3.45it/s] 38%|███▊      | 298/780 [01:54<02:19,  3.45it/s] 38%|███▊      | 299/780 [01:55<02:19,  3.45it/s] 38%|███▊      | 300/780 [01:55<02:20,  3.42it/s] 39%|███▊      | 301/780 [01:55<02:19,  3.43it/s] 39%|███▊      | 302/780 [01:55<02:18,  3.44it/s] 39%|███▉      | 303/780 [01:56<02:18,  3.44it/s] 39%|███▉      | 304/780 [01:56<02:18,  3.45it/s] 39%|███▉      | 305/780 [01:56<02:17,  3.45it/s] 39%|███▉      | 306/780 [01:57<02:17,  3.45it/s] 39%|███▉      | 307/780 [01:57<02:16,  3.45it/s] 39%|███▉      | 308/780 [01:57<02:16,  3.45it/s] 40%|███▉      | 309/780 [01:57<02:16,  3.45it/s] 40%|███▉      | 310/780 [01:58<02:16,  3.45it/s] 40%|███▉      | 311/780 [01:58<02:17,  3.42it/s] 40%|████      | 312/780 [01:58<02:16,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 21:27:15,324 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:27:15,324 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 21:27:15,324 >>   Batch size = 8
{'eval_loss': 0.9936943054199219, 'eval_runtime': 9.3767, 'eval_samples_per_second': 370.812, 'eval_steps_per_second': 46.392, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.99it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.53it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.69it/s][A
  5%|▌         | 23/435 [00:00<00:08, 48.04it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.56it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.08it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.98it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.87it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.85it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.81it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.69it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.71it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.64it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.68it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.65it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.65it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.71it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.73it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.61it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.66it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.65it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.63it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.61it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.65it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.47it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.59it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.60it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.56it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.59it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.62it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.57it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.64it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.62it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.67it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.61it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.58it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.55it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.59it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.63it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.67it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.63it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.67it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.55it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.60it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.64it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.68it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.64it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.73it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.63it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.73it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.71it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.75it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.74it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.68it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.56it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.64it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.69it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.72it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.70it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.70it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.66it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.69it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.73it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.59it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.67it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.69it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.66it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.66it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.71it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.68it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.74it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.66it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.67it/s][A
 86%|████████▌ | 373/435 [00:07<00:01, 46.66it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.64it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.67it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.72it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.63it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 42.58it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 43.82it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 44.68it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 45.22it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 45.64it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.00it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.17it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.34it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 46.34it/s][A 40%|████      | 312/780 [02:08<02:16,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:27:24,793 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 21:27:24,833 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:27:30,222 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:27:30,241 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:27:30,249 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:29<1:14:11,  9.53s/it] 40%|████      | 314/780 [02:30<52:37,  6.78s/it]   40%|████      | 315/780 [02:30<37:25,  4.83s/it] 41%|████      | 316/780 [02:30<26:48,  3.47s/it] 41%|████      | 317/780 [02:31<19:23,  2.51s/it] 41%|████      | 318/780 [02:31<14:12,  1.85s/it] 41%|████      | 319/780 [02:31<10:35,  1.38s/it] 41%|████      | 320/780 [02:31<08:03,  1.05s/it] 41%|████      | 321/780 [02:32<06:17,  1.22it/s] 41%|████▏     | 322/780 [02:32<05:03,  1.51it/s] 41%|████▏     | 323/780 [02:32<04:11,  1.82it/s] 42%|████▏     | 324/780 [02:33<03:34,  2.12it/s] 42%|████▏     | 325/780 [02:33<03:14,  2.34it/s] 42%|████▏     | 326/780 [02:33<02:55,  2.59it/s] 42%|████▏     | 327/780 [02:34<02:41,  2.80it/s] 42%|████▏     | 328/780 [02:34<02:32,  2.97it/s] 42%|████▏     | 329/780 [02:34<02:25,  3.10it/s] 42%|████▏     | 330/780 [02:34<02:20,  3.20it/s] 42%|████▏     | 331/780 [02:35<02:17,  3.27it/s] 43%|████▎     | 332/780 [02:35<02:14,  3.33it/s] 43%|████▎     | 333/780 [02:35<02:12,  3.37it/s] 43%|████▎     | 334/780 [02:36<02:11,  3.40it/s] 43%|████▎     | 335/780 [02:36<02:10,  3.42it/s] 43%|████▎     | 336/780 [02:36<02:17,  3.24it/s] 43%|████▎     | 337/780 [02:36<02:14,  3.30it/s] 43%|████▎     | 338/780 [02:37<02:12,  3.35it/s] 43%|████▎     | 339/780 [02:37<02:10,  3.38it/s] 44%|████▎     | 340/780 [02:37<02:09,  3.41it/s] 44%|████▎     | 341/780 [02:38<02:08,  3.42it/s] 44%|████▍     | 342/780 [02:38<02:07,  3.43it/s] 44%|████▍     | 343/780 [02:38<02:07,  3.44it/s] 44%|████▍     | 344/780 [02:39<02:06,  3.45it/s] 44%|████▍     | 345/780 [02:39<02:05,  3.45it/s] 44%|████▍     | 346/780 [02:39<02:05,  3.45it/s] 44%|████▍     | 347/780 [02:39<02:10,  3.32it/s] 45%|████▍     | 348/780 [02:40<02:08,  3.36it/s] 45%|████▍     | 349/780 [02:40<02:07,  3.39it/s] 45%|████▍     | 350/780 [02:40<02:05,  3.42it/s] 45%|████▌     | 351/780 [02:41<02:04,  3.43it/s] 45%|████▌     | 352/780 [02:41<02:04,  3.44it/s] 45%|████▌     | 353/780 [02:41<02:03,  3.45it/s] 45%|████▌     | 354/780 [02:41<02:03,  3.45it/s] 46%|████▌     | 355/780 [02:42<02:03,  3.45it/s] 46%|████▌     | 356/780 [02:42<02:02,  3.46it/s] 46%|████▌     | 357/780 [02:42<02:02,  3.46it/s] 46%|████▌     | 358/780 [02:43<02:07,  3.32it/s] 46%|████▌     | 359/780 [02:43<02:05,  3.36it/s] 46%|████▌     | 360/780 [02:43<02:03,  3.39it/s] 46%|████▋     | 361/780 [02:44<02:07,  3.28it/s] 46%|████▋     | 362/780 [02:44<02:05,  3.32it/s] 47%|████▋     | 363/780 [02:44<02:03,  3.36it/s] 47%|████▋     | 364/780 [02:44<02:02,  3.39it/s] 47%|████▋     | 365/780 [02:45<02:01,  3.41it/s] 47%|████▋     | 366/780 [02:45<02:00,  3.43it/s] 47%|████▋     | 367/780 [02:45<02:00,  3.43it/s] 47%|████▋     | 368/780 [02:46<01:59,  3.44it/s] 47%|████▋     | 369/780 [02:46<01:59,  3.43it/s] 47%|████▋     | 370/780 [02:46<01:59,  3.44it/s] 48%|████▊     | 371/780 [02:46<01:58,  3.44it/s] 48%|████▊     | 372/780 [02:47<01:58,  3.45it/s] 48%|████▊     | 373/780 [02:47<01:57,  3.45it/s] 48%|████▊     | 374/780 [02:47<01:57,  3.45it/s] 48%|████▊     | 375/780 [02:48<01:57,  3.45it/s] 48%|████▊     | 376/780 [02:48<01:56,  3.46it/s] 48%|████▊     | 377/780 [02:48<01:56,  3.45it/s] 48%|████▊     | 378/780 [02:48<01:56,  3.46it/s] 49%|████▊     | 379/780 [02:49<01:56,  3.45it/s] 49%|████▊     | 380/780 [02:49<02:02,  3.27it/s] 49%|████▉     | 381/780 [02:49<01:59,  3.33it/s] 49%|████▉     | 382/780 [02:50<01:58,  3.36it/s] 49%|████▉     | 383/780 [02:50<01:57,  3.39it/s] 49%|████▉     | 384/780 [02:50<01:56,  3.41it/s] 49%|████▉     | 385/780 [02:51<01:55,  3.42it/s] 49%|████▉     | 386/780 [02:51<01:54,  3.43it/s] 50%|████▉     | 387/780 [02:51<01:54,  3.44it/s] 50%|████▉     | 388/780 [02:51<01:53,  3.45it/s] 50%|████▉     | 389/780 [02:52<01:53,  3.45it/s] 50%|█████     | 390/780 [02:52<01:53,  3.45it/s] 50%|█████     | 391/780 [02:52<01:53,  3.44it/s] 50%|█████     | 392/780 [02:53<01:52,  3.44it/s] 50%|█████     | 393/780 [02:53<01:52,  3.45it/s] 51%|█████     | 394/780 [02:53<01:51,  3.45it/s] 51%|█████     | 395/780 [02:53<01:51,  3.45it/s] 51%|█████     | 396/780 [02:54<01:51,  3.46it/s] 51%|█████     | 397/780 [02:54<01:50,  3.46it/s] 51%|█████     | 398/780 [02:54<01:50,  3.46it/s] 51%|█████     | 399/780 [02:55<01:50,  3.46it/s] 51%|█████▏    | 400/780 [02:55<01:49,  3.46it/s] 51%|█████▏    | 401/780 [02:55<01:49,  3.46it/s] 52%|█████▏    | 402/780 [02:55<01:49,  3.46it/s] 52%|█████▏    | 403/780 [02:56<01:48,  3.46it/s] 52%|█████▏    | 404/780 [02:56<01:48,  3.46it/s] 52%|█████▏    | 405/780 [02:56<01:48,  3.44it/s] 52%|█████▏    | 406/780 [02:57<01:48,  3.45it/s] 52%|█████▏    | 407/780 [02:57<01:48,  3.45it/s] 52%|█████▏    | 408/780 [02:57<01:47,  3.46it/s] 52%|█████▏    | 409/780 [02:57<01:47,  3.45it/s] 53%|█████▎    | 410/780 [02:58<01:47,  3.46it/s] 53%|█████▎    | 411/780 [02:58<01:46,  3.46it/s] 53%|█████▎    | 412/780 [02:58<01:46,  3.46it/s] 53%|█████▎    | 413/780 [02:59<01:46,  3.46it/s] 53%|█████▎    | 414/780 [02:59<01:45,  3.46it/s] 53%|█████▎    | 415/780 [02:59<01:45,  3.46it/s] 53%|█████▎    | 416/780 [03:00<01:48,  3.35it/s] 53%|█████▎    | 417/780 [03:00<01:47,  3.39it/s] 54%|█████▎    | 418/780 [03:00<01:46,  3.41it/s] 54%|█████▎    | 419/780 [03:00<01:45,  3.42it/s] 54%|█████▍    | 420/780 [03:01<01:44,  3.44it/s] 54%|█████▍    | 421/780 [03:01<01:44,  3.44it/s] 54%|█████▍    | 422/780 [03:01<01:43,  3.45it/s] 54%|█████▍    | 423/780 [03:02<01:43,  3.45it/s] 54%|█████▍    | 424/780 [03:02<01:43,  3.46it/s] 54%|█████▍    | 425/780 [03:02<01:42,  3.46it/s] 55%|█████▍    | 426/780 [03:02<01:42,  3.46it/s] 55%|█████▍    | 427/780 [03:03<01:43,  3.41it/s] 55%|█████▍    | 428/780 [03:03<01:42,  3.43it/s] 55%|█████▌    | 429/780 [03:03<01:42,  3.44it/s] 55%|█████▌    | 430/780 [03:04<01:41,  3.44it/s] 55%|█████▌    | 431/780 [03:04<01:41,  3.45it/s] 55%|█████▌    | 432/780 [03:04<01:40,  3.45it/s] 56%|█████▌    | 433/780 [03:04<01:40,  3.45it/s] 56%|█████▌    | 434/780 [03:05<01:40,  3.45it/s] 56%|█████▌    | 435/780 [03:05<01:40,  3.45it/s] 56%|█████▌    | 436/780 [03:05<01:39,  3.45it/s] 56%|█████▌    | 437/780 [03:06<01:39,  3.45it/s] 56%|█████▌    | 438/780 [03:06<01:39,  3.43it/s] 56%|█████▋    | 439/780 [03:06<01:39,  3.44it/s] 56%|█████▋    | 440/780 [03:07<01:38,  3.45it/s] 57%|█████▋    | 441/780 [03:07<01:38,  3.45it/s] 57%|█████▋    | 442/780 [03:07<01:37,  3.45it/s] 57%|█████▋    | 443/780 [03:07<01:37,  3.46it/s] 57%|█████▋    | 444/780 [03:08<01:37,  3.46it/s] 57%|█████▋    | 445/780 [03:08<01:36,  3.46it/s] 57%|█████▋    | 446/780 [03:08<01:36,  3.46it/s] 57%|█████▋    | 447/780 [03:09<01:36,  3.45it/s] 57%|█████▋    | 448/780 [03:09<01:36,  3.46it/s] 58%|█████▊    | 449/780 [03:09<01:36,  3.44it/s] 58%|█████▊    | 450/780 [03:09<01:35,  3.45it/s] 58%|█████▊    | 451/780 [03:10<01:35,  3.45it/s] 58%|█████▊    | 452/780 [03:10<01:34,  3.45it/s] 58%|█████▊    | 453/780 [03:10<01:34,  3.45it/s] 58%|█████▊    | 454/780 [03:11<01:34,  3.46it/s] 58%|█████▊    | 455/780 [03:11<01:33,  3.46it/s] 58%|█████▊    | 456/780 [03:11<01:33,  3.46it/s] 59%|█████▊    | 457/780 [03:11<01:33,  3.46it/s] 59%|█████▊    | 458/780 [03:12<01:33,  3.46it/s] 59%|█████▉    | 459/780 [03:12<01:32,  3.45it/s] 59%|█████▉    | 460/780 [03:12<01:33,  3.42it/s] 59%|█████▉    | 461/780 [03:13<01:32,  3.43it/s] 59%|█████▉    | 462/780 [03:13<01:32,  3.44it/s] 59%|█████▉    | 463/780 [03:13<01:31,  3.45it/s] 59%|█████▉    | 464/780 [03:13<01:31,  3.45it/s] 60%|█████▉    | 465/780 [03:14<01:31,  3.44it/s] 60%|█████▉    | 466/780 [03:14<01:31,  3.44it/s] 60%|█████▉    | 467/780 [03:14<01:30,  3.44it/s] 60%|██████    | 468/780 [03:15<01:30,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 21:28:31,628 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:28:31,628 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 21:28:31,628 >>   Batch size = 8
{'eval_loss': 1.0078742504119873, 'eval_runtime': 9.3759, 'eval_samples_per_second': 370.843, 'eval_steps_per_second': 46.395, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.37it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.54it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.67it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.95it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.47it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.30it/s][A
  9%|▊         | 38/435 [00:00<00:08, 47.24it/s][A
 10%|▉         | 43/435 [00:00<00:08, 47.10it/s][A
 11%|█         | 48/435 [00:01<00:08, 47.02it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.98it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.84it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.79it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.67it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.64it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.66it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.64it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.74it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.82it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.72it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.78it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.66it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.67it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.55it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.58it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.56it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.71it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.74it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.77it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.77it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.70it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.65it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.65it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.59it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.60it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.66it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.74it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.77it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.73it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.60it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.61it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.62it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.61it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.69it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.63it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.71it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.74it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.49it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.46it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.55it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.54it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.56it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.57it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.61it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.70it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.67it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.64it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.69it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.65it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.59it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.61it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.60it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.64it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.64it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.07it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.37it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 45.16it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 45.68it/s][A
 79%|███████▉  | 343/435 [00:07<00:02, 45.95it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.19it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.31it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.43it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.51it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.49it/s][A
 86%|████████▌ | 373/435 [00:07<00:01, 46.52it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.51it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.55it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.67it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.56it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.61it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.62it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.64it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.64it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.58it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.56it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.62it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.62it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 46.62it/s][A 60%|██████    | 468/780 [03:24<01:30,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:28:40,999 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 21:28:41,021 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:28:45,857 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:28:45,875 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:28:45,884 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:39<39:18,  7.58s/it] 60%|██████    | 470/780 [03:40<27:53,  5.40s/it] 60%|██████    | 471/780 [03:40<19:54,  3.87s/it] 61%|██████    | 472/780 [03:40<14:20,  2.79s/it] 61%|██████    | 473/780 [03:40<10:26,  2.04s/it] 61%|██████    | 474/780 [03:41<07:43,  1.51s/it] 61%|██████    | 475/780 [03:41<05:49,  1.15s/it] 61%|██████    | 476/780 [03:41<04:30,  1.13it/s] 61%|██████    | 477/780 [03:42<03:34,  1.41it/s] 61%|██████▏   | 478/780 [03:42<02:55,  1.72it/s] 61%|██████▏   | 479/780 [03:42<02:28,  2.02it/s] 62%|██████▏   | 480/780 [03:42<02:09,  2.31it/s] 62%|██████▏   | 481/780 [03:43<02:39,  1.87it/s] 62%|██████▏   | 482/780 [03:43<02:17,  2.17it/s] 62%|██████▏   | 483/780 [03:44<02:05,  2.36it/s] 62%|██████▏   | 484/780 [03:44<01:53,  2.61it/s] 62%|██████▏   | 485/780 [03:44<01:44,  2.82it/s] 62%|██████▏   | 486/780 [03:45<01:38,  2.98it/s] 62%|██████▏   | 487/780 [03:45<01:34,  3.11it/s] 63%|██████▎   | 488/780 [03:45<01:30,  3.21it/s] 63%|██████▎   | 489/780 [03:46<01:28,  3.28it/s] 63%|██████▎   | 490/780 [03:46<01:49,  2.64it/s] 63%|██████▎   | 491/780 [03:46<01:41,  2.84it/s] 63%|██████▎   | 492/780 [03:47<01:35,  3.01it/s] 63%|██████▎   | 493/780 [03:47<01:31,  3.13it/s] 63%|██████▎   | 494/780 [03:47<01:28,  3.22it/s] 63%|██████▎   | 495/780 [03:48<01:26,  3.29it/s] 64%|██████▎   | 496/780 [03:48<01:25,  3.33it/s] 64%|██████▎   | 497/780 [03:48<01:23,  3.37it/s] 64%|██████▍   | 498/780 [03:48<01:23,  3.40it/s] 64%|██████▍   | 499/780 [03:49<01:22,  3.42it/s] 64%|██████▍   | 500/780 [03:49<01:28,  3.15it/s]                                                  64%|██████▍   | 500/780 [03:49<01:28,  3.15it/s] 64%|██████▍   | 501/780 [03:49<01:26,  3.24it/s] 64%|██████▍   | 502/780 [03:50<01:24,  3.31it/s] 64%|██████▍   | 503/780 [03:50<01:22,  3.35it/s] 65%|██████▍   | 504/780 [03:50<01:21,  3.39it/s] 65%|██████▍   | 505/780 [03:51<01:20,  3.41it/s] 65%|██████▍   | 506/780 [03:51<01:19,  3.43it/s] 65%|██████▌   | 507/780 [03:51<01:19,  3.44it/s] 65%|██████▌   | 508/780 [03:51<01:18,  3.45it/s] 65%|██████▌   | 509/780 [03:52<01:18,  3.45it/s] 65%|██████▌   | 510/780 [03:52<01:18,  3.46it/s] 66%|██████▌   | 511/780 [03:52<01:24,  3.17it/s] 66%|██████▌   | 512/780 [03:53<01:22,  3.25it/s] 66%|██████▌   | 513/780 [03:53<01:20,  3.32it/s] 66%|██████▌   | 514/780 [03:53<01:19,  3.36it/s] 66%|██████▌   | 515/780 [03:53<01:18,  3.39it/s] 66%|██████▌   | 516/780 [03:54<01:17,  3.41it/s] 66%|██████▋   | 517/780 [03:54<01:16,  3.43it/s] 66%|██████▋   | 518/780 [03:54<01:16,  3.44it/s] 67%|██████▋   | 519/780 [03:55<01:15,  3.45it/s] 67%|██████▋   | 520/780 [03:55<01:16,  3.40it/s] 67%|██████▋   | 521/780 [03:55<01:15,  3.41it/s] 67%|██████▋   | 522/780 [03:56<01:15,  3.42it/s] 67%|██████▋   | 523/780 [03:56<01:14,  3.43it/s] 67%|██████▋   | 524/780 [03:56<01:14,  3.44it/s] 67%|██████▋   | 525/780 [03:56<01:14,  3.45it/s] 67%|██████▋   | 526/780 [03:57<01:13,  3.46it/s] 68%|██████▊   | 527/780 [03:57<01:13,  3.46it/s] 68%|██████▊   | 528/780 [03:57<01:12,  3.46it/s] 68%|██████▊   | 529/780 [03:58<01:12,  3.46it/s] 68%|██████▊   | 530/780 [03:58<01:29,  2.78it/s] 68%|██████▊   | 531/780 [03:58<01:24,  2.96it/s] 68%|██████▊   | 532/780 [03:59<01:20,  3.10it/s] 68%|██████▊   | 533/780 [03:59<01:17,  3.20it/s] 68%|██████▊   | 534/780 [03:59<01:15,  3.28it/s] 69%|██████▊   | 535/780 [04:00<01:13,  3.33it/s] 69%|██████▊   | 536/780 [04:00<01:12,  3.37it/s] 69%|██████▉   | 537/780 [04:00<01:11,  3.40it/s] 69%|██████▉   | 538/780 [04:00<01:10,  3.42it/s] 69%|██████▉   | 539/780 [04:01<01:10,  3.44it/s] 69%|██████▉   | 540/780 [04:01<01:39,  2.42it/s] 69%|██████▉   | 541/780 [04:02<01:29,  2.66it/s] 69%|██████▉   | 542/780 [04:02<01:23,  2.86it/s] 70%|██████▉   | 543/780 [04:02<01:18,  3.02it/s] 70%|██████▉   | 544/780 [04:03<01:15,  3.14it/s] 70%|██████▉   | 545/780 [04:03<01:12,  3.23it/s] 70%|███████   | 546/780 [04:03<01:10,  3.30it/s] 70%|███████   | 547/780 [04:03<01:09,  3.35it/s] 70%|███████   | 548/780 [04:04<01:08,  3.38it/s] 70%|███████   | 549/780 [04:04<01:33,  2.46it/s] 71%|███████   | 550/780 [04:05<01:25,  2.69it/s] 71%|███████   | 551/780 [04:05<01:19,  2.89it/s] 71%|███████   | 552/780 [04:05<01:14,  3.04it/s] 71%|███████   | 553/780 [04:05<01:11,  3.16it/s] 71%|███████   | 554/780 [04:06<01:09,  3.24it/s] 71%|███████   | 555/780 [04:06<01:08,  3.30it/s] 71%|███████▏  | 556/780 [04:06<01:06,  3.35it/s] 71%|███████▏  | 557/780 [04:07<01:05,  3.38it/s] 72%|███████▏  | 558/780 [04:07<01:05,  3.41it/s] 72%|███████▏  | 559/780 [04:08<01:34,  2.34it/s] 72%|███████▏  | 560/780 [04:08<01:24,  2.59it/s] 72%|███████▏  | 561/780 [04:08<01:18,  2.80it/s] 72%|███████▏  | 562/780 [04:09<01:13,  2.97it/s] 72%|███████▏  | 563/780 [04:09<01:09,  3.11it/s] 72%|███████▏  | 564/780 [04:09<01:07,  3.21it/s] 72%|███████▏  | 565/780 [04:09<01:05,  3.28it/s] 73%|███████▎  | 566/780 [04:10<01:04,  3.33it/s] 73%|███████▎  | 567/780 [04:10<01:03,  3.37it/s] 73%|███████▎  | 568/780 [04:10<01:03,  3.35it/s] 73%|███████▎  | 569/780 [04:11<01:02,  3.38it/s] 73%|███████▎  | 570/780 [04:11<01:01,  3.41it/s] 73%|███████▎  | 571/780 [04:11<01:01,  3.42it/s] 73%|███████▎  | 572/780 [04:11<01:00,  3.43it/s] 73%|███████▎  | 573/780 [04:12<01:00,  3.40it/s] 74%|███████▎  | 574/780 [04:12<01:00,  3.42it/s] 74%|███████▎  | 575/780 [04:12<01:01,  3.34it/s] 74%|███████▍  | 576/780 [04:13<01:00,  3.37it/s] 74%|███████▍  | 577/780 [04:13<01:02,  3.26it/s] 74%|███████▍  | 578/780 [04:13<01:01,  3.31it/s] 74%|███████▍  | 579/780 [04:14<01:01,  3.25it/s] 74%|███████▍  | 580/780 [04:14<01:00,  3.31it/s] 74%|███████▍  | 581/780 [04:14<00:59,  3.35it/s] 75%|███████▍  | 582/780 [04:14<00:58,  3.38it/s] 75%|███████▍  | 583/780 [04:15<00:57,  3.40it/s] 75%|███████▍  | 584/780 [04:15<00:57,  3.42it/s] 75%|███████▌  | 585/780 [04:15<00:56,  3.43it/s] 75%|███████▌  | 586/780 [04:16<00:56,  3.44it/s] 75%|███████▌  | 587/780 [04:16<00:55,  3.45it/s] 75%|███████▌  | 588/780 [04:16<00:55,  3.45it/s] 76%|███████▌  | 589/780 [04:16<00:55,  3.46it/s] 76%|███████▌  | 590/780 [04:17<00:57,  3.33it/s] 76%|███████▌  | 591/780 [04:17<00:56,  3.37it/s] 76%|███████▌  | 592/780 [04:17<00:55,  3.40it/s] 76%|███████▌  | 593/780 [04:18<00:54,  3.41it/s] 76%|███████▌  | 594/780 [04:18<00:54,  3.43it/s] 76%|███████▋  | 595/780 [04:18<00:53,  3.44it/s] 76%|███████▋  | 596/780 [04:19<00:53,  3.44it/s] 77%|███████▋  | 597/780 [04:19<00:53,  3.45it/s] 77%|███████▋  | 598/780 [04:19<00:52,  3.45it/s] 77%|███████▋  | 599/780 [04:19<00:52,  3.46it/s] 77%|███████▋  | 600/780 [04:20<00:52,  3.46it/s] 77%|███████▋  | 601/780 [04:20<00:52,  3.44it/s] 77%|███████▋  | 602/780 [04:20<00:51,  3.44it/s] 77%|███████▋  | 603/780 [04:21<00:51,  3.45it/s] 77%|███████▋  | 604/780 [04:21<00:50,  3.45it/s] 78%|███████▊  | 605/780 [04:21<00:50,  3.46it/s] 78%|███████▊  | 606/780 [04:21<00:50,  3.46it/s] 78%|███████▊  | 607/780 [04:22<00:50,  3.46it/s] 78%|███████▊  | 608/780 [04:22<00:49,  3.46it/s] 78%|███████▊  | 609/780 [04:22<00:49,  3.46it/s] 78%|███████▊  | 610/780 [04:23<00:49,  3.45it/s] 78%|███████▊  | 611/780 [04:23<00:48,  3.45it/s] 78%|███████▊  | 612/780 [04:23<00:48,  3.43it/s] 79%|███████▊  | 613/780 [04:23<00:48,  3.44it/s] 79%|███████▊  | 614/780 [04:24<00:48,  3.44it/s] 79%|███████▉  | 615/780 [04:24<00:47,  3.45it/s] 79%|███████▉  | 616/780 [04:24<00:47,  3.45it/s] 79%|███████▉  | 617/780 [04:25<00:47,  3.45it/s] 79%|███████▉  | 618/780 [04:25<00:46,  3.45it/s] 79%|███████▉  | 619/780 [04:25<00:46,  3.46it/s] 79%|███████▉  | 620/780 [04:26<00:51,  3.08it/s] 80%|███████▉  | 621/780 [04:26<00:49,  3.19it/s] 80%|███████▉  | 622/780 [04:26<00:48,  3.26it/s] 80%|███████▉  | 623/780 [04:26<00:47,  3.32it/s] 80%|████████  | 624/780 [04:27<00:46,  3.36it/s][INFO|trainer.py:2140] 2023-08-28 21:29:43,745 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:29:43,745 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 21:29:43,745 >>   Batch size = 8
{'eval_loss': 1.0213831663131714, 'eval_runtime': 9.3522, 'eval_samples_per_second': 371.785, 'eval_steps_per_second': 46.513, 'epoch': 3.0}
{'loss': 0.5308, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.43it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.67it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.85it/s][A
  5%|▌         | 23/435 [00:00<00:08, 48.14it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.64it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.31it/s][A
  9%|▊         | 38/435 [00:00<00:08, 47.12it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.88it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.90it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.85it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.85it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.93it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.91it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.84it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.80it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.68it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.58it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.51it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.57it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.74it/s][A
 25%|██▍       | 108/435 [00:02<00:06, 46.80it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.81it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.76it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.78it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.64it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.63it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.62it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.59it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.65it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.71it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.78it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.79it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.81it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.65it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.51it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.45it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.54it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.65it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.66it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.72it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.08it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.33it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.44it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.54it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.55it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.61it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.67it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.61it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.62it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.66it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.72it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.63it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.66it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.69it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.71it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.67it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.71it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.64it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.69it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.75it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.62it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.63it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.73it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.70it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.67it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.69it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.63it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.68it/s][A
 80%|████████  | 348/435 [00:07<00:02, 38.65it/s][A
 81%|████████  | 353/435 [00:07<00:02, 40.78it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 42.40it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 43.64it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 44.51it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 45.21it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 45.70it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.01it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.06it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.24it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.39it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.45it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.60it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.66it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 46.74it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.79it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.70it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.69it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 46.69it/s][A 80%|████████  | 624/780 [04:36<00:46,  3.36it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:29:53,206 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-28 21:29:53,283 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:29:59,878 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:29:59,916 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:29:59,955 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:55<22:19,  8.64s/it] 80%|████████  | 626/780 [04:55<15:47,  6.15s/it] 80%|████████  | 627/780 [04:55<11:12,  4.39s/it] 81%|████████  | 628/780 [04:56<08:00,  3.16s/it] 81%|████████  | 629/780 [04:56<05:47,  2.30s/it] 81%|████████  | 630/780 [04:56<04:14,  1.70s/it] 81%|████████  | 631/780 [04:57<03:09,  1.27s/it] 81%|████████  | 632/780 [04:57<02:24,  1.02it/s] 81%|████████  | 633/780 [04:57<01:53,  1.30it/s] 81%|████████▏ | 634/780 [04:58<01:31,  1.60it/s] 81%|████████▏ | 635/780 [04:58<01:16,  1.90it/s] 82%|████████▏ | 636/780 [04:58<01:05,  2.20it/s] 82%|████████▏ | 637/780 [04:58<00:57,  2.47it/s] 82%|████████▏ | 638/780 [04:59<00:52,  2.70it/s] 82%|████████▏ | 639/780 [04:59<00:48,  2.89it/s] 82%|████████▏ | 640/780 [04:59<00:46,  3.04it/s] 82%|████████▏ | 641/780 [05:00<00:44,  3.15it/s] 82%|████████▏ | 642/780 [05:00<00:42,  3.24it/s] 82%|████████▏ | 643/780 [05:00<00:41,  3.30it/s] 83%|████████▎ | 644/780 [05:00<00:40,  3.35it/s] 83%|████████▎ | 645/780 [05:01<00:39,  3.38it/s] 83%|████████▎ | 646/780 [05:01<00:39,  3.40it/s] 83%|████████▎ | 647/780 [05:01<00:38,  3.42it/s] 83%|████████▎ | 648/780 [05:02<00:38,  3.43it/s] 83%|████████▎ | 649/780 [05:02<00:38,  3.44it/s] 83%|████████▎ | 650/780 [05:02<00:37,  3.45it/s] 83%|████████▎ | 651/780 [05:02<00:37,  3.43it/s] 84%|████████▎ | 652/780 [05:03<00:37,  3.44it/s] 84%|████████▎ | 653/780 [05:03<00:36,  3.45it/s] 84%|████████▍ | 654/780 [05:03<00:36,  3.45it/s] 84%|████████▍ | 655/780 [05:04<00:36,  3.46it/s] 84%|████████▍ | 656/780 [05:04<00:35,  3.46it/s] 84%|████████▍ | 657/780 [05:04<00:35,  3.46it/s] 84%|████████▍ | 658/780 [05:04<00:35,  3.46it/s] 84%|████████▍ | 659/780 [05:05<00:34,  3.46it/s] 85%|████████▍ | 660/780 [05:05<00:34,  3.46it/s] 85%|████████▍ | 661/780 [05:05<00:34,  3.46it/s] 85%|████████▍ | 662/780 [05:06<00:37,  3.14it/s] 85%|████████▌ | 663/780 [05:06<00:36,  3.23it/s] 85%|████████▌ | 664/780 [05:06<00:35,  3.29it/s] 85%|████████▌ | 665/780 [05:07<00:34,  3.34it/s] 85%|████████▌ | 666/780 [05:07<00:33,  3.37it/s] 86%|████████▌ | 667/780 [05:07<00:33,  3.40it/s] 86%|████████▌ | 668/780 [05:07<00:32,  3.42it/s] 86%|████████▌ | 669/780 [05:08<00:32,  3.42it/s] 86%|████████▌ | 670/780 [05:08<00:32,  3.43it/s] 86%|████████▌ | 671/780 [05:08<00:31,  3.44it/s] 86%|████████▌ | 672/780 [05:09<00:31,  3.45it/s] 86%|████████▋ | 673/780 [05:10<00:50,  2.12it/s] 86%|████████▋ | 674/780 [05:10<00:44,  2.40it/s] 87%|████████▋ | 675/780 [05:10<00:39,  2.64it/s] 87%|████████▋ | 676/780 [05:10<00:36,  2.84it/s] 87%|████████▋ | 677/780 [05:11<00:34,  3.00it/s] 87%|████████▋ | 678/780 [05:11<00:32,  3.13it/s] 87%|████████▋ | 679/780 [05:11<00:31,  3.22it/s] 87%|████████▋ | 680/780 [05:12<00:30,  3.29it/s] 87%|████████▋ | 681/780 [05:12<00:29,  3.31it/s] 87%|████████▋ | 682/780 [05:12<00:29,  3.34it/s] 88%|████████▊ | 683/780 [05:12<00:28,  3.35it/s] 88%|████████▊ | 684/780 [05:13<00:28,  3.37it/s] 88%|████████▊ | 685/780 [05:13<00:28,  3.37it/s] 88%|████████▊ | 686/780 [05:13<00:27,  3.40it/s] 88%|████████▊ | 687/780 [05:14<00:27,  3.42it/s] 88%|████████▊ | 688/780 [05:14<00:26,  3.43it/s] 88%|████████▊ | 689/780 [05:14<00:26,  3.44it/s] 88%|████████▊ | 690/780 [05:14<00:26,  3.45it/s] 89%|████████▊ | 691/780 [05:15<00:25,  3.45it/s] 89%|████████▊ | 692/780 [05:15<00:25,  3.45it/s] 89%|████████▉ | 693/780 [05:16<00:34,  2.51it/s] 89%|████████▉ | 694/780 [05:16<00:31,  2.73it/s] 89%|████████▉ | 695/780 [05:16<00:29,  2.92it/s] 89%|████████▉ | 696/780 [05:17<00:27,  3.06it/s] 89%|████████▉ | 697/780 [05:17<00:26,  3.17it/s] 89%|████████▉ | 698/780 [05:17<00:25,  3.25it/s] 90%|████████▉ | 699/780 [05:17<00:24,  3.31it/s] 90%|████████▉ | 700/780 [05:18<00:23,  3.35it/s] 90%|████████▉ | 701/780 [05:18<00:23,  3.38it/s] 90%|█████████ | 702/780 [05:18<00:22,  3.40it/s] 90%|█████████ | 703/780 [05:19<00:25,  3.00it/s] 90%|█████████ | 704/780 [05:19<00:24,  3.12it/s] 90%|█████████ | 705/780 [05:19<00:23,  3.22it/s] 91%|█████████ | 706/780 [05:20<00:22,  3.29it/s] 91%|█████████ | 707/780 [05:20<00:21,  3.34it/s] 91%|█████████ | 708/780 [05:20<00:21,  3.37it/s] 91%|█████████ | 709/780 [05:20<00:20,  3.40it/s] 91%|█████████ | 710/780 [05:21<00:20,  3.42it/s] 91%|█████████ | 711/780 [05:21<00:20,  3.43it/s] 91%|█████████▏| 712/780 [05:21<00:19,  3.44it/s] 91%|█████████▏| 713/780 [05:22<00:28,  2.37it/s] 92%|█████████▏| 714/780 [05:22<00:25,  2.61it/s] 92%|█████████▏| 715/780 [05:23<00:23,  2.82it/s] 92%|█████████▏| 716/780 [05:23<00:21,  2.99it/s] 92%|█████████▏| 717/780 [05:23<00:20,  3.11it/s] 92%|█████████▏| 718/780 [05:23<00:19,  3.21it/s] 92%|█████████▏| 719/780 [05:24<00:18,  3.28it/s] 92%|█████████▏| 720/780 [05:24<00:18,  3.33it/s] 92%|█████████▏| 721/780 [05:24<00:17,  3.37it/s] 93%|█████████▎| 722/780 [05:25<00:17,  3.32it/s] 93%|█████████▎| 723/780 [05:25<00:16,  3.36it/s] 93%|█████████▎| 724/780 [05:25<00:16,  3.39it/s] 93%|█████████▎| 725/780 [05:26<00:16,  3.41it/s] 93%|█████████▎| 726/780 [05:26<00:15,  3.42it/s] 93%|█████████▎| 727/780 [05:26<00:15,  3.43it/s] 93%|█████████▎| 728/780 [05:26<00:15,  3.44it/s] 93%|█████████▎| 729/780 [05:27<00:14,  3.44it/s] 94%|█████████▎| 730/780 [05:27<00:14,  3.45it/s] 94%|█████████▎| 731/780 [05:27<00:14,  3.45it/s] 94%|█████████▍| 732/780 [05:28<00:13,  3.45it/s] 94%|█████████▍| 733/780 [05:28<00:13,  3.45it/s] 94%|█████████▍| 734/780 [05:28<00:13,  3.46it/s] 94%|█████████▍| 735/780 [05:28<00:13,  3.46it/s] 94%|█████████▍| 736/780 [05:29<00:13,  3.32it/s] 94%|█████████▍| 737/780 [05:29<00:12,  3.35it/s] 95%|█████████▍| 738/780 [05:29<00:12,  3.38it/s] 95%|█████████▍| 739/780 [05:30<00:12,  3.40it/s] 95%|█████████▍| 740/780 [05:30<00:12,  3.31it/s] 95%|█████████▌| 741/780 [05:30<00:11,  3.35it/s] 95%|█████████▌| 742/780 [05:31<00:11,  3.38it/s] 95%|█████████▌| 743/780 [05:31<00:10,  3.40it/s] 95%|█████████▌| 744/780 [05:31<00:10,  3.42it/s] 96%|█████████▌| 745/780 [05:31<00:10,  3.43it/s] 96%|█████████▌| 746/780 [05:32<00:09,  3.44it/s] 96%|█████████▌| 747/780 [05:32<00:09,  3.44it/s] 96%|█████████▌| 748/780 [05:32<00:09,  3.45it/s] 96%|█████████▌| 749/780 [05:33<00:08,  3.45it/s] 96%|█████████▌| 750/780 [05:33<00:08,  3.45it/s] 96%|█████████▋| 751/780 [05:33<00:08,  3.41it/s] 96%|█████████▋| 752/780 [05:33<00:08,  3.42it/s] 97%|█████████▋| 753/780 [05:34<00:07,  3.43it/s] 97%|█████████▋| 754/780 [05:34<00:07,  3.44it/s] 97%|█████████▋| 755/780 [05:34<00:07,  3.44it/s] 97%|█████████▋| 756/780 [05:35<00:06,  3.45it/s] 97%|█████████▋| 757/780 [05:35<00:06,  3.45it/s] 97%|█████████▋| 758/780 [05:35<00:06,  3.45it/s] 97%|█████████▋| 759/780 [05:35<00:06,  3.46it/s] 97%|█████████▋| 760/780 [05:36<00:05,  3.45it/s] 98%|█████████▊| 761/780 [05:36<00:05,  3.45it/s] 98%|█████████▊| 762/780 [05:36<00:05,  3.42it/s] 98%|█████████▊| 763/780 [05:37<00:04,  3.43it/s] 98%|█████████▊| 764/780 [05:37<00:04,  3.44it/s] 98%|█████████▊| 765/780 [05:37<00:04,  3.45it/s] 98%|█████████▊| 766/780 [05:37<00:04,  3.45it/s] 98%|█████████▊| 767/780 [05:38<00:03,  3.45it/s] 98%|█████████▊| 768/780 [05:38<00:03,  3.45it/s] 99%|█████████▊| 769/780 [05:38<00:03,  3.45it/s] 99%|█████████▊| 770/780 [05:39<00:02,  3.45it/s] 99%|█████████▉| 771/780 [05:39<00:02,  3.45it/s] 99%|█████████▉| 772/780 [05:39<00:02,  3.45it/s] 99%|█████████▉| 773/780 [05:40<00:02,  3.38it/s] 99%|█████████▉| 774/780 [05:40<00:01,  3.39it/s] 99%|█████████▉| 775/780 [05:40<00:01,  3.41it/s] 99%|█████████▉| 776/780 [05:40<00:01,  3.42it/s]100%|█████████▉| 777/780 [05:41<00:00,  3.43it/s]100%|█████████▉| 778/780 [05:41<00:00,  3.44it/s]100%|█████████▉| 779/780 [05:41<00:00,  3.44it/s]100%|██████████| 780/780 [05:42<00:00,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 21:30:58,523 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:30:58,523 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 21:30:58,523 >>   Batch size = 8
{'eval_loss': 1.0228071212768555, 'eval_runtime': 9.4097, 'eval_samples_per_second': 369.511, 'eval_steps_per_second': 46.229, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.09it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.51it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.61it/s][A
  5%|▌         | 23/435 [00:00<00:08, 48.00it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.53it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.28it/s][A
  9%|▊         | 38/435 [00:00<00:08, 47.14it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.86it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.83it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.73it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.74it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.76it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.70it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.69it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.67it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.62it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.68it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.71it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.56it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.62it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.57it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.58it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.62it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.64it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.67it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.62it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.55it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.52it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.61it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.57it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.75it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.70it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.64it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.71it/s][A
 41%|████      | 178/435 [00:04<00:08, 29.14it/s][A
 42%|████▏     | 183/435 [00:04<00:07, 32.80it/s][A
 43%|████▎     | 188/435 [00:04<00:06, 35.99it/s][A
 44%|████▍     | 193/435 [00:04<00:06, 38.71it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 40.81it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 42.50it/s][A
 48%|████▊     | 208/435 [00:04<00:05, 43.76it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 44.53it/s][A
 50%|█████     | 218/435 [00:04<00:04, 44.97it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 45.40it/s][A
 52%|█████▏    | 228/435 [00:05<00:04, 45.80it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 46.14it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.33it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.37it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.42it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.50it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.62it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.64it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.60it/s][A
 63%|██████▎   | 273/435 [00:06<00:03, 46.57it/s][A
 64%|██████▍   | 278/435 [00:06<00:03, 46.60it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.65it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.55it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.64it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.66it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.68it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.75it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.69it/s][A
 73%|███████▎  | 318/435 [00:07<00:02, 46.59it/s][A
 74%|███████▍  | 323/435 [00:07<00:02, 46.67it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.65it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.59it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.58it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.72it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.71it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.61it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.49it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.59it/s][A
 85%|████████▍ | 368/435 [00:08<00:01, 46.63it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.64it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.61it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.47it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.59it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.60it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.58it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.62it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.54it/s][A
 95%|█████████▍| 413/435 [00:09<00:00, 46.28it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 46.46it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.53it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.53it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.51it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 46.51it/s][A100%|██████████| 780/780 [05:51<00:00,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:31:08,178 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-28 21:31:08,257 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:31:12,325 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:31:12,339 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:31:12,356 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:31:22,629 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:31:22,640 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-156 (score: 0.9936943054199219).
                                                 100%|██████████| 780/780 [06:13<00:00,  3.44it/s]100%|██████████| 780/780 [06:13<00:00,  2.09it/s]
[INFO|trainer.py:1894] 2023-08-28 21:31:29,700 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 21:31:29,716 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:31:36,793 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:31:37,055 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:31:37,348 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:31:37,695 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:31:37,696 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:31:37,696 >>   train_loss               =     0.5215
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:31:37,696 >>   train_runtime            = 0:06:13.24
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:31:37,696 >>   train_samples            =       9999
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:31:37,696 >>   train_samples_per_second =    133.948
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:31:37,696 >>   train_steps_per_second   =       2.09
{'eval_loss': 1.032858967781067, 'eval_runtime': 9.5388, 'eval_samples_per_second': 364.513, 'eval_steps_per_second': 45.603, 'epoch': 5.0}
{'train_runtime': 373.2415, 'train_samples_per_second': 133.948, 'train_steps_per_second': 2.09, 'train_loss': 0.5215135036370693, 'epoch': 5.0}
08/28/2023 21:31:37 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:31:37,785 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:31:37,785 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-28 21:31:37,785 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|▏         | 6/435 [00:00<00:07, 57.76it/s]  3%|▎         | 12/435 [00:00<00:08, 50.92it/s]  4%|▍         | 18/435 [00:00<00:08, 49.30it/s]  5%|▌         | 23/435 [00:00<00:08, 48.60it/s]  6%|▋         | 28/435 [00:00<00:08, 48.18it/s]  8%|▊         | 33/435 [00:00<00:08, 47.75it/s]  9%|▊         | 38/435 [00:00<00:08, 47.71it/s] 10%|▉         | 43/435 [00:00<00:08, 47.56it/s] 11%|█         | 48/435 [00:01<00:22, 17.49it/s] 12%|█▏        | 53/435 [00:01<00:17, 21.61it/s] 13%|█▎        | 58/435 [00:01<00:14, 25.84it/s] 14%|█▍        | 63/435 [00:01<00:12, 29.93it/s] 16%|█▌        | 68/435 [00:02<00:10, 33.63it/s] 17%|█▋        | 73/435 [00:02<00:11, 30.59it/s] 18%|█▊        | 78/435 [00:02<00:10, 34.19it/s] 19%|█▉        | 83/435 [00:02<00:09, 37.15it/s] 20%|██        | 88/435 [00:02<00:08, 39.70it/s] 21%|██▏       | 93/435 [00:02<00:08, 41.67it/s] 23%|██▎       | 98/435 [00:02<00:07, 43.09it/s] 24%|██▎       | 103/435 [00:02<00:07, 44.18it/s] 25%|██▍       | 108/435 [00:02<00:07, 44.97it/s] 26%|██▌       | 113/435 [00:03<00:07, 45.37it/s] 27%|██▋       | 118/435 [00:03<00:06, 45.94it/s] 28%|██▊       | 123/435 [00:03<00:06, 46.11it/s] 29%|██▉       | 128/435 [00:03<00:06, 46.42it/s] 31%|███       | 133/435 [00:03<00:06, 46.57it/s] 32%|███▏      | 138/435 [00:03<00:06, 46.62it/s] 33%|███▎      | 143/435 [00:03<00:06, 46.73it/s] 34%|███▍      | 148/435 [00:03<00:06, 46.79it/s] 35%|███▌      | 153/435 [00:03<00:06, 46.80it/s] 36%|███▋      | 158/435 [00:04<00:05, 46.86it/s] 37%|███▋      | 163/435 [00:04<00:05, 46.91it/s] 39%|███▊      | 168/435 [00:04<00:05, 46.78it/s] 40%|███▉      | 173/435 [00:04<00:05, 46.85it/s] 41%|████      | 178/435 [00:04<00:05, 46.99it/s] 42%|████▏     | 183/435 [00:04<00:05, 47.05it/s] 43%|████▎     | 188/435 [00:04<00:05, 47.04it/s] 44%|████▍     | 193/435 [00:04<00:05, 47.15it/s] 46%|████▌     | 198/435 [00:04<00:05, 47.10it/s] 47%|████▋     | 203/435 [00:04<00:04, 46.92it/s] 48%|████▊     | 208/435 [00:05<00:04, 47.03it/s] 49%|████▉     | 213/435 [00:05<00:07, 29.22it/s] 50%|█████     | 218/435 [00:05<00:06, 32.89it/s] 51%|█████▏    | 223/435 [00:05<00:05, 36.15it/s] 52%|█████▏    | 228/435 [00:05<00:05, 38.89it/s] 54%|█████▎    | 233/435 [00:05<00:04, 41.01it/s] 55%|█████▍    | 238/435 [00:05<00:04, 42.64it/s] 56%|█████▌    | 243/435 [00:06<00:04, 44.00it/s] 57%|█████▋    | 248/435 [00:06<00:04, 44.89it/s] 58%|█████▊    | 253/435 [00:06<00:04, 45.33it/s] 59%|█████▉    | 258/435 [00:06<00:03, 45.75it/s] 60%|██████    | 263/435 [00:06<00:03, 45.97it/s] 62%|██████▏   | 268/435 [00:06<00:03, 46.14it/s] 63%|██████▎   | 273/435 [00:06<00:03, 46.35it/s] 64%|██████▍   | 278/435 [00:06<00:03, 46.78it/s] 65%|██████▌   | 283/435 [00:06<00:03, 46.85it/s] 66%|██████▌   | 288/435 [00:06<00:03, 46.87it/s] 67%|██████▋   | 293/435 [00:07<00:03, 46.97it/s] 69%|██████▊   | 298/435 [00:07<00:02, 46.78it/s] 70%|██████▉   | 303/435 [00:07<00:02, 46.86it/s] 71%|███████   | 308/435 [00:07<00:02, 46.71it/s] 72%|███████▏  | 313/435 [00:07<00:02, 46.49it/s] 73%|███████▎  | 318/435 [00:07<00:02, 46.63it/s] 74%|███████▍  | 323/435 [00:07<00:02, 46.78it/s] 75%|███████▌  | 328/435 [00:07<00:02, 46.38it/s] 77%|███████▋  | 333/435 [00:07<00:02, 46.90it/s] 78%|███████▊  | 338/435 [00:08<00:02, 46.83it/s] 79%|███████▉  | 343/435 [00:08<00:01, 46.69it/s] 80%|████████  | 348/435 [00:08<00:01, 46.58it/s] 81%|████████  | 353/435 [00:08<00:01, 46.52it/s] 82%|████████▏ | 358/435 [00:08<00:01, 46.57it/s] 83%|████████▎ | 363/435 [00:08<00:01, 46.58it/s] 85%|████████▍ | 368/435 [00:08<00:01, 46.72it/s] 86%|████████▌ | 373/435 [00:08<00:01, 46.90it/s] 87%|████████▋ | 378/435 [00:08<00:01, 47.00it/s] 88%|████████▊ | 383/435 [00:09<00:01, 46.90it/s] 89%|████████▉ | 388/435 [00:09<00:01, 46.85it/s] 90%|█████████ | 393/435 [00:09<00:00, 46.86it/s] 91%|█████████▏| 398/435 [00:09<00:00, 46.71it/s] 93%|█████████▎| 403/435 [00:09<00:00, 46.83it/s] 94%|█████████▍| 408/435 [00:09<00:00, 46.81it/s] 95%|█████████▍| 413/435 [00:09<00:00, 46.75it/s] 96%|█████████▌| 418/435 [00:09<00:00, 46.72it/s] 97%|█████████▋| 423/435 [00:09<00:00, 46.89it/s] 98%|█████████▊| 428/435 [00:09<00:00, 46.91it/s]100%|█████████▉| 433/435 [00:10<00:00, 46.95it/s]100%|██████████| 435/435 [00:10<00:00, 42.88it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:31:48,274 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:31:48,275 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:31:48,275 >>   eval_loss               =     0.9937
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:31:48,275 >>   eval_runtime            = 0:00:10.16
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:31:48,275 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:31:48,275 >>   eval_samples_per_second =    341.947
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:31:48,275 >>   eval_steps_per_second   =      42.78
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:31:48,275 >>   perplexity              =     2.7012
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:02,857 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:02,869 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:02,870 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:02,870 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:02,870 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:32:03,573 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:32:03,574 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:32:03,837 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:32:04,858 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:32:04,858 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:07,288 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:07,295 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:07,295 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:07,295 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:07,295 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:32:08,037 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:32:08,038 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:32:08,348 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:32:08,489 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:32:08,489 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/checkpoint-780
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:05,  1.50it/s]Extractor Predicting: 10it [00:06,  1.46it/s]Extractor Predicting: 11it [00:07,  1.46it/s]Extractor Predicting: 12it [00:08,  1.45it/s]Extractor Predicting: 13it [00:08,  1.45it/s]Extractor Predicting: 14it [00:09,  1.42it/s]Extractor Predicting: 15it [00:10,  1.45it/s]Extractor Predicting: 16it [00:10,  1.45it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:12,  1.48it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:14,  1.49it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:15,  1.51it/s]Extractor Predicting: 24it [00:16,  1.52it/s]Extractor Predicting: 25it [00:16,  1.48it/s]Extractor Predicting: 26it [00:17,  1.46it/s]Extractor Predicting: 27it [00:18,  1.48it/s]Extractor Predicting: 28it [00:18,  1.45it/s]Extractor Predicting: 29it [00:19,  1.44it/s]Extractor Predicting: 30it [00:20,  1.45it/s]Extractor Predicting: 31it [00:20,  1.45it/s]Extractor Predicting: 32it [00:21,  1.45it/s]Extractor Predicting: 33it [00:22,  1.42it/s]Extractor Predicting: 34it [00:23,  1.44it/s]Extractor Predicting: 35it [00:23,  1.48it/s]Extractor Predicting: 36it [00:24,  1.51it/s]Extractor Predicting: 37it [00:24,  1.50it/s]Extractor Predicting: 38it [00:25,  1.45it/s]Extractor Predicting: 39it [00:26,  1.47it/s]Extractor Predicting: 40it [00:27,  1.48it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.50it/s]Extractor Predicting: 43it [00:29,  1.48it/s]Extractor Predicting: 44it [00:29,  1.47it/s]Extractor Predicting: 45it [00:30,  1.46it/s]Extractor Predicting: 46it [00:31,  1.48it/s]Extractor Predicting: 47it [00:31,  1.49it/s]Extractor Predicting: 48it [00:32,  1.46it/s]Extractor Predicting: 49it [00:33,  1.49it/s]Extractor Predicting: 50it [00:33,  1.53it/s]Extractor Predicting: 51it [00:34,  1.53it/s]Extractor Predicting: 52it [00:35,  1.49it/s]Extractor Predicting: 53it [00:35,  1.48it/s]Extractor Predicting: 54it [00:36,  1.50it/s]Extractor Predicting: 55it [00:37,  1.49it/s]Extractor Predicting: 56it [00:37,  1.47it/s]Extractor Predicting: 57it [00:38,  1.47it/s]Extractor Predicting: 58it [00:39,  1.49it/s]Extractor Predicting: 59it [00:39,  1.52it/s]Extractor Predicting: 60it [00:40,  1.53it/s]Extractor Predicting: 61it [00:41,  1.53it/s]Extractor Predicting: 62it [00:41,  1.54it/s]Extractor Predicting: 63it [00:42,  1.54it/s]Extractor Predicting: 64it [00:43,  1.28it/s]Extractor Predicting: 65it [00:44,  1.35it/s]Extractor Predicting: 66it [00:44,  1.38it/s]Extractor Predicting: 67it [00:45,  1.42it/s]Extractor Predicting: 68it [00:46,  1.28it/s]Extractor Predicting: 69it [00:47,  1.34it/s]Extractor Predicting: 70it [00:47,  1.38it/s]Extractor Predicting: 71it [00:48,  1.41it/s]Extractor Predicting: 72it [00:49,  1.45it/s]Extractor Predicting: 73it [00:49,  1.40it/s]Extractor Predicting: 74it [00:50,  1.45it/s]Extractor Predicting: 75it [00:51,  1.50it/s]Extractor Predicting: 76it [00:51,  1.50it/s]Extractor Predicting: 77it [00:52,  1.47it/s]Extractor Predicting: 78it [00:53,  1.46it/s]Extractor Predicting: 79it [00:53,  1.46it/s]Extractor Predicting: 80it [00:54,  1.46it/s]Extractor Predicting: 81it [00:55,  1.45it/s]Extractor Predicting: 82it [00:55,  1.48it/s]Extractor Predicting: 83it [00:56,  1.35it/s]Extractor Predicting: 84it [00:57,  1.39it/s]Extractor Predicting: 85it [00:58,  1.41it/s]Extractor Predicting: 86it [00:58,  1.44it/s]Extractor Predicting: 87it [00:59,  1.44it/s]Extractor Predicting: 88it [01:00,  1.45it/s]Extractor Predicting: 89it [01:00,  1.48it/s]Extractor Predicting: 90it [01:01,  1.48it/s]Extractor Predicting: 91it [01:02,  1.52it/s]Extractor Predicting: 92it [01:02,  1.56it/s]Extractor Predicting: 93it [01:03,  1.50it/s]Extractor Predicting: 94it [01:04,  1.49it/s]Extractor Predicting: 95it [01:04,  1.53it/s]Extractor Predicting: 96it [01:05,  1.54it/s]Extractor Predicting: 97it [01:05,  1.54it/s]Extractor Predicting: 98it [01:06,  1.53it/s]Extractor Predicting: 99it [01:07,  1.50it/s]Extractor Predicting: 100it [01:08,  1.48it/s]Extractor Predicting: 101it [01:08,  1.48it/s]Extractor Predicting: 102it [01:09,  1.55it/s]Extractor Predicting: 103it [01:09,  1.56it/s]Extractor Predicting: 104it [01:10,  1.55it/s]Extractor Predicting: 105it [01:11,  1.55it/s]Extractor Predicting: 106it [01:11,  1.55it/s]Extractor Predicting: 107it [01:12,  1.56it/s]Extractor Predicting: 108it [01:13,  1.54it/s]Extractor Predicting: 109it [01:13,  1.54it/s]Extractor Predicting: 110it [01:14,  1.54it/s]Extractor Predicting: 111it [01:15,  1.55it/s]Extractor Predicting: 112it [01:15,  1.55it/s]Extractor Predicting: 113it [01:16,  1.58it/s]Extractor Predicting: 114it [01:16,  1.61it/s]Extractor Predicting: 115it [01:17,  1.60it/s]Extractor Predicting: 116it [01:18,  1.59it/s]Extractor Predicting: 117it [01:18,  1.59it/s]Extractor Predicting: 118it [01:19,  1.56it/s]Extractor Predicting: 119it [01:20,  1.57it/s]Extractor Predicting: 120it [01:20,  1.53it/s]Extractor Predicting: 121it [01:21,  1.51it/s]Extractor Predicting: 122it [01:22,  1.52it/s]Extractor Predicting: 123it [01:22,  1.47it/s]Extractor Predicting: 124it [01:23,  1.47it/s]Extractor Predicting: 125it [01:24,  1.49it/s]Extractor Predicting: 126it [01:24,  1.51it/s]Extractor Predicting: 127it [01:25,  1.50it/s]Extractor Predicting: 128it [01:26,  1.50it/s]Extractor Predicting: 129it [01:26,  1.51it/s]Extractor Predicting: 130it [01:27,  1.49it/s]Extractor Predicting: 131it [01:28,  1.50it/s]Extractor Predicting: 132it [01:28,  1.48it/s]Extractor Predicting: 133it [01:29,  1.50it/s]Extractor Predicting: 134it [01:30,  1.53it/s]Extractor Predicting: 135it [01:30,  1.52it/s]Extractor Predicting: 136it [01:31,  1.49it/s]Extractor Predicting: 137it [01:32,  1.53it/s]Extractor Predicting: 138it [01:32,  1.51it/s]Extractor Predicting: 139it [01:33,  1.49it/s]Extractor Predicting: 140it [01:34,  1.51it/s]Extractor Predicting: 141it [01:34,  1.53it/s]Extractor Predicting: 142it [01:35,  1.51it/s]Extractor Predicting: 143it [01:36,  1.54it/s]Extractor Predicting: 144it [01:36,  1.57it/s]Extractor Predicting: 144it [01:36,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:58,226 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:58,231 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:58,231 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:58,231 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:58,232 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:33:59,035 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:33:59,036 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:34:00,273 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:34:01,376 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:34:01,385 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:04,588 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:04,962 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:04,962 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:04,962 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:04,962 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:34:05,874 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:34:05,876 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:34:06,334 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:34:06,882 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:34:06,882 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.38202247191011235,
  "recall": 0.09778544722461892,
  "score": 0.1557133043279139,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.50it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:04,  1.47it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:08,  1.16it/s]Extractor Predicting: 13it [00:09,  1.25it/s]Extractor Predicting: 14it [00:09,  1.33it/s]Extractor Predicting: 15it [00:10,  1.40it/s]Extractor Predicting: 16it [00:11,  1.43it/s]Extractor Predicting: 17it [00:11,  1.45it/s]Extractor Predicting: 18it [00:12,  1.50it/s]Extractor Predicting: 19it [00:13,  1.55it/s]Extractor Predicting: 20it [00:13,  1.52it/s]Extractor Predicting: 21it [00:14,  1.53it/s]Extractor Predicting: 22it [00:15,  1.55it/s]Extractor Predicting: 23it [00:15,  1.56it/s]Extractor Predicting: 24it [00:16,  1.55it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:17,  1.55it/s]Extractor Predicting: 27it [00:18,  1.53it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:19,  1.54it/s]Extractor Predicting: 30it [00:20,  1.57it/s]Extractor Predicting: 31it [00:20,  1.49it/s]Extractor Predicting: 32it [00:21,  1.56it/s]Extractor Predicting: 33it [00:22,  1.54it/s]Extractor Predicting: 34it [00:22,  1.53it/s]Extractor Predicting: 35it [00:23,  1.53it/s]Extractor Predicting: 36it [00:24,  1.53it/s]Extractor Predicting: 37it [00:24,  1.55it/s]Extractor Predicting: 38it [00:25,  1.57it/s]Extractor Predicting: 39it [00:25,  1.58it/s]Extractor Predicting: 40it [00:26,  1.56it/s]Extractor Predicting: 41it [00:27,  1.33it/s]Extractor Predicting: 42it [00:28,  1.42it/s]Extractor Predicting: 43it [00:28,  1.42it/s]Extractor Predicting: 44it [00:29,  1.46it/s]Extractor Predicting: 45it [00:30,  1.47it/s]Extractor Predicting: 46it [00:30,  1.47it/s]Extractor Predicting: 47it [00:31,  1.51it/s]Extractor Predicting: 48it [00:32,  1.51it/s]Extractor Predicting: 49it [00:32,  1.54it/s]Extractor Predicting: 50it [00:33,  1.54it/s]Extractor Predicting: 51it [00:34,  1.52it/s]Extractor Predicting: 52it [00:34,  1.51it/s]Extractor Predicting: 53it [00:35,  1.53it/s]Extractor Predicting: 54it [00:36,  1.51it/s]Extractor Predicting: 55it [00:36,  1.56it/s]Extractor Predicting: 56it [00:37,  1.52it/s]Extractor Predicting: 57it [00:38,  1.51it/s]Extractor Predicting: 58it [00:38,  1.49it/s]Extractor Predicting: 59it [00:39,  1.48it/s]Extractor Predicting: 60it [00:40,  1.36it/s]Extractor Predicting: 61it [00:41,  1.39it/s]Extractor Predicting: 62it [00:41,  1.38it/s]Extractor Predicting: 63it [00:42,  1.43it/s]Extractor Predicting: 64it [00:43,  1.44it/s]Extractor Predicting: 65it [00:43,  1.50it/s]Extractor Predicting: 66it [00:44,  1.47it/s]Extractor Predicting: 67it [00:45,  1.46it/s]Extractor Predicting: 68it [00:45,  1.46it/s]Extractor Predicting: 69it [00:46,  1.47it/s]Extractor Predicting: 70it [00:47,  1.45it/s]Extractor Predicting: 71it [00:47,  1.47it/s]Extractor Predicting: 72it [00:48,  1.45it/s]Extractor Predicting: 73it [00:49,  1.44it/s]Extractor Predicting: 74it [00:49,  1.47it/s]Extractor Predicting: 75it [00:50,  1.48it/s]Extractor Predicting: 76it [00:51,  1.49it/s]Extractor Predicting: 77it [00:51,  1.50it/s]Extractor Predicting: 78it [00:52,  1.52it/s]Extractor Predicting: 79it [00:53,  1.52it/s]Extractor Predicting: 80it [00:53,  1.57it/s]Extractor Predicting: 81it [00:54,  1.57it/s]Extractor Predicting: 82it [00:55,  1.55it/s]Extractor Predicting: 83it [00:55,  1.54it/s]Extractor Predicting: 84it [00:56,  1.54it/s]Extractor Predicting: 85it [00:57,  1.52it/s]Extractor Predicting: 86it [00:57,  1.47it/s]Extractor Predicting: 87it [00:58,  1.43it/s]Extractor Predicting: 88it [00:59,  1.47it/s]Extractor Predicting: 89it [00:59,  1.46it/s]Extractor Predicting: 90it [01:00,  1.43it/s]Extractor Predicting: 91it [01:01,  1.43it/s]Extractor Predicting: 92it [01:01,  1.44it/s]Extractor Predicting: 93it [01:02,  1.46it/s]Extractor Predicting: 94it [01:03,  1.48it/s]Extractor Predicting: 95it [01:03,  1.47it/s]Extractor Predicting: 96it [01:04,  1.48it/s]Extractor Predicting: 97it [01:05,  1.45it/s]Extractor Predicting: 98it [01:06,  1.42it/s]Extractor Predicting: 99it [01:06,  1.43it/s]Extractor Predicting: 100it [01:07,  1.46it/s]Extractor Predicting: 101it [01:08,  1.47it/s]Extractor Predicting: 102it [01:08,  1.48it/s]Extractor Predicting: 103it [01:09,  1.46it/s]Extractor Predicting: 104it [01:10,  1.45it/s]Extractor Predicting: 105it [01:10,  1.47it/s]Extractor Predicting: 106it [01:11,  1.46it/s]Extractor Predicting: 107it [01:12,  1.47it/s]Extractor Predicting: 108it [01:12,  1.48it/s]Extractor Predicting: 109it [01:13,  1.47it/s]Extractor Predicting: 110it [01:14,  1.46it/s]Extractor Predicting: 111it [01:14,  1.47it/s]Extractor Predicting: 112it [01:15,  1.46it/s]Extractor Predicting: 113it [01:16,  1.46it/s]Extractor Predicting: 114it [01:16,  1.48it/s]Extractor Predicting: 115it [01:17,  1.48it/s]Extractor Predicting: 116it [01:18,  1.44it/s]Extractor Predicting: 117it [01:19,  1.44it/s]Extractor Predicting: 118it [01:19,  1.45it/s]Extractor Predicting: 119it [01:20,  1.45it/s]Extractor Predicting: 120it [01:21,  1.45it/s]Extractor Predicting: 121it [01:21,  1.46it/s]Extractor Predicting: 122it [01:22,  1.49it/s]Extractor Predicting: 123it [01:23,  1.48it/s]Extractor Predicting: 124it [01:23,  1.48it/s]Extractor Predicting: 125it [01:24,  1.48it/s]Extractor Predicting: 126it [01:25,  1.54it/s]Extractor Predicting: 127it [01:25,  1.54it/s]Extractor Predicting: 128it [01:26,  1.50it/s]Extractor Predicting: 129it [01:27,  1.54it/s]Extractor Predicting: 130it [01:27,  1.54it/s]Extractor Predicting: 131it [01:28,  1.52it/s]Extractor Predicting: 132it [01:29,  1.54it/s]Extractor Predicting: 133it [01:29,  1.57it/s]Extractor Predicting: 134it [01:30,  1.59it/s]Extractor Predicting: 135it [01:30,  1.54it/s]Extractor Predicting: 136it [01:31,  1.55it/s]Extractor Predicting: 137it [01:32,  1.56it/s]Extractor Predicting: 138it [01:32,  1.60it/s]Extractor Predicting: 139it [01:33,  1.58it/s]Extractor Predicting: 140it [01:34,  1.57it/s]Extractor Predicting: 141it [01:34,  1.55it/s]Extractor Predicting: 142it [01:35,  1.57it/s]Extractor Predicting: 143it [01:36,  1.51it/s]Extractor Predicting: 144it [01:36,  1.50it/s]Extractor Predicting: 145it [01:37,  1.51it/s]Extractor Predicting: 146it [01:38,  1.53it/s]Extractor Predicting: 147it [01:38,  1.58it/s]Extractor Predicting: 148it [01:39,  1.59it/s]Extractor Predicting: 149it [01:39,  1.60it/s]Extractor Predicting: 150it [01:40,  1.64it/s]Extractor Predicting: 151it [01:41,  1.63it/s]Extractor Predicting: 152it [01:41,  1.63it/s]Extractor Predicting: 153it [01:42,  1.63it/s]Extractor Predicting: 154it [01:42,  1.66it/s]Extractor Predicting: 155it [01:43,  1.63it/s]Extractor Predicting: 156it [01:44,  1.64it/s]Extractor Predicting: 157it [01:44,  1.67it/s]Extractor Predicting: 158it [01:45,  1.66it/s]Extractor Predicting: 159it [01:45,  1.71it/s]Extractor Predicting: 160it [01:46,  1.75it/s]Extractor Predicting: 161it [01:47,  1.67it/s]Extractor Predicting: 162it [01:47,  1.63it/s]Extractor Predicting: 163it [01:48,  1.63it/s]Extractor Predicting: 164it [01:48,  1.63it/s]Extractor Predicting: 165it [01:49,  1.67it/s]Extractor Predicting: 166it [01:50,  1.68it/s]Extractor Predicting: 167it [01:50,  1.67it/s]Extractor Predicting: 168it [01:51,  1.64it/s]Extractor Predicting: 169it [01:51,  1.67it/s]Extractor Predicting: 170it [01:52,  1.66it/s]Extractor Predicting: 171it [01:53,  1.69it/s]Extractor Predicting: 172it [01:53,  1.46it/s]Extractor Predicting: 173it [01:54,  1.52it/s]Extractor Predicting: 174it [01:55,  1.48it/s]Extractor Predicting: 175it [01:55,  1.50it/s]Extractor Predicting: 176it [01:56,  1.49it/s]Extractor Predicting: 177it [01:57,  1.49it/s]Extractor Predicting: 178it [01:57,  1.46it/s]Extractor Predicting: 179it [01:58,  1.45it/s]Extractor Predicting: 180it [01:59,  1.48it/s]Extractor Predicting: 181it [02:00,  1.47it/s]Extractor Predicting: 182it [02:00,  1.47it/s]Extractor Predicting: 183it [02:01,  1.49it/s]Extractor Predicting: 184it [02:02,  1.49it/s]Extractor Predicting: 185it [02:02,  1.49it/s]Extractor Predicting: 186it [02:03,  1.51it/s]Extractor Predicting: 187it [02:04,  1.50it/s]Extractor Predicting: 188it [02:04,  1.48it/s]Extractor Predicting: 189it [02:05,  1.50it/s]Extractor Predicting: 190it [02:06,  1.48it/s]Extractor Predicting: 191it [02:06,  1.46it/s]Extractor Predicting: 192it [02:07,  1.45it/s]Extractor Predicting: 193it [02:08,  1.45it/s]Extractor Predicting: 194it [02:08,  1.44it/s]Extractor Predicting: 195it [02:09,  1.45it/s]Extractor Predicting: 196it [02:10,  1.48it/s]Extractor Predicting: 197it [02:10,  1.50it/s]Extractor Predicting: 198it [02:11,  1.49it/s]Extractor Predicting: 199it [02:12,  1.50it/s]Extractor Predicting: 200it [02:12,  1.47it/s]Extractor Predicting: 201it [02:13,  1.46it/s]Extractor Predicting: 202it [02:14,  1.43it/s]Extractor Predicting: 203it [02:15,  1.41it/s]Extractor Predicting: 204it [02:15,  1.41it/s]Extractor Predicting: 205it [02:16,  1.41it/s]Extractor Predicting: 206it [02:17,  1.41it/s]Extractor Predicting: 207it [02:17,  1.42it/s]Extractor Predicting: 208it [02:18,  1.41it/s]Extractor Predicting: 209it [02:19,  1.38it/s]Extractor Predicting: 210it [02:20,  1.38it/s]Extractor Predicting: 211it [02:20,  1.38it/s]Extractor Predicting: 212it [02:21,  1.40it/s]Extractor Predicting: 213it [02:22,  1.40it/s]Extractor Predicting: 214it [02:22,  1.43it/s]Extractor Predicting: 215it [02:23,  1.40it/s]Extractor Predicting: 216it [02:24,  1.40it/s]Extractor Predicting: 217it [02:25,  1.41it/s]Extractor Predicting: 218it [02:25,  1.41it/s]Extractor Predicting: 219it [02:26,  1.40it/s]Extractor Predicting: 220it [02:27,  1.38it/s]Extractor Predicting: 221it [02:27,  1.37it/s]Extractor Predicting: 222it [02:28,  1.36it/s]Extractor Predicting: 223it [02:29,  1.39it/s]Extractor Predicting: 224it [02:30,  1.41it/s]Extractor Predicting: 225it [02:30,  1.41it/s]Extractor Predicting: 226it [02:31,  1.40it/s]Extractor Predicting: 227it [02:32,  1.40it/s]Extractor Predicting: 228it [02:32,  1.41it/s]Extractor Predicting: 229it [02:33,  1.43it/s]Extractor Predicting: 230it [02:34,  1.46it/s]Extractor Predicting: 231it [02:34,  1.49it/s]Extractor Predicting: 232it [02:35,  1.52it/s]Extractor Predicting: 233it [02:36,  1.54it/s]Extractor Predicting: 234it [02:36,  1.49it/s]Extractor Predicting: 235it [02:37,  1.51it/s]Extractor Predicting: 236it [02:38,  1.52it/s]Extractor Predicting: 237it [02:38,  1.51it/s]Extractor Predicting: 238it [02:39,  1.51it/s]Extractor Predicting: 239it [02:40,  1.49it/s]Extractor Predicting: 240it [02:40,  1.53it/s]Extractor Predicting: 241it [02:41,  1.55it/s]Extractor Predicting: 242it [02:42,  1.57it/s]Extractor Predicting: 243it [02:42,  1.56it/s]Extractor Predicting: 244it [02:43,  1.61it/s]Extractor Predicting: 245it [02:43,  1.57it/s]Extractor Predicting: 246it [02:44,  1.54it/s]Extractor Predicting: 247it [02:45,  1.52it/s]Extractor Predicting: 248it [02:45,  1.50it/s]Extractor Predicting: 249it [02:46,  1.53it/s]Extractor Predicting: 250it [02:47,  1.53it/s]Extractor Predicting: 251it [02:47,  1.52it/s]Extractor Predicting: 252it [02:48,  1.56it/s]Extractor Predicting: 253it [02:49,  1.54it/s]Extractor Predicting: 254it [02:49,  1.51it/s]Extractor Predicting: 255it [02:50,  1.52it/s]Extractor Predicting: 256it [02:51,  1.48it/s]Extractor Predicting: 257it [02:51,  1.45it/s]Extractor Predicting: 258it [02:52,  1.46it/s]Extractor Predicting: 259it [02:53,  1.48it/s]Extractor Predicting: 260it [02:53,  1.47it/s]Extractor Predicting: 261it [02:54,  1.46it/s]Extractor Predicting: 262it [02:55,  1.48it/s]Extractor Predicting: 263it [02:55,  1.51it/s]Extractor Predicting: 264it [02:56,  1.48it/s]Extractor Predicting: 265it [02:57,  1.48it/s]Extractor Predicting: 266it [02:58,  1.46it/s]Extractor Predicting: 267it [02:58,  1.46it/s]Extractor Predicting: 268it [02:59,  1.44it/s]Extractor Predicting: 269it [03:00,  1.45it/s]Extractor Predicting: 270it [03:00,  1.46it/s]Extractor Predicting: 271it [03:01,  1.46it/s]Extractor Predicting: 272it [03:02,  1.50it/s]Extractor Predicting: 273it [03:02,  1.45it/s]Extractor Predicting: 274it [03:03,  1.44it/s]Extractor Predicting: 275it [03:04,  1.28it/s]Extractor Predicting: 276it [03:05,  1.31it/s]Extractor Predicting: 277it [03:05,  1.35it/s]Extractor Predicting: 278it [03:06,  1.38it/s]Extractor Predicting: 279it [03:07,  1.39it/s]Extractor Predicting: 280it [03:08,  1.40it/s]Extractor Predicting: 281it [03:08,  1.44it/s]Extractor Predicting: 282it [03:09,  1.43it/s]Extractor Predicting: 283it [03:10,  1.41it/s]Extractor Predicting: 284it [03:10,  1.47it/s]Extractor Predicting: 285it [03:11,  1.45it/s]Extractor Predicting: 286it [03:12,  1.46it/s]Extractor Predicting: 287it [03:12,  1.47it/s]Extractor Predicting: 288it [03:13,  1.46it/s]Extractor Predicting: 289it [03:14,  1.46it/s]Extractor Predicting: 290it [03:14,  1.45it/s]Extractor Predicting: 291it [03:15,  1.41it/s]Extractor Predicting: 292it [03:16,  1.40it/s]Extractor Predicting: 293it [03:17,  1.43it/s]Extractor Predicting: 294it [03:17,  1.41it/s]Extractor Predicting: 295it [03:18,  1.44it/s]Extractor Predicting: 296it [03:19,  1.42it/s]Extractor Predicting: 297it [03:19,  1.44it/s]Extractor Predicting: 298it [03:20,  1.43it/s]Extractor Predicting: 299it [03:21,  1.42it/s]Extractor Predicting: 300it [03:21,  1.45it/s]Extractor Predicting: 301it [03:22,  1.42it/s]Extractor Predicting: 302it [03:23,  1.46it/s]Extractor Predicting: 303it [03:24,  1.42it/s]Extractor Predicting: 304it [03:24,  1.42it/s]Extractor Predicting: 305it [03:25,  1.42it/s]Extractor Predicting: 306it [03:26,  1.45it/s]Extractor Predicting: 307it [03:26,  1.44it/s]Extractor Predicting: 308it [03:27,  1.46it/s]Extractor Predicting: 309it [03:28,  1.47it/s]Extractor Predicting: 310it [03:28,  1.49it/s]Extractor Predicting: 311it [03:29,  1.47it/s]Extractor Predicting: 312it [03:30,  1.51it/s]Extractor Predicting: 313it [03:30,  1.55it/s]Extractor Predicting: 314it [03:31,  1.57it/s]Extractor Predicting: 315it [03:31,  1.58it/s]Extractor Predicting: 316it [03:32,  1.54it/s]Extractor Predicting: 317it [03:33,  1.52it/s]Extractor Predicting: 318it [03:34,  1.51it/s]Extractor Predicting: 319it [03:34,  1.50it/s]Extractor Predicting: 320it [03:35,  1.49it/s]Extractor Predicting: 321it [03:36,  1.48it/s]Extractor Predicting: 322it [03:36,  1.49it/s]Extractor Predicting: 323it [03:37,  1.45it/s]Extractor Predicting: 324it [03:38,  1.46it/s]Extractor Predicting: 325it [03:38,  1.46it/s]Extractor Predicting: 326it [03:39,  1.46it/s]Extractor Predicting: 327it [03:40,  1.49it/s]Extractor Predicting: 328it [03:40,  1.48it/s]Extractor Predicting: 329it [03:41,  1.50it/s]Extractor Predicting: 330it [03:42,  1.49it/s]Extractor Predicting: 331it [03:42,  1.48it/s]Extractor Predicting: 332it [03:43,  1.48it/s]Extractor Predicting: 333it [03:44,  1.50it/s]Extractor Predicting: 334it [03:44,  1.50it/s]Extractor Predicting: 335it [03:45,  1.52it/s]Extractor Predicting: 336it [03:46,  1.48it/s]Extractor Predicting: 337it [03:46,  1.49it/s]Extractor Predicting: 338it [03:47,  1.50it/s]Extractor Predicting: 339it [03:48,  1.54it/s]Extractor Predicting: 340it [03:48,  1.54it/s]Extractor Predicting: 341it [03:49,  1.53it/s]Extractor Predicting: 342it [03:50,  1.52it/s]Extractor Predicting: 343it [03:50,  1.51it/s]Extractor Predicting: 344it [03:51,  1.53it/s]Extractor Predicting: 345it [03:52,  1.49it/s]Extractor Predicting: 346it [03:52,  1.49it/s]Extractor Predicting: 347it [03:53,  1.49it/s]Extractor Predicting: 348it [03:54,  1.49it/s]Extractor Predicting: 349it [03:54,  1.51it/s]Extractor Predicting: 350it [03:55,  1.52it/s]Extractor Predicting: 351it [03:56,  1.53it/s]Extractor Predicting: 352it [03:56,  1.49it/s]Extractor Predicting: 353it [03:57,  1.51it/s]Extractor Predicting: 354it [03:57,  1.54it/s]Extractor Predicting: 355it [03:58,  1.57it/s]Extractor Predicting: 356it [03:59,  1.54it/s]Extractor Predicting: 357it [03:59,  1.54it/s]Extractor Predicting: 358it [04:00,  1.54it/s]Extractor Predicting: 359it [04:01,  1.52it/s]Extractor Predicting: 360it [04:01,  1.52it/s]Extractor Predicting: 361it [04:02,  1.53it/s]Extractor Predicting: 362it [04:03,  1.53it/s]Extractor Predicting: 363it [04:03,  1.55it/s]Extractor Predicting: 364it [04:04,  1.55it/s]Extractor Predicting: 365it [04:05,  1.57it/s]Extractor Predicting: 366it [04:05,  1.54it/s]Extractor Predicting: 367it [04:06,  1.50it/s]Extractor Predicting: 368it [04:07,  1.50it/s]Extractor Predicting: 369it [04:07,  1.51it/s]Extractor Predicting: 370it [04:08,  1.55it/s]Extractor Predicting: 371it [04:09,  1.56it/s]Extractor Predicting: 372it [04:09,  1.56it/s]Extractor Predicting: 373it [04:10,  1.53it/s]Extractor Predicting: 374it [04:11,  1.54it/s]Extractor Predicting: 375it [04:11,  1.54it/s]Extractor Predicting: 376it [04:12,  1.54it/s]Extractor Predicting: 377it [04:12,  1.56it/s]Extractor Predicting: 378it [04:13,  1.58it/s]Extractor Predicting: 379it [04:14,  1.54it/s]Extractor Predicting: 380it [04:14,  1.54it/s]Extractor Predicting: 381it [04:15,  1.55it/s]Extractor Predicting: 382it [04:16,  1.55it/s]Extractor Predicting: 383it [04:16,  1.57it/s]Extractor Predicting: 384it [04:17,  1.60it/s]Extractor Predicting: 385it [04:18,  1.56it/s]Extractor Predicting: 386it [04:18,  1.57it/s]Extractor Predicting: 387it [04:19,  1.56it/s]Extractor Predicting: 388it [04:19,  1.54it/s]Extractor Predicting: 389it [04:20,  1.55it/s]Extractor Predicting: 390it [04:21,  1.35it/s]Extractor Predicting: 391it [04:22,  1.39it/s]Extractor Predicting: 392it [04:22,  1.43it/s]Extractor Predicting: 393it [04:23,  1.47it/s]Extractor Predicting: 394it [04:24,  1.43it/s]Extractor Predicting: 395it [04:25,  1.40it/s]Extractor Predicting: 396it [04:25,  1.41it/s]Extractor Predicting: 397it [04:26,  1.41it/s]Extractor Predicting: 398it [04:27,  1.41it/s]Extractor Predicting: 399it [04:27,  1.40it/s]Extractor Predicting: 400it [04:28,  1.45it/s]Extractor Predicting: 401it [04:29,  1.43it/s]Extractor Predicting: 402it [04:29,  1.41it/s]Extractor Predicting: 403it [04:30,  1.41it/s]Extractor Predicting: 404it [04:31,  1.37it/s]Extractor Predicting: 405it [04:32,  1.38it/s]Extractor Predicting: 406it [04:32,  1.39it/s]Extractor Predicting: 407it [04:33,  1.38it/s]Extractor Predicting: 408it [04:34,  1.41it/s]Extractor Predicting: 409it [04:34,  1.43it/s]Extractor Predicting: 410it [04:35,  1.40it/s]Extractor Predicting: 411it [04:36,  1.41it/s]Extractor Predicting: 412it [04:37,  1.42it/s]Extractor Predicting: 413it [04:37,  1.45it/s]Extractor Predicting: 414it [04:38,  1.47it/s]Extractor Predicting: 415it [04:39,  1.50it/s]Extractor Predicting: 416it [04:39,  1.49it/s]Extractor Predicting: 417it [04:40,  1.50it/s]Extractor Predicting: 418it [04:41,  1.48it/s]Extractor Predicting: 419it [04:41,  1.43it/s]Extractor Predicting: 420it [04:42,  1.43it/s]Extractor Predicting: 421it [04:43,  1.51it/s]Extractor Predicting: 421it [04:43,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:03,385 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:03,394 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:03,394 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:03,395 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:03,395 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:39:04,107 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:39:04,108 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:39:04,669 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:39:05,794 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:39:05,794 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:09,542 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:09,562 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:09,563 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:09,563 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:09,563 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:39:10,286 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:39:10,287 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:39:10,969 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:39:11,138 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:39:11,138 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2939687895402784,
  "recall": 0.06904408122833086,
  "score": 0.11182416171987809,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:02,  1.37it/s]Extractor Predicting: 4it [00:02,  1.36it/s]Extractor Predicting: 5it [00:03,  1.42it/s]Extractor Predicting: 6it [00:04,  1.39it/s]Extractor Predicting: 7it [00:05,  1.38it/s]Extractor Predicting: 8it [00:05,  1.41it/s]Extractor Predicting: 9it [00:05,  1.81it/s]Extractor Predicting: 9it [00:05,  1.52it/s]
[INFO|configuration_utils.py:515] 2023-08-28 21:39:18,010 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:39:18,011 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:39:18,016 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:39:18,016 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 21:39:18,019 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:39:25,096 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 21:39:25,107 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 21:39:25,120 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:39:25,121 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:39:25,134 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:39:25,146 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:39:25,146 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:39:25,146 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:39:25,146 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:39:25,146 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:39:25,146 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.45454545454545453,
  "recall": 0.024691358024691357,
  "score": 0.046838407494145196,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 21:39:25,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:26,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:26,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:27,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:28,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:29,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:29,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:30,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:31,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:31,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:32,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:33,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:34,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:34,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:35,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:36,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:36,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:37,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:37,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:38,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:39,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:40,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:15<04:55, 15.55s/it][WARNING|generation_utils.py:914] 2023-08-28 21:39:40,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:41,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:42,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:43,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:43,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:44,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:45,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:46,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:46,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:47,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:48,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:48,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:49,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:50,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:51,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:51,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:52,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:53,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:53,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:54,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:55,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:56,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:56,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:32<04:51, 16.18s/it][WARNING|generation_utils.py:914] 2023-08-28 21:39:57,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:58,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:58,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:59,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:59,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:00,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:01,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:01,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:02,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:03,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:03,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:04,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:05,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:05,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:06,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:07,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:08,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:08,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:09,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:10,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:45<04:12, 14.88s/it][WARNING|generation_utils.py:914] 2023-08-28 21:40:10,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:11,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:12,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:12,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:13,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:14,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:15,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:15,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:16,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:17,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:18,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:19,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:20,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:20,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:21,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:22,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:23,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:23,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:24,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:25,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:26,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:01<04:04, 15.27s/it][WARNING|generation_utils.py:914] 2023-08-28 21:40:26,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:27,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:28,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:28,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:29,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:30,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:31,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:31,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:32,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:33,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:34,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:34,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:35,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:36,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:36,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:37,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:38,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:39,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:39,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:40,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:41,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:16<03:49, 15.30s/it][WARNING|generation_utils.py:914] 2023-08-28 21:40:42,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:42,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:43,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:44,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:45,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:45,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:46,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:47,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:48,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:48,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:49,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:50,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:51,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:51,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:52,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:53,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:53,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:54,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:55,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:56,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:31<03:31, 15.14s/it][WARNING|generation_utils.py:914] 2023-08-28 21:40:56,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:57,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:58,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:59,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:59,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:00,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:01,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:02,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:02,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:03,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:04,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:05,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:05,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:06,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:07,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:08,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:08,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:09,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:10,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:11,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:12,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:47<03:19, 15.36s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:12,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:13,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:14,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:15,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:16,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:17,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:18,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:19,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:20,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:20,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:21,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:22,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:23,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:24,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:25,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:26,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:26,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:27,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:28,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:29,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:30,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:31,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:06<03:18, 16.55s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:31,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:32,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:33,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:34,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:34,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:35,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:36,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:37,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:37,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:38,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:39,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:40,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:40,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:41,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:42,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:43,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:43,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:44,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:45,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:45,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:46,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:22<02:58, 16.25s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:47,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:48,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:48,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:49,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:50,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:51,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:51,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:52,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:53,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:54,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:54,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:55,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:56,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:57,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:58,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:58,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:59,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:00,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:01,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:02,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:02,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:38<02:42, 16.21s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:03,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:04,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:04,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:05,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:06,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:07,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:08,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:08,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:09,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:10,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:10,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:11,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:12,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:13,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:13,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:14,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:15,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:16,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:16,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:17,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:53<02:22, 15.79s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:18,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:19,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:19,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:20,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:21,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:22,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:22,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:23,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:24,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:24,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:25,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:26,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:27,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:27,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:28,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:29,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:30,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:31,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:31,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:32,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:33,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:08<02:05, 15.70s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:33,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:34,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:35,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:36,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:37,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:37,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:38,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:39,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:40,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:40,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:41,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:42,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:43,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:43,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:44,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:45,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:46,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:47,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:48,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:49,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:50,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:50,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:26<01:54, 16.36s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:51,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:52,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:52,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:53,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:54,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:54,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:55,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:56,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:56,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:57,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:57,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:58,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:58,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:59,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:59,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:00,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:01,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:01,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:02,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:03,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:38<01:30, 15.03s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:03,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:04,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:05,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:05,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:06,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:07,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:08,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:08,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:09,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:10,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:11,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:11,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:12,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:12,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:13,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:14,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:15,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:15,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:16,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:17,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:18,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:53<01:15, 15.03s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:18,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:19,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:20,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:20,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:21,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:22,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:22,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:23,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:24,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:24,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:25,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:26,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:26,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:27,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:28,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:29,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:29,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:30,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:31,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:32,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:07<00:58, 14.71s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:32,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:33,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:34,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:34,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:35,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:36,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:37,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:38,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:38,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:39,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:40,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:40,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:41,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:42,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:43,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:43,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:44,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:45,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:45,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:46,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:47,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:22<00:44, 14.87s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:47,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:48,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:49,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:50,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:50,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:51,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:52,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:52,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:53,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:54,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:54,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:55,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:56,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:56,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:57,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:58,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:58,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:59,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:00,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:00,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:01,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:02,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:37<00:29, 14.89s/it][WARNING|generation_utils.py:914] 2023-08-28 21:44:02,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:03,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:04,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:05,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:05,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:06,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:07,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:08,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:08,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:09,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:10,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:11,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:11,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:12,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:13,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:13,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:14,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:15,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:16,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:17,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:52<00:14, 14.90s/it][WARNING|generation_utils.py:914] 2023-08-28 21:44:17,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:18,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:19,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:20,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:21,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:22,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:22,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:23,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:24,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:25,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:25,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:26,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:27,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:28,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:29,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:29,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:30,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:31,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:32,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:33,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:08<00:00, 15.26s/it]Generating: 100%|██████████| 20/20 [05:08<00:00, 15.43s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:43,220 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:43,228 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:43,228 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:43,228 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:43,228 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:44:44,341 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:44:44,342 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:44:45,119 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:44:46,202 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:44:46,202 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:49,741 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:49,743 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:49,744 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:49,744 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:44:49,744 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:44:51,012 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:44:51,013 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:44:51,584 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:44:51,792 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:44:51,792 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : genre .', 'success_rate': 0.8328804347826086, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : head of government .', 'success_rate': 0.9390625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : military branch .', 'success_rate': 0.9300595238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : winner .', 'success_rate': 0.9270833333333334, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : characters .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.9107142857142857, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : crosses .', 'success_rate': 0.8522727272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.9226190476190477, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9330357142857143, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9578125, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9360119047619048, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8877840909090909, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 403, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 616, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : participant .', 'success_rate': 0.9345238095238095, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : participating team .', 'success_rate': 0.9375, 'errors': {''}}
['Relation : platform . Context : The game is a sequel to " The Legend of Zelda " and is a remake of the " Zelda " series . Head Entity : The Legend of Zelda , Tail Entity : Nintendo .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.9047619047619048, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.890625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : publisher .', 'success_rate': 0.9421875, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : spouse .', 'success_rate': 0.95625, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/4_ext.jsonl'}}
estimate vocab size: 10128
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10228, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.85it/s]Extractor Estimating: 2it [00:01,  1.69it/s]Extractor Estimating: 3it [00:01,  1.71it/s]Extractor Estimating: 4it [00:02,  1.62it/s]Extractor Estimating: 5it [00:02,  1.69it/s]Extractor Estimating: 6it [00:03,  1.71it/s]Extractor Estimating: 7it [00:04,  1.74it/s]Extractor Estimating: 8it [00:04,  1.65it/s]Extractor Estimating: 9it [00:05,  1.71it/s]Extractor Estimating: 10it [00:05,  1.78it/s]Extractor Estimating: 11it [00:06,  1.65it/s]Extractor Estimating: 12it [00:07,  1.71it/s]Extractor Estimating: 13it [00:07,  1.70it/s]Extractor Estimating: 14it [00:08,  1.67it/s]Extractor Estimating: 15it [00:08,  1.65it/s]Extractor Estimating: 16it [00:09,  1.69it/s]Extractor Estimating: 17it [00:10,  1.69it/s]Extractor Estimating: 18it [00:10,  1.73it/s]Extractor Estimating: 19it [00:11,  1.77it/s]Extractor Estimating: 20it [00:11,  1.78it/s]Extractor Estimating: 21it [00:12,  1.76it/s]Extractor Estimating: 22it [00:12,  1.74it/s]Extractor Estimating: 23it [00:13,  1.80it/s]Extractor Estimating: 24it [00:13,  1.73it/s]Extractor Estimating: 25it [00:14,  1.73it/s]Extractor Estimating: 26it [00:15,  1.59it/s]Extractor Estimating: 27it [00:15,  1.56it/s]Extractor Estimating: 28it [00:16,  1.56it/s]Extractor Estimating: 29it [00:17,  1.56it/s]Extractor Estimating: 30it [00:17,  1.52it/s]Extractor Estimating: 31it [00:18,  1.54it/s]Extractor Estimating: 32it [00:19,  1.54it/s]Extractor Estimating: 33it [00:19,  1.55it/s]Extractor Estimating: 34it [00:20,  1.58it/s]Extractor Estimating: 35it [00:21,  1.54it/s]Extractor Estimating: 36it [00:21,  1.53it/s]Extractor Estimating: 37it [00:22,  1.48it/s]Extractor Estimating: 38it [00:23,  1.52it/s]Extractor Estimating: 39it [00:23,  1.47it/s]Extractor Estimating: 40it [00:24,  1.51it/s]Extractor Estimating: 41it [00:25,  1.54it/s]Extractor Estimating: 42it [00:25,  1.52it/s]Extractor Estimating: 43it [00:26,  1.56it/s]Extractor Estimating: 44it [00:27,  1.51it/s]Extractor Estimating: 45it [00:27,  1.54it/s]Extractor Estimating: 46it [00:28,  1.52it/s]Extractor Estimating: 47it [00:29,  1.51it/s]Extractor Estimating: 48it [00:29,  1.53it/s]Extractor Estimating: 49it [00:30,  1.53it/s]Extractor Estimating: 50it [00:31,  1.51it/s]Extractor Estimating: 51it [00:31,  1.64it/s]Extractor Estimating: 52it [00:32,  1.70it/s]Extractor Estimating: 53it [00:32,  1.73it/s]Extractor Estimating: 54it [00:33,  1.81it/s]Extractor Estimating: 55it [00:33,  1.78it/s]Extractor Estimating: 56it [00:34,  1.73it/s]Extractor Estimating: 57it [00:34,  1.72it/s]Extractor Estimating: 58it [00:35,  1.71it/s]Extractor Estimating: 59it [00:36,  1.72it/s]Extractor Estimating: 60it [00:36,  1.76it/s]Extractor Estimating: 61it [00:37,  1.81it/s]Extractor Estimating: 62it [00:37,  1.76it/s]Extractor Estimating: 63it [00:38,  1.73it/s]Extractor Estimating: 64it [00:38,  1.73it/s]Extractor Estimating: 65it [00:39,  1.76it/s]Extractor Estimating: 66it [00:40,  1.71it/s]Extractor Estimating: 67it [00:40,  1.70it/s]Extractor Estimating: 68it [00:41,  1.75it/s]Extractor Estimating: 69it [00:41,  1.77it/s]Extractor Estimating: 70it [00:42,  1.81it/s]Extractor Estimating: 71it [00:42,  1.77it/s]Extractor Estimating: 72it [00:43,  1.76it/s]Extractor Estimating: 73it [00:44,  1.78it/s]Extractor Estimating: 74it [00:44,  1.80it/s]Extractor Estimating: 75it [00:45,  1.74it/s]Extractor Estimating: 76it [00:45,  1.69it/s]Extractor Estimating: 77it [00:46,  1.66it/s]Extractor Estimating: 78it [00:47,  1.65it/s]Extractor Estimating: 79it [00:47,  1.62it/s]Extractor Estimating: 80it [00:48,  1.64it/s]Extractor Estimating: 81it [00:48,  1.60it/s]Extractor Estimating: 82it [00:49,  1.59it/s]Extractor Estimating: 83it [00:50,  1.54it/s]Extractor Estimating: 84it [00:50,  1.54it/s]Extractor Estimating: 85it [00:51,  1.57it/s]Extractor Estimating: 86it [00:52,  1.52it/s]Extractor Estimating: 87it [00:53,  1.43it/s]Extractor Estimating: 88it [00:53,  1.48it/s]Extractor Estimating: 89it [00:54,  1.53it/s]Extractor Estimating: 90it [00:55,  1.37it/s]Extractor Estimating: 91it [00:55,  1.39it/s]Extractor Estimating: 92it [00:56,  1.45it/s]Extractor Estimating: 93it [00:57,  1.52it/s]Extractor Estimating: 94it [00:57,  1.47it/s]Extractor Estimating: 95it [00:58,  1.48it/s]Extractor Estimating: 96it [00:59,  1.53it/s]Extractor Estimating: 97it [00:59,  1.52it/s]Extractor Estimating: 98it [01:00,  1.52it/s]Extractor Estimating: 99it [01:01,  1.49it/s]Extractor Estimating: 100it [01:01,  1.50it/s]Extractor Estimating: 101it [01:02,  1.57it/s]Extractor Estimating: 102it [01:02,  1.62it/s]Extractor Estimating: 103it [01:03,  1.64it/s]Extractor Estimating: 104it [01:04,  1.48it/s]Extractor Estimating: 105it [01:04,  1.54it/s]Extractor Estimating: 106it [01:05,  1.60it/s]Extractor Estimating: 107it [01:06,  1.63it/s]Extractor Estimating: 108it [01:06,  1.64it/s]Extractor Estimating: 109it [01:07,  1.64it/s]Extractor Estimating: 110it [01:07,  1.67it/s]Extractor Estimating: 111it [01:08,  1.64it/s]Extractor Estimating: 112it [01:09,  1.62it/s]Extractor Estimating: 113it [01:09,  1.65it/s]Extractor Estimating: 114it [01:10,  1.66it/s]Extractor Estimating: 115it [01:10,  1.69it/s]Extractor Estimating: 116it [01:11,  1.68it/s]Extractor Estimating: 117it [01:12,  1.61it/s]Extractor Estimating: 118it [01:12,  1.65it/s]Extractor Estimating: 119it [01:13,  1.64it/s]Extractor Estimating: 120it [01:13,  1.66it/s]Extractor Estimating: 121it [01:14,  1.67it/s]Extractor Estimating: 122it [01:15,  1.59it/s]Extractor Estimating: 123it [01:15,  1.54it/s]Extractor Estimating: 124it [01:16,  1.60it/s]Extractor Estimating: 125it [01:17,  1.60it/s]Extractor Estimating: 126it [01:17,  1.57it/s]Extractor Estimating: 127it [01:18,  1.54it/s]Extractor Estimating: 128it [01:19,  1.53it/s]Extractor Estimating: 129it [01:19,  1.50it/s]Extractor Estimating: 130it [01:20,  1.51it/s]Extractor Estimating: 131it [01:21,  1.47it/s]Extractor Estimating: 132it [01:21,  1.43it/s]Extractor Estimating: 133it [01:22,  1.42it/s]Extractor Estimating: 134it [01:23,  1.38it/s]Extractor Estimating: 135it [01:24,  1.41it/s]Extractor Estimating: 136it [01:24,  1.41it/s]Extractor Estimating: 137it [01:25,  1.44it/s]Extractor Estimating: 138it [01:26,  1.39it/s]Extractor Estimating: 139it [01:26,  1.39it/s]Extractor Estimating: 140it [01:27,  1.37it/s]Extractor Estimating: 141it [01:28,  1.43it/s]Extractor Estimating: 142it [01:29,  1.42it/s]Extractor Estimating: 143it [01:29,  1.41it/s]Extractor Estimating: 144it [01:30,  1.44it/s]Extractor Estimating: 145it [01:31,  1.43it/s]Extractor Estimating: 146it [01:31,  1.41it/s]Extractor Estimating: 147it [01:32,  1.39it/s]Extractor Estimating: 148it [01:33,  1.43it/s]Extractor Estimating: 149it [01:33,  1.48it/s]Extractor Estimating: 150it [01:34,  1.46it/s]Extractor Estimating: 151it [01:35,  1.53it/s]Extractor Estimating: 152it [01:35,  1.56it/s]Extractor Estimating: 153it [01:36,  1.55it/s]Extractor Estimating: 154it [01:36,  1.64it/s]Extractor Estimating: 155it [01:37,  1.73it/s]Extractor Estimating: 156it [01:38,  1.76it/s]Extractor Estimating: 157it [01:38,  1.70it/s]Extractor Estimating: 158it [01:39,  1.69it/s]Extractor Estimating: 159it [01:39,  1.66it/s]Extractor Estimating: 160it [01:40,  1.64it/s]Extractor Estimating: 161it [01:41,  1.68it/s]Extractor Estimating: 162it [01:41,  1.75it/s]Extractor Estimating: 163it [01:42,  1.78it/s]Extractor Estimating: 164it [01:42,  1.76it/s]Extractor Estimating: 165it [01:43,  1.80it/s]Extractor Estimating: 166it [01:43,  1.83it/s]Extractor Estimating: 167it [01:44,  1.83it/s]Extractor Estimating: 168it [01:44,  1.73it/s]Extractor Estimating: 169it [01:45,  1.81it/s]Extractor Estimating: 170it [01:46,  1.75it/s]Extractor Estimating: 171it [01:46,  1.79it/s]Extractor Estimating: 172it [01:47,  1.76it/s]Extractor Estimating: 173it [01:47,  1.76it/s]Extractor Estimating: 174it [01:48,  1.79it/s]Extractor Estimating: 175it [01:48,  1.74it/s]Extractor Estimating: 176it [01:49,  1.59it/s]Extractor Estimating: 177it [01:50,  1.57it/s]Extractor Estimating: 178it [01:50,  1.58it/s]Extractor Estimating: 179it [01:51,  1.57it/s]Extractor Estimating: 180it [01:52,  1.57it/s]Extractor Estimating: 181it [01:52,  1.60it/s]Extractor Estimating: 182it [01:53,  1.59it/s]Extractor Estimating: 183it [01:54,  1.57it/s]Extractor Estimating: 184it [01:54,  1.54it/s]Extractor Estimating: 185it [01:55,  1.52it/s]Extractor Estimating: 186it [01:56,  1.55it/s]Extractor Estimating: 187it [01:56,  1.55it/s]Extractor Estimating: 188it [01:57,  1.60it/s]Extractor Estimating: 189it [01:57,  1.55it/s]Extractor Estimating: 190it [01:58,  1.57it/s]Extractor Estimating: 191it [01:59,  1.56it/s]Extractor Estimating: 192it [01:59,  1.57it/s]Extractor Estimating: 193it [02:00,  1.55it/s]Extractor Estimating: 194it [02:01,  1.55it/s]Extractor Estimating: 195it [02:01,  1.53it/s]Extractor Estimating: 196it [02:02,  1.53it/s]Extractor Estimating: 197it [02:03,  1.50it/s]Extractor Estimating: 198it [02:03,  1.55it/s]Extractor Estimating: 199it [02:04,  1.51it/s]Extractor Estimating: 200it [02:05,  1.51it/s]Extractor Estimating: 201it [02:05,  1.49it/s]Extractor Estimating: 202it [02:06,  1.47it/s]Extractor Estimating: 203it [02:07,  1.47it/s]Extractor Estimating: 204it [02:08,  1.34it/s]Extractor Estimating: 205it [02:08,  1.38it/s]Extractor Estimating: 206it [02:09,  1.41it/s]Extractor Estimating: 207it [02:10,  1.39it/s]Extractor Estimating: 208it [02:10,  1.40it/s]Extractor Estimating: 209it [02:11,  1.36it/s]Extractor Estimating: 210it [02:12,  1.39it/s]Extractor Estimating: 211it [02:13,  1.41it/s]Extractor Estimating: 212it [02:13,  1.39it/s]Extractor Estimating: 213it [02:14,  1.43it/s]Extractor Estimating: 214it [02:15,  1.40it/s]Extractor Estimating: 215it [02:15,  1.43it/s]Extractor Estimating: 216it [02:16,  1.43it/s]Extractor Estimating: 217it [02:17,  1.47it/s]Extractor Estimating: 218it [02:17,  1.48it/s]Extractor Estimating: 219it [02:18,  1.46it/s]Extractor Estimating: 220it [02:19,  1.44it/s]Extractor Estimating: 221it [02:20,  1.44it/s]Extractor Estimating: 222it [02:20,  1.48it/s]Extractor Estimating: 223it [02:21,  1.47it/s]Extractor Estimating: 224it [02:21,  1.50it/s]Extractor Estimating: 225it [02:22,  1.53it/s]Extractor Estimating: 226it [02:23,  1.58it/s]Extractor Estimating: 227it [02:23,  1.58it/s]Extractor Estimating: 228it [02:24,  1.62it/s]Extractor Estimating: 229it [02:25,  1.57it/s]Extractor Estimating: 230it [02:25,  1.61it/s]Extractor Estimating: 231it [02:26,  1.65it/s]Extractor Estimating: 232it [02:26,  1.62it/s]Extractor Estimating: 233it [02:27,  1.58it/s]Extractor Estimating: 234it [02:28,  1.59it/s]Extractor Estimating: 235it [02:28,  1.61it/s]Extractor Estimating: 236it [02:29,  1.59it/s]Extractor Estimating: 237it [02:30,  1.58it/s]Extractor Estimating: 238it [02:30,  1.67it/s]Extractor Estimating: 239it [02:31,  1.65it/s]Extractor Estimating: 240it [02:31,  1.67it/s]Extractor Estimating: 241it [02:32,  1.66it/s]Extractor Estimating: 242it [02:33,  1.59it/s]Extractor Estimating: 243it [02:33,  1.62it/s]Extractor Estimating: 244it [02:34,  1.59it/s]Extractor Estimating: 245it [02:34,  1.58it/s]Extractor Estimating: 246it [02:35,  1.53it/s]Extractor Estimating: 247it [02:36,  1.54it/s]Extractor Estimating: 248it [02:36,  1.60it/s]Extractor Estimating: 249it [02:37,  1.57it/s]Extractor Estimating: 250it [02:38,  1.58it/s]Extractor Estimating: 251it [02:38,  1.56it/s]Extractor Estimating: 252it [02:39,  1.55it/s]Extractor Estimating: 253it [02:40,  1.52it/s]Extractor Estimating: 254it [02:40,  1.52it/s]Extractor Estimating: 255it [02:41,  1.52it/s]Extractor Estimating: 256it [02:42,  1.45it/s]Extractor Estimating: 257it [02:42,  1.46it/s]Extractor Estimating: 258it [02:43,  1.49it/s]Extractor Estimating: 259it [02:44,  1.40it/s]Extractor Estimating: 260it [02:45,  1.44it/s]Extractor Estimating: 261it [02:45,  1.43it/s]Extractor Estimating: 262it [02:46,  1.44it/s]Extractor Estimating: 263it [02:47,  1.46it/s]Extractor Estimating: 264it [02:47,  1.47it/s]Extractor Estimating: 265it [02:48,  1.50it/s]Extractor Estimating: 266it [02:49,  1.51it/s]Extractor Estimating: 267it [02:49,  1.53it/s]Extractor Estimating: 268it [02:50,  1.54it/s]Extractor Estimating: 269it [02:51,  1.48it/s]Extractor Estimating: 270it [02:51,  1.50it/s]Extractor Estimating: 271it [02:52,  1.49it/s]Extractor Estimating: 272it [02:53,  1.50it/s]Extractor Estimating: 273it [02:53,  1.47it/s]Extractor Estimating: 274it [02:54,  1.31it/s]Extractor Estimating: 275it [02:55,  1.36it/s]Extractor Estimating: 276it [02:55,  1.43it/s]Extractor Estimating: 277it [02:56,  1.53it/s]Extractor Estimating: 278it [02:57,  1.56it/s]Extractor Estimating: 279it [02:57,  1.59it/s]Extractor Estimating: 280it [02:58,  1.57it/s]Extractor Estimating: 281it [02:58,  1.66it/s]Extractor Estimating: 282it [02:59,  1.65it/s]Extractor Estimating: 283it [03:00,  1.67it/s]Extractor Estimating: 284it [03:00,  1.73it/s]Extractor Estimating: 285it [03:01,  1.74it/s]Extractor Estimating: 286it [03:01,  1.69it/s]Extractor Estimating: 287it [03:02,  1.65it/s]Extractor Estimating: 288it [03:03,  1.63it/s]Extractor Estimating: 289it [03:03,  1.62it/s]Extractor Estimating: 290it [03:04,  1.62it/s]Extractor Estimating: 291it [03:04,  1.63it/s]Extractor Estimating: 292it [03:05,  1.60it/s]Extractor Estimating: 293it [03:06,  1.63it/s]Extractor Estimating: 294it [03:06,  1.64it/s]Extractor Estimating: 295it [03:07,  1.57it/s]Extractor Estimating: 296it [03:08,  1.62it/s]Extractor Estimating: 297it [03:08,  1.63it/s]Extractor Estimating: 298it [03:09,  1.65it/s]Extractor Estimating: 299it [03:09,  1.66it/s]Extractor Estimating: 300it [03:10,  1.61it/s]Extractor Estimating: 301it [03:11,  1.56it/s]Extractor Estimating: 302it [03:11,  1.59it/s]Extractor Estimating: 303it [03:12,  1.59it/s]Extractor Estimating: 304it [03:13,  1.57it/s]Extractor Estimating: 305it [03:13,  1.58it/s]Extractor Estimating: 306it [03:14,  1.60it/s]Extractor Estimating: 307it [03:14,  1.61it/s]Extractor Estimating: 308it [03:15,  1.60it/s]Extractor Estimating: 309it [03:16,  1.57it/s]Extractor Estimating: 310it [03:16,  1.58it/s]Extractor Estimating: 311it [03:17,  1.59it/s]Extractor Estimating: 312it [03:18,  1.59it/s]Extractor Estimating: 313it [03:18,  1.59it/s]Extractor Estimating: 314it [03:19,  1.57it/s]Extractor Estimating: 315it [03:20,  1.57it/s]Extractor Estimating: 316it [03:20,  1.54it/s]Extractor Estimating: 317it [03:21,  1.54it/s]Extractor Estimating: 318it [03:21,  1.57it/s]Extractor Estimating: 319it [03:22,  1.56it/s]Extractor Estimating: 320it [03:23,  1.43it/s]Extractor Estimating: 321it [03:24,  1.51it/s]Extractor Estimating: 322it [03:24,  1.46it/s]Extractor Estimating: 323it [03:25,  1.49it/s]Extractor Estimating: 324it [03:26,  1.53it/s]Extractor Estimating: 325it [03:26,  1.59it/s]Extractor Estimating: 326it [03:27,  1.71it/s]Extractor Estimating: 327it [03:27,  1.81it/s]Extractor Estimating: 328it [03:27,  1.93it/s]Extractor Estimating: 329it [03:28,  2.02it/s]Extractor Estimating: 330it [03:28,  2.00it/s]Extractor Estimating: 331it [03:29,  1.99it/s]Extractor Estimating: 332it [03:29,  2.00it/s]Extractor Estimating: 333it [03:30,  1.98it/s]Extractor Estimating: 334it [03:30,  1.99it/s]Extractor Estimating: 335it [03:31,  2.01it/s]Extractor Estimating: 336it [03:31,  2.00it/s]Extractor Estimating: 337it [03:32,  2.09it/s]Extractor Estimating: 338it [03:32,  2.04it/s]Extractor Estimating: 339it [03:33,  2.10it/s]Extractor Estimating: 340it [03:33,  2.13it/s]Extractor Estimating: 341it [03:34,  2.05it/s]Extractor Estimating: 342it [03:34,  2.09it/s]Extractor Estimating: 343it [03:35,  2.13it/s]Extractor Estimating: 344it [03:35,  2.12it/s]Extractor Estimating: 345it [03:36,  2.10it/s]Extractor Estimating: 346it [03:36,  2.04it/s]Extractor Estimating: 347it [03:37,  1.91it/s]Extractor Estimating: 348it [03:37,  1.96it/s]Extractor Estimating: 349it [03:38,  2.02it/s]Extractor Estimating: 350it [03:38,  2.02it/s]Extractor Estimating: 351it [03:39,  1.97it/s]Extractor Estimating: 352it [03:39,  1.82it/s]Extractor Estimating: 353it [03:40,  1.79it/s]Extractor Estimating: 354it [03:41,  1.83it/s]Extractor Estimating: 355it [03:41,  1.78it/s]Extractor Estimating: 356it [03:42,  1.75it/s]Extractor Estimating: 357it [03:42,  1.64it/s]Extractor Estimating: 358it [03:43,  1.66it/s]Extractor Estimating: 359it [03:44,  1.62it/s]Extractor Estimating: 360it [03:44,  1.65it/s]Extractor Estimating: 361it [03:45,  1.64it/s]Extractor Estimating: 362it [03:45,  1.67it/s]Extractor Estimating: 363it [03:46,  1.70it/s]Extractor Estimating: 364it [03:47,  1.75it/s]Extractor Estimating: 365it [03:47,  1.76it/s]Extractor Estimating: 366it [03:48,  1.75it/s]Extractor Estimating: 367it [03:48,  1.73it/s]Extractor Estimating: 368it [03:49,  1.71it/s]Extractor Estimating: 369it [03:49,  1.69it/s]Extractor Estimating: 370it [03:50,  1.71it/s]Extractor Estimating: 371it [03:51,  1.75it/s]Extractor Estimating: 372it [03:51,  1.77it/s]Extractor Estimating: 373it [03:52,  1.73it/s]Extractor Estimating: 374it [03:52,  1.69it/s]Extractor Estimating: 375it [03:53,  1.68it/s]Extractor Estimating: 376it [03:54,  1.70it/s]Extractor Estimating: 377it [03:54,  1.66it/s]Extractor Estimating: 378it [03:55,  1.68it/s]Extractor Estimating: 379it [03:55,  1.68it/s]Extractor Estimating: 380it [03:56,  1.71it/s]Extractor Estimating: 381it [03:56,  1.79it/s]Extractor Estimating: 382it [03:57,  1.75it/s]Extractor Estimating: 383it [03:58,  1.74it/s]Extractor Estimating: 384it [03:58,  1.77it/s]Extractor Estimating: 385it [03:59,  1.74it/s]Extractor Estimating: 386it [04:00,  1.53it/s]Extractor Estimating: 387it [04:00,  1.58it/s]Extractor Estimating: 388it [04:01,  1.64it/s]Extractor Estimating: 389it [04:01,  1.64it/s]Extractor Estimating: 390it [04:02,  1.64it/s]Extractor Estimating: 391it [04:03,  1.63it/s]Extractor Estimating: 392it [04:03,  1.66it/s]Extractor Estimating: 393it [04:04,  1.62it/s]Extractor Estimating: 394it [04:04,  1.63it/s]Extractor Estimating: 395it [04:05,  1.66it/s]Extractor Estimating: 396it [04:06,  1.61it/s]Extractor Estimating: 397it [04:06,  1.64it/s]Extractor Estimating: 398it [04:07,  1.65it/s]Extractor Estimating: 399it [04:07,  1.72it/s]Extractor Estimating: 400it [04:08,  1.72it/s]Extractor Estimating: 401it [04:08,  1.75it/s]Extractor Estimating: 402it [04:09,  1.75it/s]Extractor Estimating: 403it [04:10,  1.73it/s]Extractor Estimating: 404it [04:10,  1.71it/s]Extractor Estimating: 405it [04:11,  1.68it/s]Extractor Estimating: 406it [04:11,  1.72it/s]Extractor Estimating: 407it [04:12,  1.74it/s]Extractor Estimating: 408it [04:13,  1.62it/s]Extractor Estimating: 409it [04:13,  1.68it/s]Extractor Estimating: 410it [04:14,  1.67it/s]Extractor Estimating: 411it [04:15,  1.59it/s]Extractor Estimating: 412it [04:15,  1.61it/s]Extractor Estimating: 413it [04:16,  1.63it/s]Extractor Estimating: 414it [04:16,  1.67it/s]Extractor Estimating: 415it [04:17,  1.68it/s]Extractor Estimating: 416it [04:17,  1.66it/s]Extractor Estimating: 417it [04:18,  1.68it/s]Extractor Estimating: 418it [04:19,  1.69it/s]Extractor Estimating: 419it [04:19,  1.62it/s]Extractor Estimating: 420it [04:20,  1.64it/s]Extractor Estimating: 421it [04:20,  1.73it/s]Extractor Estimating: 422it [04:21,  1.64it/s]Extractor Estimating: 423it [04:22,  1.62it/s]Extractor Estimating: 424it [04:22,  1.60it/s]Extractor Estimating: 425it [04:23,  1.61it/s]Extractor Estimating: 426it [04:24,  1.62it/s]Extractor Estimating: 427it [04:24,  1.66it/s]Extractor Estimating: 428it [04:25,  1.64it/s]Extractor Estimating: 429it [04:25,  1.69it/s]Extractor Estimating: 430it [04:26,  1.75it/s]Extractor Estimating: 431it [04:26,  1.76it/s]Extractor Estimating: 432it [04:27,  1.73it/s]Extractor Estimating: 433it [04:28,  1.74it/s]Extractor Estimating: 434it [04:28,  1.80it/s]Extractor Estimating: 435it [04:29,  1.79it/s]Extractor Estimating: 436it [04:29,  1.74it/s]Extractor Estimating: 437it [04:30,  1.77it/s]Extractor Estimating: 438it [04:30,  1.79it/s]Extractor Estimating: 439it [04:31,  1.81it/s]Extractor Estimating: 440it [04:31,  1.81it/s]Extractor Estimating: 441it [04:32,  1.77it/s]Extractor Estimating: 442it [04:33,  1.79it/s]Extractor Estimating: 443it [04:33,  1.78it/s]Extractor Estimating: 444it [04:34,  1.80it/s]Extractor Estimating: 445it [04:34,  1.80it/s]Extractor Estimating: 446it [04:35,  1.81it/s]Extractor Estimating: 447it [04:35,  1.79it/s]Extractor Estimating: 448it [04:36,  1.77it/s]Extractor Estimating: 449it [04:37,  1.77it/s]Extractor Estimating: 450it [04:37,  1.68it/s]Extractor Estimating: 451it [04:38,  1.62it/s]Extractor Estimating: 452it [04:38,  1.61it/s]Extractor Estimating: 453it [04:39,  1.61it/s]Extractor Estimating: 454it [04:40,  1.54it/s]Extractor Estimating: 455it [04:40,  1.54it/s]Extractor Estimating: 456it [04:41,  1.57it/s]Extractor Estimating: 457it [04:42,  1.52it/s]Extractor Estimating: 458it [04:42,  1.58it/s]Extractor Estimating: 459it [04:43,  1.55it/s]Extractor Estimating: 460it [04:44,  1.53it/s]Extractor Estimating: 461it [04:44,  1.53it/s]Extractor Estimating: 462it [04:45,  1.56it/s]Extractor Estimating: 463it [04:46,  1.51it/s]Extractor Estimating: 464it [04:46,  1.54it/s]Extractor Estimating: 465it [04:47,  1.57it/s]Extractor Estimating: 466it [04:48,  1.56it/s]Extractor Estimating: 467it [04:48,  1.51it/s]Extractor Estimating: 468it [04:49,  1.52it/s]Extractor Estimating: 469it [04:50,  1.53it/s]Extractor Estimating: 470it [04:50,  1.50it/s]Extractor Estimating: 471it [04:51,  1.51it/s]Extractor Estimating: 472it [04:52,  1.48it/s]Extractor Estimating: 473it [04:52,  1.43it/s]Extractor Estimating: 474it [04:53,  1.50it/s]Extractor Estimating: 475it [04:54,  1.48it/s]Extractor Estimating: 476it [04:54,  1.45it/s]Extractor Estimating: 477it [04:55,  1.44it/s]Extractor Estimating: 478it [04:56,  1.54it/s]Extractor Estimating: 479it [04:56,  1.54it/s]Extractor Estimating: 480it [04:57,  1.56it/s]Extractor Estimating: 481it [04:58,  1.58it/s]Extractor Estimating: 482it [04:58,  1.60it/s]Extractor Estimating: 483it [04:59,  1.43it/s]Extractor Estimating: 484it [05:00,  1.47it/s]Extractor Estimating: 485it [05:00,  1.49it/s]Extractor Estimating: 486it [05:01,  1.49it/s]Extractor Estimating: 487it [05:02,  1.54it/s]Extractor Estimating: 488it [05:02,  1.51it/s]Extractor Estimating: 489it [05:03,  1.56it/s]Extractor Estimating: 490it [05:03,  1.58it/s]Extractor Estimating: 491it [05:04,  1.56it/s]Extractor Estimating: 492it [05:05,  1.56it/s]Extractor Estimating: 493it [05:05,  1.55it/s]Extractor Estimating: 494it [05:06,  1.57it/s]Extractor Estimating: 495it [05:07,  1.50it/s]Extractor Estimating: 496it [05:07,  1.47it/s]Extractor Estimating: 497it [05:08,  1.45it/s]Extractor Estimating: 498it [05:09,  1.52it/s]Extractor Estimating: 499it [05:09,  1.53it/s]Extractor Estimating: 500it [05:10,  1.50it/s]Extractor Estimating: 500it [05:10,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:18,566 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:18,576 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:18,576 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:18,576 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:18,576 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:50:19,479 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:50:19,480 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:50:20,127 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:50:21,198 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:50:21,198 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:26,440 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:26,460 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:26,460 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:26,460 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:50:26,461 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:50:27,526 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:50:27,528 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:50:28,250 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:50:28,471 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:50:28,471 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 01:02:30,469 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 01:02:30,488 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 9993 mean pseudo reward: 0.9460607755005475
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl'}
train vocab size: 17188
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17288, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=17288, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.121, loss:570.5749
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.104, loss:518.6272
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.117, loss:504.5744
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.136, loss:510.2115
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 1.121, loss:464.5216
>> valid entity prec:0.6076, rec:0.5819, f1:0.5944
>> valid relation prec:0.1508, rec:0.0705, f1:0.0960
>> valid relation with NER prec:0.1508, rec:0.0705, f1:0.0960
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.471, loss:494.1644
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 1.129, loss:499.8814
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 1.125, loss:503.1011
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 1.109, loss:478.1110
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 1.114, loss:464.2426
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5581, rec:0.6454, f1:0.5986
>> valid relation prec:0.1749, rec:0.1032, f1:0.1298
>> valid relation with NER prec:0.1749, rec:0.1032, f1:0.1298
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 266, avg_time 2.503, loss:488.6001
g_step 1200, step 366, avg_time 1.119, loss:499.2756
g_step 1300, step 49, avg_time 1.125, loss:451.6742
g_step 1400, step 149, avg_time 1.109, loss:472.0570
g_step 1500, step 249, avg_time 1.104, loss:471.6863
>> valid entity prec:0.6322, rec:0.5762, f1:0.6029
>> valid relation prec:0.2395, rec:0.1283, f1:0.1671
>> valid relation with NER prec:0.2395, rec:0.1283, f1:0.1671
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 349, avg_time 2.511, loss:476.4228
g_step 1700, step 32, avg_time 1.108, loss:448.6955
g_step 1800, step 132, avg_time 1.121, loss:428.4595
g_step 1900, step 232, avg_time 1.122, loss:449.7741
g_step 2000, step 332, avg_time 1.105, loss:447.7428
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5876, rec:0.5689, f1:0.5781
>> valid relation prec:0.1791, rec:0.0952, f1:0.1243
>> valid relation with NER prec:0.1791, rec:0.0952, f1:0.1243
g_step 2100, step 15, avg_time 2.486, loss:452.2574
g_step 2200, step 115, avg_time 1.129, loss:424.5105
g_step 2300, step 215, avg_time 1.113, loss:416.7599
g_step 2400, step 315, avg_time 1.122, loss:413.0880
g_step 2500, step 415, avg_time 1.116, loss:445.8303
>> valid entity prec:0.6064, rec:0.6223, f1:0.6142
>> valid relation prec:0.1687, rec:0.1056, f1:0.1299
>> valid relation with NER prec:0.1687, rec:0.1056, f1:0.1299
new max entity f1 on valid!
g_step 2600, step 98, avg_time 2.482, loss:374.6977
g_step 2700, step 198, avg_time 1.098, loss:391.9771
g_step 2800, step 298, avg_time 1.126, loss:403.6218
g_step 2900, step 398, avg_time 1.133, loss:437.8312
g_step 3000, step 81, avg_time 1.121, loss:386.9213
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5901, rec:0.5758, f1:0.5829
>> valid relation prec:0.1658, rec:0.0871, f1:0.1142
>> valid relation with NER prec:0.1658, rec:0.0871, f1:0.1142
g_step 3100, step 181, avg_time 2.497, loss:392.4319
g_step 3200, step 281, avg_time 1.136, loss:373.3584
g_step 3300, step 381, avg_time 1.108, loss:389.1221
g_step 3400, step 64, avg_time 1.116, loss:377.2797
g_step 3500, step 164, avg_time 1.134, loss:361.5277
>> valid entity prec:0.6196, rec:0.5635, f1:0.5902
>> valid relation prec:0.1369, rec:0.0751, f1:0.0970
>> valid relation with NER prec:0.1369, rec:0.0751, f1:0.0970
g_step 3600, step 264, avg_time 2.474, loss:367.6178
g_step 3700, step 364, avg_time 1.117, loss:376.1981
g_step 3800, step 47, avg_time 1.118, loss:361.6001
g_step 3900, step 147, avg_time 1.130, loss:347.8701
g_step 4000, step 247, avg_time 1.116, loss:353.3863
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6213, rec:0.5784, f1:0.5991
>> valid relation prec:0.1330, rec:0.0854, f1:0.1040
>> valid relation with NER prec:0.1330, rec:0.0854, f1:0.1040
g_step 4100, step 347, avg_time 2.503, loss:367.8987
g_step 4200, step 30, avg_time 1.101, loss:350.2781
g_step 4300, step 130, avg_time 1.116, loss:315.2211
g_step 4400, step 230, avg_time 1.128, loss:346.9315
g_step 4500, step 330, avg_time 1.127, loss:364.7977
>> valid entity prec:0.5969, rec:0.5619, f1:0.5789
>> valid relation prec:0.1877, rec:0.1032, f1:0.1332
>> valid relation with NER prec:0.1877, rec:0.1032, f1:0.1332
g_step 4600, step 13, avg_time 2.480, loss:363.6767
g_step 4700, step 113, avg_time 1.124, loss:334.7890
g_step 4800, step 213, avg_time 1.113, loss:326.3949
g_step 4900, step 313, avg_time 1.123, loss:350.3142
g_step 5000, step 413, avg_time 1.104, loss:340.8548
learning rate was adjusted to 0.0008
>> valid entity prec:0.5943, rec:0.6211, f1:0.6074
>> valid relation prec:0.1816, rec:0.1165, f1:0.1419
>> valid relation with NER prec:0.1816, rec:0.1165, f1:0.1419
g_step 5100, step 96, avg_time 2.505, loss:305.3607
g_step 5200, step 196, avg_time 1.098, loss:325.7112
g_step 5300, step 296, avg_time 1.124, loss:320.5435
g_step 5400, step 396, avg_time 1.122, loss:325.7870
g_step 5500, step 79, avg_time 1.119, loss:301.5001
>> valid entity prec:0.6030, rec:0.5728, f1:0.5875
>> valid relation prec:0.1582, rec:0.0935, f1:0.1175
>> valid relation with NER prec:0.1582, rec:0.0935, f1:0.1175
g_step 5600, step 179, avg_time 2.505, loss:300.1337
g_step 5700, step 279, avg_time 1.098, loss:323.4016
g_step 5800, step 379, avg_time 1.125, loss:312.0583
g_step 5900, step 62, avg_time 1.098, loss:293.5943
g_step 6000, step 162, avg_time 1.117, loss:289.1517
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5897, rec:0.6118, f1:0.6005
>> valid relation prec:0.1383, rec:0.1153, f1:0.1258
>> valid relation with NER prec:0.1383, rec:0.1153, f1:0.1258
g_step 6100, step 262, avg_time 2.503, loss:297.7308
g_step 6200, step 362, avg_time 1.113, loss:308.7367
g_step 6300, step 45, avg_time 1.108, loss:288.3033
g_step 6400, step 145, avg_time 1.122, loss:284.5949
g_step 6500, step 245, avg_time 1.111, loss:296.4188
>> valid entity prec:0.5664, rec:0.6025, f1:0.5839
>> valid relation prec:0.1663, rec:0.1156, f1:0.1364
>> valid relation with NER prec:0.1663, rec:0.1156, f1:0.1364
g_step 6600, step 345, avg_time 2.498, loss:297.4782
g_step 6700, step 28, avg_time 1.117, loss:278.4596
g_step 6800, step 128, avg_time 1.119, loss:276.5468
g_step 6900, step 228, avg_time 1.101, loss:277.5448
g_step 7000, step 328, avg_time 1.121, loss:289.9500
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.6014, rec:0.6086, f1:0.6050
>> valid relation prec:0.1559, rec:0.1044, f1:0.1250
>> valid relation with NER prec:0.1559, rec:0.1044, f1:0.1250
g_step 7100, step 11, avg_time 2.497, loss:289.9793
g_step 7200, step 111, avg_time 1.107, loss:275.6088
g_step 7300, step 211, avg_time 1.110, loss:264.4080
g_step 7400, step 311, avg_time 1.120, loss:278.0984
g_step 7500, step 411, avg_time 1.119, loss:282.4991
>> valid entity prec:0.5940, rec:0.5869, f1:0.5904
>> valid relation prec:0.1632, rec:0.1032, f1:0.1265
>> valid relation with NER prec:0.1632, rec:0.1032, f1:0.1265
g_step 7600, step 94, avg_time 2.489, loss:253.2249
g_step 7700, step 194, avg_time 1.121, loss:279.1333
g_step 7800, step 294, avg_time 1.104, loss:258.2804
g_step 7900, step 394, avg_time 1.123, loss:275.4469
g_step 8000, step 77, avg_time 1.132, loss:257.7287
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.6242, rec:0.5742, f1:0.5982
>> valid relation prec:0.1610, rec:0.1056, f1:0.1275
>> valid relation with NER prec:0.1610, rec:0.1056, f1:0.1275
g_step 8100, step 177, avg_time 2.475, loss:249.7362
g_step 8200, step 277, avg_time 1.132, loss:252.2645
g_step 8300, step 377, avg_time 1.114, loss:263.7900
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 01:02:30 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 01:02:30 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_01-02-30_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 01:02:31 - WARNING - datasets.builder -   Using custom data configuration default-456b5e69bffb2b87
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-456b5e69bffb2b87/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 01:02:31,809 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:02:31,810 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 01:02:31,810 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:02:31,811 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 01:02:31,823 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:02:31,828 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:02:31,828 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:02:31,828 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:02:31,828 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:02:31,828 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:02:31,828 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 01:02:31,984 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 01:02:35,091 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 01:02:35,091 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-456b5e69bffb2b87/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  3.68ba/s] 20%|██        | 2/10 [00:00<00:01,  4.28ba/s] 30%|███       | 3/10 [00:00<00:01,  4.58ba/s] 40%|████      | 4/10 [00:00<00:01,  4.75ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.83ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.90ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.93ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.07ba/s] 90%|█████████ | 9/10 [00:02<00:00,  4.32ba/s]100%|██████████| 10/10 [00:02<00:00,  4.50ba/s]100%|██████████| 10/10 [00:02<00:00,  4.51ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.95ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.26ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.37ba/s]100%|██████████| 4/4 [00:00<00:00,  5.51ba/s]100%|██████████| 4/4 [00:00<00:00,  4.96ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:01,  8.13ba/s] 30%|███       | 3/10 [00:00<00:00,  9.45ba/s] 50%|█████     | 5/10 [00:00<00:00,  9.85ba/s] 70%|███████   | 7/10 [00:00<00:00,  9.94ba/s] 90%|█████████ | 9/10 [00:00<00:00,  9.90ba/s]100%|██████████| 10/10 [00:01<00:00,  9.82ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.38ba/s] 50%|█████     | 2/4 [00:00<00:00,  9.21ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.52ba/s]100%|██████████| 4/4 [00:00<00:00, 10.80ba/s]
[INFO|trainer.py:414] 2023-08-29 01:02:39,901 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 01:02:39,914 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 01:02:39,915 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 01:02:39,915 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 01:02:39,915 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 01:02:39,915 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 01:02:39,915 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 01:02:39,915 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:54,  3.32it/s]  0%|          | 2/780 [00:00<03:48,  3.41it/s]  0%|          | 3/780 [00:00<03:46,  3.43it/s]  1%|          | 4/780 [00:01<03:45,  3.45it/s]  1%|          | 5/780 [00:01<03:44,  3.45it/s]  1%|          | 6/780 [00:01<03:43,  3.46it/s]  1%|          | 7/780 [00:02<03:43,  3.46it/s]  1%|          | 8/780 [00:02<03:42,  3.46it/s]  1%|          | 9/780 [00:02<03:42,  3.47it/s]  1%|▏         | 10/780 [00:02<03:42,  3.46it/s]  1%|▏         | 11/780 [00:03<03:41,  3.47it/s]  2%|▏         | 12/780 [00:03<03:41,  3.47it/s]  2%|▏         | 13/780 [00:03<03:41,  3.47it/s]  2%|▏         | 14/780 [00:04<03:41,  3.47it/s]  2%|▏         | 15/780 [00:04<03:40,  3.46it/s]  2%|▏         | 16/780 [00:04<03:40,  3.46it/s]  2%|▏         | 17/780 [00:04<03:40,  3.46it/s]  2%|▏         | 18/780 [00:05<03:39,  3.46it/s]  2%|▏         | 19/780 [00:05<03:39,  3.46it/s]  3%|▎         | 20/780 [00:05<03:40,  3.44it/s]  3%|▎         | 21/780 [00:06<03:40,  3.45it/s]  3%|▎         | 22/780 [00:06<03:39,  3.46it/s]  3%|▎         | 23/780 [00:06<03:38,  3.46it/s]  3%|▎         | 24/780 [00:06<03:38,  3.46it/s]  3%|▎         | 25/780 [00:07<03:38,  3.46it/s]  3%|▎         | 26/780 [00:07<03:37,  3.46it/s]  3%|▎         | 27/780 [00:07<03:37,  3.46it/s]  4%|▎         | 28/780 [00:08<03:37,  3.46it/s]  4%|▎         | 29/780 [00:08<03:36,  3.47it/s]  4%|▍         | 30/780 [00:08<03:36,  3.47it/s]  4%|▍         | 31/780 [00:08<03:36,  3.47it/s]  4%|▍         | 32/780 [00:09<03:35,  3.47it/s]  4%|▍         | 33/780 [00:09<03:35,  3.47it/s]  4%|▍         | 34/780 [00:09<03:35,  3.47it/s]  4%|▍         | 35/780 [00:10<03:34,  3.47it/s]  5%|▍         | 36/780 [00:10<03:34,  3.47it/s]  5%|▍         | 37/780 [00:10<03:34,  3.47it/s]  5%|▍         | 38/780 [00:10<03:34,  3.46it/s]  5%|▌         | 39/780 [00:11<03:34,  3.46it/s]  5%|▌         | 40/780 [00:11<03:33,  3.46it/s]  5%|▌         | 41/780 [00:11<03:33,  3.46it/s]  5%|▌         | 42/780 [00:12<03:33,  3.46it/s]  6%|▌         | 43/780 [00:12<03:32,  3.46it/s]  6%|▌         | 44/780 [00:12<03:32,  3.46it/s]  6%|▌         | 45/780 [00:13<03:32,  3.47it/s]  6%|▌         | 46/780 [00:13<03:31,  3.46it/s]  6%|▌         | 47/780 [00:13<03:31,  3.47it/s]  6%|▌         | 48/780 [00:13<03:31,  3.46it/s]  6%|▋         | 49/780 [00:14<03:31,  3.46it/s]  6%|▋         | 50/780 [00:14<03:30,  3.46it/s]  7%|▋         | 51/780 [00:14<03:30,  3.47it/s]  7%|▋         | 52/780 [00:15<03:30,  3.47it/s]  7%|▋         | 53/780 [00:15<03:29,  3.47it/s]  7%|▋         | 54/780 [00:15<03:29,  3.47it/s]  7%|▋         | 55/780 [00:15<03:29,  3.47it/s]  7%|▋         | 56/780 [00:16<03:29,  3.46it/s]  7%|▋         | 57/780 [00:16<03:29,  3.46it/s]  7%|▋         | 58/780 [00:16<03:28,  3.46it/s]  8%|▊         | 59/780 [00:17<03:28,  3.46it/s]  8%|▊         | 60/780 [00:17<03:28,  3.46it/s]  8%|▊         | 61/780 [00:17<03:27,  3.46it/s]  8%|▊         | 62/780 [00:17<03:27,  3.46it/s]  8%|▊         | 63/780 [00:18<03:27,  3.46it/s]  8%|▊         | 64/780 [00:18<03:26,  3.46it/s]  8%|▊         | 65/780 [00:18<03:26,  3.46it/s]  8%|▊         | 66/780 [00:19<03:26,  3.46it/s]  9%|▊         | 67/780 [00:19<03:26,  3.46it/s]  9%|▊         | 68/780 [00:19<03:25,  3.46it/s]  9%|▉         | 69/780 [00:19<03:25,  3.46it/s]  9%|▉         | 70/780 [00:20<03:25,  3.45it/s]  9%|▉         | 71/780 [00:20<03:25,  3.45it/s]  9%|▉         | 72/780 [00:20<03:24,  3.45it/s]  9%|▉         | 73/780 [00:21<03:25,  3.45it/s]  9%|▉         | 74/780 [00:21<03:24,  3.45it/s] 10%|▉         | 75/780 [00:21<03:24,  3.45it/s] 10%|▉         | 76/780 [00:21<03:23,  3.45it/s] 10%|▉         | 77/780 [00:22<03:23,  3.45it/s] 10%|█         | 78/780 [00:22<03:23,  3.45it/s] 10%|█         | 79/780 [00:22<03:22,  3.45it/s] 10%|█         | 80/780 [00:23<03:22,  3.45it/s] 10%|█         | 81/780 [00:23<03:22,  3.45it/s] 11%|█         | 82/780 [00:23<03:22,  3.45it/s] 11%|█         | 83/780 [00:23<03:21,  3.45it/s] 11%|█         | 84/780 [00:24<03:21,  3.45it/s] 11%|█         | 85/780 [00:24<03:21,  3.45it/s] 11%|█         | 86/780 [00:24<03:20,  3.46it/s] 11%|█         | 87/780 [00:25<03:20,  3.45it/s] 11%|█▏        | 88/780 [00:25<03:20,  3.46it/s] 11%|█▏        | 89/780 [00:25<03:20,  3.45it/s] 12%|█▏        | 90/780 [00:26<03:19,  3.46it/s] 12%|█▏        | 91/780 [00:26<03:19,  3.45it/s] 12%|█▏        | 92/780 [00:26<03:19,  3.45it/s] 12%|█▏        | 93/780 [00:26<03:19,  3.45it/s] 12%|█▏        | 94/780 [00:27<03:18,  3.45it/s] 12%|█▏        | 95/780 [00:27<03:18,  3.45it/s] 12%|█▏        | 96/780 [00:27<03:18,  3.45it/s] 12%|█▏        | 97/780 [00:28<03:17,  3.45it/s] 13%|█▎        | 98/780 [00:28<03:17,  3.45it/s] 13%|█▎        | 99/780 [00:28<03:17,  3.45it/s] 13%|█▎        | 100/780 [00:28<03:16,  3.46it/s] 13%|█▎        | 101/780 [00:29<03:16,  3.45it/s] 13%|█▎        | 102/780 [00:29<03:16,  3.45it/s] 13%|█▎        | 103/780 [00:29<03:16,  3.45it/s] 13%|█▎        | 104/780 [00:30<03:16,  3.45it/s] 13%|█▎        | 105/780 [00:30<03:15,  3.45it/s] 14%|█▎        | 106/780 [00:30<03:15,  3.45it/s] 14%|█▎        | 107/780 [00:30<03:15,  3.45it/s] 14%|█▍        | 108/780 [00:31<03:14,  3.45it/s] 14%|█▍        | 109/780 [00:31<03:14,  3.44it/s] 14%|█▍        | 110/780 [00:31<03:14,  3.45it/s] 14%|█▍        | 111/780 [00:32<03:13,  3.45it/s] 14%|█▍        | 112/780 [00:32<03:13,  3.45it/s] 14%|█▍        | 113/780 [00:32<03:13,  3.45it/s] 15%|█▍        | 114/780 [00:32<03:13,  3.45it/s] 15%|█▍        | 115/780 [00:33<03:12,  3.45it/s] 15%|█▍        | 116/780 [00:33<03:12,  3.45it/s] 15%|█▌        | 117/780 [00:33<03:12,  3.45it/s] 15%|█▌        | 118/780 [00:34<03:12,  3.45it/s] 15%|█▌        | 119/780 [00:34<03:11,  3.45it/s] 15%|█▌        | 120/780 [00:34<03:11,  3.45it/s] 16%|█▌        | 121/780 [00:35<03:11,  3.45it/s] 16%|█▌        | 122/780 [00:35<03:10,  3.45it/s] 16%|█▌        | 123/780 [00:35<03:10,  3.45it/s] 16%|█▌        | 124/780 [00:35<03:10,  3.45it/s] 16%|█▌        | 125/780 [00:36<03:09,  3.45it/s] 16%|█▌        | 126/780 [00:36<03:09,  3.44it/s] 16%|█▋        | 127/780 [00:36<03:09,  3.44it/s] 16%|█▋        | 128/780 [00:37<03:09,  3.45it/s] 17%|█▋        | 129/780 [00:37<03:08,  3.45it/s] 17%|█▋        | 130/780 [00:37<03:08,  3.45it/s] 17%|█▋        | 131/780 [00:37<03:08,  3.45it/s] 17%|█▋        | 132/780 [00:38<03:07,  3.45it/s] 17%|█▋        | 133/780 [00:38<03:07,  3.45it/s] 17%|█▋        | 134/780 [00:38<03:07,  3.45it/s] 17%|█▋        | 135/780 [00:39<03:07,  3.45it/s] 17%|█▋        | 136/780 [00:39<03:06,  3.45it/s] 18%|█▊        | 137/780 [00:39<03:06,  3.45it/s] 18%|█▊        | 138/780 [00:39<03:06,  3.45it/s] 18%|█▊        | 139/780 [00:40<03:05,  3.45it/s] 18%|█▊        | 140/780 [00:40<03:05,  3.45it/s] 18%|█▊        | 141/780 [00:40<03:05,  3.45it/s] 18%|█▊        | 142/780 [00:41<03:05,  3.45it/s] 18%|█▊        | 143/780 [00:41<03:04,  3.45it/s] 18%|█▊        | 144/780 [00:41<03:04,  3.44it/s] 19%|█▊        | 145/780 [00:41<03:04,  3.44it/s] 19%|█▊        | 146/780 [00:42<03:04,  3.44it/s] 19%|█▉        | 147/780 [00:42<03:03,  3.44it/s] 19%|█▉        | 148/780 [00:42<03:03,  3.44it/s] 19%|█▉        | 149/780 [00:43<03:03,  3.45it/s] 19%|█▉        | 150/780 [00:43<03:02,  3.45it/s] 19%|█▉        | 151/780 [00:43<03:02,  3.44it/s] 19%|█▉        | 152/780 [00:44<03:02,  3.44it/s] 20%|█▉        | 153/780 [00:44<03:02,  3.44it/s] 20%|█▉        | 154/780 [00:44<03:01,  3.44it/s] 20%|█▉        | 155/780 [00:44<03:01,  3.44it/s] 20%|██        | 156/780 [00:45<03:01,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 01:03:25,128 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:03:25,128 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 01:03:25,128 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.08it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.56it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.70it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.98it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.37it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.08it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.92it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.52it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.52it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.53it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.51it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.64it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.63it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.63it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.60it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.57it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.48it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.51it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.48it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.49it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.53it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.50it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.60it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.68it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.57it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.52it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.43it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.47it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.41it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.47it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.54it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.60it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.60it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.43it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.59it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.50it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.50it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.54it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.50it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.54it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.53it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.60it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.59it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.58it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.49it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.38it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.50it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.51it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 44.33it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 45.88it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.12it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.23it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.26it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.34it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.35it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.52it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.46it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.34it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.37it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.36it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.45it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.58it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.56it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.54it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.38it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.41it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.41it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.43it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.48it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.48it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.52it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.60it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.48it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.44it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.43it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.42it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.34it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.42it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.42it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.48it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.50it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.55it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.57it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.37it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.49it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.52it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 46.52it/s][A 20%|██        | 156/780 [00:54<03:01,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:03:34,534 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 01:03:34,551 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:03:36,895 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:03:36,915 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:03:36,925 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:01<54:25,  5.24s/it] 20%|██        | 158/780 [01:02<38:56,  3.76s/it] 20%|██        | 159/780 [01:02<28:06,  2.72s/it] 21%|██        | 160/780 [01:02<20:32,  1.99s/it] 21%|██        | 161/780 [01:03<15:14,  1.48s/it] 21%|██        | 162/780 [01:03<11:32,  1.12s/it] 21%|██        | 163/780 [01:03<08:58,  1.15it/s] 21%|██        | 164/780 [01:03<07:09,  1.43it/s] 21%|██        | 165/780 [01:04<05:53,  1.74it/s] 21%|██▏       | 166/780 [01:04<05:00,  2.04it/s] 21%|██▏       | 167/780 [01:04<04:23,  2.33it/s] 22%|██▏       | 168/780 [01:05<03:57,  2.58it/s] 22%|██▏       | 169/780 [01:05<03:40,  2.77it/s] 22%|██▏       | 170/780 [01:05<03:26,  2.95it/s] 22%|██▏       | 171/780 [01:06<03:17,  3.08it/s] 22%|██▏       | 172/780 [01:06<03:10,  3.19it/s] 22%|██▏       | 173/780 [01:06<03:06,  3.26it/s] 22%|██▏       | 174/780 [01:06<03:02,  3.31it/s] 22%|██▏       | 175/780 [01:07<03:00,  3.35it/s] 23%|██▎       | 176/780 [01:07<02:58,  3.38it/s] 23%|██▎       | 177/780 [01:07<02:57,  3.40it/s] 23%|██▎       | 178/780 [01:08<02:56,  3.41it/s] 23%|██▎       | 179/780 [01:08<02:55,  3.42it/s] 23%|██▎       | 180/780 [01:08<02:55,  3.42it/s] 23%|██▎       | 181/780 [01:08<02:54,  3.43it/s] 23%|██▎       | 182/780 [01:09<02:54,  3.43it/s] 23%|██▎       | 183/780 [01:09<02:53,  3.43it/s] 24%|██▎       | 184/780 [01:09<02:53,  3.43it/s] 24%|██▎       | 185/780 [01:10<02:52,  3.44it/s] 24%|██▍       | 186/780 [01:10<02:52,  3.44it/s] 24%|██▍       | 187/780 [01:10<02:52,  3.45it/s] 24%|██▍       | 188/780 [01:10<02:51,  3.45it/s] 24%|██▍       | 189/780 [01:11<02:51,  3.45it/s] 24%|██▍       | 190/780 [01:11<02:51,  3.45it/s] 24%|██▍       | 191/780 [01:11<02:51,  3.44it/s] 25%|██▍       | 192/780 [01:12<02:50,  3.45it/s] 25%|██▍       | 193/780 [01:12<02:50,  3.45it/s] 25%|██▍       | 194/780 [01:12<02:50,  3.45it/s] 25%|██▌       | 195/780 [01:12<02:49,  3.45it/s] 25%|██▌       | 196/780 [01:13<02:49,  3.45it/s] 25%|██▌       | 197/780 [01:13<02:48,  3.45it/s] 25%|██▌       | 198/780 [01:13<02:48,  3.45it/s] 26%|██▌       | 199/780 [01:14<02:48,  3.45it/s] 26%|██▌       | 200/780 [01:14<02:48,  3.45it/s] 26%|██▌       | 201/780 [01:14<02:47,  3.45it/s] 26%|██▌       | 202/780 [01:15<02:48,  3.44it/s] 26%|██▌       | 203/780 [01:15<02:47,  3.44it/s] 26%|██▌       | 204/780 [01:15<02:47,  3.45it/s] 26%|██▋       | 205/780 [01:15<02:46,  3.45it/s] 26%|██▋       | 206/780 [01:16<02:46,  3.45it/s] 27%|██▋       | 207/780 [01:16<02:46,  3.45it/s] 27%|██▋       | 208/780 [01:16<02:45,  3.45it/s] 27%|██▋       | 209/780 [01:17<02:45,  3.45it/s] 27%|██▋       | 210/780 [01:17<02:45,  3.45it/s] 27%|██▋       | 211/780 [01:17<02:44,  3.45it/s] 27%|██▋       | 212/780 [01:17<02:44,  3.45it/s] 27%|██▋       | 213/780 [01:18<02:45,  3.43it/s] 27%|██▋       | 214/780 [01:18<02:44,  3.43it/s] 28%|██▊       | 215/780 [01:18<02:44,  3.44it/s] 28%|██▊       | 216/780 [01:19<02:43,  3.44it/s] 28%|██▊       | 217/780 [01:19<02:43,  3.44it/s] 28%|██▊       | 218/780 [01:19<02:43,  3.45it/s] 28%|██▊       | 219/780 [01:19<02:42,  3.45it/s] 28%|██▊       | 220/780 [01:20<02:42,  3.45it/s] 28%|██▊       | 221/780 [01:20<02:42,  3.45it/s] 28%|██▊       | 222/780 [01:20<02:41,  3.45it/s] 29%|██▊       | 223/780 [01:21<02:41,  3.45it/s] 29%|██▊       | 224/780 [01:21<02:41,  3.44it/s] 29%|██▉       | 225/780 [01:21<02:41,  3.44it/s] 29%|██▉       | 226/780 [01:21<02:40,  3.45it/s] 29%|██▉       | 227/780 [01:22<02:40,  3.45it/s] 29%|██▉       | 228/780 [01:22<02:40,  3.45it/s] 29%|██▉       | 229/780 [01:22<02:39,  3.45it/s] 29%|██▉       | 230/780 [01:23<02:39,  3.45it/s] 30%|██▉       | 231/780 [01:23<02:39,  3.45it/s] 30%|██▉       | 232/780 [01:23<02:39,  3.44it/s] 30%|██▉       | 233/780 [01:24<02:38,  3.45it/s] 30%|███       | 234/780 [01:24<02:38,  3.45it/s] 30%|███       | 235/780 [01:24<02:38,  3.44it/s] 30%|███       | 236/780 [01:24<02:38,  3.44it/s] 30%|███       | 237/780 [01:25<02:37,  3.44it/s] 31%|███       | 238/780 [01:25<02:37,  3.45it/s] 31%|███       | 239/780 [01:25<02:36,  3.45it/s] 31%|███       | 240/780 [01:26<02:36,  3.45it/s] 31%|███       | 241/780 [01:26<02:36,  3.45it/s] 31%|███       | 242/780 [01:26<02:36,  3.45it/s] 31%|███       | 243/780 [01:26<02:35,  3.45it/s] 31%|███▏      | 244/780 [01:27<02:35,  3.45it/s] 31%|███▏      | 245/780 [01:27<02:35,  3.45it/s] 32%|███▏      | 246/780 [01:27<02:35,  3.44it/s] 32%|███▏      | 247/780 [01:28<02:34,  3.44it/s] 32%|███▏      | 248/780 [01:28<02:34,  3.44it/s] 32%|███▏      | 249/780 [01:28<02:34,  3.45it/s] 32%|███▏      | 250/780 [01:28<02:33,  3.45it/s] 32%|███▏      | 251/780 [01:29<02:33,  3.45it/s] 32%|███▏      | 252/780 [01:29<02:33,  3.45it/s] 32%|███▏      | 253/780 [01:29<02:32,  3.45it/s] 33%|███▎      | 254/780 [01:30<02:32,  3.45it/s] 33%|███▎      | 255/780 [01:30<02:32,  3.45it/s] 33%|███▎      | 256/780 [01:30<02:32,  3.45it/s] 33%|███▎      | 257/780 [01:30<02:32,  3.42it/s] 33%|███▎      | 258/780 [01:31<02:32,  3.43it/s] 33%|███▎      | 259/780 [01:31<02:31,  3.43it/s] 33%|███▎      | 260/780 [01:31<02:31,  3.44it/s] 33%|███▎      | 261/780 [01:32<02:30,  3.44it/s] 34%|███▎      | 262/780 [01:32<02:30,  3.44it/s] 34%|███▎      | 263/780 [01:32<02:30,  3.44it/s] 34%|███▍      | 264/780 [01:33<02:29,  3.44it/s] 34%|███▍      | 265/780 [01:33<02:29,  3.44it/s] 34%|███▍      | 266/780 [01:33<02:29,  3.45it/s] 34%|███▍      | 267/780 [01:33<02:28,  3.45it/s] 34%|███▍      | 268/780 [01:34<02:28,  3.44it/s] 34%|███▍      | 269/780 [01:34<02:28,  3.44it/s] 35%|███▍      | 270/780 [01:34<02:28,  3.44it/s] 35%|███▍      | 271/780 [01:35<02:27,  3.44it/s] 35%|███▍      | 272/780 [01:35<02:27,  3.44it/s] 35%|███▌      | 273/780 [01:35<02:27,  3.44it/s] 35%|███▌      | 274/780 [01:35<02:27,  3.44it/s] 35%|███▌      | 275/780 [01:36<02:26,  3.44it/s] 35%|███▌      | 276/780 [01:36<02:26,  3.44it/s] 36%|███▌      | 277/780 [01:36<02:26,  3.44it/s] 36%|███▌      | 278/780 [01:37<02:25,  3.44it/s] 36%|███▌      | 279/780 [01:37<02:25,  3.44it/s] 36%|███▌      | 280/780 [01:37<02:25,  3.44it/s] 36%|███▌      | 281/780 [01:37<02:25,  3.43it/s] 36%|███▌      | 282/780 [01:38<02:24,  3.44it/s] 36%|███▋      | 283/780 [01:38<02:24,  3.44it/s] 36%|███▋      | 284/780 [01:38<02:24,  3.44it/s] 37%|███▋      | 285/780 [01:39<02:23,  3.44it/s] 37%|███▋      | 286/780 [01:39<02:23,  3.44it/s] 37%|███▋      | 287/780 [01:39<02:23,  3.44it/s] 37%|███▋      | 288/780 [01:39<02:23,  3.44it/s] 37%|███▋      | 289/780 [01:40<02:22,  3.44it/s] 37%|███▋      | 290/780 [01:40<02:22,  3.44it/s] 37%|███▋      | 291/780 [01:40<02:22,  3.44it/s] 37%|███▋      | 292/780 [01:41<02:21,  3.44it/s] 38%|███▊      | 293/780 [01:41<02:21,  3.44it/s] 38%|███▊      | 294/780 [01:41<02:21,  3.44it/s] 38%|███▊      | 295/780 [01:42<02:20,  3.44it/s] 38%|███▊      | 296/780 [01:42<02:20,  3.44it/s] 38%|███▊      | 297/780 [01:42<02:20,  3.44it/s] 38%|███▊      | 298/780 [01:42<02:19,  3.44it/s] 38%|███▊      | 299/780 [01:43<02:21,  3.40it/s] 38%|███▊      | 300/780 [01:43<02:20,  3.42it/s] 39%|███▊      | 301/780 [01:43<02:19,  3.43it/s] 39%|███▊      | 302/780 [01:44<02:19,  3.43it/s] 39%|███▉      | 303/780 [01:44<02:18,  3.44it/s] 39%|███▉      | 304/780 [01:44<02:18,  3.44it/s] 39%|███▉      | 305/780 [01:44<02:17,  3.44it/s] 39%|███▉      | 306/780 [01:45<02:17,  3.44it/s] 39%|███▉      | 307/780 [01:45<02:17,  3.44it/s] 39%|███▉      | 308/780 [01:45<02:17,  3.44it/s] 40%|███▉      | 309/780 [01:46<02:16,  3.44it/s] 40%|███▉      | 310/780 [01:46<02:16,  3.44it/s] 40%|███▉      | 311/780 [01:46<02:16,  3.44it/s] 40%|████      | 312/780 [01:46<02:15,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 01:04:26,937 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:04:26,937 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 01:04:26,937 >>   Batch size = 8
{'eval_loss': 1.04639732837677, 'eval_runtime': 9.386, 'eval_samples_per_second': 370.445, 'eval_steps_per_second': 46.346, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.94it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.35it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.65it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.74it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.37it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.17it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.83it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.48it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.41it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.49it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.47it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.63it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.51it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.52it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.49it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.39it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.28it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.23it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.26it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.22it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.19it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.26it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.30it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.46it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.39it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.45it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.39it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.33it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.35it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.38it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.41it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.34it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 45.53it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 45.86it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.05it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.22it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.32it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.37it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.32it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 46.35it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.38it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.37it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.39it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.42it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.41it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 46.51it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.56it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.40it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.31it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.32it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.38it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.39it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.46it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.41it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.39it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.43it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.41it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.42it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.44it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.40it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.36it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.30it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.28it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.29it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.28it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.20it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.26it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.14it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.13it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.16it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.08it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.12it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.08it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.17it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.18it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.20it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.11it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.14it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.15it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.09it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.12it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.03it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 46.13it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.09it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.15it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.10it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 46.10it/s][A 40%|████      | 312/780 [01:56<02:15,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:04:36,373 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 01:04:36,392 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:04:38,652 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:04:38,672 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:04:38,685 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:03<41:11,  5.29s/it] 40%|████      | 314/780 [02:04<29:26,  3.79s/it] 40%|████      | 315/780 [02:04<21:14,  2.74s/it] 41%|████      | 316/780 [02:04<15:30,  2.01s/it] 41%|████      | 317/780 [02:05<11:30,  1.49s/it] 41%|████      | 318/780 [02:05<08:42,  1.13s/it] 41%|████      | 319/780 [02:05<06:44,  1.14it/s] 41%|████      | 320/780 [02:05<05:22,  1.43it/s] 41%|████      | 321/780 [02:06<04:25,  1.73it/s] 41%|████▏     | 322/780 [02:06<03:45,  2.03it/s] 41%|████▏     | 323/780 [02:06<03:17,  2.32it/s] 42%|████▏     | 324/780 [02:07<02:57,  2.57it/s] 42%|████▏     | 325/780 [02:07<02:43,  2.78it/s] 42%|████▏     | 326/780 [02:07<02:34,  2.95it/s] 42%|████▏     | 327/780 [02:07<02:26,  3.08it/s] 42%|████▏     | 328/780 [02:08<02:21,  3.19it/s] 42%|████▏     | 329/780 [02:08<02:18,  3.26it/s] 42%|████▏     | 330/780 [02:08<02:15,  3.32it/s] 42%|████▏     | 331/780 [02:09<02:13,  3.35it/s] 43%|████▎     | 332/780 [02:09<02:12,  3.38it/s] 43%|████▎     | 333/780 [02:09<02:11,  3.40it/s] 43%|████▎     | 334/780 [02:10<02:10,  3.41it/s] 43%|████▎     | 335/780 [02:10<02:10,  3.42it/s] 43%|████▎     | 336/780 [02:10<02:10,  3.41it/s] 43%|████▎     | 337/780 [02:10<02:09,  3.42it/s] 43%|████▎     | 338/780 [02:11<02:08,  3.43it/s] 43%|████▎     | 339/780 [02:11<02:08,  3.43it/s] 44%|████▎     | 340/780 [02:11<02:07,  3.44it/s] 44%|████▎     | 341/780 [02:12<02:07,  3.44it/s] 44%|████▍     | 342/780 [02:12<02:07,  3.44it/s] 44%|████▍     | 343/780 [02:12<02:06,  3.45it/s] 44%|████▍     | 344/780 [02:12<02:06,  3.44it/s] 44%|████▍     | 345/780 [02:13<02:06,  3.45it/s] 44%|████▍     | 346/780 [02:13<02:05,  3.44it/s] 44%|████▍     | 347/780 [02:13<02:06,  3.44it/s] 45%|████▍     | 348/780 [02:14<02:05,  3.44it/s] 45%|████▍     | 349/780 [02:14<02:05,  3.44it/s] 45%|████▍     | 350/780 [02:14<02:05,  3.44it/s] 45%|████▌     | 351/780 [02:14<02:04,  3.44it/s] 45%|████▌     | 352/780 [02:15<02:04,  3.44it/s] 45%|████▌     | 353/780 [02:15<02:03,  3.45it/s] 45%|████▌     | 354/780 [02:15<02:03,  3.44it/s] 46%|████▌     | 355/780 [02:16<02:03,  3.45it/s] 46%|████▌     | 356/780 [02:16<02:03,  3.45it/s] 46%|████▌     | 357/780 [02:16<02:02,  3.45it/s] 46%|████▌     | 358/780 [02:17<02:03,  3.43it/s] 46%|████▌     | 359/780 [02:17<02:02,  3.43it/s] 46%|████▌     | 360/780 [02:17<02:02,  3.44it/s] 46%|████▋     | 361/780 [02:17<02:01,  3.44it/s] 46%|████▋     | 362/780 [02:18<02:01,  3.44it/s] 47%|████▋     | 363/780 [02:18<02:01,  3.44it/s] 47%|████▋     | 364/780 [02:18<02:00,  3.44it/s] 47%|████▋     | 365/780 [02:19<02:00,  3.44it/s] 47%|████▋     | 366/780 [02:19<02:00,  3.44it/s] 47%|████▋     | 367/780 [02:19<01:59,  3.44it/s] 47%|████▋     | 368/780 [02:19<01:59,  3.44it/s] 47%|████▋     | 369/780 [02:20<01:59,  3.43it/s] 47%|████▋     | 370/780 [02:20<01:59,  3.44it/s] 48%|████▊     | 371/780 [02:20<01:58,  3.44it/s] 48%|████▊     | 372/780 [02:21<01:58,  3.44it/s] 48%|████▊     | 373/780 [02:21<01:58,  3.44it/s] 48%|████▊     | 374/780 [02:21<01:57,  3.44it/s] 48%|████▊     | 375/780 [02:21<01:57,  3.45it/s] 48%|████▊     | 376/780 [02:22<01:57,  3.45it/s] 48%|████▊     | 377/780 [02:22<01:56,  3.45it/s] 48%|████▊     | 378/780 [02:22<01:56,  3.45it/s] 49%|████▊     | 379/780 [02:23<01:56,  3.45it/s] 49%|████▊     | 380/780 [02:23<01:56,  3.44it/s] 49%|████▉     | 381/780 [02:23<02:25,  2.74it/s] 49%|████▉     | 382/780 [02:24<02:16,  2.91it/s] 49%|████▉     | 383/780 [02:24<02:10,  3.05it/s] 49%|████▉     | 384/780 [02:24<02:05,  3.16it/s] 49%|████▉     | 385/780 [02:25<02:01,  3.24it/s] 49%|████▉     | 386/780 [02:25<01:59,  3.30it/s] 50%|████▉     | 387/780 [02:25<01:57,  3.34it/s] 50%|████▉     | 388/780 [02:25<01:56,  3.37it/s] 50%|████▉     | 389/780 [02:26<01:55,  3.39it/s] 50%|█████     | 390/780 [02:26<01:54,  3.41it/s] 50%|█████     | 391/780 [02:26<01:53,  3.42it/s] 50%|█████     | 392/780 [02:27<01:53,  3.43it/s] 50%|█████     | 393/780 [02:27<01:52,  3.43it/s] 51%|█████     | 394/780 [02:27<01:52,  3.44it/s] 51%|█████     | 395/780 [02:28<01:51,  3.44it/s] 51%|█████     | 396/780 [02:28<01:51,  3.44it/s] 51%|█████     | 397/780 [02:28<01:51,  3.44it/s] 51%|█████     | 398/780 [02:28<01:51,  3.44it/s] 51%|█████     | 399/780 [02:29<01:51,  3.43it/s] 51%|█████▏    | 400/780 [02:29<01:50,  3.44it/s] 51%|█████▏    | 401/780 [02:29<01:50,  3.44it/s] 52%|█████▏    | 402/780 [02:30<01:49,  3.44it/s] 52%|█████▏    | 403/780 [02:30<01:49,  3.44it/s] 52%|█████▏    | 404/780 [02:30<01:49,  3.44it/s] 52%|█████▏    | 405/780 [02:30<01:48,  3.44it/s] 52%|█████▏    | 406/780 [02:31<01:48,  3.44it/s] 52%|█████▏    | 407/780 [02:31<01:48,  3.44it/s] 52%|█████▏    | 408/780 [02:31<01:47,  3.44it/s] 52%|█████▏    | 409/780 [02:32<01:47,  3.44it/s] 53%|█████▎    | 410/780 [02:32<01:47,  3.43it/s] 53%|█████▎    | 411/780 [02:32<01:47,  3.43it/s] 53%|█████▎    | 412/780 [02:32<01:47,  3.44it/s] 53%|█████▎    | 413/780 [02:33<01:46,  3.44it/s] 53%|█████▎    | 414/780 [02:33<01:46,  3.44it/s] 53%|█████▎    | 415/780 [02:33<01:45,  3.44it/s] 53%|█████▎    | 416/780 [02:34<01:45,  3.44it/s] 53%|█████▎    | 417/780 [02:34<01:45,  3.45it/s] 54%|█████▎    | 418/780 [02:34<01:45,  3.44it/s] 54%|█████▎    | 419/780 [02:34<01:44,  3.44it/s] 54%|█████▍    | 420/780 [02:35<01:44,  3.44it/s] 54%|█████▍    | 421/780 [02:35<01:44,  3.44it/s] 54%|█████▍    | 422/780 [02:35<01:44,  3.44it/s] 54%|█████▍    | 423/780 [02:36<01:43,  3.44it/s] 54%|█████▍    | 424/780 [02:36<01:43,  3.44it/s] 54%|█████▍    | 425/780 [02:36<01:43,  3.44it/s] 55%|█████▍    | 426/780 [02:37<01:42,  3.44it/s] 55%|█████▍    | 427/780 [02:37<01:42,  3.44it/s] 55%|█████▍    | 428/780 [02:37<01:42,  3.44it/s] 55%|█████▌    | 429/780 [02:37<01:41,  3.44it/s] 55%|█████▌    | 430/780 [02:38<01:41,  3.44it/s] 55%|█████▌    | 431/780 [02:38<01:41,  3.44it/s] 55%|█████▌    | 432/780 [02:38<01:41,  3.44it/s] 56%|█████▌    | 433/780 [02:39<01:40,  3.44it/s] 56%|█████▌    | 434/780 [02:39<01:40,  3.44it/s] 56%|█████▌    | 435/780 [02:39<01:40,  3.44it/s] 56%|█████▌    | 436/780 [02:39<01:40,  3.44it/s] 56%|█████▌    | 437/780 [02:40<01:39,  3.44it/s] 56%|█████▌    | 438/780 [02:40<01:39,  3.44it/s] 56%|█████▋    | 439/780 [02:40<01:39,  3.44it/s] 56%|█████▋    | 440/780 [02:41<01:38,  3.44it/s] 57%|█████▋    | 441/780 [02:41<01:38,  3.44it/s] 57%|█████▋    | 442/780 [02:41<01:38,  3.44it/s] 57%|█████▋    | 443/780 [02:41<01:37,  3.44it/s] 57%|█████▋    | 444/780 [02:42<01:37,  3.44it/s] 57%|█████▋    | 445/780 [02:42<01:37,  3.44it/s] 57%|█████▋    | 446/780 [02:42<01:36,  3.44it/s] 57%|█████▋    | 447/780 [02:43<01:36,  3.44it/s] 57%|█████▋    | 448/780 [02:43<01:36,  3.44it/s] 58%|█████▊    | 449/780 [02:43<01:36,  3.44it/s] 58%|█████▊    | 450/780 [02:43<01:35,  3.45it/s] 58%|█████▊    | 451/780 [02:44<01:35,  3.44it/s] 58%|█████▊    | 452/780 [02:44<01:35,  3.42it/s] 58%|█████▊    | 453/780 [02:44<01:35,  3.43it/s] 58%|█████▊    | 454/780 [02:45<01:34,  3.43it/s] 58%|█████▊    | 455/780 [02:45<01:34,  3.44it/s] 58%|█████▊    | 456/780 [02:45<01:34,  3.44it/s] 59%|█████▊    | 457/780 [02:46<01:33,  3.44it/s] 59%|█████▊    | 458/780 [02:46<01:33,  3.44it/s] 59%|█████▉    | 459/780 [02:46<01:33,  3.44it/s] 59%|█████▉    | 460/780 [02:46<01:32,  3.44it/s] 59%|█████▉    | 461/780 [02:47<01:32,  3.44it/s] 59%|█████▉    | 462/780 [02:47<01:32,  3.44it/s] 59%|█████▉    | 463/780 [02:47<01:32,  3.44it/s] 59%|█████▉    | 464/780 [02:48<01:31,  3.44it/s] 60%|█████▉    | 465/780 [02:48<01:31,  3.44it/s] 60%|█████▉    | 466/780 [02:48<01:31,  3.44it/s] 60%|█████▉    | 467/780 [02:48<01:30,  3.44it/s] 60%|██████    | 468/780 [02:49<01:30,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 01:05:29,183 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:05:29,183 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 01:05:29,183 >>   Batch size = 8
{'eval_loss': 1.0647953748703003, 'eval_runtime': 9.4156, 'eval_samples_per_second': 369.282, 'eval_steps_per_second': 46.2, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 58.91it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.84it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.79it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.94it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.53it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.12it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.85it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.53it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.45it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.63it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.65it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 45.32it/s][A
 16%|█▌        | 68/435 [00:01<00:08, 45.67it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 45.90it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.12it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.28it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.30it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.28it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.40it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.37it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.41it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.33it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.40it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.45it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.43it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.56it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.47it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.40it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.38it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.48it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.48it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.37it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.42it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.38it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.45it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.41it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.47it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.55it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.52it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.40it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.34it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.41it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.37it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.50it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.35it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.45it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.43it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.49it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.49it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.42it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.43it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.39it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.47it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.38it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.39it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.44it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.47it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.49it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.43it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.42it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.38it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.37it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.41it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.49it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.44it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.42it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.43it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.40it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.47it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.39it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.39it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.34it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.35it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.44it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.48it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.49it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.42it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.49it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.46it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.34it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.40it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.36it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.30it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.41it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.42it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.39it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 46.39it/s][A 60%|██████    | 468/780 [02:58<01:30,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:05:38,601 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 01:05:38,626 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:05:40,948 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:05:40,965 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:05:40,974 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:06<27:13,  5.25s/it] 60%|██████    | 470/780 [03:06<19:27,  3.77s/it] 60%|██████    | 471/780 [03:06<14:01,  2.72s/it] 61%|██████    | 472/780 [03:06<10:13,  1.99s/it] 61%|██████    | 473/780 [03:07<07:34,  1.48s/it] 61%|██████    | 474/780 [03:07<05:44,  1.12s/it] 61%|██████    | 475/780 [03:07<04:26,  1.14it/s] 61%|██████    | 476/780 [03:08<03:32,  1.43it/s] 61%|██████    | 477/780 [03:08<02:54,  1.74it/s] 61%|██████▏   | 478/780 [03:08<02:28,  2.04it/s] 61%|██████▏   | 479/780 [03:08<02:09,  2.32it/s] 62%|██████▏   | 480/780 [03:09<01:56,  2.58it/s] 62%|██████▏   | 481/780 [03:09<01:47,  2.78it/s] 62%|██████▏   | 482/780 [03:09<01:40,  2.95it/s] 62%|██████▏   | 483/780 [03:10<01:36,  3.08it/s] 62%|██████▏   | 484/780 [03:10<01:32,  3.19it/s] 62%|██████▏   | 485/780 [03:10<01:30,  3.26it/s] 62%|██████▏   | 486/780 [03:10<01:28,  3.32it/s] 62%|██████▏   | 487/780 [03:11<01:27,  3.36it/s] 63%|██████▎   | 488/780 [03:11<01:26,  3.38it/s] 63%|██████▎   | 489/780 [03:11<01:25,  3.40it/s] 63%|██████▎   | 490/780 [03:12<01:24,  3.41it/s] 63%|██████▎   | 491/780 [03:12<01:24,  3.42it/s] 63%|██████▎   | 492/780 [03:12<01:24,  3.41it/s] 63%|██████▎   | 493/780 [03:13<01:23,  3.42it/s] 63%|██████▎   | 494/780 [03:13<01:23,  3.43it/s] 63%|██████▎   | 495/780 [03:13<01:23,  3.43it/s] 64%|██████▎   | 496/780 [03:13<01:22,  3.44it/s] 64%|██████▎   | 497/780 [03:14<01:22,  3.44it/s] 64%|██████▍   | 498/780 [03:14<01:21,  3.44it/s] 64%|██████▍   | 499/780 [03:14<01:21,  3.45it/s] 64%|██████▍   | 500/780 [03:15<01:21,  3.45it/s]                                                  64%|██████▍   | 500/780 [03:15<01:21,  3.45it/s] 64%|██████▍   | 501/780 [03:15<01:20,  3.45it/s] 64%|██████▍   | 502/780 [03:15<01:20,  3.44it/s] 64%|██████▍   | 503/780 [03:15<01:20,  3.43it/s] 65%|██████▍   | 504/780 [03:16<01:20,  3.44it/s] 65%|██████▍   | 505/780 [03:16<01:19,  3.44it/s] 65%|██████▍   | 506/780 [03:16<01:19,  3.44it/s] 65%|██████▌   | 507/780 [03:17<01:19,  3.44it/s] 65%|██████▌   | 508/780 [03:17<01:18,  3.45it/s] 65%|██████▌   | 509/780 [03:17<01:18,  3.45it/s] 65%|██████▌   | 510/780 [03:17<01:18,  3.44it/s] 66%|██████▌   | 511/780 [03:18<01:18,  3.44it/s] 66%|██████▌   | 512/780 [03:18<01:17,  3.44it/s] 66%|██████▌   | 513/780 [03:18<01:17,  3.45it/s] 66%|██████▌   | 514/780 [03:19<01:17,  3.44it/s] 66%|██████▌   | 515/780 [03:19<01:17,  3.44it/s] 66%|██████▌   | 516/780 [03:19<01:16,  3.44it/s] 66%|██████▋   | 517/780 [03:19<01:16,  3.44it/s] 66%|██████▋   | 518/780 [03:20<01:16,  3.44it/s] 67%|██████▋   | 519/780 [03:20<01:15,  3.44it/s] 67%|██████▋   | 520/780 [03:20<01:15,  3.44it/s] 67%|██████▋   | 521/780 [03:21<01:15,  3.44it/s] 67%|██████▋   | 522/780 [03:21<01:14,  3.45it/s] 67%|██████▋   | 523/780 [03:21<01:14,  3.45it/s] 67%|██████▋   | 524/780 [03:22<01:14,  3.44it/s] 67%|██████▋   | 525/780 [03:22<01:14,  3.44it/s] 67%|██████▋   | 526/780 [03:22<01:13,  3.44it/s] 68%|██████▊   | 527/780 [03:22<01:13,  3.44it/s] 68%|██████▊   | 528/780 [03:23<01:13,  3.44it/s] 68%|██████▊   | 529/780 [03:23<01:12,  3.44it/s] 68%|██████▊   | 530/780 [03:23<01:12,  3.43it/s] 68%|██████▊   | 531/780 [03:24<01:12,  3.42it/s] 68%|██████▊   | 532/780 [03:24<01:12,  3.43it/s] 68%|██████▊   | 533/780 [03:24<01:11,  3.44it/s] 68%|██████▊   | 534/780 [03:24<01:11,  3.44it/s] 69%|██████▊   | 535/780 [03:25<01:11,  3.44it/s] 69%|██████▊   | 536/780 [03:25<01:16,  3.21it/s] 69%|██████▉   | 537/780 [03:25<01:14,  3.27it/s] 69%|██████▉   | 538/780 [03:26<01:12,  3.32it/s] 69%|██████▉   | 539/780 [03:26<01:11,  3.36it/s] 69%|██████▉   | 540/780 [03:26<01:10,  3.38it/s] 69%|██████▉   | 541/780 [03:27<01:10,  3.40it/s] 69%|██████▉   | 542/780 [03:27<01:09,  3.41it/s] 70%|██████▉   | 543/780 [03:27<01:10,  3.34it/s] 70%|██████▉   | 544/780 [03:27<01:09,  3.37it/s] 70%|██████▉   | 545/780 [03:28<01:09,  3.39it/s] 70%|███████   | 546/780 [03:28<01:08,  3.41it/s] 70%|███████   | 547/780 [03:28<01:08,  3.42it/s] 70%|███████   | 548/780 [03:29<01:07,  3.43it/s] 70%|███████   | 549/780 [03:29<01:07,  3.43it/s] 71%|███████   | 550/780 [03:29<01:06,  3.44it/s] 71%|███████   | 551/780 [03:29<01:06,  3.44it/s] 71%|███████   | 552/780 [03:30<01:06,  3.44it/s] 71%|███████   | 553/780 [03:30<01:06,  3.43it/s] 71%|███████   | 554/780 [03:30<01:05,  3.44it/s] 71%|███████   | 555/780 [03:31<01:05,  3.44it/s] 71%|███████▏  | 556/780 [03:31<01:05,  3.44it/s] 71%|███████▏  | 557/780 [03:31<01:04,  3.44it/s] 72%|███████▏  | 558/780 [03:32<01:04,  3.44it/s] 72%|███████▏  | 559/780 [03:32<01:04,  3.44it/s] 72%|███████▏  | 560/780 [03:32<01:03,  3.44it/s] 72%|███████▏  | 561/780 [03:32<01:03,  3.44it/s] 72%|███████▏  | 562/780 [03:33<01:03,  3.44it/s] 72%|███████▏  | 563/780 [03:33<01:02,  3.45it/s] 72%|███████▏  | 564/780 [03:33<01:03,  3.43it/s] 72%|███████▏  | 565/780 [03:34<01:02,  3.44it/s] 73%|███████▎  | 566/780 [03:34<01:02,  3.44it/s] 73%|███████▎  | 567/780 [03:34<01:01,  3.44it/s] 73%|███████▎  | 568/780 [03:34<01:01,  3.44it/s] 73%|███████▎  | 569/780 [03:35<01:01,  3.44it/s] 73%|███████▎  | 570/780 [03:35<01:00,  3.44it/s] 73%|███████▎  | 571/780 [03:35<01:00,  3.44it/s] 73%|███████▎  | 572/780 [03:36<01:00,  3.44it/s] 73%|███████▎  | 573/780 [03:36<01:00,  3.45it/s] 74%|███████▎  | 574/780 [03:36<00:59,  3.45it/s] 74%|███████▎  | 575/780 [03:36<00:59,  3.43it/s] 74%|███████▍  | 576/780 [03:37<00:59,  3.44it/s] 74%|███████▍  | 577/780 [03:37<00:59,  3.44it/s] 74%|███████▍  | 578/780 [03:37<00:58,  3.44it/s] 74%|███████▍  | 579/780 [03:38<00:58,  3.44it/s] 74%|███████▍  | 580/780 [03:38<00:58,  3.44it/s] 74%|███████▍  | 581/780 [03:38<00:57,  3.44it/s] 75%|███████▍  | 582/780 [03:38<00:57,  3.44it/s] 75%|███████▍  | 583/780 [03:39<00:57,  3.44it/s] 75%|███████▍  | 584/780 [03:39<00:56,  3.45it/s] 75%|███████▌  | 585/780 [03:39<00:56,  3.44it/s] 75%|███████▌  | 586/780 [03:40<00:56,  3.45it/s] 75%|███████▌  | 587/780 [03:40<00:56,  3.44it/s] 75%|███████▌  | 588/780 [03:40<00:55,  3.45it/s] 76%|███████▌  | 589/780 [03:41<00:55,  3.44it/s] 76%|███████▌  | 590/780 [03:41<00:55,  3.44it/s] 76%|███████▌  | 591/780 [03:41<00:54,  3.44it/s] 76%|███████▌  | 592/780 [03:41<00:54,  3.44it/s] 76%|███████▌  | 593/780 [03:42<00:54,  3.44it/s] 76%|███████▌  | 594/780 [03:42<00:54,  3.44it/s] 76%|███████▋  | 595/780 [03:42<00:53,  3.44it/s] 76%|███████▋  | 596/780 [03:43<00:53,  3.44it/s] 77%|███████▋  | 597/780 [03:43<00:53,  3.44it/s] 77%|███████▋  | 598/780 [03:43<00:52,  3.44it/s] 77%|███████▋  | 599/780 [03:43<00:52,  3.44it/s] 77%|███████▋  | 600/780 [03:44<00:52,  3.44it/s] 77%|███████▋  | 601/780 [03:44<00:51,  3.44it/s] 77%|███████▋  | 602/780 [03:44<00:51,  3.45it/s] 77%|███████▋  | 603/780 [03:45<00:51,  3.44it/s] 77%|███████▋  | 604/780 [03:45<00:51,  3.45it/s] 78%|███████▊  | 605/780 [03:45<00:50,  3.44it/s] 78%|███████▊  | 606/780 [03:45<00:50,  3.43it/s] 78%|███████▊  | 607/780 [03:46<00:50,  3.43it/s] 78%|███████▊  | 608/780 [03:46<00:50,  3.44it/s] 78%|███████▊  | 609/780 [03:46<00:49,  3.44it/s] 78%|███████▊  | 610/780 [03:47<00:49,  3.44it/s] 78%|███████▊  | 611/780 [03:47<00:49,  3.44it/s] 78%|███████▊  | 612/780 [03:47<00:48,  3.44it/s] 79%|███████▊  | 613/780 [03:47<00:48,  3.44it/s] 79%|███████▊  | 614/780 [03:48<00:48,  3.44it/s] 79%|███████▉  | 615/780 [03:48<00:47,  3.44it/s] 79%|███████▉  | 616/780 [03:48<00:47,  3.44it/s] 79%|███████▉  | 617/780 [03:49<00:47,  3.44it/s] 79%|███████▉  | 618/780 [03:49<00:47,  3.44it/s] 79%|███████▉  | 619/780 [03:49<00:46,  3.44it/s] 79%|███████▉  | 620/780 [03:50<00:46,  3.44it/s] 80%|███████▉  | 621/780 [03:50<00:46,  3.44it/s] 80%|███████▉  | 622/780 [03:50<00:46,  3.38it/s] 80%|███████▉  | 623/780 [03:50<00:46,  3.39it/s] 80%|████████  | 624/780 [03:51<00:45,  3.40it/s][INFO|trainer.py:2140] 2023-08-29 01:06:31,170 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:06:31,170 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 01:06:31,170 >>   Batch size = 8
{'eval_loss': 1.08748197555542, 'eval_runtime': 9.3979, 'eval_samples_per_second': 369.976, 'eval_steps_per_second': 46.287, 'epoch': 3.0}
{'loss': 0.4062, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.54it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.35it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.58it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.80it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.41it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.07it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.77it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.43it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.40it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.41it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.40it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 46.37it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.47it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.57it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.61it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.45it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.43it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.36it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.32it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.39it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.31it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.29it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.45it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.50it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.43it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.49it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.45it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.36it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.29it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.34it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.36it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.46it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.46it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.37it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.42it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.44it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.46it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.50it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.41it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.41it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.34it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.35it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.42it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.42it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.45it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.44it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.39it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.42it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.48it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.29it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.32it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.40it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.38it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.39it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.36it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.51it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.38it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.46it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.31it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.33it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.36it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.27it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.40it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.38it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.33it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.44it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.44it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.49it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.46it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.40it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.29it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.35it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.32it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.38it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.39it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.43it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.40it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.42it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.41it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.33it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.39it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.40it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.26it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.40it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.27it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.40it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 46.40it/s][A 80%|████████  | 624/780 [04:00<00:45,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:06:40,590 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 01:06:40,605 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:06:43,068 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:06:43,091 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:06:43,101 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:08<13:55,  5.39s/it] 80%|████████  | 626/780 [04:08<09:54,  3.86s/it] 80%|████████  | 627/780 [04:09<07:06,  2.79s/it] 81%|████████  | 628/780 [04:09<05:10,  2.04s/it] 81%|████████  | 629/780 [04:09<03:48,  1.51s/it] 81%|████████  | 630/780 [04:09<02:52,  1.15s/it] 81%|████████  | 631/780 [04:10<02:12,  1.12it/s] 81%|████████  | 632/780 [04:10<01:45,  1.41it/s] 81%|████████  | 633/780 [04:10<01:25,  1.71it/s] 81%|████████▏ | 634/780 [04:11<01:12,  2.02it/s] 81%|████████▏ | 635/780 [04:11<01:02,  2.30it/s] 82%|████████▏ | 636/780 [04:11<00:56,  2.56it/s] 82%|████████▏ | 637/780 [04:11<00:51,  2.77it/s] 82%|████████▏ | 638/780 [04:12<00:48,  2.94it/s] 82%|████████▏ | 639/780 [04:12<00:45,  3.08it/s] 82%|████████▏ | 640/780 [04:12<00:44,  3.18it/s] 82%|████████▏ | 641/780 [04:13<00:42,  3.26it/s] 82%|████████▏ | 642/780 [04:13<00:41,  3.31it/s] 82%|████████▏ | 643/780 [04:13<00:40,  3.35it/s] 83%|████████▎ | 644/780 [04:14<00:40,  3.38it/s] 83%|████████▎ | 645/780 [04:14<00:39,  3.40it/s] 83%|████████▎ | 646/780 [04:14<00:39,  3.41it/s] 83%|████████▎ | 647/780 [04:14<00:38,  3.42it/s] 83%|████████▎ | 648/780 [04:15<00:38,  3.42it/s] 83%|████████▎ | 649/780 [04:15<00:38,  3.43it/s] 83%|████████▎ | 650/780 [04:15<00:37,  3.44it/s] 83%|████████▎ | 651/780 [04:16<00:37,  3.44it/s] 84%|████████▎ | 652/780 [04:16<00:37,  3.44it/s] 84%|████████▎ | 653/780 [04:16<00:36,  3.44it/s] 84%|████████▍ | 654/780 [04:16<00:36,  3.45it/s] 84%|████████▍ | 655/780 [04:17<00:36,  3.45it/s] 84%|████████▍ | 656/780 [04:17<00:35,  3.45it/s] 84%|████████▍ | 657/780 [04:17<00:35,  3.45it/s] 84%|████████▍ | 658/780 [04:18<00:35,  3.45it/s] 84%|████████▍ | 659/780 [04:18<00:35,  3.44it/s] 85%|████████▍ | 660/780 [04:18<00:34,  3.44it/s] 85%|████████▍ | 661/780 [04:18<00:34,  3.44it/s] 85%|████████▍ | 662/780 [04:19<00:34,  3.44it/s] 85%|████████▌ | 663/780 [04:19<00:33,  3.44it/s] 85%|████████▌ | 664/780 [04:19<00:33,  3.44it/s] 85%|████████▌ | 665/780 [04:20<00:33,  3.45it/s] 85%|████████▌ | 666/780 [04:20<00:33,  3.45it/s] 86%|████████▌ | 667/780 [04:20<00:32,  3.45it/s] 86%|████████▌ | 668/780 [04:20<00:32,  3.45it/s] 86%|████████▌ | 669/780 [04:21<00:32,  3.45it/s] 86%|████████▌ | 670/780 [04:21<00:32,  3.44it/s] 86%|████████▌ | 671/780 [04:21<00:31,  3.44it/s] 86%|████████▌ | 672/780 [04:22<00:31,  3.44it/s] 86%|████████▋ | 673/780 [04:22<00:31,  3.44it/s] 86%|████████▋ | 674/780 [04:22<00:30,  3.44it/s] 87%|████████▋ | 675/780 [04:23<00:30,  3.44it/s] 87%|████████▋ | 676/780 [04:23<00:30,  3.44it/s] 87%|████████▋ | 677/780 [04:23<00:29,  3.45it/s] 87%|████████▋ | 678/780 [04:23<00:29,  3.43it/s] 87%|████████▋ | 679/780 [04:24<00:29,  3.42it/s] 87%|████████▋ | 680/780 [04:24<00:29,  3.43it/s] 87%|████████▋ | 681/780 [04:24<00:28,  3.42it/s] 87%|████████▋ | 682/780 [04:25<00:28,  3.43it/s] 88%|████████▊ | 683/780 [04:25<00:28,  3.44it/s] 88%|████████▊ | 684/780 [04:25<00:27,  3.44it/s] 88%|████████▊ | 685/780 [04:25<00:27,  3.44it/s] 88%|████████▊ | 686/780 [04:26<00:27,  3.44it/s] 88%|████████▊ | 687/780 [04:26<00:26,  3.45it/s] 88%|████████▊ | 688/780 [04:26<00:26,  3.45it/s] 88%|████████▊ | 689/780 [04:27<00:26,  3.45it/s] 88%|████████▊ | 690/780 [04:27<00:26,  3.45it/s] 89%|████████▊ | 691/780 [04:27<00:25,  3.45it/s] 89%|████████▊ | 692/780 [04:27<00:25,  3.43it/s] 89%|████████▉ | 693/780 [04:28<00:25,  3.44it/s] 89%|████████▉ | 694/780 [04:28<00:25,  3.44it/s] 89%|████████▉ | 695/780 [04:28<00:24,  3.44it/s] 89%|████████▉ | 696/780 [04:29<00:24,  3.44it/s] 89%|████████▉ | 697/780 [04:29<00:24,  3.45it/s] 89%|████████▉ | 698/780 [04:29<00:23,  3.44it/s] 90%|████████▉ | 699/780 [04:29<00:23,  3.44it/s] 90%|████████▉ | 700/780 [04:30<00:23,  3.44it/s] 90%|████████▉ | 701/780 [04:30<00:22,  3.44it/s] 90%|█████████ | 702/780 [04:30<00:22,  3.45it/s] 90%|█████████ | 703/780 [04:31<00:22,  3.44it/s] 90%|█████████ | 704/780 [04:31<00:22,  3.44it/s] 90%|█████████ | 705/780 [04:31<00:21,  3.44it/s] 91%|█████████ | 706/780 [04:32<00:21,  3.44it/s] 91%|█████████ | 707/780 [04:32<00:21,  3.44it/s] 91%|█████████ | 708/780 [04:32<00:20,  3.44it/s] 91%|█████████ | 709/780 [04:32<00:20,  3.44it/s] 91%|█████████ | 710/780 [04:33<00:20,  3.44it/s] 91%|█████████ | 711/780 [04:33<00:20,  3.44it/s] 91%|█████████▏| 712/780 [04:33<00:19,  3.45it/s] 91%|█████████▏| 713/780 [04:34<00:19,  3.45it/s] 92%|█████████▏| 714/780 [04:34<00:19,  3.45it/s] 92%|█████████▏| 715/780 [04:34<00:18,  3.45it/s] 92%|█████████▏| 716/780 [04:34<00:18,  3.45it/s] 92%|█████████▏| 717/780 [04:35<00:18,  3.44it/s] 92%|█████████▏| 718/780 [04:35<00:18,  3.44it/s] 92%|█████████▏| 719/780 [04:35<00:17,  3.44it/s] 92%|█████████▏| 720/780 [04:36<00:17,  3.45it/s] 92%|█████████▏| 721/780 [04:36<00:17,  3.44it/s] 93%|█████████▎| 722/780 [04:36<00:16,  3.44it/s] 93%|█████████▎| 723/780 [04:36<00:16,  3.45it/s] 93%|█████████▎| 724/780 [04:37<00:16,  3.45it/s] 93%|█████████▎| 725/780 [04:37<00:15,  3.45it/s] 93%|█████████▎| 726/780 [04:37<00:15,  3.45it/s] 93%|█████████▎| 727/780 [04:38<00:15,  3.45it/s] 93%|█████████▎| 728/780 [04:38<00:15,  3.43it/s] 93%|█████████▎| 729/780 [04:38<00:14,  3.44it/s] 94%|█████████▎| 730/780 [04:38<00:14,  3.44it/s] 94%|█████████▎| 731/780 [04:39<00:14,  3.44it/s] 94%|█████████▍| 732/780 [04:39<00:13,  3.44it/s] 94%|█████████▍| 733/780 [04:39<00:13,  3.45it/s] 94%|█████████▍| 734/780 [04:40<00:13,  3.44it/s] 94%|█████████▍| 735/780 [04:40<00:13,  3.45it/s] 94%|█████████▍| 736/780 [04:40<00:12,  3.44it/s] 94%|█████████▍| 737/780 [04:41<00:12,  3.45it/s] 95%|█████████▍| 738/780 [04:41<00:12,  3.45it/s] 95%|█████████▍| 739/780 [04:41<00:11,  3.45it/s] 95%|█████████▍| 740/780 [04:41<00:11,  3.45it/s] 95%|█████████▌| 741/780 [04:42<00:11,  3.45it/s] 95%|█████████▌| 742/780 [04:42<00:11,  3.44it/s] 95%|█████████▌| 743/780 [04:42<00:10,  3.44it/s] 95%|█████████▌| 744/780 [04:43<00:10,  3.44it/s] 96%|█████████▌| 745/780 [04:43<00:10,  3.44it/s] 96%|█████████▌| 746/780 [04:43<00:09,  3.44it/s] 96%|█████████▌| 747/780 [04:43<00:09,  3.44it/s] 96%|█████████▌| 748/780 [04:44<00:09,  3.44it/s] 96%|█████████▌| 749/780 [04:44<00:09,  3.44it/s] 96%|█████████▌| 750/780 [04:44<00:08,  3.44it/s] 96%|█████████▋| 751/780 [04:45<00:08,  3.44it/s] 96%|█████████▋| 752/780 [04:45<00:08,  3.45it/s] 97%|█████████▋| 753/780 [04:45<00:07,  3.44it/s] 97%|█████████▋| 754/780 [04:45<00:07,  3.45it/s] 97%|█████████▋| 755/780 [04:46<00:07,  3.44it/s] 97%|█████████▋| 756/780 [04:46<00:06,  3.44it/s] 97%|█████████▋| 757/780 [04:46<00:06,  3.44it/s] 97%|█████████▋| 758/780 [04:47<00:06,  3.44it/s] 97%|█████████▋| 759/780 [04:47<00:06,  3.43it/s] 97%|█████████▋| 760/780 [04:47<00:05,  3.44it/s] 98%|█████████▊| 761/780 [04:47<00:05,  3.44it/s] 98%|█████████▊| 762/780 [04:48<00:05,  3.45it/s] 98%|█████████▊| 763/780 [04:48<00:04,  3.44it/s] 98%|█████████▊| 764/780 [04:48<00:04,  3.45it/s] 98%|█████████▊| 765/780 [04:49<00:04,  3.45it/s] 98%|█████████▊| 766/780 [04:49<00:04,  3.26it/s] 98%|█████████▊| 767/780 [04:49<00:03,  3.31it/s] 98%|█████████▊| 768/780 [04:50<00:03,  3.35it/s] 99%|█████████▊| 769/780 [04:50<00:03,  3.38it/s] 99%|█████████▊| 770/780 [04:50<00:02,  3.35it/s] 99%|█████████▉| 771/780 [04:50<00:02,  3.37it/s] 99%|█████████▉| 772/780 [04:51<00:02,  3.39it/s] 99%|█████████▉| 773/780 [04:51<00:02,  3.41it/s] 99%|█████████▉| 774/780 [04:51<00:01,  3.42it/s] 99%|█████████▉| 775/780 [04:52<00:01,  3.43it/s] 99%|█████████▉| 776/780 [04:52<00:01,  3.43it/s]100%|█████████▉| 777/780 [04:52<00:00,  3.43it/s]100%|█████████▉| 778/780 [04:53<00:00,  3.43it/s]100%|█████████▉| 779/780 [04:53<00:00,  3.43it/s]100%|██████████| 780/780 [04:53<00:00,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 01:07:33,502 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:07:33,502 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 01:07:33,502 >>   Batch size = 8
{'eval_loss': 1.0938220024108887, 'eval_runtime': 9.3971, 'eval_samples_per_second': 370.007, 'eval_steps_per_second': 46.291, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.20it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.29it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.65it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.81it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.41it/s][A
  8%|▊         | 33/435 [00:00<00:08, 47.15it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.73it/s][A
 10%|▉         | 43/435 [00:00<00:08, 46.49it/s][A
 11%|█         | 48/435 [00:01<00:08, 46.58it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 46.53it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 46.57it/s][A
 14%|█▍        | 63/435 [00:01<00:07, 46.57it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 46.46it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 46.44it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 46.44it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 46.31it/s][A
 20%|██        | 88/435 [00:01<00:07, 46.30it/s][A
 21%|██▏       | 93/435 [00:01<00:07, 46.33it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 46.40it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 46.53it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 46.43it/s][A
 26%|██▌       | 113/435 [00:02<00:06, 46.52it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 46.47it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 46.44it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 46.23it/s][A
 31%|███       | 133/435 [00:02<00:06, 46.29it/s][A
 32%|███▏      | 138/435 [00:02<00:06, 46.29it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 46.31it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 46.43it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 46.38it/s][A
 36%|███▋      | 158/435 [00:03<00:05, 46.33it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 46.46it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 46.35it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 46.30it/s][A
 41%|████      | 178/435 [00:03<00:05, 46.30it/s][A
 42%|████▏     | 183/435 [00:03<00:05, 46.29it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 46.32it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 46.46it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 46.41it/s][A
 47%|████▋     | 203/435 [00:04<00:04, 46.46it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 46.49it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 46.50it/s][A
 50%|█████     | 218/435 [00:04<00:04, 46.37it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 46.30it/s][A
 52%|█████▏    | 228/435 [00:04<00:04, 46.31it/s][A
 54%|█████▎    | 233/435 [00:04<00:04, 46.36it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 46.49it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 46.45it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 46.54it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 46.48it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 46.47it/s][A
 60%|██████    | 263/435 [00:05<00:03, 46.41it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 46.41it/s][A
 63%|██████▎   | 273/435 [00:05<00:03, 46.35it/s][A
 64%|██████▍   | 278/435 [00:05<00:03, 46.39it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 46.52it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 46.39it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 46.53it/s][A
 69%|██████▊   | 298/435 [00:06<00:02, 46.47it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 46.43it/s][A
 71%|███████   | 308/435 [00:06<00:02, 46.44it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 46.39it/s][A
 73%|███████▎  | 318/435 [00:06<00:02, 46.31it/s][A
 74%|███████▍  | 323/435 [00:06<00:02, 46.34it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 46.46it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 46.43it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 46.50it/s][A
 79%|███████▉  | 343/435 [00:07<00:01, 46.48it/s][A
 80%|████████  | 348/435 [00:07<00:01, 46.36it/s][A
 81%|████████  | 353/435 [00:07<00:01, 46.40it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 46.36it/s][A
 83%|████████▎ | 363/435 [00:07<00:01, 46.30it/s][A
 85%|████████▍ | 368/435 [00:07<00:01, 46.31it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 46.43it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 46.38it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 46.48it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 46.41it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 46.43it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 46.44it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 46.41it/s][A
 94%|█████████▍| 408/435 [00:08<00:00, 46.35it/s][A
 95%|█████████▍| 413/435 [00:08<00:00, 46.28it/s][A
 96%|█████████▌| 418/435 [00:08<00:00, 46.38it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 46.44it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 46.43it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 46.36it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 46.36it/s][A100%|██████████| 780/780 [05:02<00:00,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:07:42,897 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 01:07:42,908 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:07:45,337 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:07:45,354 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:07:45,363 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 01:07:50,303 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 01:07:50,306 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156 (score: 1.04639732837677).
                                                 100%|██████████| 780/780 [05:13<00:00,  3.44it/s]100%|██████████| 780/780 [05:13<00:00,  2.49it/s]
[INFO|trainer.py:1894] 2023-08-29 01:07:53,521 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 01:07:53,540 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:07:56,970 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:07:57,182 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:07:57,288 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:07:57,914 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:07:57,914 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:07:57,914 >>   train_loss               =     0.3988
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:07:57,914 >>   train_runtime            = 0:05:13.60
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:07:57,914 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:07:57,914 >>   train_samples_per_second =    159.438
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:07:57,914 >>   train_steps_per_second   =      2.487
{'eval_loss': 1.0997207164764404, 'eval_runtime': 9.3685, 'eval_samples_per_second': 371.136, 'eval_steps_per_second': 46.432, 'epoch': 5.0}
{'train_runtime': 313.602, 'train_samples_per_second': 159.438, 'train_steps_per_second': 2.487, 'train_loss': 0.39884118300217847, 'epoch': 5.0}
08/29/2023 01:07:58 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 01:07:58,248 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:07:58,248 >>   Num examples = 3477
[INFO|trainer.py:2145] 2023-08-29 01:07:58,248 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|▏         | 6/435 [00:00<00:07, 58.86it/s]  3%|▎         | 12/435 [00:00<00:08, 51.39it/s]  4%|▍         | 18/435 [00:00<00:08, 49.21it/s]  5%|▌         | 23/435 [00:00<00:08, 48.33it/s]  6%|▋         | 28/435 [00:00<00:08, 47.90it/s]  8%|▊         | 33/435 [00:00<00:08, 47.70it/s]  9%|▊         | 38/435 [00:00<00:08, 47.47it/s] 10%|▉         | 43/435 [00:00<00:08, 47.12it/s] 11%|█         | 48/435 [00:00<00:08, 46.94it/s] 12%|█▏        | 53/435 [00:01<00:08, 46.98it/s] 13%|█▎        | 58/435 [00:01<00:08, 47.00it/s] 14%|█▍        | 63/435 [00:01<00:07, 46.92it/s] 16%|█▌        | 68/435 [00:01<00:07, 46.90it/s] 17%|█▋        | 73/435 [00:01<00:07, 46.92it/s] 18%|█▊        | 78/435 [00:01<00:07, 46.90it/s] 19%|█▉        | 83/435 [00:01<00:07, 46.84it/s] 20%|██        | 88/435 [00:01<00:07, 46.86it/s] 21%|██▏       | 93/435 [00:01<00:07, 46.78it/s] 23%|██▎       | 98/435 [00:02<00:07, 46.87it/s] 24%|██▎       | 103/435 [00:02<00:07, 46.86it/s] 25%|██▍       | 108/435 [00:02<00:06, 46.85it/s] 26%|██▌       | 113/435 [00:02<00:06, 46.84it/s] 27%|██▋       | 118/435 [00:02<00:06, 46.90it/s] 28%|██▊       | 123/435 [00:02<00:06, 46.82it/s] 29%|██▉       | 128/435 [00:02<00:06, 46.89it/s] 31%|███       | 133/435 [00:02<00:06, 46.85it/s] 32%|███▏      | 138/435 [00:02<00:06, 46.73it/s] 33%|███▎      | 143/435 [00:03<00:06, 46.81it/s] 34%|███▍      | 148/435 [00:03<00:06, 46.83it/s] 35%|███▌      | 153/435 [00:03<00:06, 46.78it/s] 36%|███▋      | 158/435 [00:03<00:05, 46.89it/s] 37%|███▋      | 163/435 [00:03<00:05, 46.86it/s] 39%|███▊      | 168/435 [00:03<00:05, 46.83it/s] 40%|███▉      | 173/435 [00:03<00:05, 46.89it/s] 41%|████      | 178/435 [00:03<00:05, 46.90it/s] 42%|████▏     | 183/435 [00:03<00:05, 46.80it/s] 43%|████▎     | 188/435 [00:03<00:05, 46.84it/s] 44%|████▍     | 193/435 [00:04<00:05, 46.80it/s] 46%|████▌     | 198/435 [00:04<00:05, 46.80it/s] 47%|████▋     | 203/435 [00:04<00:04, 46.84it/s] 48%|████▊     | 208/435 [00:04<00:04, 46.89it/s] 49%|████▉     | 213/435 [00:04<00:04, 46.87it/s] 50%|█████     | 218/435 [00:04<00:04, 46.86it/s] 51%|█████▏    | 223/435 [00:04<00:04, 46.83it/s] 52%|█████▏    | 228/435 [00:04<00:04, 46.83it/s] 54%|█████▎    | 233/435 [00:04<00:04, 46.90it/s] 55%|█████▍    | 238/435 [00:05<00:04, 46.93it/s] 56%|█████▌    | 243/435 [00:05<00:04, 46.80it/s] 57%|█████▋    | 248/435 [00:05<00:03, 46.86it/s] 58%|█████▊    | 253/435 [00:05<00:03, 46.76it/s] 59%|█████▉    | 258/435 [00:05<00:03, 46.82it/s] 60%|██████    | 263/435 [00:05<00:03, 46.92it/s] 62%|██████▏   | 268/435 [00:05<00:03, 46.92it/s] 63%|██████▎   | 273/435 [00:05<00:03, 46.86it/s] 64%|██████▍   | 278/435 [00:05<00:03, 46.85it/s] 65%|██████▌   | 283/435 [00:06<00:03, 46.78it/s] 66%|██████▌   | 288/435 [00:06<00:03, 46.76it/s] 67%|██████▋   | 293/435 [00:06<00:03, 46.84it/s] 69%|██████▊   | 298/435 [00:06<00:02, 46.79it/s] 70%|██████▉   | 303/435 [00:06<00:02, 46.79it/s] 71%|███████   | 308/435 [00:06<00:02, 46.77it/s] 72%|███████▏  | 313/435 [00:06<00:02, 46.75it/s] 73%|███████▎  | 318/435 [00:06<00:02, 46.76it/s] 74%|███████▍  | 323/435 [00:06<00:02, 46.87it/s] 75%|███████▌  | 328/435 [00:06<00:02, 46.81it/s] 77%|███████▋  | 333/435 [00:07<00:02, 46.86it/s] 78%|███████▊  | 338/435 [00:07<00:02, 46.74it/s] 79%|███████▉  | 343/435 [00:07<00:01, 46.72it/s] 80%|████████  | 348/435 [00:07<00:01, 46.74it/s] 81%|████████  | 353/435 [00:07<00:01, 46.80it/s] 82%|████████▏ | 358/435 [00:07<00:01, 46.76it/s] 83%|████████▎ | 363/435 [00:07<00:01, 46.82it/s] 85%|████████▍ | 368/435 [00:07<00:01, 46.76it/s] 86%|████████▌ | 373/435 [00:07<00:01, 46.75it/s] 87%|████████▋ | 378/435 [00:08<00:01, 46.78it/s] 88%|████████▊ | 383/435 [00:08<00:01, 46.80it/s] 89%|████████▉ | 388/435 [00:08<00:01, 46.78it/s] 90%|█████████ | 393/435 [00:08<00:00, 46.84it/s] 91%|█████████▏| 398/435 [00:08<00:00, 46.71it/s] 93%|█████████▎| 403/435 [00:08<00:00, 46.77it/s] 94%|█████████▍| 408/435 [00:08<00:00, 46.80it/s] 95%|█████████▍| 413/435 [00:08<00:00, 46.79it/s] 96%|█████████▌| 418/435 [00:08<00:00, 46.76it/s] 97%|█████████▋| 423/435 [00:09<00:00, 46.83it/s] 98%|█████████▊| 428/435 [00:09<00:00, 46.64it/s]100%|█████████▉| 433/435 [00:09<00:00, 46.76it/s]100%|██████████| 435/435 [00:09<00:00, 46.94it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:08:07,540 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:08:07,540 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:08:07,540 >>   eval_loss               =     1.0464
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:08:07,540 >>   eval_runtime            = 0:00:09.29
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:08:07,540 >>   eval_samples            =       3477
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:08:07,540 >>   eval_samples_per_second =    374.226
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:08:07,540 >>   eval_steps_per_second   =     46.819
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:08:07,540 >>   perplexity              =     2.8474
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:12,987 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:12,992 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:12,992 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:12,992 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:12,992 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:08:13,283 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:08:13,284 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:08:13,964 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:08:15,000 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:08:15,001 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:16,815 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:16,828 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:16,828 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:16,828 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:08:16,828 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:08:17,572 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:08:17,573 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:08:18,255 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:08:18,395 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:08:18,395 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/dev.jsonl', 'labels': ['country of citizenship', 'genre', 'head of government', 'military branch', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12650
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12750, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:05,  1.49it/s]Extractor Predicting: 10it [00:06,  1.39it/s]Extractor Predicting: 11it [00:07,  1.41it/s]Extractor Predicting: 12it [00:08,  1.41it/s]Extractor Predicting: 13it [00:08,  1.45it/s]Extractor Predicting: 14it [00:09,  1.43it/s]Extractor Predicting: 15it [00:10,  1.45it/s]Extractor Predicting: 16it [00:10,  1.45it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:14,  1.49it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:15,  1.51it/s]Extractor Predicting: 24it [00:16,  1.52it/s]Extractor Predicting: 25it [00:16,  1.48it/s]Extractor Predicting: 26it [00:17,  1.46it/s]Extractor Predicting: 27it [00:18,  1.48it/s]Extractor Predicting: 28it [00:19,  1.45it/s]Extractor Predicting: 29it [00:19,  1.44it/s]Extractor Predicting: 30it [00:20,  1.44it/s]Extractor Predicting: 31it [00:21,  1.44it/s]Extractor Predicting: 32it [00:21,  1.45it/s]Extractor Predicting: 33it [00:22,  1.42it/s]Extractor Predicting: 34it [00:23,  1.44it/s]Extractor Predicting: 35it [00:23,  1.48it/s]Extractor Predicting: 36it [00:24,  1.50it/s]Extractor Predicting: 37it [00:25,  1.49it/s]Extractor Predicting: 38it [00:25,  1.44it/s]Extractor Predicting: 39it [00:26,  1.46it/s]Extractor Predicting: 40it [00:27,  1.46it/s]Extractor Predicting: 41it [00:27,  1.49it/s]Extractor Predicting: 42it [00:28,  1.49it/s]Extractor Predicting: 43it [00:29,  1.47it/s]Extractor Predicting: 44it [00:29,  1.47it/s]Extractor Predicting: 45it [00:30,  1.45it/s]Extractor Predicting: 46it [00:31,  1.47it/s]Extractor Predicting: 47it [00:31,  1.49it/s]Extractor Predicting: 48it [00:32,  1.48it/s]Extractor Predicting: 49it [00:33,  1.51it/s]Extractor Predicting: 50it [00:33,  1.54it/s]Extractor Predicting: 51it [00:34,  1.53it/s]Extractor Predicting: 52it [00:35,  1.48it/s]Extractor Predicting: 53it [00:35,  1.47it/s]Extractor Predicting: 54it [00:36,  1.49it/s]Extractor Predicting: 55it [00:37,  1.47it/s]Extractor Predicting: 56it [00:38,  1.45it/s]Extractor Predicting: 57it [00:38,  1.44it/s]Extractor Predicting: 58it [00:39,  1.47it/s]Extractor Predicting: 59it [00:40,  1.51it/s]Extractor Predicting: 60it [00:40,  1.52it/s]Extractor Predicting: 61it [00:41,  1.52it/s]Extractor Predicting: 62it [00:41,  1.53it/s]Extractor Predicting: 63it [00:42,  1.53it/s]Extractor Predicting: 64it [00:43,  1.53it/s]Extractor Predicting: 65it [00:43,  1.53it/s]Extractor Predicting: 66it [00:44,  1.51it/s]Extractor Predicting: 67it [00:45,  1.52it/s]Extractor Predicting: 68it [00:45,  1.52it/s]Extractor Predicting: 69it [00:46,  1.53it/s]Extractor Predicting: 70it [00:47,  1.52it/s]Extractor Predicting: 71it [00:47,  1.51it/s]Extractor Predicting: 72it [00:48,  1.53it/s]Extractor Predicting: 73it [00:49,  1.52it/s]Extractor Predicting: 74it [00:49,  1.54it/s]Extractor Predicting: 75it [00:50,  1.56it/s]Extractor Predicting: 76it [00:51,  1.54it/s]Extractor Predicting: 77it [00:51,  1.50it/s]Extractor Predicting: 78it [00:52,  1.49it/s]Extractor Predicting: 79it [00:53,  1.48it/s]Extractor Predicting: 80it [00:53,  1.48it/s]Extractor Predicting: 81it [00:54,  1.46it/s]Extractor Predicting: 82it [00:55,  1.49it/s]Extractor Predicting: 83it [00:55,  1.49it/s]Extractor Predicting: 84it [00:56,  1.50it/s]Extractor Predicting: 85it [00:57,  1.50it/s]Extractor Predicting: 86it [00:57,  1.50it/s]Extractor Predicting: 87it [00:58,  1.49it/s]Extractor Predicting: 88it [00:59,  1.49it/s]Extractor Predicting: 89it [00:59,  1.51it/s]Extractor Predicting: 90it [01:00,  1.51it/s]Extractor Predicting: 91it [01:01,  1.54it/s]Extractor Predicting: 92it [01:01,  1.57it/s]Extractor Predicting: 93it [01:02,  1.58it/s]Extractor Predicting: 94it [01:03,  1.55it/s]Extractor Predicting: 95it [01:03,  1.58it/s]Extractor Predicting: 96it [01:04,  1.57it/s]Extractor Predicting: 97it [01:04,  1.56it/s]Extractor Predicting: 98it [01:05,  1.55it/s]Extractor Predicting: 99it [01:06,  1.52it/s]Extractor Predicting: 100it [01:07,  1.49it/s]Extractor Predicting: 101it [01:07,  1.37it/s]Extractor Predicting: 102it [01:08,  1.45it/s]Extractor Predicting: 103it [01:09,  1.50it/s]Extractor Predicting: 104it [01:09,  1.50it/s]Extractor Predicting: 105it [01:10,  1.50it/s]Extractor Predicting: 106it [01:11,  1.50it/s]Extractor Predicting: 107it [01:11,  1.52it/s]Extractor Predicting: 108it [01:12,  1.51it/s]Extractor Predicting: 109it [01:13,  1.51it/s]Extractor Predicting: 110it [01:13,  1.52it/s]Extractor Predicting: 111it [01:14,  1.52it/s]Extractor Predicting: 112it [01:15,  1.52it/s]Extractor Predicting: 113it [01:15,  1.56it/s]Extractor Predicting: 114it [01:16,  1.59it/s]Extractor Predicting: 115it [01:16,  1.57it/s]Extractor Predicting: 116it [01:17,  1.56it/s]Extractor Predicting: 117it [01:18,  1.56it/s]Extractor Predicting: 118it [01:18,  1.54it/s]Extractor Predicting: 119it [01:19,  1.55it/s]Extractor Predicting: 120it [01:20,  1.51it/s]Extractor Predicting: 121it [01:20,  1.49it/s]Extractor Predicting: 122it [01:21,  1.49it/s]Extractor Predicting: 123it [01:22,  1.49it/s]Extractor Predicting: 124it [01:22,  1.48it/s]Extractor Predicting: 125it [01:23,  1.49it/s]Extractor Predicting: 126it [01:24,  1.50it/s]Extractor Predicting: 127it [01:24,  1.49it/s]Extractor Predicting: 128it [01:25,  1.50it/s]Extractor Predicting: 129it [01:26,  1.50it/s]Extractor Predicting: 130it [01:26,  1.48it/s]Extractor Predicting: 131it [01:27,  1.49it/s]Extractor Predicting: 132it [01:28,  1.47it/s]Extractor Predicting: 133it [01:28,  1.49it/s]Extractor Predicting: 134it [01:29,  1.51it/s]Extractor Predicting: 135it [01:30,  1.50it/s]Extractor Predicting: 136it [01:30,  1.48it/s]Extractor Predicting: 137it [01:31,  1.51it/s]Extractor Predicting: 138it [01:32,  1.50it/s]Extractor Predicting: 139it [01:32,  1.48it/s]Extractor Predicting: 140it [01:33,  1.49it/s]Extractor Predicting: 141it [01:34,  1.51it/s]Extractor Predicting: 142it [01:34,  1.50it/s]Extractor Predicting: 143it [01:35,  1.53it/s]Extractor Predicting: 144it [01:36,  1.55it/s]Extractor Predicting: 144it [01:36,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:10:03,526 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:10:03,530 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:10:03,530 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:10:03,530 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:10:03,530 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:10:03,839 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:10:03,840 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:10:04,528 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:10:05,573 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:10:05,573 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:10:08,137 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:10:08,141 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:10:08,141 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:10:08,141 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:10:08,141 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:10:08,773 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:10:08,774 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:10:09,349 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:10:09,497 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:10:09,497 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.35657225853304286,
  "recall": 0.14121368996261144,
  "score": 0.20230737536052742,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25608
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25708, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.56it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:11,  1.52it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.56it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:18,  1.54it/s]Extractor Predicting: 30it [00:19,  1.56it/s]Extractor Predicting: 31it [00:20,  1.55it/s]Extractor Predicting: 32it [00:20,  1.61it/s]Extractor Predicting: 33it [00:21,  1.58it/s]Extractor Predicting: 34it [00:22,  1.54it/s]Extractor Predicting: 35it [00:22,  1.54it/s]Extractor Predicting: 36it [00:23,  1.55it/s]Extractor Predicting: 37it [00:23,  1.58it/s]Extractor Predicting: 38it [00:24,  1.59it/s]Extractor Predicting: 39it [00:25,  1.59it/s]Extractor Predicting: 40it [00:25,  1.57it/s]Extractor Predicting: 41it [00:26,  1.57it/s]Extractor Predicting: 42it [00:27,  1.60it/s]Extractor Predicting: 43it [00:27,  1.55it/s]Extractor Predicting: 44it [00:28,  1.55it/s]Extractor Predicting: 45it [00:29,  1.53it/s]Extractor Predicting: 46it [00:29,  1.51it/s]Extractor Predicting: 47it [00:30,  1.55it/s]Extractor Predicting: 48it [00:31,  1.53it/s]Extractor Predicting: 49it [00:31,  1.55it/s]Extractor Predicting: 50it [00:32,  1.55it/s]Extractor Predicting: 51it [00:33,  1.54it/s]Extractor Predicting: 52it [00:33,  1.52it/s]Extractor Predicting: 53it [00:34,  1.54it/s]Extractor Predicting: 54it [00:34,  1.51it/s]Extractor Predicting: 55it [00:35,  1.56it/s]Extractor Predicting: 56it [00:36,  1.54it/s]Extractor Predicting: 57it [00:36,  1.52it/s]Extractor Predicting: 58it [00:37,  1.50it/s]Extractor Predicting: 59it [00:38,  1.48it/s]Extractor Predicting: 60it [00:38,  1.50it/s]Extractor Predicting: 61it [00:39,  1.49it/s]Extractor Predicting: 62it [00:40,  1.50it/s]Extractor Predicting: 63it [00:40,  1.51it/s]Extractor Predicting: 64it [00:41,  1.50it/s]Extractor Predicting: 65it [00:42,  1.55it/s]Extractor Predicting: 66it [00:42,  1.52it/s]Extractor Predicting: 67it [00:43,  1.50it/s]Extractor Predicting: 68it [00:44,  1.49it/s]Extractor Predicting: 69it [00:44,  1.50it/s]Extractor Predicting: 70it [00:45,  1.47it/s]Extractor Predicting: 71it [00:46,  1.48it/s]Extractor Predicting: 72it [00:46,  1.48it/s]Extractor Predicting: 73it [00:47,  1.46it/s]Extractor Predicting: 74it [00:48,  1.48it/s]Extractor Predicting: 75it [00:49,  1.49it/s]Extractor Predicting: 76it [00:49,  1.50it/s]Extractor Predicting: 77it [00:50,  1.50it/s]Extractor Predicting: 78it [00:50,  1.52it/s]Extractor Predicting: 79it [00:51,  1.52it/s]Extractor Predicting: 80it [00:52,  1.58it/s]Extractor Predicting: 81it [00:52,  1.56it/s]Extractor Predicting: 82it [00:53,  1.55it/s]Extractor Predicting: 83it [00:54,  1.55it/s]Extractor Predicting: 84it [00:54,  1.54it/s]Extractor Predicting: 85it [00:55,  1.52it/s]Extractor Predicting: 86it [00:56,  1.47it/s]Extractor Predicting: 87it [00:57,  1.29it/s]Extractor Predicting: 88it [00:57,  1.36it/s]Extractor Predicting: 89it [00:58,  1.37it/s]Extractor Predicting: 90it [00:59,  1.37it/s]Extractor Predicting: 91it [01:00,  1.38it/s]Extractor Predicting: 92it [01:00,  1.41it/s]Extractor Predicting: 93it [01:01,  1.43it/s]Extractor Predicting: 94it [01:02,  1.45it/s]Extractor Predicting: 95it [01:02,  1.45it/s]Extractor Predicting: 96it [01:03,  1.45it/s]Extractor Predicting: 97it [01:04,  1.43it/s]Extractor Predicting: 98it [01:04,  1.41it/s]Extractor Predicting: 99it [01:05,  1.42it/s]Extractor Predicting: 100it [01:06,  1.44it/s]Extractor Predicting: 101it [01:06,  1.45it/s]Extractor Predicting: 102it [01:07,  1.46it/s]Extractor Predicting: 103it [01:08,  1.44it/s]Extractor Predicting: 104it [01:09,  1.43it/s]Extractor Predicting: 105it [01:09,  1.46it/s]Extractor Predicting: 106it [01:10,  1.45it/s]Extractor Predicting: 107it [01:11,  1.45it/s]Extractor Predicting: 108it [01:11,  1.47it/s]Extractor Predicting: 109it [01:12,  1.46it/s]Extractor Predicting: 110it [01:13,  1.44it/s]Extractor Predicting: 111it [01:13,  1.46it/s]Extractor Predicting: 112it [01:14,  1.45it/s]Extractor Predicting: 113it [01:15,  1.45it/s]Extractor Predicting: 114it [01:15,  1.48it/s]Extractor Predicting: 115it [01:16,  1.47it/s]Extractor Predicting: 116it [01:17,  1.43it/s]Extractor Predicting: 117it [01:17,  1.43it/s]Extractor Predicting: 118it [01:18,  1.44it/s]Extractor Predicting: 119it [01:19,  1.43it/s]Extractor Predicting: 120it [01:20,  1.43it/s]Extractor Predicting: 121it [01:20,  1.45it/s]Extractor Predicting: 122it [01:21,  1.48it/s]Extractor Predicting: 123it [01:22,  1.48it/s]Extractor Predicting: 124it [01:22,  1.48it/s]Extractor Predicting: 125it [01:23,  1.47it/s]Extractor Predicting: 126it [01:24,  1.53it/s]Extractor Predicting: 127it [01:24,  1.53it/s]Extractor Predicting: 128it [01:25,  1.50it/s]Extractor Predicting: 129it [01:25,  1.54it/s]Extractor Predicting: 130it [01:26,  1.53it/s]Extractor Predicting: 131it [01:27,  1.51it/s]Extractor Predicting: 132it [01:27,  1.52it/s]Extractor Predicting: 133it [01:28,  1.56it/s]Extractor Predicting: 134it [01:29,  1.58it/s]Extractor Predicting: 135it [01:29,  1.52it/s]Extractor Predicting: 136it [01:30,  1.53it/s]Extractor Predicting: 137it [01:31,  1.55it/s]Extractor Predicting: 138it [01:31,  1.59it/s]Extractor Predicting: 139it [01:32,  1.57it/s]Extractor Predicting: 140it [01:33,  1.56it/s]Extractor Predicting: 141it [01:33,  1.54it/s]Extractor Predicting: 142it [01:34,  1.55it/s]Extractor Predicting: 143it [01:35,  1.52it/s]Extractor Predicting: 144it [01:35,  1.51it/s]Extractor Predicting: 145it [01:36,  1.50it/s]Extractor Predicting: 146it [01:37,  1.52it/s]Extractor Predicting: 147it [01:37,  1.57it/s]Extractor Predicting: 148it [01:38,  1.58it/s]Extractor Predicting: 149it [01:38,  1.58it/s]Extractor Predicting: 150it [01:39,  1.62it/s]Extractor Predicting: 151it [01:40,  1.62it/s]Extractor Predicting: 152it [01:40,  1.62it/s]Extractor Predicting: 153it [01:41,  1.62it/s]Extractor Predicting: 154it [01:41,  1.64it/s]Extractor Predicting: 155it [01:42,  1.62it/s]Extractor Predicting: 156it [01:43,  1.64it/s]Extractor Predicting: 157it [01:43,  1.66it/s]Extractor Predicting: 158it [01:44,  1.65it/s]Extractor Predicting: 159it [01:44,  1.71it/s]Extractor Predicting: 160it [01:45,  1.75it/s]Extractor Predicting: 161it [01:46,  1.68it/s]Extractor Predicting: 162it [01:46,  1.64it/s]Extractor Predicting: 163it [01:47,  1.63it/s]Extractor Predicting: 164it [01:47,  1.63it/s]Extractor Predicting: 165it [01:48,  1.66it/s]Extractor Predicting: 166it [01:49,  1.67it/s]Extractor Predicting: 167it [01:49,  1.66it/s]Extractor Predicting: 168it [01:50,  1.63it/s]Extractor Predicting: 169it [01:50,  1.65it/s]Extractor Predicting: 170it [01:51,  1.64it/s]Extractor Predicting: 171it [01:52,  1.67it/s]Extractor Predicting: 172it [01:52,  1.67it/s]Extractor Predicting: 173it [01:53,  1.68it/s]Extractor Predicting: 174it [01:54,  1.59it/s]Extractor Predicting: 175it [01:54,  1.56it/s]Extractor Predicting: 176it [01:55,  1.54it/s]Extractor Predicting: 177it [01:56,  1.52it/s]Extractor Predicting: 178it [01:56,  1.49it/s]Extractor Predicting: 179it [01:57,  1.48it/s]Extractor Predicting: 180it [01:58,  1.49it/s]Extractor Predicting: 181it [01:58,  1.49it/s]Extractor Predicting: 182it [01:59,  1.50it/s]Extractor Predicting: 183it [02:00,  1.52it/s]Extractor Predicting: 184it [02:00,  1.51it/s]Extractor Predicting: 185it [02:01,  1.51it/s]Extractor Predicting: 186it [02:02,  1.53it/s]Extractor Predicting: 187it [02:02,  1.52it/s]Extractor Predicting: 188it [02:03,  1.49it/s]Extractor Predicting: 189it [02:04,  1.51it/s]Extractor Predicting: 190it [02:04,  1.49it/s]Extractor Predicting: 191it [02:05,  1.47it/s]Extractor Predicting: 192it [02:06,  1.47it/s]Extractor Predicting: 193it [02:06,  1.45it/s]Extractor Predicting: 194it [02:07,  1.45it/s]Extractor Predicting: 195it [02:08,  1.46it/s]Extractor Predicting: 196it [02:08,  1.50it/s]Extractor Predicting: 197it [02:09,  1.51it/s]Extractor Predicting: 198it [02:10,  1.50it/s]Extractor Predicting: 199it [02:10,  1.51it/s]Extractor Predicting: 200it [02:11,  1.48it/s]Extractor Predicting: 201it [02:12,  1.46it/s]Extractor Predicting: 202it [02:12,  1.44it/s]Extractor Predicting: 203it [02:13,  1.42it/s]Extractor Predicting: 204it [02:14,  1.41it/s]Extractor Predicting: 205it [02:15,  1.26it/s]Extractor Predicting: 206it [02:16,  1.29it/s]Extractor Predicting: 207it [02:16,  1.34it/s]Extractor Predicting: 208it [02:17,  1.34it/s]Extractor Predicting: 209it [02:18,  1.33it/s]Extractor Predicting: 210it [02:19,  1.34it/s]Extractor Predicting: 211it [02:19,  1.34it/s]Extractor Predicting: 212it [02:20,  1.38it/s]Extractor Predicting: 213it [02:21,  1.38it/s]Extractor Predicting: 214it [02:21,  1.40it/s]Extractor Predicting: 215it [02:22,  1.38it/s]Extractor Predicting: 216it [02:23,  1.38it/s]Extractor Predicting: 217it [02:24,  1.40it/s]Extractor Predicting: 218it [02:24,  1.39it/s]Extractor Predicting: 219it [02:25,  1.38it/s]Extractor Predicting: 220it [02:26,  1.34it/s]Extractor Predicting: 221it [02:27,  1.34it/s]Extractor Predicting: 222it [02:27,  1.34it/s]Extractor Predicting: 223it [02:28,  1.38it/s]Extractor Predicting: 224it [02:29,  1.39it/s]Extractor Predicting: 225it [02:29,  1.36it/s]Extractor Predicting: 226it [02:30,  1.35it/s]Extractor Predicting: 227it [02:31,  1.37it/s]Extractor Predicting: 228it [02:32,  1.39it/s]Extractor Predicting: 229it [02:32,  1.41it/s]Extractor Predicting: 230it [02:33,  1.43it/s]Extractor Predicting: 231it [02:34,  1.47it/s]Extractor Predicting: 232it [02:34,  1.50it/s]Extractor Predicting: 233it [02:35,  1.52it/s]Extractor Predicting: 234it [02:36,  1.47it/s]Extractor Predicting: 235it [02:36,  1.49it/s]Extractor Predicting: 236it [02:37,  1.49it/s]Extractor Predicting: 237it [02:38,  1.49it/s]Extractor Predicting: 238it [02:38,  1.50it/s]Extractor Predicting: 239it [02:39,  1.48it/s]Extractor Predicting: 240it [02:40,  1.51it/s]Extractor Predicting: 241it [02:40,  1.53it/s]Extractor Predicting: 242it [02:41,  1.55it/s]Extractor Predicting: 243it [02:41,  1.55it/s]Extractor Predicting: 244it [02:42,  1.60it/s]Extractor Predicting: 245it [02:43,  1.56it/s]Extractor Predicting: 246it [02:43,  1.53it/s]Extractor Predicting: 247it [02:44,  1.51it/s]Extractor Predicting: 248it [02:45,  1.49it/s]Extractor Predicting: 249it [02:45,  1.52it/s]Extractor Predicting: 250it [02:46,  1.52it/s]Extractor Predicting: 251it [02:47,  1.54it/s]Extractor Predicting: 252it [02:47,  1.57it/s]Extractor Predicting: 253it [02:48,  1.55it/s]Extractor Predicting: 254it [02:49,  1.51it/s]Extractor Predicting: 255it [02:49,  1.52it/s]Extractor Predicting: 256it [02:50,  1.51it/s]Extractor Predicting: 257it [02:51,  1.47it/s]Extractor Predicting: 258it [02:51,  1.47it/s]Extractor Predicting: 259it [02:52,  1.48it/s]Extractor Predicting: 260it [02:53,  1.47it/s]Extractor Predicting: 261it [02:53,  1.47it/s]Extractor Predicting: 262it [02:54,  1.48it/s]Extractor Predicting: 263it [02:55,  1.50it/s]Extractor Predicting: 264it [02:55,  1.48it/s]Extractor Predicting: 265it [02:56,  1.48it/s]Extractor Predicting: 266it [02:57,  1.47it/s]Extractor Predicting: 267it [02:57,  1.46it/s]Extractor Predicting: 268it [02:58,  1.44it/s]Extractor Predicting: 269it [02:59,  1.45it/s]Extractor Predicting: 270it [03:00,  1.46it/s]Extractor Predicting: 271it [03:00,  1.46it/s]Extractor Predicting: 272it [03:01,  1.49it/s]Extractor Predicting: 273it [03:02,  1.45it/s]Extractor Predicting: 274it [03:02,  1.43it/s]Extractor Predicting: 275it [03:03,  1.44it/s]Extractor Predicting: 276it [03:04,  1.44it/s]Extractor Predicting: 277it [03:04,  1.45it/s]Extractor Predicting: 278it [03:05,  1.46it/s]Extractor Predicting: 279it [03:06,  1.45it/s]Extractor Predicting: 280it [03:06,  1.46it/s]Extractor Predicting: 281it [03:07,  1.48it/s]Extractor Predicting: 282it [03:08,  1.47it/s]Extractor Predicting: 283it [03:09,  1.43it/s]Extractor Predicting: 284it [03:09,  1.49it/s]Extractor Predicting: 285it [03:10,  1.48it/s]Extractor Predicting: 286it [03:10,  1.50it/s]Extractor Predicting: 287it [03:11,  1.50it/s]Extractor Predicting: 288it [03:12,  1.49it/s]Extractor Predicting: 289it [03:13,  1.48it/s]Extractor Predicting: 290it [03:13,  1.47it/s]Extractor Predicting: 291it [03:14,  1.42it/s]Extractor Predicting: 292it [03:15,  1.42it/s]Extractor Predicting: 293it [03:15,  1.44it/s]Extractor Predicting: 294it [03:16,  1.41it/s]Extractor Predicting: 295it [03:17,  1.44it/s]Extractor Predicting: 296it [03:17,  1.43it/s]Extractor Predicting: 297it [03:18,  1.45it/s]Extractor Predicting: 298it [03:19,  1.44it/s]Extractor Predicting: 299it [03:20,  1.42it/s]Extractor Predicting: 300it [03:20,  1.45it/s]Extractor Predicting: 301it [03:21,  1.44it/s]Extractor Predicting: 302it [03:22,  1.47it/s]Extractor Predicting: 303it [03:22,  1.43it/s]Extractor Predicting: 304it [03:23,  1.42it/s]Extractor Predicting: 305it [03:24,  1.43it/s]Extractor Predicting: 306it [03:24,  1.46it/s]Extractor Predicting: 307it [03:25,  1.28it/s]Extractor Predicting: 308it [03:26,  1.34it/s]Extractor Predicting: 309it [03:27,  1.37it/s]Extractor Predicting: 310it [03:27,  1.41it/s]Extractor Predicting: 311it [03:28,  1.41it/s]Extractor Predicting: 312it [03:29,  1.46it/s]Extractor Predicting: 313it [03:29,  1.51it/s]Extractor Predicting: 314it [03:30,  1.53it/s]Extractor Predicting: 315it [03:31,  1.55it/s]Extractor Predicting: 316it [03:31,  1.52it/s]Extractor Predicting: 317it [03:32,  1.49it/s]Extractor Predicting: 318it [03:33,  1.48it/s]Extractor Predicting: 319it [03:33,  1.48it/s]Extractor Predicting: 320it [03:34,  1.47it/s]Extractor Predicting: 321it [03:35,  1.46it/s]Extractor Predicting: 322it [03:35,  1.47it/s]Extractor Predicting: 323it [03:36,  1.43it/s]Extractor Predicting: 324it [03:37,  1.44it/s]Extractor Predicting: 325it [03:38,  1.44it/s]Extractor Predicting: 326it [03:38,  1.45it/s]Extractor Predicting: 327it [03:39,  1.48it/s]Extractor Predicting: 328it [03:40,  1.47it/s]Extractor Predicting: 329it [03:40,  1.48it/s]Extractor Predicting: 330it [03:41,  1.48it/s]Extractor Predicting: 331it [03:42,  1.46it/s]Extractor Predicting: 332it [03:42,  1.46it/s]Extractor Predicting: 333it [03:43,  1.48it/s]Extractor Predicting: 334it [03:44,  1.48it/s]Extractor Predicting: 335it [03:44,  1.50it/s]Extractor Predicting: 336it [03:45,  1.47it/s]Extractor Predicting: 337it [03:46,  1.47it/s]Extractor Predicting: 338it [03:46,  1.49it/s]Extractor Predicting: 339it [03:47,  1.52it/s]Extractor Predicting: 340it [03:48,  1.53it/s]Extractor Predicting: 341it [03:48,  1.52it/s]Extractor Predicting: 342it [03:49,  1.50it/s]Extractor Predicting: 343it [03:50,  1.49it/s]Extractor Predicting: 344it [03:50,  1.52it/s]Extractor Predicting: 345it [03:51,  1.48it/s]Extractor Predicting: 346it [03:52,  1.48it/s]Extractor Predicting: 347it [03:52,  1.48it/s]Extractor Predicting: 348it [03:53,  1.48it/s]Extractor Predicting: 349it [03:54,  1.51it/s]Extractor Predicting: 350it [03:54,  1.52it/s]Extractor Predicting: 351it [03:55,  1.53it/s]Extractor Predicting: 352it [03:56,  1.50it/s]Extractor Predicting: 353it [03:56,  1.51it/s]Extractor Predicting: 354it [03:57,  1.54it/s]Extractor Predicting: 355it [03:57,  1.56it/s]Extractor Predicting: 356it [03:58,  1.53it/s]Extractor Predicting: 357it [03:59,  1.53it/s]Extractor Predicting: 358it [03:59,  1.54it/s]Extractor Predicting: 359it [04:00,  1.52it/s]Extractor Predicting: 360it [04:01,  1.52it/s]Extractor Predicting: 361it [04:01,  1.52it/s]Extractor Predicting: 362it [04:02,  1.53it/s]Extractor Predicting: 363it [04:03,  1.54it/s]Extractor Predicting: 364it [04:03,  1.56it/s]Extractor Predicting: 365it [04:04,  1.57it/s]Extractor Predicting: 366it [04:05,  1.54it/s]Extractor Predicting: 367it [04:05,  1.50it/s]Extractor Predicting: 368it [04:06,  1.49it/s]Extractor Predicting: 369it [04:07,  1.50it/s]Extractor Predicting: 370it [04:07,  1.54it/s]Extractor Predicting: 371it [04:08,  1.55it/s]Extractor Predicting: 372it [04:09,  1.55it/s]Extractor Predicting: 373it [04:09,  1.51it/s]Extractor Predicting: 374it [04:10,  1.53it/s]Extractor Predicting: 375it [04:11,  1.53it/s]Extractor Predicting: 376it [04:11,  1.54it/s]Extractor Predicting: 377it [04:12,  1.55it/s]Extractor Predicting: 378it [04:12,  1.58it/s]Extractor Predicting: 379it [04:13,  1.53it/s]Extractor Predicting: 380it [04:14,  1.53it/s]Extractor Predicting: 381it [04:14,  1.54it/s]Extractor Predicting: 382it [04:15,  1.54it/s]Extractor Predicting: 383it [04:16,  1.57it/s]Extractor Predicting: 384it [04:16,  1.59it/s]Extractor Predicting: 385it [04:17,  1.55it/s]Extractor Predicting: 386it [04:18,  1.55it/s]Extractor Predicting: 387it [04:18,  1.55it/s]Extractor Predicting: 388it [04:19,  1.53it/s]Extractor Predicting: 389it [04:20,  1.54it/s]Extractor Predicting: 390it [04:20,  1.53it/s]Extractor Predicting: 391it [04:21,  1.52it/s]Extractor Predicting: 392it [04:22,  1.52it/s]Extractor Predicting: 393it [04:22,  1.55it/s]Extractor Predicting: 394it [04:23,  1.49it/s]Extractor Predicting: 395it [04:24,  1.45it/s]Extractor Predicting: 396it [04:24,  1.44it/s]Extractor Predicting: 397it [04:25,  1.43it/s]Extractor Predicting: 398it [04:26,  1.44it/s]Extractor Predicting: 399it [04:27,  1.43it/s]Extractor Predicting: 400it [04:27,  1.47it/s]Extractor Predicting: 401it [04:28,  1.44it/s]Extractor Predicting: 402it [04:29,  1.43it/s]Extractor Predicting: 403it [04:29,  1.43it/s]Extractor Predicting: 404it [04:30,  1.39it/s]Extractor Predicting: 405it [04:31,  1.24it/s]Extractor Predicting: 406it [04:32,  1.29it/s]Extractor Predicting: 407it [04:32,  1.30it/s]Extractor Predicting: 408it [04:33,  1.35it/s]Extractor Predicting: 409it [04:34,  1.38it/s]Extractor Predicting: 410it [04:35,  1.36it/s]Extractor Predicting: 411it [04:35,  1.38it/s]Extractor Predicting: 412it [04:36,  1.40it/s]Extractor Predicting: 413it [04:37,  1.42it/s]Extractor Predicting: 414it [04:37,  1.45it/s]Extractor Predicting: 415it [04:38,  1.48it/s]Extractor Predicting: 416it [04:39,  1.48it/s]Extractor Predicting: 417it [04:39,  1.48it/s]Extractor Predicting: 418it [04:40,  1.47it/s]Extractor Predicting: 419it [04:41,  1.42it/s]Extractor Predicting: 420it [04:41,  1.42it/s]Extractor Predicting: 421it [04:42,  1.50it/s]Extractor Predicting: 421it [04:42,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:15:01,189 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:15:01,193 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:15:01,193 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:15:01,194 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:15:01,194 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:15:01,900 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:15:01,901 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:15:02,579 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:15:03,629 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:15:03,629 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:15:04,925 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:15:04,927 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:15:04,927 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:15:04,928 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:15:04,928 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:15:05,254 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:15:05,256 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:15:05,517 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:15:05,671 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:15:05,671 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.253021978021978,
  "recall": 0.0912332838038633,
  "score": 0.1341099381143065,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1764
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1864, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.35it/s]Extractor Predicting: 2it [00:01,  1.38it/s]Extractor Predicting: 3it [00:02,  1.38it/s]Extractor Predicting: 4it [00:02,  1.37it/s]Extractor Predicting: 5it [00:03,  1.41it/s]Extractor Predicting: 6it [00:04,  1.40it/s]Extractor Predicting: 7it [00:05,  1.39it/s]Extractor Predicting: 8it [00:05,  1.41it/s]Extractor Predicting: 9it [00:05,  1.82it/s]Extractor Predicting: 9it [00:05,  1.52it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.34146341463414637,
  "recall": 0.0345679012345679,
  "score": 0.06278026905829596,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_15_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/', 'labels': ['characters', 'contains administrative territorial entity', 'crosses', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'operating system', 'participant', 'participating team', 'platform', 'position played on team / speciality', 'publisher', 'spouse'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/results_single_is_eval_True_limit5000.json'
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_15_seed_1/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_1/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_15_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_15_seed_1', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_1/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'given name', 'participant in', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14118
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14218, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Predicting: 1it [00:14, 14.97s/it]Extractor Predicting: 2it [00:16,  6.85s/it]Extractor Predicting: 3it [00:16,  4.01s/it]Extractor Predicting: 4it [00:17,  2.90s/it]Extractor Predicting: 5it [00:18,  2.08s/it]Extractor Predicting: 6it [00:19,  1.57s/it]Extractor Predicting: 7it [00:19,  1.27s/it]Extractor Predicting: 8it [00:20,  1.08s/it]Extractor Predicting: 9it [00:21,  1.07it/s]Extractor Predicting: 10it [00:21,  1.18it/s]Extractor Predicting: 11it [00:22,  1.26it/s]Extractor Predicting: 12it [00:23,  1.35it/s]Extractor Predicting: 13it [00:23,  1.40it/s]Extractor Predicting: 14it [00:24,  1.46it/s]Extractor Predicting: 15it [00:24,  1.50it/s]Extractor Predicting: 16it [00:25,  1.53it/s]Extractor Predicting: 17it [00:26,  1.54it/s]Extractor Predicting: 18it [00:26,  1.54it/s]Extractor Predicting: 19it [00:27,  1.58it/s]Extractor Predicting: 20it [00:28,  1.61it/s]Extractor Predicting: 21it [00:28,  1.60it/s]Extractor Predicting: 22it [00:29,  1.59it/s]Extractor Predicting: 23it [00:29,  1.59it/s]Extractor Predicting: 24it [00:30,  1.61it/s]Extractor Predicting: 25it [00:31,  1.60it/s]Extractor Predicting: 26it [00:31,  1.63it/s]Extractor Predicting: 27it [00:32,  1.63it/s]Extractor Predicting: 28it [00:33,  1.62it/s]Extractor Predicting: 29it [00:33,  1.61it/s]Extractor Predicting: 30it [00:34,  1.63it/s]Extractor Predicting: 31it [00:34,  1.60it/s]Extractor Predicting: 32it [00:35,  1.60it/s]Extractor Predicting: 33it [00:36,  1.58it/s]Extractor Predicting: 34it [00:36,  1.56it/s]Extractor Predicting: 35it [00:37,  1.57it/s]Extractor Predicting: 36it [00:38,  1.59it/s]Extractor Predicting: 37it [00:38,  1.58it/s]Extractor Predicting: 38it [00:39,  1.56it/s]Extractor Predicting: 39it [00:40,  1.49it/s]Extractor Predicting: 40it [00:40,  1.50it/s]Extractor Predicting: 41it [00:41,  1.57it/s]Extractor Predicting: 42it [00:41,  1.65it/s]Extractor Predicting: 43it [00:42,  1.64it/s]Extractor Predicting: 44it [00:43,  1.25it/s]Extractor Predicting: 45it [00:44,  1.35it/s]Extractor Predicting: 46it [00:44,  1.44it/s]Extractor Predicting: 47it [00:45,  1.49it/s]Extractor Predicting: 48it [00:46,  1.55it/s]Extractor Predicting: 49it [00:46,  1.58it/s]Extractor Predicting: 50it [00:47,  1.48it/s]Extractor Predicting: 51it [00:48,  1.57it/s]Extractor Predicting: 52it [00:48,  1.62it/s]Extractor Predicting: 53it [00:49,  1.65it/s]Extractor Predicting: 54it [00:49,  1.71it/s]Extractor Predicting: 55it [00:50,  1.73it/s]Extractor Predicting: 56it [00:50,  1.74it/s]Extractor Predicting: 57it [00:51,  1.71it/s]Extractor Predicting: 58it [00:52,  1.69it/s]Extractor Predicting: 59it [00:52,  1.70it/s]Extractor Predicting: 60it [00:53,  1.70it/s]Extractor Predicting: 61it [00:53,  1.69it/s]Extractor Predicting: 62it [00:54,  1.71it/s]Extractor Predicting: 63it [00:54,  1.73it/s]Extractor Predicting: 64it [00:55,  1.75it/s]Extractor Predicting: 65it [00:56,  1.73it/s]Extractor Predicting: 66it [00:56,  1.70it/s]Extractor Predicting: 67it [00:57,  1.70it/s]Extractor Predicting: 68it [00:57,  1.67it/s]Extractor Predicting: 69it [00:58,  1.71it/s]Extractor Predicting: 70it [00:59,  1.71it/s]Extractor Predicting: 71it [00:59,  1.67it/s]Extractor Predicting: 72it [01:00,  1.69it/s]Extractor Predicting: 73it [01:00,  1.67it/s]Extractor Predicting: 74it [01:01,  1.68it/s]Extractor Predicting: 75it [01:02,  1.68it/s]Extractor Predicting: 76it [01:02,  1.67it/s]Extractor Predicting: 77it [01:03,  1.69it/s]Extractor Predicting: 78it [01:03,  1.67it/s]Extractor Predicting: 79it [01:04,  1.68it/s]Extractor Predicting: 80it [01:05,  1.66it/s]Extractor Predicting: 81it [01:05,  1.67it/s]Extractor Predicting: 82it [01:06,  1.63it/s]Extractor Predicting: 83it [01:06,  1.63it/s]Extractor Predicting: 84it [01:07,  1.65it/s]Extractor Predicting: 85it [01:08,  1.63it/s]Extractor Predicting: 86it [01:08,  1.63it/s]Extractor Predicting: 87it [01:09,  1.64it/s]Extractor Predicting: 88it [01:10,  1.60it/s]Extractor Predicting: 89it [01:10,  1.64it/s]Extractor Predicting: 90it [01:13,  1.17s/it]Extractor Predicting: 91it [01:13,  1.03s/it]Extractor Predicting: 92it [01:14,  1.11it/s]Extractor Predicting: 93it [01:15,  1.20it/s]Extractor Predicting: 94it [01:15,  1.29it/s]Extractor Predicting: 95it [01:16,  1.34it/s]Extractor Predicting: 96it [01:17,  1.39it/s]Extractor Predicting: 97it [01:17,  1.40it/s]Extractor Predicting: 98it [01:18,  1.43it/s]Extractor Predicting: 99it [01:19,  1.47it/s]Extractor Predicting: 100it [01:19,  1.50it/s]Extractor Predicting: 101it [01:20,  1.54it/s]Extractor Predicting: 102it [01:20,  1.54it/s]Extractor Predicting: 103it [01:21,  1.54it/s]Extractor Predicting: 104it [01:22,  1.54it/s]Extractor Predicting: 105it [01:22,  1.54it/s]Extractor Predicting: 106it [01:23,  1.55it/s]Extractor Predicting: 107it [01:24,  1.49it/s]Extractor Predicting: 108it [01:24,  1.50it/s]Extractor Predicting: 109it [01:25,  1.55it/s]Extractor Predicting: 110it [01:26,  1.54it/s]Extractor Predicting: 111it [01:26,  1.54it/s]Extractor Predicting: 112it [01:27,  1.55it/s]Extractor Predicting: 113it [01:28,  1.53it/s]Extractor Predicting: 114it [01:28,  1.52it/s]Extractor Predicting: 115it [01:29,  1.51it/s]Extractor Predicting: 116it [01:30,  1.51it/s]Extractor Predicting: 117it [01:30,  1.49it/s]Extractor Predicting: 118it [01:31,  1.49it/s]Extractor Predicting: 119it [01:32,  1.52it/s]Extractor Predicting: 120it [01:32,  1.53it/s]Extractor Predicting: 121it [01:33,  1.51it/s]Extractor Predicting: 122it [01:34,  1.56it/s]Extractor Predicting: 123it [01:34,  1.58it/s]Extractor Predicting: 124it [01:35,  1.54it/s]Extractor Predicting: 125it [01:35,  1.55it/s]Extractor Predicting: 126it [01:36,  1.54it/s]Extractor Predicting: 127it [01:37,  1.52it/s]Extractor Predicting: 128it [01:37,  1.50it/s]Extractor Predicting: 129it [01:38,  1.52it/s]Extractor Predicting: 130it [01:39,  1.50it/s]Extractor Predicting: 131it [01:39,  1.50it/s]Extractor Predicting: 132it [01:40,  1.51it/s]Extractor Predicting: 133it [01:41,  1.47it/s]Extractor Predicting: 134it [01:42,  1.45it/s]Extractor Predicting: 135it [01:42,  1.41it/s]Extractor Predicting: 136it [01:43,  1.41it/s]Extractor Predicting: 137it [01:44,  1.39it/s]Extractor Predicting: 138it [01:45,  1.38it/s]Extractor Predicting: 139it [01:45,  1.30it/s]Extractor Predicting: 140it [01:46,  1.34it/s]Extractor Predicting: 141it [01:47,  1.39it/s]Extractor Predicting: 142it [01:47,  1.40it/s]Extractor Predicting: 143it [01:48,  1.42it/s]Extractor Predicting: 144it [01:49,  1.40it/s]Extractor Predicting: 145it [01:50,  1.41it/s]Extractor Predicting: 146it [01:50,  1.39it/s]Extractor Predicting: 147it [01:51,  1.43it/s]Extractor Predicting: 148it [01:52,  1.43it/s]Extractor Predicting: 149it [01:52,  1.41it/s]Extractor Predicting: 150it [01:53,  1.43it/s]Extractor Predicting: 151it [01:54,  1.43it/s]Extractor Predicting: 152it [01:54,  1.43it/s]Extractor Predicting: 153it [01:55,  1.43it/s]Extractor Predicting: 154it [01:56,  1.40it/s]Extractor Predicting: 155it [01:57,  1.42it/s]Extractor Predicting: 156it [01:57,  1.39it/s]Extractor Predicting: 157it [01:58,  1.37it/s]Extractor Predicting: 158it [01:59,  1.36it/s]Extractor Predicting: 159it [01:59,  1.41it/s]Extractor Predicting: 160it [02:00,  1.45it/s]Extractor Predicting: 161it [02:01,  1.48it/s]Extractor Predicting: 162it [02:01,  1.48it/s]Extractor Predicting: 163it [02:02,  1.50it/s]Extractor Predicting: 164it [02:03,  1.52it/s]Extractor Predicting: 165it [02:03,  1.53it/s]Extractor Predicting: 166it [02:04,  1.55it/s]Extractor Predicting: 167it [02:05,  1.54it/s]Extractor Predicting: 168it [02:05,  1.54it/s]Extractor Predicting: 169it [02:06,  1.55it/s]Extractor Predicting: 170it [02:07,  1.54it/s]Extractor Predicting: 171it [02:07,  1.55it/s]Extractor Predicting: 172it [02:08,  1.56it/s]Extractor Predicting: 173it [02:09,  1.53it/s]Extractor Predicting: 174it [02:09,  1.55it/s]Extractor Predicting: 175it [02:10,  1.53it/s]Extractor Predicting: 176it [02:11,  1.50it/s]Extractor Predicting: 177it [02:11,  1.52it/s]Extractor Predicting: 178it [02:12,  1.52it/s]Extractor Predicting: 179it [02:13,  1.52it/s]Extractor Predicting: 180it [02:13,  1.53it/s]Extractor Predicting: 181it [02:14,  1.52it/s]Extractor Predicting: 182it [02:14,  1.51it/s]Extractor Predicting: 183it [02:15,  1.49it/s]Extractor Predicting: 184it [02:16,  1.50it/s]Extractor Predicting: 185it [02:17,  1.48it/s]Extractor Predicting: 185it [02:17,  1.35it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5757162346521146,
  "recall": 0.08769742310889443,
  "score": 0.1522091974752029,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'employer', 'field of work', 'languages spoken, written or signed', 'lyrics by', 'manufacturer', 'member of sports team', 'occupation', 'performer', 'residence', 'shares border with', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 31765
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31865, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.60it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:07,  1.63it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.61it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:11,  1.58it/s]Extractor Predicting: 20it [00:12,  1.55it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:13,  1.55it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:15,  1.58it/s]Extractor Predicting: 26it [00:16,  1.58it/s]Extractor Predicting: 27it [00:16,  1.61it/s]Extractor Predicting: 28it [00:17,  1.57it/s]Extractor Predicting: 29it [00:18,  1.62it/s]Extractor Predicting: 30it [00:18,  1.71it/s]Extractor Predicting: 31it [00:19,  1.73it/s]Extractor Predicting: 32it [00:19,  1.74it/s]Extractor Predicting: 33it [00:20,  1.71it/s]Extractor Predicting: 34it [00:20,  1.72it/s]Extractor Predicting: 35it [00:21,  1.70it/s]Extractor Predicting: 36it [00:22,  1.69it/s]Extractor Predicting: 37it [00:22,  1.53it/s]Extractor Predicting: 38it [00:23,  1.58it/s]Extractor Predicting: 39it [00:24,  1.57it/s]Extractor Predicting: 40it [00:24,  1.59it/s]Extractor Predicting: 41it [00:25,  1.64it/s]Extractor Predicting: 42it [00:25,  1.65it/s]Extractor Predicting: 43it [00:26,  1.66it/s]Extractor Predicting: 44it [00:27,  1.69it/s]Extractor Predicting: 45it [00:27,  1.70it/s]Extractor Predicting: 46it [00:28,  1.73it/s]Extractor Predicting: 47it [00:28,  1.75it/s]Extractor Predicting: 48it [00:29,  1.70it/s]Extractor Predicting: 49it [00:29,  1.73it/s]Extractor Predicting: 50it [00:30,  1.70it/s]Extractor Predicting: 51it [00:31,  1.69it/s]Extractor Predicting: 52it [00:31,  1.70it/s]Extractor Predicting: 53it [00:32,  1.71it/s]Extractor Predicting: 54it [00:33,  1.65it/s]Extractor Predicting: 55it [00:33,  1.65it/s]Extractor Predicting: 56it [00:34,  1.64it/s]Extractor Predicting: 57it [00:34,  1.70it/s]Extractor Predicting: 58it [00:35,  1.76it/s]Extractor Predicting: 59it [00:35,  1.70it/s]Extractor Predicting: 60it [00:36,  1.66it/s]Extractor Predicting: 61it [00:37,  1.63it/s]Extractor Predicting: 62it [00:37,  1.62it/s]Extractor Predicting: 63it [00:38,  1.60it/s]Extractor Predicting: 64it [00:39,  1.59it/s]Extractor Predicting: 65it [00:39,  1.61it/s]Extractor Predicting: 66it [00:40,  1.61it/s]Extractor Predicting: 67it [00:40,  1.60it/s]Extractor Predicting: 68it [00:41,  1.57it/s]Extractor Predicting: 69it [00:42,  1.60it/s]Extractor Predicting: 70it [00:42,  1.62it/s]Extractor Predicting: 71it [00:43,  1.61it/s]Extractor Predicting: 72it [00:44,  1.57it/s]Extractor Predicting: 73it [00:44,  1.55it/s]Extractor Predicting: 74it [00:45,  1.54it/s]Extractor Predicting: 75it [00:46,  1.56it/s]Extractor Predicting: 76it [00:46,  1.53it/s]Extractor Predicting: 77it [00:47,  1.56it/s]Extractor Predicting: 78it [00:47,  1.60it/s]Extractor Predicting: 79it [00:48,  1.59it/s]Extractor Predicting: 80it [00:49,  1.64it/s]Extractor Predicting: 81it [00:49,  1.62it/s]Extractor Predicting: 82it [00:50,  1.60it/s]Extractor Predicting: 83it [00:51,  1.63it/s]Extractor Predicting: 84it [00:51,  1.61it/s]Extractor Predicting: 85it [00:52,  1.60it/s]Extractor Predicting: 86it [00:52,  1.61it/s]Extractor Predicting: 87it [00:53,  1.59it/s]Extractor Predicting: 88it [00:54,  1.59it/s]Extractor Predicting: 89it [00:54,  1.62it/s]Extractor Predicting: 90it [00:55,  1.63it/s]Extractor Predicting: 91it [00:56,  1.61it/s]Extractor Predicting: 92it [00:56,  1.58it/s]Extractor Predicting: 93it [00:57,  1.60it/s]Extractor Predicting: 94it [00:57,  1.63it/s]Extractor Predicting: 95it [00:58,  1.62it/s]Extractor Predicting: 96it [00:59,  1.62it/s]Extractor Predicting: 97it [00:59,  1.60it/s]Extractor Predicting: 98it [01:00,  1.55it/s]Extractor Predicting: 99it [01:01,  1.53it/s]Extractor Predicting: 100it [01:01,  1.51it/s]Extractor Predicting: 101it [01:02,  1.50it/s]Extractor Predicting: 102it [01:03,  1.51it/s]Extractor Predicting: 103it [01:03,  1.47it/s]Extractor Predicting: 104it [01:04,  1.54it/s]Extractor Predicting: 105it [01:05,  1.51it/s]Extractor Predicting: 106it [01:05,  1.55it/s]Extractor Predicting: 107it [01:06,  1.57it/s]Extractor Predicting: 108it [01:06,  1.58it/s]Extractor Predicting: 109it [01:07,  1.57it/s]Extractor Predicting: 110it [01:08,  1.56it/s]Extractor Predicting: 111it [01:08,  1.60it/s]Extractor Predicting: 112it [01:09,  1.60it/s]Extractor Predicting: 113it [01:10,  1.60it/s]Extractor Predicting: 114it [01:10,  1.61it/s]Extractor Predicting: 115it [01:11,  1.62it/s]Extractor Predicting: 116it [01:11,  1.62it/s]Extractor Predicting: 117it [01:12,  1.63it/s]Extractor Predicting: 118it [01:13,  1.67it/s]Extractor Predicting: 119it [01:13,  1.74it/s]Extractor Predicting: 120it [01:14,  1.68it/s]Extractor Predicting: 121it [01:14,  1.64it/s]Extractor Predicting: 122it [01:15,  1.64it/s]Extractor Predicting: 123it [01:16,  1.59it/s]Extractor Predicting: 124it [01:16,  1.59it/s]Extractor Predicting: 125it [01:17,  1.58it/s]Extractor Predicting: 126it [01:18,  1.58it/s]Extractor Predicting: 127it [01:18,  1.53it/s]Extractor Predicting: 128it [01:19,  1.56it/s]Extractor Predicting: 129it [01:20,  1.55it/s]Extractor Predicting: 130it [01:20,  1.56it/s]Extractor Predicting: 131it [01:21,  1.56it/s]Extractor Predicting: 132it [01:22,  1.50it/s]Extractor Predicting: 133it [01:22,  1.52it/s]Extractor Predicting: 134it [01:23,  1.55it/s]Extractor Predicting: 135it [01:24,  1.50it/s]Extractor Predicting: 136it [01:24,  1.53it/s]Extractor Predicting: 137it [01:25,  1.56it/s]Extractor Predicting: 138it [01:25,  1.55it/s]Extractor Predicting: 139it [01:26,  1.56it/s]Extractor Predicting: 140it [01:27,  1.52it/s]Extractor Predicting: 141it [01:27,  1.49it/s]Extractor Predicting: 142it [01:28,  1.51it/s]Extractor Predicting: 143it [01:29,  1.54it/s]Extractor Predicting: 144it [01:29,  1.55it/s]Extractor Predicting: 145it [01:30,  1.53it/s]Extractor Predicting: 146it [01:31,  1.52it/s]Extractor Predicting: 147it [01:31,  1.53it/s]Extractor Predicting: 148it [01:32,  1.52it/s]Extractor Predicting: 149it [01:33,  1.52it/s]Extractor Predicting: 150it [01:33,  1.54it/s]Extractor Predicting: 151it [01:34,  1.54it/s]Extractor Predicting: 152it [01:35,  1.58it/s]Extractor Predicting: 153it [01:35,  1.58it/s]Extractor Predicting: 154it [01:36,  1.57it/s]Extractor Predicting: 155it [01:37,  1.53it/s]Extractor Predicting: 156it [01:37,  1.53it/s]Extractor Predicting: 157it [01:38,  1.49it/s]Extractor Predicting: 158it [01:39,  1.51it/s]Extractor Predicting: 159it [01:39,  1.48it/s]Extractor Predicting: 160it [01:40,  1.50it/s]Extractor Predicting: 161it [01:41,  1.51it/s]Extractor Predicting: 162it [01:41,  1.53it/s]Extractor Predicting: 163it [01:42,  1.56it/s]Extractor Predicting: 164it [01:42,  1.55it/s]Extractor Predicting: 165it [01:43,  1.57it/s]Extractor Predicting: 166it [01:44,  1.55it/s]Extractor Predicting: 167it [01:44,  1.55it/s]Extractor Predicting: 168it [01:45,  1.37it/s]Extractor Predicting: 169it [01:46,  1.45it/s]Extractor Predicting: 170it [01:47,  1.47it/s]Extractor Predicting: 171it [01:47,  1.51it/s]Extractor Predicting: 172it [01:48,  1.54it/s]Extractor Predicting: 173it [01:48,  1.55it/s]Extractor Predicting: 174it [01:49,  1.56it/s]Extractor Predicting: 175it [01:50,  1.56it/s]Extractor Predicting: 176it [01:50,  1.58it/s]Extractor Predicting: 177it [01:51,  1.59it/s]Extractor Predicting: 178it [01:52,  1.56it/s]Extractor Predicting: 179it [01:52,  1.59it/s]Extractor Predicting: 180it [01:53,  1.61it/s]Extractor Predicting: 181it [01:53,  1.63it/s]Extractor Predicting: 182it [01:54,  1.61it/s]Extractor Predicting: 183it [01:55,  1.62it/s]Extractor Predicting: 184it [01:55,  1.60it/s]Extractor Predicting: 185it [01:56,  1.62it/s]Extractor Predicting: 186it [01:56,  1.62it/s]Extractor Predicting: 187it [01:57,  1.61it/s]Extractor Predicting: 188it [01:58,  1.59it/s]Extractor Predicting: 189it [01:58,  1.58it/s]Extractor Predicting: 190it [01:59,  1.58it/s]Extractor Predicting: 191it [02:00,  1.58it/s]Extractor Predicting: 192it [02:00,  1.59it/s]Extractor Predicting: 193it [02:01,  1.59it/s]Extractor Predicting: 194it [02:02,  1.58it/s]Extractor Predicting: 195it [02:02,  1.60it/s]Extractor Predicting: 196it [02:03,  1.63it/s]Extractor Predicting: 197it [02:03,  1.62it/s]Extractor Predicting: 198it [02:04,  1.58it/s]Extractor Predicting: 199it [02:05,  1.58it/s]Extractor Predicting: 200it [02:05,  1.58it/s]Extractor Predicting: 201it [02:06,  1.57it/s]Extractor Predicting: 202it [02:07,  1.52it/s]Extractor Predicting: 203it [02:07,  1.53it/s]Extractor Predicting: 204it [02:08,  1.52it/s]Extractor Predicting: 205it [02:09,  1.52it/s]Extractor Predicting: 206it [02:09,  1.56it/s]Extractor Predicting: 207it [02:10,  1.57it/s]Extractor Predicting: 208it [02:11,  1.58it/s]Extractor Predicting: 209it [02:11,  1.57it/s]Extractor Predicting: 210it [02:12,  1.59it/s]Extractor Predicting: 211it [02:12,  1.57it/s]Extractor Predicting: 212it [02:13,  1.63it/s]Extractor Predicting: 213it [02:14,  1.62it/s]Extractor Predicting: 214it [02:14,  1.61it/s]Extractor Predicting: 215it [02:15,  1.55it/s]Extractor Predicting: 216it [02:16,  1.53it/s]Extractor Predicting: 217it [02:16,  1.57it/s]Extractor Predicting: 218it [02:17,  1.56it/s]Extractor Predicting: 219it [02:18,  1.54it/s]Extractor Predicting: 220it [02:18,  1.53it/s]Extractor Predicting: 221it [02:19,  1.58it/s]Extractor Predicting: 222it [02:19,  1.59it/s]Extractor Predicting: 223it [02:20,  1.60it/s]Extractor Predicting: 224it [02:21,  1.61it/s]Extractor Predicting: 225it [02:21,  1.61it/s]Extractor Predicting: 226it [02:22,  1.60it/s]Extractor Predicting: 227it [02:23,  1.58it/s]Extractor Predicting: 228it [02:23,  1.60it/s]Extractor Predicting: 229it [02:24,  1.59it/s]Extractor Predicting: 230it [02:24,  1.59it/s]Extractor Predicting: 231it [02:25,  1.62it/s]Extractor Predicting: 232it [02:26,  1.64it/s]Extractor Predicting: 233it [02:26,  1.59it/s]Extractor Predicting: 234it [02:27,  1.63it/s]Extractor Predicting: 235it [02:27,  1.64it/s]Extractor Predicting: 236it [02:28,  1.61it/s]Extractor Predicting: 237it [02:29,  1.61it/s]Extractor Predicting: 238it [02:29,  1.61it/s]Extractor Predicting: 239it [02:30,  1.60it/s]Extractor Predicting: 240it [02:31,  1.62it/s]Extractor Predicting: 241it [02:31,  1.64it/s]Extractor Predicting: 242it [02:32,  1.59it/s]Extractor Predicting: 243it [02:32,  1.55it/s]Extractor Predicting: 244it [02:33,  1.59it/s]Extractor Predicting: 245it [02:34,  1.62it/s]Extractor Predicting: 246it [02:34,  1.59it/s]Extractor Predicting: 247it [02:35,  1.59it/s]Extractor Predicting: 248it [02:36,  1.62it/s]Extractor Predicting: 249it [02:36,  1.64it/s]Extractor Predicting: 250it [02:37,  1.62it/s]Extractor Predicting: 251it [02:37,  1.65it/s]Extractor Predicting: 252it [02:38,  1.66it/s]Extractor Predicting: 253it [02:39,  1.65it/s]Extractor Predicting: 254it [02:39,  1.68it/s]Extractor Predicting: 255it [02:40,  1.65it/s]Extractor Predicting: 256it [02:40,  1.64it/s]Extractor Predicting: 257it [02:41,  1.61it/s]Extractor Predicting: 258it [02:42,  1.61it/s]Extractor Predicting: 259it [02:42,  1.60it/s]Extractor Predicting: 260it [02:43,  1.61it/s]Extractor Predicting: 261it [02:44,  1.62it/s]Extractor Predicting: 262it [02:44,  1.63it/s]Extractor Predicting: 263it [02:45,  1.62it/s]Extractor Predicting: 264it [02:45,  1.61it/s]Extractor Predicting: 265it [02:46,  1.58it/s]Extractor Predicting: 266it [02:47,  1.60it/s]Extractor Predicting: 267it [02:47,  1.62it/s]Extractor Predicting: 268it [02:48,  1.59it/s]Extractor Predicting: 269it [02:49,  1.54it/s]Extractor Predicting: 270it [02:49,  1.61it/s]Extractor Predicting: 271it [02:50,  1.62it/s]Extractor Predicting: 272it [02:50,  1.59it/s]Extractor Predicting: 273it [02:51,  1.59it/s]Extractor Predicting: 274it [02:52,  1.59it/s]Extractor Predicting: 275it [02:52,  1.58it/s]Extractor Predicting: 276it [02:53,  1.58it/s]Extractor Predicting: 277it [02:54,  1.60it/s]Extractor Predicting: 278it [02:54,  1.57it/s]Extractor Predicting: 279it [02:55,  1.56it/s]Extractor Predicting: 280it [02:56,  1.56it/s]Extractor Predicting: 281it [02:56,  1.55it/s]Extractor Predicting: 282it [02:57,  1.56it/s]Extractor Predicting: 283it [02:57,  1.59it/s]Extractor Predicting: 284it [02:58,  1.53it/s]Extractor Predicting: 285it [02:59,  1.35it/s]Extractor Predicting: 286it [03:00,  1.43it/s]Extractor Predicting: 287it [03:00,  1.46it/s]Extractor Predicting: 288it [03:01,  1.48it/s]Extractor Predicting: 289it [03:02,  1.53it/s]Extractor Predicting: 290it [03:02,  1.45it/s]Extractor Predicting: 291it [03:03,  1.53it/s]Extractor Predicting: 292it [03:04,  1.50it/s]Extractor Predicting: 293it [03:04,  1.54it/s]Extractor Predicting: 294it [03:05,  1.59it/s]Extractor Predicting: 295it [03:05,  1.58it/s]Extractor Predicting: 296it [03:06,  1.62it/s]Extractor Predicting: 297it [03:07,  1.63it/s]Extractor Predicting: 298it [03:07,  1.65it/s]Extractor Predicting: 299it [03:08,  1.61it/s]Extractor Predicting: 300it [03:08,  1.61it/s]Extractor Predicting: 301it [03:09,  1.58it/s]Extractor Predicting: 302it [03:10,  1.58it/s]Extractor Predicting: 303it [03:10,  1.62it/s]Extractor Predicting: 304it [03:11,  1.58it/s]Extractor Predicting: 305it [03:12,  1.56it/s]Extractor Predicting: 306it [03:12,  1.58it/s]Extractor Predicting: 307it [03:13,  1.58it/s]Extractor Predicting: 308it [03:14,  1.62it/s]Extractor Predicting: 309it [03:14,  1.59it/s]Extractor Predicting: 310it [03:15,  1.59it/s]Extractor Predicting: 311it [03:15,  1.58it/s]Extractor Predicting: 312it [03:16,  1.56it/s]Extractor Predicting: 313it [03:17,  1.55it/s]Extractor Predicting: 314it [03:17,  1.52it/s]Extractor Predicting: 315it [03:18,  1.51it/s]Extractor Predicting: 316it [03:19,  1.54it/s]Extractor Predicting: 317it [03:19,  1.56it/s]Extractor Predicting: 318it [03:20,  1.60it/s]Extractor Predicting: 319it [03:21,  1.58it/s]Extractor Predicting: 320it [03:21,  1.65it/s]Extractor Predicting: 321it [03:22,  1.63it/s]Extractor Predicting: 322it [03:22,  1.62it/s]Extractor Predicting: 323it [03:23,  1.61it/s]Extractor Predicting: 324it [03:24,  1.63it/s]Extractor Predicting: 325it [03:24,  1.60it/s]Extractor Predicting: 326it [03:25,  1.59it/s]Extractor Predicting: 327it [03:26,  1.54it/s]Extractor Predicting: 328it [03:26,  1.51it/s]Extractor Predicting: 329it [03:27,  1.55it/s]Extractor Predicting: 330it [03:28,  1.60it/s]Extractor Predicting: 331it [03:28,  1.62it/s]Extractor Predicting: 332it [03:29,  1.62it/s]Extractor Predicting: 333it [03:29,  1.60it/s]Extractor Predicting: 334it [03:30,  1.57it/s]Extractor Predicting: 335it [03:31,  1.58it/s]Extractor Predicting: 336it [03:31,  1.58it/s]Extractor Predicting: 337it [03:32,  1.60it/s]Extractor Predicting: 338it [03:32,  1.62it/s]Extractor Predicting: 339it [03:33,  1.64it/s]Extractor Predicting: 340it [03:34,  1.67it/s]Extractor Predicting: 341it [03:34,  1.63it/s]Extractor Predicting: 342it [03:35,  1.64it/s]Extractor Predicting: 343it [03:35,  1.67it/s]Extractor Predicting: 344it [03:36,  1.64it/s]Extractor Predicting: 345it [03:37,  1.64it/s]Extractor Predicting: 346it [03:37,  1.58it/s]Extractor Predicting: 347it [03:38,  1.60it/s]Extractor Predicting: 348it [03:39,  1.58it/s]Extractor Predicting: 349it [03:39,  1.59it/s]Extractor Predicting: 350it [03:40,  1.61it/s]Extractor Predicting: 351it [03:41,  1.62it/s]Extractor Predicting: 352it [03:41,  1.65it/s]Extractor Predicting: 353it [03:42,  1.64it/s]Extractor Predicting: 354it [03:42,  1.64it/s]Extractor Predicting: 355it [03:43,  1.62it/s]Extractor Predicting: 356it [03:44,  1.62it/s]Extractor Predicting: 357it [03:44,  1.60it/s]Extractor Predicting: 358it [03:45,  1.61it/s]Extractor Predicting: 359it [03:45,  1.59it/s]Extractor Predicting: 360it [03:46,  1.60it/s]Extractor Predicting: 361it [03:47,  1.63it/s]Extractor Predicting: 362it [03:47,  1.62it/s]Extractor Predicting: 363it [03:48,  1.64it/s]Extractor Predicting: 364it [03:49,  1.62it/s]Extractor Predicting: 365it [03:49,  1.64it/s]Extractor Predicting: 366it [03:50,  1.62it/s]Extractor Predicting: 367it [03:50,  1.63it/s]Extractor Predicting: 368it [03:51,  1.59it/s]Extractor Predicting: 369it [03:52,  1.57it/s]Extractor Predicting: 370it [03:52,  1.62it/s]Extractor Predicting: 371it [03:53,  1.62it/s]Extractor Predicting: 372it [03:53,  1.64it/s]Extractor Predicting: 373it [03:54,  1.66it/s]Extractor Predicting: 374it [03:55,  1.65it/s]Extractor Predicting: 375it [03:55,  1.65it/s]Extractor Predicting: 376it [03:56,  1.62it/s]Extractor Predicting: 377it [03:57,  1.60it/s]Extractor Predicting: 378it [03:57,  1.61it/s]Extractor Predicting: 379it [03:58,  1.63it/s]Extractor Predicting: 380it [03:58,  1.63it/s]Extractor Predicting: 381it [03:59,  1.65it/s]Extractor Predicting: 382it [04:00,  1.63it/s]Extractor Predicting: 383it [04:00,  1.64it/s]Extractor Predicting: 384it [04:01,  1.62it/s]Extractor Predicting: 385it [04:01,  1.59it/s]Extractor Predicting: 386it [04:02,  1.62it/s]Extractor Predicting: 387it [04:03,  1.63it/s]Extractor Predicting: 388it [04:03,  1.59it/s]Extractor Predicting: 389it [04:04,  1.55it/s]Extractor Predicting: 390it [04:05,  1.51it/s]Extractor Predicting: 391it [04:05,  1.53it/s]Extractor Predicting: 392it [04:06,  1.54it/s]Extractor Predicting: 393it [04:07,  1.55it/s]Extractor Predicting: 394it [04:07,  1.60it/s]Extractor Predicting: 395it [04:08,  1.60it/s]Extractor Predicting: 396it [04:08,  1.61it/s]Extractor Predicting: 397it [04:09,  1.63it/s]Extractor Predicting: 398it [04:10,  1.64it/s]Extractor Predicting: 399it [04:10,  1.66it/s]Extractor Predicting: 400it [04:11,  1.64it/s]Extractor Predicting: 401it [04:11,  1.63it/s]Extractor Predicting: 402it [04:12,  1.63it/s]Extractor Predicting: 403it [04:13,  1.60it/s]Extractor Predicting: 404it [04:13,  1.59it/s]Extractor Predicting: 405it [04:14,  1.60it/s]Extractor Predicting: 406it [04:15,  1.60it/s]Extractor Predicting: 407it [04:15,  1.57it/s]Extractor Predicting: 408it [04:16,  1.58it/s]Extractor Predicting: 409it [04:17,  1.58it/s]Extractor Predicting: 410it [04:17,  1.60it/s]Extractor Predicting: 411it [04:18,  1.58it/s]Extractor Predicting: 412it [04:18,  1.57it/s]Extractor Predicting: 413it [04:19,  1.52it/s]Extractor Predicting: 414it [04:20,  1.57it/s]Extractor Predicting: 415it [04:20,  1.62it/s]Extractor Predicting: 416it [04:21,  1.60it/s]Extractor Predicting: 417it [04:22,  1.65it/s]Extractor Predicting: 418it [04:22,  1.66it/s]Extractor Predicting: 419it [04:23,  1.68it/s]Extractor Predicting: 420it [04:23,  1.65it/s]Extractor Predicting: 421it [04:24,  1.63it/s]Extractor Predicting: 422it [04:25,  1.66it/s]Extractor Predicting: 423it [04:25,  1.62it/s]Extractor Predicting: 424it [04:26,  1.42it/s]Extractor Predicting: 425it [04:27,  1.51it/s]Extractor Predicting: 426it [04:27,  1.55it/s]Extractor Predicting: 427it [04:28,  1.58it/s]Extractor Predicting: 428it [04:29,  1.57it/s]Extractor Predicting: 429it [04:29,  1.59it/s]Extractor Predicting: 430it [04:30,  1.64it/s]Extractor Predicting: 431it [04:30,  1.59it/s]Extractor Predicting: 432it [04:31,  1.62it/s]Extractor Predicting: 433it [04:32,  1.67it/s]Extractor Predicting: 434it [04:32,  1.68it/s]Extractor Predicting: 435it [04:33,  1.67it/s]Extractor Predicting: 436it [04:33,  1.69it/s]Extractor Predicting: 437it [04:34,  1.68it/s]Extractor Predicting: 438it [04:34,  1.68it/s]Extractor Predicting: 439it [04:35,  1.67it/s]Extractor Predicting: 440it [04:36,  1.61it/s]Extractor Predicting: 441it [04:36,  1.65it/s]Extractor Predicting: 442it [04:37,  1.62it/s]Extractor Predicting: 443it [04:38,  1.64it/s]Extractor Predicting: 444it [04:38,  1.64it/s]Extractor Predicting: 445it [04:39,  1.61it/s]Extractor Predicting: 446it [04:40,  1.56it/s]Extractor Predicting: 447it [04:40,  1.58it/s]Extractor Predicting: 448it [04:41,  1.60it/s]Extractor Predicting: 449it [04:41,  1.61it/s]Extractor Predicting: 450it [04:42,  1.62it/s]Extractor Predicting: 451it [04:43,  1.63it/s]Extractor Predicting: 452it [04:43,  1.63it/s]Extractor Predicting: 453it [04:44,  1.60it/s]Extractor Predicting: 454it [04:44,  1.58it/s]Extractor Predicting: 455it [04:45,  1.54it/s]Extractor Predicting: 456it [04:46,  1.52it/s]Extractor Predicting: 457it [04:46,  1.51it/s]Extractor Predicting: 458it [04:47,  1.52it/s]Extractor Predicting: 459it [04:48,  1.54it/s]Extractor Predicting: 460it [04:48,  1.55it/s]Extractor Predicting: 461it [04:49,  1.55it/s]Extractor Predicting: 462it [04:50,  1.51it/s]Extractor Predicting: 463it [04:50,  1.53it/s]Extractor Predicting: 464it [04:51,  1.52it/s]Extractor Predicting: 465it [04:52,  1.54it/s]Extractor Predicting: 466it [04:52,  1.54it/s]Extractor Predicting: 467it [04:53,  1.58it/s]Extractor Predicting: 468it [04:54,  1.52it/s]Extractor Predicting: 469it [04:54,  1.51it/s]Extractor Predicting: 470it [04:55,  1.51it/s]Extractor Predicting: 471it [04:56,  1.52it/s]Extractor Predicting: 472it [04:56,  1.50it/s]Extractor Predicting: 473it [04:57,  1.50it/s]Extractor Predicting: 474it [04:58,  1.51it/s]Extractor Predicting: 475it [04:58,  1.51it/s]Extractor Predicting: 476it [04:59,  1.48it/s]Extractor Predicting: 477it [05:00,  1.49it/s]Extractor Predicting: 478it [05:00,  1.51it/s]Extractor Predicting: 479it [05:01,  1.50it/s]Extractor Predicting: 480it [05:02,  1.52it/s]Extractor Predicting: 481it [05:02,  1.53it/s]Extractor Predicting: 482it [05:03,  1.52it/s]Extractor Predicting: 483it [05:04,  1.50it/s]Extractor Predicting: 484it [05:04,  1.48it/s]Extractor Predicting: 485it [05:05,  1.50it/s]Extractor Predicting: 486it [05:06,  1.54it/s]Extractor Predicting: 487it [05:06,  1.54it/s]Extractor Predicting: 488it [05:07,  1.54it/s]Extractor Predicting: 489it [05:08,  1.49it/s]Extractor Predicting: 490it [05:08,  1.52it/s]Extractor Predicting: 491it [05:09,  1.51it/s]Extractor Predicting: 492it [05:10,  1.46it/s]Extractor Predicting: 493it [05:10,  1.46it/s]Extractor Predicting: 494it [05:11,  1.52it/s]Extractor Predicting: 495it [05:12,  1.47it/s]Extractor Predicting: 496it [05:12,  1.47it/s]Extractor Predicting: 497it [05:13,  1.48it/s]Extractor Predicting: 498it [05:14,  1.53it/s]Extractor Predicting: 499it [05:14,  1.54it/s]Extractor Predicting: 500it [05:15,  1.52it/s]Extractor Predicting: 501it [05:16,  1.50it/s]Extractor Predicting: 502it [05:16,  1.46it/s]Extractor Predicting: 503it [05:17,  1.47it/s]Extractor Predicting: 504it [05:18,  1.48it/s]Extractor Predicting: 505it [05:18,  1.53it/s]Extractor Predicting: 506it [05:19,  1.51it/s]Extractor Predicting: 507it [05:20,  1.54it/s]Extractor Predicting: 508it [05:20,  1.55it/s]Extractor Predicting: 509it [05:21,  1.57it/s]Extractor Predicting: 510it [05:21,  1.61it/s]Extractor Predicting: 511it [05:22,  1.57it/s]Extractor Predicting: 512it [05:23,  1.58it/s]Extractor Predicting: 513it [05:23,  1.55it/s]Extractor Predicting: 514it [05:24,  1.56it/s]Extractor Predicting: 515it [05:25,  1.55it/s]Extractor Predicting: 516it [05:25,  1.50it/s]Extractor Predicting: 517it [05:26,  1.43it/s]Extractor Predicting: 518it [05:27,  1.43it/s]Extractor Predicting: 519it [05:27,  1.48it/s]Extractor Predicting: 520it [05:28,  1.55it/s]Extractor Predicting: 521it [05:29,  1.56it/s]Extractor Predicting: 522it [05:29,  1.55it/s]Extractor Predicting: 523it [05:30,  1.54it/s]Extractor Predicting: 524it [05:31,  1.54it/s]Extractor Predicting: 525it [05:31,  1.52it/s]Extractor Predicting: 526it [05:32,  1.46it/s]Extractor Predicting: 527it [05:33,  1.46it/s]Extractor Predicting: 528it [05:33,  1.50it/s]Extractor Predicting: 529it [05:34,  1.51it/s]Extractor Predicting: 530it [05:35,  1.50it/s]Extractor Predicting: 531it [05:35,  1.53it/s]Extractor Predicting: 532it [05:36,  1.47it/s]Extractor Predicting: 533it [05:37,  1.49it/s]Extractor Predicting: 534it [05:37,  1.50it/s]Extractor Predicting: 535it [05:38,  1.53it/s]Extractor Predicting: 536it [05:39,  1.47it/s]Extractor Predicting: 537it [05:39,  1.50it/s]Extractor Predicting: 538it [05:40,  1.48it/s]Extractor Predicting: 539it [05:41,  1.48it/s]Extractor Predicting: 540it [05:41,  1.49it/s]Extractor Predicting: 541it [05:42,  1.39it/s]Extractor Predicting: 542it [05:43,  1.41it/s]Extractor Predicting: 543it [05:44,  1.44it/s]Extractor Predicting: 544it [05:44,  1.47it/s]Extractor Predicting: 545it [05:45,  1.44it/s]Extractor Predicting: 546it [05:46,  1.28it/s]Extractor Predicting: 547it [05:47,  1.34it/s]Extractor Predicting: 548it [05:47,  1.41it/s]Extractor Predicting: 549it [05:48,  1.39it/s]Extractor Predicting: 550it [05:49,  1.42it/s]Extractor Predicting: 551it [05:49,  1.46it/s]Extractor Predicting: 552it [05:50,  1.46it/s]Extractor Predicting: 553it [05:51,  1.50it/s]Extractor Predicting: 554it [05:51,  1.51it/s]Extractor Predicting: 555it [05:52,  1.51it/s]Extractor Predicting: 556it [05:53,  1.52it/s]Extractor Predicting: 557it [05:53,  1.50it/s]Extractor Predicting: 558it [05:54,  1.49it/s]Extractor Predicting: 559it [05:55,  1.51it/s]Extractor Predicting: 560it [05:55,  1.55it/s]Extractor Predicting: 561it [05:56,  1.53it/s]Extractor Predicting: 562it [05:57,  1.54it/s]Extractor Predicting: 563it [05:57,  1.53it/s]Extractor Predicting: 564it [05:58,  1.51it/s]Extractor Predicting: 565it [05:59,  1.51it/s]Extractor Predicting: 566it [05:59,  1.51it/s]Extractor Predicting: 567it [06:00,  1.52it/s]Extractor Predicting: 568it [06:00,  1.58it/s]Extractor Predicting: 569it [06:01,  1.53it/s]Extractor Predicting: 570it [06:02,  1.51it/s]Extractor Predicting: 571it [06:02,  1.49it/s]Extractor Predicting: 572it [06:03,  1.45it/s]Extractor Predicting: 573it [06:04,  1.40it/s]Extractor Predicting: 574it [06:05,  1.41it/s]Extractor Predicting: 575it [06:05,  1.43it/s]Extractor Predicting: 576it [06:06,  1.46it/s]Extractor Predicting: 577it [06:06,  1.80it/s]Extractor Predicting: 577it [06:06,  1.57it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.22362602653190145,
  "recall": 0.025600231414521262,
  "score": 0.04594121082343781,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'employer', 'field of work', 'languages spoken, written or signed', 'lyrics by', 'manufacturer', 'member of sports team', 'occupation', 'performer', 'residence', 'shares border with', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 11382
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11482, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.61it/s]Extractor Predicting: 9it [00:05,  1.66it/s]Extractor Predicting: 10it [00:06,  1.71it/s]Extractor Predicting: 11it [00:06,  1.67it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:07,  1.66it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:09,  1.60it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:11,  1.54it/s]Extractor Predicting: 20it [00:12,  1.51it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:13,  1.54it/s]Extractor Predicting: 23it [00:14,  1.54it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:15,  1.49it/s]Extractor Predicting: 26it [00:16,  1.52it/s]Extractor Predicting: 27it [00:17,  1.51it/s]Extractor Predicting: 28it [00:17,  1.51it/s]Extractor Predicting: 29it [00:18,  1.47it/s]Extractor Predicting: 30it [00:19,  1.47it/s]Extractor Predicting: 31it [00:19,  1.53it/s]Extractor Predicting: 32it [00:20,  1.56it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 34it [00:21,  1.53it/s]Extractor Predicting: 35it [00:22,  1.58it/s]Extractor Predicting: 36it [00:22,  1.58it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:24,  1.52it/s]Extractor Predicting: 39it [00:25,  1.48it/s]Extractor Predicting: 40it [00:25,  1.46it/s]Extractor Predicting: 41it [00:26,  1.46it/s]Extractor Predicting: 42it [00:27,  1.48it/s]Extractor Predicting: 43it [00:27,  1.48it/s]Extractor Predicting: 44it [00:28,  1.41it/s]Extractor Predicting: 45it [00:29,  1.43it/s]Extractor Predicting: 46it [00:29,  1.46it/s]Extractor Predicting: 47it [00:30,  1.49it/s]Extractor Predicting: 48it [00:31,  1.49it/s]Extractor Predicting: 49it [00:31,  1.50it/s]Extractor Predicting: 50it [00:32,  1.49it/s]Extractor Predicting: 51it [00:33,  1.51it/s]Extractor Predicting: 52it [00:33,  1.48it/s]Extractor Predicting: 53it [00:34,  1.49it/s]Extractor Predicting: 54it [00:35,  1.53it/s]Extractor Predicting: 55it [00:35,  1.55it/s]Extractor Predicting: 56it [00:36,  1.57it/s]Extractor Predicting: 57it [00:37,  1.56it/s]Extractor Predicting: 58it [00:37,  1.58it/s]Extractor Predicting: 59it [00:38,  1.58it/s]Extractor Predicting: 60it [00:39,  1.40it/s]Extractor Predicting: 61it [00:39,  1.48it/s]Extractor Predicting: 62it [00:40,  1.48it/s]Extractor Predicting: 63it [00:41,  1.49it/s]Extractor Predicting: 64it [00:41,  1.45it/s]Extractor Predicting: 65it [00:42,  1.47it/s]Extractor Predicting: 66it [00:43,  1.48it/s]Extractor Predicting: 67it [00:43,  1.49it/s]Extractor Predicting: 68it [00:44,  1.51it/s]Extractor Predicting: 69it [00:45,  1.50it/s]Extractor Predicting: 70it [00:45,  1.50it/s]Extractor Predicting: 71it [00:46,  1.51it/s]Extractor Predicting: 72it [00:47,  1.53it/s]Extractor Predicting: 73it [00:47,  1.55it/s]Extractor Predicting: 74it [00:48,  1.59it/s]Extractor Predicting: 75it [00:48,  1.58it/s]Extractor Predicting: 76it [00:49,  1.59it/s]Extractor Predicting: 77it [00:50,  1.61it/s]Extractor Predicting: 78it [00:50,  1.59it/s]Extractor Predicting: 79it [00:51,  1.62it/s]Extractor Predicting: 80it [00:51,  1.63it/s]Extractor Predicting: 81it [00:52,  1.63it/s]Extractor Predicting: 82it [00:53,  1.63it/s]Extractor Predicting: 83it [00:53,  1.60it/s]Extractor Predicting: 84it [00:54,  1.55it/s]Extractor Predicting: 85it [00:55,  1.59it/s]Extractor Predicting: 86it [00:55,  1.57it/s]Extractor Predicting: 87it [00:56,  1.55it/s]Extractor Predicting: 88it [00:57,  1.54it/s]Extractor Predicting: 89it [00:57,  1.50it/s]Extractor Predicting: 90it [00:58,  1.52it/s]Extractor Predicting: 91it [00:59,  1.50it/s]Extractor Predicting: 92it [00:59,  1.49it/s]Extractor Predicting: 93it [01:00,  1.49it/s]Extractor Predicting: 94it [01:01,  1.48it/s]Extractor Predicting: 95it [01:01,  1.45it/s]Extractor Predicting: 96it [01:02,  1.45it/s]Extractor Predicting: 97it [01:03,  1.47it/s]Extractor Predicting: 98it [01:03,  1.47it/s]Extractor Predicting: 99it [01:04,  1.47it/s]Extractor Predicting: 100it [01:05,  1.50it/s]Extractor Predicting: 101it [01:05,  1.50it/s]Extractor Predicting: 102it [01:06,  1.49it/s]Extractor Predicting: 103it [01:07,  1.47it/s]Extractor Predicting: 104it [01:07,  1.47it/s]Extractor Predicting: 105it [01:08,  1.50it/s]Extractor Predicting: 106it [01:09,  1.50it/s]Extractor Predicting: 107it [01:10,  1.47it/s]Extractor Predicting: 108it [01:10,  1.45it/s]Extractor Predicting: 109it [01:11,  1.42it/s]Extractor Predicting: 110it [01:12,  1.43it/s]Extractor Predicting: 110it [01:12,  1.52it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3176470588235294,
  "recall": 0.008739278200356044,
  "score": 0.017010552842967396,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:17<05:39, 17.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:31<04:36, 15.38s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:49<04:38, 16.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:02<04:05, 15.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:19<03:57, 15.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:35<03:42, 15.89s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:50<03:22, 15.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:04<03:01, 15.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:19<02:45, 15.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:35<02:34, 15.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:54<02:27, 16.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:09<02:08, 16.05s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:22<01:46, 15.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:34<01:24, 14.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:51<01:14, 14.97s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:05<00:59, 14.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:21<00:45, 15.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:36<00:29, 14.97s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:48<00:14, 14.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:05<00:00, 14.94s/it]Generating: 100%|██████████| 20/20 [05:05<00:00, 15.26s/it]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 395, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 472, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 537, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 624, 'raw': 800}
{'prompt': 'Relation : conflict .', 'success_rate': 0.78, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Red Sea', 'conflict', '', 'It includes the areas surrounding its borders , such as Gomorrah , Eresden , the Wallach Mountains , Iqbal and the Red Sea , the former of which was designated a UNESCO World Heritage Site in 2004 .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : developer .', 'success_rate': 0.8835227272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : given name .', 'success_rate': 0.8260869565217391, 'errors': {'', "('Irish Episcopal Church', 'given name', '', 'The next year he was appointed a chaplain and the following year he became chaplain of the Irish Episcopal Church in Boston , Massachusetts , serving in both a commission and the United States Army .')", "('Thomas John W. Blofeld', 'given name', '', 'In the 17th century , Sir Thomas John W. Blofeld , Sr. and his wife were a British citizen , and were later part of the New York City and New York City s municipal nobility .')", 'not enough values to unpack (expected 2, got 1)', "('Washington Redskins', 'given name', '', 'The New England Patriots beat the Washington Redskins 27 to 6 in the Super Bowl at the Providence Corporation Center in Providence , Rhode Island .')", "('Taunton', 'given name', '', 'She is from the village of Taunton in the Hertfordshire district of York , England .')", "('Cambridge College', 'given name', '', 'He was educated at Cambridge College , Cambridge and was a naturalist and scientist in the early 19th century in England .')", "('MP', 'given name', '', 'He is best known in the West Midlands as a Liberal Democrat politician , who was appointed MP for Wiltshire in the 1997 general election , and was the Member for South Derbyshire in the 1997 general election .')"}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : participant in .', 'success_rate': 0.9136904761904762, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 429, 'raw': 544}
{'target': 600, 'success': 453, 'raw': 576}
{'target': 600, 'success': 479, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 563, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 615, 'raw': 768}
{'prompt': 'Relation : work location .', 'success_rate': 0.80078125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9241071428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : country of citizenship . Context : Alexander Cottler ( born 8 September 1974 , Glamorgan , County Down , England ) is a former Welsh cricket er . Head Entity : Alexander Cottler , Tail Entity : Wales .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.9255952380952381, 'errors': {'', 'too many values to unpack (expected 2)', "('I', 'country of citizenship', '', 'He attended the University of Oxford and at the University of Glasgow ( which I was a member of as well as a professor of history ) , where he was a fellow of the Royal Society .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : creator .', 'success_rate': 0.9017857142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : employer .', 'success_rate': 0.8877840909090909, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8220108695652174, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 266, 'raw': 352}
{'target': 600, 'success': 288, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 361, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 464, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 510, 'raw': 672}
{'target': 600, 'success': 532, 'raw': 704}
{'target': 600, 'success': 556, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 607, 'raw': 800}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.75875, 'errors': {'', "('', 'languages spoken, written or signed', 'English', 'The first bilingual edition , in English , was published in 1885 but only one of twelve original languages was also published.')", "('Mandarin Chinese', 'languages spoken, written or signed', '', 'At the end of the 1940s , she was educated at the Institute of Advanced Study and became the first postdoc teaching Mandarin Chinese in the United States .')", "('Malaysia', 'languages spoken, written or signed', '', 'The language is spoken to almost 1,400 people in Malaysia , including its 2,000 speakers ( of Malay and Mandarin ) .')", 'not enough values to unpack (expected 2, got 1)', "('Republic of Croatia', 'languages spoken, written or signed', '', 'According to the census , the municipality was in the territory of Vojvodina from 1939 to 1950 and the municipality was also a part of the Republic of Croatia from 1951 to 1955 .')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8806818181818182, 'errors': {'', "('She Will Remember', 'lyrics by', '', 'In 1975 , she released the second single in the American rock bands The Blue Sky and She Will Remember .')"}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9453125, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.9390625, 'errors': {''}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8059895833333334, 'errors': {'', "('Royal Academy of Music', 'occupation', '', 'Andrew Clements of Royal College College , London , as well as John Mayall of the Royal Academy of Music at Oxford University , were all pupils of composers of the era .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.946875, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8764204545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 533, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.8693181818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('shares', 'shares border with', '', 'Other shares traded at $ 576 .')"}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 616, 'raw': 640}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8707386363636364, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/1_ext.jsonl'}}
estimate vocab size: 14079
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14179, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.41it/s]Extractor Estimating: 2it [00:01,  1.40it/s]Extractor Estimating: 3it [00:02,  1.42it/s]Extractor Estimating: 4it [00:02,  1.51it/s]Extractor Estimating: 5it [00:03,  1.50it/s]Extractor Estimating: 6it [00:04,  1.52it/s]Extractor Estimating: 7it [00:04,  1.55it/s]Extractor Estimating: 8it [00:05,  1.54it/s]Extractor Estimating: 9it [00:05,  1.59it/s]Extractor Estimating: 10it [00:06,  1.62it/s]Extractor Estimating: 11it [00:07,  1.61it/s]Extractor Estimating: 12it [00:07,  1.61it/s]Extractor Estimating: 13it [00:08,  1.61it/s]Extractor Estimating: 14it [00:08,  1.62it/s]Extractor Estimating: 15it [00:09,  1.59it/s]Extractor Estimating: 16it [00:10,  1.60it/s]Extractor Estimating: 17it [00:10,  1.55it/s]Extractor Estimating: 18it [00:11,  1.53it/s]Extractor Estimating: 19it [00:12,  1.47it/s]Extractor Estimating: 20it [00:12,  1.50it/s]Extractor Estimating: 21it [00:13,  1.55it/s]Extractor Estimating: 22it [00:14,  1.56it/s]Extractor Estimating: 23it [00:14,  1.53it/s]Extractor Estimating: 24it [00:15,  1.56it/s]Extractor Estimating: 25it [00:16,  1.58it/s]Extractor Estimating: 26it [00:16,  1.59it/s]Extractor Estimating: 27it [00:17,  1.66it/s]Extractor Estimating: 28it [00:17,  1.68it/s]Extractor Estimating: 29it [00:18,  1.71it/s]Extractor Estimating: 30it [00:18,  1.71it/s]Extractor Estimating: 31it [00:19,  1.74it/s]Extractor Estimating: 32it [00:20,  1.75it/s]Extractor Estimating: 33it [00:20,  1.72it/s]Extractor Estimating: 34it [00:21,  1.73it/s]Extractor Estimating: 35it [00:21,  1.70it/s]Extractor Estimating: 36it [00:22,  1.71it/s]Extractor Estimating: 37it [00:23,  1.69it/s]Extractor Estimating: 38it [00:23,  1.69it/s]Extractor Estimating: 39it [00:24,  1.67it/s]Extractor Estimating: 40it [00:24,  1.62it/s]Extractor Estimating: 41it [00:25,  1.63it/s]Extractor Estimating: 42it [00:26,  1.66it/s]Extractor Estimating: 43it [00:26,  1.65it/s]Extractor Estimating: 44it [00:27,  1.70it/s]Extractor Estimating: 45it [00:27,  1.69it/s]Extractor Estimating: 46it [00:28,  1.70it/s]Extractor Estimating: 47it [00:29,  1.71it/s]Extractor Estimating: 48it [00:29,  1.71it/s]Extractor Estimating: 49it [00:30,  1.67it/s]Extractor Estimating: 50it [00:30,  1.64it/s]Extractor Estimating: 51it [00:31,  1.55it/s]Extractor Estimating: 52it [00:32,  1.55it/s]Extractor Estimating: 53it [00:32,  1.54it/s]Extractor Estimating: 54it [00:33,  1.52it/s]Extractor Estimating: 55it [00:34,  1.51it/s]Extractor Estimating: 56it [00:34,  1.53it/s]Extractor Estimating: 57it [00:35,  1.48it/s]Extractor Estimating: 58it [00:36,  1.47it/s]Extractor Estimating: 59it [00:36,  1.49it/s]Extractor Estimating: 60it [00:37,  1.54it/s]Extractor Estimating: 61it [00:38,  1.54it/s]Extractor Estimating: 62it [00:38,  1.56it/s]Extractor Estimating: 63it [00:39,  1.53it/s]Extractor Estimating: 64it [00:40,  1.51it/s]Extractor Estimating: 65it [00:40,  1.50it/s]Extractor Estimating: 66it [00:41,  1.53it/s]Extractor Estimating: 67it [00:42,  1.55it/s]Extractor Estimating: 68it [00:42,  1.51it/s]Extractor Estimating: 69it [00:43,  1.51it/s]Extractor Estimating: 70it [00:44,  1.17it/s]Extractor Estimating: 71it [00:45,  1.27it/s]Extractor Estimating: 72it [00:46,  1.37it/s]Extractor Estimating: 73it [00:46,  1.30it/s]Extractor Estimating: 74it [00:47,  1.36it/s]Extractor Estimating: 75it [00:48,  1.36it/s]Extractor Estimating: 76it [00:48,  1.45it/s]Extractor Estimating: 77it [00:49,  1.53it/s]Extractor Estimating: 78it [00:49,  1.63it/s]Extractor Estimating: 79it [00:50,  1.67it/s]Extractor Estimating: 80it [00:51,  1.68it/s]Extractor Estimating: 81it [00:51,  1.67it/s]Extractor Estimating: 82it [00:52,  1.65it/s]Extractor Estimating: 83it [00:52,  1.67it/s]Extractor Estimating: 84it [00:53,  1.70it/s]Extractor Estimating: 85it [00:54,  1.68it/s]Extractor Estimating: 86it [00:54,  1.68it/s]Extractor Estimating: 87it [00:55,  1.66it/s]Extractor Estimating: 88it [00:55,  1.69it/s]Extractor Estimating: 89it [00:56,  1.69it/s]Extractor Estimating: 90it [00:57,  1.71it/s]Extractor Estimating: 91it [00:57,  1.71it/s]Extractor Estimating: 92it [00:58,  1.77it/s]Extractor Estimating: 93it [00:58,  1.75it/s]Extractor Estimating: 94it [00:59,  1.71it/s]Extractor Estimating: 95it [00:59,  1.68it/s]Extractor Estimating: 96it [01:00,  1.74it/s]Extractor Estimating: 97it [01:01,  1.69it/s]Extractor Estimating: 98it [01:01,  1.71it/s]Extractor Estimating: 99it [01:02,  1.65it/s]Extractor Estimating: 100it [01:03,  1.58it/s]Extractor Estimating: 101it [01:03,  1.57it/s]Extractor Estimating: 102it [01:04,  1.56it/s]Extractor Estimating: 103it [01:04,  1.57it/s]Extractor Estimating: 104it [01:05,  1.61it/s]Extractor Estimating: 105it [01:06,  1.58it/s]Extractor Estimating: 106it [01:06,  1.57it/s]Extractor Estimating: 107it [01:07,  1.58it/s]Extractor Estimating: 108it [01:08,  1.59it/s]Extractor Estimating: 109it [01:08,  1.53it/s]Extractor Estimating: 110it [01:09,  1.55it/s]Extractor Estimating: 111it [01:10,  1.55it/s]Extractor Estimating: 112it [01:10,  1.56it/s]Extractor Estimating: 113it [01:11,  1.57it/s]Extractor Estimating: 114it [01:12,  1.54it/s]Extractor Estimating: 115it [01:12,  1.54it/s]Extractor Estimating: 116it [01:13,  1.55it/s]Extractor Estimating: 117it [01:13,  1.57it/s]Extractor Estimating: 118it [01:14,  1.58it/s]Extractor Estimating: 119it [01:15,  1.57it/s]Extractor Estimating: 120it [01:15,  1.55it/s]Extractor Estimating: 121it [01:16,  1.58it/s]Extractor Estimating: 122it [01:17,  1.55it/s]Extractor Estimating: 123it [01:17,  1.57it/s]Extractor Estimating: 124it [01:18,  1.57it/s]Extractor Estimating: 125it [01:18,  1.62it/s]Extractor Estimating: 126it [01:19,  1.61it/s]Extractor Estimating: 127it [01:20,  1.61it/s]Extractor Estimating: 128it [01:20,  1.58it/s]Extractor Estimating: 129it [01:21,  1.58it/s]Extractor Estimating: 130it [01:22,  1.57it/s]Extractor Estimating: 131it [01:22,  1.60it/s]Extractor Estimating: 132it [01:23,  1.65it/s]Extractor Estimating: 133it [01:23,  1.60it/s]Extractor Estimating: 134it [01:24,  1.62it/s]Extractor Estimating: 135it [01:25,  1.60it/s]Extractor Estimating: 136it [01:25,  1.58it/s]Extractor Estimating: 137it [01:26,  1.58it/s]Extractor Estimating: 138it [01:27,  1.57it/s]Extractor Estimating: 139it [01:27,  1.56it/s]Extractor Estimating: 140it [01:28,  1.59it/s]Extractor Estimating: 141it [01:29,  1.60it/s]Extractor Estimating: 142it [01:29,  1.52it/s]Extractor Estimating: 143it [01:30,  1.55it/s]Extractor Estimating: 144it [01:31,  1.56it/s]Extractor Estimating: 145it [01:31,  1.53it/s]Extractor Estimating: 146it [01:32,  1.54it/s]Extractor Estimating: 147it [01:33,  1.49it/s]Extractor Estimating: 148it [01:33,  1.52it/s]Extractor Estimating: 149it [01:34,  1.55it/s]Extractor Estimating: 150it [01:36,  1.02s/it]Extractor Estimating: 151it [01:36,  1.10it/s]Extractor Estimating: 152it [01:37,  1.20it/s]Extractor Estimating: 153it [01:38,  1.31it/s]Extractor Estimating: 154it [01:38,  1.38it/s]Extractor Estimating: 155it [01:39,  1.42it/s]Extractor Estimating: 156it [01:40,  1.44it/s]Extractor Estimating: 157it [01:40,  1.49it/s]Extractor Estimating: 158it [01:41,  1.53it/s]Extractor Estimating: 159it [01:41,  1.54it/s]Extractor Estimating: 160it [01:42,  1.51it/s]Extractor Estimating: 161it [01:43,  1.54it/s]Extractor Estimating: 162it [01:43,  1.47it/s]Extractor Estimating: 163it [01:44,  1.54it/s]Extractor Estimating: 164it [01:45,  1.58it/s]Extractor Estimating: 165it [01:45,  1.58it/s]Extractor Estimating: 166it [01:46,  1.43it/s]Extractor Estimating: 167it [01:47,  1.48it/s]Extractor Estimating: 168it [01:47,  1.49it/s]Extractor Estimating: 169it [01:48,  1.46it/s]Extractor Estimating: 170it [01:49,  1.48it/s]Extractor Estimating: 171it [01:49,  1.49it/s]Extractor Estimating: 172it [01:50,  1.50it/s]Extractor Estimating: 173it [01:51,  1.49it/s]Extractor Estimating: 174it [01:51,  1.51it/s]Extractor Estimating: 175it [01:52,  1.48it/s]Extractor Estimating: 176it [01:53,  1.54it/s]Extractor Estimating: 177it [01:53,  1.59it/s]Extractor Estimating: 178it [01:54,  1.55it/s]Extractor Estimating: 179it [01:55,  1.57it/s]Extractor Estimating: 180it [01:55,  1.58it/s]Extractor Estimating: 181it [01:56,  1.56it/s]Extractor Estimating: 182it [01:57,  1.55it/s]Extractor Estimating: 183it [01:57,  1.55it/s]Extractor Estimating: 184it [01:58,  1.53it/s]Extractor Estimating: 185it [01:58,  1.55it/s]Extractor Estimating: 186it [01:59,  1.51it/s]Extractor Estimating: 187it [02:00,  1.54it/s]Extractor Estimating: 188it [02:00,  1.61it/s]Extractor Estimating: 189it [02:01,  1.60it/s]Extractor Estimating: 190it [02:02,  1.63it/s]Extractor Estimating: 191it [02:02,  1.59it/s]Extractor Estimating: 192it [02:03,  1.56it/s]Extractor Estimating: 193it [02:04,  1.58it/s]Extractor Estimating: 194it [02:04,  1.54it/s]Extractor Estimating: 195it [02:05,  1.54it/s]Extractor Estimating: 196it [02:05,  1.57it/s]Extractor Estimating: 197it [02:06,  1.59it/s]Extractor Estimating: 198it [02:07,  1.61it/s]Extractor Estimating: 199it [02:07,  1.65it/s]Extractor Estimating: 200it [02:08,  1.66it/s]Extractor Estimating: 201it [02:08,  1.64it/s]Extractor Estimating: 202it [02:09,  1.63it/s]Extractor Estimating: 203it [02:10,  1.60it/s]Extractor Estimating: 204it [02:10,  1.61it/s]Extractor Estimating: 205it [02:11,  1.64it/s]Extractor Estimating: 206it [02:12,  1.62it/s]Extractor Estimating: 207it [02:12,  1.59it/s]Extractor Estimating: 208it [02:13,  1.59it/s]Extractor Estimating: 209it [02:13,  1.60it/s]Extractor Estimating: 210it [02:14,  1.63it/s]Extractor Estimating: 211it [02:15,  1.59it/s]Extractor Estimating: 212it [02:15,  1.63it/s]Extractor Estimating: 213it [02:16,  1.65it/s]Extractor Estimating: 214it [02:17,  1.62it/s]Extractor Estimating: 215it [02:17,  1.61it/s]Extractor Estimating: 216it [02:18,  1.55it/s]Extractor Estimating: 217it [02:18,  1.60it/s]Extractor Estimating: 218it [02:19,  1.60it/s]Extractor Estimating: 219it [02:20,  1.62it/s]Extractor Estimating: 220it [02:20,  1.61it/s]Extractor Estimating: 221it [02:21,  1.58it/s]Extractor Estimating: 222it [02:22,  1.57it/s]Extractor Estimating: 223it [02:22,  1.61it/s]Extractor Estimating: 224it [02:23,  1.62it/s]Extractor Estimating: 225it [02:24,  1.55it/s]Extractor Estimating: 226it [02:24,  1.51it/s]Extractor Estimating: 227it [02:25,  1.51it/s]Extractor Estimating: 228it [02:26,  1.54it/s]Extractor Estimating: 229it [02:26,  1.54it/s]Extractor Estimating: 230it [02:27,  1.50it/s]Extractor Estimating: 231it [02:28,  1.52it/s]Extractor Estimating: 232it [02:28,  1.55it/s]Extractor Estimating: 233it [02:29,  1.56it/s]Extractor Estimating: 234it [02:29,  1.53it/s]Extractor Estimating: 235it [02:30,  1.55it/s]Extractor Estimating: 236it [02:31,  1.56it/s]Extractor Estimating: 237it [02:31,  1.54it/s]Extractor Estimating: 238it [02:32,  1.58it/s]Extractor Estimating: 239it [02:33,  1.56it/s]Extractor Estimating: 240it [02:33,  1.58it/s]Extractor Estimating: 241it [02:34,  1.59it/s]Extractor Estimating: 242it [02:34,  1.57it/s]Extractor Estimating: 243it [02:35,  1.64it/s]Extractor Estimating: 244it [02:36,  1.68it/s]Extractor Estimating: 245it [02:36,  1.63it/s]Extractor Estimating: 246it [02:37,  1.59it/s]Extractor Estimating: 247it [02:38,  1.56it/s]Extractor Estimating: 248it [02:38,  1.56it/s]Extractor Estimating: 249it [02:39,  1.51it/s]Extractor Estimating: 250it [02:40,  1.33it/s]Extractor Estimating: 251it [02:40,  1.44it/s]Extractor Estimating: 252it [02:41,  1.46it/s]Extractor Estimating: 253it [02:42,  1.54it/s]Extractor Estimating: 254it [02:42,  1.61it/s]Extractor Estimating: 255it [02:43,  1.62it/s]Extractor Estimating: 256it [02:43,  1.64it/s]Extractor Estimating: 257it [02:44,  1.70it/s]Extractor Estimating: 258it [02:45,  1.67it/s]Extractor Estimating: 259it [02:45,  1.62it/s]Extractor Estimating: 260it [02:46,  1.65it/s]Extractor Estimating: 261it [02:47,  1.60it/s]Extractor Estimating: 262it [02:47,  1.59it/s]Extractor Estimating: 263it [02:48,  1.55it/s]Extractor Estimating: 264it [02:48,  1.56it/s]Extractor Estimating: 265it [02:49,  1.62it/s]Extractor Estimating: 266it [02:50,  1.66it/s]Extractor Estimating: 267it [02:50,  1.67it/s]Extractor Estimating: 268it [02:51,  1.65it/s]Extractor Estimating: 269it [02:51,  1.62it/s]Extractor Estimating: 270it [02:52,  1.65it/s]Extractor Estimating: 271it [02:53,  1.67it/s]Extractor Estimating: 272it [02:53,  1.67it/s]Extractor Estimating: 273it [02:54,  1.69it/s]Extractor Estimating: 274it [02:54,  1.69it/s]Extractor Estimating: 275it [02:55,  1.73it/s]Extractor Estimating: 276it [02:56,  1.65it/s]Extractor Estimating: 277it [02:56,  1.61it/s]Extractor Estimating: 278it [02:57,  1.55it/s]Extractor Estimating: 279it [02:58,  1.56it/s]Extractor Estimating: 280it [02:58,  1.58it/s]Extractor Estimating: 281it [02:59,  1.59it/s]Extractor Estimating: 282it [02:59,  1.61it/s]Extractor Estimating: 283it [03:00,  1.59it/s]Extractor Estimating: 284it [03:01,  1.60it/s]Extractor Estimating: 285it [03:01,  1.59it/s]Extractor Estimating: 286it [03:02,  1.61it/s]Extractor Estimating: 287it [03:03,  1.59it/s]Extractor Estimating: 288it [03:03,  1.60it/s]Extractor Estimating: 289it [03:04,  1.61it/s]Extractor Estimating: 290it [03:05,  1.55it/s]Extractor Estimating: 291it [03:05,  1.52it/s]Extractor Estimating: 292it [03:06,  1.51it/s]Extractor Estimating: 293it [03:06,  1.54it/s]Extractor Estimating: 294it [03:07,  1.55it/s]Extractor Estimating: 295it [03:08,  1.55it/s]Extractor Estimating: 296it [03:08,  1.55it/s]Extractor Estimating: 297it [03:09,  1.53it/s]Extractor Estimating: 298it [03:10,  1.51it/s]Extractor Estimating: 299it [03:10,  1.51it/s]Extractor Estimating: 300it [03:11,  1.52it/s]Extractor Estimating: 301it [03:12,  1.58it/s]Extractor Estimating: 302it [03:12,  1.62it/s]Extractor Estimating: 303it [03:13,  1.60it/s]Extractor Estimating: 304it [03:13,  1.69it/s]Extractor Estimating: 305it [03:14,  1.74it/s]Extractor Estimating: 306it [03:14,  1.77it/s]Extractor Estimating: 307it [03:15,  1.87it/s]Extractor Estimating: 308it [03:16,  1.84it/s]Extractor Estimating: 309it [03:17,  1.31it/s]Extractor Estimating: 310it [03:17,  1.41it/s]Extractor Estimating: 311it [03:18,  1.53it/s]Extractor Estimating: 312it [03:18,  1.63it/s]Extractor Estimating: 313it [03:19,  1.66it/s]Extractor Estimating: 314it [03:19,  1.74it/s]Extractor Estimating: 315it [03:20,  1.76it/s]Extractor Estimating: 316it [03:21,  1.78it/s]Extractor Estimating: 317it [03:21,  1.73it/s]Extractor Estimating: 318it [03:22,  1.76it/s]Extractor Estimating: 319it [03:22,  1.78it/s]Extractor Estimating: 320it [03:23,  1.77it/s]Extractor Estimating: 321it [03:24,  1.67it/s]Extractor Estimating: 322it [03:24,  1.69it/s]Extractor Estimating: 323it [03:25,  1.74it/s]Extractor Estimating: 324it [03:25,  1.80it/s]Extractor Estimating: 325it [03:26,  1.76it/s]Extractor Estimating: 326it [03:26,  1.68it/s]Extractor Estimating: 327it [03:27,  1.67it/s]Extractor Estimating: 328it [03:28,  1.66it/s]Extractor Estimating: 329it [03:28,  1.66it/s]Extractor Estimating: 330it [03:29,  1.63it/s]Extractor Estimating: 331it [03:30,  1.60it/s]Extractor Estimating: 332it [03:30,  1.59it/s]Extractor Estimating: 333it [03:31,  1.61it/s]Extractor Estimating: 334it [03:31,  1.62it/s]Extractor Estimating: 335it [03:32,  1.60it/s]Extractor Estimating: 336it [03:33,  1.62it/s]Extractor Estimating: 337it [03:33,  1.66it/s]Extractor Estimating: 338it [03:34,  1.67it/s]Extractor Estimating: 339it [03:34,  1.67it/s]Extractor Estimating: 340it [03:35,  1.46it/s]Extractor Estimating: 341it [03:36,  1.49it/s]Extractor Estimating: 342it [03:37,  1.54it/s]Extractor Estimating: 343it [03:37,  1.56it/s]Extractor Estimating: 344it [03:38,  1.55it/s]Extractor Estimating: 345it [03:38,  1.60it/s]Extractor Estimating: 346it [03:39,  1.60it/s]Extractor Estimating: 347it [03:40,  1.62it/s]Extractor Estimating: 348it [03:40,  1.63it/s]Extractor Estimating: 349it [03:41,  1.64it/s]Extractor Estimating: 350it [03:41,  1.59it/s]Extractor Estimating: 351it [03:42,  1.58it/s]Extractor Estimating: 352it [03:43,  1.50it/s]Extractor Estimating: 353it [03:44,  1.49it/s]Extractor Estimating: 354it [03:44,  1.50it/s]Extractor Estimating: 355it [03:45,  1.53it/s]Extractor Estimating: 356it [03:45,  1.52it/s]Extractor Estimating: 357it [03:46,  1.51it/s]Extractor Estimating: 358it [03:47,  1.51it/s]Extractor Estimating: 359it [03:47,  1.51it/s]Extractor Estimating: 360it [03:48,  1.49it/s]Extractor Estimating: 361it [03:49,  1.50it/s]Extractor Estimating: 362it [03:49,  1.51it/s]Extractor Estimating: 363it [03:50,  1.50it/s]Extractor Estimating: 364it [03:51,  1.48it/s]Extractor Estimating: 365it [03:51,  1.53it/s]Extractor Estimating: 366it [03:52,  1.51it/s]Extractor Estimating: 367it [03:53,  1.54it/s]Extractor Estimating: 368it [03:53,  1.56it/s]Extractor Estimating: 369it [03:54,  1.51it/s]Extractor Estimating: 370it [03:55,  1.49it/s]Extractor Estimating: 371it [03:55,  1.49it/s]Extractor Estimating: 372it [03:56,  1.51it/s]Extractor Estimating: 373it [03:57,  1.56it/s]Extractor Estimating: 374it [03:57,  1.55it/s]Extractor Estimating: 375it [03:58,  1.54it/s]Extractor Estimating: 376it [03:59,  1.60it/s]Extractor Estimating: 377it [03:59,  1.60it/s]Extractor Estimating: 378it [04:00,  1.55it/s]Extractor Estimating: 379it [04:01,  1.56it/s]Extractor Estimating: 380it [04:01,  1.55it/s]Extractor Estimating: 381it [04:02,  1.56it/s]Extractor Estimating: 382it [04:02,  1.56it/s]Extractor Estimating: 383it [04:03,  1.54it/s]Extractor Estimating: 384it [04:04,  1.57it/s]Extractor Estimating: 385it [04:04,  1.53it/s]Extractor Estimating: 386it [04:05,  1.54it/s]Extractor Estimating: 387it [04:06,  1.56it/s]Extractor Estimating: 388it [04:06,  1.58it/s]Extractor Estimating: 389it [04:07,  1.55it/s]Extractor Estimating: 390it [04:08,  1.56it/s]Extractor Estimating: 391it [04:08,  1.53it/s]Extractor Estimating: 392it [04:09,  1.52it/s]Extractor Estimating: 393it [04:10,  1.52it/s]Extractor Estimating: 394it [04:10,  1.53it/s]Extractor Estimating: 395it [04:11,  1.48it/s]Extractor Estimating: 396it [04:12,  1.49it/s]Extractor Estimating: 397it [04:12,  1.49it/s]Extractor Estimating: 398it [04:13,  1.50it/s]Extractor Estimating: 399it [04:14,  1.53it/s]Extractor Estimating: 400it [04:14,  1.58it/s]Extractor Estimating: 401it [04:15,  1.53it/s]Extractor Estimating: 402it [04:15,  1.57it/s]Extractor Estimating: 403it [04:16,  1.55it/s]Extractor Estimating: 404it [04:17,  1.55it/s]Extractor Estimating: 405it [04:17,  1.56it/s]Extractor Estimating: 406it [04:18,  1.53it/s]Extractor Estimating: 407it [04:19,  1.59it/s]Extractor Estimating: 408it [04:19,  1.56it/s]Extractor Estimating: 409it [04:20,  1.57it/s]Extractor Estimating: 410it [04:21,  1.55it/s]Extractor Estimating: 411it [04:21,  1.55it/s]Extractor Estimating: 412it [04:22,  1.52it/s]Extractor Estimating: 413it [04:23,  1.49it/s]Extractor Estimating: 414it [04:23,  1.51it/s]Extractor Estimating: 415it [04:24,  1.52it/s]Extractor Estimating: 416it [04:25,  1.49it/s]Extractor Estimating: 417it [04:25,  1.46it/s]Extractor Estimating: 418it [04:26,  1.52it/s]Extractor Estimating: 419it [04:27,  1.55it/s]Extractor Estimating: 420it [04:27,  1.51it/s]Extractor Estimating: 421it [04:28,  1.56it/s]Extractor Estimating: 422it [04:29,  1.56it/s]Extractor Estimating: 423it [04:29,  1.57it/s]Extractor Estimating: 424it [04:30,  1.59it/s]Extractor Estimating: 425it [04:30,  1.57it/s]Extractor Estimating: 426it [04:31,  1.52it/s]Extractor Estimating: 427it [04:32,  1.40it/s]Extractor Estimating: 428it [04:33,  1.45it/s]Extractor Estimating: 429it [04:33,  1.55it/s]Extractor Estimating: 430it [04:34,  1.55it/s]Extractor Estimating: 431it [04:34,  1.58it/s]Extractor Estimating: 432it [04:35,  1.63it/s]Extractor Estimating: 433it [04:36,  1.61it/s]Extractor Estimating: 434it [04:36,  1.63it/s]Extractor Estimating: 435it [04:37,  1.66it/s]Extractor Estimating: 436it [04:37,  1.71it/s]Extractor Estimating: 437it [04:38,  1.70it/s]Extractor Estimating: 438it [04:39,  1.69it/s]Extractor Estimating: 439it [04:39,  1.65it/s]Extractor Estimating: 440it [04:40,  1.62it/s]Extractor Estimating: 441it [04:40,  1.63it/s]Extractor Estimating: 442it [04:41,  1.68it/s]Extractor Estimating: 443it [04:42,  1.69it/s]Extractor Estimating: 444it [04:42,  1.70it/s]Extractor Estimating: 445it [04:43,  1.68it/s]Extractor Estimating: 446it [04:43,  1.69it/s]Extractor Estimating: 447it [04:44,  1.70it/s]Extractor Estimating: 448it [04:44,  1.74it/s]Extractor Estimating: 449it [04:45,  1.75it/s]Extractor Estimating: 450it [04:46,  1.68it/s]Extractor Estimating: 451it [04:46,  1.64it/s]Extractor Estimating: 452it [04:47,  1.60it/s]Extractor Estimating: 453it [04:48,  1.55it/s]Extractor Estimating: 454it [04:48,  1.52it/s]Extractor Estimating: 455it [04:49,  1.54it/s]Extractor Estimating: 456it [04:50,  1.52it/s]Extractor Estimating: 457it [04:50,  1.53it/s]Extractor Estimating: 458it [04:51,  1.55it/s]Extractor Estimating: 459it [04:52,  1.57it/s]Extractor Estimating: 460it [04:52,  1.57it/s]Extractor Estimating: 461it [04:53,  1.58it/s]Extractor Estimating: 462it [04:53,  1.55it/s]Extractor Estimating: 463it [04:54,  1.55it/s]Extractor Estimating: 464it [04:55,  1.53it/s]Extractor Estimating: 465it [04:55,  1.55it/s]Extractor Estimating: 466it [04:56,  1.52it/s]Extractor Estimating: 467it [04:57,  1.49it/s]Extractor Estimating: 468it [04:57,  1.53it/s]Extractor Estimating: 469it [04:58,  1.55it/s]Extractor Estimating: 470it [04:59,  1.57it/s]Extractor Estimating: 471it [04:59,  1.55it/s]Extractor Estimating: 472it [05:00,  1.54it/s]Extractor Estimating: 473it [05:01,  1.53it/s]Extractor Estimating: 474it [05:01,  1.53it/s]Extractor Estimating: 475it [05:02,  1.55it/s]Extractor Estimating: 476it [05:03,  1.50it/s]Extractor Estimating: 477it [05:03,  1.51it/s]Extractor Estimating: 478it [05:04,  1.47it/s]Extractor Estimating: 479it [05:05,  1.50it/s]Extractor Estimating: 480it [05:05,  1.54it/s]Extractor Estimating: 481it [05:06,  1.51it/s]Extractor Estimating: 482it [05:07,  1.53it/s]Extractor Estimating: 483it [05:07,  1.56it/s]Extractor Estimating: 484it [05:08,  1.59it/s]Extractor Estimating: 485it [05:08,  1.60it/s]Extractor Estimating: 486it [05:09,  1.64it/s]Extractor Estimating: 487it [05:10,  1.61it/s]Extractor Estimating: 488it [05:10,  1.57it/s]Extractor Estimating: 489it [05:11,  1.54it/s]Extractor Estimating: 490it [05:12,  1.56it/s]Extractor Estimating: 491it [05:12,  1.52it/s]Extractor Estimating: 492it [05:13,  1.52it/s]Extractor Estimating: 493it [05:14,  1.50it/s]Extractor Estimating: 494it [05:14,  1.53it/s]Extractor Estimating: 495it [05:15,  1.57it/s]Extractor Estimating: 496it [05:15,  1.58it/s]Extractor Estimating: 497it [05:16,  1.58it/s]Extractor Estimating: 498it [05:17,  1.60it/s]Extractor Estimating: 499it [05:17,  1.54it/s]Extractor Estimating: 500it [05:18,  1.88it/s]Extractor Estimating: 500it [05:18,  1.57it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 9954 mean pseudo reward: 0.9323219928069816
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_1/dev.jsonl'}
train vocab size: 27940
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28040, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=28040, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.334, loss:1124.6353
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.092, loss:1075.5971
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.063, loss:1062.4854
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.065, loss:1010.3775
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 85, avg_time 1.065, loss:1000.0920
>> valid entity prec:0.6235, rec:0.5268, f1:0.5711
>> valid relation prec:0.5382, rec:0.0805, f1:0.1400
>> valid relation with NER prec:0.5382, rec:0.0805, f1:0.1400
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 185, avg_time 2.851, loss:969.0501
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 285, avg_time 1.073, loss:1016.4887
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 385, avg_time 1.061, loss:1052.5830
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 70, avg_time 1.072, loss:998.4811
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 170, avg_time 1.070, loss:979.4846
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5756, rec:0.6323, f1:0.6026
>> valid relation prec:0.4545, rec:0.0967, f1:0.1595
>> valid relation with NER prec:0.4545, rec:0.0967, f1:0.1595
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 270, avg_time 2.858, loss:982.9343
g_step 1200, step 370, avg_time 1.070, loss:941.2754
g_step 1300, step 55, avg_time 1.058, loss:924.1421
g_step 1400, step 155, avg_time 1.068, loss:898.3446
g_step 1500, step 255, avg_time 1.061, loss:919.5378
>> valid entity prec:0.5706, rec:0.5608, f1:0.5656
>> valid relation prec:0.4106, rec:0.0740, f1:0.1254
>> valid relation with NER prec:0.4106, rec:0.0740, f1:0.1254
g_step 1600, step 355, avg_time 2.853, loss:883.9074
g_step 1700, step 40, avg_time 1.067, loss:882.4807
g_step 1800, step 140, avg_time 1.072, loss:867.3522
g_step 1900, step 240, avg_time 1.061, loss:875.7041
g_step 2000, step 340, avg_time 1.079, loss:871.0576
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5054, rec:0.4780, f1:0.4913
>> valid relation prec:0.3940, rec:0.0599, f1:0.1040
>> valid relation with NER prec:0.3940, rec:0.0599, f1:0.1040
g_step 2100, step 25, avg_time 2.845, loss:816.6261
g_step 2200, step 125, avg_time 1.069, loss:819.8622
g_step 2300, step 225, avg_time 1.052, loss:812.1767
g_step 2400, step 325, avg_time 1.072, loss:785.6698
g_step 2500, step 10, avg_time 1.066, loss:807.5434
>> valid entity prec:0.6050, rec:0.4756, f1:0.5326
>> valid relation prec:0.4104, rec:0.0605, f1:0.1055
>> valid relation with NER prec:0.4104, rec:0.0605, f1:0.1055
g_step 2600, step 110, avg_time 2.839, loss:757.3004
g_step 2700, step 210, avg_time 1.060, loss:799.8996
g_step 2800, step 310, avg_time 1.067, loss:806.5160
g_step 2900, step 410, avg_time 1.076, loss:803.9111
g_step 3000, step 95, avg_time 1.059, loss:703.7706
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5652, rec:0.5585, f1:0.5618
>> valid relation prec:0.2616, rec:0.0703, f1:0.1108
>> valid relation with NER prec:0.2616, rec:0.0703, f1:0.1108
g_step 3100, step 195, avg_time 2.844, loss:726.6828
g_step 3200, step 295, avg_time 1.064, loss:800.5869
g_step 3300, step 395, avg_time 1.066, loss:776.9226
g_step 3400, step 80, avg_time 1.059, loss:729.6907
g_step 3500, step 180, avg_time 1.073, loss:723.1739
>> valid entity prec:0.5540, rec:0.4924, f1:0.5214
>> valid relation prec:0.3596, rec:0.0578, f1:0.0996
>> valid relation with NER prec:0.3596, rec:0.0578, f1:0.0996
g_step 3600, step 280, avg_time 2.824, loss:735.7752
g_step 3700, step 380, avg_time 1.067, loss:744.3426
g_step 3800, step 65, avg_time 1.067, loss:681.7160
g_step 3900, step 165, avg_time 1.072, loss:673.5863
g_step 4000, step 265, avg_time 1.060, loss:708.3223
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5549, rec:0.5662, f1:0.5605
>> valid relation prec:0.2719, rec:0.0763, f1:0.1192
>> valid relation with NER prec:0.2719, rec:0.0763, f1:0.1192
g_step 4100, step 365, avg_time 2.843, loss:723.7201
g_step 4200, step 50, avg_time 1.064, loss:683.0892
g_step 4300, step 150, avg_time 1.054, loss:668.9248
g_step 4400, step 250, avg_time 1.071, loss:659.6322
g_step 4500, step 350, avg_time 1.076, loss:674.7795
>> valid entity prec:0.5380, rec:0.6149, f1:0.5739
>> valid relation prec:0.3233, rec:0.0717, f1:0.1174
>> valid relation with NER prec:0.3233, rec:0.0717, f1:0.1174
g_step 4600, step 35, avg_time 2.873, loss:719.7384
g_step 4700, step 135, avg_time 1.077, loss:653.4906
g_step 4800, step 235, avg_time 1.072, loss:657.1682
g_step 4900, step 335, avg_time 1.056, loss:649.8753
g_step 5000, step 20, avg_time 1.061, loss:640.1856
learning rate was adjusted to 0.0008
>> valid entity prec:0.5475, rec:0.5173, f1:0.5320
>> valid relation prec:0.3147, rec:0.0622, f1:0.1038
>> valid relation with NER prec:0.3147, rec:0.0622, f1:0.1038
g_step 5100, step 120, avg_time 2.855, loss:631.2849
g_step 5200, step 220, avg_time 1.069, loss:629.0062
g_step 5300, step 320, avg_time 1.062, loss:645.5813
g_step 5400, step 5, avg_time 1.064, loss:614.2776
g_step 5500, step 105, avg_time 1.063, loss:597.6077
>> valid entity prec:0.5219, rec:0.4669, f1:0.4929
>> valid relation prec:0.3434, rec:0.0659, f1:0.1106
>> valid relation with NER prec:0.3434, rec:0.0659, f1:0.1106
g_step 5600, step 205, avg_time 2.857, loss:614.8606
g_step 5700, step 305, avg_time 1.064, loss:631.8158
g_step 5800, step 405, avg_time 1.058, loss:621.2627
g_step 5900, step 90, avg_time 1.058, loss:590.1470
g_step 6000, step 190, avg_time 1.074, loss:592.3459
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5402, rec:0.4193, f1:0.4721
>> valid relation prec:0.1990, rec:0.0347, f1:0.0591
>> valid relation with NER prec:0.1990, rec:0.0347, f1:0.0591
g_step 6100, step 290, avg_time 2.848, loss:592.8153
g_step 6200, step 390, avg_time 1.065, loss:596.0482
g_step 6300, step 75, avg_time 1.056, loss:539.7716
g_step 6400, step 175, avg_time 1.065, loss:570.2970
g_step 6500, step 275, avg_time 1.070, loss:586.2401
>> valid entity prec:0.5227, rec:0.4791, f1:0.5000
>> valid relation prec:0.2627, rec:0.0526, f1:0.0877
>> valid relation with NER prec:0.2627, rec:0.0526, f1:0.0877
g_step 6600, step 375, avg_time 2.857, loss:583.0667
g_step 6700, step 60, avg_time 1.064, loss:576.8493
g_step 6800, step 160, avg_time 1.068, loss:540.7121
g_step 6900, step 260, avg_time 1.066, loss:570.4551
g_step 7000, step 360, avg_time 1.062, loss:554.9044
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5061, rec:0.4609, f1:0.4825
>> valid relation prec:0.1990, rec:0.0418, f1:0.0691
>> valid relation with NER prec:0.1990, rec:0.0418, f1:0.0691
g_step 7100, step 45, avg_time 2.840, loss:545.2192
g_step 7200, step 145, avg_time 1.054, loss:510.6447
g_step 7300, step 245, avg_time 1.060, loss:522.0513
g_step 7400, step 345, avg_time 1.060, loss:539.6652
g_step 7500, step 30, avg_time 1.059, loss:555.8782
>> valid entity prec:0.5233, rec:0.5036, f1:0.5133
>> valid relation prec:0.2271, rec:0.0630, f1:0.0986
>> valid relation with NER prec:0.2271, rec:0.0630, f1:0.0986
g_step 7600, step 130, avg_time 2.849, loss:521.4970
g_step 7700, step 230, avg_time 1.084, loss:516.2941
g_step 7800, step 330, avg_time 1.052, loss:517.3406
g_step 7900, step 15, avg_time 1.063, loss:538.9466
g_step 8000, step 115, avg_time 1.071, loss:495.2273
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.4991, rec:0.4995, f1:0.4993
>> valid relation prec:0.2405, rec:0.0447, f1:0.0754
>> valid relation with NER prec:0.2405, rec:0.0447, f1:0.0754
g_step 8100, step 215, avg_time 2.816, loss:496.4380
g_step 8200, step 315, avg_time 1.059, loss:488.9278
g_step 8300, step 415, avg_time 1.066, loss:518.9562
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 04:52:49 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 04:52:49 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_04-52-49_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 04:52:50 - WARNING - datasets.builder -   Using custom data configuration default-f25b50d805d0fe5c
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-f25b50d805d0fe5c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 04:52:50,833 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:52:50,834 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 04:52:50,835 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:52:50,836 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 04:52:50,843 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:52:50,846 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:52:50,846 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:52:50,846 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:52:50,846 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:52:50,846 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:52:50,846 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 04:52:50,999 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 04:52:54,117 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 04:52:54,118 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-f25b50d805d0fe5c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/29/2023 04:52:54 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14fecf36b3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  3.01ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.86ba/s] 27%|██▋       | 3/11 [00:00<00:01,  4.21ba/s] 36%|███▋      | 4/11 [00:00<00:01,  4.36ba/s] 45%|████▌     | 5/11 [00:01<00:01,  4.46ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.52ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.56ba/s] 73%|███████▎  | 8/11 [00:01<00:00,  4.60ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.62ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.62ba/s]100%|██████████| 11/11 [00:02<00:00,  4.81ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.48ba/s] 40%|████      | 2/5 [00:00<00:00,  3.37ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.81ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.04ba/s]100%|██████████| 5/5 [00:01<00:00,  4.45ba/s]100%|██████████| 5/5 [00:01<00:00,  3.98ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  8.63ba/s] 27%|██▋       | 3/11 [00:00<00:00, 10.27ba/s] 45%|████▌     | 5/11 [00:00<00:00, 10.61ba/s] 64%|██████▎   | 7/11 [00:00<00:00, 10.68ba/s] 82%|████████▏ | 9/11 [00:00<00:00, 10.80ba/s]100%|██████████| 11/11 [00:00<00:00, 12.85ba/s]100%|██████████| 11/11 [00:00<00:00, 11.60ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.41ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.38ba/s]100%|██████████| 5/5 [00:00<00:00, 11.02ba/s]100%|██████████| 5/5 [00:00<00:00, 10.71ba/s]
[INFO|trainer.py:414] 2023-08-29 04:52:59,485 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 04:52:59,505 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 04:52:59,505 >>   Num examples = 10097
[INFO|trainer.py:1149] 2023-08-29 04:52:59,505 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 04:52:59,505 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 04:52:59,505 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 04:52:59,505 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 04:52:59,505 >>   Total optimization steps = 790
  0%|          | 0/790 [00:00<?, ?it/s]  0%|          | 1/790 [00:00<04:05,  3.21it/s]  0%|          | 2/790 [00:00<03:54,  3.37it/s]  0%|          | 3/790 [00:00<03:50,  3.42it/s]  1%|          | 4/790 [00:01<03:48,  3.44it/s]  1%|          | 5/790 [00:01<03:47,  3.45it/s]  1%|          | 6/790 [00:01<03:46,  3.46it/s]  1%|          | 7/790 [00:02<03:45,  3.47it/s]  1%|          | 8/790 [00:02<03:45,  3.47it/s]  1%|          | 9/790 [00:02<03:44,  3.47it/s]  1%|▏         | 10/790 [00:02<03:44,  3.48it/s]  1%|▏         | 11/790 [00:03<03:44,  3.48it/s]  2%|▏         | 12/790 [00:03<03:43,  3.48it/s]  2%|▏         | 13/790 [00:03<03:43,  3.48it/s]  2%|▏         | 14/790 [00:04<03:43,  3.48it/s]  2%|▏         | 15/790 [00:04<03:43,  3.47it/s]  2%|▏         | 16/790 [00:04<03:42,  3.47it/s]  2%|▏         | 17/790 [00:04<03:42,  3.47it/s]  2%|▏         | 18/790 [00:05<03:42,  3.48it/s]  2%|▏         | 19/790 [00:05<03:41,  3.48it/s]  3%|▎         | 20/790 [00:05<03:41,  3.48it/s]  3%|▎         | 21/790 [00:06<03:41,  3.48it/s]  3%|▎         | 22/790 [00:06<03:40,  3.48it/s]  3%|▎         | 23/790 [00:06<03:40,  3.48it/s]  3%|▎         | 24/790 [00:06<03:40,  3.48it/s]  3%|▎         | 25/790 [00:07<03:40,  3.48it/s]  3%|▎         | 26/790 [00:07<03:40,  3.47it/s]  3%|▎         | 27/790 [00:07<03:39,  3.47it/s]  4%|▎         | 28/790 [00:08<03:39,  3.47it/s]  4%|▎         | 29/790 [00:08<03:38,  3.47it/s]  4%|▍         | 30/790 [00:08<03:39,  3.47it/s]  4%|▍         | 31/790 [00:08<03:38,  3.47it/s]  4%|▍         | 32/790 [00:09<03:38,  3.47it/s]  4%|▍         | 33/790 [00:09<03:38,  3.47it/s]  4%|▍         | 34/790 [00:09<03:37,  3.47it/s]  4%|▍         | 35/790 [00:10<03:37,  3.47it/s]  5%|▍         | 36/790 [00:10<03:37,  3.47it/s]  5%|▍         | 37/790 [00:10<03:36,  3.48it/s]  5%|▍         | 38/790 [00:10<03:36,  3.47it/s]  5%|▍         | 39/790 [00:11<03:36,  3.47it/s]  5%|▌         | 40/790 [00:11<03:35,  3.47it/s]  5%|▌         | 41/790 [00:11<03:35,  3.48it/s]  5%|▌         | 42/790 [00:12<03:35,  3.47it/s]  5%|▌         | 43/790 [00:12<03:34,  3.47it/s]  6%|▌         | 44/790 [00:12<03:34,  3.47it/s]  6%|▌         | 45/790 [00:12<03:34,  3.47it/s]  6%|▌         | 46/790 [00:13<03:34,  3.47it/s]  6%|▌         | 47/790 [00:13<03:33,  3.47it/s]  6%|▌         | 48/790 [00:13<03:34,  3.47it/s]  6%|▌         | 49/790 [00:14<03:33,  3.47it/s]  6%|▋         | 50/790 [00:14<03:33,  3.47it/s]  6%|▋         | 51/790 [00:14<03:32,  3.47it/s]  7%|▋         | 52/790 [00:14<03:32,  3.47it/s]  7%|▋         | 53/790 [00:15<03:32,  3.47it/s]  7%|▋         | 54/790 [00:15<03:32,  3.47it/s]  7%|▋         | 55/790 [00:15<03:32,  3.47it/s]  7%|▋         | 56/790 [00:16<03:31,  3.47it/s]  7%|▋         | 57/790 [00:16<03:31,  3.47it/s]  7%|▋         | 58/790 [00:16<03:31,  3.47it/s]  7%|▋         | 59/790 [00:17<03:30,  3.47it/s]  8%|▊         | 60/790 [00:17<03:30,  3.47it/s]  8%|▊         | 61/790 [00:17<03:30,  3.47it/s]  8%|▊         | 62/790 [00:17<03:29,  3.47it/s]  8%|▊         | 63/790 [00:18<03:29,  3.47it/s]  8%|▊         | 64/790 [00:18<03:29,  3.47it/s]  8%|▊         | 65/790 [00:18<03:28,  3.47it/s]  8%|▊         | 66/790 [00:19<03:29,  3.46it/s]  8%|▊         | 67/790 [00:19<03:28,  3.46it/s]  9%|▊         | 68/790 [00:19<03:28,  3.46it/s]  9%|▊         | 69/790 [00:19<03:28,  3.47it/s]  9%|▉         | 70/790 [00:20<03:27,  3.46it/s]  9%|▉         | 71/790 [00:20<03:27,  3.46it/s]  9%|▉         | 72/790 [00:20<03:27,  3.46it/s]  9%|▉         | 73/790 [00:21<03:26,  3.47it/s]  9%|▉         | 74/790 [00:21<03:26,  3.46it/s]  9%|▉         | 75/790 [00:21<03:26,  3.47it/s] 10%|▉         | 76/790 [00:21<03:25,  3.47it/s] 10%|▉         | 77/790 [00:22<03:25,  3.47it/s] 10%|▉         | 78/790 [00:22<03:25,  3.47it/s] 10%|█         | 79/790 [00:22<03:25,  3.47it/s] 10%|█         | 80/790 [00:23<03:24,  3.47it/s] 10%|█         | 81/790 [00:23<03:24,  3.46it/s] 10%|█         | 82/790 [00:23<03:24,  3.46it/s] 11%|█         | 83/790 [00:23<03:23,  3.47it/s] 11%|█         | 84/790 [00:24<03:24,  3.46it/s] 11%|█         | 85/790 [00:24<03:23,  3.46it/s] 11%|█         | 86/790 [00:24<03:23,  3.46it/s] 11%|█         | 87/790 [00:25<03:22,  3.46it/s] 11%|█         | 88/790 [00:25<03:22,  3.46it/s] 11%|█▏        | 89/790 [00:25<03:22,  3.46it/s] 11%|█▏        | 90/790 [00:25<03:22,  3.46it/s] 12%|█▏        | 91/790 [00:26<03:21,  3.46it/s] 12%|█▏        | 92/790 [00:26<03:21,  3.46it/s] 12%|█▏        | 93/790 [00:26<03:21,  3.47it/s] 12%|█▏        | 94/790 [00:27<03:20,  3.47it/s] 12%|█▏        | 95/790 [00:27<03:20,  3.47it/s] 12%|█▏        | 96/790 [00:27<03:20,  3.47it/s] 12%|█▏        | 97/790 [00:27<03:20,  3.46it/s] 12%|█▏        | 98/790 [00:28<03:19,  3.47it/s] 13%|█▎        | 99/790 [00:28<03:19,  3.47it/s] 13%|█▎        | 100/790 [00:28<03:19,  3.47it/s] 13%|█▎        | 101/790 [00:29<03:19,  3.46it/s] 13%|█▎        | 102/790 [00:29<03:19,  3.46it/s] 13%|█▎        | 103/790 [00:29<03:18,  3.46it/s] 13%|█▎        | 104/790 [00:30<03:18,  3.46it/s] 13%|█▎        | 105/790 [00:30<03:17,  3.46it/s] 13%|█▎        | 106/790 [00:30<03:17,  3.46it/s] 14%|█▎        | 107/790 [00:30<03:17,  3.46it/s] 14%|█▎        | 108/790 [00:31<03:16,  3.46it/s] 14%|█▍        | 109/790 [00:31<03:16,  3.46it/s] 14%|█▍        | 110/790 [00:31<03:16,  3.47it/s] 14%|█▍        | 111/790 [00:32<03:16,  3.46it/s] 14%|█▍        | 112/790 [00:32<03:15,  3.47it/s] 14%|█▍        | 113/790 [00:32<03:15,  3.46it/s] 14%|█▍        | 114/790 [00:32<03:15,  3.46it/s] 15%|█▍        | 115/790 [00:33<03:14,  3.46it/s] 15%|█▍        | 116/790 [00:33<03:14,  3.46it/s] 15%|█▍        | 117/790 [00:33<03:14,  3.46it/s] 15%|█▍        | 118/790 [00:34<03:14,  3.46it/s] 15%|█▌        | 119/790 [00:34<03:14,  3.44it/s] 15%|█▌        | 120/790 [00:34<03:14,  3.45it/s] 15%|█▌        | 121/790 [00:34<03:13,  3.45it/s] 15%|█▌        | 122/790 [00:35<03:13,  3.46it/s] 16%|█▌        | 123/790 [00:35<03:12,  3.46it/s] 16%|█▌        | 124/790 [00:35<03:12,  3.46it/s] 16%|█▌        | 125/790 [00:36<03:12,  3.46it/s] 16%|█▌        | 126/790 [00:36<03:11,  3.46it/s] 16%|█▌        | 127/790 [00:36<03:11,  3.46it/s] 16%|█▌        | 128/790 [00:36<03:11,  3.46it/s] 16%|█▋        | 129/790 [00:37<03:10,  3.46it/s] 16%|█▋        | 130/790 [00:37<03:10,  3.46it/s] 17%|█▋        | 131/790 [00:37<03:10,  3.46it/s] 17%|█▋        | 132/790 [00:38<03:10,  3.46it/s] 17%|█▋        | 133/790 [00:38<03:09,  3.46it/s] 17%|█▋        | 134/790 [00:38<03:09,  3.46it/s] 17%|█▋        | 135/790 [00:38<03:09,  3.46it/s] 17%|█▋        | 136/790 [00:39<03:08,  3.46it/s] 17%|█▋        | 137/790 [00:39<03:09,  3.45it/s] 17%|█▋        | 138/790 [00:39<03:08,  3.45it/s] 18%|█▊        | 139/790 [00:40<03:08,  3.46it/s] 18%|█▊        | 140/790 [00:40<03:07,  3.46it/s] 18%|█▊        | 141/790 [00:40<03:07,  3.46it/s] 18%|█▊        | 142/790 [00:40<03:07,  3.46it/s] 18%|█▊        | 143/790 [00:41<03:07,  3.46it/s] 18%|█▊        | 144/790 [00:41<03:06,  3.46it/s] 18%|█▊        | 145/790 [00:41<03:06,  3.46it/s] 18%|█▊        | 146/790 [00:42<03:06,  3.46it/s] 19%|█▊        | 147/790 [00:42<03:05,  3.46it/s] 19%|█▊        | 148/790 [00:42<03:05,  3.46it/s] 19%|█▉        | 149/790 [00:43<03:05,  3.46it/s] 19%|█▉        | 150/790 [00:43<03:04,  3.46it/s] 19%|█▉        | 151/790 [00:43<03:04,  3.46it/s] 19%|█▉        | 152/790 [00:43<03:04,  3.46it/s] 19%|█▉        | 153/790 [00:44<03:04,  3.46it/s] 19%|█▉        | 154/790 [00:44<03:04,  3.45it/s] 20%|█▉        | 155/790 [00:44<03:04,  3.45it/s] 20%|█▉        | 156/790 [00:45<03:03,  3.45it/s] 20%|█▉        | 157/790 [00:45<03:03,  3.45it/s] 20%|██        | 158/790 [00:45<02:52,  3.67it/s][INFO|trainer.py:2140] 2023-08-29 04:53:45,064 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:53:45,064 >>   Num examples = 4812
[INFO|trainer.py:2145] 2023-08-29 04:53:45,064 >>   Batch size = 8

  0%|          | 0/602 [00:00<?, ?it/s][A
  1%|          | 6/602 [00:00<00:10, 57.32it/s][A
  2%|▏         | 12/602 [00:00<00:11, 50.64it/s][A
  3%|▎         | 18/602 [00:00<00:11, 48.77it/s][A
  4%|▍         | 23/602 [00:00<00:12, 48.04it/s][A
  5%|▍         | 28/602 [00:00<00:12, 47.57it/s][A
  5%|▌         | 33/602 [00:00<00:12, 47.32it/s][A
  6%|▋         | 38/602 [00:00<00:12, 46.95it/s][A
  7%|▋         | 43/602 [00:00<00:11, 46.69it/s][A
  8%|▊         | 48/602 [00:01<00:11, 46.65it/s][A
  9%|▉         | 53/602 [00:01<00:11, 46.62it/s][A
 10%|▉         | 58/602 [00:01<00:11, 46.71it/s][A
 10%|█         | 63/602 [00:01<00:11, 46.78it/s][A
 11%|█▏        | 68/602 [00:01<00:11, 46.66it/s][A
 12%|█▏        | 73/602 [00:01<00:11, 46.78it/s][A
 13%|█▎        | 78/602 [00:01<00:11, 46.73it/s][A
 14%|█▍        | 83/602 [00:01<00:11, 46.67it/s][A
 15%|█▍        | 88/602 [00:01<00:11, 46.56it/s][A
 15%|█▌        | 93/602 [00:01<00:10, 46.50it/s][A
 16%|█▋        | 98/602 [00:02<00:10, 46.47it/s][A
 17%|█▋        | 103/602 [00:02<00:10, 46.60it/s][A
 18%|█▊        | 108/602 [00:02<00:10, 46.67it/s][A
 19%|█▉        | 113/602 [00:02<00:10, 46.66it/s][A
 20%|█▉        | 118/602 [00:02<00:10, 46.65it/s][A
 20%|██        | 123/602 [00:02<00:10, 46.67it/s][A
 21%|██▏       | 128/602 [00:02<00:10, 46.64it/s][A
 22%|██▏       | 133/602 [00:02<00:10, 46.68it/s][A
 23%|██▎       | 138/602 [00:02<00:09, 46.55it/s][A
 24%|██▍       | 143/602 [00:03<00:09, 46.53it/s][A
 25%|██▍       | 148/602 [00:03<00:09, 46.54it/s][A
 25%|██▌       | 153/602 [00:03<00:09, 46.60it/s][A
 26%|██▌       | 158/602 [00:03<00:09, 46.60it/s][A
 27%|██▋       | 163/602 [00:03<00:09, 46.64it/s][A
 28%|██▊       | 168/602 [00:03<00:09, 46.63it/s][A
 29%|██▊       | 173/602 [00:03<00:09, 46.65it/s][A
 30%|██▉       | 178/602 [00:03<00:09, 46.61it/s][A
 30%|███       | 183/602 [00:03<00:08, 46.63it/s][A
 31%|███       | 188/602 [00:04<00:08, 46.57it/s][A
 32%|███▏      | 193/602 [00:04<00:08, 46.52it/s][A
 33%|███▎      | 198/602 [00:04<00:08, 46.53it/s][A
 34%|███▎      | 203/602 [00:04<00:08, 46.60it/s][A
 35%|███▍      | 208/602 [00:04<00:08, 46.65it/s][A
 35%|███▌      | 213/602 [00:04<00:09, 42.84it/s][A
 36%|███▌      | 218/602 [00:04<00:08, 44.01it/s][A
 37%|███▋      | 223/602 [00:04<00:08, 44.79it/s][A
 38%|███▊      | 228/602 [00:04<00:08, 45.27it/s][A
 39%|███▊      | 233/602 [00:05<00:08, 45.72it/s][A
 40%|███▉      | 238/602 [00:05<00:07, 45.99it/s][A
 40%|████      | 243/602 [00:05<00:07, 46.15it/s][A
 41%|████      | 248/602 [00:05<00:07, 46.39it/s][A
 42%|████▏     | 253/602 [00:05<00:07, 46.28it/s][A
 43%|████▎     | 258/602 [00:05<00:07, 46.36it/s][A
 44%|████▎     | 263/602 [00:05<00:07, 46.50it/s][A
 45%|████▍     | 268/602 [00:05<00:07, 46.53it/s][A
 45%|████▌     | 273/602 [00:05<00:07, 46.55it/s][A
 46%|████▌     | 278/602 [00:05<00:06, 46.58it/s][A
 47%|████▋     | 283/602 [00:06<00:06, 46.67it/s][A
 48%|████▊     | 288/602 [00:06<00:06, 46.67it/s][A
 49%|████▊     | 293/602 [00:06<00:06, 46.59it/s][A
 50%|████▉     | 298/602 [00:06<00:06, 46.57it/s][A
 50%|█████     | 303/602 [00:06<00:06, 46.42it/s][A
 51%|█████     | 308/602 [00:06<00:06, 46.52it/s][A
 52%|█████▏    | 313/602 [00:06<00:06, 46.61it/s][A
 53%|█████▎    | 318/602 [00:06<00:06, 46.62it/s][A
 54%|█████▎    | 323/602 [00:06<00:05, 46.56it/s][A
 54%|█████▍    | 328/602 [00:07<00:05, 46.69it/s][A
 55%|█████▌    | 333/602 [00:07<00:05, 46.70it/s][A
 56%|█████▌    | 338/602 [00:07<00:05, 46.64it/s][A
 57%|█████▋    | 343/602 [00:07<00:05, 46.62it/s][A
 58%|█████▊    | 348/602 [00:07<00:05, 46.52it/s][A
 59%|█████▊    | 353/602 [00:07<00:05, 46.56it/s][A
 59%|█████▉    | 358/602 [00:07<00:05, 46.58it/s][A
 60%|██████    | 363/602 [00:07<00:05, 46.59it/s][A
 61%|██████    | 368/602 [00:07<00:05, 46.64it/s][A
 62%|██████▏   | 373/602 [00:08<00:04, 46.64it/s][A
 63%|██████▎   | 378/602 [00:08<00:04, 46.60it/s][A
 64%|██████▎   | 383/602 [00:08<00:04, 46.66it/s][A
 64%|██████▍   | 388/602 [00:08<00:04, 46.70it/s][A
 65%|██████▌   | 393/602 [00:08<00:04, 46.51it/s][A
 66%|██████▌   | 398/602 [00:08<00:04, 46.53it/s][A
 67%|██████▋   | 403/602 [00:08<00:04, 46.54it/s][A
 68%|██████▊   | 408/602 [00:08<00:04, 46.59it/s][A
 69%|██████▊   | 413/602 [00:08<00:04, 46.59it/s][A
 69%|██████▉   | 418/602 [00:08<00:03, 46.51it/s][A
 70%|███████   | 423/602 [00:09<00:03, 46.60it/s][A
 71%|███████   | 428/602 [00:09<00:03, 46.67it/s][A
 72%|███████▏  | 433/602 [00:09<00:03, 46.56it/s][A
 73%|███████▎  | 438/602 [00:09<00:03, 46.58it/s][A
 74%|███████▎  | 443/602 [00:09<00:03, 46.37it/s][A
 74%|███████▍  | 448/602 [00:09<00:03, 46.50it/s][A
 75%|███████▌  | 453/602 [00:09<00:03, 46.56it/s][A
 76%|███████▌  | 458/602 [00:09<00:03, 46.62it/s][A
 77%|███████▋  | 463/602 [00:09<00:02, 46.52it/s][A
 78%|███████▊  | 468/602 [00:10<00:02, 46.55it/s][A
 79%|███████▊  | 473/602 [00:10<00:02, 46.60it/s][A
 79%|███████▉  | 478/602 [00:10<00:02, 46.64it/s][A
 80%|████████  | 483/602 [00:10<00:02, 46.59it/s][A
 81%|████████  | 488/602 [00:10<00:02, 46.55it/s][A
 82%|████████▏ | 493/602 [00:10<00:02, 46.48it/s][A
 83%|████████▎ | 498/602 [00:10<00:02, 46.54it/s][A
 84%|████████▎ | 503/602 [00:10<00:02, 46.55it/s][A
 84%|████████▍ | 508/602 [00:10<00:02, 46.58it/s][A
 85%|████████▌ | 513/602 [00:11<00:01, 46.58it/s][A
 86%|████████▌ | 518/602 [00:11<00:01, 46.65it/s][A
 87%|████████▋ | 523/602 [00:11<00:01, 46.61it/s][A
 88%|████████▊ | 528/602 [00:11<00:01, 46.57it/s][A
 89%|████████▊ | 533/602 [00:11<00:01, 46.55it/s][A
 89%|████████▉ | 538/602 [00:11<00:01, 46.49it/s][A
 90%|█████████ | 543/602 [00:11<00:01, 46.54it/s][A
 91%|█████████ | 548/602 [00:11<00:01, 46.59it/s][A
 92%|█████████▏| 553/602 [00:11<00:01, 46.54it/s][A
 93%|█████████▎| 558/602 [00:11<00:00, 46.61it/s][A
 94%|█████████▎| 563/602 [00:12<00:00, 46.64it/s][A
 94%|█████████▍| 568/602 [00:12<00:00, 46.61it/s][A
 95%|█████████▌| 573/602 [00:12<00:00, 46.59it/s][A
 96%|█████████▌| 578/602 [00:12<00:00, 46.47it/s][A
 97%|█████████▋| 583/602 [00:12<00:00, 46.51it/s][A
 98%|█████████▊| 588/602 [00:12<00:00, 46.56it/s][A
 99%|█████████▊| 593/602 [00:12<00:00, 46.59it/s][A
 99%|█████████▉| 598/602 [00:12<00:00, 46.60it/s][A                                                 
                                                 [A 20%|██        | 158/790 [00:58<02:52,  3.67it/s]
100%|██████████| 602/602 [00:12<00:00, 46.60it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:53:58,034 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-158
[INFO|configuration_utils.py:351] 2023-08-29 04:53:58,049 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-158/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:54:00,319 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-158/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:54:00,335 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-158/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:54:00,350 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-158/special_tokens_map.json
 20%|██        | 159/790 [01:05<1:05:41,  6.25s/it] 20%|██        | 160/790 [01:06<46:51,  4.46s/it]   20%|██        | 161/790 [01:06<33:39,  3.21s/it] 21%|██        | 162/790 [01:06<24:25,  2.33s/it] 21%|██        | 163/790 [01:06<17:58,  1.72s/it] 21%|██        | 164/790 [01:07<13:28,  1.29s/it] 21%|██        | 165/790 [01:07<10:19,  1.01it/s] 21%|██        | 166/790 [01:07<08:06,  1.28it/s] 21%|██        | 167/790 [01:08<06:34,  1.58it/s] 21%|██▏       | 168/790 [01:08<05:29,  1.89it/s] 21%|██▏       | 169/790 [01:08<04:44,  2.18it/s] 22%|██▏       | 170/790 [01:08<04:12,  2.46it/s] 22%|██▏       | 171/790 [01:09<03:50,  2.69it/s] 22%|██▏       | 172/790 [01:09<03:34,  2.88it/s] 22%|██▏       | 173/790 [01:09<03:23,  3.03it/s] 22%|██▏       | 174/790 [01:10<03:15,  3.15it/s] 22%|██▏       | 175/790 [01:10<03:10,  3.23it/s] 22%|██▏       | 176/790 [01:10<03:06,  3.30it/s] 22%|██▏       | 177/790 [01:10<03:03,  3.34it/s] 23%|██▎       | 178/790 [01:11<03:01,  3.37it/s] 23%|██▎       | 179/790 [01:11<02:59,  3.40it/s] 23%|██▎       | 180/790 [01:11<02:58,  3.42it/s] 23%|██▎       | 181/790 [01:12<02:57,  3.43it/s] 23%|██▎       | 182/790 [01:12<02:57,  3.43it/s] 23%|██▎       | 183/790 [01:12<02:56,  3.44it/s] 23%|██▎       | 184/790 [01:12<02:56,  3.44it/s] 23%|██▎       | 185/790 [01:13<02:55,  3.45it/s] 24%|██▎       | 186/790 [01:13<02:55,  3.45it/s] 24%|██▎       | 187/790 [01:13<02:54,  3.45it/s] 24%|██▍       | 188/790 [01:14<02:54,  3.45it/s] 24%|██▍       | 189/790 [01:14<02:53,  3.45it/s] 24%|██▍       | 190/790 [01:14<02:53,  3.45it/s] 24%|██▍       | 191/790 [01:15<02:53,  3.45it/s] 24%|██▍       | 192/790 [01:15<02:53,  3.46it/s] 24%|██▍       | 193/790 [01:15<02:53,  3.45it/s] 25%|██▍       | 194/790 [01:15<02:52,  3.45it/s] 25%|██▍       | 195/790 [01:16<02:52,  3.45it/s] 25%|██▍       | 196/790 [01:16<02:52,  3.45it/s] 25%|██▍       | 197/790 [01:16<02:51,  3.45it/s] 25%|██▌       | 198/790 [01:17<02:51,  3.45it/s] 25%|██▌       | 199/790 [01:17<02:51,  3.46it/s] 25%|██▌       | 200/790 [01:17<02:50,  3.46it/s] 25%|██▌       | 201/790 [01:17<02:50,  3.46it/s] 26%|██▌       | 202/790 [01:18<02:50,  3.46it/s] 26%|██▌       | 203/790 [01:18<02:50,  3.45it/s] 26%|██▌       | 204/790 [01:18<02:50,  3.45it/s] 26%|██▌       | 205/790 [01:19<02:49,  3.45it/s] 26%|██▌       | 206/790 [01:19<02:49,  3.45it/s] 26%|██▌       | 207/790 [01:19<02:48,  3.45it/s] 26%|██▋       | 208/790 [01:19<02:48,  3.45it/s] 26%|██▋       | 209/790 [01:20<02:48,  3.45it/s] 27%|██▋       | 210/790 [01:20<02:47,  3.46it/s] 27%|██▋       | 211/790 [01:20<02:47,  3.45it/s] 27%|██▋       | 212/790 [01:21<02:47,  3.46it/s] 27%|██▋       | 213/790 [01:21<02:47,  3.45it/s] 27%|██▋       | 214/790 [01:21<02:46,  3.45it/s] 27%|██▋       | 215/790 [01:21<02:47,  3.44it/s] 27%|██▋       | 216/790 [01:22<02:46,  3.45it/s] 27%|██▋       | 217/790 [01:22<02:46,  3.45it/s] 28%|██▊       | 218/790 [01:22<02:45,  3.45it/s] 28%|██▊       | 219/790 [01:23<02:45,  3.45it/s] 28%|██▊       | 220/790 [01:23<02:45,  3.45it/s] 28%|██▊       | 221/790 [01:23<02:44,  3.46it/s] 28%|██▊       | 222/790 [01:23<02:44,  3.45it/s] 28%|██▊       | 223/790 [01:24<02:44,  3.46it/s] 28%|██▊       | 224/790 [01:24<02:43,  3.46it/s] 28%|██▊       | 225/790 [01:24<02:43,  3.45it/s] 29%|██▊       | 226/790 [01:25<02:44,  3.43it/s] 29%|██▊       | 227/790 [01:25<02:43,  3.44it/s] 29%|██▉       | 228/790 [01:25<02:43,  3.44it/s] 29%|██▉       | 229/790 [01:26<02:42,  3.44it/s] 29%|██▉       | 230/790 [01:26<02:42,  3.45it/s] 29%|██▉       | 231/790 [01:26<02:42,  3.45it/s] 29%|██▉       | 232/790 [01:26<02:41,  3.45it/s] 29%|██▉       | 233/790 [01:27<02:41,  3.45it/s] 30%|██▉       | 234/790 [01:27<02:41,  3.45it/s] 30%|██▉       | 235/790 [01:27<02:40,  3.45it/s] 30%|██▉       | 236/790 [01:28<02:40,  3.45it/s] 30%|███       | 237/790 [01:28<02:40,  3.45it/s] 30%|███       | 238/790 [01:28<02:39,  3.45it/s] 30%|███       | 239/790 [01:28<02:39,  3.45it/s] 30%|███       | 240/790 [01:29<02:39,  3.45it/s] 31%|███       | 241/790 [01:29<02:38,  3.45it/s] 31%|███       | 242/790 [01:29<02:38,  3.45it/s] 31%|███       | 243/790 [01:30<02:38,  3.45it/s] 31%|███       | 244/790 [01:30<02:38,  3.45it/s] 31%|███       | 245/790 [01:30<02:38,  3.44it/s] 31%|███       | 246/790 [01:30<02:38,  3.44it/s] 31%|███▏      | 247/790 [01:31<02:37,  3.44it/s] 31%|███▏      | 248/790 [01:31<02:37,  3.45it/s] 32%|███▏      | 249/790 [01:31<02:36,  3.45it/s] 32%|███▏      | 250/790 [01:32<02:36,  3.45it/s] 32%|███▏      | 251/790 [01:32<02:36,  3.45it/s] 32%|███▏      | 252/790 [01:32<02:35,  3.45it/s] 32%|███▏      | 253/790 [01:32<02:35,  3.45it/s] 32%|███▏      | 254/790 [01:33<02:35,  3.45it/s] 32%|███▏      | 255/790 [01:33<02:34,  3.45it/s] 32%|███▏      | 256/790 [01:33<02:35,  3.44it/s] 33%|███▎      | 257/790 [01:34<02:34,  3.45it/s] 33%|███▎      | 258/790 [01:34<02:34,  3.45it/s] 33%|███▎      | 259/790 [01:34<02:33,  3.45it/s] 33%|███▎      | 260/790 [01:35<02:33,  3.45it/s] 33%|███▎      | 261/790 [01:35<02:33,  3.45it/s] 33%|███▎      | 262/790 [01:35<02:32,  3.45it/s] 33%|███▎      | 263/790 [01:35<02:32,  3.45it/s] 33%|███▎      | 264/790 [01:36<02:32,  3.45it/s] 34%|███▎      | 265/790 [01:36<02:31,  3.45it/s] 34%|███▎      | 266/790 [01:36<02:31,  3.46it/s] 34%|███▍      | 267/790 [01:37<02:31,  3.44it/s] 34%|███▍      | 268/790 [01:37<02:31,  3.45it/s] 34%|███▍      | 269/790 [01:37<02:31,  3.44it/s] 34%|███▍      | 270/790 [01:37<02:30,  3.45it/s] 34%|███▍      | 271/790 [01:38<02:30,  3.45it/s] 34%|███▍      | 272/790 [01:38<02:30,  3.45it/s] 35%|███▍      | 273/790 [01:38<02:29,  3.45it/s] 35%|███▍      | 274/790 [01:39<02:29,  3.45it/s] 35%|███▍      | 275/790 [01:39<02:29,  3.45it/s] 35%|███▍      | 276/790 [01:39<02:28,  3.45it/s] 35%|███▌      | 277/790 [01:39<02:28,  3.45it/s] 35%|███▌      | 278/790 [01:40<02:28,  3.45it/s] 35%|███▌      | 279/790 [01:40<02:27,  3.45it/s] 35%|███▌      | 280/790 [01:40<02:27,  3.45it/s] 36%|███▌      | 281/790 [01:41<02:27,  3.45it/s] 36%|███▌      | 282/790 [01:41<02:27,  3.45it/s] 36%|███▌      | 283/790 [01:41<02:26,  3.45it/s] 36%|███▌      | 284/790 [01:41<02:26,  3.45it/s] 36%|███▌      | 285/790 [01:42<02:26,  3.45it/s] 36%|███▌      | 286/790 [01:42<02:26,  3.45it/s] 36%|███▋      | 287/790 [01:42<02:25,  3.45it/s] 36%|███▋      | 288/790 [01:43<02:25,  3.45it/s] 37%|███▋      | 289/790 [01:43<02:25,  3.45it/s] 37%|███▋      | 290/790 [01:43<02:24,  3.45it/s] 37%|███▋      | 291/790 [01:43<02:24,  3.46it/s] 37%|███▋      | 292/790 [01:44<02:24,  3.45it/s] 37%|███▋      | 293/790 [01:44<02:24,  3.45it/s] 37%|███▋      | 294/790 [01:44<02:23,  3.45it/s] 37%|███▋      | 295/790 [01:45<02:23,  3.45it/s] 37%|███▋      | 296/790 [01:45<02:23,  3.45it/s] 38%|███▊      | 297/790 [01:45<02:22,  3.45it/s] 38%|███▊      | 298/790 [01:46<02:22,  3.45it/s] 38%|███▊      | 299/790 [01:46<02:22,  3.45it/s] 38%|███▊      | 300/790 [01:46<02:22,  3.45it/s] 38%|███▊      | 301/790 [01:46<02:21,  3.45it/s] 38%|███▊      | 302/790 [01:47<02:21,  3.45it/s] 38%|███▊      | 303/790 [01:47<02:21,  3.45it/s] 38%|███▊      | 304/790 [01:47<02:20,  3.45it/s] 39%|███▊      | 305/790 [01:48<02:20,  3.45it/s] 39%|███▊      | 306/790 [01:48<02:20,  3.45it/s] 39%|███▉      | 307/790 [01:48<02:20,  3.43it/s] 39%|███▉      | 308/790 [01:48<02:20,  3.44it/s] 39%|███▉      | 309/790 [01:49<02:19,  3.44it/s] 39%|███▉      | 310/790 [01:49<02:19,  3.45it/s] 39%|███▉      | 311/790 [01:49<02:18,  3.45it/s] 39%|███▉      | 312/790 [01:50<02:27,  3.25it/s] 40%|███▉      | 313/790 [01:50<02:24,  3.29it/s] 40%|███▉      | 314/790 [01:50<02:22,  3.34it/s] 40%|███▉      | 315/790 [01:51<02:21,  3.36it/s] 40%|████      | 316/790 [01:51<02:11,  3.60it/s][INFO|trainer.py:2140] 2023-08-29 04:54:50,758 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:54:50,758 >>   Num examples = 4812
[INFO|trainer.py:2145] 2023-08-29 04:54:50,758 >>   Batch size = 8
{'eval_loss': 0.927378237247467, 'eval_runtime': 12.9444, 'eval_samples_per_second': 371.743, 'eval_steps_per_second': 46.507, 'epoch': 1.0}

  0%|          | 0/602 [00:00<?, ?it/s][A
  1%|          | 6/602 [00:00<00:10, 57.02it/s][A
  2%|▏         | 12/602 [00:00<00:11, 50.45it/s][A
  3%|▎         | 18/602 [00:00<00:12, 48.58it/s][A
  4%|▍         | 23/602 [00:00<00:12, 47.71it/s][A
  5%|▍         | 28/602 [00:00<00:12, 47.36it/s][A
  5%|▌         | 33/602 [00:00<00:12, 47.06it/s][A
  6%|▋         | 38/602 [00:00<00:12, 46.88it/s][A
  7%|▋         | 43/602 [00:00<00:11, 46.78it/s][A
  8%|▊         | 48/602 [00:01<00:11, 46.61it/s][A
  9%|▉         | 53/602 [00:01<00:11, 46.60it/s][A
 10%|▉         | 58/602 [00:01<00:11, 46.54it/s][A
 10%|█         | 63/602 [00:01<00:11, 46.66it/s][A
 11%|█▏        | 68/602 [00:01<00:11, 46.64it/s][A
 12%|█▏        | 73/602 [00:01<00:11, 46.65it/s][A
 13%|█▎        | 78/602 [00:01<00:11, 46.61it/s][A
 14%|█▍        | 83/602 [00:01<00:11, 46.58it/s][A
 15%|█▍        | 88/602 [00:01<00:11, 46.64it/s][A
 15%|█▌        | 93/602 [00:01<00:10, 46.60it/s][A
 16%|█▋        | 98/602 [00:02<00:10, 46.59it/s][A
 17%|█▋        | 103/602 [00:02<00:10, 46.57it/s][A
 18%|█▊        | 108/602 [00:02<00:10, 46.56it/s][A
 19%|█▉        | 113/602 [00:02<00:10, 46.60it/s][A
 20%|█▉        | 118/602 [00:02<00:10, 46.59it/s][A
 20%|██        | 123/602 [00:02<00:10, 46.63it/s][A
 21%|██▏       | 128/602 [00:02<00:10, 46.65it/s][A
 22%|██▏       | 133/602 [00:02<00:10, 46.61it/s][A
 23%|██▎       | 138/602 [00:02<00:09, 46.50it/s][A
 24%|██▍       | 143/602 [00:03<00:09, 46.60it/s][A
 25%|██▍       | 148/602 [00:03<00:09, 46.54it/s][A
 25%|██▌       | 153/602 [00:03<00:09, 46.54it/s][A
 26%|██▌       | 158/602 [00:03<00:09, 46.57it/s][A
 27%|██▋       | 163/602 [00:03<00:09, 46.66it/s][A
 28%|██▊       | 168/602 [00:03<00:09, 46.67it/s][A
 29%|██▊       | 173/602 [00:03<00:09, 46.60it/s][A
 30%|██▉       | 178/602 [00:03<00:09, 46.61it/s][A
 30%|███       | 183/602 [00:03<00:08, 46.59it/s][A
 31%|███       | 188/602 [00:04<00:08, 46.46it/s][A
 32%|███▏      | 193/602 [00:04<00:08, 46.54it/s][A
 33%|███▎      | 198/602 [00:04<00:08, 46.59it/s][A
 34%|███▎      | 203/602 [00:04<00:08, 46.61it/s][A
 35%|███▍      | 208/602 [00:04<00:08, 46.60it/s][A
 35%|███▌      | 213/602 [00:04<00:08, 46.65it/s][A
 36%|███▌      | 218/602 [00:04<00:08, 46.60it/s][A
 37%|███▋      | 223/602 [00:04<00:08, 46.56it/s][A
 38%|███▊      | 228/602 [00:04<00:08, 46.54it/s][A
 39%|███▊      | 233/602 [00:04<00:07, 46.53it/s][A
 40%|███▉      | 238/602 [00:05<00:07, 46.54it/s][A
 40%|████      | 243/602 [00:05<00:07, 46.58it/s][A
 41%|████      | 248/602 [00:05<00:07, 46.58it/s][A
 42%|████▏     | 253/602 [00:05<00:07, 46.53it/s][A
 43%|████▎     | 258/602 [00:05<00:07, 46.61it/s][A
 44%|████▎     | 263/602 [00:05<00:07, 46.64it/s][A
 45%|████▍     | 268/602 [00:05<00:07, 46.61it/s][A
 45%|████▌     | 273/602 [00:05<00:07, 46.54it/s][A
 46%|████▌     | 278/602 [00:05<00:06, 46.57it/s][A
 47%|████▋     | 283/602 [00:06<00:06, 46.57it/s][A
 48%|████▊     | 288/602 [00:06<00:06, 46.53it/s][A
 49%|████▊     | 293/602 [00:06<00:06, 46.58it/s][A
 50%|████▉     | 298/602 [00:06<00:06, 46.59it/s][A
 50%|█████     | 303/602 [00:06<00:06, 46.63it/s][A
 51%|█████     | 308/602 [00:06<00:06, 46.61it/s][A
 52%|█████▏    | 313/602 [00:06<00:06, 46.59it/s][A
 53%|█████▎    | 318/602 [00:06<00:06, 46.63it/s][A
 54%|█████▎    | 323/602 [00:06<00:05, 46.50it/s][A
 54%|█████▍    | 328/602 [00:07<00:05, 46.53it/s][A
 55%|█████▌    | 333/602 [00:07<00:05, 46.59it/s][A
 56%|█████▌    | 338/602 [00:07<00:05, 46.46it/s][A
 57%|█████▋    | 343/602 [00:07<00:05, 46.58it/s][A
 58%|█████▊    | 348/602 [00:07<00:05, 46.52it/s][A
 59%|█████▊    | 353/602 [00:07<00:05, 46.47it/s][A
 59%|█████▉    | 358/602 [00:07<00:05, 46.51it/s][A
 60%|██████    | 363/602 [00:07<00:05, 46.38it/s][A
 61%|██████    | 368/602 [00:07<00:05, 46.56it/s][A
 62%|██████▏   | 373/602 [00:07<00:04, 46.53it/s][A
 63%|██████▎   | 378/602 [00:08<00:04, 46.50it/s][A
 64%|██████▎   | 383/602 [00:08<00:04, 46.57it/s][A
 64%|██████▍   | 388/602 [00:08<00:04, 46.62it/s][A
 65%|██████▌   | 393/602 [00:08<00:04, 46.53it/s][A
 66%|██████▌   | 398/602 [00:08<00:04, 46.53it/s][A
 67%|██████▋   | 403/602 [00:08<00:04, 46.56it/s][A
 68%|██████▊   | 408/602 [00:08<00:04, 46.56it/s][A
 69%|██████▊   | 413/602 [00:08<00:04, 46.61it/s][A
 69%|██████▉   | 418/602 [00:08<00:03, 46.53it/s][A
 70%|███████   | 423/602 [00:09<00:03, 46.55it/s][A
 71%|███████   | 428/602 [00:09<00:03, 46.61it/s][A
 72%|███████▏  | 433/602 [00:09<00:03, 46.56it/s][A
 73%|███████▎  | 438/602 [00:09<00:03, 46.58it/s][A
 74%|███████▎  | 443/602 [00:09<00:03, 46.59it/s][A
 74%|███████▍  | 448/602 [00:09<00:03, 46.49it/s][A
 75%|███████▌  | 453/602 [00:09<00:03, 46.55it/s][A
 76%|███████▌  | 458/602 [00:09<00:03, 46.53it/s][A
 77%|███████▋  | 463/602 [00:09<00:02, 46.45it/s][A
 78%|███████▊  | 468/602 [00:10<00:02, 46.56it/s][A
 79%|███████▊  | 473/602 [00:10<00:02, 46.56it/s][A
 79%|███████▉  | 478/602 [00:10<00:02, 46.56it/s][A
 80%|████████  | 483/602 [00:10<00:02, 46.55it/s][A
 81%|████████  | 488/602 [00:10<00:02, 46.56it/s][A
 82%|████████▏ | 493/602 [00:10<00:02, 46.56it/s][A
 83%|████████▎ | 498/602 [00:10<00:02, 46.62it/s][A
 84%|████████▎ | 503/602 [00:10<00:02, 46.53it/s][A
 84%|████████▍ | 508/602 [00:10<00:02, 46.43it/s][A
 85%|████████▌ | 513/602 [00:10<00:01, 46.52it/s][A
 86%|████████▌ | 518/602 [00:11<00:01, 46.56it/s][A
 87%|████████▋ | 523/602 [00:11<00:01, 46.61it/s][A
 88%|████████▊ | 528/602 [00:11<00:01, 46.51it/s][A
 89%|████████▊ | 533/602 [00:11<00:01, 46.57it/s][A
 89%|████████▉ | 538/602 [00:11<00:01, 46.59it/s][A
 90%|█████████ | 543/602 [00:11<00:01, 46.55it/s][A
 91%|█████████ | 548/602 [00:11<00:01, 46.50it/s][A
 92%|█████████▏| 553/602 [00:11<00:01, 46.48it/s][A
 93%|█████████▎| 558/602 [00:11<00:00, 46.48it/s][A
 94%|█████████▎| 563/602 [00:12<00:00, 46.50it/s][A
 94%|█████████▍| 568/602 [00:12<00:00, 46.58it/s][A
 95%|█████████▌| 573/602 [00:12<00:00, 46.58it/s][A
 96%|█████████▌| 578/602 [00:12<00:00, 46.51it/s][A
 97%|█████████▋| 583/602 [00:12<00:00, 46.58it/s][A
 98%|█████████▊| 588/602 [00:12<00:00, 46.61it/s][A
 99%|█████████▊| 593/602 [00:12<00:00, 46.57it/s][A
 99%|█████████▉| 598/602 [00:12<00:00, 46.49it/s][A                                                 
                                                 [A 40%|████      | 316/790 [02:04<02:11,  3.60it/s]
100%|██████████| 602/602 [00:12<00:00, 46.49it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:55:03,713 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-316
[INFO|configuration_utils.py:351] 2023-08-29 04:55:03,729 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-316/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:55:06,106 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-316/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:55:06,124 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-316/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:55:06,132 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-316/special_tokens_map.json
 40%|████      | 317/790 [02:11<48:16,  6.12s/it] 40%|████      | 318/790 [02:11<34:24,  4.37s/it] 40%|████      | 319/790 [02:11<24:42,  3.15s/it] 41%|████      | 320/790 [02:11<17:56,  2.29s/it] 41%|████      | 321/790 [02:12<13:12,  1.69s/it] 41%|████      | 322/790 [02:12<09:54,  1.27s/it] 41%|████      | 323/790 [02:12<07:35,  1.03it/s] 41%|████      | 324/790 [02:13<05:58,  1.30it/s] 41%|████      | 325/790 [02:13<04:50,  1.60it/s] 41%|████▏     | 326/790 [02:13<04:03,  1.91it/s] 41%|████▏     | 327/790 [02:13<03:30,  2.20it/s] 42%|████▏     | 328/790 [02:14<03:06,  2.47it/s] 42%|████▏     | 329/790 [02:14<02:51,  2.69it/s] 42%|████▏     | 330/790 [02:14<02:39,  2.89it/s] 42%|████▏     | 331/790 [02:15<02:31,  3.04it/s] 42%|████▏     | 332/790 [02:15<02:25,  3.15it/s] 42%|████▏     | 333/790 [02:15<02:21,  3.24it/s] 42%|████▏     | 334/790 [02:15<02:17,  3.31it/s] 42%|████▏     | 335/790 [02:16<02:15,  3.35it/s] 43%|████▎     | 336/790 [02:16<02:14,  3.38it/s] 43%|████▎     | 337/790 [02:16<02:12,  3.41it/s] 43%|████▎     | 338/790 [02:17<02:12,  3.42it/s] 43%|████▎     | 339/790 [02:17<02:11,  3.44it/s] 43%|████▎     | 340/790 [02:17<02:10,  3.44it/s] 43%|████▎     | 341/790 [02:17<02:10,  3.45it/s] 43%|████▎     | 342/790 [02:18<02:09,  3.45it/s] 43%|████▎     | 343/790 [02:18<02:09,  3.46it/s] 44%|████▎     | 344/790 [02:18<02:08,  3.46it/s] 44%|████▎     | 345/790 [02:19<02:08,  3.46it/s] 44%|████▍     | 346/790 [02:19<02:08,  3.46it/s] 44%|████▍     | 347/790 [02:19<02:08,  3.46it/s] 44%|████▍     | 348/790 [02:19<02:07,  3.46it/s] 44%|████▍     | 349/790 [02:20<02:07,  3.46it/s] 44%|████▍     | 350/790 [02:20<02:07,  3.46it/s] 44%|████▍     | 351/790 [02:20<02:07,  3.46it/s] 45%|████▍     | 352/790 [02:21<02:06,  3.46it/s] 45%|████▍     | 353/790 [02:21<02:06,  3.46it/s] 45%|████▍     | 354/790 [02:21<02:05,  3.46it/s] 45%|████▍     | 355/790 [02:21<02:05,  3.46it/s] 45%|████▌     | 356/790 [02:22<02:05,  3.46it/s] 45%|████▌     | 357/790 [02:22<02:05,  3.46it/s] 45%|████▌     | 358/790 [02:22<02:04,  3.46it/s] 45%|████▌     | 359/790 [02:23<02:04,  3.46it/s] 46%|████▌     | 360/790 [02:23<02:04,  3.46it/s] 46%|████▌     | 361/790 [02:23<02:03,  3.46it/s] 46%|████▌     | 362/790 [02:24<02:03,  3.45it/s] 46%|████▌     | 363/790 [02:24<02:03,  3.46it/s] 46%|████▌     | 364/790 [02:24<02:03,  3.46it/s] 46%|████▌     | 365/790 [02:24<02:02,  3.46it/s] 46%|████▋     | 366/790 [02:25<02:02,  3.45it/s] 46%|████▋     | 367/790 [02:25<02:02,  3.45it/s] 47%|████▋     | 368/790 [02:25<02:02,  3.46it/s] 47%|████▋     | 369/790 [02:26<02:01,  3.46it/s] 47%|████▋     | 370/790 [02:26<02:01,  3.46it/s] 47%|████▋     | 371/790 [02:26<02:01,  3.46it/s] 47%|████▋     | 372/790 [02:26<02:00,  3.46it/s] 47%|████▋     | 373/790 [02:27<02:00,  3.45it/s] 47%|████▋     | 374/790 [02:27<02:00,  3.45it/s] 47%|████▋     | 375/790 [02:27<02:00,  3.45it/s] 48%|████▊     | 376/790 [02:28<01:59,  3.46it/s] 48%|████▊     | 377/790 [02:28<01:59,  3.46it/s] 48%|████▊     | 378/790 [02:28<01:59,  3.46it/s] 48%|████▊     | 379/790 [02:28<01:58,  3.46it/s] 48%|████▊     | 380/790 [02:29<01:58,  3.46it/s] 48%|████▊     | 381/790 [02:29<01:58,  3.46it/s] 48%|████▊     | 382/790 [02:29<01:57,  3.46it/s] 48%|████▊     | 383/790 [02:30<01:57,  3.46it/s] 49%|████▊     | 384/790 [02:30<01:57,  3.45it/s] 49%|████▊     | 385/790 [02:30<01:57,  3.45it/s] 49%|████▉     | 386/790 [02:30<01:56,  3.45it/s] 49%|████▉     | 387/790 [02:31<01:56,  3.46it/s] 49%|████▉     | 388/790 [02:31<01:56,  3.46it/s] 49%|████▉     | 389/790 [02:31<01:55,  3.46it/s] 49%|████▉     | 390/790 [02:32<01:55,  3.46it/s] 49%|████▉     | 391/790 [02:32<01:55,  3.46it/s] 50%|████▉     | 392/790 [02:32<01:55,  3.46it/s] 50%|████▉     | 393/790 [02:32<01:54,  3.46it/s] 50%|████▉     | 394/790 [02:33<01:54,  3.46it/s] 50%|█████     | 395/790 [02:33<01:54,  3.45it/s] 50%|█████     | 396/790 [02:33<01:54,  3.45it/s] 50%|█████     | 397/790 [02:34<01:53,  3.45it/s] 50%|█████     | 398/790 [02:34<01:53,  3.46it/s] 51%|█████     | 399/790 [02:34<01:53,  3.46it/s] 51%|█████     | 400/790 [02:35<01:52,  3.46it/s] 51%|█████     | 401/790 [02:35<01:52,  3.46it/s] 51%|█████     | 402/790 [02:35<01:52,  3.46it/s] 51%|█████     | 403/790 [02:35<01:51,  3.46it/s] 51%|█████     | 404/790 [02:36<01:51,  3.46it/s] 51%|█████▏    | 405/790 [02:36<01:51,  3.46it/s] 51%|█████▏    | 406/790 [02:36<01:50,  3.46it/s] 52%|█████▏    | 407/790 [02:37<01:50,  3.46it/s] 52%|█████▏    | 408/790 [02:37<01:50,  3.45it/s] 52%|█████▏    | 409/790 [02:37<01:50,  3.45it/s] 52%|█████▏    | 410/790 [02:37<01:49,  3.46it/s] 52%|█████▏    | 411/790 [02:38<01:49,  3.46it/s] 52%|█████▏    | 412/790 [02:38<01:49,  3.46it/s] 52%|█████▏    | 413/790 [02:38<01:48,  3.46it/s] 52%|█████▏    | 414/790 [02:39<01:48,  3.46it/s] 53%|█████▎    | 415/790 [02:39<01:48,  3.46it/s] 53%|█████▎    | 416/790 [02:39<01:48,  3.46it/s] 53%|█████▎    | 417/790 [02:39<01:47,  3.46it/s] 53%|█████▎    | 418/790 [02:40<01:47,  3.46it/s] 53%|█████▎    | 419/790 [02:40<01:47,  3.45it/s] 53%|█████▎    | 420/790 [02:40<01:47,  3.46it/s] 53%|█████▎    | 421/790 [02:41<01:46,  3.46it/s] 53%|█████▎    | 422/790 [02:41<01:46,  3.46it/s] 54%|█████▎    | 423/790 [02:41<01:46,  3.46it/s] 54%|█████▎    | 424/790 [02:41<01:45,  3.46it/s] 54%|█████▍    | 425/790 [02:42<01:45,  3.46it/s] 54%|█████▍    | 426/790 [02:42<01:45,  3.46it/s] 54%|█████▍    | 427/790 [02:42<01:44,  3.46it/s] 54%|█████▍    | 428/790 [02:43<01:44,  3.46it/s] 54%|█████▍    | 429/790 [02:43<01:44,  3.46it/s] 54%|█████▍    | 430/790 [02:43<01:44,  3.45it/s] 55%|█████▍    | 431/790 [02:43<01:43,  3.45it/s] 55%|█████▍    | 432/790 [02:44<01:43,  3.46it/s] 55%|█████▍    | 433/790 [02:44<01:43,  3.46it/s] 55%|█████▍    | 434/790 [02:44<01:42,  3.46it/s] 55%|█████▌    | 435/790 [02:45<01:42,  3.46it/s] 55%|█████▌    | 436/790 [02:45<01:42,  3.46it/s] 55%|█████▌    | 437/790 [02:45<01:42,  3.46it/s] 55%|█████▌    | 438/790 [02:46<01:41,  3.46it/s] 56%|█████▌    | 439/790 [02:46<01:41,  3.46it/s] 56%|█████▌    | 440/790 [02:46<01:41,  3.46it/s] 56%|█████▌    | 441/790 [02:46<01:41,  3.45it/s] 56%|█████▌    | 442/790 [02:47<01:40,  3.45it/s] 56%|█████▌    | 443/790 [02:47<01:40,  3.46it/s] 56%|█████▌    | 444/790 [02:47<01:40,  3.46it/s] 56%|█████▋    | 445/790 [02:48<01:39,  3.46it/s] 56%|█████▋    | 446/790 [02:48<01:39,  3.46it/s] 57%|█████▋    | 447/790 [02:48<01:39,  3.46it/s] 57%|█████▋    | 448/790 [02:48<01:39,  3.45it/s] 57%|█████▋    | 449/790 [02:49<01:38,  3.45it/s] 57%|█████▋    | 450/790 [02:49<01:38,  3.45it/s] 57%|█████▋    | 451/790 [02:49<01:38,  3.46it/s] 57%|█████▋    | 452/790 [02:50<01:38,  3.44it/s] 57%|█████▋    | 453/790 [02:50<01:40,  3.36it/s] 57%|█████▋    | 454/790 [02:50<01:39,  3.38it/s] 58%|█████▊    | 455/790 [02:50<01:38,  3.40it/s] 58%|█████▊    | 456/790 [02:51<01:37,  3.41it/s] 58%|█████▊    | 457/790 [02:51<01:37,  3.42it/s] 58%|█████▊    | 458/790 [02:51<01:36,  3.43it/s] 58%|█████▊    | 459/790 [02:52<01:36,  3.44it/s] 58%|█████▊    | 460/790 [02:52<01:35,  3.44it/s] 58%|█████▊    | 461/790 [02:52<01:35,  3.44it/s] 58%|█████▊    | 462/790 [02:52<01:35,  3.44it/s] 59%|█████▊    | 463/790 [02:53<01:35,  3.41it/s] 59%|█████▊    | 464/790 [02:53<01:35,  3.42it/s] 59%|█████▉    | 465/790 [02:53<01:34,  3.43it/s] 59%|█████▉    | 466/790 [02:54<01:34,  3.44it/s] 59%|█████▉    | 467/790 [02:54<01:33,  3.44it/s] 59%|█████▉    | 468/790 [02:54<01:33,  3.44it/s] 59%|█████▉    | 469/790 [02:55<01:33,  3.45it/s] 59%|█████▉    | 470/790 [02:55<01:32,  3.45it/s] 60%|█████▉    | 471/790 [02:55<01:32,  3.45it/s] 60%|█████▉    | 472/790 [02:55<01:32,  3.45it/s] 60%|█████▉    | 473/790 [02:56<01:31,  3.45it/s] 60%|██████    | 474/790 [02:56<01:26,  3.66it/s][INFO|trainer.py:2140] 2023-08-29 04:55:55,930 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:55:55,930 >>   Num examples = 4812
[INFO|trainer.py:2145] 2023-08-29 04:55:55,930 >>   Batch size = 8
{'eval_loss': 0.9364662170410156, 'eval_runtime': 12.9255, 'eval_samples_per_second': 372.289, 'eval_steps_per_second': 46.575, 'epoch': 2.0}

  0%|          | 0/602 [00:00<?, ?it/s][A
  1%|          | 6/602 [00:00<00:10, 57.00it/s][A
  2%|▏         | 12/602 [00:00<00:11, 50.35it/s][A
  3%|▎         | 18/602 [00:00<00:12, 48.55it/s][A
  4%|▍         | 23/602 [00:00<00:12, 47.75it/s][A
  5%|▍         | 28/602 [00:00<00:12, 47.43it/s][A
  5%|▌         | 33/602 [00:00<00:12, 47.16it/s][A
  6%|▋         | 38/602 [00:00<00:12, 46.86it/s][A
  7%|▋         | 43/602 [00:00<00:11, 46.66it/s][A
  8%|▊         | 48/602 [00:01<00:11, 46.58it/s][A
  9%|▉         | 53/602 [00:01<00:11, 46.70it/s][A
 10%|▉         | 58/602 [00:01<00:11, 46.70it/s][A
 10%|█         | 63/602 [00:01<00:11, 46.71it/s][A
 11%|█▏        | 68/602 [00:01<00:11, 46.70it/s][A
 12%|█▏        | 73/602 [00:01<00:11, 46.62it/s][A
 13%|█▎        | 78/602 [00:01<00:11, 46.62it/s][A
 14%|█▍        | 83/602 [00:01<00:11, 46.61it/s][A
 15%|█▍        | 88/602 [00:01<00:11, 46.54it/s][A
 15%|█▌        | 93/602 [00:01<00:10, 46.52it/s][A
 16%|█▋        | 98/602 [00:02<00:10, 46.60it/s][A
 17%|█▋        | 103/602 [00:02<00:10, 46.55it/s][A
 18%|█▊        | 108/602 [00:02<00:10, 46.56it/s][A
 19%|█▉        | 113/602 [00:02<00:10, 46.63it/s][A
 20%|█▉        | 118/602 [00:02<00:10, 46.58it/s][A
 20%|██        | 123/602 [00:02<00:10, 46.56it/s][A
 21%|██▏       | 128/602 [00:02<00:10, 46.57it/s][A
 22%|██▏       | 133/602 [00:02<00:10, 46.57it/s][A
 23%|██▎       | 138/602 [00:02<00:09, 46.55it/s][A
 24%|██▍       | 143/602 [00:03<00:09, 46.48it/s][A
 25%|██▍       | 148/602 [00:03<00:09, 46.64it/s][A
 25%|██▌       | 153/602 [00:03<00:09, 46.60it/s][A
 26%|██▌       | 158/602 [00:03<00:09, 46.61it/s][A
 27%|██▋       | 163/602 [00:03<00:09, 46.61it/s][A
 28%|██▊       | 168/602 [00:03<00:09, 46.39it/s][A
 29%|██▊       | 173/602 [00:03<00:09, 46.44it/s][A
 30%|██▉       | 178/602 [00:03<00:09, 46.48it/s][A
 30%|███       | 183/602 [00:03<00:09, 46.54it/s][A
 31%|███       | 188/602 [00:04<00:08, 46.60it/s][A
 32%|███▏      | 193/602 [00:04<00:08, 46.60it/s][A
 33%|███▎      | 198/602 [00:04<00:08, 46.49it/s][A
 34%|███▎      | 203/602 [00:04<00:08, 46.61it/s][A
 35%|███▍      | 208/602 [00:04<00:08, 46.60it/s][A
 35%|███▌      | 213/602 [00:04<00:08, 46.55it/s][A
 36%|███▌      | 218/602 [00:04<00:08, 46.54it/s][A
 37%|███▋      | 223/602 [00:04<00:08, 46.54it/s][A
 38%|███▊      | 228/602 [00:04<00:08, 46.46it/s][A
 39%|███▊      | 233/602 [00:04<00:07, 46.60it/s][A
 40%|███▉      | 238/602 [00:05<00:07, 46.64it/s][A
 40%|████      | 243/602 [00:05<00:07, 46.59it/s][A
 41%|████      | 248/602 [00:05<00:07, 46.53it/s][A
 42%|████▏     | 253/602 [00:05<00:07, 46.51it/s][A
 43%|████▎     | 258/602 [00:05<00:07, 46.52it/s][A
 44%|████▎     | 263/602 [00:05<00:07, 46.51it/s][A
 45%|████▍     | 268/602 [00:05<00:07, 46.52it/s][A
 45%|████▌     | 273/602 [00:05<00:07, 46.58it/s][A
 46%|████▌     | 278/602 [00:05<00:06, 46.57it/s][A
 47%|████▋     | 283/602 [00:06<00:06, 46.63it/s][A
 48%|████▊     | 288/602 [00:06<00:06, 46.60it/s][A
 49%|████▊     | 293/602 [00:06<00:06, 46.48it/s][A
 50%|████▉     | 298/602 [00:06<00:06, 46.50it/s][A
 50%|█████     | 303/602 [00:06<00:06, 46.48it/s][A
 51%|█████     | 308/602 [00:06<00:06, 46.45it/s][A
 52%|█████▏    | 313/602 [00:06<00:06, 46.53it/s][A
 53%|█████▎    | 318/602 [00:06<00:06, 46.56it/s][A
 54%|█████▎    | 323/602 [00:06<00:05, 46.55it/s][A
 54%|█████▍    | 328/602 [00:07<00:05, 46.64it/s][A
 55%|█████▌    | 333/602 [00:07<00:05, 46.53it/s][A
 56%|█████▌    | 338/602 [00:07<00:05, 46.54it/s][A
 57%|█████▋    | 343/602 [00:07<00:05, 46.47it/s][A
 58%|█████▊    | 348/602 [00:07<00:05, 46.50it/s][A
 59%|█████▊    | 353/602 [00:07<00:05, 46.43it/s][A
 59%|█████▉    | 358/602 [00:07<00:05, 46.47it/s][A
 60%|██████    | 363/602 [00:07<00:05, 46.52it/s][A
 61%|██████    | 368/602 [00:07<00:05, 46.58it/s][A
 62%|██████▏   | 373/602 [00:07<00:04, 46.64it/s][A
 63%|██████▎   | 378/602 [00:08<00:04, 46.57it/s][A
 64%|██████▎   | 383/602 [00:08<00:04, 46.57it/s][A
 64%|██████▍   | 388/602 [00:08<00:04, 46.55it/s][A
 65%|██████▌   | 393/602 [00:08<00:04, 46.56it/s][A
 66%|██████▌   | 398/602 [00:08<00:04, 46.50it/s][A
 67%|██████▋   | 403/602 [00:08<00:04, 46.54it/s][A
 68%|██████▊   | 408/602 [00:08<00:04, 46.42it/s][A
 69%|██████▊   | 413/602 [00:08<00:04, 46.51it/s][A
 69%|██████▉   | 418/602 [00:08<00:03, 46.52it/s][A
 70%|███████   | 423/602 [00:09<00:03, 46.60it/s][A
 71%|███████   | 428/602 [00:09<00:03, 46.58it/s][A
 72%|███████▏  | 433/602 [00:09<00:03, 46.52it/s][A
 73%|███████▎  | 438/602 [00:09<00:03, 46.54it/s][A
 74%|███████▎  | 443/602 [00:09<00:03, 46.57it/s][A
 74%|███████▍  | 448/602 [00:09<00:03, 46.43it/s][A
 75%|███████▌  | 453/602 [00:09<00:03, 46.46it/s][A
 76%|███████▌  | 458/602 [00:09<00:03, 46.54it/s][A
 77%|███████▋  | 463/602 [00:09<00:02, 46.53it/s][A
 78%|███████▊  | 468/602 [00:10<00:02, 46.59it/s][A
 79%|███████▊  | 473/602 [00:10<00:02, 46.55it/s][A
 79%|███████▉  | 478/602 [00:10<00:02, 46.46it/s][A
 80%|████████  | 483/602 [00:10<00:02, 46.49it/s][A
 81%|████████  | 488/602 [00:10<00:02, 46.54it/s][A
 82%|████████▏ | 493/602 [00:10<00:02, 46.55it/s][A
 83%|████████▎ | 498/602 [00:10<00:02, 46.44it/s][A
 84%|████████▎ | 503/602 [00:10<00:02, 46.49it/s][A
 84%|████████▍ | 508/602 [00:10<00:02, 46.51it/s][A
 85%|████████▌ | 513/602 [00:10<00:01, 46.57it/s][A
 86%|████████▌ | 518/602 [00:11<00:01, 46.47it/s][A
 87%|████████▋ | 523/602 [00:11<00:01, 46.52it/s][A
 88%|████████▊ | 528/602 [00:11<00:01, 46.55it/s][A
 89%|████████▊ | 533/602 [00:11<00:01, 46.40it/s][A
 89%|████████▉ | 538/602 [00:11<00:01, 46.52it/s][A
 90%|█████████ | 543/602 [00:11<00:01, 46.46it/s][A
 91%|█████████ | 548/602 [00:11<00:01, 46.48it/s][A
 92%|█████████▏| 553/602 [00:11<00:01, 46.59it/s][A
 93%|█████████▎| 558/602 [00:11<00:00, 46.53it/s][A
 94%|█████████▎| 563/602 [00:12<00:00, 46.62it/s][A
 94%|█████████▍| 568/602 [00:12<00:00, 46.59it/s][A
 95%|█████████▌| 573/602 [00:12<00:00, 46.51it/s][A
 96%|█████████▌| 578/602 [00:12<00:00, 46.49it/s][A
 97%|█████████▋| 583/602 [00:12<00:00, 46.50it/s][A
 98%|█████████▊| 588/602 [00:12<00:00, 46.55it/s][A
 99%|█████████▊| 593/602 [00:12<00:00, 46.49it/s][A
 99%|█████████▉| 598/602 [00:12<00:00, 46.43it/s][A                                                 
                                                 [A 60%|██████    | 474/790 [03:09<01:26,  3.66it/s]
100%|██████████| 602/602 [00:12<00:00, 46.43it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:56:08,886 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-474
[INFO|configuration_utils.py:351] 2023-08-29 04:56:08,901 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-474/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:56:10,956 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-474/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:56:10,974 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-474/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:56:10,986 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-474/special_tokens_map.json
 60%|██████    | 475/790 [03:16<32:21,  6.16s/it] 60%|██████    | 476/790 [03:16<23:02,  4.40s/it] 60%|██████    | 477/790 [03:16<16:31,  3.17s/it] 61%|██████    | 478/790 [03:17<11:58,  2.30s/it] 61%|██████    | 479/790 [03:17<08:48,  1.70s/it] 61%|██████    | 480/790 [03:17<06:35,  1.28s/it] 61%|██████    | 481/790 [03:18<05:02,  1.02it/s] 61%|██████    | 482/790 [03:18<03:57,  1.29it/s] 61%|██████    | 483/790 [03:18<03:12,  1.59it/s] 61%|██████▏   | 484/790 [03:18<02:40,  1.90it/s] 61%|██████▏   | 485/790 [03:19<02:18,  2.20it/s] 62%|██████▏   | 486/790 [03:19<02:03,  2.47it/s] 62%|██████▏   | 487/790 [03:19<01:52,  2.70it/s] 62%|██████▏   | 488/790 [03:20<01:44,  2.89it/s] 62%|██████▏   | 489/790 [03:20<01:39,  3.04it/s] 62%|██████▏   | 490/790 [03:20<01:35,  3.15it/s] 62%|██████▏   | 491/790 [03:20<01:32,  3.24it/s] 62%|██████▏   | 492/790 [03:21<01:30,  3.30it/s] 62%|██████▏   | 493/790 [03:21<01:28,  3.35it/s] 63%|██████▎   | 494/790 [03:21<01:27,  3.38it/s] 63%|██████▎   | 495/790 [03:22<01:26,  3.41it/s] 63%|██████▎   | 496/790 [03:22<01:25,  3.42it/s] 63%|██████▎   | 497/790 [03:22<01:25,  3.44it/s] 63%|██████▎   | 498/790 [03:22<01:25,  3.43it/s] 63%|██████▎   | 499/790 [03:23<01:24,  3.44it/s] 63%|██████▎   | 500/790 [03:23<01:24,  3.45it/s]                                                  63%|██████▎   | 500/790 [03:23<01:24,  3.45it/s] 63%|██████▎   | 501/790 [03:23<01:23,  3.45it/s] 64%|██████▎   | 502/790 [03:24<01:23,  3.46it/s] 64%|██████▎   | 503/790 [03:24<01:23,  3.46it/s] 64%|██████▍   | 504/790 [03:24<01:22,  3.46it/s] 64%|██████▍   | 505/790 [03:24<01:22,  3.46it/s] 64%|██████▍   | 506/790 [03:25<01:22,  3.46it/s] 64%|██████▍   | 507/790 [03:25<01:21,  3.46it/s] 64%|██████▍   | 508/790 [03:25<01:21,  3.46it/s] 64%|██████▍   | 509/790 [03:26<01:21,  3.45it/s] 65%|██████▍   | 510/790 [03:26<01:21,  3.46it/s] 65%|██████▍   | 511/790 [03:26<01:20,  3.46it/s] 65%|██████▍   | 512/790 [03:27<01:20,  3.46it/s] 65%|██████▍   | 513/790 [03:27<01:19,  3.46it/s] 65%|██████▌   | 514/790 [03:27<01:19,  3.46it/s] 65%|██████▌   | 515/790 [03:27<01:19,  3.46it/s] 65%|██████▌   | 516/790 [03:28<01:19,  3.46it/s] 65%|██████▌   | 517/790 [03:28<01:18,  3.46it/s] 66%|██████▌   | 518/790 [03:28<01:18,  3.46it/s] 66%|██████▌   | 519/790 [03:29<01:18,  3.46it/s] 66%|██████▌   | 520/790 [03:29<01:18,  3.46it/s] 66%|██████▌   | 521/790 [03:29<01:17,  3.46it/s] 66%|██████▌   | 522/790 [03:29<01:17,  3.46it/s] 66%|██████▌   | 523/790 [03:30<01:17,  3.46it/s] 66%|██████▋   | 524/790 [03:30<01:16,  3.46it/s] 66%|██████▋   | 525/790 [03:30<01:16,  3.46it/s] 67%|██████▋   | 526/790 [03:31<01:16,  3.46it/s] 67%|██████▋   | 527/790 [03:31<01:15,  3.46it/s] 67%|██████▋   | 528/790 [03:31<01:15,  3.46it/s] 67%|██████▋   | 529/790 [03:31<01:15,  3.46it/s] 67%|██████▋   | 530/790 [03:32<01:15,  3.46it/s] 67%|██████▋   | 531/790 [03:32<01:15,  3.45it/s] 67%|██████▋   | 532/790 [03:32<01:14,  3.45it/s] 67%|██████▋   | 533/790 [03:33<01:14,  3.45it/s] 68%|██████▊   | 534/790 [03:33<01:14,  3.46it/s] 68%|██████▊   | 535/790 [03:33<01:13,  3.46it/s] 68%|██████▊   | 536/790 [03:33<01:13,  3.46it/s] 68%|██████▊   | 537/790 [03:34<01:13,  3.46it/s] 68%|██████▊   | 538/790 [03:34<01:12,  3.46it/s] 68%|██████▊   | 539/790 [03:34<01:12,  3.46it/s] 68%|██████▊   | 540/790 [03:35<01:12,  3.46it/s] 68%|██████▊   | 541/790 [03:35<01:11,  3.46it/s] 69%|██████▊   | 542/790 [03:35<01:11,  3.45it/s] 69%|██████▊   | 543/790 [03:35<01:11,  3.45it/s] 69%|██████▉   | 544/790 [03:36<01:11,  3.45it/s] 69%|██████▉   | 545/790 [03:36<01:10,  3.46it/s] 69%|██████▉   | 546/790 [03:36<01:10,  3.46it/s] 69%|██████▉   | 547/790 [03:37<01:10,  3.46it/s] 69%|██████▉   | 548/790 [03:37<01:09,  3.46it/s] 69%|██████▉   | 549/790 [03:37<01:09,  3.46it/s] 70%|██████▉   | 550/790 [03:38<01:09,  3.46it/s] 70%|██████▉   | 551/790 [03:38<01:09,  3.46it/s] 70%|██████▉   | 552/790 [03:38<01:08,  3.46it/s] 70%|███████   | 553/790 [03:38<01:08,  3.45it/s] 70%|███████   | 554/790 [03:39<01:08,  3.45it/s] 70%|███████   | 555/790 [03:39<01:08,  3.46it/s] 70%|███████   | 556/790 [03:39<01:07,  3.46it/s] 71%|███████   | 557/790 [03:40<01:07,  3.46it/s] 71%|███████   | 558/790 [03:40<01:07,  3.46it/s] 71%|███████   | 559/790 [03:40<01:06,  3.46it/s] 71%|███████   | 560/790 [03:40<01:06,  3.46it/s] 71%|███████   | 561/790 [03:41<01:06,  3.46it/s] 71%|███████   | 562/790 [03:41<01:05,  3.46it/s] 71%|███████▏  | 563/790 [03:41<01:05,  3.46it/s] 71%|███████▏  | 564/790 [03:42<01:05,  3.45it/s] 72%|███████▏  | 565/790 [03:42<01:05,  3.45it/s] 72%|███████▏  | 566/790 [03:42<01:04,  3.45it/s] 72%|███████▏  | 567/790 [03:42<01:04,  3.46it/s] 72%|███████▏  | 568/790 [03:43<01:04,  3.46it/s] 72%|███████▏  | 569/790 [03:43<01:03,  3.46it/s] 72%|███████▏  | 570/790 [03:43<01:03,  3.46it/s] 72%|███████▏  | 571/790 [03:44<01:03,  3.46it/s] 72%|███████▏  | 572/790 [03:44<01:03,  3.46it/s] 73%|███████▎  | 573/790 [03:44<01:02,  3.46it/s] 73%|███████▎  | 574/790 [03:44<01:02,  3.46it/s] 73%|███████▎  | 575/790 [03:45<01:02,  3.45it/s] 73%|███████▎  | 576/790 [03:45<01:01,  3.45it/s] 73%|███████▎  | 577/790 [03:45<01:01,  3.45it/s] 73%|███████▎  | 578/790 [03:46<01:01,  3.46it/s] 73%|███████▎  | 579/790 [03:46<01:01,  3.46it/s] 73%|███████▎  | 580/790 [03:46<01:00,  3.46it/s] 74%|███████▎  | 581/790 [03:46<01:00,  3.46it/s] 74%|███████▎  | 582/790 [03:47<01:00,  3.46it/s] 74%|███████▍  | 583/790 [03:47<00:59,  3.46it/s] 74%|███████▍  | 584/790 [03:47<00:59,  3.46it/s] 74%|███████▍  | 585/790 [03:48<00:59,  3.46it/s] 74%|███████▍  | 586/790 [03:48<00:58,  3.46it/s] 74%|███████▍  | 587/790 [03:48<00:58,  3.46it/s] 74%|███████▍  | 588/790 [03:49<00:58,  3.45it/s] 75%|███████▍  | 589/790 [03:49<00:58,  3.45it/s] 75%|███████▍  | 590/790 [03:49<00:57,  3.45it/s] 75%|███████▍  | 591/790 [03:49<00:57,  3.45it/s] 75%|███████▍  | 592/790 [03:50<00:57,  3.45it/s] 75%|███████▌  | 593/790 [03:50<01:00,  3.25it/s] 75%|███████▌  | 594/790 [03:50<00:59,  3.30it/s] 75%|███████▌  | 595/790 [03:51<00:58,  3.34it/s] 75%|███████▌  | 596/790 [03:51<00:57,  3.38it/s] 76%|███████▌  | 597/790 [03:51<00:56,  3.40it/s] 76%|███████▌  | 598/790 [03:51<00:56,  3.41it/s] 76%|███████▌  | 599/790 [03:52<00:55,  3.41it/s] 76%|███████▌  | 600/790 [03:52<00:55,  3.43it/s] 76%|███████▌  | 601/790 [03:52<00:55,  3.43it/s] 76%|███████▌  | 602/790 [03:53<00:54,  3.43it/s] 76%|███████▋  | 603/790 [03:53<00:54,  3.44it/s] 76%|███████▋  | 604/790 [03:53<00:54,  3.44it/s] 77%|███████▋  | 605/790 [03:53<00:53,  3.44it/s] 77%|███████▋  | 606/790 [03:54<00:53,  3.44it/s] 77%|███████▋  | 607/790 [03:54<00:53,  3.45it/s] 77%|███████▋  | 608/790 [03:54<00:52,  3.45it/s] 77%|███████▋  | 609/790 [03:55<00:52,  3.45it/s] 77%|███████▋  | 610/790 [03:55<00:52,  3.43it/s] 77%|███████▋  | 611/790 [03:55<00:52,  3.44it/s] 77%|███████▋  | 612/790 [03:56<00:51,  3.44it/s] 78%|███████▊  | 613/790 [03:56<00:51,  3.44it/s] 78%|███████▊  | 614/790 [03:56<00:51,  3.44it/s] 78%|███████▊  | 615/790 [03:56<00:50,  3.45it/s] 78%|███████▊  | 616/790 [03:57<00:50,  3.45it/s] 78%|███████▊  | 617/790 [03:57<00:50,  3.45it/s] 78%|███████▊  | 618/790 [03:57<00:49,  3.45it/s] 78%|███████▊  | 619/790 [03:58<00:49,  3.45it/s] 78%|███████▊  | 620/790 [03:58<00:49,  3.45it/s] 79%|███████▊  | 621/790 [03:58<00:49,  3.44it/s] 79%|███████▊  | 622/790 [03:58<00:48,  3.44it/s] 79%|███████▉  | 623/790 [03:59<00:48,  3.45it/s] 79%|███████▉  | 624/790 [03:59<00:48,  3.44it/s] 79%|███████▉  | 625/790 [03:59<00:47,  3.44it/s] 79%|███████▉  | 626/790 [04:00<00:47,  3.44it/s] 79%|███████▉  | 627/790 [04:00<00:47,  3.45it/s] 79%|███████▉  | 628/790 [04:00<00:47,  3.45it/s] 80%|███████▉  | 629/790 [04:00<00:46,  3.45it/s] 80%|███████▉  | 630/790 [04:01<00:46,  3.45it/s] 80%|███████▉  | 631/790 [04:01<00:46,  3.45it/s] 80%|████████  | 632/790 [04:01<00:43,  3.66it/s][INFO|trainer.py:2140] 2023-08-29 04:57:01,291 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:57:01,291 >>   Num examples = 4812
[INFO|trainer.py:2145] 2023-08-29 04:57:01,291 >>   Batch size = 8
{'eval_loss': 0.9502619504928589, 'eval_runtime': 12.9307, 'eval_samples_per_second': 372.139, 'eval_steps_per_second': 46.556, 'epoch': 3.0}
{'loss': 0.6175, 'learning_rate': 1.3765822784810127e-05, 'epoch': 3.16}

  0%|          | 0/602 [00:00<?, ?it/s][A
  1%|          | 6/602 [00:00<00:10, 56.53it/s][A
  2%|▏         | 12/602 [00:00<00:11, 50.14it/s][A
  3%|▎         | 18/602 [00:00<00:12, 48.38it/s][A
  4%|▍         | 23/602 [00:00<00:12, 47.86it/s][A
  5%|▍         | 28/602 [00:00<00:12, 47.50it/s][A
  5%|▌         | 33/602 [00:00<00:12, 47.20it/s][A
  6%|▋         | 38/602 [00:00<00:12, 46.84it/s][A
  7%|▋         | 43/602 [00:00<00:12, 46.56it/s][A
  8%|▊         | 48/602 [00:01<00:11, 46.52it/s][A
  9%|▉         | 53/602 [00:01<00:11, 46.44it/s][A
 10%|▉         | 58/602 [00:01<00:11, 46.39it/s][A
 10%|█         | 63/602 [00:01<00:11, 46.51it/s][A
 11%|█▏        | 68/602 [00:01<00:11, 46.48it/s][A
 12%|█▏        | 73/602 [00:01<00:11, 46.56it/s][A
 13%|█▎        | 78/602 [00:01<00:11, 46.58it/s][A
 14%|█▍        | 83/602 [00:01<00:11, 46.62it/s][A
 15%|█▍        | 88/602 [00:01<00:11, 46.52it/s][A
 15%|█▌        | 93/602 [00:01<00:10, 46.46it/s][A
 16%|█▋        | 98/602 [00:02<00:10, 46.48it/s][A
 17%|█▋        | 103/602 [00:02<00:10, 46.47it/s][A
 18%|█▊        | 108/602 [00:02<00:10, 46.42it/s][A
 19%|█▉        | 113/602 [00:02<00:10, 46.37it/s][A
 20%|█▉        | 118/602 [00:02<00:10, 46.53it/s][A
 20%|██        | 123/602 [00:02<00:10, 46.55it/s][A
 21%|██▏       | 128/602 [00:02<00:10, 46.61it/s][A
 22%|██▏       | 133/602 [00:02<00:10, 46.54it/s][A
 23%|██▎       | 138/602 [00:02<00:09, 46.43it/s][A
 24%|██▍       | 143/602 [00:03<00:09, 46.48it/s][A
 25%|██▍       | 148/602 [00:03<00:09, 46.48it/s][A
 25%|██▌       | 153/602 [00:03<00:09, 46.44it/s][A
 26%|██▌       | 158/602 [00:03<00:09, 46.49it/s][A
 27%|██▋       | 163/602 [00:03<00:09, 46.42it/s][A
 28%|██▊       | 168/602 [00:03<00:09, 46.48it/s][A
 29%|██▊       | 173/602 [00:03<00:09, 46.53it/s][A
 30%|██▉       | 178/602 [00:03<00:09, 46.59it/s][A
 30%|███       | 183/602 [00:03<00:09, 46.54it/s][A
 31%|███       | 188/602 [00:04<00:08, 46.55it/s][A
 32%|███▏      | 193/602 [00:04<00:08, 46.43it/s][A
 33%|███▎      | 198/602 [00:04<00:08, 46.43it/s][A
 34%|███▎      | 203/602 [00:04<00:08, 46.48it/s][A
 35%|███▍      | 208/602 [00:04<00:08, 46.50it/s][A
 35%|███▌      | 213/602 [00:04<00:08, 46.42it/s][A
 36%|███▌      | 218/602 [00:04<00:08, 46.51it/s][A
 37%|███▋      | 223/602 [00:04<00:08, 46.54it/s][A
 38%|███▊      | 228/602 [00:04<00:08, 46.56it/s][A
 39%|███▊      | 233/602 [00:04<00:07, 46.56it/s][A
 40%|███▉      | 238/602 [00:05<00:07, 46.52it/s][A
 40%|████      | 243/602 [00:05<00:07, 46.51it/s][A
 41%|████      | 248/602 [00:05<00:07, 46.56it/s][A
 42%|████▏     | 253/602 [00:05<00:07, 46.41it/s][A
 43%|████▎     | 258/602 [00:05<00:07, 46.48it/s][A
 44%|████▎     | 263/602 [00:05<00:07, 46.44it/s][A
 45%|████▍     | 268/602 [00:05<00:07, 46.50it/s][A
 45%|████▌     | 273/602 [00:05<00:07, 46.53it/s][A
 46%|████▌     | 278/602 [00:05<00:06, 46.52it/s][A
 47%|████▋     | 283/602 [00:06<00:06, 46.47it/s][A
 48%|████▊     | 288/602 [00:06<00:06, 46.52it/s][A
 49%|████▊     | 293/602 [00:06<00:06, 46.45it/s][A
 50%|████▉     | 298/602 [00:06<00:06, 46.48it/s][A
 50%|█████     | 303/602 [00:06<00:06, 46.54it/s][A
 51%|█████     | 308/602 [00:06<00:06, 46.49it/s][A
 52%|█████▏    | 313/602 [00:06<00:06, 46.47it/s][A
 53%|█████▎    | 318/602 [00:06<00:06, 46.49it/s][A
 54%|█████▎    | 323/602 [00:06<00:05, 46.50it/s][A
 54%|█████▍    | 328/602 [00:07<00:05, 46.47it/s][A
 55%|█████▌    | 333/602 [00:07<00:05, 46.40it/s][A
 56%|█████▌    | 338/602 [00:07<00:05, 46.39it/s][A
 57%|█████▋    | 343/602 [00:07<00:05, 46.51it/s][A
 58%|█████▊    | 348/602 [00:07<00:05, 46.45it/s][A
 59%|█████▊    | 353/602 [00:07<00:05, 46.50it/s][A
 59%|█████▉    | 358/602 [00:07<00:05, 46.42it/s][A
 60%|██████    | 363/602 [00:07<00:05, 46.46it/s][A
 61%|██████    | 368/602 [00:07<00:05, 46.52it/s][A
 62%|██████▏   | 373/602 [00:08<00:04, 46.49it/s][A
 63%|██████▎   | 378/602 [00:08<00:04, 46.48it/s][A
 64%|██████▎   | 383/602 [00:08<00:04, 46.43it/s][A
 64%|██████▍   | 388/602 [00:08<00:04, 46.52it/s][A
 65%|██████▌   | 393/602 [00:08<00:04, 46.52it/s][A
 66%|██████▌   | 398/602 [00:08<00:04, 46.47it/s][A
 67%|██████▋   | 403/602 [00:08<00:04, 46.47it/s][A
 68%|██████▊   | 408/602 [00:08<00:04, 46.51it/s][A
 69%|██████▊   | 413/602 [00:08<00:04, 46.51it/s][A
 69%|██████▉   | 418/602 [00:08<00:03, 46.51it/s][A
 70%|███████   | 423/602 [00:09<00:03, 46.45it/s][A
 71%|███████   | 428/602 [00:09<00:03, 46.45it/s][A
 72%|███████▏  | 433/602 [00:09<00:03, 46.56it/s][A
 73%|███████▎  | 438/602 [00:09<00:03, 46.54it/s][A
 74%|███████▎  | 443/602 [00:09<00:03, 46.49it/s][A
 74%|███████▍  | 448/602 [00:09<00:03, 46.44it/s][A
 75%|███████▌  | 453/602 [00:09<00:03, 46.51it/s][A
 76%|███████▌  | 458/602 [00:09<00:03, 46.44it/s][A
 77%|███████▋  | 463/602 [00:09<00:02, 46.45it/s][A
 78%|███████▊  | 468/602 [00:10<00:02, 46.44it/s][A
 79%|███████▊  | 473/602 [00:10<00:02, 46.43it/s][A
 79%|███████▉  | 478/602 [00:10<00:02, 46.43it/s][A
 80%|████████  | 483/602 [00:10<00:02, 46.48it/s][A
 81%|████████  | 488/602 [00:10<00:02, 46.55it/s][A
 82%|████████▏ | 493/602 [00:10<00:02, 46.51it/s][A
 83%|████████▎ | 498/602 [00:10<00:02, 46.46it/s][A
 84%|████████▎ | 503/602 [00:10<00:02, 46.56it/s][A
 84%|████████▍ | 508/602 [00:10<00:02, 46.47it/s][A
 85%|████████▌ | 513/602 [00:11<00:01, 46.44it/s][A
 86%|████████▌ | 518/602 [00:11<00:01, 46.44it/s][A
 87%|████████▋ | 523/602 [00:11<00:01, 46.46it/s][A
 88%|████████▊ | 528/602 [00:11<00:01, 46.47it/s][A
 89%|████████▊ | 533/602 [00:11<00:01, 46.48it/s][A
 89%|████████▉ | 538/602 [00:11<00:01, 46.51it/s][A
 90%|█████████ | 543/602 [00:11<00:01, 46.48it/s][A
 91%|█████████ | 548/602 [00:11<00:01, 46.41it/s][A
 92%|█████████▏| 553/602 [00:11<00:01, 46.53it/s][A
 93%|█████████▎| 558/602 [00:11<00:00, 46.52it/s][A
 94%|█████████▎| 563/602 [00:12<00:00, 46.50it/s][A
 94%|█████████▍| 568/602 [00:12<00:00, 46.48it/s][A
 95%|█████████▌| 573/602 [00:12<00:00, 46.46it/s][A
 96%|█████████▌| 578/602 [00:12<00:00, 46.40it/s][A
 97%|█████████▋| 583/602 [00:12<00:00, 46.46it/s][A
 98%|█████████▊| 588/602 [00:12<00:00, 46.47it/s][A
 99%|█████████▊| 593/602 [00:12<00:00, 46.48it/s][A
 99%|█████████▉| 598/602 [00:12<00:00, 46.47it/s][A                                                 
                                                 [A 80%|████████  | 632/790 [04:14<00:43,  3.66it/s]
100%|██████████| 602/602 [00:12<00:00, 46.47it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:57:14,261 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-632
[INFO|configuration_utils.py:351] 2023-08-29 04:57:14,278 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-632/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:57:16,334 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-632/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:57:16,348 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-632/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:57:16,360 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-632/special_tokens_map.json
 80%|████████  | 633/790 [04:21<15:52,  6.07s/it] 80%|████████  | 634/790 [04:21<11:16,  4.33s/it] 80%|████████  | 635/790 [04:21<08:03,  3.12s/it] 81%|████████  | 636/790 [04:22<05:49,  2.27s/it] 81%|████████  | 637/790 [04:22<04:16,  1.68s/it] 81%|████████  | 638/790 [04:22<03:11,  1.26s/it] 81%|████████  | 639/790 [04:23<02:26,  1.03it/s] 81%|████████  | 640/790 [04:23<01:54,  1.31it/s] 81%|████████  | 641/790 [04:23<01:32,  1.61it/s] 81%|████████▏ | 642/790 [04:23<01:17,  1.92it/s] 81%|████████▏ | 643/790 [04:24<01:06,  2.21it/s] 82%|████████▏ | 644/790 [04:24<00:58,  2.48it/s] 82%|████████▏ | 645/790 [04:24<00:53,  2.70it/s] 82%|████████▏ | 646/790 [04:25<00:49,  2.89it/s] 82%|████████▏ | 647/790 [04:25<00:46,  3.04it/s] 82%|████████▏ | 648/790 [04:25<00:44,  3.16it/s] 82%|████████▏ | 649/790 [04:25<00:43,  3.24it/s] 82%|████████▏ | 650/790 [04:26<00:42,  3.31it/s] 82%|████████▏ | 651/790 [04:26<00:41,  3.35it/s] 83%|████████▎ | 652/790 [04:26<00:40,  3.38it/s] 83%|████████▎ | 653/790 [04:27<00:40,  3.40it/s] 83%|████████▎ | 654/790 [04:27<00:39,  3.42it/s] 83%|████████▎ | 655/790 [04:27<00:39,  3.43it/s] 83%|████████▎ | 656/790 [04:28<00:39,  3.43it/s] 83%|████████▎ | 657/790 [04:28<00:38,  3.44it/s] 83%|████████▎ | 658/790 [04:28<00:38,  3.44it/s] 83%|████████▎ | 659/790 [04:28<00:37,  3.45it/s] 84%|████████▎ | 660/790 [04:29<00:37,  3.45it/s] 84%|████████▎ | 661/790 [04:29<00:37,  3.45it/s] 84%|████████▍ | 662/790 [04:29<00:37,  3.46it/s] 84%|████████▍ | 663/790 [04:30<00:36,  3.46it/s] 84%|████████▍ | 664/790 [04:30<00:36,  3.46it/s] 84%|████████▍ | 665/790 [04:30<00:36,  3.46it/s] 84%|████████▍ | 666/790 [04:30<00:35,  3.46it/s] 84%|████████▍ | 667/790 [04:31<00:35,  3.45it/s] 85%|████████▍ | 668/790 [04:31<00:35,  3.45it/s] 85%|████████▍ | 669/790 [04:31<00:35,  3.45it/s] 85%|████████▍ | 670/790 [04:32<00:34,  3.46it/s] 85%|████████▍ | 671/790 [04:32<00:34,  3.46it/s] 85%|████████▌ | 672/790 [04:32<00:34,  3.45it/s] 85%|████████▌ | 673/790 [04:32<00:33,  3.46it/s] 85%|████████▌ | 674/790 [04:33<00:33,  3.46it/s] 85%|████████▌ | 675/790 [04:33<00:33,  3.46it/s] 86%|████████▌ | 676/790 [04:33<00:32,  3.46it/s] 86%|████████▌ | 677/790 [04:34<00:32,  3.46it/s] 86%|████████▌ | 678/790 [04:34<00:32,  3.44it/s] 86%|████████▌ | 679/790 [04:34<00:32,  3.45it/s] 86%|████████▌ | 680/790 [04:34<00:31,  3.45it/s] 86%|████████▌ | 681/790 [04:35<00:31,  3.46it/s] 86%|████████▋ | 682/790 [04:35<00:31,  3.46it/s] 86%|████████▋ | 683/790 [04:35<00:30,  3.45it/s] 87%|████████▋ | 684/790 [04:36<00:30,  3.46it/s] 87%|████████▋ | 685/790 [04:36<00:30,  3.46it/s] 87%|████████▋ | 686/790 [04:36<00:30,  3.46it/s] 87%|████████▋ | 687/790 [04:36<00:29,  3.46it/s] 87%|████████▋ | 688/790 [04:37<00:29,  3.46it/s] 87%|████████▋ | 689/790 [04:37<00:29,  3.45it/s] 87%|████████▋ | 690/790 [04:37<00:28,  3.46it/s] 87%|████████▋ | 691/790 [04:38<00:28,  3.46it/s] 88%|████████▊ | 692/790 [04:38<00:28,  3.46it/s] 88%|████████▊ | 693/790 [04:38<00:28,  3.46it/s] 88%|████████▊ | 694/790 [04:39<00:27,  3.46it/s] 88%|████████▊ | 695/790 [04:39<00:27,  3.46it/s] 88%|████████▊ | 696/790 [04:39<00:27,  3.46it/s] 88%|████████▊ | 697/790 [04:39<00:26,  3.46it/s] 88%|████████▊ | 698/790 [04:40<00:26,  3.46it/s] 88%|████████▊ | 699/790 [04:40<00:26,  3.46it/s] 89%|████████▊ | 700/790 [04:40<00:26,  3.44it/s] 89%|████████▊ | 701/790 [04:41<00:25,  3.45it/s] 89%|████████▉ | 702/790 [04:41<00:25,  3.45it/s] 89%|████████▉ | 703/790 [04:41<00:25,  3.45it/s] 89%|████████▉ | 704/790 [04:41<00:24,  3.46it/s] 89%|████████▉ | 705/790 [04:42<00:24,  3.46it/s] 89%|████████▉ | 706/790 [04:42<00:24,  3.46it/s] 89%|████████▉ | 707/790 [04:42<00:24,  3.46it/s] 90%|████████▉ | 708/790 [04:43<00:23,  3.46it/s] 90%|████████▉ | 709/790 [04:43<00:23,  3.46it/s] 90%|████████▉ | 710/790 [04:43<00:23,  3.46it/s] 90%|█████████ | 711/790 [04:43<00:23,  3.43it/s] 90%|█████████ | 712/790 [04:44<00:22,  3.44it/s] 90%|█████████ | 713/790 [04:44<00:22,  3.45it/s] 90%|█████████ | 714/790 [04:44<00:22,  3.45it/s] 91%|█████████ | 715/790 [04:45<00:21,  3.45it/s] 91%|█████████ | 716/790 [04:45<00:21,  3.46it/s] 91%|█████████ | 717/790 [04:45<00:21,  3.46it/s] 91%|█████████ | 718/790 [04:45<00:20,  3.46it/s] 91%|█████████ | 719/790 [04:46<00:20,  3.46it/s] 91%|█████████ | 720/790 [04:46<00:20,  3.46it/s] 91%|█████████▏| 721/790 [04:46<00:19,  3.46it/s] 91%|█████████▏| 722/790 [04:47<00:19,  3.45it/s] 92%|█████████▏| 723/790 [04:47<00:19,  3.45it/s] 92%|█████████▏| 724/790 [04:47<00:19,  3.45it/s] 92%|█████████▏| 725/790 [04:47<00:18,  3.45it/s] 92%|█████████▏| 726/790 [04:48<00:18,  3.46it/s] 92%|█████████▏| 727/790 [04:48<00:18,  3.46it/s] 92%|█████████▏| 728/790 [04:48<00:17,  3.46it/s] 92%|█████████▏| 729/790 [04:49<00:17,  3.46it/s] 92%|█████████▏| 730/790 [04:49<00:17,  3.46it/s] 93%|█████████▎| 731/790 [04:49<00:17,  3.45it/s] 93%|█████████▎| 732/790 [04:50<00:16,  3.45it/s] 93%|█████████▎| 733/790 [04:50<00:16,  3.45it/s] 93%|█████████▎| 734/790 [04:50<00:16,  3.35it/s] 93%|█████████▎| 735/790 [04:50<00:16,  3.37it/s] 93%|█████████▎| 736/790 [04:51<00:15,  3.40it/s] 93%|█████████▎| 737/790 [04:51<00:15,  3.41it/s] 93%|█████████▎| 738/790 [04:51<00:15,  3.42it/s] 94%|█████████▎| 739/790 [04:52<00:14,  3.43it/s] 94%|█████████▎| 740/790 [04:52<00:14,  3.44it/s] 94%|█████████▍| 741/790 [04:52<00:14,  3.44it/s] 94%|█████████▍| 742/790 [04:52<00:13,  3.44it/s] 94%|█████████▍| 743/790 [04:53<00:13,  3.44it/s] 94%|█████████▍| 744/790 [04:53<00:13,  3.44it/s] 94%|█████████▍| 745/790 [04:53<00:13,  3.44it/s] 94%|█████████▍| 746/790 [04:54<00:12,  3.44it/s] 95%|█████████▍| 747/790 [04:54<00:12,  3.44it/s] 95%|█████████▍| 748/790 [04:54<00:12,  3.44it/s] 95%|█████████▍| 749/790 [04:54<00:11,  3.44it/s] 95%|█████████▍| 750/790 [04:55<00:11,  3.45it/s] 95%|█████████▌| 751/790 [04:55<00:11,  3.45it/s] 95%|█████████▌| 752/790 [04:55<00:11,  3.45it/s] 95%|█████████▌| 753/790 [04:56<00:10,  3.45it/s] 95%|█████████▌| 754/790 [04:56<00:10,  3.45it/s] 96%|█████████▌| 755/790 [04:56<00:10,  3.44it/s] 96%|█████████▌| 756/790 [04:57<00:09,  3.44it/s] 96%|█████████▌| 757/790 [04:57<00:09,  3.45it/s] 96%|█████████▌| 758/790 [04:57<00:09,  3.44it/s] 96%|█████████▌| 759/790 [04:57<00:08,  3.45it/s] 96%|█████████▌| 760/790 [04:58<00:08,  3.45it/s] 96%|█████████▋| 761/790 [04:58<00:08,  3.45it/s] 96%|█████████▋| 762/790 [04:58<00:08,  3.45it/s] 97%|█████████▋| 763/790 [04:59<00:07,  3.45it/s] 97%|█████████▋| 764/790 [04:59<00:07,  3.45it/s] 97%|█████████▋| 765/790 [04:59<00:07,  3.45it/s] 97%|█████████▋| 766/790 [04:59<00:06,  3.44it/s] 97%|█████████▋| 767/790 [05:00<00:06,  3.44it/s] 97%|█████████▋| 768/790 [05:00<00:06,  3.45it/s] 97%|█████████▋| 769/790 [05:00<00:06,  3.45it/s] 97%|█████████▋| 770/790 [05:01<00:05,  3.45it/s] 98%|█████████▊| 771/790 [05:01<00:05,  3.45it/s] 98%|█████████▊| 772/790 [05:01<00:05,  3.45it/s] 98%|█████████▊| 773/790 [05:01<00:04,  3.45it/s] 98%|█████████▊| 774/790 [05:02<00:04,  3.45it/s] 98%|█████████▊| 775/790 [05:02<00:04,  3.45it/s] 98%|█████████▊| 776/790 [05:02<00:04,  3.45it/s] 98%|█████████▊| 777/790 [05:03<00:03,  3.44it/s] 98%|█████████▊| 778/790 [05:03<00:03,  3.44it/s] 99%|█████████▊| 779/790 [05:03<00:03,  3.45it/s] 99%|█████████▊| 780/790 [05:03<00:02,  3.45it/s] 99%|█████████▉| 781/790 [05:04<00:02,  3.45it/s] 99%|█████████▉| 782/790 [05:04<00:02,  3.45it/s] 99%|█████████▉| 783/790 [05:04<00:02,  3.45it/s] 99%|█████████▉| 784/790 [05:05<00:01,  3.45it/s] 99%|█████████▉| 785/790 [05:05<00:01,  3.45it/s] 99%|█████████▉| 786/790 [05:05<00:01,  3.45it/s]100%|█████████▉| 787/790 [05:06<00:00,  3.45it/s]100%|█████████▉| 788/790 [05:06<00:00,  3.44it/s]100%|█████████▉| 789/790 [05:06<00:00,  3.44it/s]100%|██████████| 790/790 [05:06<00:00,  3.66it/s][INFO|trainer.py:2140] 2023-08-29 04:58:06,326 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:58:06,326 >>   Num examples = 4812
[INFO|trainer.py:2145] 2023-08-29 04:58:06,326 >>   Batch size = 8
{'eval_loss': 0.9579627513885498, 'eval_runtime': 12.9454, 'eval_samples_per_second': 371.714, 'eval_steps_per_second': 46.503, 'epoch': 4.0}

  0%|          | 0/602 [00:00<?, ?it/s][A
  1%|          | 6/602 [00:00<00:10, 57.10it/s][A
  2%|▏         | 12/602 [00:00<00:11, 50.23it/s][A
  3%|▎         | 18/602 [00:00<00:12, 48.38it/s][A
  4%|▍         | 23/602 [00:00<00:12, 47.67it/s][A
  5%|▍         | 28/602 [00:00<00:12, 47.35it/s][A
  5%|▌         | 33/602 [00:00<00:12, 47.02it/s][A
  6%|▋         | 38/602 [00:00<00:12, 46.77it/s][A
  7%|▋         | 43/602 [00:00<00:11, 46.63it/s][A
  8%|▊         | 48/602 [00:01<00:11, 46.63it/s][A
  9%|▉         | 53/602 [00:01<00:11, 46.66it/s][A
 10%|▉         | 58/602 [00:01<00:11, 46.68it/s][A
 10%|█         | 63/602 [00:01<00:11, 46.65it/s][A
 11%|█▏        | 68/602 [00:01<00:11, 46.65it/s][A
 12%|█▏        | 73/602 [00:01<00:11, 46.71it/s][A
 13%|█▎        | 78/602 [00:01<00:11, 46.64it/s][A
 14%|█▍        | 83/602 [00:01<00:11, 46.60it/s][A
 15%|█▍        | 88/602 [00:01<00:11, 46.50it/s][A
 15%|█▌        | 93/602 [00:01<00:10, 46.47it/s][A
 16%|█▋        | 98/602 [00:02<00:10, 46.48it/s][A
 17%|█▋        | 103/602 [00:02<00:10, 46.45it/s][A
 18%|█▊        | 108/602 [00:02<00:10, 46.57it/s][A
 19%|█▉        | 113/602 [00:02<00:10, 46.62it/s][A
 20%|█▉        | 118/602 [00:02<00:10, 46.59it/s][A
 20%|██        | 123/602 [00:02<00:10, 46.65it/s][A
 21%|██▏       | 128/602 [00:02<00:10, 46.54it/s][A
 22%|██▏       | 133/602 [00:02<00:10, 46.57it/s][A
 23%|██▎       | 138/602 [00:02<00:09, 46.45it/s][A
 24%|██▍       | 143/602 [00:03<00:09, 46.48it/s][A
 25%|██▍       | 148/602 [00:03<00:09, 46.47it/s][A
 25%|██▌       | 153/602 [00:03<00:09, 46.43it/s][A
 26%|██▌       | 158/602 [00:03<00:09, 46.52it/s][A
 27%|██▋       | 163/602 [00:03<00:09, 46.54it/s][A
 28%|██▊       | 168/602 [00:03<00:09, 46.58it/s][A
 29%|██▊       | 173/602 [00:03<00:09, 46.62it/s][A
 30%|██▉       | 178/602 [00:03<00:09, 46.54it/s][A
 30%|███       | 183/602 [00:03<00:09, 46.53it/s][A
 31%|███       | 188/602 [00:04<00:08, 46.41it/s][A
 32%|███▏      | 193/602 [00:04<00:08, 46.46it/s][A
 33%|███▎      | 198/602 [00:04<00:08, 46.47it/s][A
 34%|███▎      | 203/602 [00:04<00:08, 46.51it/s][A
 35%|███▍      | 208/602 [00:04<00:08, 46.50it/s][A
 35%|███▌      | 213/602 [00:04<00:08, 46.45it/s][A
 36%|███▌      | 218/602 [00:04<00:08, 46.55it/s][A
 37%|███▋      | 223/602 [00:04<00:08, 46.53it/s][A
 38%|███▊      | 228/602 [00:04<00:08, 46.60it/s][A
 39%|███▊      | 233/602 [00:04<00:07, 46.58it/s][A
 40%|███▉      | 238/602 [00:05<00:07, 46.42it/s][A
 40%|████      | 243/602 [00:05<00:07, 46.46it/s][A
 41%|████      | 248/602 [00:05<00:07, 46.46it/s][A
 42%|████▏     | 253/602 [00:05<00:07, 46.52it/s][A
 43%|████▎     | 258/602 [00:05<00:07, 46.53it/s][A
 44%|████▎     | 263/602 [00:05<00:07, 46.48it/s][A
 45%|████▍     | 268/602 [00:05<00:07, 46.46it/s][A
 45%|████▌     | 273/602 [00:05<00:07, 46.45it/s][A
 46%|████▌     | 278/602 [00:05<00:06, 46.48it/s][A
 47%|████▋     | 283/602 [00:06<00:06, 46.52it/s][A
 48%|████▊     | 288/602 [00:06<00:06, 46.44it/s][A
 49%|████▊     | 293/602 [00:06<00:06, 46.48it/s][A
 50%|████▉     | 298/602 [00:06<00:06, 46.48it/s][A
 50%|█████     | 303/602 [00:06<00:06, 46.51it/s][A
 51%|█████     | 308/602 [00:06<00:06, 46.50it/s][A
 52%|█████▏    | 313/602 [00:06<00:06, 46.54it/s][A
 53%|█████▎    | 318/602 [00:06<00:06, 46.57it/s][A
 54%|█████▎    | 323/602 [00:06<00:05, 46.56it/s][A
 54%|█████▍    | 328/602 [00:07<00:05, 46.59it/s][A
 55%|█████▌    | 333/602 [00:07<00:05, 46.49it/s][A
 56%|█████▌    | 338/602 [00:07<00:05, 46.47it/s][A
 57%|█████▋    | 343/602 [00:07<00:05, 46.43it/s][A
 58%|█████▊    | 348/602 [00:07<00:05, 46.51it/s][A
 59%|█████▊    | 353/602 [00:07<00:05, 46.49it/s][A
 59%|█████▉    | 358/602 [00:07<00:05, 46.54it/s][A
 60%|██████    | 363/602 [00:07<00:05, 46.51it/s][A
 61%|██████    | 368/602 [00:07<00:05, 46.44it/s][A
 62%|██████▏   | 373/602 [00:07<00:04, 46.51it/s][A
 63%|██████▎   | 378/602 [00:08<00:04, 46.52it/s][A
 64%|██████▎   | 383/602 [00:08<00:04, 46.51it/s][A
 64%|██████▍   | 388/602 [00:08<00:04, 46.49it/s][A
 65%|██████▌   | 393/602 [00:08<00:04, 46.51it/s][A
 66%|██████▌   | 398/602 [00:08<00:04, 46.46it/s][A
 67%|██████▋   | 403/602 [00:08<00:04, 46.52it/s][A
 68%|██████▊   | 408/602 [00:08<00:04, 46.54it/s][A
 69%|██████▊   | 413/602 [00:08<00:04, 46.49it/s][A
 69%|██████▉   | 418/602 [00:08<00:03, 46.48it/s][A
 70%|███████   | 423/602 [00:09<00:03, 46.54it/s][A
 71%|███████   | 428/602 [00:09<00:03, 46.51it/s][A
 72%|███████▏  | 433/602 [00:09<00:03, 46.46it/s][A
 73%|███████▎  | 438/602 [00:09<00:03, 46.47it/s][A
 74%|███████▎  | 443/602 [00:09<00:03, 46.52it/s][A
 74%|███████▍  | 448/602 [00:09<00:03, 46.56it/s][A
 75%|███████▌  | 453/602 [00:09<00:03, 46.62it/s][A
 76%|███████▌  | 458/602 [00:09<00:03, 46.50it/s][A
 77%|███████▋  | 463/602 [00:09<00:02, 46.48it/s][A
 78%|███████▊  | 468/602 [00:10<00:02, 46.53it/s][A
 79%|███████▊  | 473/602 [00:10<00:02, 46.47it/s][A
 79%|███████▉  | 478/602 [00:10<00:02, 46.53it/s][A
 80%|████████  | 483/602 [00:10<00:02, 46.51it/s][A
 81%|████████  | 488/602 [00:10<00:02, 46.36it/s][A
 82%|████████▏ | 493/602 [00:10<00:02, 46.49it/s][A
 83%|████████▎ | 498/602 [00:10<00:02, 46.54it/s][A
 84%|████████▎ | 503/602 [00:10<00:02, 46.47it/s][A
 84%|████████▍ | 508/602 [00:10<00:02, 46.52it/s][A
 85%|████████▌ | 513/602 [00:11<00:01, 46.53it/s][A
 86%|████████▌ | 518/602 [00:11<00:01, 46.45it/s][A
 87%|████████▋ | 523/602 [00:11<00:01, 46.52it/s][A
 88%|████████▊ | 528/602 [00:11<00:01, 46.54it/s][A
 89%|████████▊ | 533/602 [00:11<00:01, 46.51it/s][A
 89%|████████▉ | 538/602 [00:11<00:01, 46.44it/s][A
 90%|█████████ | 543/602 [00:11<00:01, 46.48it/s][A
 91%|█████████ | 548/602 [00:11<00:01, 46.44it/s][A
 92%|█████████▏| 553/602 [00:11<00:01, 46.47it/s][A
 93%|█████████▎| 558/602 [00:11<00:00, 46.51it/s][A
 94%|█████████▎| 563/602 [00:12<00:00, 46.51it/s][A
 94%|█████████▍| 568/602 [00:12<00:00, 46.47it/s][A
 95%|█████████▌| 573/602 [00:12<00:00, 46.50it/s][A
 96%|█████████▌| 578/602 [00:12<00:00, 46.52it/s][A
 97%|█████████▋| 583/602 [00:12<00:00, 46.57it/s][A
 98%|█████████▊| 588/602 [00:12<00:00, 46.53it/s][A
 99%|█████████▊| 593/602 [00:12<00:00, 46.42it/s][A
 99%|█████████▉| 598/602 [00:12<00:00, 46.48it/s][A                                                 
                                                 [A100%|██████████| 790/790 [05:19<00:00,  3.66it/s]
100%|██████████| 602/602 [00:12<00:00, 46.48it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:58:19,284 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-790
[INFO|configuration_utils.py:351] 2023-08-29 04:58:19,302 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-790/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:58:21,313 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-790/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:58:21,328 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-790/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:58:21,340 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-790/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 04:58:25,556 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 04:58:25,558 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-158 (score: 0.927378237247467).
                                                 100%|██████████| 790/790 [05:27<00:00,  3.66it/s]100%|██████████| 790/790 [05:27<00:00,  2.41it/s]
[INFO|trainer.py:1894] 2023-08-29 04:58:27,298 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 04:58:27,315 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:58:29,626 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:58:29,640 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:58:29,654 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 04:58:29,833 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:58:29,833 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:58:29,833 >>   train_loss               =     0.6069
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:58:29,833 >>   train_runtime            = 0:05:27.78
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:58:29,833 >>   train_samples            =      10097
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:58:29,833 >>   train_samples_per_second =    154.017
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:58:29,833 >>   train_steps_per_second   =       2.41
{'eval_loss': 0.9626830220222473, 'eval_runtime': 12.9389, 'eval_samples_per_second': 371.903, 'eval_steps_per_second': 46.527, 'epoch': 5.0}
{'train_runtime': 327.789, 'train_samples_per_second': 154.017, 'train_steps_per_second': 2.41, 'train_loss': 0.6069208072710641, 'epoch': 5.0}
08/29/2023 04:58:29 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 04:58:29,879 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:58:29,879 >>   Num examples = 4812
[INFO|trainer.py:2145] 2023-08-29 04:58:29,879 >>   Batch size = 8
  0%|          | 0/602 [00:00<?, ?it/s]  1%|          | 6/602 [00:00<00:10, 57.59it/s]  2%|▏         | 12/602 [00:00<00:11, 50.86it/s]  3%|▎         | 18/602 [00:00<00:11, 49.04it/s]  4%|▍         | 23/602 [00:00<00:11, 48.26it/s]  5%|▍         | 28/602 [00:00<00:12, 47.77it/s]  5%|▌         | 33/602 [00:00<00:11, 47.51it/s]  6%|▋         | 38/602 [00:00<00:11, 47.23it/s]  7%|▋         | 43/602 [00:00<00:11, 47.00it/s]  8%|▊         | 48/602 [00:01<00:11, 46.74it/s]  9%|▉         | 53/602 [00:01<00:11, 46.68it/s] 10%|▉         | 58/602 [00:01<00:11, 46.64it/s] 10%|█         | 63/602 [00:01<00:11, 46.72it/s] 11%|█▏        | 68/602 [00:01<00:11, 46.80it/s] 12%|█▏        | 73/602 [00:01<00:11, 46.78it/s] 13%|█▎        | 78/602 [00:01<00:11, 46.74it/s] 14%|█▍        | 83/602 [00:01<00:11, 46.81it/s] 15%|█▍        | 88/602 [00:01<00:10, 46.74it/s] 15%|█▌        | 93/602 [00:01<00:10, 46.64it/s] 16%|█▋        | 98/602 [00:02<00:10, 46.62it/s] 17%|█▋        | 103/602 [00:02<00:10, 46.59it/s] 18%|█▊        | 108/602 [00:02<00:10, 46.58it/s] 19%|█▉        | 113/602 [00:02<00:10, 46.62it/s] 20%|█▉        | 118/602 [00:02<00:10, 46.71it/s] 20%|██        | 123/602 [00:02<00:10, 46.68it/s] 21%|██▏       | 128/602 [00:02<00:10, 46.72it/s] 22%|██▏       | 133/602 [00:02<00:10, 46.72it/s] 23%|██▎       | 138/602 [00:02<00:09, 46.77it/s] 24%|██▍       | 143/602 [00:03<00:09, 46.73it/s] 25%|██▍       | 148/602 [00:03<00:09, 46.66it/s] 25%|██▌       | 153/602 [00:03<00:09, 46.77it/s] 26%|██▌       | 158/602 [00:03<00:09, 46.87it/s] 27%|██▋       | 163/602 [00:03<00:09, 46.78it/s] 28%|██▊       | 168/602 [00:03<00:09, 46.89it/s] 29%|██▊       | 173/602 [00:03<00:09, 46.83it/s] 30%|██▉       | 178/602 [00:03<00:09, 46.75it/s] 30%|███       | 183/602 [00:03<00:08, 46.79it/s] 31%|███       | 188/602 [00:03<00:08, 46.73it/s] 32%|███▏      | 193/602 [00:04<00:08, 46.72it/s] 33%|███▎      | 198/602 [00:04<00:08, 46.80it/s] 34%|███▎      | 203/602 [00:04<00:08, 46.77it/s] 35%|███▍      | 208/602 [00:04<00:08, 46.76it/s] 35%|███▌      | 213/602 [00:04<00:08, 46.86it/s] 36%|███▌      | 218/602 [00:04<00:08, 46.83it/s] 37%|███▋      | 223/602 [00:04<00:08, 46.75it/s] 38%|███▊      | 228/602 [00:04<00:07, 46.81it/s] 39%|███▊      | 233/602 [00:04<00:07, 46.75it/s] 40%|███▉      | 238/602 [00:05<00:07, 46.70it/s] 40%|████      | 243/602 [00:05<00:07, 46.73it/s] 41%|████      | 248/602 [00:05<00:07, 46.71it/s] 42%|████▏     | 253/602 [00:05<00:07, 46.80it/s] 43%|████▎     | 258/602 [00:05<00:07, 46.76it/s] 44%|████▎     | 263/602 [00:05<00:07, 46.75it/s] 45%|████▍     | 268/602 [00:05<00:07, 46.71it/s] 45%|████▌     | 273/602 [00:05<00:07, 46.78it/s] 46%|████▌     | 278/602 [00:05<00:06, 46.72it/s] 47%|████▋     | 283/602 [00:06<00:06, 46.76it/s] 48%|████▊     | 288/602 [00:06<00:06, 46.64it/s] 49%|████▊     | 293/602 [00:06<00:06, 46.71it/s] 50%|████▉     | 298/602 [00:06<00:06, 46.71it/s] 50%|█████     | 303/602 [00:06<00:06, 46.77it/s] 51%|█████     | 308/602 [00:06<00:06, 46.72it/s] 52%|█████▏    | 313/602 [00:06<00:06, 46.83it/s] 53%|█████▎    | 318/602 [00:06<00:06, 46.76it/s] 54%|█████▎    | 323/602 [00:06<00:05, 46.66it/s] 54%|█████▍    | 328/602 [00:06<00:05, 46.71it/s] 55%|█████▌    | 333/602 [00:07<00:05, 46.69it/s] 56%|█████▌    | 338/602 [00:07<00:05, 46.75it/s] 57%|█████▋    | 343/602 [00:07<00:05, 46.82it/s] 58%|█████▊    | 348/602 [00:07<00:05, 46.83it/s] 59%|█████▊    | 353/602 [00:07<00:05, 46.73it/s] 59%|█████▉    | 358/602 [00:07<00:05, 46.69it/s] 60%|██████    | 363/602 [00:07<00:05, 46.63it/s] 61%|██████    | 368/602 [00:07<00:05, 46.72it/s] 62%|██████▏   | 373/602 [00:07<00:04, 46.75it/s] 63%|██████▎   | 378/602 [00:08<00:04, 46.72it/s] 64%|██████▎   | 383/602 [00:08<00:04, 46.75it/s] 64%|██████▍   | 388/602 [00:08<00:04, 46.82it/s] 65%|██████▌   | 393/602 [00:08<00:04, 46.76it/s] 66%|██████▌   | 398/602 [00:08<00:04, 46.78it/s] 67%|██████▋   | 403/602 [00:08<00:04, 46.70it/s] 68%|██████▊   | 408/602 [00:08<00:04, 46.70it/s] 69%|██████▊   | 413/602 [00:08<00:04, 46.74it/s] 69%|██████▉   | 418/602 [00:08<00:03, 46.76it/s] 70%|███████   | 423/602 [00:09<00:03, 46.67it/s] 71%|███████   | 428/602 [00:09<00:03, 46.70it/s] 72%|███████▏  | 433/602 [00:09<00:03, 46.75it/s] 73%|███████▎  | 438/602 [00:09<00:03, 46.76it/s] 74%|███████▎  | 443/602 [00:09<00:03, 46.75it/s] 74%|███████▍  | 448/602 [00:09<00:03, 46.68it/s] 75%|███████▌  | 453/602 [00:09<00:03, 46.64it/s] 76%|███████▌  | 458/602 [00:09<00:03, 46.67it/s] 77%|███████▋  | 463/602 [00:09<00:02, 46.69it/s] 78%|███████▊  | 468/602 [00:09<00:02, 46.74it/s] 79%|███████▊  | 473/602 [00:10<00:02, 46.73it/s] 79%|███████▉  | 478/602 [00:10<00:02, 46.70it/s] 80%|████████  | 483/602 [00:10<00:02, 46.70it/s] 81%|████████  | 488/602 [00:10<00:02, 46.77it/s] 82%|████████▏ | 493/602 [00:10<00:02, 46.73it/s] 83%|████████▎ | 498/602 [00:10<00:02, 46.68it/s] 84%|████████▎ | 503/602 [00:10<00:02, 46.68it/s] 84%|████████▍ | 508/602 [00:10<00:02, 46.69it/s] 85%|████████▌ | 513/602 [00:10<00:01, 46.72it/s] 86%|████████▌ | 518/602 [00:11<00:01, 46.70it/s] 87%|████████▋ | 523/602 [00:11<00:01, 46.72it/s] 88%|████████▊ | 528/602 [00:11<00:01, 46.74it/s] 89%|████████▊ | 533/602 [00:11<00:01, 46.78it/s] 89%|████████▉ | 538/602 [00:11<00:01, 46.79it/s] 90%|█████████ | 543/602 [00:11<00:01, 46.78it/s] 91%|█████████ | 548/602 [00:11<00:01, 46.68it/s] 92%|█████████▏| 553/602 [00:11<00:01, 46.66it/s] 93%|█████████▎| 558/602 [00:11<00:00, 46.72it/s] 94%|█████████▎| 563/602 [00:12<00:00, 46.68it/s] 94%|█████████▍| 568/602 [00:12<00:00, 46.73it/s] 95%|█████████▌| 573/602 [00:12<00:00, 46.72it/s] 96%|█████████▌| 578/602 [00:12<00:00, 46.68it/s] 97%|█████████▋| 583/602 [00:12<00:00, 46.77it/s] 98%|█████████▊| 588/602 [00:12<00:00, 46.79it/s] 99%|█████████▊| 593/602 [00:12<00:00, 46.75it/s] 99%|█████████▉| 598/602 [00:12<00:00, 46.75it/s]100%|██████████| 602/602 [00:12<00:00, 46.81it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 04:58:42,762 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:58:42,762 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:58:42,762 >>   eval_loss               =     0.9274
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:58:42,762 >>   eval_runtime            = 0:00:12.88
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:58:42,762 >>   eval_samples            =       4812
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:58:42,762 >>   eval_samples_per_second =    373.522
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:58:42,762 >>   eval_steps_per_second   =     46.729
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:58:42,762 >>   perplexity              =     2.5279
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:58:49,715 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:58:49,717 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:58:49,717 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:58:49,717 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:58:49,717 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:58:50,348 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:58:50,349 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:58:50,916 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:58:51,996 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:58:51,996 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:58:54,805 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:58:54,812 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:58:54,812 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:58:54,812 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:58:54,812 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:58:55,427 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:58:55,429 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:58:56,020 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:58:56,186 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:58:56,186 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-632
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-474
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-790
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-158
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/checkpoint-316
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'given name', 'participant in', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14118
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14218, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.27it/s]Extractor Predicting: 2it [00:01,  1.44it/s]Extractor Predicting: 3it [00:02,  1.50it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.49it/s]Extractor Predicting: 6it [00:04,  1.52it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:06,  1.49it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.50it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:10,  1.52it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.52it/s]Extractor Predicting: 18it [00:12,  1.50it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.54it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.52it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:17,  1.56it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:19,  1.53it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:20,  1.52it/s]Extractor Predicting: 32it [00:21,  1.52it/s]Extractor Predicting: 33it [00:21,  1.50it/s]Extractor Predicting: 34it [00:22,  1.49it/s]Extractor Predicting: 35it [00:23,  1.50it/s]Extractor Predicting: 36it [00:23,  1.52it/s]Extractor Predicting: 37it [00:24,  1.51it/s]Extractor Predicting: 38it [00:25,  1.51it/s]Extractor Predicting: 39it [00:25,  1.45it/s]Extractor Predicting: 40it [00:26,  1.46it/s]Extractor Predicting: 41it [00:27,  1.52it/s]Extractor Predicting: 42it [00:27,  1.59it/s]Extractor Predicting: 43it [00:28,  1.58it/s]Extractor Predicting: 44it [00:29,  1.53it/s]Extractor Predicting: 45it [00:29,  1.56it/s]Extractor Predicting: 46it [00:30,  1.58it/s]Extractor Predicting: 47it [00:30,  1.57it/s]Extractor Predicting: 48it [00:31,  1.58it/s]Extractor Predicting: 49it [00:32,  1.59it/s]Extractor Predicting: 50it [00:32,  1.57it/s]Extractor Predicting: 51it [00:33,  1.61it/s]Extractor Predicting: 52it [00:34,  1.63it/s]Extractor Predicting: 53it [00:34,  1.63it/s]Extractor Predicting: 54it [00:35,  1.66it/s]Extractor Predicting: 55it [00:35,  1.66it/s]Extractor Predicting: 56it [00:36,  1.67it/s]Extractor Predicting: 57it [00:37,  1.64it/s]Extractor Predicting: 58it [00:37,  1.61it/s]Extractor Predicting: 59it [00:38,  1.62it/s]Extractor Predicting: 60it [00:38,  1.61it/s]Extractor Predicting: 61it [00:39,  1.61it/s]Extractor Predicting: 62it [00:40,  1.61it/s]Extractor Predicting: 63it [00:40,  1.62it/s]Extractor Predicting: 64it [00:41,  1.65it/s]Extractor Predicting: 65it [00:41,  1.64it/s]Extractor Predicting: 66it [00:42,  1.61it/s]Extractor Predicting: 67it [00:43,  1.60it/s]Extractor Predicting: 68it [00:43,  1.58it/s]Extractor Predicting: 69it [00:44,  1.63it/s]Extractor Predicting: 70it [00:45,  1.62it/s]Extractor Predicting: 71it [00:45,  1.59it/s]Extractor Predicting: 72it [00:46,  1.60it/s]Extractor Predicting: 73it [00:47,  1.58it/s]Extractor Predicting: 74it [00:47,  1.60it/s]Extractor Predicting: 75it [00:48,  1.59it/s]Extractor Predicting: 76it [00:48,  1.58it/s]Extractor Predicting: 77it [00:49,  1.60it/s]Extractor Predicting: 78it [00:50,  1.59it/s]Extractor Predicting: 79it [00:50,  1.46it/s]Extractor Predicting: 80it [00:51,  1.48it/s]Extractor Predicting: 81it [00:52,  1.51it/s]Extractor Predicting: 82it [00:52,  1.50it/s]Extractor Predicting: 83it [00:53,  1.52it/s]Extractor Predicting: 84it [00:54,  1.55it/s]Extractor Predicting: 85it [00:54,  1.53it/s]Extractor Predicting: 86it [00:55,  1.54it/s]Extractor Predicting: 87it [00:56,  1.55it/s]Extractor Predicting: 88it [00:56,  1.55it/s]Extractor Predicting: 89it [00:57,  1.58it/s]Extractor Predicting: 90it [00:58,  1.42it/s]Extractor Predicting: 91it [00:58,  1.41it/s]Extractor Predicting: 92it [00:59,  1.45it/s]Extractor Predicting: 93it [01:00,  1.44it/s]Extractor Predicting: 94it [01:00,  1.46it/s]Extractor Predicting: 95it [01:01,  1.44it/s]Extractor Predicting: 96it [01:02,  1.45it/s]Extractor Predicting: 97it [01:03,  1.41it/s]Extractor Predicting: 98it [01:03,  1.42it/s]Extractor Predicting: 99it [01:04,  1.45it/s]Extractor Predicting: 100it [01:05,  1.46it/s]Extractor Predicting: 101it [01:05,  1.48it/s]Extractor Predicting: 102it [01:06,  1.48it/s]Extractor Predicting: 103it [01:07,  1.48it/s]Extractor Predicting: 104it [01:07,  1.47it/s]Extractor Predicting: 105it [01:08,  1.46it/s]Extractor Predicting: 106it [01:09,  1.47it/s]Extractor Predicting: 107it [01:09,  1.44it/s]Extractor Predicting: 108it [01:10,  1.46it/s]Extractor Predicting: 109it [01:11,  1.51it/s]Extractor Predicting: 110it [01:11,  1.49it/s]Extractor Predicting: 111it [01:12,  1.49it/s]Extractor Predicting: 112it [01:13,  1.50it/s]Extractor Predicting: 113it [01:13,  1.48it/s]Extractor Predicting: 114it [01:14,  1.47it/s]Extractor Predicting: 115it [01:15,  1.45it/s]Extractor Predicting: 116it [01:15,  1.46it/s]Extractor Predicting: 117it [01:16,  1.45it/s]Extractor Predicting: 118it [01:17,  1.46it/s]Extractor Predicting: 119it [01:18,  1.47it/s]Extractor Predicting: 120it [01:18,  1.47it/s]Extractor Predicting: 121it [01:19,  1.45it/s]Extractor Predicting: 122it [01:20,  1.50it/s]Extractor Predicting: 123it [01:20,  1.52it/s]Extractor Predicting: 124it [01:21,  1.48it/s]Extractor Predicting: 125it [01:22,  1.49it/s]Extractor Predicting: 126it [01:22,  1.47it/s]Extractor Predicting: 127it [01:23,  1.46it/s]Extractor Predicting: 128it [01:24,  1.44it/s]Extractor Predicting: 129it [01:24,  1.46it/s]Extractor Predicting: 130it [01:25,  1.44it/s]Extractor Predicting: 131it [01:26,  1.45it/s]Extractor Predicting: 132it [01:26,  1.44it/s]Extractor Predicting: 133it [01:27,  1.41it/s]Extractor Predicting: 134it [01:28,  1.39it/s]Extractor Predicting: 135it [01:29,  1.36it/s]Extractor Predicting: 136it [01:29,  1.36it/s]Extractor Predicting: 137it [01:30,  1.33it/s]Extractor Predicting: 138it [01:31,  1.33it/s]Extractor Predicting: 139it [01:32,  1.34it/s]Extractor Predicting: 140it [01:32,  1.36it/s]Extractor Predicting: 141it [01:33,  1.39it/s]Extractor Predicting: 142it [01:34,  1.38it/s]Extractor Predicting: 143it [01:35,  1.39it/s]Extractor Predicting: 144it [01:35,  1.37it/s]Extractor Predicting: 145it [01:36,  1.37it/s]Extractor Predicting: 146it [01:37,  1.35it/s]Extractor Predicting: 147it [01:37,  1.38it/s]Extractor Predicting: 148it [01:38,  1.39it/s]Extractor Predicting: 149it [01:39,  1.37it/s]Extractor Predicting: 150it [01:40,  1.38it/s]Extractor Predicting: 151it [01:40,  1.38it/s]Extractor Predicting: 152it [01:41,  1.38it/s]Extractor Predicting: 153it [01:42,  1.38it/s]Extractor Predicting: 154it [01:43,  1.36it/s]Extractor Predicting: 155it [01:43,  1.37it/s]Extractor Predicting: 156it [01:44,  1.35it/s]Extractor Predicting: 157it [01:45,  1.33it/s]Extractor Predicting: 158it [01:46,  1.32it/s]Extractor Predicting: 159it [01:46,  1.36it/s]Extractor Predicting: 160it [01:47,  1.39it/s]Extractor Predicting: 161it [01:48,  1.42it/s]Extractor Predicting: 162it [01:49,  1.30it/s]Extractor Predicting: 163it [01:49,  1.34it/s]Extractor Predicting: 164it [01:50,  1.39it/s]Extractor Predicting: 165it [01:51,  1.41it/s]Extractor Predicting: 166it [01:51,  1.45it/s]Extractor Predicting: 167it [01:52,  1.45it/s]Extractor Predicting: 168it [01:53,  1.45it/s]Extractor Predicting: 169it [01:53,  1.47it/s]Extractor Predicting: 170it [01:54,  1.48it/s]Extractor Predicting: 171it [01:55,  1.48it/s]Extractor Predicting: 172it [01:55,  1.50it/s]Extractor Predicting: 173it [01:56,  1.47it/s]Extractor Predicting: 174it [01:57,  1.49it/s]Extractor Predicting: 175it [01:57,  1.46it/s]Extractor Predicting: 176it [01:58,  1.45it/s]Extractor Predicting: 177it [01:59,  1.46it/s]Extractor Predicting: 178it [01:59,  1.46it/s]Extractor Predicting: 179it [02:00,  1.45it/s]Extractor Predicting: 180it [02:01,  1.46it/s]Extractor Predicting: 181it [02:01,  1.45it/s]Extractor Predicting: 182it [02:02,  1.44it/s]Extractor Predicting: 183it [02:03,  1.42it/s]Extractor Predicting: 184it [02:04,  1.43it/s]Extractor Predicting: 185it [02:04,  1.61it/s]Extractor Predicting: 185it [02:04,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:13,383 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:13,388 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:13,388 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:13,388 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:13,388 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:01:14,001 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:01:14,002 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:01:14,558 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:01:15,610 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:01:15,611 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:18,479 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:18,485 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:18,485 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:18,485 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:01:18,485 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:01:19,136 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:01:19,137 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:01:19,838 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:01:20,005 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:01:20,005 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.581986143187067,
  "recall": 0.10473815461346633,
  "score": 0.17752729834448747,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'employer', 'field of work', 'languages spoken, written or signed', 'lyrics by', 'manufacturer', 'member of sports team', 'occupation', 'performer', 'residence', 'shares border with', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 31765
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31865, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.56it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.55it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.50it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:13,  1.48it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:17,  1.52it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:19,  1.63it/s]Extractor Predicting: 31it [00:20,  1.65it/s]Extractor Predicting: 32it [00:20,  1.66it/s]Extractor Predicting: 33it [00:21,  1.64it/s]Extractor Predicting: 34it [00:21,  1.65it/s]Extractor Predicting: 35it [00:22,  1.63it/s]Extractor Predicting: 36it [00:23,  1.62it/s]Extractor Predicting: 37it [00:23,  1.61it/s]Extractor Predicting: 38it [00:24,  1.61it/s]Extractor Predicting: 39it [00:25,  1.58it/s]Extractor Predicting: 40it [00:25,  1.57it/s]Extractor Predicting: 41it [00:26,  1.60it/s]Extractor Predicting: 42it [00:26,  1.60it/s]Extractor Predicting: 43it [00:27,  1.61it/s]Extractor Predicting: 44it [00:28,  1.63it/s]Extractor Predicting: 45it [00:28,  1.64it/s]Extractor Predicting: 46it [00:29,  1.66it/s]Extractor Predicting: 47it [00:29,  1.68it/s]Extractor Predicting: 48it [00:30,  1.64it/s]Extractor Predicting: 49it [00:31,  1.66it/s]Extractor Predicting: 50it [00:31,  1.63it/s]Extractor Predicting: 51it [00:32,  1.62it/s]Extractor Predicting: 52it [00:32,  1.63it/s]Extractor Predicting: 53it [00:33,  1.64it/s]Extractor Predicting: 54it [00:34,  1.58it/s]Extractor Predicting: 55it [00:34,  1.59it/s]Extractor Predicting: 56it [00:35,  1.57it/s]Extractor Predicting: 57it [00:36,  1.63it/s]Extractor Predicting: 58it [00:36,  1.68it/s]Extractor Predicting: 59it [00:37,  1.64it/s]Extractor Predicting: 60it [00:37,  1.60it/s]Extractor Predicting: 61it [00:38,  1.57it/s]Extractor Predicting: 62it [00:39,  1.56it/s]Extractor Predicting: 63it [00:39,  1.53it/s]Extractor Predicting: 64it [00:40,  1.52it/s]Extractor Predicting: 65it [00:41,  1.54it/s]Extractor Predicting: 66it [00:41,  1.54it/s]Extractor Predicting: 67it [00:42,  1.53it/s]Extractor Predicting: 68it [00:43,  1.50it/s]Extractor Predicting: 69it [00:43,  1.53it/s]Extractor Predicting: 70it [00:44,  1.55it/s]Extractor Predicting: 71it [00:45,  1.54it/s]Extractor Predicting: 72it [00:46,  1.35it/s]Extractor Predicting: 73it [00:46,  1.38it/s]Extractor Predicting: 74it [00:47,  1.40it/s]Extractor Predicting: 75it [00:48,  1.43it/s]Extractor Predicting: 76it [00:48,  1.43it/s]Extractor Predicting: 77it [00:49,  1.47it/s]Extractor Predicting: 78it [00:50,  1.51it/s]Extractor Predicting: 79it [00:50,  1.51it/s]Extractor Predicting: 80it [00:51,  1.55it/s]Extractor Predicting: 81it [00:52,  1.54it/s]Extractor Predicting: 82it [00:52,  1.52it/s]Extractor Predicting: 83it [00:53,  1.54it/s]Extractor Predicting: 84it [00:54,  1.54it/s]Extractor Predicting: 85it [00:54,  1.53it/s]Extractor Predicting: 86it [00:55,  1.53it/s]Extractor Predicting: 87it [00:56,  1.52it/s]Extractor Predicting: 88it [00:56,  1.50it/s]Extractor Predicting: 89it [00:57,  1.54it/s]Extractor Predicting: 90it [00:57,  1.55it/s]Extractor Predicting: 91it [00:58,  1.53it/s]Extractor Predicting: 92it [00:59,  1.49it/s]Extractor Predicting: 93it [00:59,  1.50it/s]Extractor Predicting: 94it [01:00,  1.54it/s]Extractor Predicting: 95it [01:01,  1.53it/s]Extractor Predicting: 96it [01:01,  1.54it/s]Extractor Predicting: 97it [01:02,  1.53it/s]Extractor Predicting: 98it [01:03,  1.48it/s]Extractor Predicting: 99it [01:03,  1.46it/s]Extractor Predicting: 100it [01:04,  1.44it/s]Extractor Predicting: 101it [01:05,  1.44it/s]Extractor Predicting: 102it [01:06,  1.45it/s]Extractor Predicting: 103it [01:06,  1.42it/s]Extractor Predicting: 104it [01:07,  1.48it/s]Extractor Predicting: 105it [01:08,  1.46it/s]Extractor Predicting: 106it [01:08,  1.50it/s]Extractor Predicting: 107it [01:09,  1.50it/s]Extractor Predicting: 108it [01:10,  1.51it/s]Extractor Predicting: 109it [01:10,  1.51it/s]Extractor Predicting: 110it [01:11,  1.49it/s]Extractor Predicting: 111it [01:12,  1.53it/s]Extractor Predicting: 112it [01:12,  1.53it/s]Extractor Predicting: 113it [01:13,  1.53it/s]Extractor Predicting: 114it [01:13,  1.54it/s]Extractor Predicting: 115it [01:14,  1.54it/s]Extractor Predicting: 116it [01:15,  1.54it/s]Extractor Predicting: 117it [01:15,  1.56it/s]Extractor Predicting: 118it [01:16,  1.60it/s]Extractor Predicting: 119it [01:17,  1.66it/s]Extractor Predicting: 120it [01:17,  1.61it/s]Extractor Predicting: 121it [01:18,  1.57it/s]Extractor Predicting: 122it [01:19,  1.58it/s]Extractor Predicting: 123it [01:19,  1.54it/s]Extractor Predicting: 124it [01:20,  1.55it/s]Extractor Predicting: 125it [01:20,  1.53it/s]Extractor Predicting: 126it [01:21,  1.53it/s]Extractor Predicting: 127it [01:22,  1.50it/s]Extractor Predicting: 128it [01:22,  1.52it/s]Extractor Predicting: 129it [01:23,  1.51it/s]Extractor Predicting: 130it [01:24,  1.51it/s]Extractor Predicting: 131it [01:24,  1.51it/s]Extractor Predicting: 132it [01:25,  1.46it/s]Extractor Predicting: 133it [01:26,  1.47it/s]Extractor Predicting: 134it [01:27,  1.49it/s]Extractor Predicting: 135it [01:27,  1.45it/s]Extractor Predicting: 136it [01:28,  1.48it/s]Extractor Predicting: 137it [01:29,  1.51it/s]Extractor Predicting: 138it [01:29,  1.49it/s]Extractor Predicting: 139it [01:30,  1.50it/s]Extractor Predicting: 140it [01:31,  1.47it/s]Extractor Predicting: 141it [01:31,  1.44it/s]Extractor Predicting: 142it [01:32,  1.46it/s]Extractor Predicting: 143it [01:33,  1.49it/s]Extractor Predicting: 144it [01:33,  1.50it/s]Extractor Predicting: 145it [01:34,  1.48it/s]Extractor Predicting: 146it [01:35,  1.47it/s]Extractor Predicting: 147it [01:35,  1.48it/s]Extractor Predicting: 148it [01:36,  1.47it/s]Extractor Predicting: 149it [01:37,  1.48it/s]Extractor Predicting: 150it [01:37,  1.49it/s]Extractor Predicting: 151it [01:38,  1.49it/s]Extractor Predicting: 152it [01:39,  1.52it/s]Extractor Predicting: 153it [01:39,  1.53it/s]Extractor Predicting: 154it [01:40,  1.52it/s]Extractor Predicting: 155it [01:41,  1.47it/s]Extractor Predicting: 156it [01:41,  1.48it/s]Extractor Predicting: 157it [01:42,  1.45it/s]Extractor Predicting: 158it [01:43,  1.46it/s]Extractor Predicting: 159it [01:43,  1.45it/s]Extractor Predicting: 160it [01:44,  1.46it/s]Extractor Predicting: 161it [01:45,  1.47it/s]Extractor Predicting: 162it [01:45,  1.48it/s]Extractor Predicting: 163it [01:46,  1.50it/s]Extractor Predicting: 164it [01:47,  1.49it/s]Extractor Predicting: 165it [01:47,  1.51it/s]Extractor Predicting: 166it [01:48,  1.49it/s]Extractor Predicting: 167it [01:49,  1.50it/s]Extractor Predicting: 168it [01:49,  1.51it/s]Extractor Predicting: 169it [01:50,  1.54it/s]Extractor Predicting: 170it [01:51,  1.51it/s]Extractor Predicting: 171it [01:51,  1.51it/s]Extractor Predicting: 172it [01:52,  1.52it/s]Extractor Predicting: 173it [01:53,  1.51it/s]Extractor Predicting: 174it [01:53,  1.51it/s]Extractor Predicting: 175it [01:54,  1.49it/s]Extractor Predicting: 176it [01:55,  1.51it/s]Extractor Predicting: 177it [01:55,  1.52it/s]Extractor Predicting: 178it [01:56,  1.48it/s]Extractor Predicting: 179it [01:57,  1.51it/s]Extractor Predicting: 180it [01:57,  1.53it/s]Extractor Predicting: 181it [01:58,  1.53it/s]Extractor Predicting: 182it [01:59,  1.52it/s]Extractor Predicting: 183it [01:59,  1.54it/s]Extractor Predicting: 184it [02:00,  1.51it/s]Extractor Predicting: 185it [02:01,  1.54it/s]Extractor Predicting: 186it [02:01,  1.52it/s]Extractor Predicting: 187it [02:02,  1.52it/s]Extractor Predicting: 188it [02:03,  1.51it/s]Extractor Predicting: 189it [02:03,  1.51it/s]Extractor Predicting: 190it [02:04,  1.51it/s]Extractor Predicting: 191it [02:05,  1.50it/s]Extractor Predicting: 192it [02:05,  1.51it/s]Extractor Predicting: 193it [02:06,  1.51it/s]Extractor Predicting: 194it [02:07,  1.50it/s]Extractor Predicting: 195it [02:07,  1.52it/s]Extractor Predicting: 196it [02:08,  1.56it/s]Extractor Predicting: 197it [02:09,  1.53it/s]Extractor Predicting: 198it [02:09,  1.49it/s]Extractor Predicting: 199it [02:10,  1.49it/s]Extractor Predicting: 200it [02:11,  1.32it/s]Extractor Predicting: 201it [02:12,  1.35it/s]Extractor Predicting: 202it [02:12,  1.37it/s]Extractor Predicting: 203it [02:13,  1.40it/s]Extractor Predicting: 204it [02:14,  1.43it/s]Extractor Predicting: 205it [02:14,  1.43it/s]Extractor Predicting: 206it [02:15,  1.48it/s]Extractor Predicting: 207it [02:16,  1.50it/s]Extractor Predicting: 208it [02:16,  1.52it/s]Extractor Predicting: 209it [02:17,  1.54it/s]Extractor Predicting: 210it [02:18,  1.54it/s]Extractor Predicting: 211it [02:18,  1.53it/s]Extractor Predicting: 212it [02:19,  1.57it/s]Extractor Predicting: 213it [02:19,  1.56it/s]Extractor Predicting: 214it [02:20,  1.55it/s]Extractor Predicting: 215it [02:21,  1.51it/s]Extractor Predicting: 216it [02:21,  1.48it/s]Extractor Predicting: 217it [02:22,  1.52it/s]Extractor Predicting: 218it [02:23,  1.51it/s]Extractor Predicting: 219it [02:23,  1.49it/s]Extractor Predicting: 220it [02:24,  1.50it/s]Extractor Predicting: 221it [02:25,  1.53it/s]Extractor Predicting: 222it [02:25,  1.53it/s]Extractor Predicting: 223it [02:26,  1.54it/s]Extractor Predicting: 224it [02:27,  1.55it/s]Extractor Predicting: 225it [02:27,  1.55it/s]Extractor Predicting: 226it [02:28,  1.53it/s]Extractor Predicting: 227it [02:29,  1.51it/s]Extractor Predicting: 228it [02:29,  1.52it/s]Extractor Predicting: 229it [02:30,  1.52it/s]Extractor Predicting: 230it [02:31,  1.53it/s]Extractor Predicting: 231it [02:31,  1.57it/s]Extractor Predicting: 232it [02:32,  1.58it/s]Extractor Predicting: 233it [02:33,  1.54it/s]Extractor Predicting: 234it [02:33,  1.57it/s]Extractor Predicting: 235it [02:34,  1.58it/s]Extractor Predicting: 236it [02:34,  1.55it/s]Extractor Predicting: 237it [02:35,  1.54it/s]Extractor Predicting: 238it [02:36,  1.54it/s]Extractor Predicting: 239it [02:36,  1.54it/s]Extractor Predicting: 240it [02:37,  1.55it/s]Extractor Predicting: 241it [02:38,  1.57it/s]Extractor Predicting: 242it [02:38,  1.54it/s]Extractor Predicting: 243it [02:39,  1.50it/s]Extractor Predicting: 244it [02:40,  1.53it/s]Extractor Predicting: 245it [02:40,  1.56it/s]Extractor Predicting: 246it [02:41,  1.53it/s]Extractor Predicting: 247it [02:42,  1.52it/s]Extractor Predicting: 248it [02:42,  1.55it/s]Extractor Predicting: 249it [02:43,  1.56it/s]Extractor Predicting: 250it [02:44,  1.54it/s]Extractor Predicting: 251it [02:44,  1.56it/s]Extractor Predicting: 252it [02:45,  1.57it/s]Extractor Predicting: 253it [02:45,  1.57it/s]Extractor Predicting: 254it [02:46,  1.60it/s]Extractor Predicting: 255it [02:47,  1.57it/s]Extractor Predicting: 256it [02:47,  1.56it/s]Extractor Predicting: 257it [02:48,  1.53it/s]Extractor Predicting: 258it [02:49,  1.54it/s]Extractor Predicting: 259it [02:49,  1.53it/s]Extractor Predicting: 260it [02:50,  1.54it/s]Extractor Predicting: 261it [02:51,  1.55it/s]Extractor Predicting: 262it [02:51,  1.56it/s]Extractor Predicting: 263it [02:52,  1.55it/s]Extractor Predicting: 264it [02:53,  1.54it/s]Extractor Predicting: 265it [02:53,  1.51it/s]Extractor Predicting: 266it [02:54,  1.53it/s]Extractor Predicting: 267it [02:54,  1.56it/s]Extractor Predicting: 268it [02:55,  1.54it/s]Extractor Predicting: 269it [02:56,  1.50it/s]Extractor Predicting: 270it [02:56,  1.55it/s]Extractor Predicting: 271it [02:57,  1.56it/s]Extractor Predicting: 272it [02:58,  1.53it/s]Extractor Predicting: 273it [02:58,  1.53it/s]Extractor Predicting: 274it [02:59,  1.52it/s]Extractor Predicting: 275it [03:00,  1.52it/s]Extractor Predicting: 276it [03:00,  1.52it/s]Extractor Predicting: 277it [03:01,  1.55it/s]Extractor Predicting: 278it [03:02,  1.51it/s]Extractor Predicting: 279it [03:02,  1.51it/s]Extractor Predicting: 280it [03:03,  1.51it/s]Extractor Predicting: 281it [03:04,  1.50it/s]Extractor Predicting: 282it [03:04,  1.51it/s]Extractor Predicting: 283it [03:05,  1.54it/s]Extractor Predicting: 284it [03:06,  1.49it/s]Extractor Predicting: 285it [03:06,  1.51it/s]Extractor Predicting: 286it [03:07,  1.52it/s]Extractor Predicting: 287it [03:08,  1.53it/s]Extractor Predicting: 288it [03:08,  1.52it/s]Extractor Predicting: 289it [03:09,  1.53it/s]Extractor Predicting: 290it [03:10,  1.46it/s]Extractor Predicting: 291it [03:10,  1.51it/s]Extractor Predicting: 292it [03:11,  1.49it/s]Extractor Predicting: 293it [03:12,  1.51it/s]Extractor Predicting: 294it [03:12,  1.56it/s]Extractor Predicting: 295it [03:13,  1.54it/s]Extractor Predicting: 296it [03:14,  1.57it/s]Extractor Predicting: 297it [03:14,  1.59it/s]Extractor Predicting: 298it [03:15,  1.60it/s]Extractor Predicting: 299it [03:15,  1.56it/s]Extractor Predicting: 300it [03:16,  1.56it/s]Extractor Predicting: 301it [03:17,  1.52it/s]Extractor Predicting: 302it [03:17,  1.52it/s]Extractor Predicting: 303it [03:18,  1.56it/s]Extractor Predicting: 304it [03:19,  1.53it/s]Extractor Predicting: 305it [03:19,  1.51it/s]Extractor Predicting: 306it [03:20,  1.53it/s]Extractor Predicting: 307it [03:21,  1.52it/s]Extractor Predicting: 308it [03:21,  1.56it/s]Extractor Predicting: 309it [03:22,  1.53it/s]Extractor Predicting: 310it [03:23,  1.54it/s]Extractor Predicting: 311it [03:23,  1.51it/s]Extractor Predicting: 312it [03:24,  1.49it/s]Extractor Predicting: 313it [03:25,  1.48it/s]Extractor Predicting: 314it [03:25,  1.45it/s]Extractor Predicting: 315it [03:26,  1.44it/s]Extractor Predicting: 316it [03:27,  1.47it/s]Extractor Predicting: 317it [03:27,  1.49it/s]Extractor Predicting: 318it [03:28,  1.53it/s]Extractor Predicting: 319it [03:29,  1.52it/s]Extractor Predicting: 320it [03:29,  1.58it/s]Extractor Predicting: 321it [03:30,  1.56it/s]Extractor Predicting: 322it [03:31,  1.55it/s]Extractor Predicting: 323it [03:31,  1.54it/s]Extractor Predicting: 324it [03:32,  1.55it/s]Extractor Predicting: 325it [03:33,  1.53it/s]Extractor Predicting: 326it [03:33,  1.52it/s]Extractor Predicting: 327it [03:34,  1.48it/s]Extractor Predicting: 328it [03:35,  1.45it/s]Extractor Predicting: 329it [03:35,  1.49it/s]Extractor Predicting: 330it [03:36,  1.53it/s]Extractor Predicting: 331it [03:37,  1.54it/s]Extractor Predicting: 332it [03:37,  1.55it/s]Extractor Predicting: 333it [03:38,  1.52it/s]Extractor Predicting: 334it [03:39,  1.51it/s]Extractor Predicting: 335it [03:39,  1.51it/s]Extractor Predicting: 336it [03:40,  1.51it/s]Extractor Predicting: 337it [03:41,  1.53it/s]Extractor Predicting: 338it [03:41,  1.36it/s]Extractor Predicting: 339it [03:42,  1.42it/s]Extractor Predicting: 340it [03:43,  1.49it/s]Extractor Predicting: 341it [03:43,  1.49it/s]Extractor Predicting: 342it [03:44,  1.52it/s]Extractor Predicting: 343it [03:45,  1.56it/s]Extractor Predicting: 344it [03:45,  1.54it/s]Extractor Predicting: 345it [03:46,  1.55it/s]Extractor Predicting: 346it [03:47,  1.51it/s]Extractor Predicting: 347it [03:47,  1.53it/s]Extractor Predicting: 348it [03:48,  1.53it/s]Extractor Predicting: 349it [03:48,  1.54it/s]Extractor Predicting: 350it [03:49,  1.55it/s]Extractor Predicting: 351it [03:50,  1.56it/s]Extractor Predicting: 352it [03:50,  1.59it/s]Extractor Predicting: 353it [03:51,  1.57it/s]Extractor Predicting: 354it [03:52,  1.57it/s]Extractor Predicting: 355it [03:52,  1.55it/s]Extractor Predicting: 356it [03:53,  1.54it/s]Extractor Predicting: 357it [03:54,  1.53it/s]Extractor Predicting: 358it [03:54,  1.54it/s]Extractor Predicting: 359it [03:55,  1.53it/s]Extractor Predicting: 360it [03:56,  1.53it/s]Extractor Predicting: 361it [03:56,  1.56it/s]Extractor Predicting: 362it [03:57,  1.55it/s]Extractor Predicting: 363it [03:57,  1.57it/s]Extractor Predicting: 364it [03:58,  1.55it/s]Extractor Predicting: 365it [03:59,  1.57it/s]Extractor Predicting: 366it [03:59,  1.56it/s]Extractor Predicting: 367it [04:00,  1.56it/s]Extractor Predicting: 368it [04:01,  1.53it/s]Extractor Predicting: 369it [04:01,  1.52it/s]Extractor Predicting: 370it [04:02,  1.56it/s]Extractor Predicting: 371it [04:03,  1.55it/s]Extractor Predicting: 372it [04:03,  1.57it/s]Extractor Predicting: 373it [04:04,  1.58it/s]Extractor Predicting: 374it [04:05,  1.58it/s]Extractor Predicting: 375it [04:05,  1.58it/s]Extractor Predicting: 376it [04:06,  1.55it/s]Extractor Predicting: 377it [04:07,  1.54it/s]Extractor Predicting: 378it [04:07,  1.54it/s]Extractor Predicting: 379it [04:08,  1.57it/s]Extractor Predicting: 380it [04:08,  1.57it/s]Extractor Predicting: 381it [04:09,  1.60it/s]Extractor Predicting: 382it [04:10,  1.57it/s]Extractor Predicting: 383it [04:10,  1.57it/s]Extractor Predicting: 384it [04:11,  1.55it/s]Extractor Predicting: 385it [04:12,  1.53it/s]Extractor Predicting: 386it [04:12,  1.55it/s]Extractor Predicting: 387it [04:13,  1.57it/s]Extractor Predicting: 388it [04:14,  1.52it/s]Extractor Predicting: 389it [04:14,  1.50it/s]Extractor Predicting: 390it [04:15,  1.45it/s]Extractor Predicting: 391it [04:16,  1.48it/s]Extractor Predicting: 392it [04:16,  1.48it/s]Extractor Predicting: 393it [04:17,  1.49it/s]Extractor Predicting: 394it [04:18,  1.54it/s]Extractor Predicting: 395it [04:18,  1.53it/s]Extractor Predicting: 396it [04:19,  1.54it/s]Extractor Predicting: 397it [04:20,  1.56it/s]Extractor Predicting: 398it [04:20,  1.57it/s]Extractor Predicting: 399it [04:21,  1.59it/s]Extractor Predicting: 400it [04:21,  1.58it/s]Extractor Predicting: 401it [04:22,  1.57it/s]Extractor Predicting: 402it [04:23,  1.56it/s]Extractor Predicting: 403it [04:23,  1.54it/s]Extractor Predicting: 404it [04:24,  1.53it/s]Extractor Predicting: 405it [04:25,  1.54it/s]Extractor Predicting: 406it [04:25,  1.53it/s]Extractor Predicting: 407it [04:26,  1.51it/s]Extractor Predicting: 408it [04:27,  1.52it/s]Extractor Predicting: 409it [04:27,  1.53it/s]Extractor Predicting: 410it [04:28,  1.55it/s]Extractor Predicting: 411it [04:29,  1.52it/s]Extractor Predicting: 412it [04:29,  1.51it/s]Extractor Predicting: 413it [04:30,  1.48it/s]Extractor Predicting: 414it [04:31,  1.51it/s]Extractor Predicting: 415it [04:31,  1.55it/s]Extractor Predicting: 416it [04:32,  1.55it/s]Extractor Predicting: 417it [04:32,  1.59it/s]Extractor Predicting: 418it [04:33,  1.60it/s]Extractor Predicting: 419it [04:34,  1.62it/s]Extractor Predicting: 420it [04:34,  1.57it/s]Extractor Predicting: 421it [04:35,  1.56it/s]Extractor Predicting: 422it [04:36,  1.58it/s]Extractor Predicting: 423it [04:36,  1.56it/s]Extractor Predicting: 424it [04:37,  1.57it/s]Extractor Predicting: 425it [04:38,  1.61it/s]Extractor Predicting: 426it [04:38,  1.61it/s]Extractor Predicting: 427it [04:39,  1.61it/s]Extractor Predicting: 428it [04:39,  1.56it/s]Extractor Predicting: 429it [04:40,  1.58it/s]Extractor Predicting: 430it [04:41,  1.61it/s]Extractor Predicting: 431it [04:41,  1.58it/s]Extractor Predicting: 432it [04:42,  1.59it/s]Extractor Predicting: 433it [04:43,  1.62it/s]Extractor Predicting: 434it [04:43,  1.62it/s]Extractor Predicting: 435it [04:44,  1.62it/s]Extractor Predicting: 436it [04:44,  1.64it/s]Extractor Predicting: 437it [04:45,  1.63it/s]Extractor Predicting: 438it [04:46,  1.63it/s]Extractor Predicting: 439it [04:46,  1.62it/s]Extractor Predicting: 440it [04:47,  1.58it/s]Extractor Predicting: 441it [04:47,  1.60it/s]Extractor Predicting: 442it [04:48,  1.57it/s]Extractor Predicting: 443it [04:49,  1.59it/s]Extractor Predicting: 444it [04:49,  1.60it/s]Extractor Predicting: 445it [04:50,  1.57it/s]Extractor Predicting: 446it [04:51,  1.53it/s]Extractor Predicting: 447it [04:51,  1.55it/s]Extractor Predicting: 448it [04:52,  1.57it/s]Extractor Predicting: 449it [04:53,  1.58it/s]Extractor Predicting: 450it [04:53,  1.60it/s]Extractor Predicting: 451it [04:54,  1.59it/s]Extractor Predicting: 452it [04:55,  1.57it/s]Extractor Predicting: 453it [04:55,  1.54it/s]Extractor Predicting: 454it [04:56,  1.52it/s]Extractor Predicting: 455it [04:57,  1.49it/s]Extractor Predicting: 456it [04:57,  1.46it/s]Extractor Predicting: 457it [04:58,  1.46it/s]Extractor Predicting: 458it [04:59,  1.47it/s]Extractor Predicting: 459it [04:59,  1.49it/s]Extractor Predicting: 460it [05:00,  1.49it/s]Extractor Predicting: 461it [05:01,  1.49it/s]Extractor Predicting: 462it [05:01,  1.45it/s]Extractor Predicting: 463it [05:02,  1.48it/s]Extractor Predicting: 464it [05:03,  1.46it/s]Extractor Predicting: 465it [05:03,  1.48it/s]Extractor Predicting: 466it [05:04,  1.49it/s]Extractor Predicting: 467it [05:05,  1.52it/s]Extractor Predicting: 468it [05:05,  1.46it/s]Extractor Predicting: 469it [05:06,  1.45it/s]Extractor Predicting: 470it [05:07,  1.46it/s]Extractor Predicting: 471it [05:07,  1.46it/s]Extractor Predicting: 472it [05:08,  1.44it/s]Extractor Predicting: 473it [05:09,  1.44it/s]Extractor Predicting: 474it [05:10,  1.45it/s]Extractor Predicting: 475it [05:10,  1.45it/s]Extractor Predicting: 476it [05:11,  1.44it/s]Extractor Predicting: 477it [05:12,  1.45it/s]Extractor Predicting: 478it [05:13,  1.28it/s]Extractor Predicting: 479it [05:13,  1.33it/s]Extractor Predicting: 480it [05:14,  1.38it/s]Extractor Predicting: 481it [05:15,  1.41it/s]Extractor Predicting: 482it [05:15,  1.42it/s]Extractor Predicting: 483it [05:16,  1.41it/s]Extractor Predicting: 484it [05:17,  1.41it/s]Extractor Predicting: 485it [05:17,  1.43it/s]Extractor Predicting: 486it [05:18,  1.46it/s]Extractor Predicting: 487it [05:19,  1.47it/s]Extractor Predicting: 488it [05:19,  1.47it/s]Extractor Predicting: 489it [05:20,  1.44it/s]Extractor Predicting: 490it [05:21,  1.47it/s]Extractor Predicting: 491it [05:22,  1.46it/s]Extractor Predicting: 492it [05:22,  1.43it/s]Extractor Predicting: 493it [05:23,  1.42it/s]Extractor Predicting: 494it [05:24,  1.47it/s]Extractor Predicting: 495it [05:24,  1.45it/s]Extractor Predicting: 496it [05:25,  1.45it/s]Extractor Predicting: 497it [05:26,  1.46it/s]Extractor Predicting: 498it [05:26,  1.50it/s]Extractor Predicting: 499it [05:27,  1.50it/s]Extractor Predicting: 500it [05:28,  1.47it/s]Extractor Predicting: 501it [05:28,  1.46it/s]Extractor Predicting: 502it [05:29,  1.44it/s]Extractor Predicting: 503it [05:30,  1.44it/s]Extractor Predicting: 504it [05:30,  1.44it/s]Extractor Predicting: 505it [05:31,  1.48it/s]Extractor Predicting: 506it [05:32,  1.46it/s]Extractor Predicting: 507it [05:32,  1.47it/s]Extractor Predicting: 508it [05:33,  1.49it/s]Extractor Predicting: 509it [05:34,  1.51it/s]Extractor Predicting: 510it [05:34,  1.54it/s]Extractor Predicting: 511it [05:35,  1.51it/s]Extractor Predicting: 512it [05:36,  1.52it/s]Extractor Predicting: 513it [05:36,  1.48it/s]Extractor Predicting: 514it [05:37,  1.49it/s]Extractor Predicting: 515it [05:38,  1.49it/s]Extractor Predicting: 516it [05:38,  1.47it/s]Extractor Predicting: 517it [05:39,  1.41it/s]Extractor Predicting: 518it [05:40,  1.42it/s]Extractor Predicting: 519it [05:41,  1.44it/s]Extractor Predicting: 520it [05:41,  1.51it/s]Extractor Predicting: 521it [05:42,  1.50it/s]Extractor Predicting: 522it [05:43,  1.49it/s]Extractor Predicting: 523it [05:43,  1.48it/s]Extractor Predicting: 524it [05:44,  1.47it/s]Extractor Predicting: 525it [05:45,  1.47it/s]Extractor Predicting: 526it [05:45,  1.43it/s]Extractor Predicting: 527it [05:46,  1.45it/s]Extractor Predicting: 528it [05:47,  1.47it/s]Extractor Predicting: 529it [05:47,  1.47it/s]Extractor Predicting: 530it [05:48,  1.47it/s]Extractor Predicting: 531it [05:49,  1.48it/s]Extractor Predicting: 532it [05:49,  1.44it/s]Extractor Predicting: 533it [05:50,  1.45it/s]Extractor Predicting: 534it [05:51,  1.44it/s]Extractor Predicting: 535it [05:51,  1.48it/s]Extractor Predicting: 536it [05:52,  1.44it/s]Extractor Predicting: 537it [05:53,  1.46it/s]Extractor Predicting: 538it [05:54,  1.45it/s]Extractor Predicting: 539it [05:54,  1.46it/s]Extractor Predicting: 540it [05:55,  1.46it/s]Extractor Predicting: 541it [05:56,  1.38it/s]Extractor Predicting: 542it [05:56,  1.40it/s]Extractor Predicting: 543it [05:57,  1.42it/s]Extractor Predicting: 544it [05:58,  1.44it/s]Extractor Predicting: 545it [05:59,  1.43it/s]Extractor Predicting: 546it [05:59,  1.44it/s]Extractor Predicting: 547it [06:00,  1.44it/s]Extractor Predicting: 548it [06:01,  1.47it/s]Extractor Predicting: 549it [06:01,  1.43it/s]Extractor Predicting: 550it [06:02,  1.44it/s]Extractor Predicting: 551it [06:03,  1.45it/s]Extractor Predicting: 552it [06:03,  1.44it/s]Extractor Predicting: 553it [06:04,  1.48it/s]Extractor Predicting: 554it [06:05,  1.47it/s]Extractor Predicting: 555it [06:05,  1.47it/s]Extractor Predicting: 556it [06:06,  1.48it/s]Extractor Predicting: 557it [06:07,  1.45it/s]Extractor Predicting: 558it [06:07,  1.44it/s]Extractor Predicting: 559it [06:08,  1.47it/s]Extractor Predicting: 560it [06:09,  1.50it/s]Extractor Predicting: 561it [06:09,  1.48it/s]Extractor Predicting: 562it [06:10,  1.49it/s]Extractor Predicting: 563it [06:11,  1.48it/s]Extractor Predicting: 564it [06:11,  1.46it/s]Extractor Predicting: 565it [06:12,  1.46it/s]Extractor Predicting: 566it [06:13,  1.45it/s]Extractor Predicting: 567it [06:14,  1.47it/s]Extractor Predicting: 568it [06:14,  1.52it/s]Extractor Predicting: 569it [06:15,  1.49it/s]Extractor Predicting: 570it [06:16,  1.47it/s]Extractor Predicting: 571it [06:16,  1.46it/s]Extractor Predicting: 572it [06:17,  1.43it/s]Extractor Predicting: 573it [06:18,  1.39it/s]Extractor Predicting: 574it [06:18,  1.39it/s]Extractor Predicting: 575it [06:19,  1.40it/s]Extractor Predicting: 576it [06:20,  1.43it/s]Extractor Predicting: 577it [06:20,  1.75it/s]Extractor Predicting: 577it [06:20,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:51,970 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:51,975 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:51,975 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:51,975 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:51,975 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:07:52,593 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:07:52,594 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:07:53,156 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:07:54,198 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:07:54,198 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:57,093 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:57,097 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:57,097 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:57,097 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:57,097 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:07:57,718 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:07:57,719 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:07:58,293 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:07:58,450 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:07:58,450 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2635690789473684,
  "recall": 0.04635522129013596,
  "score": 0.07884378843788438,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'employer', 'field of work', 'languages spoken, written or signed', 'lyrics by', 'manufacturer', 'member of sports team', 'occupation', 'performer', 'residence', 'shares border with', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 11382
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11482, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:01,  1.50it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.56it/s]Extractor Predicting: 15it [00:09,  1.52it/s]Extractor Predicting: 16it [00:10,  1.51it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:11,  1.45it/s]Extractor Predicting: 19it [00:12,  1.46it/s]Extractor Predicting: 20it [00:13,  1.43it/s]Extractor Predicting: 21it [00:13,  1.45it/s]Extractor Predicting: 22it [00:14,  1.46it/s]Extractor Predicting: 23it [00:15,  1.46it/s]Extractor Predicting: 24it [00:16,  1.45it/s]Extractor Predicting: 25it [00:16,  1.42it/s]Extractor Predicting: 26it [00:17,  1.44it/s]Extractor Predicting: 27it [00:18,  1.43it/s]Extractor Predicting: 28it [00:18,  1.43it/s]Extractor Predicting: 29it [00:19,  1.40it/s]Extractor Predicting: 30it [00:20,  1.40it/s]Extractor Predicting: 31it [00:20,  1.45it/s]Extractor Predicting: 32it [00:21,  1.48it/s]Extractor Predicting: 33it [00:22,  1.46it/s]Extractor Predicting: 34it [00:22,  1.45it/s]Extractor Predicting: 35it [00:23,  1.50it/s]Extractor Predicting: 36it [00:24,  1.50it/s]Extractor Predicting: 37it [00:24,  1.49it/s]Extractor Predicting: 38it [00:25,  1.45it/s]Extractor Predicting: 39it [00:26,  1.40it/s]Extractor Predicting: 40it [00:27,  1.38it/s]Extractor Predicting: 41it [00:27,  1.39it/s]Extractor Predicting: 42it [00:28,  1.40it/s]Extractor Predicting: 43it [00:29,  1.40it/s]Extractor Predicting: 44it [00:30,  1.34it/s]Extractor Predicting: 45it [00:30,  1.36it/s]Extractor Predicting: 46it [00:31,  1.40it/s]Extractor Predicting: 47it [00:32,  1.43it/s]Extractor Predicting: 48it [00:32,  1.43it/s]Extractor Predicting: 49it [00:33,  1.44it/s]Extractor Predicting: 50it [00:34,  1.43it/s]Extractor Predicting: 51it [00:34,  1.45it/s]Extractor Predicting: 52it [00:35,  1.42it/s]Extractor Predicting: 53it [00:36,  1.43it/s]Extractor Predicting: 54it [00:37,  1.46it/s]Extractor Predicting: 55it [00:37,  1.48it/s]Extractor Predicting: 56it [00:38,  1.50it/s]Extractor Predicting: 57it [00:39,  1.49it/s]Extractor Predicting: 58it [00:39,  1.51it/s]Extractor Predicting: 59it [00:40,  1.51it/s]Extractor Predicting: 60it [00:41,  1.46it/s]Extractor Predicting: 61it [00:41,  1.50it/s]Extractor Predicting: 62it [00:42,  1.48it/s]Extractor Predicting: 63it [00:43,  1.47it/s]Extractor Predicting: 64it [00:43,  1.42it/s]Extractor Predicting: 65it [00:44,  1.42it/s]Extractor Predicting: 66it [00:45,  1.43it/s]Extractor Predicting: 67it [00:45,  1.44it/s]Extractor Predicting: 68it [00:46,  1.44it/s]Extractor Predicting: 69it [00:47,  1.43it/s]Extractor Predicting: 70it [00:47,  1.43it/s]Extractor Predicting: 71it [00:48,  1.45it/s]Extractor Predicting: 72it [00:49,  1.46it/s]Extractor Predicting: 73it [00:50,  1.48it/s]Extractor Predicting: 74it [00:50,  1.51it/s]Extractor Predicting: 75it [00:51,  1.51it/s]Extractor Predicting: 76it [00:51,  1.51it/s]Extractor Predicting: 77it [00:52,  1.53it/s]Extractor Predicting: 78it [00:53,  1.51it/s]Extractor Predicting: 79it [00:53,  1.54it/s]Extractor Predicting: 80it [00:54,  1.55it/s]Extractor Predicting: 81it [00:55,  1.56it/s]Extractor Predicting: 82it [00:55,  1.56it/s]Extractor Predicting: 83it [00:56,  1.53it/s]Extractor Predicting: 84it [00:57,  1.49it/s]Extractor Predicting: 85it [00:57,  1.53it/s]Extractor Predicting: 86it [00:58,  1.51it/s]Extractor Predicting: 87it [00:59,  1.47it/s]Extractor Predicting: 88it [00:59,  1.46it/s]Extractor Predicting: 89it [01:00,  1.42it/s]Extractor Predicting: 90it [01:01,  1.44it/s]Extractor Predicting: 91it [01:02,  1.30it/s]Extractor Predicting: 92it [01:02,  1.32it/s]Extractor Predicting: 93it [01:03,  1.34it/s]Extractor Predicting: 94it [01:04,  1.35it/s]Extractor Predicting: 95it [01:05,  1.35it/s]Extractor Predicting: 96it [01:05,  1.35it/s]Extractor Predicting: 97it [01:06,  1.37it/s]Extractor Predicting: 98it [01:07,  1.37it/s]Extractor Predicting: 99it [01:08,  1.38it/s]Extractor Predicting: 100it [01:08,  1.42it/s]Extractor Predicting: 101it [01:09,  1.42it/s]Extractor Predicting: 102it [01:10,  1.41it/s]Extractor Predicting: 103it [01:10,  1.38it/s]Extractor Predicting: 104it [01:11,  1.39it/s]Extractor Predicting: 105it [01:12,  1.41it/s]Extractor Predicting: 106it [01:12,  1.42it/s]Extractor Predicting: 107it [01:13,  1.40it/s]Extractor Predicting: 108it [01:14,  1.38it/s]Extractor Predicting: 109it [01:15,  1.36it/s]Extractor Predicting: 110it [01:15,  1.36it/s]Extractor Predicting: 110it [01:15,  1.45it/s]
[INFO|configuration_utils.py:515] 2023-08-29 05:09:16,035 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:09:16,037 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 05:09:16,039 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:09:16,040 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 05:09:16,044 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 05:09:19,002 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 05:09:19,002 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 05:09:19,013 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:09:19,014 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 05:09:19,019 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:09:19,022 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:09:19,022 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:09:19,022 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:09:19,022 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:09:19,022 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:09:19,022 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4144486692015209,
  "recall": 0.017640394885903868,
  "score": 0.03384042222912139,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 05:09:19,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:19,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:20,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:21,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:22,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:22,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:23,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:24,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:24,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:25,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:26,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:27,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:27,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:28,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:29,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:30,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:30,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:31,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:32,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:33,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:33,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:34,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:35,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:36,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:17<05:35, 17.68s/it][WARNING|generation_utils.py:914] 2023-08-29 05:09:36,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:37,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:38,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:38,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:39,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:40,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:40,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:41,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:42,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:43,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:43,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:44,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:44,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:45,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:46,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:46,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:47,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:48,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:48,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:49,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:50,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:50,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:31<04:41, 15.66s/it][WARNING|generation_utils.py:914] 2023-08-29 05:09:51,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:51,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:52,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:53,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:54,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:55,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:55,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:56,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:57,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:58,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:58,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:09:59,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:00,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:01,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:02,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:03,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:03,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:04,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:05,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:06,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:07,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:07,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:49<04:39, 16.42s/it][WARNING|generation_utils.py:914] 2023-08-29 05:10:08,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:09,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:09,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:10,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:11,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:11,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:12,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:13,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:13,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:14,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:15,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:15,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:16,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:17,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:17,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:18,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:19,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:19,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:20,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:21,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:02<04:01, 15.11s/it][WARNING|generation_utils.py:914] 2023-08-29 05:10:21,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:22,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:23,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:23,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:24,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:25,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:25,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:26,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:27,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:28,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:28,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:29,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:30,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:31,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:31,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:32,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:32,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:33,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:34,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:35,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:35,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:36,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:37,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:38,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:19<03:58, 15.88s/it][WARNING|generation_utils.py:914] 2023-08-29 05:10:38,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:39,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:40,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:41,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:41,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:42,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:43,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:44,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:44,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:45,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:46,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:47,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:47,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:48,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:49,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:50,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:51,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:51,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:52,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:53,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:34<03:38, 15.62s/it][WARNING|generation_utils.py:914] 2023-08-29 05:10:53,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:54,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:55,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:56,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:56,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:57,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:58,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:58,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:10:59,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:00,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:01,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:01,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:02,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:03,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:04,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:04,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:05,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:06,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:06,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:07,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:49<03:18, 15.25s/it][WARNING|generation_utils.py:914] 2023-08-29 05:11:08,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:09,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:09,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:10,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:11,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:11,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:12,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:13,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:14,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:14,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:15,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:16,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:17,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:17,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:18,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:19,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:19,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:20,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:21,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:21,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:22,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:03<03:01, 15.10s/it][WARNING|generation_utils.py:914] 2023-08-29 05:11:23,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:23,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:24,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:25,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:26,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:26,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:27,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:28,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:28,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:29,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:30,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:30,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:31,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:32,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:32,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:33,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:34,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:35,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:35,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:36,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:37,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:18<02:44, 14.96s/it][WARNING|generation_utils.py:914] 2023-08-29 05:11:37,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:38,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:39,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:40,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:40,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:42,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:42,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:43,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:44,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:45,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:46,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:46,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:47,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:48,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:49,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:49,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:50,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:51,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:52,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:52,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:53,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:54,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:55,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:36<02:39, 15.93s/it][WARNING|generation_utils.py:914] 2023-08-29 05:11:56,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:56,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:57,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:58,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:58,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:11:59,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:00,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:00,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:01,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:02,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:04,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:04,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:05,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:06,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:06,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:07,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:08,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:08,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:09,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:10,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:11,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:11,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:12,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:54<02:27, 16.36s/it][WARNING|generation_utils.py:914] 2023-08-29 05:12:13,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:14,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:14,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:15,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:16,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:16,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:17,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:18,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:19,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:20,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:20,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:21,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:22,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:22,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:23,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:24,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:25,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:25,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:26,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:27,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:28,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:29,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:10<02:11, 16.39s/it][WARNING|generation_utils.py:914] 2023-08-29 05:12:29,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:30,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:31,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:31,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:32,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:33,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:34,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:35,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:35,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:36,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:36,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:37,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:38,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:38,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:39,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:39,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:40,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:41,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:41,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:42,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:23<01:48, 15.48s/it][WARNING|generation_utils.py:914] 2023-08-29 05:12:43,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:43,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:44,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:44,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:45,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:46,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:46,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:47,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:47,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:48,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:49,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:49,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:50,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:50,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:51,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:51,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:52,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:53,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:53,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:54,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:35<01:26, 14.34s/it][WARNING|generation_utils.py:914] 2023-08-29 05:12:54,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:55,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:56,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:57,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:58,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:58,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:12:59,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:00,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:01,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:01,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:02,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:03,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:04,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:04,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:05,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:06,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:06,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:07,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:08,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:08,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:09,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:10,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:11,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:52<01:15, 15.13s/it][WARNING|generation_utils.py:914] 2023-08-29 05:13:11,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:12,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:13,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:14,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:14,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:15,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:16,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:16,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:17,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:18,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:19,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:19,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:20,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:21,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:22,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:22,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:23,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:24,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:24,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:25,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:06<00:59, 14.90s/it][WARNING|generation_utils.py:914] 2023-08-29 05:13:26,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:27,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:27,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:28,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:29,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:30,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:30,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:31,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:32,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:32,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:33,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:34,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:35,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:35,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:36,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:37,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:38,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:38,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:39,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:40,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:41,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:41,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:42,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:24<00:46, 15.58s/it][WARNING|generation_utils.py:914] 2023-08-29 05:13:43,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:43,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:44,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:45,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:45,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:46,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:47,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:48,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:48,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:49,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:50,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:50,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:51,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:52,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:52,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:53,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:54,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:54,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:55,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:56,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:56,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:57,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:58,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:39<00:30, 15.50s/it][WARNING|generation_utils.py:914] 2023-08-29 05:13:58,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:59,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:13:59,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:00,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:01,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:01,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:02,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:03,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:03,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:04,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:04,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:05,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:06,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:06,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:07,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:07,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:08,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:09,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:09,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:51<00:14, 14.36s/it][WARNING|generation_utils.py:914] 2023-08-29 05:14:10,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:11,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:11,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:12,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:13,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:14,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:15,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:15,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:16,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:17,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:18,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:19,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:20,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:20,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:21,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:22,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:23,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:24,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:24,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:25,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:26,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:14:27,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:08<00:00, 15.36s/it]Generating: 100%|██████████| 20/20 [05:08<00:00, 15.44s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:14:34,877 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:14:34,883 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:14:34,883 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:14:34,883 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:14:34,883 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:14:35,507 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:14:35,508 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:14:36,070 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:14:37,146 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:14:37,146 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:14:39,984 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:14:39,989 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:14:39,989 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:14:39,989 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:14:39,989 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:14:40,627 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:14:40,628 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:14:41,195 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:14:41,357 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:14:41,357 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 377, 'raw': 480}
{'target': 600, 'success': 402, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 455, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : conflict .', 'success_rate': 0.7838541666666666, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 629, 'raw': 704}
{'prompt': 'Relation : developer .', 'success_rate': 0.8934659090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : given name .', 'success_rate': 0.859375, 'errors': {'', "('Tōhoku Prefectural High School', 'given name', '', 'From 1882 to 1885 , he was educated at the Tōhoku Prefectural High School , where he taught Chinese literature and Chinese philosophy , and was the ninth dean of teachers from 1877 to the present .')", "('Louisiana State University', 'given name', '', 'He graduated from the Louisiana State University in Baton Rouge , Louisiana and received his M. A. in 1951 with a Masters in Civil Engineering from University of Louisiana at Baton Rouge .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : participant in .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 617, 'raw': 768}
{'prompt': 'Relation : work location .', 'success_rate': 0.8033854166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.9453125, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.9453125, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : creator .', 'success_rate': 0.90625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : employer .', 'success_rate': 0.9285714285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : field of work . Context : Later in Life he studied at the Lydden School of Art in Luton at the end of the 17th century and at the Luton Institute under the supervision of David Bode . Head Entity : David Bode , Tail Entity : his own .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8233695652173914, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.8233695652173914, 'errors': {'', "('', 'languages spoken, written or signed', 'Indian language', 'It is also the official language of the Indian Subcontinent ( India ) and Indian Subcontinent is a language that was introduced into the Indian language in India during the Puri period .')", "('', 'languages spoken, written or signed', 'Hungarian', 'The first language is usually Chinese , followed by Spanish , Bengali , Persian and Hungarian.')", 'not enough values to unpack (expected 2, got 1)', "('', 'languages spoken, written or signed', 'Esperanto', 'An Esperanto is a single type of spoken , written or signed Esperanto , written in the Spanish language .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8835227272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Her Boyfriend', 'lyrics by', '', 'In 2005 , she released her single , Her Boyfriend , in an effort to foster romance through love .')"}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 494, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.965625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 313, 'raw': 320}
{'target': 600, 'success': 345, 'raw': 352}
{'target': 600, 'success': 376, 'raw': 384}
{'target': 600, 'success': 407, 'raw': 416}
{'target': 600, 'success': 439, 'raw': 448}
{'target': 600, 'success': 470, 'raw': 480}
{'target': 600, 'success': 501, 'raw': 512}
{'target': 600, 'success': 533, 'raw': 544}
{'target': 600, 'success': 563, 'raw': 576}
{'target': 600, 'success': 594, 'raw': 608}
{'target': 600, 'success': 626, 'raw': 640}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.978125, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 462, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8165760869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.9453125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 627, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.8519021739130435, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.8505434782608695, 'errors': {'', "('the price', 'shares border with', '', 'Although the stock closed at a rather low point before settling down below 12, below the price of .')", "('shares trading', 'shares border with', '', 'They traded at $ 1.16 at the time of the shares trading trading at .')", 'not enough values to unpack (expected 2, got 1)', "('shares', 'shares border with', '', 'The shares have traded at between $ 13 and $ 16 in total , and are currently trading at around $ 2 .')"}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 221, 'raw': 224}
{'target': 600, 'success': 253, 'raw': 256}
{'target': 600, 'success': 285, 'raw': 288}
{'target': 600, 'success': 316, 'raw': 320}
{'target': 600, 'success': 348, 'raw': 352}
{'target': 600, 'success': 379, 'raw': 384}
{'target': 600, 'success': 411, 'raw': 416}
{'target': 600, 'success': 441, 'raw': 448}
{'target': 600, 'success': 473, 'raw': 480}
{'target': 600, 'success': 505, 'raw': 512}
{'target': 600, 'success': 537, 'raw': 544}
{'target': 600, 'success': 569, 'raw': 576}
{'target': 600, 'success': 600, 'raw': 608}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9868421052631579, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8607954545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/2_ext.jsonl'}}
estimate vocab size: 12783
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12883, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.43it/s]Extractor Estimating: 2it [00:01,  1.46it/s]Extractor Estimating: 3it [00:02,  1.45it/s]Extractor Estimating: 4it [00:02,  1.51it/s]Extractor Estimating: 5it [00:03,  1.53it/s]Extractor Estimating: 6it [00:03,  1.54it/s]Extractor Estimating: 7it [00:04,  1.59it/s]Extractor Estimating: 8it [00:05,  1.61it/s]Extractor Estimating: 9it [00:05,  1.58it/s]Extractor Estimating: 10it [00:06,  1.60it/s]Extractor Estimating: 11it [00:07,  1.60it/s]Extractor Estimating: 12it [00:07,  1.55it/s]Extractor Estimating: 13it [00:08,  1.55it/s]Extractor Estimating: 14it [00:09,  1.54it/s]Extractor Estimating: 15it [00:09,  1.56it/s]Extractor Estimating: 16it [00:10,  1.57it/s]Extractor Estimating: 17it [00:10,  1.62it/s]Extractor Estimating: 18it [00:11,  1.62it/s]Extractor Estimating: 19it [00:12,  1.58it/s]Extractor Estimating: 20it [00:12,  1.59it/s]Extractor Estimating: 21it [00:13,  1.60it/s]Extractor Estimating: 22it [00:14,  1.54it/s]Extractor Estimating: 23it [00:14,  1.59it/s]Extractor Estimating: 24it [00:15,  1.55it/s]Extractor Estimating: 25it [00:16,  1.54it/s]Extractor Estimating: 26it [00:16,  1.56it/s]Extractor Estimating: 27it [00:17,  1.60it/s]Extractor Estimating: 28it [00:17,  1.62it/s]Extractor Estimating: 29it [00:18,  1.64it/s]Extractor Estimating: 30it [00:19,  1.66it/s]Extractor Estimating: 31it [00:19,  1.64it/s]Extractor Estimating: 32it [00:20,  1.72it/s]Extractor Estimating: 33it [00:20,  1.72it/s]Extractor Estimating: 34it [00:21,  1.71it/s]Extractor Estimating: 35it [00:21,  1.76it/s]Extractor Estimating: 36it [00:22,  1.75it/s]Extractor Estimating: 37it [00:23,  1.72it/s]Extractor Estimating: 38it [00:23,  1.71it/s]Extractor Estimating: 39it [00:24,  1.72it/s]Extractor Estimating: 40it [00:24,  1.73it/s]Extractor Estimating: 41it [00:25,  1.73it/s]Extractor Estimating: 42it [00:25,  1.78it/s]Extractor Estimating: 43it [00:26,  1.76it/s]Extractor Estimating: 44it [00:27,  1.75it/s]Extractor Estimating: 45it [00:27,  1.73it/s]Extractor Estimating: 46it [00:28,  1.74it/s]Extractor Estimating: 47it [00:28,  1.77it/s]Extractor Estimating: 48it [00:29,  1.76it/s]Extractor Estimating: 49it [00:29,  1.76it/s]Extractor Estimating: 50it [00:30,  1.80it/s]Extractor Estimating: 51it [00:31,  1.71it/s]Extractor Estimating: 52it [00:31,  1.56it/s]Extractor Estimating: 53it [00:32,  1.52it/s]Extractor Estimating: 54it [00:33,  1.49it/s]Extractor Estimating: 55it [00:33,  1.53it/s]Extractor Estimating: 56it [00:34,  1.53it/s]Extractor Estimating: 57it [00:35,  1.50it/s]Extractor Estimating: 58it [00:35,  1.50it/s]Extractor Estimating: 59it [00:36,  1.51it/s]Extractor Estimating: 60it [00:37,  1.51it/s]Extractor Estimating: 61it [00:37,  1.50it/s]Extractor Estimating: 62it [00:38,  1.50it/s]Extractor Estimating: 63it [00:39,  1.47it/s]Extractor Estimating: 64it [00:39,  1.50it/s]Extractor Estimating: 65it [00:40,  1.49it/s]Extractor Estimating: 66it [00:41,  1.45it/s]Extractor Estimating: 67it [00:41,  1.45it/s]Extractor Estimating: 68it [00:42,  1.43it/s]Extractor Estimating: 69it [00:43,  1.41it/s]Extractor Estimating: 70it [00:44,  1.42it/s]Extractor Estimating: 71it [00:44,  1.39it/s]Extractor Estimating: 72it [00:45,  1.47it/s]Extractor Estimating: 73it [00:46,  1.51it/s]Extractor Estimating: 74it [00:46,  1.51it/s]Extractor Estimating: 75it [00:47,  1.51it/s]Extractor Estimating: 76it [00:47,  1.58it/s]Extractor Estimating: 77it [00:48,  1.60it/s]Extractor Estimating: 78it [00:49,  1.64it/s]Extractor Estimating: 79it [00:49,  1.68it/s]Extractor Estimating: 80it [00:50,  1.66it/s]Extractor Estimating: 81it [00:50,  1.72it/s]Extractor Estimating: 82it [00:51,  1.78it/s]Extractor Estimating: 83it [00:51,  1.75it/s]Extractor Estimating: 84it [00:52,  1.75it/s]Extractor Estimating: 85it [00:53,  1.72it/s]Extractor Estimating: 86it [00:53,  1.82it/s]Extractor Estimating: 87it [00:54,  1.79it/s]Extractor Estimating: 88it [00:54,  1.81it/s]Extractor Estimating: 89it [00:55,  1.78it/s]Extractor Estimating: 90it [00:56,  1.63it/s]Extractor Estimating: 91it [00:56,  1.67it/s]Extractor Estimating: 92it [00:57,  1.72it/s]Extractor Estimating: 93it [00:57,  1.76it/s]Extractor Estimating: 94it [00:58,  1.72it/s]Extractor Estimating: 95it [00:58,  1.70it/s]Extractor Estimating: 96it [00:59,  1.76it/s]Extractor Estimating: 97it [00:59,  1.84it/s]Extractor Estimating: 98it [01:00,  1.87it/s]Extractor Estimating: 99it [01:01,  1.84it/s]Extractor Estimating: 100it [01:01,  1.82it/s]Extractor Estimating: 101it [01:02,  1.72it/s]Extractor Estimating: 102it [01:02,  1.67it/s]Extractor Estimating: 103it [01:03,  1.64it/s]Extractor Estimating: 104it [01:04,  1.63it/s]Extractor Estimating: 105it [01:04,  1.61it/s]Extractor Estimating: 106it [01:05,  1.55it/s]Extractor Estimating: 107it [01:06,  1.60it/s]Extractor Estimating: 108it [01:06,  1.57it/s]Extractor Estimating: 109it [01:07,  1.56it/s]Extractor Estimating: 110it [01:08,  1.55it/s]Extractor Estimating: 111it [01:08,  1.57it/s]Extractor Estimating: 112it [01:09,  1.54it/s]Extractor Estimating: 113it [01:09,  1.54it/s]Extractor Estimating: 114it [01:10,  1.58it/s]Extractor Estimating: 115it [01:11,  1.58it/s]Extractor Estimating: 116it [01:11,  1.63it/s]Extractor Estimating: 117it [01:12,  1.62it/s]Extractor Estimating: 118it [01:13,  1.63it/s]Extractor Estimating: 119it [01:13,  1.58it/s]Extractor Estimating: 120it [01:14,  1.59it/s]Extractor Estimating: 121it [01:14,  1.61it/s]Extractor Estimating: 122it [01:15,  1.58it/s]Extractor Estimating: 123it [01:16,  1.57it/s]Extractor Estimating: 124it [01:16,  1.58it/s]Extractor Estimating: 125it [01:17,  1.51it/s]Extractor Estimating: 126it [01:18,  1.55it/s]Extractor Estimating: 127it [01:18,  1.58it/s]Extractor Estimating: 128it [01:19,  1.59it/s]Extractor Estimating: 129it [01:20,  1.59it/s]Extractor Estimating: 130it [01:20,  1.61it/s]Extractor Estimating: 131it [01:21,  1.59it/s]Extractor Estimating: 132it [01:21,  1.59it/s]Extractor Estimating: 133it [01:22,  1.60it/s]Extractor Estimating: 134it [01:23,  1.64it/s]Extractor Estimating: 135it [01:23,  1.61it/s]Extractor Estimating: 136it [01:24,  1.59it/s]Extractor Estimating: 137it [01:25,  1.58it/s]Extractor Estimating: 138it [01:25,  1.60it/s]Extractor Estimating: 139it [01:26,  1.58it/s]Extractor Estimating: 140it [01:26,  1.56it/s]Extractor Estimating: 141it [01:27,  1.58it/s]Extractor Estimating: 142it [01:28,  1.57it/s]Extractor Estimating: 143it [01:28,  1.58it/s]Extractor Estimating: 144it [01:29,  1.61it/s]Extractor Estimating: 145it [01:30,  1.58it/s]Extractor Estimating: 146it [01:30,  1.59it/s]Extractor Estimating: 147it [01:31,  1.61it/s]Extractor Estimating: 148it [01:31,  1.58it/s]Extractor Estimating: 149it [01:32,  1.56it/s]Extractor Estimating: 150it [01:33,  1.58it/s]Extractor Estimating: 151it [01:33,  1.61it/s]Extractor Estimating: 152it [01:34,  1.54it/s]Extractor Estimating: 153it [01:35,  1.53it/s]Extractor Estimating: 154it [01:35,  1.53it/s]Extractor Estimating: 155it [01:36,  1.54it/s]Extractor Estimating: 156it [01:37,  1.52it/s]Extractor Estimating: 157it [01:37,  1.49it/s]Extractor Estimating: 158it [01:38,  1.50it/s]Extractor Estimating: 159it [01:39,  1.52it/s]Extractor Estimating: 160it [01:39,  1.55it/s]Extractor Estimating: 161it [01:40,  1.56it/s]Extractor Estimating: 162it [01:41,  1.54it/s]Extractor Estimating: 163it [01:41,  1.53it/s]Extractor Estimating: 164it [01:42,  1.53it/s]Extractor Estimating: 165it [01:43,  1.53it/s]Extractor Estimating: 166it [01:43,  1.52it/s]Extractor Estimating: 167it [01:44,  1.51it/s]Extractor Estimating: 168it [01:45,  1.39it/s]Extractor Estimating: 169it [01:45,  1.43it/s]Extractor Estimating: 170it [01:46,  1.46it/s]Extractor Estimating: 171it [01:47,  1.46it/s]Extractor Estimating: 172it [01:47,  1.47it/s]Extractor Estimating: 173it [01:48,  1.51it/s]Extractor Estimating: 174it [01:49,  1.55it/s]Extractor Estimating: 175it [01:49,  1.51it/s]Extractor Estimating: 176it [01:50,  1.49it/s]Extractor Estimating: 177it [01:51,  1.53it/s]Extractor Estimating: 178it [01:51,  1.58it/s]Extractor Estimating: 179it [01:52,  1.59it/s]Extractor Estimating: 180it [01:52,  1.58it/s]Extractor Estimating: 181it [01:53,  1.62it/s]Extractor Estimating: 182it [01:54,  1.59it/s]Extractor Estimating: 183it [01:54,  1.49it/s]Extractor Estimating: 184it [01:55,  1.54it/s]Extractor Estimating: 185it [01:56,  1.55it/s]Extractor Estimating: 186it [01:56,  1.62it/s]Extractor Estimating: 187it [01:57,  1.62it/s]Extractor Estimating: 188it [01:58,  1.61it/s]Extractor Estimating: 189it [01:58,  1.62it/s]Extractor Estimating: 190it [01:59,  1.64it/s]Extractor Estimating: 191it [01:59,  1.68it/s]Extractor Estimating: 192it [02:00,  1.64it/s]Extractor Estimating: 193it [02:01,  1.66it/s]Extractor Estimating: 194it [02:01,  1.66it/s]Extractor Estimating: 195it [02:02,  1.67it/s]Extractor Estimating: 196it [02:02,  1.64it/s]Extractor Estimating: 197it [02:03,  1.63it/s]Extractor Estimating: 198it [02:04,  1.60it/s]Extractor Estimating: 199it [02:04,  1.61it/s]Extractor Estimating: 200it [02:05,  1.64it/s]Extractor Estimating: 201it [02:05,  1.64it/s]Extractor Estimating: 202it [02:06,  1.61it/s]Extractor Estimating: 203it [02:07,  1.63it/s]Extractor Estimating: 204it [02:07,  1.58it/s]Extractor Estimating: 205it [02:08,  1.58it/s]Extractor Estimating: 206it [02:09,  1.63it/s]Extractor Estimating: 207it [02:09,  1.60it/s]Extractor Estimating: 208it [02:10,  1.63it/s]Extractor Estimating: 209it [02:10,  1.60it/s]Extractor Estimating: 210it [02:11,  1.60it/s]Extractor Estimating: 211it [02:12,  1.61it/s]Extractor Estimating: 212it [02:12,  1.64it/s]Extractor Estimating: 213it [02:13,  1.60it/s]Extractor Estimating: 214it [02:14,  1.62it/s]Extractor Estimating: 215it [02:14,  1.65it/s]Extractor Estimating: 216it [02:15,  1.63it/s]Extractor Estimating: 217it [02:15,  1.55it/s]Extractor Estimating: 218it [02:16,  1.57it/s]Extractor Estimating: 219it [02:17,  1.56it/s]Extractor Estimating: 220it [02:17,  1.59it/s]Extractor Estimating: 221it [02:18,  1.60it/s]Extractor Estimating: 222it [02:19,  1.60it/s]Extractor Estimating: 223it [02:19,  1.59it/s]Extractor Estimating: 224it [02:20,  1.63it/s]Extractor Estimating: 225it [02:20,  1.62it/s]Extractor Estimating: 226it [02:21,  1.59it/s]Extractor Estimating: 227it [02:22,  1.57it/s]Extractor Estimating: 228it [02:22,  1.53it/s]Extractor Estimating: 229it [02:23,  1.56it/s]Extractor Estimating: 230it [02:24,  1.54it/s]Extractor Estimating: 231it [02:24,  1.55it/s]Extractor Estimating: 232it [02:25,  1.60it/s]Extractor Estimating: 233it [02:26,  1.61it/s]Extractor Estimating: 234it [02:26,  1.61it/s]Extractor Estimating: 235it [02:28,  1.08s/it]Extractor Estimating: 236it [02:29,  1.05it/s]Extractor Estimating: 237it [02:30,  1.17it/s]Extractor Estimating: 238it [02:30,  1.27it/s]Extractor Estimating: 239it [02:31,  1.33it/s]Extractor Estimating: 240it [02:32,  1.39it/s]Extractor Estimating: 241it [02:32,  1.43it/s]Extractor Estimating: 242it [02:33,  1.45it/s]Extractor Estimating: 243it [02:33,  1.48it/s]Extractor Estimating: 244it [02:34,  1.55it/s]Extractor Estimating: 245it [02:35,  1.53it/s]Extractor Estimating: 246it [02:35,  1.54it/s]Extractor Estimating: 247it [02:36,  1.55it/s]Extractor Estimating: 248it [02:37,  1.50it/s]Extractor Estimating: 249it [02:37,  1.51it/s]Extractor Estimating: 250it [02:38,  1.52it/s]Extractor Estimating: 251it [02:39,  1.60it/s]Extractor Estimating: 252it [02:39,  1.43it/s]Extractor Estimating: 253it [02:40,  1.55it/s]Extractor Estimating: 254it [02:41,  1.57it/s]Extractor Estimating: 255it [02:41,  1.64it/s]Extractor Estimating: 256it [02:42,  1.68it/s]Extractor Estimating: 257it [02:42,  1.63it/s]Extractor Estimating: 258it [02:43,  1.59it/s]Extractor Estimating: 259it [02:44,  1.62it/s]Extractor Estimating: 260it [02:44,  1.63it/s]Extractor Estimating: 261it [02:45,  1.64it/s]Extractor Estimating: 262it [02:45,  1.70it/s]Extractor Estimating: 263it [02:46,  1.73it/s]Extractor Estimating: 264it [02:46,  1.70it/s]Extractor Estimating: 265it [02:47,  1.68it/s]Extractor Estimating: 266it [02:48,  1.73it/s]Extractor Estimating: 267it [02:48,  1.70it/s]Extractor Estimating: 268it [02:49,  1.76it/s]Extractor Estimating: 269it [02:49,  1.69it/s]Extractor Estimating: 270it [02:50,  1.77it/s]Extractor Estimating: 271it [02:50,  1.78it/s]Extractor Estimating: 272it [02:51,  1.73it/s]Extractor Estimating: 273it [02:52,  1.75it/s]Extractor Estimating: 274it [02:52,  1.69it/s]Extractor Estimating: 275it [02:53,  1.68it/s]Extractor Estimating: 276it [02:54,  1.63it/s]Extractor Estimating: 277it [02:54,  1.60it/s]Extractor Estimating: 278it [02:55,  1.57it/s]Extractor Estimating: 279it [02:56,  1.55it/s]Extractor Estimating: 280it [02:56,  1.56it/s]Extractor Estimating: 281it [02:57,  1.57it/s]Extractor Estimating: 282it [02:57,  1.55it/s]Extractor Estimating: 283it [02:58,  1.58it/s]Extractor Estimating: 284it [02:59,  1.58it/s]Extractor Estimating: 285it [02:59,  1.57it/s]Extractor Estimating: 286it [03:00,  1.56it/s]Extractor Estimating: 287it [03:01,  1.55it/s]Extractor Estimating: 288it [03:01,  1.55it/s]Extractor Estimating: 289it [03:02,  1.56it/s]Extractor Estimating: 290it [03:03,  1.56it/s]Extractor Estimating: 291it [03:03,  1.49it/s]Extractor Estimating: 292it [03:04,  1.48it/s]Extractor Estimating: 293it [03:05,  1.52it/s]Extractor Estimating: 294it [03:05,  1.50it/s]Extractor Estimating: 295it [03:06,  1.43it/s]Extractor Estimating: 296it [03:07,  1.45it/s]Extractor Estimating: 297it [03:07,  1.50it/s]Extractor Estimating: 298it [03:08,  1.49it/s]Extractor Estimating: 299it [03:09,  1.52it/s]Extractor Estimating: 300it [03:09,  1.52it/s]Extractor Estimating: 301it [03:10,  1.60it/s]Extractor Estimating: 302it [03:10,  1.69it/s]Extractor Estimating: 303it [03:11,  1.72it/s]Extractor Estimating: 304it [03:11,  1.77it/s]Extractor Estimating: 305it [03:12,  1.78it/s]Extractor Estimating: 306it [03:13,  1.81it/s]Extractor Estimating: 307it [03:13,  1.86it/s]Extractor Estimating: 308it [03:14,  1.87it/s]Extractor Estimating: 309it [03:14,  1.91it/s]Extractor Estimating: 310it [03:15,  1.86it/s]Extractor Estimating: 311it [03:15,  1.95it/s]Extractor Estimating: 312it [03:16,  1.94it/s]Extractor Estimating: 313it [03:16,  1.82it/s]Extractor Estimating: 314it [03:17,  1.83it/s]Extractor Estimating: 315it [03:17,  1.91it/s]Extractor Estimating: 316it [03:18,  1.96it/s]Extractor Estimating: 317it [03:18,  1.97it/s]Extractor Estimating: 318it [03:19,  1.99it/s]Extractor Estimating: 319it [03:19,  1.98it/s]Extractor Estimating: 320it [03:20,  2.00it/s]Extractor Estimating: 321it [03:20,  1.99it/s]Extractor Estimating: 322it [03:21,  1.92it/s]Extractor Estimating: 323it [03:21,  1.89it/s]Extractor Estimating: 324it [03:22,  1.90it/s]Extractor Estimating: 325it [03:22,  1.83it/s]Extractor Estimating: 326it [03:23,  1.73it/s]Extractor Estimating: 327it [03:24,  1.70it/s]Extractor Estimating: 328it [03:24,  1.68it/s]Extractor Estimating: 329it [03:25,  1.66it/s]Extractor Estimating: 330it [03:26,  1.64it/s]Extractor Estimating: 331it [03:26,  1.62it/s]Extractor Estimating: 332it [03:27,  1.59it/s]Extractor Estimating: 333it [03:28,  1.59it/s]Extractor Estimating: 334it [03:28,  1.61it/s]Extractor Estimating: 335it [03:29,  1.63it/s]Extractor Estimating: 336it [03:29,  1.64it/s]Extractor Estimating: 337it [03:30,  1.62it/s]Extractor Estimating: 338it [03:31,  1.62it/s]Extractor Estimating: 339it [03:31,  1.60it/s]Extractor Estimating: 340it [03:32,  1.60it/s]Extractor Estimating: 341it [03:32,  1.60it/s]Extractor Estimating: 342it [03:33,  1.62it/s]Extractor Estimating: 343it [03:34,  1.63it/s]Extractor Estimating: 344it [03:34,  1.49it/s]Extractor Estimating: 345it [03:35,  1.52it/s]Extractor Estimating: 346it [03:36,  1.51it/s]Extractor Estimating: 347it [03:36,  1.53it/s]Extractor Estimating: 348it [03:37,  1.57it/s]Extractor Estimating: 349it [03:38,  1.62it/s]Extractor Estimating: 350it [03:38,  1.62it/s]Extractor Estimating: 351it [03:39,  1.57it/s]Extractor Estimating: 352it [03:40,  1.54it/s]Extractor Estimating: 353it [03:40,  1.55it/s]Extractor Estimating: 354it [03:41,  1.53it/s]Extractor Estimating: 355it [03:41,  1.53it/s]Extractor Estimating: 356it [03:42,  1.52it/s]Extractor Estimating: 357it [03:43,  1.53it/s]Extractor Estimating: 358it [03:43,  1.54it/s]Extractor Estimating: 359it [03:44,  1.56it/s]Extractor Estimating: 360it [03:45,  1.54it/s]Extractor Estimating: 361it [03:45,  1.51it/s]Extractor Estimating: 362it [03:46,  1.52it/s]Extractor Estimating: 363it [03:47,  1.52it/s]Extractor Estimating: 364it [03:47,  1.53it/s]Extractor Estimating: 365it [03:48,  1.55it/s]Extractor Estimating: 366it [03:49,  1.56it/s]Extractor Estimating: 367it [03:49,  1.56it/s]Extractor Estimating: 368it [03:50,  1.56it/s]Extractor Estimating: 369it [03:51,  1.56it/s]Extractor Estimating: 370it [03:51,  1.58it/s]Extractor Estimating: 371it [03:52,  1.57it/s]Extractor Estimating: 372it [03:52,  1.57it/s]Extractor Estimating: 373it [03:53,  1.58it/s]Extractor Estimating: 374it [03:54,  1.57it/s]Extractor Estimating: 375it [03:54,  1.52it/s]Extractor Estimating: 376it [03:55,  1.55it/s]Extractor Estimating: 377it [03:56,  1.60it/s]Extractor Estimating: 378it [03:56,  1.60it/s]Extractor Estimating: 379it [03:57,  1.62it/s]Extractor Estimating: 380it [03:58,  1.57it/s]Extractor Estimating: 381it [03:58,  1.56it/s]Extractor Estimating: 382it [03:59,  1.54it/s]Extractor Estimating: 383it [03:59,  1.55it/s]Extractor Estimating: 384it [04:00,  1.59it/s]Extractor Estimating: 385it [04:01,  1.57it/s]Extractor Estimating: 386it [04:01,  1.56it/s]Extractor Estimating: 387it [04:02,  1.50it/s]Extractor Estimating: 388it [04:03,  1.55it/s]Extractor Estimating: 389it [04:03,  1.57it/s]Extractor Estimating: 390it [04:04,  1.56it/s]Extractor Estimating: 391it [04:05,  1.53it/s]Extractor Estimating: 392it [04:05,  1.49it/s]Extractor Estimating: 393it [04:06,  1.53it/s]Extractor Estimating: 394it [04:07,  1.59it/s]Extractor Estimating: 395it [04:07,  1.56it/s]Extractor Estimating: 396it [04:08,  1.61it/s]Extractor Estimating: 397it [04:08,  1.62it/s]Extractor Estimating: 398it [04:09,  1.62it/s]Extractor Estimating: 399it [04:10,  1.61it/s]Extractor Estimating: 400it [04:10,  1.57it/s]Extractor Estimating: 401it [04:11,  1.56it/s]Extractor Estimating: 402it [04:12,  1.55it/s]Extractor Estimating: 403it [04:12,  1.60it/s]Extractor Estimating: 404it [04:13,  1.59it/s]Extractor Estimating: 405it [04:14,  1.55it/s]Extractor Estimating: 406it [04:14,  1.53it/s]Extractor Estimating: 407it [04:15,  1.51it/s]Extractor Estimating: 408it [04:16,  1.52it/s]Extractor Estimating: 409it [04:16,  1.53it/s]Extractor Estimating: 410it [04:17,  1.54it/s]Extractor Estimating: 411it [04:17,  1.54it/s]Extractor Estimating: 412it [04:18,  1.55it/s]Extractor Estimating: 413it [04:19,  1.56it/s]Extractor Estimating: 414it [04:19,  1.55it/s]Extractor Estimating: 415it [04:20,  1.57it/s]Extractor Estimating: 416it [04:21,  1.60it/s]Extractor Estimating: 417it [04:21,  1.59it/s]Extractor Estimating: 418it [04:22,  1.56it/s]Extractor Estimating: 419it [04:23,  1.53it/s]Extractor Estimating: 420it [04:23,  1.52it/s]Extractor Estimating: 421it [04:24,  1.50it/s]Extractor Estimating: 422it [04:25,  1.55it/s]Extractor Estimating: 423it [04:25,  1.57it/s]Extractor Estimating: 424it [04:26,  1.52it/s]Extractor Estimating: 425it [04:27,  1.53it/s]Extractor Estimating: 426it [04:27,  1.59it/s]Extractor Estimating: 427it [04:28,  1.63it/s]Extractor Estimating: 428it [04:28,  1.72it/s]Extractor Estimating: 429it [04:29,  1.74it/s]Extractor Estimating: 430it [04:29,  1.73it/s]Extractor Estimating: 431it [04:30,  1.72it/s]Extractor Estimating: 432it [04:30,  1.78it/s]Extractor Estimating: 433it [04:31,  1.68it/s]Extractor Estimating: 434it [04:32,  1.70it/s]Extractor Estimating: 435it [04:32,  1.65it/s]Extractor Estimating: 436it [04:33,  1.62it/s]Extractor Estimating: 437it [04:34,  1.50it/s]Extractor Estimating: 438it [04:34,  1.57it/s]Extractor Estimating: 439it [04:35,  1.59it/s]Extractor Estimating: 440it [04:35,  1.65it/s]Extractor Estimating: 441it [04:36,  1.65it/s]Extractor Estimating: 442it [04:37,  1.69it/s]Extractor Estimating: 443it [04:37,  1.71it/s]Extractor Estimating: 444it [04:38,  1.68it/s]Extractor Estimating: 445it [04:38,  1.67it/s]Extractor Estimating: 446it [04:39,  1.71it/s]Extractor Estimating: 447it [04:40,  1.70it/s]Extractor Estimating: 448it [04:40,  1.69it/s]Extractor Estimating: 449it [04:41,  1.70it/s]Extractor Estimating: 450it [04:41,  1.69it/s]Extractor Estimating: 451it [04:42,  1.63it/s]Extractor Estimating: 452it [04:43,  1.54it/s]Extractor Estimating: 453it [04:43,  1.55it/s]Extractor Estimating: 454it [04:44,  1.54it/s]Extractor Estimating: 455it [04:45,  1.53it/s]Extractor Estimating: 456it [04:45,  1.49it/s]Extractor Estimating: 457it [04:46,  1.50it/s]Extractor Estimating: 458it [04:47,  1.52it/s]Extractor Estimating: 459it [04:47,  1.50it/s]Extractor Estimating: 460it [04:48,  1.49it/s]Extractor Estimating: 461it [04:49,  1.49it/s]Extractor Estimating: 462it [04:49,  1.51it/s]Extractor Estimating: 463it [04:50,  1.52it/s]Extractor Estimating: 464it [04:51,  1.55it/s]Extractor Estimating: 465it [04:51,  1.56it/s]Extractor Estimating: 466it [04:52,  1.53it/s]Extractor Estimating: 467it [04:53,  1.54it/s]Extractor Estimating: 468it [04:53,  1.54it/s]Extractor Estimating: 469it [04:54,  1.55it/s]Extractor Estimating: 470it [04:55,  1.53it/s]Extractor Estimating: 471it [04:55,  1.55it/s]Extractor Estimating: 472it [04:56,  1.57it/s]Extractor Estimating: 473it [04:57,  1.51it/s]Extractor Estimating: 474it [04:57,  1.53it/s]Extractor Estimating: 475it [04:58,  1.55it/s]Extractor Estimating: 476it [04:58,  1.54it/s]Extractor Estimating: 477it [04:59,  1.55it/s]Extractor Estimating: 478it [05:00,  1.62it/s]Extractor Estimating: 479it [05:00,  1.60it/s]Extractor Estimating: 480it [05:01,  1.62it/s]Extractor Estimating: 481it [05:02,  1.61it/s]Extractor Estimating: 482it [05:02,  1.63it/s]Extractor Estimating: 483it [05:03,  1.59it/s]Extractor Estimating: 484it [05:03,  1.58it/s]Extractor Estimating: 485it [05:04,  1.56it/s]Extractor Estimating: 486it [05:05,  1.52it/s]Extractor Estimating: 487it [05:05,  1.55it/s]Extractor Estimating: 488it [05:06,  1.51it/s]Extractor Estimating: 489it [05:07,  1.53it/s]Extractor Estimating: 490it [05:07,  1.56it/s]Extractor Estimating: 491it [05:08,  1.56it/s]Extractor Estimating: 492it [05:09,  1.58it/s]Extractor Estimating: 493it [05:09,  1.59it/s]Extractor Estimating: 494it [05:10,  1.54it/s]Extractor Estimating: 495it [05:11,  1.52it/s]Extractor Estimating: 496it [05:11,  1.57it/s]Extractor Estimating: 497it [05:12,  1.56it/s]Extractor Estimating: 498it [05:12,  1.55it/s]Extractor Estimating: 499it [05:13,  1.54it/s]Extractor Estimating: 500it [05:13,  2.00it/s]Extractor Estimating: 500it [05:13,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:20:19,215 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:20:19,218 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:20:19,218 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:20:19,218 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:20:19,218 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:20:19,921 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:20:19,922 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:20:20,191 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:20:21,263 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:20:21,267 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:20:23,465 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:20:23,467 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:20:23,467 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:20:23,467 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:20:23,467 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:20:24,217 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:20:24,218 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:20:24,493 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:20:24,677 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:20:24,677 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 08:35:44,585 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 08:35:44,608 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 9960 mean pseudo reward: 0.9530639526776835
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_1/dev.jsonl'}
train vocab size: 25327
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25427, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25427, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.058, loss:801.5845
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.065, loss:801.8509
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.062, loss:788.7128
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.117, loss:763.2367
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 85, avg_time 1.067, loss:759.7208
>> valid entity prec:0.6353, rec:0.5031, f1:0.5615
>> valid relation prec:0.4162, rec:0.0738, f1:0.1254
>> valid relation with NER prec:0.4162, rec:0.0738, f1:0.1254
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 185, avg_time 2.840, loss:779.2252
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 285, avg_time 1.059, loss:742.4796
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 385, avg_time 1.074, loss:769.2559
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 70, avg_time 1.065, loss:755.3178
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 170, avg_time 1.071, loss:758.6850
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5888, rec:0.5648, f1:0.5765
>> valid relation prec:0.4086, rec:0.0534, f1:0.0945
>> valid relation with NER prec:0.4086, rec:0.0534, f1:0.0945
new max entity f1 on valid!
g_step 1100, step 270, avg_time 2.856, loss:738.1297
g_step 1200, step 370, avg_time 1.056, loss:777.6407
g_step 1300, step 55, avg_time 1.067, loss:737.3096
g_step 1400, step 155, avg_time 1.057, loss:685.5649
g_step 1500, step 255, avg_time 1.071, loss:704.1579
>> valid entity prec:0.6097, rec:0.5457, f1:0.5760
>> valid relation prec:0.3528, rec:0.0603, f1:0.1030
>> valid relation with NER prec:0.3528, rec:0.0603, f1:0.1030
g_step 1600, step 355, avg_time 2.849, loss:766.1662
g_step 1700, step 40, avg_time 1.062, loss:703.5055
g_step 1800, step 140, avg_time 1.062, loss:667.8328
g_step 1900, step 240, avg_time 1.058, loss:699.1463
g_step 2000, step 340, avg_time 1.087, loss:683.2577
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6107, rec:0.5962, f1:0.6034
>> valid relation prec:0.3606, rec:0.0659, f1:0.1115
>> valid relation with NER prec:0.3606, rec:0.0659, f1:0.1115
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 25, avg_time 2.841, loss:720.8371
g_step 2200, step 125, avg_time 1.080, loss:662.0083
g_step 2300, step 225, avg_time 1.073, loss:664.6763
g_step 2400, step 325, avg_time 1.056, loss:656.4836
g_step 2500, step 10, avg_time 1.056, loss:652.4680
>> valid entity prec:0.6336, rec:0.5352, f1:0.5802
>> valid relation prec:0.3336, rec:0.0844, f1:0.1347
>> valid relation with NER prec:0.3336, rec:0.0844, f1:0.1347
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 110, avg_time 2.839, loss:611.5667
g_step 2700, step 210, avg_time 1.083, loss:623.0252
g_step 2800, step 310, avg_time 1.065, loss:650.8643
g_step 2900, step 410, avg_time 1.062, loss:651.6238
g_step 3000, step 95, avg_time 1.063, loss:591.9016
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5874, rec:0.5356, f1:0.5603
>> valid relation prec:0.2669, rec:0.0568, f1:0.0936
>> valid relation with NER prec:0.2669, rec:0.0568, f1:0.0936
g_step 3100, step 195, avg_time 2.850, loss:592.0902
g_step 3200, step 295, avg_time 1.063, loss:633.5565
g_step 3300, step 395, avg_time 1.062, loss:633.2531
g_step 3400, step 80, avg_time 1.080, loss:586.4979
g_step 3500, step 180, avg_time 1.066, loss:579.0042
>> valid entity prec:0.5386, rec:0.6129, f1:0.5734
>> valid relation prec:0.2764, rec:0.0794, f1:0.1234
>> valid relation with NER prec:0.2764, rec:0.0794, f1:0.1234
g_step 3600, step 280, avg_time 2.853, loss:598.3543
g_step 3700, step 380, avg_time 1.065, loss:585.9803
g_step 3800, step 65, avg_time 1.067, loss:626.7509
g_step 3900, step 165, avg_time 1.062, loss:532.5518
g_step 4000, step 265, avg_time 1.068, loss:568.9455
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5538, rec:0.5876, f1:0.5702
>> valid relation prec:0.2728, rec:0.0528, f1:0.0885
>> valid relation with NER prec:0.2728, rec:0.0528, f1:0.0885
g_step 4100, step 365, avg_time 2.854, loss:587.1633
g_step 4200, step 50, avg_time 1.080, loss:540.6103
g_step 4300, step 150, avg_time 1.066, loss:548.5662
g_step 4400, step 250, avg_time 1.067, loss:557.0062
g_step 4500, step 350, avg_time 1.069, loss:541.8791
>> valid entity prec:0.5375, rec:0.5477, f1:0.5425
>> valid relation prec:0.2277, rec:0.0457, f1:0.0762
>> valid relation with NER prec:0.2277, rec:0.0457, f1:0.0762
g_step 4600, step 35, avg_time 2.839, loss:555.6300
g_step 4700, step 135, avg_time 1.075, loss:514.1018
g_step 4800, step 235, avg_time 1.077, loss:551.3789
g_step 4900, step 335, avg_time 1.051, loss:545.9001
g_step 5000, step 20, avg_time 1.065, loss:526.8403
learning rate was adjusted to 0.0008
>> valid entity prec:0.5317, rec:0.5303, f1:0.5310
>> valid relation prec:0.2297, rec:0.0489, f1:0.0806
>> valid relation with NER prec:0.2297, rec:0.0489, f1:0.0806
g_step 5100, step 120, avg_time 2.845, loss:504.7112
g_step 5200, step 220, avg_time 1.065, loss:503.5250
g_step 5300, step 320, avg_time 1.069, loss:526.8515
g_step 5400, step 5, avg_time 1.068, loss:526.6618
g_step 5500, step 105, avg_time 1.068, loss:488.7018
>> valid entity prec:0.5809, rec:0.5024, f1:0.5388
>> valid relation prec:0.2743, rec:0.0645, f1:0.1044
>> valid relation with NER prec:0.2743, rec:0.0645, f1:0.1044
g_step 5600, step 205, avg_time 2.847, loss:502.9622
g_step 5700, step 305, avg_time 1.064, loss:504.2745
g_step 5800, step 405, avg_time 1.079, loss:527.2914
g_step 5900, step 90, avg_time 1.063, loss:480.4588
g_step 6000, step 190, avg_time 1.069, loss:489.1170
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4859, rec:0.4786, f1:0.4822
>> valid relation prec:0.2453, rec:0.0620, f1:0.0989
>> valid relation with NER prec:0.2453, rec:0.0620, f1:0.0989
g_step 6100, step 290, avg_time 2.841, loss:502.6716
g_step 6200, step 390, avg_time 1.076, loss:482.9731
g_step 6300, step 75, avg_time 1.066, loss:468.1966
g_step 6400, step 175, avg_time 1.059, loss:455.3399
g_step 6500, step 275, avg_time 1.075, loss:449.9182
>> valid entity prec:0.5632, rec:0.4343, f1:0.4904
>> valid relation prec:0.2492, rec:0.0476, f1:0.0800
>> valid relation with NER prec:0.2492, rec:0.0476, f1:0.0800
g_step 6600, step 375, avg_time 2.838, loss:494.5064
g_step 6700, step 60, avg_time 1.070, loss:466.6727
g_step 6800, step 160, avg_time 1.074, loss:461.0324
g_step 6900, step 260, avg_time 1.079, loss:445.1678
g_step 7000, step 360, avg_time 1.073, loss:461.9825
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5435, rec:0.5375, f1:0.5405
>> valid relation prec:0.2233, rec:0.0439, f1:0.0733
>> valid relation with NER prec:0.2233, rec:0.0439, f1:0.0733
g_step 7100, step 45, avg_time 2.860, loss:454.2656
g_step 7200, step 145, avg_time 1.065, loss:440.9385
g_step 7300, step 245, avg_time 1.072, loss:439.2272
g_step 7400, step 345, avg_time 1.066, loss:445.8634
g_step 7500, step 30, avg_time 1.063, loss:440.9858
>> valid entity prec:0.5197, rec:0.5120, f1:0.5158
>> valid relation prec:0.2956, rec:0.0543, f1:0.0917
>> valid relation with NER prec:0.2956, rec:0.0543, f1:0.0917
g_step 7600, step 130, avg_time 2.845, loss:429.5356
g_step 7700, step 230, avg_time 1.077, loss:442.7643
g_step 7800, step 330, avg_time 1.083, loss:423.4689
g_step 7900, step 15, avg_time 1.079, loss:433.5447
g_step 8000, step 115, avg_time 1.066, loss:406.3624
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5604, rec:0.4728, f1:0.5129
>> valid relation prec:0.1966, rec:0.0414, f1:0.0684
>> valid relation with NER prec:0.1966, rec:0.0414, f1:0.0684
g_step 8100, step 215, avg_time 2.846, loss:411.6870
g_step 8200, step 315, avg_time 1.071, loss:405.4305
g_step 8300, step 415, avg_time 1.082, loss:441.3977
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 08:35:44 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 08:35:44 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_08-35-44_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 08:35:45 - WARNING - datasets.builder -   Using custom data configuration default-69e5266f88b20646
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-69e5266f88b20646/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 08:35:46,060 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:35:46,061 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 08:35:46,062 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:35:46,063 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 08:35:46,071 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:35:46,073 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:35:46,074 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:35:46,074 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:35:46,074 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:35:46,074 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:35:46,074 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 08:35:46,207 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 08:35:49,452 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 08:35:49,453 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-69e5266f88b20646/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  3.13ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.92ba/s] 27%|██▋       | 3/11 [00:00<00:01,  4.29ba/s] 36%|███▋      | 4/11 [00:00<00:01,  4.45ba/s] 45%|████▌     | 5/11 [00:01<00:01,  4.54ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.60ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.64ba/s] 73%|███████▎  | 8/11 [00:01<00:00,  4.66ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.68ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.69ba/s]100%|██████████| 11/11 [00:02<00:00,  4.93ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.11ba/s] 40%|████      | 2/5 [00:00<00:00,  4.33ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.41ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.43ba/s]100%|██████████| 5/5 [00:01<00:00,  4.73ba/s]100%|██████████| 5/5 [00:01<00:00,  4.55ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  9.22ba/s] 27%|██▋       | 3/11 [00:00<00:00, 10.38ba/s] 45%|████▌     | 5/11 [00:00<00:00, 10.52ba/s] 64%|██████▎   | 7/11 [00:00<00:00, 10.59ba/s] 82%|████████▏ | 9/11 [00:00<00:00, 10.54ba/s]100%|██████████| 11/11 [00:00<00:00, 11.49ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.09ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.26ba/s]100%|██████████| 5/5 [00:00<00:00, 11.20ba/s]100%|██████████| 5/5 [00:00<00:00, 10.88ba/s]
[INFO|trainer.py:414] 2023-08-29 08:35:54,583 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 08:35:54,594 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 08:35:54,594 >>   Num examples = 10032
[INFO|trainer.py:1149] 2023-08-29 08:35:54,594 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 08:35:54,594 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 08:35:54,594 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 08:35:54,594 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 08:35:54,594 >>   Total optimization steps = 785
  0%|          | 0/785 [00:00<?, ?it/s]  0%|          | 1/785 [00:00<03:53,  3.35it/s]  0%|          | 2/785 [00:00<03:48,  3.43it/s]  0%|          | 3/785 [00:00<03:46,  3.46it/s]  1%|          | 4/785 [00:01<03:45,  3.47it/s]  1%|          | 5/785 [00:01<03:45,  3.46it/s]  1%|          | 6/785 [00:01<03:45,  3.46it/s]  1%|          | 7/785 [00:02<03:44,  3.47it/s]  1%|          | 8/785 [00:02<03:43,  3.47it/s]  1%|          | 9/785 [00:02<03:43,  3.48it/s]  1%|▏         | 10/785 [00:02<03:42,  3.48it/s]  1%|▏         | 11/785 [00:03<03:42,  3.48it/s]  2%|▏         | 12/785 [00:03<03:42,  3.48it/s]  2%|▏         | 13/785 [00:03<03:41,  3.48it/s]  2%|▏         | 14/785 [00:04<03:41,  3.48it/s]  2%|▏         | 15/785 [00:04<03:41,  3.48it/s]  2%|▏         | 16/785 [00:04<03:41,  3.47it/s]  2%|▏         | 17/785 [00:04<03:41,  3.47it/s]  2%|▏         | 18/785 [00:05<03:40,  3.48it/s]  2%|▏         | 19/785 [00:05<03:40,  3.48it/s]  3%|▎         | 20/785 [00:05<03:40,  3.48it/s]  3%|▎         | 21/785 [00:06<03:39,  3.48it/s]  3%|▎         | 22/785 [00:06<03:39,  3.48it/s]  3%|▎         | 23/785 [00:06<03:38,  3.48it/s]  3%|▎         | 24/785 [00:06<03:38,  3.48it/s]  3%|▎         | 25/785 [00:07<03:38,  3.48it/s]  3%|▎         | 26/785 [00:07<03:37,  3.48it/s]  3%|▎         | 27/785 [00:07<03:38,  3.47it/s]  4%|▎         | 28/785 [00:08<03:38,  3.47it/s]  4%|▎         | 29/785 [00:08<03:37,  3.47it/s]  4%|▍         | 30/785 [00:08<03:37,  3.47it/s]  4%|▍         | 31/785 [00:08<03:36,  3.48it/s]  4%|▍         | 32/785 [00:09<03:36,  3.48it/s]  4%|▍         | 33/785 [00:09<03:36,  3.48it/s]  4%|▍         | 34/785 [00:09<03:35,  3.48it/s]  4%|▍         | 35/785 [00:10<03:35,  3.48it/s]  5%|▍         | 36/785 [00:10<03:35,  3.48it/s]  5%|▍         | 37/785 [00:10<03:34,  3.48it/s]  5%|▍         | 38/785 [00:10<03:35,  3.47it/s]  5%|▍         | 39/785 [00:11<03:34,  3.47it/s]  5%|▌         | 40/785 [00:11<03:34,  3.47it/s]  5%|▌         | 41/785 [00:11<03:34,  3.48it/s]  5%|▌         | 42/785 [00:12<03:33,  3.48it/s]  5%|▌         | 43/785 [00:12<03:33,  3.48it/s]  6%|▌         | 44/785 [00:12<03:33,  3.48it/s]  6%|▌         | 45/785 [00:12<03:32,  3.48it/s]  6%|▌         | 46/785 [00:13<03:32,  3.48it/s]  6%|▌         | 47/785 [00:13<03:32,  3.48it/s]  6%|▌         | 48/785 [00:13<03:32,  3.48it/s]  6%|▌         | 49/785 [00:14<03:32,  3.46it/s]  6%|▋         | 50/785 [00:14<03:31,  3.47it/s]  6%|▋         | 51/785 [00:14<03:31,  3.47it/s]  7%|▋         | 52/785 [00:14<03:31,  3.47it/s]  7%|▋         | 53/785 [00:15<03:30,  3.47it/s]  7%|▋         | 54/785 [00:15<03:30,  3.47it/s]  7%|▋         | 55/785 [00:15<03:30,  3.47it/s]  7%|▋         | 56/785 [00:16<03:29,  3.48it/s]  7%|▋         | 57/785 [00:16<03:29,  3.48it/s]  7%|▋         | 58/785 [00:16<03:29,  3.48it/s]  8%|▊         | 59/785 [00:16<03:28,  3.48it/s]  8%|▊         | 60/785 [00:17<03:29,  3.47it/s]  8%|▊         | 61/785 [00:17<03:28,  3.47it/s]  8%|▊         | 62/785 [00:17<03:28,  3.47it/s]  8%|▊         | 63/785 [00:18<03:27,  3.47it/s]  8%|▊         | 64/785 [00:18<03:27,  3.47it/s]  8%|▊         | 65/785 [00:18<03:27,  3.47it/s]  8%|▊         | 66/785 [00:18<03:26,  3.47it/s]  9%|▊         | 67/785 [00:19<03:26,  3.47it/s]  9%|▊         | 68/785 [00:19<03:26,  3.48it/s]  9%|▉         | 69/785 [00:19<03:25,  3.48it/s]  9%|▉         | 70/785 [00:20<03:25,  3.48it/s]  9%|▉         | 71/785 [00:20<03:25,  3.47it/s]  9%|▉         | 72/785 [00:20<03:25,  3.47it/s]  9%|▉         | 73/785 [00:21<03:25,  3.47it/s]  9%|▉         | 74/785 [00:21<03:24,  3.47it/s] 10%|▉         | 75/785 [00:21<03:24,  3.47it/s] 10%|▉         | 76/785 [00:21<03:24,  3.47it/s] 10%|▉         | 77/785 [00:22<03:23,  3.47it/s] 10%|▉         | 78/785 [00:22<03:23,  3.47it/s] 10%|█         | 79/785 [00:22<03:23,  3.47it/s] 10%|█         | 80/785 [00:23<03:23,  3.47it/s] 10%|█         | 81/785 [00:23<03:23,  3.47it/s] 10%|█         | 82/785 [00:23<03:23,  3.46it/s] 11%|█         | 83/785 [00:23<03:22,  3.46it/s] 11%|█         | 84/785 [00:24<03:22,  3.46it/s] 11%|█         | 85/785 [00:24<03:22,  3.46it/s] 11%|█         | 86/785 [00:24<03:22,  3.46it/s] 11%|█         | 87/785 [00:25<03:21,  3.46it/s] 11%|█         | 88/785 [00:25<03:21,  3.46it/s] 11%|█▏        | 89/785 [00:25<03:21,  3.46it/s] 11%|█▏        | 90/785 [00:25<03:20,  3.46it/s] 12%|█▏        | 91/785 [00:26<03:20,  3.46it/s] 12%|█▏        | 92/785 [00:26<03:20,  3.46it/s] 12%|█▏        | 93/785 [00:26<03:20,  3.45it/s] 12%|█▏        | 94/785 [00:27<03:19,  3.46it/s] 12%|█▏        | 95/785 [00:27<03:19,  3.45it/s] 12%|█▏        | 96/785 [00:27<03:19,  3.46it/s] 12%|█▏        | 97/785 [00:27<03:18,  3.46it/s] 12%|█▏        | 98/785 [00:28<03:18,  3.46it/s] 13%|█▎        | 99/785 [00:28<03:18,  3.46it/s] 13%|█▎        | 100/785 [00:28<03:17,  3.46it/s] 13%|█▎        | 101/785 [00:29<03:17,  3.47it/s] 13%|█▎        | 102/785 [00:29<03:17,  3.46it/s] 13%|█▎        | 103/785 [00:29<03:16,  3.46it/s] 13%|█▎        | 104/785 [00:29<03:16,  3.46it/s] 13%|█▎        | 105/785 [00:30<03:16,  3.46it/s] 14%|█▎        | 106/785 [00:30<03:16,  3.46it/s] 14%|█▎        | 107/785 [00:30<03:15,  3.46it/s] 14%|█▍        | 108/785 [00:31<03:15,  3.46it/s] 14%|█▍        | 109/785 [00:31<03:15,  3.46it/s] 14%|█▍        | 110/785 [00:31<03:14,  3.46it/s] 14%|█▍        | 111/785 [00:31<03:14,  3.46it/s] 14%|█▍        | 112/785 [00:32<03:14,  3.46it/s] 14%|█▍        | 113/785 [00:32<03:14,  3.46it/s] 15%|█▍        | 114/785 [00:32<03:13,  3.46it/s] 15%|█▍        | 115/785 [00:33<03:14,  3.45it/s] 15%|█▍        | 116/785 [00:33<03:13,  3.45it/s] 15%|█▍        | 117/785 [00:33<03:13,  3.45it/s] 15%|█▌        | 118/785 [00:34<03:12,  3.46it/s] 15%|█▌        | 119/785 [00:34<03:12,  3.46it/s] 15%|█▌        | 120/785 [00:34<03:12,  3.46it/s] 15%|█▌        | 121/785 [00:34<03:12,  3.46it/s] 16%|█▌        | 122/785 [00:35<03:11,  3.46it/s] 16%|█▌        | 123/785 [00:35<03:11,  3.46it/s] 16%|█▌        | 124/785 [00:35<03:11,  3.46it/s] 16%|█▌        | 125/785 [00:36<03:10,  3.46it/s] 16%|█▌        | 126/785 [00:36<03:11,  3.45it/s] 16%|█▌        | 127/785 [00:36<03:10,  3.45it/s] 16%|█▋        | 128/785 [00:36<03:10,  3.45it/s] 16%|█▋        | 129/785 [00:37<03:09,  3.46it/s] 17%|█▋        | 130/785 [00:37<03:09,  3.46it/s] 17%|█▋        | 131/785 [00:37<03:09,  3.46it/s] 17%|█▋        | 132/785 [00:38<03:08,  3.46it/s] 17%|█▋        | 133/785 [00:38<03:08,  3.46it/s] 17%|█▋        | 134/785 [00:38<03:08,  3.46it/s] 17%|█▋        | 135/785 [00:38<03:07,  3.46it/s] 17%|█▋        | 136/785 [00:39<03:07,  3.46it/s] 17%|█▋        | 137/785 [00:39<03:07,  3.45it/s] 18%|█▊        | 138/785 [00:39<03:07,  3.46it/s] 18%|█▊        | 139/785 [00:40<03:06,  3.46it/s] 18%|█▊        | 140/785 [00:40<03:06,  3.46it/s] 18%|█▊        | 141/785 [00:40<03:06,  3.45it/s] 18%|█▊        | 142/785 [00:40<03:06,  3.45it/s] 18%|█▊        | 143/785 [00:41<03:05,  3.46it/s] 18%|█▊        | 144/785 [00:41<03:05,  3.46it/s] 18%|█▊        | 145/785 [00:41<03:05,  3.46it/s] 19%|█▊        | 146/785 [00:42<03:04,  3.46it/s] 19%|█▊        | 147/785 [00:42<03:04,  3.46it/s] 19%|█▉        | 148/785 [00:42<03:05,  3.44it/s] 19%|█▉        | 149/785 [00:42<03:04,  3.44it/s] 19%|█▉        | 150/785 [00:43<03:04,  3.45it/s] 19%|█▉        | 151/785 [00:43<03:03,  3.45it/s] 19%|█▉        | 152/785 [00:43<03:03,  3.46it/s] 19%|█▉        | 153/785 [00:44<03:02,  3.46it/s] 20%|█▉        | 154/785 [00:44<03:02,  3.46it/s] 20%|█▉        | 155/785 [00:44<03:02,  3.46it/s] 20%|█▉        | 156/785 [00:45<03:01,  3.46it/s] 20%|██        | 157/785 [00:45<02:49,  3.70it/s][INFO|trainer.py:2140] 2023-08-29 08:36:39,837 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:36:39,838 >>   Num examples = 4812
[INFO|trainer.py:2145] 2023-08-29 08:36:39,838 >>   Batch size = 8

  0%|          | 0/602 [00:00<?, ?it/s][A
  1%|          | 6/602 [00:00<00:10, 57.09it/s][A
  2%|▏         | 12/602 [00:00<00:11, 50.44it/s][A
  3%|▎         | 18/602 [00:00<00:12, 48.58it/s][A
  4%|▍         | 23/602 [00:00<00:12, 47.98it/s][A
  5%|▍         | 28/602 [00:00<00:12, 47.48it/s][A
  5%|▌         | 33/602 [00:00<00:12, 47.18it/s][A
  6%|▋         | 38/602 [00:00<00:11, 47.14it/s][A
  7%|▋         | 43/602 [00:00<00:11, 46.77it/s][A
  8%|▊         | 48/602 [00:01<00:11, 46.78it/s][A
  9%|▉         | 53/602 [00:01<00:11, 46.72it/s][A
 10%|▉         | 58/602 [00:01<00:11, 46.62it/s][A
 10%|█         | 63/602 [00:01<00:11, 46.72it/s][A
 11%|█▏        | 68/602 [00:01<00:11, 46.69it/s][A
 12%|█▏        | 73/602 [00:01<00:11, 46.65it/s][A
 13%|█▎        | 78/602 [00:01<00:11, 46.71it/s][A
 14%|█▍        | 83/602 [00:01<00:11, 46.76it/s][A
 15%|█▍        | 88/602 [00:01<00:11, 46.54it/s][A
 15%|█▌        | 93/602 [00:01<00:10, 46.62it/s][A
 16%|█▋        | 98/602 [00:02<00:10, 46.55it/s][A
 17%|█▋        | 103/602 [00:02<00:10, 46.65it/s][A
 18%|█▊        | 108/602 [00:02<00:10, 46.66it/s][A
 19%|█▉        | 113/602 [00:02<00:10, 46.55it/s][A
 20%|█▉        | 118/602 [00:02<00:10, 46.61it/s][A
 20%|██        | 123/602 [00:02<00:10, 46.68it/s][A
 21%|██▏       | 128/602 [00:02<00:10, 46.61it/s][A
 22%|██▏       | 133/602 [00:02<00:10, 46.61it/s][A
 23%|██▎       | 138/602 [00:02<00:09, 46.60it/s][A
 24%|██▍       | 143/602 [00:03<00:09, 46.61it/s][A
 25%|██▍       | 148/602 [00:03<00:09, 46.54it/s][A
 25%|██▌       | 153/602 [00:03<00:09, 46.57it/s][A
 26%|██▌       | 158/602 [00:03<00:09, 46.63it/s][A
 27%|██▋       | 163/602 [00:03<00:09, 46.63it/s][A
 28%|██▊       | 168/602 [00:03<00:09, 46.65it/s][A
 29%|██▊       | 173/602 [00:03<00:09, 46.69it/s][A
 30%|██▉       | 178/602 [00:03<00:09, 46.62it/s][A
 30%|███       | 183/602 [00:03<00:09, 46.54it/s][A
 31%|███       | 188/602 [00:04<00:08, 46.60it/s][A
 32%|███▏      | 193/602 [00:04<00:08, 46.63it/s][A
 33%|███▎      | 198/602 [00:04<00:08, 46.67it/s][A
 34%|███▎      | 203/602 [00:04<00:08, 46.62it/s][A
 35%|███▍      | 208/602 [00:04<00:08, 46.63it/s][A
 35%|███▌      | 213/602 [00:04<00:08, 46.59it/s][A
 36%|███▌      | 218/602 [00:04<00:08, 46.64it/s][A
 37%|███▋      | 223/602 [00:04<00:08, 42.46it/s][A
 38%|███▊      | 228/602 [00:04<00:08, 44.27it/s][A
 39%|███▊      | 233/602 [00:05<00:08, 44.88it/s][A
 40%|███▉      | 238/602 [00:05<00:08, 45.44it/s][A
 40%|████      | 243/602 [00:05<00:07, 45.83it/s][A
 41%|████      | 248/602 [00:05<00:07, 46.08it/s][A
 42%|████▏     | 253/602 [00:05<00:07, 46.32it/s][A
 43%|████▎     | 258/602 [00:05<00:07, 46.39it/s][A
 44%|████▎     | 263/602 [00:05<00:07, 46.40it/s][A
 45%|████▍     | 268/602 [00:05<00:07, 46.38it/s][A
 45%|████▌     | 273/602 [00:05<00:07, 46.37it/s][A
 46%|████▌     | 278/602 [00:05<00:06, 46.48it/s][A
 47%|████▋     | 283/602 [00:06<00:06, 46.59it/s][A
 48%|████▊     | 288/602 [00:06<00:06, 46.60it/s][A
 49%|████▊     | 293/602 [00:06<00:06, 46.60it/s][A
 50%|████▉     | 298/602 [00:06<00:06, 46.55it/s][A
 50%|█████     | 303/602 [00:06<00:06, 46.62it/s][A
 51%|█████     | 308/602 [00:06<00:06, 46.59it/s][A
 52%|█████▏    | 313/602 [00:06<00:06, 46.51it/s][A
 53%|█████▎    | 318/602 [00:06<00:06, 46.58it/s][A
 54%|█████▎    | 323/602 [00:06<00:05, 46.59it/s][A
 54%|█████▍    | 328/602 [00:07<00:05, 46.51it/s][A
 55%|█████▌    | 333/602 [00:07<00:05, 46.56it/s][A
 56%|█████▌    | 338/602 [00:07<00:05, 46.67it/s][A
 57%|█████▋    | 343/602 [00:07<00:05, 46.64it/s][A
 58%|█████▊    | 348/602 [00:07<00:05, 46.67it/s][A
 59%|█████▊    | 353/602 [00:07<00:05, 46.61it/s][A
 59%|█████▉    | 358/602 [00:07<00:05, 46.57it/s][A
 60%|██████    | 363/602 [00:07<00:05, 46.52it/s][A
 61%|██████    | 368/602 [00:07<00:05, 46.52it/s][A
 62%|██████▏   | 373/602 [00:08<00:04, 46.55it/s][A
 63%|██████▎   | 378/602 [00:08<00:04, 46.61it/s][A
 64%|██████▎   | 383/602 [00:08<00:04, 46.58it/s][A
 64%|██████▍   | 388/602 [00:08<00:04, 46.61it/s][A
 65%|██████▌   | 393/602 [00:08<00:04, 46.70it/s][A
 66%|██████▌   | 398/602 [00:08<00:04, 46.72it/s][A
 67%|██████▋   | 403/602 [00:08<00:04, 46.53it/s][A
 68%|██████▊   | 408/602 [00:08<00:04, 46.50it/s][A
 69%|██████▊   | 413/602 [00:08<00:04, 46.60it/s][A
 69%|██████▉   | 418/602 [00:08<00:03, 46.59it/s][A
 70%|███████   | 423/602 [00:09<00:03, 46.57it/s][A
 71%|███████   | 428/602 [00:09<00:03, 46.59it/s][A
 72%|███████▏  | 433/602 [00:09<00:03, 46.62it/s][A
 73%|███████▎  | 438/602 [00:09<00:03, 46.64it/s][A
 74%|███████▎  | 443/602 [00:09<00:03, 46.68it/s][A
 74%|███████▍  | 448/602 [00:09<00:03, 46.58it/s][A
 75%|███████▌  | 453/602 [00:09<00:03, 46.59it/s][A
 76%|███████▌  | 458/602 [00:09<00:03, 46.45it/s][A
 77%|███████▋  | 463/602 [00:09<00:02, 46.52it/s][A
 78%|███████▊  | 468/602 [00:10<00:02, 46.60it/s][A
 79%|███████▊  | 473/602 [00:10<00:02, 46.61it/s][A
 79%|███████▉  | 478/602 [00:10<00:02, 46.56it/s][A
 80%|████████  | 483/602 [00:10<00:02, 46.63it/s][A
 81%|████████  | 488/602 [00:10<00:02, 46.65it/s][A
 82%|████████▏ | 493/602 [00:10<00:02, 46.58it/s][A
 83%|████████▎ | 498/602 [00:10<00:02, 46.57it/s][A
 84%|████████▎ | 503/602 [00:10<00:02, 46.56it/s][A
 84%|████████▍ | 508/602 [00:10<00:02, 46.50it/s][A
 85%|████████▌ | 513/602 [00:11<00:01, 46.54it/s][A
 86%|████████▌ | 518/602 [00:11<00:01, 46.58it/s][A
 87%|████████▋ | 523/602 [00:11<00:01, 46.60it/s][A
 88%|████████▊ | 528/602 [00:11<00:01, 46.63it/s][A
 89%|████████▊ | 533/602 [00:11<00:01, 46.61it/s][A
 89%|████████▉ | 538/602 [00:11<00:01, 46.56it/s][A
 90%|█████████ | 543/602 [00:11<00:01, 46.53it/s][A
 91%|█████████ | 548/602 [00:11<00:01, 46.45it/s][A
 92%|█████████▏| 553/602 [00:11<00:01, 46.58it/s][A
 93%|█████████▎| 558/602 [00:11<00:00, 46.54it/s][A
 94%|█████████▎| 563/602 [00:12<00:00, 46.56it/s][A
 94%|█████████▍| 568/602 [00:12<00:00, 46.54it/s][A
 95%|█████████▌| 573/602 [00:12<00:00, 46.54it/s][A
 96%|█████████▌| 578/602 [00:12<00:00, 46.62it/s][A
 97%|█████████▋| 583/602 [00:12<00:00, 46.62it/s][A
 98%|█████████▊| 588/602 [00:12<00:00, 46.50it/s][A
 99%|█████████▊| 593/602 [00:12<00:00, 46.42it/s][A
 99%|█████████▉| 598/602 [00:12<00:00, 46.39it/s][A                                                 
                                                 [A 20%|██        | 157/785 [00:58<02:49,  3.70it/s]
100%|██████████| 602/602 [00:12<00:00, 46.39it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:36:52,807 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-157
[INFO|configuration_utils.py:351] 2023-08-29 08:36:52,824 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-157/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:36:55,066 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-157/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:36:55,082 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-157/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:36:55,092 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-157/special_tokens_map.json
 20%|██        | 158/785 [01:05<1:06:42,  6.38s/it] 20%|██        | 159/785 [01:06<47:33,  4.56s/it]   20%|██        | 160/785 [01:06<34:08,  3.28s/it] 21%|██        | 161/785 [01:06<24:45,  2.38s/it] 21%|██        | 162/785 [01:07<18:11,  1.75s/it] 21%|██        | 163/785 [01:07<13:37,  1.31s/it] 21%|██        | 164/785 [01:07<10:24,  1.01s/it] 21%|██        | 165/785 [01:07<08:10,  1.26it/s] 21%|██        | 166/785 [01:08<06:36,  1.56it/s] 21%|██▏       | 167/785 [01:08<05:30,  1.87it/s] 21%|██▏       | 168/785 [01:08<04:44,  2.17it/s] 22%|██▏       | 169/785 [01:09<04:11,  2.45it/s] 22%|██▏       | 170/785 [01:09<03:49,  2.68it/s] 22%|██▏       | 171/785 [01:09<03:33,  2.87it/s] 22%|██▏       | 172/785 [01:09<03:22,  3.03it/s] 22%|██▏       | 173/785 [01:10<03:14,  3.15it/s] 22%|██▏       | 174/785 [01:10<03:08,  3.24it/s] 22%|██▏       | 175/785 [01:10<03:04,  3.30it/s] 22%|██▏       | 176/785 [01:11<03:01,  3.35it/s] 23%|██▎       | 177/785 [01:11<02:59,  3.38it/s] 23%|██▎       | 178/785 [01:11<02:58,  3.41it/s] 23%|██▎       | 179/785 [01:11<02:57,  3.42it/s] 23%|██▎       | 180/785 [01:12<02:56,  3.44it/s] 23%|██▎       | 181/785 [01:12<02:56,  3.43it/s] 23%|██▎       | 182/785 [01:12<02:55,  3.44it/s] 23%|██▎       | 183/785 [01:13<02:54,  3.44it/s] 23%|██▎       | 184/785 [01:13<02:54,  3.44it/s] 24%|██▎       | 185/785 [01:13<02:53,  3.45it/s] 24%|██▎       | 186/785 [01:13<02:53,  3.45it/s] 24%|██▍       | 187/785 [01:14<02:53,  3.46it/s] 24%|██▍       | 188/785 [01:14<02:52,  3.46it/s] 24%|██▍       | 189/785 [01:14<02:52,  3.46it/s] 24%|██▍       | 190/785 [01:15<02:52,  3.46it/s] 24%|██▍       | 191/785 [01:15<02:51,  3.46it/s] 24%|██▍       | 192/785 [01:15<02:51,  3.46it/s] 25%|██▍       | 193/785 [01:16<02:51,  3.46it/s] 25%|██▍       | 194/785 [01:16<02:50,  3.46it/s] 25%|██▍       | 195/785 [01:16<02:50,  3.46it/s] 25%|██▍       | 196/785 [01:16<02:50,  3.46it/s] 25%|██▌       | 197/785 [01:17<02:50,  3.46it/s] 25%|██▌       | 198/785 [01:17<02:49,  3.46it/s] 25%|██▌       | 199/785 [01:17<02:49,  3.46it/s] 25%|██▌       | 200/785 [01:18<02:49,  3.46it/s] 26%|██▌       | 201/785 [01:18<02:48,  3.46it/s] 26%|██▌       | 202/785 [01:18<02:48,  3.46it/s] 26%|██▌       | 203/785 [01:18<02:49,  3.44it/s] 26%|██▌       | 204/785 [01:19<02:48,  3.44it/s] 26%|██▌       | 205/785 [01:19<02:48,  3.45it/s] 26%|██▌       | 206/785 [01:19<02:47,  3.45it/s] 26%|██▋       | 207/785 [01:20<02:47,  3.45it/s] 26%|██▋       | 208/785 [01:20<02:47,  3.45it/s] 27%|██▋       | 209/785 [01:20<02:46,  3.45it/s] 27%|██▋       | 210/785 [01:20<02:46,  3.45it/s] 27%|██▋       | 211/785 [01:21<02:46,  3.46it/s] 27%|██▋       | 212/785 [01:21<02:45,  3.46it/s] 27%|██▋       | 213/785 [01:21<02:45,  3.46it/s] 27%|██▋       | 214/785 [01:22<02:45,  3.46it/s] 27%|██▋       | 215/785 [01:22<02:44,  3.46it/s] 28%|██▊       | 216/785 [01:22<02:44,  3.46it/s] 28%|██▊       | 217/785 [01:22<02:44,  3.46it/s] 28%|██▊       | 218/785 [01:23<02:44,  3.46it/s] 28%|██▊       | 219/785 [01:23<02:43,  3.46it/s] 28%|██▊       | 220/785 [01:23<02:43,  3.46it/s] 28%|██▊       | 221/785 [01:24<02:43,  3.46it/s] 28%|██▊       | 222/785 [01:24<02:42,  3.46it/s] 28%|██▊       | 223/785 [01:24<02:42,  3.46it/s] 29%|██▊       | 224/785 [01:24<02:42,  3.46it/s] 29%|██▊       | 225/785 [01:25<02:42,  3.45it/s] 29%|██▉       | 226/785 [01:25<02:41,  3.45it/s] 29%|██▉       | 227/785 [01:25<02:41,  3.46it/s] 29%|██▉       | 228/785 [01:26<02:41,  3.46it/s] 29%|██▉       | 229/785 [01:26<02:40,  3.46it/s] 29%|██▉       | 230/785 [01:26<02:40,  3.46it/s] 29%|██▉       | 231/785 [01:27<02:40,  3.46it/s] 30%|██▉       | 232/785 [01:27<02:39,  3.46it/s] 30%|██▉       | 233/785 [01:27<02:39,  3.46it/s] 30%|██▉       | 234/785 [01:27<02:39,  3.46it/s] 30%|██▉       | 235/785 [01:28<02:38,  3.46it/s] 30%|███       | 236/785 [01:28<02:39,  3.45it/s] 30%|███       | 237/785 [01:28<02:38,  3.45it/s] 30%|███       | 238/785 [01:29<02:38,  3.45it/s] 30%|███       | 239/785 [01:29<02:38,  3.45it/s] 31%|███       | 240/785 [01:29<02:37,  3.45it/s] 31%|███       | 241/785 [01:29<02:37,  3.46it/s] 31%|███       | 242/785 [01:30<02:37,  3.45it/s] 31%|███       | 243/785 [01:30<02:36,  3.46it/s] 31%|███       | 244/785 [01:30<02:36,  3.46it/s] 31%|███       | 245/785 [01:31<02:36,  3.46it/s] 31%|███▏      | 246/785 [01:31<02:35,  3.46it/s] 31%|███▏      | 247/785 [01:31<02:36,  3.45it/s] 32%|███▏      | 248/785 [01:31<02:35,  3.45it/s] 32%|███▏      | 249/785 [01:32<02:35,  3.45it/s] 32%|███▏      | 250/785 [01:32<02:34,  3.45it/s] 32%|███▏      | 251/785 [01:32<02:34,  3.45it/s] 32%|███▏      | 252/785 [01:33<02:34,  3.45it/s] 32%|███▏      | 253/785 [01:33<02:34,  3.45it/s] 32%|███▏      | 254/785 [01:33<02:33,  3.46it/s] 32%|███▏      | 255/785 [01:33<02:33,  3.46it/s] 33%|███▎      | 256/785 [01:34<02:33,  3.46it/s] 33%|███▎      | 257/785 [01:34<02:32,  3.46it/s] 33%|███▎      | 258/785 [01:34<02:33,  3.44it/s] 33%|███▎      | 259/785 [01:35<02:32,  3.44it/s] 33%|███▎      | 260/785 [01:35<02:32,  3.44it/s] 33%|███▎      | 261/785 [01:35<02:32,  3.44it/s] 33%|███▎      | 262/785 [01:35<02:31,  3.45it/s] 34%|███▎      | 263/785 [01:36<02:31,  3.45it/s] 34%|███▎      | 264/785 [01:36<02:30,  3.45it/s] 34%|███▍      | 265/785 [01:36<02:30,  3.45it/s] 34%|███▍      | 266/785 [01:37<02:30,  3.45it/s] 34%|███▍      | 267/785 [01:37<02:29,  3.46it/s] 34%|███▍      | 268/785 [01:37<02:29,  3.46it/s] 34%|███▍      | 269/785 [01:38<02:35,  3.32it/s] 34%|███▍      | 270/785 [01:38<02:33,  3.36it/s] 35%|███▍      | 271/785 [01:38<02:31,  3.39it/s] 35%|███▍      | 272/785 [01:38<02:30,  3.41it/s] 35%|███▍      | 273/785 [01:39<02:29,  3.42it/s] 35%|███▍      | 274/785 [01:39<02:28,  3.43it/s] 35%|███▌      | 275/785 [01:39<02:28,  3.44it/s] 35%|███▌      | 276/785 [01:40<02:27,  3.45it/s] 35%|███▌      | 277/785 [01:40<02:27,  3.45it/s] 35%|███▌      | 278/785 [01:40<02:27,  3.45it/s] 36%|███▌      | 279/785 [01:40<02:26,  3.45it/s] 36%|███▌      | 280/785 [01:41<02:26,  3.44it/s] 36%|███▌      | 281/785 [01:41<02:26,  3.44it/s] 36%|███▌      | 282/785 [01:41<02:25,  3.45it/s] 36%|███▌      | 283/785 [01:42<02:25,  3.45it/s] 36%|███▌      | 284/785 [01:42<02:25,  3.45it/s] 36%|███▋      | 285/785 [01:42<02:24,  3.46it/s] 36%|███▋      | 286/785 [01:42<02:24,  3.46it/s] 37%|███▋      | 287/785 [01:43<02:23,  3.46it/s] 37%|███▋      | 288/785 [01:43<02:23,  3.46it/s] 37%|███▋      | 289/785 [01:43<02:23,  3.46it/s] 37%|███▋      | 290/785 [01:44<02:23,  3.46it/s] 37%|███▋      | 291/785 [01:44<02:23,  3.45it/s] 37%|███▋      | 292/785 [01:44<02:22,  3.45it/s] 37%|███▋      | 293/785 [01:44<02:22,  3.46it/s] 37%|███▋      | 294/785 [01:45<02:22,  3.46it/s] 38%|███▊      | 295/785 [01:45<02:21,  3.46it/s] 38%|███▊      | 296/785 [01:45<02:21,  3.46it/s] 38%|███▊      | 297/785 [01:46<02:21,  3.46it/s] 38%|███▊      | 298/785 [01:46<02:20,  3.46it/s] 38%|███▊      | 299/785 [01:46<02:20,  3.46it/s] 38%|███▊      | 300/785 [01:47<02:20,  3.46it/s] 38%|███▊      | 301/785 [01:47<02:20,  3.46it/s] 38%|███▊      | 302/785 [01:47<02:20,  3.45it/s] 39%|███▊      | 303/785 [01:47<02:19,  3.45it/s] 39%|███▊      | 304/785 [01:48<02:19,  3.45it/s] 39%|███▉      | 305/785 [01:48<02:19,  3.45it/s] 39%|███▉      | 306/785 [01:48<02:19,  3.44it/s] 39%|███▉      | 307/785 [01:49<02:18,  3.45it/s] 39%|███▉      | 308/785 [01:49<02:18,  3.45it/s] 39%|███▉      | 309/785 [01:49<02:17,  3.45it/s] 39%|███▉      | 310/785 [01:49<02:17,  3.45it/s] 40%|███▉      | 311/785 [01:50<02:21,  3.35it/s] 40%|███▉      | 312/785 [01:50<02:20,  3.37it/s] 40%|███▉      | 313/785 [01:50<02:20,  3.36it/s] 40%|████      | 314/785 [01:51<02:10,  3.62it/s][INFO|trainer.py:2140] 2023-08-29 08:37:45,656 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:37:45,656 >>   Num examples = 4812
[INFO|trainer.py:2145] 2023-08-29 08:37:45,656 >>   Batch size = 8
{'eval_loss': 0.9452928900718689, 'eval_runtime': 12.9435, 'eval_samples_per_second': 371.77, 'eval_steps_per_second': 46.51, 'epoch': 1.0}

  0%|          | 0/602 [00:00<?, ?it/s][A
  1%|          | 6/602 [00:00<00:10, 56.96it/s][A
  2%|▏         | 12/602 [00:00<00:11, 50.32it/s][A
  3%|▎         | 18/602 [00:00<00:12, 48.56it/s][A
  4%|▍         | 23/602 [00:00<00:12, 47.97it/s][A
  5%|▍         | 28/602 [00:00<00:12, 47.43it/s][A
  5%|▌         | 33/602 [00:00<00:12, 47.21it/s][A
  6%|▋         | 38/602 [00:00<00:11, 47.03it/s][A
  7%|▋         | 43/602 [00:00<00:11, 46.73it/s][A
  8%|▊         | 48/602 [00:01<00:11, 46.63it/s][A
  9%|▉         | 53/602 [00:01<00:11, 46.69it/s][A
 10%|▉         | 58/602 [00:01<00:11, 46.66it/s][A
 10%|█         | 63/602 [00:01<00:11, 46.65it/s][A
 11%|█▏        | 68/602 [00:01<00:11, 46.65it/s][A
 12%|█▏        | 73/602 [00:01<00:11, 46.62it/s][A
 13%|█▎        | 78/602 [00:01<00:11, 46.64it/s][A
 14%|█▍        | 83/602 [00:01<00:11, 46.61it/s][A
 15%|█▍        | 88/602 [00:01<00:11, 46.63it/s][A
 15%|█▌        | 93/602 [00:01<00:10, 46.47it/s][A
 16%|█▋        | 98/602 [00:02<00:10, 46.48it/s][A
 17%|█▋        | 103/602 [00:02<00:10, 46.56it/s][A
 18%|█▊        | 108/602 [00:02<00:10, 46.66it/s][A
 19%|█▉        | 113/602 [00:02<00:10, 46.46it/s][A
 20%|█▉        | 118/602 [00:02<00:10, 46.64it/s][A
 20%|██        | 123/602 [00:02<00:10, 46.66it/s][A
 21%|██▏       | 128/602 [00:02<00:10, 46.64it/s][A
 22%|██▏       | 133/602 [00:02<00:10, 46.54it/s][A
 23%|██▎       | 138/602 [00:02<00:09, 46.53it/s][A
 24%|██▍       | 143/602 [00:03<00:09, 46.47it/s][A
 25%|██▍       | 148/602 [00:03<00:09, 46.38it/s][A
 25%|██▌       | 153/602 [00:03<00:09, 46.48it/s][A
 26%|██▌       | 158/602 [00:03<00:09, 46.55it/s][A
 27%|██▋       | 163/602 [00:03<00:09, 46.54it/s][A
 28%|██▊       | 168/602 [00:03<00:09, 46.60it/s][A
 29%|██▊       | 173/602 [00:03<00:09, 46.58it/s][A
 30%|██▉       | 178/602 [00:03<00:09, 46.61it/s][A
 30%|███       | 183/602 [00:03<00:08, 46.60it/s][A
 31%|███       | 188/602 [00:04<00:08, 46.60it/s][A
 32%|███▏      | 193/602 [00:04<00:08, 46.54it/s][A
 33%|███▎      | 198/602 [00:04<00:08, 46.53it/s][A
 34%|███▎      | 203/602 [00:04<00:08, 46.55it/s][A
 35%|███▍      | 208/602 [00:04<00:08, 46.53it/s][A
 35%|███▌      | 213/602 [00:04<00:08, 46.63it/s][A
 36%|███▌      | 218/602 [00:04<00:08, 46.60it/s][A
 37%|███▋      | 223/602 [00:04<00:08, 46.58it/s][A
 38%|███▊      | 228/602 [00:04<00:08, 46.65it/s][A
 39%|███▊      | 233/602 [00:04<00:07, 46.63it/s][A
 40%|███▉      | 238/602 [00:05<00:07, 46.54it/s][A
 40%|████      | 243/602 [00:05<00:07, 46.52it/s][A
 41%|████      | 248/602 [00:05<00:07, 46.47it/s][A
 42%|████▏     | 253/602 [00:05<00:07, 46.53it/s][A
 43%|████▎     | 258/602 [00:05<00:07, 46.44it/s][A
 44%|████▎     | 263/602 [00:05<00:07, 46.56it/s][A
 45%|████▍     | 268/602 [00:05<00:07, 46.57it/s][A
 45%|████▌     | 273/602 [00:05<00:07, 46.62it/s][A
 46%|████▌     | 278/602 [00:05<00:06, 46.67it/s][A
 47%|████▋     | 283/602 [00:06<00:06, 46.58it/s][A
 48%|████▊     | 288/602 [00:06<00:06, 46.58it/s][A
 49%|████▊     | 293/602 [00:06<00:06, 46.51it/s][A
 50%|████▉     | 298/602 [00:06<00:06, 46.55it/s][A
 50%|█████     | 303/602 [00:06<00:06, 46.48it/s][A
 51%|█████     | 308/602 [00:06<00:06, 46.55it/s][A
 52%|█████▏    | 313/602 [00:06<00:06, 46.65it/s][A
 53%|█████▎    | 318/602 [00:06<00:06, 46.62it/s][A
 54%|█████▎    | 323/602 [00:06<00:05, 46.59it/s][A
 54%|█████▍    | 328/602 [00:07<00:05, 46.58it/s][A
 55%|█████▌    | 333/602 [00:07<00:05, 46.60it/s][A
 56%|█████▌    | 338/602 [00:07<00:05, 46.59it/s][A
 57%|█████▋    | 343/602 [00:07<00:05, 46.57it/s][A
 58%|█████▊    | 348/602 [00:07<00:05, 46.44it/s][A
 59%|█████▊    | 353/602 [00:07<00:05, 46.52it/s][A
 59%|█████▉    | 358/602 [00:07<00:05, 46.59it/s][A
 60%|██████    | 363/602 [00:07<00:05, 46.63it/s][A
 61%|██████    | 368/602 [00:07<00:05, 46.60it/s][A
 62%|██████▏   | 373/602 [00:07<00:04, 46.57it/s][A
 63%|██████▎   | 378/602 [00:08<00:04, 46.53it/s][A
 64%|██████▎   | 383/602 [00:08<00:04, 46.54it/s][A
 64%|██████▍   | 388/602 [00:08<00:04, 46.55it/s][A
 65%|██████▌   | 393/602 [00:08<00:04, 46.48it/s][A
 66%|██████▌   | 398/602 [00:08<00:04, 46.54it/s][A
 67%|██████▋   | 403/602 [00:08<00:04, 46.49it/s][A
 68%|██████▊   | 408/602 [00:08<00:04, 46.51it/s][A
 69%|██████▊   | 413/602 [00:08<00:04, 46.55it/s][A
 69%|██████▉   | 418/602 [00:08<00:03, 46.63it/s][A
 70%|███████   | 423/602 [00:09<00:03, 46.55it/s][A
 71%|███████   | 428/602 [00:09<00:03, 46.57it/s][A
 72%|███████▏  | 433/602 [00:09<00:03, 46.50it/s][A
 73%|███████▎  | 438/602 [00:09<00:03, 46.51it/s][A
 74%|███████▎  | 443/602 [00:09<00:03, 46.52it/s][A
 74%|███████▍  | 448/602 [00:09<00:03, 46.41it/s][A
 75%|███████▌  | 453/602 [00:09<00:03, 46.43it/s][A
 76%|███████▌  | 458/602 [00:09<00:03, 46.49it/s][A
 77%|███████▋  | 463/602 [00:09<00:02, 46.53it/s][A
 78%|███████▊  | 468/602 [00:10<00:02, 46.60it/s][A
 79%|███████▊  | 473/602 [00:10<00:02, 46.50it/s][A
 79%|███████▉  | 478/602 [00:10<00:02, 46.55it/s][A
 80%|████████  | 483/602 [00:10<00:02, 46.52it/s][A
 81%|████████  | 488/602 [00:10<00:02, 46.55it/s][A
 82%|████████▏ | 493/602 [00:10<00:02, 46.57it/s][A
 83%|████████▎ | 498/602 [00:10<00:02, 46.50it/s][A
 84%|████████▎ | 503/602 [00:10<00:02, 46.46it/s][A
 84%|████████▍ | 508/602 [00:10<00:02, 46.49it/s][A
 85%|████████▌ | 513/602 [00:10<00:01, 46.57it/s][A
 86%|████████▌ | 518/602 [00:11<00:01, 46.61it/s][A
 87%|████████▋ | 523/602 [00:11<00:01, 46.63it/s][A
 88%|████████▊ | 528/602 [00:11<00:01, 46.55it/s][A
 89%|████████▊ | 533/602 [00:11<00:01, 46.55it/s][A
 89%|████████▉ | 538/602 [00:11<00:01, 46.55it/s][A
 90%|█████████ | 543/602 [00:11<00:01, 46.51it/s][A
 91%|█████████ | 548/602 [00:11<00:01, 46.59it/s][A
 92%|█████████▏| 553/602 [00:11<00:01, 46.58it/s][A
 93%|█████████▎| 558/602 [00:11<00:00, 46.63it/s][A
 94%|█████████▎| 563/602 [00:12<00:00, 46.51it/s][A
 94%|█████████▍| 568/602 [00:12<00:00, 46.58it/s][A
 95%|█████████▌| 573/602 [00:12<00:00, 46.62it/s][A
 96%|█████████▌| 578/602 [00:12<00:00, 46.57it/s][A
 97%|█████████▋| 583/602 [00:12<00:00, 46.49it/s][A
 98%|█████████▊| 588/602 [00:12<00:00, 46.58it/s][A
 99%|█████████▊| 593/602 [00:12<00:00, 46.58it/s][A
 99%|█████████▉| 598/602 [00:12<00:00, 46.58it/s][A                                                 
                                                 [A 40%|████      | 314/785 [02:03<02:10,  3.62it/s]
100%|██████████| 602/602 [00:12<00:00, 46.58it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:37:58,601 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-314
[INFO|configuration_utils.py:351] 2023-08-29 08:37:58,644 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-314/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:38:01,100 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-314/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:38:01,121 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-314/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:38:01,134 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-314/special_tokens_map.json
 40%|████      | 315/785 [02:11<50:18,  6.42s/it] 40%|████      | 316/785 [02:12<35:49,  4.58s/it] 40%|████      | 317/785 [02:12<25:41,  3.29s/it] 41%|████      | 318/785 [02:12<18:37,  2.39s/it] 41%|████      | 319/785 [02:12<13:40,  1.76s/it] 41%|████      | 320/785 [02:13<10:13,  1.32s/it] 41%|████      | 321/785 [02:13<07:48,  1.01s/it] 41%|████      | 322/785 [02:13<06:07,  1.26it/s] 41%|████      | 323/785 [02:14<04:56,  1.56it/s] 41%|████▏     | 324/785 [02:14<04:06,  1.87it/s] 41%|████▏     | 325/785 [02:14<03:32,  2.17it/s] 42%|████▏     | 326/785 [02:14<03:07,  2.44it/s] 42%|████▏     | 327/785 [02:15<02:51,  2.68it/s] 42%|████▏     | 328/785 [02:15<02:39,  2.87it/s] 42%|████▏     | 329/785 [02:15<02:30,  3.03it/s] 42%|████▏     | 330/785 [02:16<02:24,  3.15it/s] 42%|████▏     | 331/785 [02:16<02:20,  3.24it/s] 42%|████▏     | 332/785 [02:16<02:17,  3.30it/s] 42%|████▏     | 333/785 [02:17<02:14,  3.35it/s] 43%|████▎     | 334/785 [02:17<02:13,  3.38it/s] 43%|████▎     | 335/785 [02:17<02:12,  3.41it/s] 43%|████▎     | 336/785 [02:17<02:11,  3.42it/s] 43%|████▎     | 337/785 [02:18<02:10,  3.43it/s] 43%|████▎     | 338/785 [02:18<02:10,  3.43it/s] 43%|████▎     | 339/785 [02:18<02:09,  3.44it/s] 43%|████▎     | 340/785 [02:19<02:09,  3.45it/s] 43%|████▎     | 341/785 [02:19<02:08,  3.45it/s] 44%|████▎     | 342/785 [02:19<02:08,  3.45it/s] 44%|████▎     | 343/785 [02:19<02:07,  3.46it/s] 44%|████▍     | 344/785 [02:20<02:07,  3.46it/s] 44%|████▍     | 345/785 [02:20<02:07,  3.46it/s] 44%|████▍     | 346/785 [02:20<02:06,  3.46it/s] 44%|████▍     | 347/785 [02:21<02:06,  3.46it/s] 44%|████▍     | 348/785 [02:21<02:06,  3.46it/s] 44%|████▍     | 349/785 [02:21<02:06,  3.46it/s] 45%|████▍     | 350/785 [02:21<02:05,  3.46it/s] 45%|████▍     | 351/785 [02:22<02:05,  3.46it/s] 45%|████▍     | 352/785 [02:22<02:05,  3.46it/s] 45%|████▍     | 353/785 [02:22<02:04,  3.46it/s] 45%|████▌     | 354/785 [02:23<02:04,  3.46it/s] 45%|████▌     | 355/785 [02:23<02:04,  3.46it/s] 45%|████▌     | 356/785 [02:23<02:03,  3.46it/s] 45%|████▌     | 357/785 [02:23<02:03,  3.47it/s] 46%|████▌     | 358/785 [02:24<02:03,  3.46it/s] 46%|████▌     | 359/785 [02:24<02:03,  3.46it/s] 46%|████▌     | 360/785 [02:24<02:02,  3.46it/s] 46%|████▌     | 361/785 [02:25<02:02,  3.47it/s] 46%|████▌     | 362/785 [02:25<02:02,  3.46it/s] 46%|████▌     | 363/785 [02:25<02:01,  3.46it/s] 46%|████▋     | 364/785 [02:25<02:01,  3.46it/s] 46%|████▋     | 365/785 [02:26<02:01,  3.46it/s] 47%|████▋     | 366/785 [02:26<02:01,  3.46it/s] 47%|████▋     | 367/785 [02:26<02:00,  3.46it/s] 47%|████▋     | 368/785 [02:27<02:00,  3.46it/s] 47%|████▋     | 369/785 [02:27<02:00,  3.46it/s] 47%|████▋     | 370/785 [02:27<01:59,  3.46it/s] 47%|████▋     | 371/785 [02:27<01:59,  3.45it/s] 47%|████▋     | 372/785 [02:28<01:59,  3.46it/s] 48%|████▊     | 373/785 [02:28<01:59,  3.46it/s] 48%|████▊     | 374/785 [02:28<01:58,  3.46it/s] 48%|████▊     | 375/785 [02:29<01:58,  3.46it/s] 48%|████▊     | 376/785 [02:29<01:58,  3.46it/s] 48%|████▊     | 377/785 [02:29<01:58,  3.45it/s] 48%|████▊     | 378/785 [02:30<01:57,  3.45it/s] 48%|████▊     | 379/785 [02:30<01:57,  3.45it/s] 48%|████▊     | 380/785 [02:30<01:57,  3.45it/s] 49%|████▊     | 381/785 [02:30<01:56,  3.46it/s] 49%|████▊     | 382/785 [02:31<01:56,  3.46it/s] 49%|████▉     | 383/785 [02:31<01:56,  3.46it/s] 49%|████▉     | 384/785 [02:31<01:55,  3.46it/s] 49%|████▉     | 385/785 [02:32<01:55,  3.46it/s] 49%|████▉     | 386/785 [02:32<01:55,  3.46it/s] 49%|████▉     | 387/785 [02:32<01:55,  3.46it/s] 49%|████▉     | 388/785 [02:32<01:55,  3.44it/s] 50%|████▉     | 389/785 [02:33<01:55,  3.44it/s] 50%|████▉     | 390/785 [02:33<01:54,  3.44it/s] 50%|████▉     | 391/785 [02:33<01:54,  3.45it/s] 50%|████▉     | 392/785 [02:34<01:53,  3.45it/s] 50%|█████     | 393/785 [02:34<01:53,  3.46it/s] 50%|█████     | 394/785 [02:34<01:53,  3.46it/s] 50%|█████     | 395/785 [02:34<01:52,  3.46it/s] 50%|█████     | 396/785 [02:35<01:52,  3.46it/s] 51%|█████     | 397/785 [02:35<01:52,  3.46it/s] 51%|█████     | 398/785 [02:35<01:51,  3.46it/s] 51%|█████     | 399/785 [02:36<01:55,  3.34it/s] 51%|█████     | 400/785 [02:36<01:54,  3.38it/s] 51%|█████     | 401/785 [02:36<01:52,  3.40it/s] 51%|█████     | 402/785 [02:36<01:51,  3.42it/s] 51%|█████▏    | 403/785 [02:37<01:51,  3.43it/s] 51%|█████▏    | 404/785 [02:37<01:50,  3.44it/s] 52%|█████▏    | 405/785 [02:37<01:50,  3.45it/s] 52%|█████▏    | 406/785 [02:38<01:49,  3.45it/s] 52%|█████▏    | 407/785 [02:38<01:49,  3.45it/s] 52%|█████▏    | 408/785 [02:38<01:49,  3.45it/s] 52%|█████▏    | 409/785 [02:39<01:48,  3.46it/s] 52%|█████▏    | 410/785 [02:39<01:48,  3.45it/s] 52%|█████▏    | 411/785 [02:39<01:48,  3.45it/s] 52%|█████▏    | 412/785 [02:39<01:47,  3.46it/s] 53%|█████▎    | 413/785 [02:40<01:47,  3.46it/s] 53%|█████▎    | 414/785 [02:40<01:47,  3.46it/s] 53%|█████▎    | 415/785 [02:40<01:46,  3.46it/s] 53%|█████▎    | 416/785 [02:41<01:46,  3.46it/s] 53%|█████▎    | 417/785 [02:41<01:46,  3.46it/s] 53%|█████▎    | 418/785 [02:41<01:46,  3.46it/s] 53%|█████▎    | 419/785 [02:41<01:45,  3.46it/s] 54%|█████▎    | 420/785 [02:42<01:45,  3.46it/s] 54%|█████▎    | 421/785 [02:42<01:45,  3.45it/s] 54%|█████▍    | 422/785 [02:42<01:45,  3.46it/s] 54%|█████▍    | 423/785 [02:43<01:44,  3.46it/s] 54%|█████▍    | 424/785 [02:43<01:44,  3.46it/s] 54%|█████▍    | 425/785 [02:43<01:44,  3.46it/s] 54%|█████▍    | 426/785 [02:43<01:43,  3.46it/s] 54%|█████▍    | 427/785 [02:44<01:43,  3.46it/s] 55%|█████▍    | 428/785 [02:44<01:43,  3.46it/s] 55%|█████▍    | 429/785 [02:44<01:42,  3.46it/s] 55%|█████▍    | 430/785 [02:45<01:42,  3.46it/s] 55%|█████▍    | 431/785 [02:45<01:42,  3.46it/s] 55%|█████▌    | 432/785 [02:45<01:42,  3.45it/s] 55%|█████▌    | 433/785 [02:45<01:41,  3.45it/s] 55%|█████▌    | 434/785 [02:46<01:41,  3.46it/s] 55%|█████▌    | 435/785 [02:46<01:41,  3.46it/s] 56%|█████▌    | 436/785 [02:46<01:40,  3.46it/s] 56%|█████▌    | 437/785 [02:47<01:40,  3.46it/s] 56%|█████▌    | 438/785 [02:47<01:40,  3.46it/s] 56%|█████▌    | 439/785 [02:47<01:39,  3.46it/s] 56%|█████▌    | 440/785 [02:47<01:39,  3.46it/s] 56%|█████▌    | 441/785 [02:48<01:39,  3.46it/s] 56%|█████▋    | 442/785 [02:48<01:39,  3.46it/s] 56%|█████▋    | 443/785 [02:48<01:39,  3.44it/s] 57%|█████▋    | 444/785 [02:49<01:38,  3.45it/s] 57%|█████▋    | 445/785 [02:49<01:38,  3.45it/s] 57%|█████▋    | 446/785 [02:49<01:38,  3.45it/s] 57%|█████▋    | 447/785 [02:50<01:37,  3.45it/s] 57%|█████▋    | 448/785 [02:50<01:40,  3.36it/s] 57%|█████▋    | 449/785 [02:50<01:39,  3.38it/s] 57%|█████▋    | 450/785 [02:50<01:38,  3.41it/s] 57%|█████▋    | 451/785 [02:51<01:37,  3.43it/s] 58%|█████▊    | 452/785 [02:51<01:37,  3.43it/s] 58%|█████▊    | 453/785 [02:51<01:36,  3.44it/s] 58%|█████▊    | 454/785 [02:52<01:36,  3.44it/s] 58%|█████▊    | 455/785 [02:52<01:35,  3.44it/s] 58%|█████▊    | 456/785 [02:52<01:35,  3.45it/s] 58%|█████▊    | 457/785 [02:52<01:34,  3.45it/s] 58%|█████▊    | 458/785 [02:53<01:34,  3.45it/s] 58%|█████▊    | 459/785 [02:53<01:34,  3.46it/s] 59%|█████▊    | 460/785 [02:53<01:34,  3.46it/s] 59%|█████▊    | 461/785 [02:54<01:33,  3.46it/s] 59%|█████▉    | 462/785 [02:54<01:33,  3.46it/s] 59%|█████▉    | 463/785 [02:54<01:33,  3.46it/s] 59%|█████▉    | 464/785 [02:54<01:32,  3.46it/s] 59%|█████▉    | 465/785 [02:55<01:32,  3.46it/s] 59%|█████▉    | 466/785 [02:55<01:32,  3.46it/s] 59%|█████▉    | 467/785 [02:55<01:31,  3.46it/s] 60%|█████▉    | 468/785 [02:56<01:31,  3.46it/s] 60%|█████▉    | 469/785 [02:56<01:31,  3.46it/s] 60%|█████▉    | 470/785 [02:56<01:30,  3.46it/s] 60%|██████    | 471/785 [02:56<01:24,  3.70it/s][INFO|trainer.py:2140] 2023-08-29 08:38:51,521 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:38:51,521 >>   Num examples = 4812
[INFO|trainer.py:2145] 2023-08-29 08:38:51,521 >>   Batch size = 8
{'eval_loss': 0.9571865797042847, 'eval_runtime': 12.9247, 'eval_samples_per_second': 372.311, 'eval_steps_per_second': 46.578, 'epoch': 2.0}

  0%|          | 0/602 [00:00<?, ?it/s][A
  1%|          | 6/602 [00:00<00:10, 57.61it/s][A
  2%|▏         | 12/602 [00:00<00:11, 50.59it/s][A
  3%|▎         | 18/602 [00:00<00:11, 48.76it/s][A
  4%|▍         | 23/602 [00:00<00:12, 47.98it/s][A
  5%|▍         | 28/602 [00:00<00:12, 47.47it/s][A
  5%|▌         | 33/602 [00:00<00:12, 47.23it/s][A
  6%|▋         | 38/602 [00:00<00:11, 47.02it/s][A
  7%|▋         | 43/602 [00:00<00:11, 46.67it/s][A
  8%|▊         | 48/602 [00:01<00:11, 46.54it/s][A
  9%|▉         | 53/602 [00:01<00:11, 46.65it/s][A
 10%|▉         | 58/602 [00:01<00:11, 46.66it/s][A
 10%|█         | 63/602 [00:01<00:11, 46.69it/s][A
 11%|█▏        | 68/602 [00:01<00:11, 46.63it/s][A
 12%|█▏        | 73/602 [00:01<00:11, 46.73it/s][A
 13%|█▎        | 78/602 [00:01<00:11, 46.61it/s][A
 14%|█▍        | 83/602 [00:01<00:11, 46.62it/s][A
 15%|█▍        | 88/602 [00:01<00:11, 46.53it/s][A
 15%|█▌        | 93/602 [00:01<00:10, 46.51it/s][A
 16%|█▋        | 98/602 [00:02<00:10, 46.44it/s][A
 17%|█▋        | 103/602 [00:02<00:10, 46.56it/s][A
 18%|█▊        | 108/602 [00:02<00:10, 46.57it/s][A
 19%|█▉        | 113/602 [00:02<00:10, 46.63it/s][A
 20%|█▉        | 118/602 [00:02<00:10, 46.51it/s][A
 20%|██        | 123/602 [00:02<00:10, 46.57it/s][A
 21%|██▏       | 128/602 [00:02<00:10, 46.58it/s][A
 22%|██▏       | 133/602 [00:02<00:10, 46.59it/s][A
 23%|██▎       | 138/602 [00:02<00:09, 46.49it/s][A
 24%|██▍       | 143/602 [00:03<00:09, 46.51it/s][A
 25%|██▍       | 148/602 [00:03<00:09, 46.46it/s][A
 25%|██▌       | 153/602 [00:03<00:09, 46.51it/s][A
 26%|██▌       | 158/602 [00:03<00:09, 46.57it/s][A
 27%|██▋       | 163/602 [00:03<00:09, 46.58it/s][A
 28%|██▊       | 168/602 [00:03<00:09, 46.51it/s][A
 29%|██▊       | 173/602 [00:03<00:09, 46.59it/s][A
 30%|██▉       | 178/602 [00:03<00:09, 46.54it/s][A
 30%|███       | 183/602 [00:03<00:08, 46.59it/s][A
 31%|███       | 188/602 [00:04<00:08, 46.56it/s][A
 32%|███▏      | 193/602 [00:04<00:08, 46.58it/s][A
 33%|███▎      | 198/602 [00:04<00:08, 46.44it/s][A
 34%|███▎      | 203/602 [00:04<00:08, 46.53it/s][A
 35%|███▍      | 208/602 [00:04<00:08, 46.64it/s][A
 35%|███▌      | 213/602 [00:04<00:08, 46.64it/s][A
 36%|███▌      | 218/602 [00:04<00:08, 46.68it/s][A
 37%|███▋      | 223/602 [00:04<00:08, 46.60it/s][A
 38%|███▊      | 228/602 [00:04<00:08, 46.55it/s][A
 39%|███▊      | 233/602 [00:04<00:07, 46.57it/s][A
 40%|███▉      | 238/602 [00:05<00:07, 46.58it/s][A
 40%|████      | 243/602 [00:05<00:07, 46.57it/s][A
 41%|████      | 248/602 [00:05<00:07, 46.53it/s][A
 42%|████▏     | 253/602 [00:05<00:07, 46.55it/s][A
 43%|████▎     | 258/602 [00:05<00:07, 46.49it/s][A
 44%|████▎     | 263/602 [00:05<00:07, 46.58it/s][A
 45%|████▍     | 268/602 [00:05<00:07, 46.64it/s][A
 45%|████▌     | 273/602 [00:05<00:07, 46.58it/s][A
 46%|████▌     | 278/602 [00:05<00:06, 46.54it/s][A
 47%|████▋     | 283/602 [00:06<00:06, 46.54it/s][A
 48%|████▊     | 288/602 [00:06<00:06, 46.52it/s][A
 49%|████▊     | 293/602 [00:06<00:06, 46.57it/s][A
 50%|████▉     | 298/602 [00:06<00:06, 46.49it/s][A
 50%|█████     | 303/602 [00:06<00:06, 46.49it/s][A
 51%|█████     | 308/602 [00:06<00:06, 46.51it/s][A
 52%|█████▏    | 313/602 [00:06<00:06, 46.52it/s][A
 53%|█████▎    | 318/602 [00:06<00:06, 46.61it/s][A
 54%|█████▎    | 323/602 [00:06<00:05, 46.61it/s][A
 54%|█████▍    | 328/602 [00:07<00:05, 46.58it/s][A
 55%|█████▌    | 333/602 [00:07<00:05, 46.53it/s][A
 56%|█████▌    | 338/602 [00:07<00:05, 46.51it/s][A
 57%|█████▋    | 343/602 [00:07<00:05, 46.59it/s][A
 58%|█████▊    | 348/602 [00:07<00:05, 46.48it/s][A
 59%|█████▊    | 353/602 [00:07<00:05, 46.48it/s][A
 59%|█████▉    | 358/602 [00:07<00:05, 46.58it/s][A
 60%|██████    | 363/602 [00:07<00:05, 46.54it/s][A
 61%|██████    | 368/602 [00:07<00:05, 46.59it/s][A
 62%|██████▏   | 373/602 [00:07<00:04, 46.55it/s][A
 63%|██████▎   | 378/602 [00:08<00:04, 46.56it/s][A
 64%|██████▎   | 383/602 [00:08<00:04, 46.54it/s][A
 64%|██████▍   | 388/602 [00:08<00:04, 46.52it/s][A
 65%|██████▌   | 393/602 [00:08<00:04, 46.52it/s][A
 66%|██████▌   | 398/602 [00:08<00:04, 46.33it/s][A
 67%|██████▋   | 403/602 [00:08<00:04, 46.62it/s][A
 68%|██████▊   | 408/602 [00:08<00:04, 46.58it/s][A
 69%|██████▊   | 413/602 [00:08<00:04, 46.65it/s][A
 69%|██████▉   | 418/602 [00:08<00:03, 46.70it/s][A
 70%|███████   | 423/602 [00:09<00:03, 46.59it/s][A
 71%|███████   | 428/602 [00:09<00:03, 46.51it/s][A
 72%|███████▏  | 433/602 [00:09<00:03, 46.47it/s][A
 73%|███████▎  | 438/602 [00:09<00:03, 46.52it/s][A
 74%|███████▎  | 443/602 [00:09<00:03, 46.50it/s][A
 74%|███████▍  | 448/602 [00:09<00:03, 46.51it/s][A
 75%|███████▌  | 453/602 [00:09<00:03, 46.60it/s][A
 76%|███████▌  | 458/602 [00:09<00:03, 46.51it/s][A
 77%|███████▋  | 463/602 [00:09<00:02, 46.53it/s][A
 78%|███████▊  | 468/602 [00:10<00:02, 46.57it/s][A
 79%|███████▊  | 473/602 [00:10<00:02, 46.59it/s][A
 79%|███████▉  | 478/602 [00:10<00:02, 46.58it/s][A
 80%|████████  | 483/602 [00:10<00:02, 46.44it/s][A
 81%|████████  | 488/602 [00:10<00:02, 46.44it/s][A
 82%|████████▏ | 493/602 [00:10<00:02, 46.44it/s][A
 83%|████████▎ | 498/602 [00:10<00:02, 46.47it/s][A
 84%|████████▎ | 503/602 [00:10<00:02, 46.56it/s][A
 84%|████████▍ | 508/602 [00:10<00:02, 46.52it/s][A
 85%|████████▌ | 513/602 [00:10<00:01, 46.60it/s][A
 86%|████████▌ | 518/602 [00:11<00:01, 46.55it/s][A
 87%|████████▋ | 523/602 [00:11<00:01, 46.47it/s][A
 88%|████████▊ | 528/602 [00:11<00:01, 46.51it/s][A
 89%|████████▊ | 533/602 [00:11<00:01, 46.51it/s][A
 89%|████████▉ | 538/602 [00:11<00:01, 46.43it/s][A
 90%|█████████ | 543/602 [00:11<00:01, 46.46it/s][A
 91%|█████████ | 548/602 [00:11<00:01, 46.56it/s][A
 92%|█████████▏| 553/602 [00:11<00:01, 46.49it/s][A
 93%|█████████▎| 558/602 [00:11<00:00, 46.59it/s][A
 94%|█████████▎| 563/602 [00:12<00:00, 46.49it/s][A
 94%|█████████▍| 568/602 [00:12<00:00, 46.52it/s][A
 95%|█████████▌| 573/602 [00:12<00:00, 46.54it/s][A
 96%|█████████▌| 578/602 [00:12<00:00, 46.48it/s][A
 97%|█████████▋| 583/602 [00:12<00:00, 46.55it/s][A
 98%|█████████▊| 588/602 [00:12<00:00, 46.45it/s][A
 99%|█████████▊| 593/602 [00:12<00:00, 46.42it/s][A
 99%|█████████▉| 598/602 [00:12<00:00, 46.56it/s][A                                                 
                                                 [A 60%|██████    | 471/785 [03:09<01:24,  3.70it/s]
100%|██████████| 602/602 [00:12<00:00, 46.56it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:39:04,478 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-471
[INFO|configuration_utils.py:351] 2023-08-29 08:39:04,500 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-471/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:39:06,890 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-471/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:39:06,920 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-471/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:39:06,933 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-471/special_tokens_map.json
 60%|██████    | 472/785 [03:17<32:40,  6.26s/it] 60%|██████    | 473/785 [03:17<23:15,  4.47s/it] 60%|██████    | 474/785 [03:17<16:40,  3.22s/it] 61%|██████    | 475/785 [03:18<12:04,  2.34s/it] 61%|██████    | 476/785 [03:18<08:52,  1.72s/it] 61%|██████    | 477/785 [03:18<06:38,  1.29s/it] 61%|██████    | 478/785 [03:18<05:04,  1.01it/s] 61%|██████    | 479/785 [03:19<03:58,  1.28it/s] 61%|██████    | 480/785 [03:19<03:12,  1.58it/s] 61%|██████▏   | 481/785 [03:19<02:41,  1.89it/s] 61%|██████▏   | 482/785 [03:20<02:18,  2.19it/s] 62%|██████▏   | 483/785 [03:20<02:02,  2.46it/s] 62%|██████▏   | 484/785 [03:20<01:51,  2.69it/s] 62%|██████▏   | 485/785 [03:20<01:44,  2.88it/s] 62%|██████▏   | 486/785 [03:21<01:38,  3.04it/s] 62%|██████▏   | 487/785 [03:21<01:34,  3.15it/s] 62%|██████▏   | 488/785 [03:21<01:31,  3.24it/s] 62%|██████▏   | 489/785 [03:22<01:29,  3.30it/s] 62%|██████▏   | 490/785 [03:22<01:28,  3.35it/s] 63%|██████▎   | 491/785 [03:22<01:26,  3.38it/s] 63%|██████▎   | 492/785 [03:22<01:25,  3.41it/s] 63%|██████▎   | 493/785 [03:23<01:25,  3.43it/s] 63%|██████▎   | 494/785 [03:23<01:24,  3.44it/s] 63%|██████▎   | 495/785 [03:23<01:24,  3.42it/s] 63%|██████▎   | 496/785 [03:24<01:24,  3.44it/s] 63%|██████▎   | 497/785 [03:24<01:23,  3.45it/s] 63%|██████▎   | 498/785 [03:24<01:23,  3.45it/s] 64%|██████▎   | 499/785 [03:24<01:22,  3.46it/s] 64%|██████▎   | 500/785 [03:25<01:22,  3.46it/s]                                                  64%|██████▎   | 500/785 [03:25<01:22,  3.46it/s] 64%|██████▍   | 501/785 [03:25<01:22,  3.46it/s] 64%|██████▍   | 502/785 [03:25<01:21,  3.46it/s] 64%|██████▍   | 503/785 [03:26<01:21,  3.46it/s] 64%|██████▍   | 504/785 [03:26<01:21,  3.46it/s] 64%|██████▍   | 505/785 [03:26<01:20,  3.46it/s] 64%|██████▍   | 506/785 [03:26<01:20,  3.46it/s] 65%|██████▍   | 507/785 [03:27<01:20,  3.46it/s] 65%|██████▍   | 508/785 [03:27<01:19,  3.47it/s] 65%|██████▍   | 509/785 [03:27<01:19,  3.46it/s] 65%|██████▍   | 510/785 [03:28<01:19,  3.45it/s] 65%|██████▌   | 511/785 [03:28<01:19,  3.45it/s] 65%|██████▌   | 512/785 [03:28<01:18,  3.46it/s] 65%|██████▌   | 513/785 [03:29<01:18,  3.46it/s] 65%|██████▌   | 514/785 [03:29<01:18,  3.46it/s] 66%|██████▌   | 515/785 [03:29<01:17,  3.46it/s] 66%|██████▌   | 516/785 [03:29<01:17,  3.46it/s] 66%|██████▌   | 517/785 [03:30<01:17,  3.46it/s] 66%|██████▌   | 518/785 [03:30<01:17,  3.46it/s] 66%|██████▌   | 519/785 [03:30<01:16,  3.46it/s] 66%|██████▌   | 520/785 [03:31<01:16,  3.46it/s] 66%|██████▋   | 521/785 [03:31<01:16,  3.45it/s] 66%|██████▋   | 522/785 [03:31<01:16,  3.46it/s] 67%|██████▋   | 523/785 [03:31<01:15,  3.46it/s] 67%|██████▋   | 524/785 [03:32<01:15,  3.46it/s] 67%|██████▋   | 525/785 [03:32<01:15,  3.46it/s] 67%|██████▋   | 526/785 [03:32<01:14,  3.46it/s] 67%|██████▋   | 527/785 [03:33<01:14,  3.46it/s] 67%|██████▋   | 528/785 [03:33<01:14,  3.46it/s] 67%|██████▋   | 529/785 [03:33<01:13,  3.46it/s] 68%|██████▊   | 530/785 [03:33<01:13,  3.46it/s] 68%|██████▊   | 531/785 [03:34<01:13,  3.46it/s] 68%|██████▊   | 532/785 [03:34<01:13,  3.45it/s] 68%|██████▊   | 533/785 [03:34<01:12,  3.46it/s] 68%|██████▊   | 534/785 [03:35<01:12,  3.46it/s] 68%|██████▊   | 535/785 [03:35<01:12,  3.46it/s] 68%|██████▊   | 536/785 [03:35<01:11,  3.46it/s] 68%|██████▊   | 537/785 [03:35<01:11,  3.46it/s] 69%|██████▊   | 538/785 [03:36<01:11,  3.46it/s] 69%|██████▊   | 539/785 [03:36<01:11,  3.46it/s] 69%|██████▉   | 540/785 [03:36<01:10,  3.46it/s] 69%|██████▉   | 541/785 [03:37<01:10,  3.46it/s] 69%|██████▉   | 542/785 [03:37<01:10,  3.46it/s] 69%|██████▉   | 543/785 [03:37<01:10,  3.45it/s] 69%|██████▉   | 544/785 [03:37<01:09,  3.45it/s] 69%|██████▉   | 545/785 [03:38<01:09,  3.46it/s] 70%|██████▉   | 546/785 [03:38<01:09,  3.45it/s] 70%|██████▉   | 547/785 [03:38<01:08,  3.46it/s] 70%|██████▉   | 548/785 [03:39<01:08,  3.46it/s] 70%|██████▉   | 549/785 [03:39<01:08,  3.46it/s] 70%|███████   | 550/785 [03:39<01:07,  3.46it/s] 70%|███████   | 551/785 [03:39<01:07,  3.46it/s] 70%|███████   | 552/785 [03:40<01:07,  3.46it/s] 70%|███████   | 553/785 [03:40<01:07,  3.46it/s] 71%|███████   | 554/785 [03:40<01:06,  3.45it/s] 71%|███████   | 555/785 [03:41<01:06,  3.46it/s] 71%|███████   | 556/785 [03:41<01:06,  3.46it/s] 71%|███████   | 557/785 [03:41<01:05,  3.46it/s] 71%|███████   | 558/785 [03:42<01:05,  3.46it/s] 71%|███████   | 559/785 [03:42<01:05,  3.46it/s] 71%|███████▏  | 560/785 [03:42<01:05,  3.46it/s] 71%|███████▏  | 561/785 [03:42<01:04,  3.46it/s] 72%|███████▏  | 562/785 [03:43<01:04,  3.46it/s] 72%|███████▏  | 563/785 [03:43<01:04,  3.46it/s] 72%|███████▏  | 564/785 [03:43<01:03,  3.46it/s] 72%|███████▏  | 565/785 [03:44<01:03,  3.45it/s] 72%|███████▏  | 566/785 [03:44<01:03,  3.46it/s] 72%|███████▏  | 567/785 [03:44<01:03,  3.46it/s] 72%|███████▏  | 568/785 [03:44<01:02,  3.46it/s] 72%|███████▏  | 569/785 [03:45<01:02,  3.46it/s] 73%|███████▎  | 570/785 [03:45<01:02,  3.46it/s] 73%|███████▎  | 571/785 [03:45<01:01,  3.46it/s] 73%|███████▎  | 572/785 [03:46<01:01,  3.46it/s] 73%|███████▎  | 573/785 [03:46<01:01,  3.46it/s] 73%|███████▎  | 574/785 [03:46<01:00,  3.46it/s] 73%|███████▎  | 575/785 [03:46<01:00,  3.46it/s] 73%|███████▎  | 576/785 [03:47<01:00,  3.45it/s] 74%|███████▎  | 577/785 [03:47<01:00,  3.45it/s] 74%|███████▎  | 578/785 [03:47<00:59,  3.45it/s] 74%|███████▍  | 579/785 [03:48<00:59,  3.46it/s] 74%|███████▍  | 580/785 [03:48<00:59,  3.45it/s] 74%|███████▍  | 581/785 [03:48<00:59,  3.45it/s] 74%|███████▍  | 582/785 [03:48<00:58,  3.46it/s] 74%|███████▍  | 583/785 [03:49<00:58,  3.46it/s] 74%|███████▍  | 584/785 [03:49<00:58,  3.46it/s] 75%|███████▍  | 585/785 [03:49<00:57,  3.46it/s] 75%|███████▍  | 586/785 [03:50<00:57,  3.46it/s] 75%|███████▍  | 587/785 [03:50<00:57,  3.45it/s] 75%|███████▍  | 588/785 [03:50<00:57,  3.45it/s] 75%|███████▌  | 589/785 [03:50<00:56,  3.46it/s] 75%|███████▌  | 590/785 [03:51<00:56,  3.46it/s] 75%|███████▌  | 591/785 [03:51<00:56,  3.46it/s] 75%|███████▌  | 592/785 [03:51<00:55,  3.46it/s] 76%|███████▌  | 593/785 [03:52<00:55,  3.46it/s] 76%|███████▌  | 594/785 [03:52<00:55,  3.46it/s] 76%|███████▌  | 595/785 [03:52<00:54,  3.46it/s] 76%|███████▌  | 596/785 [03:53<00:54,  3.46it/s] 76%|███████▌  | 597/785 [03:53<00:54,  3.46it/s] 76%|███████▌  | 598/785 [03:53<00:54,  3.45it/s] 76%|███████▋  | 599/785 [03:53<00:53,  3.45it/s] 76%|███████▋  | 600/785 [03:54<00:53,  3.45it/s] 77%|███████▋  | 601/785 [03:54<00:53,  3.46it/s] 77%|███████▋  | 602/785 [03:54<00:52,  3.46it/s] 77%|███████▋  | 603/785 [03:55<00:52,  3.46it/s] 77%|███████▋  | 604/785 [03:55<00:52,  3.45it/s] 77%|███████▋  | 605/785 [03:55<00:52,  3.46it/s] 77%|███████▋  | 606/785 [03:55<00:51,  3.46it/s] 77%|███████▋  | 607/785 [03:56<00:51,  3.46it/s] 77%|███████▋  | 608/785 [03:56<00:51,  3.46it/s] 78%|███████▊  | 609/785 [03:56<00:50,  3.46it/s] 78%|███████▊  | 610/785 [03:57<00:50,  3.46it/s] 78%|███████▊  | 611/785 [03:57<00:50,  3.46it/s] 78%|███████▊  | 612/785 [03:57<00:50,  3.46it/s] 78%|███████▊  | 613/785 [03:57<00:49,  3.46it/s] 78%|███████▊  | 614/785 [03:58<00:49,  3.46it/s] 78%|███████▊  | 615/785 [03:58<00:49,  3.46it/s] 78%|███████▊  | 616/785 [03:58<00:49,  3.43it/s] 79%|███████▊  | 617/785 [03:59<00:48,  3.43it/s] 79%|███████▊  | 618/785 [03:59<00:48,  3.44it/s] 79%|███████▉  | 619/785 [03:59<00:48,  3.45it/s] 79%|███████▉  | 620/785 [03:59<00:47,  3.45it/s] 79%|███████▉  | 621/785 [04:00<00:47,  3.45it/s] 79%|███████▉  | 622/785 [04:00<00:47,  3.46it/s] 79%|███████▉  | 623/785 [04:00<00:46,  3.46it/s] 79%|███████▉  | 624/785 [04:01<00:46,  3.46it/s] 80%|███████▉  | 625/785 [04:01<00:46,  3.46it/s] 80%|███████▉  | 626/785 [04:01<00:45,  3.46it/s] 80%|███████▉  | 627/785 [04:01<00:45,  3.45it/s] 80%|████████  | 628/785 [04:02<00:42,  3.68it/s][INFO|trainer.py:2140] 2023-08-29 08:39:56,815 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:39:56,815 >>   Num examples = 4812
[INFO|trainer.py:2145] 2023-08-29 08:39:56,816 >>   Batch size = 8
{'eval_loss': 0.9674451947212219, 'eval_runtime': 12.9271, 'eval_samples_per_second': 372.242, 'eval_steps_per_second': 46.569, 'epoch': 3.0}
{'loss': 0.5471, 'learning_rate': 1.3614649681528663e-05, 'epoch': 3.18}

  0%|          | 0/602 [00:00<?, ?it/s][A
  1%|          | 6/602 [00:00<00:10, 57.19it/s][A
  2%|▏         | 12/602 [00:00<00:11, 50.36it/s][A
  3%|▎         | 18/602 [00:00<00:12, 48.64it/s][A
  4%|▍         | 23/602 [00:00<00:12, 47.93it/s][A
  5%|▍         | 28/602 [00:00<00:12, 47.48it/s][A
  5%|▌         | 33/602 [00:00<00:12, 47.18it/s][A
  6%|▋         | 38/602 [00:00<00:11, 47.03it/s][A
  7%|▋         | 43/602 [00:00<00:11, 46.69it/s][A
  8%|▊         | 48/602 [00:01<00:11, 46.60it/s][A
  9%|▉         | 53/602 [00:01<00:11, 46.71it/s][A
 10%|▉         | 58/602 [00:01<00:11, 46.67it/s][A
 10%|█         | 63/602 [00:01<00:11, 46.61it/s][A
 11%|█▏        | 68/602 [00:01<00:11, 46.54it/s][A
 12%|█▏        | 73/602 [00:01<00:11, 46.61it/s][A
 13%|█▎        | 78/602 [00:01<00:11, 46.66it/s][A
 14%|█▍        | 83/602 [00:01<00:11, 46.65it/s][A
 15%|█▍        | 88/602 [00:01<00:11, 46.56it/s][A
 15%|█▌        | 93/602 [00:01<00:10, 46.57it/s][A
 16%|█▋        | 98/602 [00:02<00:10, 46.45it/s][A
 17%|█▋        | 103/602 [00:02<00:10, 46.56it/s][A
 18%|█▊        | 108/602 [00:02<00:10, 46.62it/s][A
 19%|█▉        | 113/602 [00:02<00:10, 46.56it/s][A
 20%|█▉        | 118/602 [00:02<00:10, 46.64it/s][A
 20%|██        | 123/602 [00:02<00:10, 46.61it/s][A
 21%|██▏       | 128/602 [00:02<00:10, 46.61it/s][A
 22%|██▏       | 133/602 [00:02<00:10, 46.57it/s][A
 23%|██▎       | 138/602 [00:02<00:09, 46.51it/s][A
 24%|██▍       | 143/602 [00:03<00:09, 46.51it/s][A
 25%|██▍       | 148/602 [00:03<00:09, 46.55it/s][A
 25%|██▌       | 153/602 [00:03<00:09, 46.43it/s][A
 26%|██▌       | 158/602 [00:03<00:09, 46.48it/s][A
 27%|██▋       | 163/602 [00:03<00:09, 46.48it/s][A
 28%|██▊       | 168/602 [00:03<00:09, 46.54it/s][A
 29%|██▊       | 173/602 [00:03<00:09, 46.61it/s][A
 30%|██▉       | 178/602 [00:03<00:09, 46.64it/s][A
 30%|███       | 183/602 [00:03<00:09, 46.54it/s][A
 31%|███       | 188/602 [00:04<00:08, 46.54it/s][A
 32%|███▏      | 193/602 [00:04<00:08, 46.43it/s][A
 33%|███▎      | 198/602 [00:04<00:08, 46.42it/s][A
 34%|███▎      | 203/602 [00:04<00:08, 46.47it/s][A
 35%|███▍      | 208/602 [00:04<00:08, 46.42it/s][A
 35%|███▌      | 213/602 [00:04<00:08, 46.52it/s][A
 36%|███▌      | 218/602 [00:04<00:08, 46.59it/s][A
 37%|███▋      | 223/602 [00:04<00:08, 46.60it/s][A
 38%|███▊      | 228/602 [00:04<00:08, 46.61it/s][A
 39%|███▊      | 233/602 [00:04<00:07, 46.57it/s][A
 40%|███▉      | 238/602 [00:05<00:07, 46.57it/s][A
 40%|████      | 243/602 [00:05<00:07, 46.56it/s][A
 41%|████      | 248/602 [00:05<00:07, 46.49it/s][A
 42%|████▏     | 253/602 [00:05<00:07, 46.43it/s][A
 43%|████▎     | 258/602 [00:05<00:07, 46.47it/s][A
 44%|████▎     | 263/602 [00:05<00:07, 46.49it/s][A
 45%|████▍     | 268/602 [00:05<00:07, 46.55it/s][A
 45%|████▌     | 273/602 [00:05<00:07, 46.57it/s][A
 46%|████▌     | 278/602 [00:05<00:06, 46.60it/s][A
 47%|████▋     | 283/602 [00:06<00:06, 46.53it/s][A
 48%|████▊     | 288/602 [00:06<00:06, 46.53it/s][A
 49%|████▊     | 293/602 [00:06<00:06, 46.58it/s][A
 50%|████▉     | 298/602 [00:06<00:06, 46.43it/s][A
 50%|█████     | 303/602 [00:06<00:06, 46.52it/s][A
 51%|█████     | 308/602 [00:06<00:06, 46.61it/s][A
 52%|█████▏    | 313/602 [00:06<00:06, 46.54it/s][A
 53%|█████▎    | 318/602 [00:06<00:06, 46.63it/s][A
 54%|█████▎    | 323/602 [00:06<00:05, 46.53it/s][A
 54%|█████▍    | 328/602 [00:07<00:05, 46.51it/s][A
 55%|█████▌    | 333/602 [00:07<00:05, 46.54it/s][A
 56%|█████▌    | 338/602 [00:07<00:05, 46.56it/s][A
 57%|█████▋    | 343/602 [00:07<00:05, 46.50it/s][A
 58%|█████▊    | 348/602 [00:07<00:05, 46.52it/s][A
 59%|█████▊    | 353/602 [00:07<00:05, 46.52it/s][A
 59%|█████▉    | 358/602 [00:07<00:05, 46.62it/s][A
 60%|██████    | 363/602 [00:07<00:05, 46.56it/s][A
 61%|██████    | 368/602 [00:07<00:05, 46.50it/s][A
 62%|██████▏   | 373/602 [00:07<00:04, 46.58it/s][A
 63%|██████▎   | 378/602 [00:08<00:04, 46.54it/s][A
 64%|██████▎   | 383/602 [00:08<00:04, 46.55it/s][A
 64%|██████▍   | 388/602 [00:08<00:04, 46.47it/s][A
 65%|██████▌   | 393/602 [00:08<00:04, 46.49it/s][A
 66%|██████▌   | 398/602 [00:08<00:04, 46.53it/s][A
 67%|██████▋   | 403/602 [00:08<00:04, 46.56it/s][A
 68%|██████▊   | 408/602 [00:08<00:04, 46.61it/s][A
 69%|██████▊   | 413/602 [00:08<00:04, 46.52it/s][A
 69%|██████▉   | 418/602 [00:08<00:03, 46.46it/s][A
 70%|███████   | 423/602 [00:09<00:03, 46.53it/s][A
 71%|███████   | 428/602 [00:09<00:03, 46.47it/s][A
 72%|███████▏  | 433/602 [00:09<00:03, 46.44it/s][A
 73%|███████▎  | 438/602 [00:09<00:03, 46.34it/s][A
 74%|███████▎  | 443/602 [00:09<00:03, 46.45it/s][A
 74%|███████▍  | 448/602 [00:09<00:03, 46.42it/s][A
 75%|███████▌  | 453/602 [00:09<00:03, 46.46it/s][A
 76%|███████▌  | 458/602 [00:09<00:03, 46.43it/s][A
 77%|███████▋  | 463/602 [00:09<00:02, 46.51it/s][A
 78%|███████▊  | 468/602 [00:10<00:02, 46.46it/s][A
 79%|███████▊  | 473/602 [00:10<00:02, 46.48it/s][A
 79%|███████▉  | 478/602 [00:10<00:02, 46.49it/s][A
 80%|████████  | 483/602 [00:10<00:02, 46.40it/s][A
 81%|████████  | 488/602 [00:10<00:02, 46.53it/s][A
 82%|████████▏ | 493/602 [00:10<00:02, 46.53it/s][A
 83%|████████▎ | 498/602 [00:10<00:02, 46.52it/s][A
 84%|████████▎ | 503/602 [00:10<00:02, 46.57it/s][A
 84%|████████▍ | 508/602 [00:10<00:02, 46.50it/s][A
 85%|████████▌ | 513/602 [00:11<00:01, 46.50it/s][A
 86%|████████▌ | 518/602 [00:11<00:01, 46.48it/s][A
 87%|████████▋ | 523/602 [00:11<00:01, 46.48it/s][A
 88%|████████▊ | 528/602 [00:11<00:01, 46.47it/s][A
 89%|████████▊ | 533/602 [00:11<00:01, 46.52it/s][A
 89%|████████▉ | 538/602 [00:11<00:01, 46.52it/s][A
 90%|█████████ | 543/602 [00:11<00:01, 46.57it/s][A
 91%|█████████ | 548/602 [00:11<00:01, 46.59it/s][A
 92%|█████████▏| 553/602 [00:11<00:01, 46.54it/s][A
 93%|█████████▎| 558/602 [00:11<00:00, 46.47it/s][A
 94%|█████████▎| 563/602 [00:12<00:00, 46.48it/s][A
 94%|█████████▍| 568/602 [00:12<00:00, 46.50it/s][A
 95%|█████████▌| 573/602 [00:12<00:00, 46.52it/s][A
 96%|█████████▌| 578/602 [00:12<00:00, 46.48it/s][A
 97%|█████████▋| 583/602 [00:12<00:00, 46.44it/s][A
 98%|█████████▊| 588/602 [00:12<00:00, 46.46it/s][A
 99%|█████████▊| 593/602 [00:12<00:00, 46.52it/s][A
 99%|█████████▉| 598/602 [00:12<00:00, 46.57it/s][A                                                 
                                                 [A 80%|████████  | 628/785 [04:15<00:42,  3.68it/s]
100%|██████████| 602/602 [00:12<00:00, 46.57it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:40:09,784 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-628
[INFO|configuration_utils.py:351] 2023-08-29 08:40:09,802 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-628/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:40:12,374 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-628/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:40:12,393 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-628/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:40:12,408 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-628/special_tokens_map.json
 80%|████████  | 629/785 [04:22<16:36,  6.39s/it] 80%|████████  | 630/785 [04:23<11:46,  4.56s/it] 80%|████████  | 631/785 [04:23<08:24,  3.28s/it] 81%|████████  | 632/785 [04:23<06:04,  2.38s/it] 81%|████████  | 633/785 [04:24<04:26,  1.75s/it] 81%|████████  | 634/785 [04:24<03:18,  1.31s/it] 81%|████████  | 635/785 [04:24<02:30,  1.01s/it] 81%|████████  | 636/785 [04:24<01:57,  1.27it/s] 81%|████████  | 637/785 [04:25<01:34,  1.56it/s] 81%|████████▏ | 638/785 [04:25<01:18,  1.87it/s] 81%|████████▏ | 639/785 [04:25<01:07,  2.17it/s] 82%|████████▏ | 640/785 [04:26<00:59,  2.44it/s] 82%|████████▏ | 641/785 [04:26<00:53,  2.67it/s] 82%|████████▏ | 642/785 [04:26<00:49,  2.87it/s] 82%|████████▏ | 643/785 [04:26<00:46,  3.03it/s] 82%|████████▏ | 644/785 [04:27<00:44,  3.15it/s] 82%|████████▏ | 645/785 [04:27<00:43,  3.24it/s] 82%|████████▏ | 646/785 [04:27<00:42,  3.30it/s] 82%|████████▏ | 647/785 [04:28<00:41,  3.35it/s] 83%|████████▎ | 648/785 [04:28<00:40,  3.38it/s] 83%|████████▎ | 649/785 [04:28<00:39,  3.41it/s] 83%|████████▎ | 650/785 [04:28<00:39,  3.42it/s] 83%|████████▎ | 651/785 [04:29<00:39,  3.43it/s] 83%|████████▎ | 652/785 [04:29<00:38,  3.43it/s] 83%|████████▎ | 653/785 [04:29<00:38,  3.43it/s] 83%|████████▎ | 654/785 [04:30<00:38,  3.44it/s] 83%|████████▎ | 655/785 [04:30<00:37,  3.45it/s] 84%|████████▎ | 656/785 [04:30<00:37,  3.46it/s] 84%|████████▎ | 657/785 [04:30<00:37,  3.46it/s] 84%|████████▍ | 658/785 [04:31<00:36,  3.46it/s] 84%|████████▍ | 659/785 [04:31<00:36,  3.46it/s] 84%|████████▍ | 660/785 [04:31<00:36,  3.46it/s] 84%|████████▍ | 661/785 [04:32<00:35,  3.47it/s] 84%|████████▍ | 662/785 [04:32<00:35,  3.47it/s] 84%|████████▍ | 663/785 [04:32<00:35,  3.46it/s] 85%|████████▍ | 664/785 [04:32<00:34,  3.46it/s] 85%|████████▍ | 665/785 [04:33<00:34,  3.46it/s] 85%|████████▍ | 666/785 [04:33<00:34,  3.46it/s] 85%|████████▍ | 667/785 [04:33<00:34,  3.46it/s] 85%|████████▌ | 668/785 [04:34<00:33,  3.46it/s] 85%|████████▌ | 669/785 [04:34<00:33,  3.46it/s] 85%|████████▌ | 670/785 [04:34<00:33,  3.46it/s] 85%|████████▌ | 671/785 [04:35<00:32,  3.46it/s] 86%|████████▌ | 672/785 [04:35<00:32,  3.46it/s] 86%|████████▌ | 673/785 [04:35<00:32,  3.46it/s] 86%|████████▌ | 674/785 [04:35<00:32,  3.46it/s] 86%|████████▌ | 675/785 [04:36<00:31,  3.46it/s] 86%|████████▌ | 676/785 [04:36<00:31,  3.46it/s] 86%|████████▌ | 677/785 [04:36<00:31,  3.46it/s] 86%|████████▋ | 678/785 [04:37<00:30,  3.46it/s] 86%|████████▋ | 679/785 [04:37<00:30,  3.46it/s] 87%|████████▋ | 680/785 [04:37<00:30,  3.46it/s] 87%|████████▋ | 681/785 [04:37<00:30,  3.46it/s] 87%|████████▋ | 682/785 [04:38<00:29,  3.46it/s] 87%|████████▋ | 683/785 [04:38<00:29,  3.46it/s] 87%|████████▋ | 684/785 [04:38<00:29,  3.46it/s] 87%|████████▋ | 685/785 [04:39<00:28,  3.45it/s] 87%|████████▋ | 686/785 [04:39<00:28,  3.45it/s] 88%|████████▊ | 687/785 [04:39<00:28,  3.46it/s] 88%|████████▊ | 688/785 [04:39<00:28,  3.46it/s] 88%|████████▊ | 689/785 [04:40<00:27,  3.46it/s] 88%|████████▊ | 690/785 [04:40<00:27,  3.46it/s] 88%|████████▊ | 691/785 [04:40<00:27,  3.46it/s] 88%|████████▊ | 692/785 [04:41<00:26,  3.46it/s] 88%|████████▊ | 693/785 [04:41<00:26,  3.46it/s] 88%|████████▊ | 694/785 [04:41<00:26,  3.46it/s] 89%|████████▊ | 695/785 [04:41<00:26,  3.46it/s] 89%|████████▊ | 696/785 [04:42<00:25,  3.45it/s] 89%|████████▉ | 697/785 [04:42<00:25,  3.45it/s] 89%|████████▉ | 698/785 [04:42<00:25,  3.45it/s] 89%|████████▉ | 699/785 [04:43<00:24,  3.45it/s] 89%|████████▉ | 700/785 [04:43<00:24,  3.46it/s] 89%|████████▉ | 701/785 [04:43<00:24,  3.46it/s] 89%|████████▉ | 702/785 [04:43<00:23,  3.46it/s] 90%|████████▉ | 703/785 [04:44<00:23,  3.46it/s] 90%|████████▉ | 704/785 [04:44<00:23,  3.46it/s] 90%|████████▉ | 705/785 [04:44<00:23,  3.46it/s] 90%|████████▉ | 706/785 [04:45<00:22,  3.46it/s] 90%|█████████ | 707/785 [04:45<00:22,  3.45it/s] 90%|█████████ | 708/785 [04:45<00:22,  3.46it/s] 90%|█████████ | 709/785 [04:46<00:21,  3.46it/s] 90%|█████████ | 710/785 [04:46<00:21,  3.45it/s] 91%|█████████ | 711/785 [04:46<00:21,  3.45it/s] 91%|█████████ | 712/785 [04:46<00:21,  3.45it/s] 91%|█████████ | 713/785 [04:47<00:20,  3.45it/s] 91%|█████████ | 714/785 [04:47<00:20,  3.45it/s] 91%|█████████ | 715/785 [04:47<00:20,  3.46it/s] 91%|█████████ | 716/785 [04:48<00:19,  3.46it/s] 91%|█████████▏| 717/785 [04:48<00:19,  3.46it/s] 91%|█████████▏| 718/785 [04:48<00:19,  3.45it/s] 92%|█████████▏| 719/785 [04:48<00:19,  3.45it/s] 92%|█████████▏| 720/785 [04:49<00:18,  3.45it/s] 92%|█████████▏| 721/785 [04:49<00:18,  3.45it/s] 92%|█████████▏| 722/785 [04:49<00:18,  3.45it/s] 92%|█████████▏| 723/785 [04:50<00:18,  3.44it/s] 92%|█████████▏| 724/785 [04:50<00:18,  3.38it/s] 92%|█████████▏| 725/785 [04:50<00:17,  3.40it/s] 92%|█████████▏| 726/785 [04:50<00:17,  3.42it/s] 93%|█████████▎| 727/785 [04:51<00:16,  3.43it/s] 93%|█████████▎| 728/785 [04:51<00:16,  3.44it/s] 93%|█████████▎| 729/785 [04:51<00:16,  3.44it/s] 93%|█████████▎| 730/785 [04:52<00:15,  3.45it/s] 93%|█████████▎| 731/785 [04:52<00:15,  3.45it/s] 93%|█████████▎| 732/785 [04:52<00:15,  3.45it/s] 93%|█████████▎| 733/785 [04:52<00:15,  3.46it/s] 94%|█████████▎| 734/785 [04:53<00:14,  3.45it/s] 94%|█████████▎| 735/785 [04:53<00:14,  3.45it/s] 94%|█████████▍| 736/785 [04:53<00:14,  3.45it/s] 94%|█████████▍| 737/785 [04:54<00:13,  3.45it/s] 94%|█████████▍| 738/785 [04:54<00:13,  3.45it/s] 94%|█████████▍| 739/785 [04:54<00:13,  3.45it/s] 94%|█████████▍| 740/785 [04:55<00:13,  3.46it/s] 94%|█████████▍| 741/785 [04:55<00:12,  3.45it/s] 95%|█████████▍| 742/785 [04:55<00:12,  3.46it/s] 95%|█████████▍| 743/785 [04:55<00:12,  3.46it/s] 95%|█████████▍| 744/785 [04:56<00:11,  3.46it/s] 95%|█████████▍| 745/785 [04:56<00:11,  3.45it/s] 95%|█████████▌| 746/785 [04:56<00:11,  3.45it/s] 95%|█████████▌| 747/785 [04:57<00:11,  3.45it/s] 95%|█████████▌| 748/785 [04:57<00:10,  3.46it/s] 95%|█████████▌| 749/785 [04:57<00:10,  3.45it/s] 96%|█████████▌| 750/785 [04:57<00:10,  3.46it/s] 96%|█████████▌| 751/785 [04:58<00:09,  3.46it/s] 96%|█████████▌| 752/785 [04:58<00:09,  3.46it/s] 96%|█████████▌| 753/785 [04:58<00:09,  3.46it/s] 96%|█████████▌| 754/785 [04:59<00:08,  3.46it/s] 96%|█████████▌| 755/785 [04:59<00:08,  3.45it/s] 96%|█████████▋| 756/785 [04:59<00:08,  3.45it/s] 96%|█████████▋| 757/785 [04:59<00:08,  3.45it/s] 97%|█████████▋| 758/785 [05:00<00:07,  3.45it/s] 97%|█████████▋| 759/785 [05:00<00:07,  3.46it/s] 97%|█████████▋| 760/785 [05:00<00:07,  3.46it/s] 97%|█████████▋| 761/785 [05:01<00:06,  3.46it/s] 97%|█████████▋| 762/785 [05:01<00:06,  3.46it/s] 97%|█████████▋| 763/785 [05:01<00:06,  3.46it/s] 97%|█████████▋| 764/785 [05:01<00:06,  3.46it/s] 97%|█████████▋| 765/785 [05:02<00:05,  3.46it/s] 98%|█████████▊| 766/785 [05:02<00:05,  3.46it/s] 98%|█████████▊| 767/785 [05:02<00:05,  3.45it/s] 98%|█████████▊| 768/785 [05:03<00:04,  3.45it/s] 98%|█████████▊| 769/785 [05:03<00:04,  3.45it/s] 98%|█████████▊| 770/785 [05:03<00:04,  3.46it/s] 98%|█████████▊| 771/785 [05:03<00:04,  3.46it/s] 98%|█████████▊| 772/785 [05:04<00:03,  3.46it/s] 98%|█████████▊| 773/785 [05:04<00:03,  3.46it/s] 99%|█████████▊| 774/785 [05:04<00:03,  3.46it/s] 99%|█████████▊| 775/785 [05:05<00:02,  3.46it/s] 99%|█████████▉| 776/785 [05:05<00:02,  3.46it/s] 99%|█████████▉| 777/785 [05:05<00:02,  3.46it/s] 99%|█████████▉| 778/785 [05:06<00:02,  3.44it/s] 99%|█████████▉| 779/785 [05:06<00:01,  3.45it/s] 99%|█████████▉| 780/785 [05:06<00:01,  3.45it/s] 99%|█████████▉| 781/785 [05:06<00:01,  3.45it/s]100%|█████████▉| 782/785 [05:07<00:00,  3.45it/s]100%|█████████▉| 783/785 [05:07<00:00,  3.45it/s]100%|█████████▉| 784/785 [05:07<00:00,  3.45it/s]100%|██████████| 785/785 [05:07<00:00,  3.69it/s][INFO|trainer.py:2140] 2023-08-29 08:41:02,566 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:41:02,566 >>   Num examples = 4812
[INFO|trainer.py:2145] 2023-08-29 08:41:02,566 >>   Batch size = 8
{'eval_loss': 0.9767109751701355, 'eval_runtime': 12.9331, 'eval_samples_per_second': 372.068, 'eval_steps_per_second': 46.547, 'epoch': 4.0}

  0%|          | 0/602 [00:00<?, ?it/s][A
  1%|          | 6/602 [00:00<00:10, 57.27it/s][A
  2%|▏         | 12/602 [00:00<00:11, 50.34it/s][A
  3%|▎         | 18/602 [00:00<00:12, 48.65it/s][A
  4%|▍         | 23/602 [00:00<00:12, 47.87it/s][A
  5%|▍         | 28/602 [00:00<00:12, 47.38it/s][A
  5%|▌         | 33/602 [00:00<00:12, 47.13it/s][A
  6%|▋         | 38/602 [00:00<00:12, 46.78it/s][A
  7%|▋         | 43/602 [00:00<00:11, 46.60it/s][A
  8%|▊         | 48/602 [00:01<00:11, 46.57it/s][A
  9%|▉         | 53/602 [00:01<00:11, 46.58it/s][A
 10%|▉         | 58/602 [00:01<00:11, 46.63it/s][A
 10%|█         | 63/602 [00:01<00:11, 46.67it/s][A
 11%|█▏        | 68/602 [00:01<00:11, 46.58it/s][A
 12%|█▏        | 73/602 [00:01<00:11, 46.62it/s][A
 13%|█▎        | 78/602 [00:01<00:11, 46.57it/s][A
 14%|█▍        | 83/602 [00:01<00:11, 46.58it/s][A
 15%|█▍        | 88/602 [00:01<00:11, 46.45it/s][A
 15%|█▌        | 93/602 [00:01<00:10, 46.37it/s][A
 16%|█▋        | 98/602 [00:02<00:10, 46.34it/s][A
 17%|█▋        | 103/602 [00:02<00:10, 46.41it/s][A
 18%|█▊        | 108/602 [00:02<00:10, 46.48it/s][A
 19%|█▉        | 113/602 [00:02<00:10, 46.48it/s][A
 20%|█▉        | 118/602 [00:02<00:10, 46.56it/s][A
 20%|██        | 123/602 [00:02<00:10, 46.55it/s][A
 21%|██▏       | 128/602 [00:02<00:10, 46.56it/s][A
 22%|██▏       | 133/602 [00:02<00:10, 46.51it/s][A
 23%|██▎       | 138/602 [00:02<00:09, 46.50it/s][A
 24%|██▍       | 143/602 [00:03<00:09, 46.45it/s][A
 25%|██▍       | 148/602 [00:03<00:09, 46.37it/s][A
 25%|██▌       | 153/602 [00:03<00:09, 46.30it/s][A
 26%|██▌       | 158/602 [00:03<00:09, 46.37it/s][A
 27%|██▋       | 163/602 [00:03<00:09, 46.45it/s][A
 28%|██▊       | 168/602 [00:03<00:09, 46.55it/s][A
 29%|██▊       | 173/602 [00:03<00:09, 46.58it/s][A
 30%|██▉       | 178/602 [00:03<00:09, 46.47it/s][A
 30%|███       | 183/602 [00:03<00:09, 46.52it/s][A
 31%|███       | 188/602 [00:04<00:08, 46.53it/s][A
 32%|███▏      | 193/602 [00:04<00:08, 46.48it/s][A
 33%|███▎      | 198/602 [00:04<00:08, 46.39it/s][A
 34%|███▎      | 203/602 [00:04<00:08, 46.45it/s][A
 35%|███▍      | 208/602 [00:04<00:08, 46.44it/s][A
 35%|███▌      | 213/602 [00:04<00:08, 46.46it/s][A
 36%|███▌      | 218/602 [00:04<00:08, 46.52it/s][A
 37%|███▋      | 223/602 [00:04<00:08, 46.48it/s][A
 38%|███▊      | 228/602 [00:04<00:08, 46.55it/s][A
 39%|███▊      | 233/602 [00:04<00:07, 46.56it/s][A
 40%|███▉      | 238/602 [00:05<00:07, 46.53it/s][A
 40%|████      | 243/602 [00:05<00:07, 46.41it/s][A
 41%|████      | 248/602 [00:05<00:07, 46.43it/s][A
 42%|████▏     | 253/602 [00:05<00:07, 46.49it/s][A
 43%|████▎     | 258/602 [00:05<00:07, 46.47it/s][A
 44%|████▎     | 263/602 [00:05<00:07, 46.53it/s][A
 45%|████▍     | 268/602 [00:05<00:07, 46.57it/s][A
 45%|████▌     | 273/602 [00:05<00:07, 46.54it/s][A
 46%|████▌     | 278/602 [00:05<00:06, 46.56it/s][A
 47%|████▋     | 283/602 [00:06<00:06, 46.48it/s][A
 48%|████▊     | 288/602 [00:06<00:06, 46.43it/s][A
 49%|████▊     | 293/602 [00:06<00:06, 46.48it/s][A
 50%|████▉     | 298/602 [00:06<00:06, 46.45it/s][A
 50%|█████     | 303/602 [00:06<00:06, 46.47it/s][A
 51%|█████     | 308/602 [00:06<00:06, 46.40it/s][A
 52%|█████▏    | 313/602 [00:06<00:06, 46.46it/s][A
 53%|█████▎    | 318/602 [00:06<00:06, 46.47it/s][A
 54%|█████▎    | 323/602 [00:06<00:05, 46.59it/s][A
 54%|█████▍    | 328/602 [00:07<00:05, 46.55it/s][A
 55%|█████▌    | 333/602 [00:07<00:05, 46.54it/s][A
 56%|█████▌    | 338/602 [00:07<00:05, 46.53it/s][A
 57%|█████▋    | 343/602 [00:07<00:05, 46.48it/s][A
 58%|█████▊    | 348/602 [00:07<00:05, 46.45it/s][A
 59%|█████▊    | 353/602 [00:07<00:05, 46.27it/s][A
 59%|█████▉    | 358/602 [00:07<00:05, 46.39it/s][A
 60%|██████    | 363/602 [00:07<00:05, 46.43it/s][A
 61%|██████    | 368/602 [00:07<00:05, 46.39it/s][A
 62%|██████▏   | 373/602 [00:08<00:04, 46.38it/s][A
 63%|██████▎   | 378/602 [00:08<00:04, 46.47it/s][A
 64%|██████▎   | 383/602 [00:08<00:04, 46.52it/s][A
 64%|██████▍   | 388/602 [00:08<00:04, 46.42it/s][A
 65%|██████▌   | 393/602 [00:08<00:04, 46.45it/s][A
 66%|██████▌   | 398/602 [00:08<00:04, 46.47it/s][A
 67%|██████▋   | 403/602 [00:08<00:04, 46.41it/s][A
 68%|██████▊   | 408/602 [00:08<00:04, 46.44it/s][A
 69%|██████▊   | 413/602 [00:08<00:04, 46.47it/s][A
 69%|██████▉   | 418/602 [00:08<00:03, 46.43it/s][A
 70%|███████   | 423/602 [00:09<00:03, 46.46it/s][A
 71%|███████   | 428/602 [00:09<00:03, 46.52it/s][A
 72%|███████▏  | 433/602 [00:09<00:03, 46.50it/s][A
 73%|███████▎  | 438/602 [00:09<00:03, 46.52it/s][A
 74%|███████▎  | 443/602 [00:09<00:03, 46.43it/s][A
 74%|███████▍  | 448/602 [00:09<00:03, 46.46it/s][A
 75%|███████▌  | 453/602 [00:09<00:03, 46.50it/s][A
 76%|███████▌  | 458/602 [00:09<00:03, 46.46it/s][A
 77%|███████▋  | 463/602 [00:09<00:02, 46.51it/s][A
 78%|███████▊  | 468/602 [00:10<00:02, 46.46it/s][A
 79%|███████▊  | 473/602 [00:10<00:02, 46.46it/s][A
 79%|███████▉  | 478/602 [00:10<00:02, 46.52it/s][A
 80%|████████  | 483/602 [00:10<00:02, 46.46it/s][A
 81%|████████  | 488/602 [00:10<00:02, 46.46it/s][A
 82%|████████▏ | 493/602 [00:10<00:02, 46.46it/s][A
 83%|████████▎ | 498/602 [00:10<00:02, 46.49it/s][A
 84%|████████▎ | 503/602 [00:10<00:02, 46.50it/s][A
 84%|████████▍ | 508/602 [00:10<00:02, 46.43it/s][A
 85%|████████▌ | 513/602 [00:11<00:01, 46.51it/s][A
 86%|████████▌ | 518/602 [00:11<00:01, 46.48it/s][A
 87%|████████▋ | 523/602 [00:11<00:01, 46.53it/s][A
 88%|████████▊ | 528/602 [00:11<00:01, 46.55it/s][A
 89%|████████▊ | 533/602 [00:11<00:01, 46.44it/s][A
 89%|████████▉ | 538/602 [00:11<00:01, 46.41it/s][A
 90%|█████████ | 543/602 [00:11<00:01, 46.53it/s][A
 91%|█████████ | 548/602 [00:11<00:01, 46.44it/s][A
 92%|█████████▏| 553/602 [00:11<00:01, 46.49it/s][A
 93%|█████████▎| 558/602 [00:11<00:00, 46.45it/s][A
 94%|█████████▎| 563/602 [00:12<00:00, 46.44it/s][A
 94%|█████████▍| 568/602 [00:12<00:00, 46.46it/s][A
 95%|█████████▌| 573/602 [00:12<00:00, 46.53it/s][A
 96%|█████████▌| 578/602 [00:12<00:00, 46.47it/s][A
 97%|█████████▋| 583/602 [00:12<00:00, 46.49it/s][A
 98%|█████████▊| 588/602 [00:12<00:00, 46.50it/s][A
 99%|█████████▊| 593/602 [00:12<00:00, 46.44it/s][A
 99%|█████████▉| 598/602 [00:12<00:00, 46.41it/s][A                                                 
                                                 [A100%|██████████| 785/785 [05:20<00:00,  3.69it/s]
100%|██████████| 602/602 [00:12<00:00, 46.41it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:41:15,532 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-785
[INFO|configuration_utils.py:351] 2023-08-29 08:41:15,550 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-785/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:41:17,799 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-785/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:41:17,815 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-785/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:41:17,824 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-785/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 08:41:24,032 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 08:41:24,060 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-157 (score: 0.9452928900718689).
                                                 100%|██████████| 785/785 [05:33<00:00,  3.69it/s]100%|██████████| 785/785 [05:33<00:00,  2.35it/s]
[INFO|trainer.py:1894] 2023-08-29 08:41:28,072 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 08:41:28,094 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:41:30,189 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:41:30,336 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:41:30,424 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:41:30,980 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:30,980 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:30,980 >>   train_loss               =     0.5375
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:30,980 >>   train_runtime            = 0:05:33.47
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:30,980 >>   train_samples            =      10032
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:30,980 >>   train_samples_per_second =    150.417
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:30,980 >>   train_steps_per_second   =      2.354
{'eval_loss': 0.9795640110969543, 'eval_runtime': 12.9465, 'eval_samples_per_second': 371.683, 'eval_steps_per_second': 46.499, 'epoch': 5.0}
{'train_runtime': 333.4728, 'train_samples_per_second': 150.417, 'train_steps_per_second': 2.354, 'train_loss': 0.5374505887365645, 'epoch': 5.0}
08/29/2023 08:41:31 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 08:41:31,306 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:41:31,307 >>   Num examples = 4812
[INFO|trainer.py:2145] 2023-08-29 08:41:31,307 >>   Batch size = 8
  0%|          | 0/602 [00:00<?, ?it/s]  1%|          | 6/602 [00:00<00:10, 58.67it/s]  2%|▏         | 12/602 [00:00<00:11, 51.39it/s]  3%|▎         | 18/602 [00:00<00:11, 49.29it/s]  4%|▍         | 23/602 [00:00<00:11, 48.51it/s]  5%|▍         | 28/602 [00:00<00:11, 48.00it/s]  5%|▌         | 33/602 [00:00<00:11, 47.74it/s]  6%|▋         | 38/602 [00:00<00:11, 47.51it/s]  7%|▋         | 43/602 [00:00<00:11, 47.38it/s]  8%|▊         | 48/602 [00:00<00:11, 46.98it/s]  9%|▉         | 53/602 [00:01<00:11, 47.00it/s] 10%|▉         | 58/602 [00:01<00:11, 47.00it/s] 10%|█         | 63/602 [00:01<00:11, 47.00it/s] 11%|█▏        | 68/602 [00:01<00:11, 47.04it/s] 12%|█▏        | 73/602 [00:01<00:11, 47.07it/s] 13%|█▎        | 78/602 [00:01<00:11, 47.03it/s] 14%|█▍        | 83/602 [00:01<00:11, 47.07it/s] 15%|█▍        | 88/602 [00:01<00:10, 47.02it/s] 15%|█▌        | 93/602 [00:01<00:10, 46.84it/s] 16%|█▋        | 98/602 [00:02<00:10, 46.89it/s] 17%|█▋        | 103/602 [00:02<00:10, 46.96it/s] 18%|█▊        | 108/602 [00:02<00:10, 46.88it/s] 19%|█▉        | 113/602 [00:02<00:10, 46.90it/s] 20%|█▉        | 118/602 [00:02<00:10, 46.83it/s] 20%|██        | 123/602 [00:02<00:10, 46.88it/s] 21%|██▏       | 128/602 [00:02<00:10, 46.94it/s] 22%|██▏       | 133/602 [00:02<00:09, 46.91it/s] 23%|██▎       | 138/602 [00:02<00:09, 46.88it/s] 24%|██▍       | 143/602 [00:03<00:09, 46.93it/s] 25%|██▍       | 148/602 [00:03<00:09, 46.91it/s] 25%|██▌       | 153/602 [00:03<00:09, 46.87it/s] 26%|██▌       | 158/602 [00:03<00:09, 46.94it/s] 27%|██▋       | 163/602 [00:03<00:09, 46.90it/s] 28%|██▊       | 168/602 [00:03<00:09, 46.82it/s] 29%|██▊       | 173/602 [00:03<00:09, 46.88it/s] 30%|██▉       | 178/602 [00:03<00:09, 46.95it/s] 30%|███       | 183/602 [00:03<00:08, 46.91it/s] 31%|███       | 188/602 [00:03<00:08, 46.84it/s] 32%|███▏      | 193/602 [00:04<00:08, 46.85it/s] 33%|███▎      | 198/602 [00:04<00:08, 46.82it/s] 34%|███▎      | 203/602 [00:04<00:08, 46.87it/s] 35%|███▍      | 208/602 [00:04<00:08, 46.94it/s] 35%|███▌      | 213/602 [00:04<00:08, 46.84it/s] 36%|███▌      | 218/602 [00:04<00:08, 46.82it/s] 37%|███▋      | 223/602 [00:04<00:08, 46.90it/s] 38%|███▊      | 228/602 [00:04<00:07, 46.83it/s] 39%|███▊      | 233/602 [00:04<00:07, 46.87it/s] 40%|███▉      | 238/602 [00:05<00:07, 46.86it/s] 40%|████      | 243/602 [00:05<00:07, 46.81it/s] 41%|████      | 248/602 [00:05<00:07, 46.89it/s] 42%|████▏     | 253/602 [00:05<00:07, 46.94it/s] 43%|████▎     | 258/602 [00:05<00:07, 46.79it/s] 44%|████▎     | 263/602 [00:05<00:07, 46.85it/s] 45%|████▍     | 268/602 [00:05<00:07, 46.85it/s] 45%|████▌     | 273/602 [00:05<00:07, 46.75it/s] 46%|████▌     | 278/602 [00:05<00:06, 46.86it/s] 47%|████▋     | 283/602 [00:06<00:06, 46.78it/s] 48%|████▊     | 288/602 [00:06<00:06, 46.77it/s] 49%|████▊     | 293/602 [00:06<00:06, 46.83it/s] 50%|████▉     | 298/602 [00:06<00:06, 46.82it/s] 50%|█████     | 303/602 [00:06<00:06, 46.80it/s] 51%|█████     | 308/602 [00:06<00:06, 46.90it/s] 52%|█████▏    | 313/602 [00:06<00:06, 46.86it/s] 53%|█████▎    | 318/602 [00:06<00:06, 46.77it/s] 54%|█████▎    | 323/602 [00:06<00:05, 46.82it/s] 54%|█████▍    | 328/602 [00:06<00:05, 46.82it/s] 55%|█████▌    | 333/602 [00:07<00:05, 46.77it/s] 56%|█████▌    | 338/602 [00:07<00:05, 46.83it/s] 57%|█████▋    | 343/602 [00:07<00:05, 46.86it/s] 58%|█████▊    | 348/602 [00:07<00:05, 46.82it/s] 59%|█████▊    | 353/602 [00:07<00:05, 46.87it/s] 59%|█████▉    | 358/602 [00:07<00:05, 46.84it/s] 60%|██████    | 363/602 [00:07<00:05, 46.85it/s] 61%|██████    | 368/602 [00:07<00:05, 46.77it/s] 62%|██████▏   | 373/602 [00:07<00:04, 46.72it/s] 63%|██████▎   | 378/602 [00:08<00:04, 46.83it/s] 64%|██████▎   | 383/602 [00:08<00:04, 46.80it/s] 64%|██████▍   | 388/602 [00:08<00:04, 46.79it/s] 65%|██████▌   | 393/602 [00:08<00:04, 46.83it/s] 66%|██████▌   | 398/602 [00:08<00:04, 46.84it/s] 67%|██████▋   | 403/602 [00:08<00:04, 46.79it/s] 68%|██████▊   | 408/602 [00:08<00:04, 46.84it/s] 69%|██████▊   | 413/602 [00:08<00:04, 46.76it/s] 69%|██████▉   | 418/602 [00:08<00:03, 46.68it/s] 70%|███████   | 423/602 [00:08<00:03, 46.77it/s] 71%|███████   | 428/602 [00:09<00:03, 46.85it/s] 72%|███████▏  | 433/602 [00:09<00:03, 46.74it/s] 73%|███████▎  | 438/602 [00:09<00:03, 46.82it/s] 74%|███████▎  | 443/602 [00:09<00:03, 46.85it/s] 74%|███████▍  | 448/602 [00:09<00:03, 46.79it/s] 75%|███████▌  | 453/602 [00:09<00:03, 46.74it/s] 76%|███████▌  | 458/602 [00:09<00:03, 46.72it/s] 77%|███████▋  | 463/602 [00:09<00:02, 46.67it/s] 78%|███████▊  | 468/602 [00:09<00:02, 46.71it/s] 79%|███████▊  | 473/602 [00:10<00:02, 46.77it/s] 79%|███████▉  | 478/602 [00:10<00:02, 46.72it/s] 80%|████████  | 483/602 [00:10<00:02, 46.77it/s] 81%|████████  | 488/602 [00:10<00:02, 46.78it/s] 82%|████████▏ | 493/602 [00:10<00:02, 46.79it/s] 83%|████████▎ | 498/602 [00:10<00:02, 46.69it/s] 84%|████████▎ | 503/602 [00:10<00:02, 46.65it/s] 84%|████████▍ | 508/602 [00:10<00:02, 46.77it/s] 85%|████████▌ | 513/602 [00:10<00:01, 46.76it/s] 86%|████████▌ | 518/602 [00:11<00:01, 46.76it/s] 87%|████████▋ | 523/602 [00:11<00:01, 46.71it/s] 88%|████████▊ | 528/602 [00:11<00:01, 46.76it/s] 89%|████████▊ | 533/602 [00:11<00:01, 46.65it/s] 89%|████████▉ | 538/602 [00:11<00:01, 46.72it/s] 90%|█████████ | 543/602 [00:11<00:01, 46.66it/s] 91%|█████████ | 548/602 [00:11<00:01, 46.67it/s] 92%|█████████▏| 553/602 [00:11<00:01, 46.74it/s] 93%|█████████▎| 558/602 [00:11<00:00, 46.72it/s] 94%|█████████▎| 563/602 [00:11<00:00, 46.77it/s] 94%|█████████▍| 568/602 [00:12<00:00, 46.80it/s] 95%|█████████▌| 573/602 [00:12<00:00, 46.83it/s] 96%|█████████▌| 578/602 [00:12<00:00, 46.77it/s] 97%|█████████▋| 583/602 [00:12<00:00, 46.81it/s] 98%|█████████▊| 588/602 [00:12<00:00, 46.66it/s] 99%|█████████▊| 593/602 [00:12<00:00, 46.65it/s] 99%|█████████▉| 598/602 [00:12<00:00, 46.67it/s]100%|██████████| 602/602 [00:12<00:00, 46.91it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:41:44,163 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:44,163 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:44,163 >>   eval_loss               =     0.9453
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:44,163 >>   eval_runtime            = 0:00:12.85
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:44,163 >>   eval_samples            =       4812
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:44,163 >>   eval_samples_per_second =    374.303
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:44,163 >>   eval_steps_per_second   =     46.827
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:44,163 >>   perplexity              =     2.5736
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:51,144 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:51,148 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:51,148 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:51,148 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:51,149 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:41:51,755 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:41:51,756 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:41:52,315 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:41:53,335 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:41:53,335 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:56,200 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:56,204 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:56,204 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:56,204 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:56,204 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:41:56,821 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:41:56,822 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:41:57,402 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:41:57,551 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:41:57,551 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-314
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-628
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-785
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-157
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/checkpoint-471
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'given name', 'participant in', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14118
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14218, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.59it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:05,  1.51it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:07,  1.51it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:11,  1.52it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:13,  1.56it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:15,  1.53it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:16,  1.57it/s]Extractor Predicting: 27it [00:17,  1.48it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.50it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:20,  1.52it/s]Extractor Predicting: 33it [00:21,  1.50it/s]Extractor Predicting: 34it [00:22,  1.48it/s]Extractor Predicting: 35it [00:22,  1.50it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:24,  1.52it/s]Extractor Predicting: 38it [00:24,  1.51it/s]Extractor Predicting: 39it [00:25,  1.46it/s]Extractor Predicting: 40it [00:26,  1.47it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:27,  1.60it/s]Extractor Predicting: 43it [00:28,  1.59it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:29,  1.56it/s]Extractor Predicting: 46it [00:30,  1.58it/s]Extractor Predicting: 47it [00:30,  1.57it/s]Extractor Predicting: 48it [00:31,  1.59it/s]Extractor Predicting: 49it [00:31,  1.59it/s]Extractor Predicting: 50it [00:32,  1.58it/s]Extractor Predicting: 51it [00:33,  1.62it/s]Extractor Predicting: 52it [00:33,  1.64it/s]Extractor Predicting: 53it [00:34,  1.64it/s]Extractor Predicting: 54it [00:34,  1.67it/s]Extractor Predicting: 55it [00:35,  1.68it/s]Extractor Predicting: 56it [00:36,  1.67it/s]Extractor Predicting: 57it [00:36,  1.65it/s]Extractor Predicting: 58it [00:37,  1.62it/s]Extractor Predicting: 59it [00:38,  1.63it/s]Extractor Predicting: 60it [00:38,  1.62it/s]Extractor Predicting: 61it [00:39,  1.62it/s]Extractor Predicting: 62it [00:39,  1.63it/s]Extractor Predicting: 63it [00:40,  1.64it/s]Extractor Predicting: 64it [00:41,  1.67it/s]Extractor Predicting: 65it [00:41,  1.65it/s]Extractor Predicting: 66it [00:42,  1.63it/s]Extractor Predicting: 67it [00:42,  1.62it/s]Extractor Predicting: 68it [00:43,  1.60it/s]Extractor Predicting: 69it [00:44,  1.64it/s]Extractor Predicting: 70it [00:44,  1.63it/s]Extractor Predicting: 71it [00:45,  1.60it/s]Extractor Predicting: 72it [00:46,  1.62it/s]Extractor Predicting: 73it [00:46,  1.60it/s]Extractor Predicting: 74it [00:47,  1.62it/s]Extractor Predicting: 75it [00:47,  1.61it/s]Extractor Predicting: 76it [00:48,  1.60it/s]Extractor Predicting: 77it [00:49,  1.61it/s]Extractor Predicting: 78it [00:49,  1.61it/s]Extractor Predicting: 79it [00:50,  1.61it/s]Extractor Predicting: 80it [00:51,  1.59it/s]Extractor Predicting: 81it [00:51,  1.59it/s]Extractor Predicting: 82it [00:52,  1.55it/s]Extractor Predicting: 83it [00:52,  1.55it/s]Extractor Predicting: 84it [00:53,  1.57it/s]Extractor Predicting: 85it [00:54,  1.56it/s]Extractor Predicting: 86it [00:54,  1.55it/s]Extractor Predicting: 87it [00:55,  1.55it/s]Extractor Predicting: 88it [00:56,  1.55it/s]Extractor Predicting: 89it [00:56,  1.58it/s]Extractor Predicting: 90it [00:57,  1.40it/s]Extractor Predicting: 91it [00:58,  1.40it/s]Extractor Predicting: 92it [00:59,  1.45it/s]Extractor Predicting: 93it [00:59,  1.44it/s]Extractor Predicting: 94it [01:00,  1.46it/s]Extractor Predicting: 95it [01:01,  1.44it/s]Extractor Predicting: 96it [01:01,  1.45it/s]Extractor Predicting: 97it [01:02,  1.41it/s]Extractor Predicting: 98it [01:03,  1.42it/s]Extractor Predicting: 99it [01:03,  1.44it/s]Extractor Predicting: 100it [01:04,  1.46it/s]Extractor Predicting: 101it [01:05,  1.49it/s]Extractor Predicting: 102it [01:05,  1.49it/s]Extractor Predicting: 103it [01:06,  1.49it/s]Extractor Predicting: 104it [01:07,  1.48it/s]Extractor Predicting: 105it [01:07,  1.49it/s]Extractor Predicting: 106it [01:08,  1.50it/s]Extractor Predicting: 107it [01:09,  1.45it/s]Extractor Predicting: 108it [01:10,  1.45it/s]Extractor Predicting: 109it [01:10,  1.51it/s]Extractor Predicting: 110it [01:11,  1.49it/s]Extractor Predicting: 111it [01:11,  1.49it/s]Extractor Predicting: 112it [01:12,  1.50it/s]Extractor Predicting: 113it [01:13,  1.48it/s]Extractor Predicting: 114it [01:14,  1.47it/s]Extractor Predicting: 115it [01:14,  1.46it/s]Extractor Predicting: 116it [01:15,  1.47it/s]Extractor Predicting: 117it [01:16,  1.45it/s]Extractor Predicting: 118it [01:16,  1.33it/s]Extractor Predicting: 119it [01:17,  1.39it/s]Extractor Predicting: 120it [01:18,  1.42it/s]Extractor Predicting: 121it [01:19,  1.42it/s]Extractor Predicting: 122it [01:19,  1.47it/s]Extractor Predicting: 123it [01:20,  1.50it/s]Extractor Predicting: 124it [01:20,  1.47it/s]Extractor Predicting: 125it [01:21,  1.49it/s]Extractor Predicting: 126it [01:22,  1.48it/s]Extractor Predicting: 127it [01:23,  1.46it/s]Extractor Predicting: 128it [01:23,  1.44it/s]Extractor Predicting: 129it [01:24,  1.46it/s]Extractor Predicting: 130it [01:25,  1.45it/s]Extractor Predicting: 131it [01:25,  1.45it/s]Extractor Predicting: 132it [01:26,  1.45it/s]Extractor Predicting: 133it [01:27,  1.42it/s]Extractor Predicting: 134it [01:27,  1.40it/s]Extractor Predicting: 135it [01:28,  1.37it/s]Extractor Predicting: 136it [01:29,  1.37it/s]Extractor Predicting: 137it [01:30,  1.34it/s]Extractor Predicting: 138it [01:31,  1.33it/s]Extractor Predicting: 139it [01:31,  1.35it/s]Extractor Predicting: 140it [01:32,  1.37it/s]Extractor Predicting: 141it [01:33,  1.40it/s]Extractor Predicting: 142it [01:33,  1.39it/s]Extractor Predicting: 143it [01:34,  1.41it/s]Extractor Predicting: 144it [01:35,  1.38it/s]Extractor Predicting: 145it [01:35,  1.38it/s]Extractor Predicting: 146it [01:36,  1.35it/s]Extractor Predicting: 147it [01:37,  1.39it/s]Extractor Predicting: 148it [01:38,  1.40it/s]Extractor Predicting: 149it [01:38,  1.38it/s]Extractor Predicting: 150it [01:39,  1.39it/s]Extractor Predicting: 151it [01:40,  1.39it/s]Extractor Predicting: 152it [01:41,  1.39it/s]Extractor Predicting: 153it [01:41,  1.39it/s]Extractor Predicting: 154it [01:42,  1.37it/s]Extractor Predicting: 155it [01:43,  1.38it/s]Extractor Predicting: 156it [01:43,  1.36it/s]Extractor Predicting: 157it [01:44,  1.34it/s]Extractor Predicting: 158it [01:45,  1.33it/s]Extractor Predicting: 159it [01:46,  1.37it/s]Extractor Predicting: 160it [01:46,  1.40it/s]Extractor Predicting: 161it [01:47,  1.43it/s]Extractor Predicting: 162it [01:48,  1.43it/s]Extractor Predicting: 163it [01:48,  1.44it/s]Extractor Predicting: 164it [01:49,  1.47it/s]Extractor Predicting: 165it [01:50,  1.46it/s]Extractor Predicting: 166it [01:50,  1.49it/s]Extractor Predicting: 167it [01:51,  1.48it/s]Extractor Predicting: 168it [01:52,  1.47it/s]Extractor Predicting: 169it [01:52,  1.49it/s]Extractor Predicting: 170it [01:53,  1.49it/s]Extractor Predicting: 171it [01:54,  1.49it/s]Extractor Predicting: 172it [01:54,  1.50it/s]Extractor Predicting: 173it [01:55,  1.48it/s]Extractor Predicting: 174it [01:56,  1.49it/s]Extractor Predicting: 175it [01:56,  1.47it/s]Extractor Predicting: 176it [01:57,  1.45it/s]Extractor Predicting: 177it [01:58,  1.46it/s]Extractor Predicting: 178it [01:59,  1.46it/s]Extractor Predicting: 179it [01:59,  1.46it/s]Extractor Predicting: 180it [02:00,  1.47it/s]Extractor Predicting: 181it [02:01,  1.45it/s]Extractor Predicting: 182it [02:01,  1.45it/s]Extractor Predicting: 183it [02:02,  1.42it/s]Extractor Predicting: 184it [02:03,  1.44it/s]Extractor Predicting: 185it [02:03,  1.61it/s]Extractor Predicting: 185it [02:03,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:10,805 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:10,811 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:10,811 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:10,812 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:10,812 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:44:11,132 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:44:11,133 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:44:11,399 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:44:12,432 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:44:12,432 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:13,759 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:13,761 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:13,761 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:13,761 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:13,761 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:44:14,084 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:44:14,086 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:44:14,768 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:44:14,926 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:44:14,926 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5423925667828107,
  "recall": 0.09704904405652535,
  "score": 0.16463952053587166,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'employer', 'field of work', 'languages spoken, written or signed', 'lyrics by', 'manufacturer', 'member of sports team', 'occupation', 'performer', 'residence', 'shares border with', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 31765
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31865, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.56it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.55it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:09,  1.43it/s]Extractor Predicting: 16it [00:10,  1.46it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:11,  1.47it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:13,  1.47it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:17,  1.52it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.56it/s]Extractor Predicting: 30it [00:19,  1.63it/s]Extractor Predicting: 31it [00:20,  1.65it/s]Extractor Predicting: 32it [00:20,  1.66it/s]Extractor Predicting: 33it [00:21,  1.64it/s]Extractor Predicting: 34it [00:22,  1.64it/s]Extractor Predicting: 35it [00:22,  1.63it/s]Extractor Predicting: 36it [00:23,  1.62it/s]Extractor Predicting: 37it [00:23,  1.60it/s]Extractor Predicting: 38it [00:24,  1.61it/s]Extractor Predicting: 39it [00:25,  1.57it/s]Extractor Predicting: 40it [00:25,  1.57it/s]Extractor Predicting: 41it [00:26,  1.60it/s]Extractor Predicting: 42it [00:27,  1.60it/s]Extractor Predicting: 43it [00:27,  1.60it/s]Extractor Predicting: 44it [00:28,  1.62it/s]Extractor Predicting: 45it [00:28,  1.63it/s]Extractor Predicting: 46it [00:29,  1.66it/s]Extractor Predicting: 47it [00:30,  1.68it/s]Extractor Predicting: 48it [00:30,  1.63it/s]Extractor Predicting: 49it [00:31,  1.66it/s]Extractor Predicting: 50it [00:31,  1.63it/s]Extractor Predicting: 51it [00:32,  1.61it/s]Extractor Predicting: 52it [00:33,  1.61it/s]Extractor Predicting: 53it [00:33,  1.63it/s]Extractor Predicting: 54it [00:34,  1.57it/s]Extractor Predicting: 55it [00:35,  1.58it/s]Extractor Predicting: 56it [00:35,  1.56it/s]Extractor Predicting: 57it [00:36,  1.62it/s]Extractor Predicting: 58it [00:36,  1.68it/s]Extractor Predicting: 59it [00:37,  1.64it/s]Extractor Predicting: 60it [00:38,  1.60it/s]Extractor Predicting: 61it [00:38,  1.57it/s]Extractor Predicting: 62it [00:39,  1.56it/s]Extractor Predicting: 63it [00:40,  1.54it/s]Extractor Predicting: 64it [00:40,  1.53it/s]Extractor Predicting: 65it [00:41,  1.55it/s]Extractor Predicting: 66it [00:42,  1.55it/s]Extractor Predicting: 67it [00:42,  1.54it/s]Extractor Predicting: 68it [00:43,  1.51it/s]Extractor Predicting: 69it [00:44,  1.55it/s]Extractor Predicting: 70it [00:44,  1.56it/s]Extractor Predicting: 71it [00:45,  1.55it/s]Extractor Predicting: 72it [00:46,  1.51it/s]Extractor Predicting: 73it [00:46,  1.49it/s]Extractor Predicting: 74it [00:47,  1.48it/s]Extractor Predicting: 75it [00:48,  1.48it/s]Extractor Predicting: 76it [00:48,  1.47it/s]Extractor Predicting: 77it [00:49,  1.49it/s]Extractor Predicting: 78it [00:50,  1.53it/s]Extractor Predicting: 79it [00:50,  1.51it/s]Extractor Predicting: 80it [00:51,  1.56it/s]Extractor Predicting: 81it [00:51,  1.55it/s]Extractor Predicting: 82it [00:52,  1.54it/s]Extractor Predicting: 83it [00:53,  1.56it/s]Extractor Predicting: 84it [00:53,  1.55it/s]Extractor Predicting: 85it [00:54,  1.55it/s]Extractor Predicting: 86it [00:55,  1.55it/s]Extractor Predicting: 87it [00:55,  1.53it/s]Extractor Predicting: 88it [00:56,  1.53it/s]Extractor Predicting: 89it [00:57,  1.56it/s]Extractor Predicting: 90it [00:57,  1.57it/s]Extractor Predicting: 91it [00:58,  1.55it/s]Extractor Predicting: 92it [00:59,  1.52it/s]Extractor Predicting: 93it [00:59,  1.54it/s]Extractor Predicting: 94it [01:00,  1.57it/s]Extractor Predicting: 95it [01:00,  1.56it/s]Extractor Predicting: 96it [01:01,  1.56it/s]Extractor Predicting: 97it [01:02,  1.55it/s]Extractor Predicting: 98it [01:02,  1.50it/s]Extractor Predicting: 99it [01:03,  1.47it/s]Extractor Predicting: 100it [01:04,  1.45it/s]Extractor Predicting: 101it [01:05,  1.45it/s]Extractor Predicting: 102it [01:05,  1.46it/s]Extractor Predicting: 103it [01:06,  1.42it/s]Extractor Predicting: 104it [01:07,  1.48it/s]Extractor Predicting: 105it [01:07,  1.46it/s]Extractor Predicting: 106it [01:08,  1.50it/s]Extractor Predicting: 107it [01:09,  1.51it/s]Extractor Predicting: 108it [01:09,  1.52it/s]Extractor Predicting: 109it [01:10,  1.51it/s]Extractor Predicting: 110it [01:11,  1.50it/s]Extractor Predicting: 111it [01:11,  1.54it/s]Extractor Predicting: 112it [01:12,  1.54it/s]Extractor Predicting: 113it [01:13,  1.55it/s]Extractor Predicting: 114it [01:13,  1.56it/s]Extractor Predicting: 115it [01:14,  1.56it/s]Extractor Predicting: 116it [01:14,  1.55it/s]Extractor Predicting: 117it [01:15,  1.57it/s]Extractor Predicting: 118it [01:16,  1.60it/s]Extractor Predicting: 119it [01:16,  1.67it/s]Extractor Predicting: 120it [01:17,  1.61it/s]Extractor Predicting: 121it [01:18,  1.58it/s]Extractor Predicting: 122it [01:18,  1.58it/s]Extractor Predicting: 123it [01:19,  1.54it/s]Extractor Predicting: 124it [01:19,  1.54it/s]Extractor Predicting: 125it [01:20,  1.53it/s]Extractor Predicting: 126it [01:21,  1.53it/s]Extractor Predicting: 127it [01:22,  1.50it/s]Extractor Predicting: 128it [01:22,  1.52it/s]Extractor Predicting: 129it [01:23,  1.51it/s]Extractor Predicting: 130it [01:23,  1.51it/s]Extractor Predicting: 131it [01:24,  1.50it/s]Extractor Predicting: 132it [01:25,  1.45it/s]Extractor Predicting: 133it [01:26,  1.46it/s]Extractor Predicting: 134it [01:26,  1.49it/s]Extractor Predicting: 135it [01:27,  1.45it/s]Extractor Predicting: 136it [01:28,  1.48it/s]Extractor Predicting: 137it [01:28,  1.51it/s]Extractor Predicting: 138it [01:29,  1.49it/s]Extractor Predicting: 139it [01:30,  1.50it/s]Extractor Predicting: 140it [01:30,  1.47it/s]Extractor Predicting: 141it [01:31,  1.44it/s]Extractor Predicting: 142it [01:32,  1.45it/s]Extractor Predicting: 143it [01:32,  1.48it/s]Extractor Predicting: 144it [01:33,  1.49it/s]Extractor Predicting: 145it [01:34,  1.48it/s]Extractor Predicting: 146it [01:34,  1.47it/s]Extractor Predicting: 147it [01:35,  1.47it/s]Extractor Predicting: 148it [01:36,  1.47it/s]Extractor Predicting: 149it [01:36,  1.47it/s]Extractor Predicting: 150it [01:37,  1.48it/s]Extractor Predicting: 151it [01:38,  1.49it/s]Extractor Predicting: 152it [01:38,  1.52it/s]Extractor Predicting: 153it [01:39,  1.52it/s]Extractor Predicting: 154it [01:40,  1.51it/s]Extractor Predicting: 155it [01:40,  1.47it/s]Extractor Predicting: 156it [01:41,  1.48it/s]Extractor Predicting: 157it [01:42,  1.45it/s]Extractor Predicting: 158it [01:42,  1.47it/s]Extractor Predicting: 159it [01:43,  1.45it/s]Extractor Predicting: 160it [01:44,  1.47it/s]Extractor Predicting: 161it [01:45,  1.27it/s]Extractor Predicting: 162it [01:46,  1.34it/s]Extractor Predicting: 163it [01:46,  1.40it/s]Extractor Predicting: 164it [01:47,  1.42it/s]Extractor Predicting: 165it [01:47,  1.45it/s]Extractor Predicting: 166it [01:48,  1.46it/s]Extractor Predicting: 167it [01:49,  1.47it/s]Extractor Predicting: 168it [01:49,  1.49it/s]Extractor Predicting: 169it [01:50,  1.52it/s]Extractor Predicting: 170it [01:51,  1.51it/s]Extractor Predicting: 171it [01:51,  1.51it/s]Extractor Predicting: 172it [01:52,  1.51it/s]Extractor Predicting: 173it [01:53,  1.51it/s]Extractor Predicting: 174it [01:53,  1.50it/s]Extractor Predicting: 175it [01:54,  1.49it/s]Extractor Predicting: 176it [01:55,  1.52it/s]Extractor Predicting: 177it [01:55,  1.52it/s]Extractor Predicting: 178it [01:56,  1.49it/s]Extractor Predicting: 179it [01:57,  1.52it/s]Extractor Predicting: 180it [01:57,  1.55it/s]Extractor Predicting: 181it [01:58,  1.56it/s]Extractor Predicting: 182it [01:59,  1.54it/s]Extractor Predicting: 183it [01:59,  1.55it/s]Extractor Predicting: 184it [02:00,  1.53it/s]Extractor Predicting: 185it [02:01,  1.55it/s]Extractor Predicting: 186it [02:01,  1.55it/s]Extractor Predicting: 187it [02:02,  1.53it/s]Extractor Predicting: 188it [02:03,  1.53it/s]Extractor Predicting: 189it [02:03,  1.51it/s]Extractor Predicting: 190it [02:04,  1.52it/s]Extractor Predicting: 191it [02:05,  1.51it/s]Extractor Predicting: 192it [02:05,  1.53it/s]Extractor Predicting: 193it [02:06,  1.52it/s]Extractor Predicting: 194it [02:07,  1.52it/s]Extractor Predicting: 195it [02:07,  1.54it/s]Extractor Predicting: 196it [02:08,  1.57it/s]Extractor Predicting: 197it [02:08,  1.56it/s]Extractor Predicting: 198it [02:09,  1.51it/s]Extractor Predicting: 199it [02:10,  1.51it/s]Extractor Predicting: 200it [02:10,  1.52it/s]Extractor Predicting: 201it [02:11,  1.51it/s]Extractor Predicting: 202it [02:12,  1.47it/s]Extractor Predicting: 203it [02:13,  1.47it/s]Extractor Predicting: 204it [02:13,  1.48it/s]Extractor Predicting: 205it [02:14,  1.47it/s]Extractor Predicting: 206it [02:14,  1.50it/s]Extractor Predicting: 207it [02:15,  1.51it/s]Extractor Predicting: 208it [02:16,  1.53it/s]Extractor Predicting: 209it [02:16,  1.55it/s]Extractor Predicting: 210it [02:17,  1.55it/s]Extractor Predicting: 211it [02:18,  1.53it/s]Extractor Predicting: 212it [02:18,  1.58it/s]Extractor Predicting: 213it [02:19,  1.56it/s]Extractor Predicting: 214it [02:20,  1.55it/s]Extractor Predicting: 215it [02:20,  1.50it/s]Extractor Predicting: 216it [02:21,  1.49it/s]Extractor Predicting: 217it [02:22,  1.51it/s]Extractor Predicting: 218it [02:22,  1.51it/s]Extractor Predicting: 219it [02:23,  1.50it/s]Extractor Predicting: 220it [02:24,  1.49it/s]Extractor Predicting: 221it [02:24,  1.53it/s]Extractor Predicting: 222it [02:25,  1.53it/s]Extractor Predicting: 223it [02:26,  1.55it/s]Extractor Predicting: 224it [02:26,  1.55it/s]Extractor Predicting: 225it [02:27,  1.55it/s]Extractor Predicting: 226it [02:28,  1.53it/s]Extractor Predicting: 227it [02:28,  1.52it/s]Extractor Predicting: 228it [02:29,  1.53it/s]Extractor Predicting: 229it [02:30,  1.53it/s]Extractor Predicting: 230it [02:30,  1.53it/s]Extractor Predicting: 231it [02:31,  1.57it/s]Extractor Predicting: 232it [02:31,  1.58it/s]Extractor Predicting: 233it [02:32,  1.54it/s]Extractor Predicting: 234it [02:33,  1.58it/s]Extractor Predicting: 235it [02:33,  1.59it/s]Extractor Predicting: 236it [02:34,  1.55it/s]Extractor Predicting: 237it [02:35,  1.55it/s]Extractor Predicting: 238it [02:35,  1.54it/s]Extractor Predicting: 239it [02:36,  1.55it/s]Extractor Predicting: 240it [02:37,  1.56it/s]Extractor Predicting: 241it [02:37,  1.58it/s]Extractor Predicting: 242it [02:38,  1.54it/s]Extractor Predicting: 243it [02:39,  1.51it/s]Extractor Predicting: 244it [02:39,  1.54it/s]Extractor Predicting: 245it [02:40,  1.57it/s]Extractor Predicting: 246it [02:40,  1.54it/s]Extractor Predicting: 247it [02:41,  1.54it/s]Extractor Predicting: 248it [02:42,  1.56it/s]Extractor Predicting: 249it [02:42,  1.58it/s]Extractor Predicting: 250it [02:43,  1.56it/s]Extractor Predicting: 251it [02:44,  1.58it/s]Extractor Predicting: 252it [02:44,  1.60it/s]Extractor Predicting: 253it [02:45,  1.58it/s]Extractor Predicting: 254it [02:45,  1.62it/s]Extractor Predicting: 255it [02:46,  1.58it/s]Extractor Predicting: 256it [02:47,  1.57it/s]Extractor Predicting: 257it [02:47,  1.54it/s]Extractor Predicting: 258it [02:48,  1.55it/s]Extractor Predicting: 259it [02:49,  1.55it/s]Extractor Predicting: 260it [02:49,  1.55it/s]Extractor Predicting: 261it [02:50,  1.56it/s]Extractor Predicting: 262it [02:51,  1.57it/s]Extractor Predicting: 263it [02:51,  1.56it/s]Extractor Predicting: 264it [02:52,  1.55it/s]Extractor Predicting: 265it [02:53,  1.52it/s]Extractor Predicting: 266it [02:53,  1.54it/s]Extractor Predicting: 267it [02:54,  1.57it/s]Extractor Predicting: 268it [02:55,  1.55it/s]Extractor Predicting: 269it [02:55,  1.50it/s]Extractor Predicting: 270it [02:56,  1.56it/s]Extractor Predicting: 271it [02:56,  1.56it/s]Extractor Predicting: 272it [02:57,  1.53it/s]Extractor Predicting: 273it [02:58,  1.53it/s]Extractor Predicting: 274it [02:58,  1.52it/s]Extractor Predicting: 275it [02:59,  1.51it/s]Extractor Predicting: 276it [03:00,  1.52it/s]Extractor Predicting: 277it [03:00,  1.54it/s]Extractor Predicting: 278it [03:01,  1.50it/s]Extractor Predicting: 279it [03:02,  1.50it/s]Extractor Predicting: 280it [03:02,  1.50it/s]Extractor Predicting: 281it [03:03,  1.50it/s]Extractor Predicting: 282it [03:04,  1.28it/s]Extractor Predicting: 283it [03:05,  1.37it/s]Extractor Predicting: 284it [03:05,  1.37it/s]Extractor Predicting: 285it [03:06,  1.42it/s]Extractor Predicting: 286it [03:07,  1.46it/s]Extractor Predicting: 287it [03:07,  1.48it/s]Extractor Predicting: 288it [03:08,  1.48it/s]Extractor Predicting: 289it [03:09,  1.51it/s]Extractor Predicting: 290it [03:10,  1.44it/s]Extractor Predicting: 291it [03:10,  1.50it/s]Extractor Predicting: 292it [03:11,  1.48it/s]Extractor Predicting: 293it [03:11,  1.51it/s]Extractor Predicting: 294it [03:12,  1.55it/s]Extractor Predicting: 295it [03:13,  1.53it/s]Extractor Predicting: 296it [03:13,  1.57it/s]Extractor Predicting: 297it [03:14,  1.58it/s]Extractor Predicting: 298it [03:15,  1.60it/s]Extractor Predicting: 299it [03:15,  1.55it/s]Extractor Predicting: 300it [03:16,  1.56it/s]Extractor Predicting: 301it [03:17,  1.52it/s]Extractor Predicting: 302it [03:17,  1.52it/s]Extractor Predicting: 303it [03:18,  1.56it/s]Extractor Predicting: 304it [03:19,  1.53it/s]Extractor Predicting: 305it [03:19,  1.52it/s]Extractor Predicting: 306it [03:20,  1.53it/s]Extractor Predicting: 307it [03:20,  1.53it/s]Extractor Predicting: 308it [03:21,  1.56it/s]Extractor Predicting: 309it [03:22,  1.53it/s]Extractor Predicting: 310it [03:22,  1.53it/s]Extractor Predicting: 311it [03:23,  1.52it/s]Extractor Predicting: 312it [03:24,  1.50it/s]Extractor Predicting: 313it [03:24,  1.49it/s]Extractor Predicting: 314it [03:25,  1.45it/s]Extractor Predicting: 315it [03:26,  1.44it/s]Extractor Predicting: 316it [03:27,  1.48it/s]Extractor Predicting: 317it [03:27,  1.50it/s]Extractor Predicting: 318it [03:28,  1.53it/s]Extractor Predicting: 319it [03:28,  1.52it/s]Extractor Predicting: 320it [03:29,  1.58it/s]Extractor Predicting: 321it [03:30,  1.57it/s]Extractor Predicting: 322it [03:30,  1.55it/s]Extractor Predicting: 323it [03:31,  1.55it/s]Extractor Predicting: 324it [03:32,  1.56it/s]Extractor Predicting: 325it [03:32,  1.54it/s]Extractor Predicting: 326it [03:33,  1.53it/s]Extractor Predicting: 327it [03:34,  1.49it/s]Extractor Predicting: 328it [03:34,  1.46it/s]Extractor Predicting: 329it [03:35,  1.49it/s]Extractor Predicting: 330it [03:36,  1.54it/s]Extractor Predicting: 331it [03:36,  1.56it/s]Extractor Predicting: 332it [03:37,  1.56it/s]Extractor Predicting: 333it [03:38,  1.53it/s]Extractor Predicting: 334it [03:38,  1.52it/s]Extractor Predicting: 335it [03:39,  1.52it/s]Extractor Predicting: 336it [03:40,  1.51it/s]Extractor Predicting: 337it [03:40,  1.53it/s]Extractor Predicting: 338it [03:41,  1.56it/s]Extractor Predicting: 339it [03:41,  1.57it/s]Extractor Predicting: 340it [03:42,  1.60it/s]Extractor Predicting: 341it [03:43,  1.57it/s]Extractor Predicting: 342it [03:43,  1.58it/s]Extractor Predicting: 343it [03:44,  1.60it/s]Extractor Predicting: 344it [03:45,  1.57it/s]Extractor Predicting: 345it [03:45,  1.57it/s]Extractor Predicting: 346it [03:46,  1.52it/s]Extractor Predicting: 347it [03:47,  1.54it/s]Extractor Predicting: 348it [03:47,  1.53it/s]Extractor Predicting: 349it [03:48,  1.54it/s]Extractor Predicting: 350it [03:49,  1.55it/s]Extractor Predicting: 351it [03:49,  1.56it/s]Extractor Predicting: 352it [03:50,  1.58it/s]Extractor Predicting: 353it [03:50,  1.57it/s]Extractor Predicting: 354it [03:51,  1.57it/s]Extractor Predicting: 355it [03:52,  1.55it/s]Extractor Predicting: 356it [03:52,  1.55it/s]Extractor Predicting: 357it [03:53,  1.53it/s]Extractor Predicting: 358it [03:54,  1.54it/s]Extractor Predicting: 359it [03:54,  1.53it/s]Extractor Predicting: 360it [03:55,  1.53it/s]Extractor Predicting: 361it [03:56,  1.56it/s]Extractor Predicting: 362it [03:56,  1.55it/s]Extractor Predicting: 363it [03:57,  1.57it/s]Extractor Predicting: 364it [03:58,  1.55it/s]Extractor Predicting: 365it [03:58,  1.57it/s]Extractor Predicting: 366it [03:59,  1.56it/s]Extractor Predicting: 367it [03:59,  1.56it/s]Extractor Predicting: 368it [04:00,  1.53it/s]Extractor Predicting: 369it [04:01,  1.52it/s]Extractor Predicting: 370it [04:01,  1.56it/s]Extractor Predicting: 371it [04:02,  1.55it/s]Extractor Predicting: 372it [04:03,  1.57it/s]Extractor Predicting: 373it [04:03,  1.58it/s]Extractor Predicting: 374it [04:04,  1.58it/s]Extractor Predicting: 375it [04:05,  1.58it/s]Extractor Predicting: 376it [04:05,  1.55it/s]Extractor Predicting: 377it [04:06,  1.54it/s]Extractor Predicting: 378it [04:07,  1.54it/s]Extractor Predicting: 379it [04:07,  1.57it/s]Extractor Predicting: 380it [04:08,  1.57it/s]Extractor Predicting: 381it [04:08,  1.60it/s]Extractor Predicting: 382it [04:09,  1.56it/s]Extractor Predicting: 383it [04:10,  1.58it/s]Extractor Predicting: 384it [04:10,  1.55it/s]Extractor Predicting: 385it [04:11,  1.53it/s]Extractor Predicting: 386it [04:12,  1.55it/s]Extractor Predicting: 387it [04:12,  1.56it/s]Extractor Predicting: 388it [04:13,  1.52it/s]Extractor Predicting: 389it [04:14,  1.50it/s]Extractor Predicting: 390it [04:14,  1.45it/s]Extractor Predicting: 391it [04:15,  1.47it/s]Extractor Predicting: 392it [04:16,  1.48it/s]Extractor Predicting: 393it [04:16,  1.49it/s]Extractor Predicting: 394it [04:17,  1.54it/s]Extractor Predicting: 395it [04:18,  1.54it/s]Extractor Predicting: 396it [04:18,  1.55it/s]Extractor Predicting: 397it [04:19,  1.56it/s]Extractor Predicting: 398it [04:20,  1.59it/s]Extractor Predicting: 399it [04:20,  1.60it/s]Extractor Predicting: 400it [04:21,  1.59it/s]Extractor Predicting: 401it [04:21,  1.59it/s]Extractor Predicting: 402it [04:22,  1.58it/s]Extractor Predicting: 403it [04:23,  1.55it/s]Extractor Predicting: 404it [04:23,  1.54it/s]Extractor Predicting: 405it [04:24,  1.55it/s]Extractor Predicting: 406it [04:25,  1.55it/s]Extractor Predicting: 407it [04:25,  1.52it/s]Extractor Predicting: 408it [04:26,  1.53it/s]Extractor Predicting: 409it [04:27,  1.53it/s]Extractor Predicting: 410it [04:27,  1.55it/s]Extractor Predicting: 411it [04:28,  1.53it/s]Extractor Predicting: 412it [04:29,  1.53it/s]Extractor Predicting: 413it [04:29,  1.48it/s]Extractor Predicting: 414it [04:30,  1.52it/s]Extractor Predicting: 415it [04:31,  1.56it/s]Extractor Predicting: 416it [04:31,  1.57it/s]Extractor Predicting: 417it [04:32,  1.60it/s]Extractor Predicting: 418it [04:32,  1.61it/s]Extractor Predicting: 419it [04:33,  1.63it/s]Extractor Predicting: 420it [04:34,  1.60it/s]Extractor Predicting: 421it [04:34,  1.58it/s]Extractor Predicting: 422it [04:35,  1.60it/s]Extractor Predicting: 423it [04:36,  1.58it/s]Extractor Predicting: 424it [04:36,  1.58it/s]Extractor Predicting: 425it [04:37,  1.62it/s]Extractor Predicting: 426it [04:37,  1.62it/s]Extractor Predicting: 427it [04:38,  1.61it/s]Extractor Predicting: 428it [04:39,  1.58it/s]Extractor Predicting: 429it [04:40,  1.36it/s]Extractor Predicting: 430it [04:40,  1.44it/s]Extractor Predicting: 431it [04:41,  1.45it/s]Extractor Predicting: 432it [04:41,  1.51it/s]Extractor Predicting: 433it [04:42,  1.57it/s]Extractor Predicting: 434it [04:43,  1.58it/s]Extractor Predicting: 435it [04:43,  1.59it/s]Extractor Predicting: 436it [04:44,  1.62it/s]Extractor Predicting: 437it [04:45,  1.62it/s]Extractor Predicting: 438it [04:45,  1.61it/s]Extractor Predicting: 439it [04:46,  1.61it/s]Extractor Predicting: 440it [04:46,  1.58it/s]Extractor Predicting: 441it [04:47,  1.60it/s]Extractor Predicting: 442it [04:48,  1.58it/s]Extractor Predicting: 443it [04:48,  1.59it/s]Extractor Predicting: 444it [04:49,  1.59it/s]Extractor Predicting: 445it [04:50,  1.56it/s]Extractor Predicting: 446it [04:50,  1.53it/s]Extractor Predicting: 447it [04:51,  1.56it/s]Extractor Predicting: 448it [04:52,  1.59it/s]Extractor Predicting: 449it [04:52,  1.58it/s]Extractor Predicting: 450it [04:53,  1.60it/s]Extractor Predicting: 451it [04:53,  1.61it/s]Extractor Predicting: 452it [04:54,  1.59it/s]Extractor Predicting: 453it [04:55,  1.56it/s]Extractor Predicting: 454it [04:55,  1.53it/s]Extractor Predicting: 455it [04:56,  1.50it/s]Extractor Predicting: 456it [04:57,  1.48it/s]Extractor Predicting: 457it [04:57,  1.47it/s]Extractor Predicting: 458it [04:58,  1.47it/s]Extractor Predicting: 459it [04:59,  1.49it/s]Extractor Predicting: 460it [04:59,  1.50it/s]Extractor Predicting: 461it [05:00,  1.50it/s]Extractor Predicting: 462it [05:01,  1.46it/s]Extractor Predicting: 463it [05:01,  1.49it/s]Extractor Predicting: 464it [05:02,  1.47it/s]Extractor Predicting: 465it [05:03,  1.49it/s]Extractor Predicting: 466it [05:03,  1.49it/s]Extractor Predicting: 467it [05:04,  1.53it/s]Extractor Predicting: 468it [05:05,  1.48it/s]Extractor Predicting: 469it [05:06,  1.46it/s]Extractor Predicting: 470it [05:06,  1.47it/s]Extractor Predicting: 471it [05:07,  1.47it/s]Extractor Predicting: 472it [05:08,  1.45it/s]Extractor Predicting: 473it [05:08,  1.45it/s]Extractor Predicting: 474it [05:09,  1.46it/s]Extractor Predicting: 475it [05:10,  1.46it/s]Extractor Predicting: 476it [05:10,  1.45it/s]Extractor Predicting: 477it [05:11,  1.45it/s]Extractor Predicting: 478it [05:12,  1.47it/s]Extractor Predicting: 479it [05:12,  1.46it/s]Extractor Predicting: 480it [05:13,  1.48it/s]Extractor Predicting: 481it [05:14,  1.48it/s]Extractor Predicting: 482it [05:14,  1.47it/s]Extractor Predicting: 483it [05:15,  1.45it/s]Extractor Predicting: 484it [05:16,  1.43it/s]Extractor Predicting: 485it [05:17,  1.45it/s]Extractor Predicting: 486it [05:17,  1.48it/s]Extractor Predicting: 487it [05:18,  1.48it/s]Extractor Predicting: 488it [05:19,  1.48it/s]Extractor Predicting: 489it [05:19,  1.45it/s]Extractor Predicting: 490it [05:20,  1.48it/s]Extractor Predicting: 491it [05:21,  1.47it/s]Extractor Predicting: 492it [05:21,  1.43it/s]Extractor Predicting: 493it [05:22,  1.43it/s]Extractor Predicting: 494it [05:23,  1.48it/s]Extractor Predicting: 495it [05:23,  1.45it/s]Extractor Predicting: 496it [05:24,  1.46it/s]Extractor Predicting: 497it [05:25,  1.46it/s]Extractor Predicting: 498it [05:25,  1.50it/s]Extractor Predicting: 499it [05:26,  1.50it/s]Extractor Predicting: 500it [05:27,  1.47it/s]Extractor Predicting: 501it [05:27,  1.46it/s]Extractor Predicting: 502it [05:28,  1.44it/s]Extractor Predicting: 503it [05:29,  1.44it/s]Extractor Predicting: 504it [05:30,  1.45it/s]Extractor Predicting: 505it [05:30,  1.49it/s]Extractor Predicting: 506it [05:31,  1.46it/s]Extractor Predicting: 507it [05:31,  1.49it/s]Extractor Predicting: 508it [05:32,  1.50it/s]Extractor Predicting: 509it [05:33,  1.52it/s]Extractor Predicting: 510it [05:33,  1.55it/s]Extractor Predicting: 511it [05:34,  1.53it/s]Extractor Predicting: 512it [05:35,  1.53it/s]Extractor Predicting: 513it [05:35,  1.50it/s]Extractor Predicting: 514it [05:36,  1.51it/s]Extractor Predicting: 515it [05:37,  1.51it/s]Extractor Predicting: 516it [05:37,  1.48it/s]Extractor Predicting: 517it [05:38,  1.43it/s]Extractor Predicting: 518it [05:39,  1.43it/s]Extractor Predicting: 519it [05:40,  1.46it/s]Extractor Predicting: 520it [05:40,  1.52it/s]Extractor Predicting: 521it [05:41,  1.52it/s]Extractor Predicting: 522it [05:41,  1.51it/s]Extractor Predicting: 523it [05:42,  1.50it/s]Extractor Predicting: 524it [05:43,  1.49it/s]Extractor Predicting: 525it [05:43,  1.50it/s]Extractor Predicting: 526it [05:44,  1.45it/s]Extractor Predicting: 527it [05:45,  1.46it/s]Extractor Predicting: 528it [05:46,  1.48it/s]Extractor Predicting: 529it [05:46,  1.47it/s]Extractor Predicting: 530it [05:47,  1.47it/s]Extractor Predicting: 531it [05:48,  1.49it/s]Extractor Predicting: 532it [05:48,  1.44it/s]Extractor Predicting: 533it [05:49,  1.45it/s]Extractor Predicting: 534it [05:50,  1.46it/s]Extractor Predicting: 535it [05:50,  1.49it/s]Extractor Predicting: 536it [05:51,  1.46it/s]Extractor Predicting: 537it [05:52,  1.47it/s]Extractor Predicting: 538it [05:52,  1.46it/s]Extractor Predicting: 539it [05:53,  1.46it/s]Extractor Predicting: 540it [05:54,  1.46it/s]Extractor Predicting: 541it [05:55,  1.38it/s]Extractor Predicting: 542it [05:55,  1.39it/s]Extractor Predicting: 543it [05:56,  1.41it/s]Extractor Predicting: 544it [05:57,  1.44it/s]Extractor Predicting: 545it [05:57,  1.42it/s]Extractor Predicting: 546it [05:58,  1.43it/s]Extractor Predicting: 547it [05:59,  1.43it/s]Extractor Predicting: 548it [05:59,  1.46it/s]Extractor Predicting: 549it [06:00,  1.42it/s]Extractor Predicting: 550it [06:01,  1.43it/s]Extractor Predicting: 551it [06:02,  1.44it/s]Extractor Predicting: 552it [06:02,  1.44it/s]Extractor Predicting: 553it [06:03,  1.47it/s]Extractor Predicting: 554it [06:04,  1.47it/s]Extractor Predicting: 555it [06:04,  1.46it/s]Extractor Predicting: 556it [06:05,  1.47it/s]Extractor Predicting: 557it [06:06,  1.45it/s]Extractor Predicting: 558it [06:06,  1.44it/s]Extractor Predicting: 559it [06:07,  1.46it/s]Extractor Predicting: 560it [06:08,  1.28it/s]Extractor Predicting: 561it [06:09,  1.32it/s]Extractor Predicting: 562it [06:09,  1.38it/s]Extractor Predicting: 563it [06:10,  1.40it/s]Extractor Predicting: 564it [06:11,  1.40it/s]Extractor Predicting: 565it [06:11,  1.41it/s]Extractor Predicting: 566it [06:12,  1.42it/s]Extractor Predicting: 567it [06:13,  1.44it/s]Extractor Predicting: 568it [06:13,  1.50it/s]Extractor Predicting: 569it [06:14,  1.48it/s]Extractor Predicting: 570it [06:15,  1.45it/s]Extractor Predicting: 571it [06:16,  1.45it/s]Extractor Predicting: 572it [06:16,  1.42it/s]Extractor Predicting: 573it [06:17,  1.39it/s]Extractor Predicting: 574it [06:18,  1.39it/s]Extractor Predicting: 575it [06:18,  1.41it/s]Extractor Predicting: 576it [06:19,  1.43it/s]Extractor Predicting: 577it [06:19,  1.75it/s]Extractor Predicting: 577it [06:19,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:45,836 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:45,841 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:45,841 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:45,841 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:45,841 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:50:46,450 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:50:46,451 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:50:47,030 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:50:48,083 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:50:48,083 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:50,922 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:50,924 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:50,924 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:50,925 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:50:50,925 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:50:51,562 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:50:51,563 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:50:52,269 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:50:52,431 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:50:52,431 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.25755324418028724,
  "recall": 0.03760485970494649,
  "score": 0.06562756357670221,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'employer', 'field of work', 'languages spoken, written or signed', 'lyrics by', 'manufacturer', 'member of sports team', 'occupation', 'performer', 'residence', 'shares border with', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 11382
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11482, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:01,  1.50it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.63it/s]Extractor Predicting: 11it [00:07,  1.59it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:10,  1.51it/s]Extractor Predicting: 18it [00:11,  1.47it/s]Extractor Predicting: 19it [00:12,  1.48it/s]Extractor Predicting: 20it [00:13,  1.45it/s]Extractor Predicting: 21it [00:13,  1.47it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.47it/s]Extractor Predicting: 24it [00:15,  1.46it/s]Extractor Predicting: 25it [00:16,  1.43it/s]Extractor Predicting: 26it [00:17,  1.45it/s]Extractor Predicting: 27it [00:17,  1.44it/s]Extractor Predicting: 28it [00:18,  1.44it/s]Extractor Predicting: 29it [00:19,  1.42it/s]Extractor Predicting: 30it [00:20,  1.41it/s]Extractor Predicting: 31it [00:20,  1.47it/s]Extractor Predicting: 32it [00:21,  1.49it/s]Extractor Predicting: 33it [00:22,  1.47it/s]Extractor Predicting: 34it [00:22,  1.46it/s]Extractor Predicting: 35it [00:23,  1.50it/s]Extractor Predicting: 36it [00:23,  1.51it/s]Extractor Predicting: 37it [00:24,  1.50it/s]Extractor Predicting: 38it [00:25,  1.45it/s]Extractor Predicting: 39it [00:26,  1.41it/s]Extractor Predicting: 40it [00:26,  1.39it/s]Extractor Predicting: 41it [00:27,  1.40it/s]Extractor Predicting: 42it [00:28,  1.41it/s]Extractor Predicting: 43it [00:29,  1.41it/s]Extractor Predicting: 44it [00:29,  1.35it/s]Extractor Predicting: 45it [00:30,  1.37it/s]Extractor Predicting: 46it [00:31,  1.41it/s]Extractor Predicting: 47it [00:31,  1.44it/s]Extractor Predicting: 48it [00:32,  1.44it/s]Extractor Predicting: 49it [00:33,  1.45it/s]Extractor Predicting: 50it [00:33,  1.44it/s]Extractor Predicting: 51it [00:34,  1.46it/s]Extractor Predicting: 52it [00:35,  1.43it/s]Extractor Predicting: 53it [00:36,  1.44it/s]Extractor Predicting: 54it [00:36,  1.48it/s]Extractor Predicting: 55it [00:37,  1.49it/s]Extractor Predicting: 56it [00:37,  1.51it/s]Extractor Predicting: 57it [00:38,  1.50it/s]Extractor Predicting: 58it [00:39,  1.53it/s]Extractor Predicting: 59it [00:39,  1.52it/s]Extractor Predicting: 60it [00:40,  1.48it/s]Extractor Predicting: 61it [00:41,  1.51it/s]Extractor Predicting: 62it [00:41,  1.49it/s]Extractor Predicting: 63it [00:42,  1.48it/s]Extractor Predicting: 64it [00:43,  1.42it/s]Extractor Predicting: 65it [00:44,  1.43it/s]Extractor Predicting: 66it [00:44,  1.44it/s]Extractor Predicting: 67it [00:45,  1.45it/s]Extractor Predicting: 68it [00:46,  1.46it/s]Extractor Predicting: 69it [00:46,  1.44it/s]Extractor Predicting: 70it [00:47,  1.45it/s]Extractor Predicting: 71it [00:48,  1.46it/s]Extractor Predicting: 72it [00:48,  1.48it/s]Extractor Predicting: 73it [00:49,  1.50it/s]Extractor Predicting: 74it [00:50,  1.54it/s]Extractor Predicting: 75it [00:50,  1.53it/s]Extractor Predicting: 76it [00:51,  1.53it/s]Extractor Predicting: 77it [00:52,  1.54it/s]Extractor Predicting: 78it [00:52,  1.53it/s]Extractor Predicting: 79it [00:53,  1.56it/s]Extractor Predicting: 80it [00:53,  1.57it/s]Extractor Predicting: 81it [00:54,  1.58it/s]Extractor Predicting: 82it [00:55,  1.58it/s]Extractor Predicting: 83it [00:55,  1.56it/s]Extractor Predicting: 84it [00:56,  1.51it/s]Extractor Predicting: 85it [00:57,  1.55it/s]Extractor Predicting: 86it [00:57,  1.53it/s]Extractor Predicting: 87it [00:58,  1.49it/s]Extractor Predicting: 88it [00:59,  1.48it/s]Extractor Predicting: 89it [01:00,  1.31it/s]Extractor Predicting: 90it [01:00,  1.37it/s]Extractor Predicting: 91it [01:01,  1.37it/s]Extractor Predicting: 92it [01:02,  1.38it/s]Extractor Predicting: 93it [01:03,  1.39it/s]Extractor Predicting: 94it [01:03,  1.39it/s]Extractor Predicting: 95it [01:04,  1.38it/s]Extractor Predicting: 96it [01:05,  1.37it/s]Extractor Predicting: 97it [01:05,  1.40it/s]Extractor Predicting: 98it [01:06,  1.40it/s]Extractor Predicting: 99it [01:07,  1.40it/s]Extractor Predicting: 100it [01:07,  1.44it/s]Extractor Predicting: 101it [01:08,  1.44it/s]Extractor Predicting: 102it [01:09,  1.43it/s]Extractor Predicting: 103it [01:10,  1.40it/s]Extractor Predicting: 104it [01:10,  1.41it/s]Extractor Predicting: 105it [01:11,  1.43it/s]Extractor Predicting: 106it [01:12,  1.44it/s]Extractor Predicting: 107it [01:12,  1.41it/s]Extractor Predicting: 108it [01:13,  1.38it/s]Extractor Predicting: 109it [01:14,  1.36it/s]Extractor Predicting: 110it [01:15,  1.37it/s]Extractor Predicting: 110it [01:15,  1.46it/s]
[INFO|configuration_utils.py:515] 2023-08-29 08:52:09,334 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:52:09,335 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 08:52:09,337 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:52:09,338 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 08:52:09,339 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 08:52:12,379 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 08:52:12,379 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 08:52:12,390 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:52:12,391 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 08:52:12,401 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:52:12,406 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:52:12,406 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:52:12,406 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:52:12,406 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:52:12,406 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:52:12,406 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4532710280373832,
  "recall": 0.015698333063602524,
  "score": 0.03034569059909276,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 08:52:12,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:13,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:14,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:14,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:15,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:16,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:16,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:17,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:18,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:19,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:20,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:20,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:21,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:22,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:22,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:23,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:24,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:25,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:25,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:26,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:27,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:28,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:28,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:16<05:22, 16.95s/it][WARNING|generation_utils.py:914] 2023-08-29 08:52:29,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:30,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:31,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:31,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:32,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:32,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:33,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:34,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:34,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:35,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:36,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:36,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:37,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:38,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:38,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:39,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:40,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:40,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:41,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:41,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:42,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:43,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:31<04:41, 15.66s/it][WARNING|generation_utils.py:914] 2023-08-29 08:52:44,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:45,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:45,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:46,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:47,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:48,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:49,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:49,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:50,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:51,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:52,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:52,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:53,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:54,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:55,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:55,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:56,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:57,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:58,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:58,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:59,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:00,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:48<04:34, 16.14s/it][WARNING|generation_utils.py:914] 2023-08-29 08:53:01,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:01,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:02,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:02,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:03,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:04,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:04,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:05,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:06,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:06,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:07,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:07,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:08,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:09,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:09,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:10,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:11,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:11,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:12,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:12,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:00<03:55, 14.70s/it][WARNING|generation_utils.py:914] 2023-08-29 08:53:13,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:14,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:15,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:15,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:16,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:17,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:17,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:18,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:19,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:20,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:20,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:21,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:22,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:23,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:23,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:24,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:25,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:25,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:26,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:27,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:27,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:28,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:29,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:30,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:30,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:18<03:57, 15.82s/it][WARNING|generation_utils.py:914] 2023-08-29 08:53:31,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:32,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:32,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:33,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:34,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:35,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:36,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:36,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:37,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:38,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:39,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:39,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:40,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:41,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:42,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:43,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:43,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:44,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:45,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:45,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:46,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:35<03:44, 16.07s/it][WARNING|generation_utils.py:914] 2023-08-29 08:53:47,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:48,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:49,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:50,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:50,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:51,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:52,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:52,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:53,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:54,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:54,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:55,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:56,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:56,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:57,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:58,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:59,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:59,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:00,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:01,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:49<03:20, 15.44s/it][WARNING|generation_utils.py:914] 2023-08-29 08:54:02,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:02,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:03,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:04,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:04,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:05,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:06,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:06,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:07,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:08,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:09,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:09,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:10,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:11,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:11,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:12,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:13,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:14,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:14,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:15,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:16,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:04<03:02, 15.21s/it][WARNING|generation_utils.py:914] 2023-08-29 08:54:16,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:17,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:18,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:18,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:19,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:20,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:20,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:21,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:22,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:22,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:23,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:24,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:24,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:25,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:26,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:26,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:27,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:28,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:28,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:29,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:17<02:40, 14.63s/it][WARNING|generation_utils.py:914] 2023-08-29 08:54:30,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:30,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:31,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:32,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:33,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:33,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:34,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:35,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:36,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:36,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:37,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:38,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:39,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:39,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:40,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:41,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:42,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:42,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:43,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:44,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:44,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:45,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:33<02:30, 15.08s/it][WARNING|generation_utils.py:914] 2023-08-29 08:54:46,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:47,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:47,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:48,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:49,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:49,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:50,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:51,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:52,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:52,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:53,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:54,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:55,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:55,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:56,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:57,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:57,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:58,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:59,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:54:59,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:00,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:01,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:02,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:50<02:19, 15.55s/it][WARNING|generation_utils.py:914] 2023-08-29 08:55:02,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:03,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:04,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:05,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:05,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:06,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:07,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:08,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:08,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:09,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:10,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:10,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:11,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:12,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:13,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:13,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:14,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:15,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:15,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:16,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:17,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:05<02:03, 15.45s/it][WARNING|generation_utils.py:914] 2023-08-29 08:55:18,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:18,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:19,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:19,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:20,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:21,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:21,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:22,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:23,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:23,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:24,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:25,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:26,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:26,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:27,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:28,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:28,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:29,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:29,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:30,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:19<01:44, 14.94s/it][WARNING|generation_utils.py:914] 2023-08-29 08:55:31,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:32,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:33,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:33,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:34,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:34,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:35,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:36,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:36,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:37,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:37,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:38,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:39,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:39,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:40,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:40,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:41,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:42,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:42,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:30<01:23, 13.88s/it][WARNING|generation_utils.py:914] 2023-08-29 08:55:43,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:43,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:44,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:45,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:46,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:46,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:47,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:48,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:49,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:49,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:50,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:51,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:51,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:52,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:53,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:54,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:54,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:55,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:56,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:57,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:57,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:58,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:55:59,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:47<01:13, 14.75s/it][WARNING|generation_utils.py:914] 2023-08-29 08:56:00,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:00,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:01,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:02,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:02,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:03,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:04,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:04,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:05,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:06,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:06,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:07,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:08,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:08,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:09,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:10,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:11,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:11,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:12,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:13,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:01<00:57, 14.46s/it][WARNING|generation_utils.py:914] 2023-08-29 08:56:13,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:14,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:15,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:15,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:16,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:17,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:18,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:18,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:19,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:20,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:20,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:21,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:22,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:23,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:23,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:24,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:25,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:25,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:26,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:27,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:27,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:16<00:43, 14.57s/it][WARNING|generation_utils.py:914] 2023-08-29 08:56:28,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:29,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:29,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:30,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:31,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:32,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:32,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:33,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:34,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:34,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:35,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:35,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:36,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:37,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:37,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:38,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:39,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:40,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:40,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:41,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:42,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:42,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:30<00:29, 14.59s/it][WARNING|generation_utils.py:914] 2023-08-29 08:56:43,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:43,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:44,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:45,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:45,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:46,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:46,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:47,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:48,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:48,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:49,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:49,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:50,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:51,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:51,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:52,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:52,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:53,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:54,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:41<00:13, 13.60s/it][WARNING|generation_utils.py:914] 2023-08-29 08:56:54,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:55,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:56,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:56,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:57,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:58,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:59,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:56:59,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:00,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:01,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:02,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:03,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:03,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:05,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:05,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:06,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:07,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:08,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:08,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:09,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:10,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:57:11,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:59<00:00, 14.69s/it]Generating: 100%|██████████| 20/20 [04:59<00:00, 14.96s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:57:17,396 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:57:17,405 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:57:17,405 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:57:17,405 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:57:17,405 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:57:17,717 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:57:17,718 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:57:17,989 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:57:19,057 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:57:19,057 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:57:20,358 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:57:20,360 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:57:20,360 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:57:20,360 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:57:20,360 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:57:21,105 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:57:21,109 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:57:21,368 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:57:21,530 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:57:21,530 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : conflict .', 'success_rate': 0.8165760869565217, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : developer .', 'success_rate': 0.8565340909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : given name .', 'success_rate': 0.859375, 'errors': {'', "('I', 'given name', '', 'She was of the family of the ancient Greek kings I ( Λαὶ ἐγγγάωνης ) and Αἱρετες ( Βὶ ρυρών ) , all the names of Heracles .')", 'not enough values to unpack (expected 2, got 1)', "('officer', 'given name', '', 'On September 9 , 1811 , as part of his third contract with the Navy , he was appointed a brigadier general in the Army of America , an officer of distinction in the United States Navy .')"}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 403, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 558, 'raw': 576}
{'target': 600, 'success': 589, 'raw': 608}
{'target': 600, 'success': 621, 'raw': 640}
{'prompt': 'Relation : participant in .', 'success_rate': 0.9703125, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 395, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 487, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 609, 'raw': 800}
{'prompt': 'Relation : work location .', 'success_rate': 0.76125, 'errors': {''}}
['Relation : composer . Context : Later in 2003 , he composed the score for the film The Phantom of the Opera at the London Academy of Music . Head Entity : The Phantom of the Opera , Tail Entity : John Geddes .\n']
['Relation : composer . Context : Later in 2003 , he composed the score for the film The Phantom of the Opera at the London Academy of Music . Head Entity : The Phantom of the Opera , Tail Entity : John Geddes .\n', 'Relation : composer . Context : He also composed two symphonies of his own , The Clouds Are Falling and The Water is Spilling . Head Entity : The Clouds Are Falling , Tail Entity : composer .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9315476190476191, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 312, 'raw': 320}
{'target': 600, 'success': 344, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 494, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 620, 'raw': 640}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.96875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : creator .', 'success_rate': 0.9226190476190477, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : employer .', 'success_rate': 0.9390625, 'errors': {''}}
['Relation : field of work . Context : Later in Life he studied at the Conservatoire de Paris and at other centres at different times , including at the Museum of Modern Art in Paris . Head Entity : Museum of Modern Art , Tail Entity : Contemporary Art .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8792613636363636, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.8369565217391305, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9609375, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 222, 'raw': 224}
{'target': 600, 'success': 253, 'raw': 256}
{'target': 600, 'success': 284, 'raw': 288}
{'target': 600, 'success': 316, 'raw': 320}
{'target': 600, 'success': 348, 'raw': 352}
{'target': 600, 'success': 379, 'raw': 384}
{'target': 600, 'success': 411, 'raw': 416}
{'target': 600, 'success': 443, 'raw': 448}
{'target': 600, 'success': 475, 'raw': 480}
{'target': 600, 'success': 507, 'raw': 512}
{'target': 600, 'success': 539, 'raw': 544}
{'target': 600, 'success': 570, 'raw': 576}
{'target': 600, 'success': 602, 'raw': 608}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.9901315789473685, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8464673913043478, 'errors': {'', "('Democrat', 'occupation', '', 'He was elected in 1983 as a Democrat to the United States House , the seat that would become United States Senate in 1986 .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 467, 'raw': 480}
{'target': 600, 'success': 499, 'raw': 512}
{'target': 600, 'success': 530, 'raw': 544}
{'target': 600, 'success': 561, 'raw': 576}
{'target': 600, 'success': 592, 'raw': 608}
{'target': 600, 'success': 624, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.975, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : residence .', 'success_rate': 0.8973214285714286, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.8721590909090909, 'errors': {'', "('shares', 'shares border with', '', 'Its shares also have traded at above $ 22 . 25 twice in the past 3 days , at $ 15 .')", "('shares', 'shares border with', '', 'The shares , traded at $ 20 , are traded at various levels in various major financial and business media and have over 7,000 shares traded in that market .')", 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 222, 'raw': 224}
{'target': 600, 'success': 254, 'raw': 256}
{'target': 600, 'success': 286, 'raw': 288}
{'target': 600, 'success': 318, 'raw': 320}
{'target': 600, 'success': 350, 'raw': 352}
{'target': 600, 'success': 382, 'raw': 384}
{'target': 600, 'success': 414, 'raw': 416}
{'target': 600, 'success': 445, 'raw': 448}
{'target': 600, 'success': 477, 'raw': 480}
{'target': 600, 'success': 509, 'raw': 512}
{'target': 600, 'success': 541, 'raw': 544}
{'target': 600, 'success': 573, 'raw': 576}
{'target': 600, 'success': 605, 'raw': 608}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9950657894736842, 'errors': {''}}
['Relation : twinned administrative body . Context : Later in the year ( 1177 88 ) , a new administrative unit , the Municipality of Istria was established to rule over the territory . Head Entity : municipality of Istria , Tail Entity : Istria .\n']
['Relation : twinned administrative body . Context : Later in the year ( 1177 88 ) , a new administrative unit , the Municipality of Istria was established to rule over the territory . Head Entity : municipality of Istria , Tail Entity : Istria .\n', 'Relation : twinned administrative body . Context : After the death of Emperor Heng of Han ( 963 979 ) in 974 , his capital remained in the Empire . Head Entity : Imperial , Tail Entity : Empire .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/3_ext.jsonl'}}
estimate vocab size: 11138
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11238, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.28it/s]Extractor Estimating: 2it [00:01,  1.43it/s]Extractor Estimating: 3it [00:02,  1.50it/s]Extractor Estimating: 4it [00:02,  1.59it/s]Extractor Estimating: 5it [00:03,  1.57it/s]Extractor Estimating: 6it [00:03,  1.63it/s]Extractor Estimating: 7it [00:04,  1.62it/s]Extractor Estimating: 8it [00:05,  1.61it/s]Extractor Estimating: 9it [00:05,  1.58it/s]Extractor Estimating: 10it [00:06,  1.63it/s]Extractor Estimating: 11it [00:06,  1.62it/s]Extractor Estimating: 12it [00:07,  1.60it/s]Extractor Estimating: 13it [00:08,  1.60it/s]Extractor Estimating: 14it [00:08,  1.59it/s]Extractor Estimating: 15it [00:09,  1.61it/s]Extractor Estimating: 16it [00:10,  1.59it/s]Extractor Estimating: 17it [00:10,  1.60it/s]Extractor Estimating: 18it [00:11,  1.62it/s]Extractor Estimating: 19it [00:11,  1.63it/s]Extractor Estimating: 20it [00:12,  1.66it/s]Extractor Estimating: 21it [00:13,  1.62it/s]Extractor Estimating: 22it [00:13,  1.62it/s]Extractor Estimating: 23it [00:14,  1.64it/s]Extractor Estimating: 24it [00:14,  1.64it/s]Extractor Estimating: 25it [00:15,  1.59it/s]Extractor Estimating: 26it [00:16,  1.64it/s]Extractor Estimating: 27it [00:16,  1.62it/s]Extractor Estimating: 28it [00:17,  1.68it/s]Extractor Estimating: 29it [00:17,  1.73it/s]Extractor Estimating: 30it [00:18,  1.71it/s]Extractor Estimating: 31it [00:19,  1.76it/s]Extractor Estimating: 32it [00:19,  1.79it/s]Extractor Estimating: 33it [00:20,  1.79it/s]Extractor Estimating: 34it [00:20,  1.79it/s]Extractor Estimating: 35it [00:21,  1.79it/s]Extractor Estimating: 36it [00:21,  1.75it/s]Extractor Estimating: 37it [00:22,  1.72it/s]Extractor Estimating: 38it [00:23,  1.73it/s]Extractor Estimating: 39it [00:23,  1.75it/s]Extractor Estimating: 40it [00:24,  1.71it/s]Extractor Estimating: 41it [00:24,  1.75it/s]Extractor Estimating: 42it [00:25,  1.70it/s]Extractor Estimating: 43it [00:25,  1.77it/s]Extractor Estimating: 44it [00:26,  1.75it/s]Extractor Estimating: 45it [00:27,  1.71it/s]Extractor Estimating: 46it [00:27,  1.70it/s]Extractor Estimating: 47it [00:28,  1.70it/s]Extractor Estimating: 48it [00:28,  1.65it/s]Extractor Estimating: 49it [00:29,  1.69it/s]Extractor Estimating: 50it [00:30,  1.71it/s]Extractor Estimating: 51it [00:30,  1.62it/s]Extractor Estimating: 52it [00:31,  1.59it/s]Extractor Estimating: 53it [00:32,  1.57it/s]Extractor Estimating: 54it [00:32,  1.52it/s]Extractor Estimating: 55it [00:33,  1.53it/s]Extractor Estimating: 56it [00:34,  1.54it/s]Extractor Estimating: 57it [00:34,  1.51it/s]Extractor Estimating: 58it [00:35,  1.50it/s]Extractor Estimating: 59it [00:36,  1.53it/s]Extractor Estimating: 60it [00:36,  1.51it/s]Extractor Estimating: 61it [00:37,  1.53it/s]Extractor Estimating: 62it [00:38,  1.48it/s]Extractor Estimating: 63it [00:38,  1.49it/s]Extractor Estimating: 64it [00:39,  1.48it/s]Extractor Estimating: 65it [00:40,  1.50it/s]Extractor Estimating: 66it [00:40,  1.55it/s]Extractor Estimating: 67it [00:41,  1.56it/s]Extractor Estimating: 68it [00:41,  1.54it/s]Extractor Estimating: 69it [00:42,  1.46it/s]Extractor Estimating: 70it [00:43,  1.50it/s]Extractor Estimating: 71it [00:44,  1.50it/s]Extractor Estimating: 72it [00:44,  1.56it/s]Extractor Estimating: 73it [00:45,  1.58it/s]Extractor Estimating: 74it [00:45,  1.57it/s]Extractor Estimating: 75it [00:46,  1.59it/s]Extractor Estimating: 76it [00:47,  1.69it/s]Extractor Estimating: 77it [00:47,  1.73it/s]Extractor Estimating: 78it [00:48,  1.74it/s]Extractor Estimating: 79it [00:48,  1.78it/s]Extractor Estimating: 80it [00:49,  1.84it/s]Extractor Estimating: 81it [00:49,  1.84it/s]Extractor Estimating: 82it [00:50,  1.87it/s]Extractor Estimating: 83it [00:50,  1.84it/s]Extractor Estimating: 84it [00:51,  1.88it/s]Extractor Estimating: 85it [00:51,  1.90it/s]Extractor Estimating: 86it [00:52,  1.85it/s]Extractor Estimating: 87it [00:52,  1.84it/s]Extractor Estimating: 88it [00:53,  1.88it/s]Extractor Estimating: 89it [00:53,  1.91it/s]Extractor Estimating: 90it [00:54,  1.88it/s]Extractor Estimating: 91it [00:54,  1.92it/s]Extractor Estimating: 92it [00:55,  1.92it/s]Extractor Estimating: 93it [00:56,  1.88it/s]Extractor Estimating: 94it [00:56,  1.88it/s]Extractor Estimating: 95it [00:57,  1.88it/s]Extractor Estimating: 96it [00:57,  1.91it/s]Extractor Estimating: 97it [00:58,  1.82it/s]Extractor Estimating: 98it [00:58,  1.87it/s]Extractor Estimating: 99it [00:59,  1.88it/s]Extractor Estimating: 100it [00:59,  1.87it/s]Extractor Estimating: 101it [01:00,  1.74it/s]Extractor Estimating: 102it [01:01,  1.68it/s]Extractor Estimating: 103it [01:01,  1.66it/s]Extractor Estimating: 104it [01:02,  1.65it/s]Extractor Estimating: 105it [01:02,  1.64it/s]Extractor Estimating: 106it [01:03,  1.60it/s]Extractor Estimating: 107it [01:04,  1.59it/s]Extractor Estimating: 108it [01:04,  1.57it/s]Extractor Estimating: 109it [01:05,  1.55it/s]Extractor Estimating: 110it [01:06,  1.56it/s]Extractor Estimating: 111it [01:06,  1.60it/s]Extractor Estimating: 112it [01:07,  1.44it/s]Extractor Estimating: 113it [01:08,  1.47it/s]Extractor Estimating: 114it [01:08,  1.50it/s]Extractor Estimating: 115it [01:09,  1.52it/s]Extractor Estimating: 116it [01:10,  1.56it/s]Extractor Estimating: 117it [01:10,  1.55it/s]Extractor Estimating: 118it [01:11,  1.55it/s]Extractor Estimating: 119it [01:12,  1.54it/s]Extractor Estimating: 120it [01:12,  1.47it/s]Extractor Estimating: 121it [01:13,  1.50it/s]Extractor Estimating: 122it [01:14,  1.48it/s]Extractor Estimating: 123it [01:14,  1.52it/s]Extractor Estimating: 124it [01:15,  1.55it/s]Extractor Estimating: 125it [01:16,  1.56it/s]Extractor Estimating: 126it [01:16,  1.53it/s]Extractor Estimating: 127it [01:17,  1.58it/s]Extractor Estimating: 128it [01:17,  1.61it/s]Extractor Estimating: 129it [01:18,  1.61it/s]Extractor Estimating: 130it [01:19,  1.60it/s]Extractor Estimating: 131it [01:19,  1.62it/s]Extractor Estimating: 132it [01:20,  1.62it/s]Extractor Estimating: 133it [01:21,  1.59it/s]Extractor Estimating: 134it [01:21,  1.61it/s]Extractor Estimating: 135it [01:22,  1.62it/s]Extractor Estimating: 136it [01:22,  1.62it/s]Extractor Estimating: 137it [01:23,  1.59it/s]Extractor Estimating: 138it [01:24,  1.59it/s]Extractor Estimating: 139it [01:24,  1.57it/s]Extractor Estimating: 140it [01:25,  1.59it/s]Extractor Estimating: 141it [01:26,  1.60it/s]Extractor Estimating: 142it [01:26,  1.59it/s]Extractor Estimating: 143it [01:27,  1.59it/s]Extractor Estimating: 144it [01:27,  1.58it/s]Extractor Estimating: 145it [01:28,  1.61it/s]Extractor Estimating: 146it [01:29,  1.60it/s]Extractor Estimating: 147it [01:29,  1.63it/s]Extractor Estimating: 148it [01:30,  1.61it/s]Extractor Estimating: 149it [01:31,  1.62it/s]Extractor Estimating: 150it [01:31,  1.58it/s]Extractor Estimating: 151it [01:32,  1.61it/s]Extractor Estimating: 152it [01:32,  1.63it/s]Extractor Estimating: 153it [01:33,  1.57it/s]Extractor Estimating: 154it [01:34,  1.59it/s]Extractor Estimating: 155it [01:34,  1.54it/s]Extractor Estimating: 156it [01:35,  1.53it/s]Extractor Estimating: 157it [01:36,  1.53it/s]Extractor Estimating: 158it [01:36,  1.55it/s]Extractor Estimating: 159it [01:37,  1.56it/s]Extractor Estimating: 160it [01:38,  1.56it/s]Extractor Estimating: 161it [01:38,  1.55it/s]Extractor Estimating: 162it [01:39,  1.58it/s]Extractor Estimating: 163it [01:40,  1.56it/s]Extractor Estimating: 164it [01:40,  1.57it/s]Extractor Estimating: 165it [01:41,  1.56it/s]Extractor Estimating: 166it [01:41,  1.52it/s]Extractor Estimating: 167it [01:42,  1.49it/s]Extractor Estimating: 168it [01:43,  1.50it/s]Extractor Estimating: 169it [01:43,  1.53it/s]Extractor Estimating: 170it [01:44,  1.53it/s]Extractor Estimating: 171it [01:45,  1.48it/s]Extractor Estimating: 172it [01:45,  1.51it/s]Extractor Estimating: 173it [01:46,  1.48it/s]Extractor Estimating: 174it [01:47,  1.47it/s]Extractor Estimating: 175it [01:48,  1.49it/s]Extractor Estimating: 176it [01:48,  1.52it/s]Extractor Estimating: 177it [01:49,  1.55it/s]Extractor Estimating: 178it [01:49,  1.60it/s]Extractor Estimating: 179it [01:50,  1.62it/s]Extractor Estimating: 180it [01:51,  1.65it/s]Extractor Estimating: 181it [01:51,  1.66it/s]Extractor Estimating: 182it [01:52,  1.66it/s]Extractor Estimating: 183it [01:52,  1.60it/s]Extractor Estimating: 184it [01:53,  1.62it/s]Extractor Estimating: 185it [01:54,  1.63it/s]Extractor Estimating: 186it [01:54,  1.61it/s]Extractor Estimating: 187it [01:55,  1.62it/s]Extractor Estimating: 188it [01:56,  1.59it/s]Extractor Estimating: 189it [01:56,  1.55it/s]Extractor Estimating: 190it [01:57,  1.60it/s]Extractor Estimating: 191it [01:57,  1.62it/s]Extractor Estimating: 192it [01:58,  1.65it/s]Extractor Estimating: 193it [01:59,  1.60it/s]Extractor Estimating: 194it [01:59,  1.46it/s]Extractor Estimating: 195it [02:00,  1.50it/s]Extractor Estimating: 196it [02:01,  1.52it/s]Extractor Estimating: 197it [02:01,  1.57it/s]Extractor Estimating: 198it [02:02,  1.56it/s]Extractor Estimating: 199it [02:03,  1.60it/s]Extractor Estimating: 200it [02:03,  1.59it/s]Extractor Estimating: 201it [02:04,  1.60it/s]Extractor Estimating: 202it [02:04,  1.62it/s]Extractor Estimating: 203it [02:05,  1.61it/s]Extractor Estimating: 204it [02:06,  1.60it/s]Extractor Estimating: 205it [02:06,  1.60it/s]Extractor Estimating: 206it [02:07,  1.65it/s]Extractor Estimating: 207it [02:07,  1.65it/s]Extractor Estimating: 208it [02:08,  1.67it/s]Extractor Estimating: 209it [02:09,  1.69it/s]Extractor Estimating: 210it [02:09,  1.73it/s]Extractor Estimating: 211it [02:10,  1.71it/s]Extractor Estimating: 212it [02:10,  1.73it/s]Extractor Estimating: 213it [02:11,  1.68it/s]Extractor Estimating: 214it [02:12,  1.71it/s]Extractor Estimating: 215it [02:12,  1.69it/s]Extractor Estimating: 216it [02:13,  1.70it/s]Extractor Estimating: 217it [02:13,  1.72it/s]Extractor Estimating: 218it [02:14,  1.71it/s]Extractor Estimating: 219it [02:14,  1.72it/s]Extractor Estimating: 220it [02:15,  1.67it/s]Extractor Estimating: 221it [02:16,  1.72it/s]Extractor Estimating: 222it [02:16,  1.68it/s]Extractor Estimating: 223it [02:17,  1.67it/s]Extractor Estimating: 224it [02:17,  1.64it/s]Extractor Estimating: 225it [02:18,  1.62it/s]Extractor Estimating: 226it [02:19,  1.61it/s]Extractor Estimating: 227it [02:19,  1.65it/s]Extractor Estimating: 228it [02:20,  1.68it/s]Extractor Estimating: 229it [02:21,  1.64it/s]Extractor Estimating: 230it [02:21,  1.60it/s]Extractor Estimating: 231it [02:22,  1.67it/s]Extractor Estimating: 232it [02:22,  1.61it/s]Extractor Estimating: 233it [02:23,  1.67it/s]Extractor Estimating: 234it [02:24,  1.69it/s]Extractor Estimating: 235it [02:24,  1.63it/s]Extractor Estimating: 236it [02:25,  1.61it/s]Extractor Estimating: 237it [02:25,  1.59it/s]Extractor Estimating: 238it [02:26,  1.59it/s]Extractor Estimating: 239it [02:27,  1.55it/s]Extractor Estimating: 240it [02:27,  1.50it/s]Extractor Estimating: 241it [02:28,  1.56it/s]Extractor Estimating: 242it [02:29,  1.61it/s]Extractor Estimating: 243it [02:29,  1.63it/s]Extractor Estimating: 244it [02:30,  1.63it/s]Extractor Estimating: 245it [02:30,  1.63it/s]Extractor Estimating: 246it [02:31,  1.60it/s]Extractor Estimating: 247it [02:32,  1.62it/s]Extractor Estimating: 248it [02:32,  1.61it/s]Extractor Estimating: 249it [02:33,  1.66it/s]Extractor Estimating: 250it [02:34,  1.63it/s]Extractor Estimating: 251it [02:34,  1.64it/s]Extractor Estimating: 252it [02:35,  1.74it/s]Extractor Estimating: 253it [02:35,  1.76it/s]Extractor Estimating: 254it [02:36,  1.71it/s]Extractor Estimating: 255it [02:36,  1.73it/s]Extractor Estimating: 256it [02:37,  1.78it/s]Extractor Estimating: 257it [02:37,  1.81it/s]Extractor Estimating: 258it [02:38,  1.82it/s]Extractor Estimating: 259it [02:39,  1.78it/s]Extractor Estimating: 260it [02:39,  1.76it/s]Extractor Estimating: 261it [02:40,  1.71it/s]Extractor Estimating: 262it [02:40,  1.73it/s]Extractor Estimating: 263it [02:41,  1.75it/s]Extractor Estimating: 264it [02:42,  1.73it/s]Extractor Estimating: 265it [02:42,  1.69it/s]Extractor Estimating: 266it [02:43,  1.71it/s]Extractor Estimating: 267it [02:43,  1.75it/s]Extractor Estimating: 268it [02:44,  1.74it/s]Extractor Estimating: 269it [02:44,  1.78it/s]Extractor Estimating: 270it [02:45,  1.69it/s]Extractor Estimating: 271it [02:46,  1.73it/s]Extractor Estimating: 272it [02:46,  1.70it/s]Extractor Estimating: 273it [02:47,  1.74it/s]Extractor Estimating: 274it [02:47,  1.73it/s]Extractor Estimating: 275it [02:48,  1.78it/s]Extractor Estimating: 276it [02:48,  1.71it/s]Extractor Estimating: 277it [02:49,  1.60it/s]Extractor Estimating: 278it [02:50,  1.55it/s]Extractor Estimating: 279it [02:50,  1.57it/s]Extractor Estimating: 280it [02:51,  1.57it/s]Extractor Estimating: 281it [02:52,  1.60it/s]Extractor Estimating: 282it [02:52,  1.57it/s]Extractor Estimating: 283it [02:53,  1.57it/s]Extractor Estimating: 284it [02:54,  1.54it/s]Extractor Estimating: 285it [02:54,  1.49it/s]Extractor Estimating: 286it [02:55,  1.51it/s]Extractor Estimating: 287it [02:56,  1.50it/s]Extractor Estimating: 288it [02:56,  1.51it/s]Extractor Estimating: 289it [02:57,  1.51it/s]Extractor Estimating: 290it [02:58,  1.40it/s]Extractor Estimating: 291it [02:59,  1.46it/s]Extractor Estimating: 292it [02:59,  1.47it/s]Extractor Estimating: 293it [03:00,  1.46it/s]Extractor Estimating: 294it [03:01,  1.47it/s]Extractor Estimating: 295it [03:01,  1.52it/s]Extractor Estimating: 296it [03:02,  1.51it/s]Extractor Estimating: 297it [03:02,  1.50it/s]Extractor Estimating: 298it [03:03,  1.51it/s]Extractor Estimating: 299it [03:04,  1.52it/s]Extractor Estimating: 300it [03:04,  1.49it/s]Extractor Estimating: 301it [03:05,  1.59it/s]Extractor Estimating: 302it [03:06,  1.67it/s]Extractor Estimating: 303it [03:06,  1.76it/s]Extractor Estimating: 304it [03:07,  1.71it/s]Extractor Estimating: 305it [03:07,  1.75it/s]Extractor Estimating: 306it [03:08,  1.85it/s]Extractor Estimating: 307it [03:08,  1.91it/s]Extractor Estimating: 308it [03:09,  1.92it/s]Extractor Estimating: 309it [03:09,  2.01it/s]Extractor Estimating: 310it [03:10,  2.09it/s]Extractor Estimating: 311it [03:10,  2.05it/s]Extractor Estimating: 312it [03:11,  1.92it/s]Extractor Estimating: 313it [03:11,  1.97it/s]Extractor Estimating: 314it [03:12,  1.98it/s]Extractor Estimating: 315it [03:12,  2.06it/s]Extractor Estimating: 316it [03:13,  1.97it/s]Extractor Estimating: 317it [03:13,  1.92it/s]Extractor Estimating: 318it [03:14,  1.95it/s]Extractor Estimating: 319it [03:14,  1.97it/s]Extractor Estimating: 320it [03:15,  1.93it/s]Extractor Estimating: 321it [03:15,  1.92it/s]Extractor Estimating: 322it [03:16,  1.96it/s]Extractor Estimating: 323it [03:16,  2.01it/s]Extractor Estimating: 324it [03:17,  2.04it/s]Extractor Estimating: 325it [03:17,  1.98it/s]Extractor Estimating: 326it [03:18,  1.77it/s]Extractor Estimating: 327it [03:19,  1.70it/s]Extractor Estimating: 328it [03:19,  1.63it/s]Extractor Estimating: 329it [03:20,  1.60it/s]Extractor Estimating: 330it [03:21,  1.58it/s]Extractor Estimating: 331it [03:21,  1.56it/s]Extractor Estimating: 332it [03:22,  1.55it/s]Extractor Estimating: 333it [03:22,  1.56it/s]Extractor Estimating: 334it [03:23,  1.55it/s]Extractor Estimating: 335it [03:24,  1.55it/s]Extractor Estimating: 336it [03:24,  1.55it/s]Extractor Estimating: 337it [03:25,  1.55it/s]Extractor Estimating: 338it [03:26,  1.57it/s]Extractor Estimating: 339it [03:26,  1.57it/s]Extractor Estimating: 340it [03:27,  1.58it/s]Extractor Estimating: 341it [03:28,  1.59it/s]Extractor Estimating: 342it [03:28,  1.54it/s]Extractor Estimating: 343it [03:29,  1.56it/s]Extractor Estimating: 344it [03:30,  1.55it/s]Extractor Estimating: 345it [03:30,  1.58it/s]Extractor Estimating: 346it [03:31,  1.56it/s]Extractor Estimating: 347it [03:31,  1.58it/s]Extractor Estimating: 348it [03:32,  1.59it/s]Extractor Estimating: 349it [03:33,  1.58it/s]Extractor Estimating: 350it [03:33,  1.57it/s]Extractor Estimating: 351it [03:34,  1.55it/s]Extractor Estimating: 352it [03:35,  1.59it/s]Extractor Estimating: 353it [03:35,  1.64it/s]Extractor Estimating: 354it [03:36,  1.66it/s]Extractor Estimating: 355it [03:36,  1.64it/s]Extractor Estimating: 356it [03:37,  1.58it/s]Extractor Estimating: 357it [03:38,  1.59it/s]Extractor Estimating: 358it [03:38,  1.58it/s]Extractor Estimating: 359it [03:39,  1.56it/s]Extractor Estimating: 360it [03:40,  1.59it/s]Extractor Estimating: 361it [03:40,  1.60it/s]Extractor Estimating: 362it [03:41,  1.57it/s]Extractor Estimating: 363it [03:41,  1.57it/s]Extractor Estimating: 364it [03:42,  1.59it/s]Extractor Estimating: 365it [03:43,  1.54it/s]Extractor Estimating: 366it [03:43,  1.55it/s]Extractor Estimating: 367it [03:44,  1.58it/s]Extractor Estimating: 368it [03:45,  1.52it/s]Extractor Estimating: 369it [03:45,  1.55it/s]Extractor Estimating: 370it [03:46,  1.53it/s]Extractor Estimating: 371it [03:47,  1.57it/s]Extractor Estimating: 372it [03:47,  1.60it/s]Extractor Estimating: 373it [03:48,  1.59it/s]Extractor Estimating: 374it [03:49,  1.56it/s]Extractor Estimating: 375it [03:49,  1.54it/s]Extractor Estimating: 376it [03:50,  1.59it/s]Extractor Estimating: 377it [03:50,  1.59it/s]Extractor Estimating: 378it [03:51,  1.60it/s]Extractor Estimating: 379it [03:52,  1.66it/s]Extractor Estimating: 380it [03:52,  1.69it/s]Extractor Estimating: 381it [03:53,  1.67it/s]Extractor Estimating: 382it [03:53,  1.60it/s]Extractor Estimating: 383it [03:54,  1.63it/s]Extractor Estimating: 384it [03:55,  1.43it/s]Extractor Estimating: 385it [03:56,  1.50it/s]Extractor Estimating: 386it [03:56,  1.52it/s]Extractor Estimating: 387it [03:57,  1.55it/s]Extractor Estimating: 388it [03:57,  1.56it/s]Extractor Estimating: 389it [03:58,  1.57it/s]Extractor Estimating: 390it [03:59,  1.59it/s]Extractor Estimating: 391it [03:59,  1.63it/s]Extractor Estimating: 392it [04:00,  1.64it/s]Extractor Estimating: 393it [04:00,  1.63it/s]Extractor Estimating: 394it [04:01,  1.64it/s]Extractor Estimating: 395it [04:02,  1.63it/s]Extractor Estimating: 396it [04:02,  1.62it/s]Extractor Estimating: 397it [04:03,  1.61it/s]Extractor Estimating: 398it [04:04,  1.60it/s]Extractor Estimating: 399it [04:04,  1.60it/s]Extractor Estimating: 400it [04:05,  1.60it/s]Extractor Estimating: 401it [04:05,  1.59it/s]Extractor Estimating: 402it [04:06,  1.61it/s]Extractor Estimating: 403it [04:07,  1.60it/s]Extractor Estimating: 404it [04:07,  1.59it/s]Extractor Estimating: 405it [04:08,  1.55it/s]Extractor Estimating: 406it [04:09,  1.52it/s]Extractor Estimating: 407it [04:09,  1.57it/s]Extractor Estimating: 408it [04:10,  1.59it/s]Extractor Estimating: 409it [04:11,  1.58it/s]Extractor Estimating: 410it [04:11,  1.59it/s]Extractor Estimating: 411it [04:12,  1.63it/s]Extractor Estimating: 412it [04:12,  1.61it/s]Extractor Estimating: 413it [04:13,  1.61it/s]Extractor Estimating: 414it [04:14,  1.63it/s]Extractor Estimating: 415it [04:14,  1.61it/s]Extractor Estimating: 416it [04:15,  1.64it/s]Extractor Estimating: 417it [04:16,  1.58it/s]Extractor Estimating: 418it [04:16,  1.57it/s]Extractor Estimating: 419it [04:17,  1.58it/s]Extractor Estimating: 420it [04:17,  1.55it/s]Extractor Estimating: 421it [04:18,  1.55it/s]Extractor Estimating: 422it [04:19,  1.59it/s]Extractor Estimating: 423it [04:19,  1.59it/s]Extractor Estimating: 424it [04:20,  1.61it/s]Extractor Estimating: 425it [04:21,  1.63it/s]Extractor Estimating: 426it [04:21,  1.65it/s]Extractor Estimating: 427it [04:22,  1.66it/s]Extractor Estimating: 428it [04:22,  1.68it/s]Extractor Estimating: 429it [04:23,  1.67it/s]Extractor Estimating: 430it [04:24,  1.64it/s]Extractor Estimating: 431it [04:24,  1.62it/s]Extractor Estimating: 432it [04:25,  1.62it/s]Extractor Estimating: 433it [04:25,  1.61it/s]Extractor Estimating: 434it [04:26,  1.65it/s]Extractor Estimating: 435it [04:27,  1.66it/s]Extractor Estimating: 436it [04:27,  1.65it/s]Extractor Estimating: 437it [04:28,  1.67it/s]Extractor Estimating: 438it [04:28,  1.72it/s]Extractor Estimating: 439it [04:29,  1.69it/s]Extractor Estimating: 440it [04:30,  1.66it/s]Extractor Estimating: 441it [04:30,  1.66it/s]Extractor Estimating: 442it [04:31,  1.70it/s]Extractor Estimating: 443it [04:31,  1.71it/s]Extractor Estimating: 444it [04:32,  1.64it/s]Extractor Estimating: 445it [04:33,  1.64it/s]Extractor Estimating: 446it [04:33,  1.68it/s]Extractor Estimating: 447it [04:34,  1.73it/s]Extractor Estimating: 448it [04:34,  1.74it/s]Extractor Estimating: 449it [04:35,  1.77it/s]Extractor Estimating: 450it [04:35,  1.77it/s]Extractor Estimating: 451it [04:36,  1.64it/s]Extractor Estimating: 452it [04:37,  1.60it/s]Extractor Estimating: 453it [04:37,  1.59it/s]Extractor Estimating: 454it [04:38,  1.57it/s]Extractor Estimating: 455it [04:39,  1.57it/s]Extractor Estimating: 456it [04:39,  1.54it/s]Extractor Estimating: 457it [04:40,  1.49it/s]Extractor Estimating: 458it [04:41,  1.49it/s]Extractor Estimating: 459it [04:41,  1.49it/s]Extractor Estimating: 460it [04:42,  1.48it/s]Extractor Estimating: 461it [04:43,  1.50it/s]Extractor Estimating: 462it [04:43,  1.51it/s]Extractor Estimating: 463it [04:44,  1.53it/s]Extractor Estimating: 464it [04:45,  1.54it/s]Extractor Estimating: 465it [04:45,  1.53it/s]Extractor Estimating: 466it [04:46,  1.54it/s]Extractor Estimating: 467it [04:47,  1.53it/s]Extractor Estimating: 468it [04:47,  1.52it/s]Extractor Estimating: 469it [04:48,  1.39it/s]Extractor Estimating: 470it [04:49,  1.44it/s]Extractor Estimating: 471it [04:49,  1.47it/s]Extractor Estimating: 472it [04:50,  1.48it/s]Extractor Estimating: 473it [04:51,  1.49it/s]Extractor Estimating: 474it [04:51,  1.52it/s]Extractor Estimating: 475it [04:52,  1.58it/s]Extractor Estimating: 476it [04:53,  1.58it/s]Extractor Estimating: 477it [04:53,  1.59it/s]Extractor Estimating: 478it [04:54,  1.59it/s]Extractor Estimating: 479it [04:54,  1.57it/s]Extractor Estimating: 480it [04:55,  1.58it/s]Extractor Estimating: 481it [04:56,  1.60it/s]Extractor Estimating: 482it [04:56,  1.64it/s]Extractor Estimating: 483it [04:57,  1.67it/s]Extractor Estimating: 484it [04:58,  1.59it/s]Extractor Estimating: 485it [04:58,  1.62it/s]Extractor Estimating: 486it [04:59,  1.66it/s]Extractor Estimating: 487it [04:59,  1.67it/s]Extractor Estimating: 488it [05:00,  1.63it/s]Extractor Estimating: 489it [05:01,  1.65it/s]Extractor Estimating: 490it [05:01,  1.63it/s]Extractor Estimating: 491it [05:02,  1.66it/s]Extractor Estimating: 492it [05:02,  1.66it/s]Extractor Estimating: 493it [05:03,  1.67it/s]Extractor Estimating: 494it [05:04,  1.66it/s]Extractor Estimating: 495it [05:04,  1.62it/s]Extractor Estimating: 496it [05:05,  1.66it/s]Extractor Estimating: 497it [05:05,  1.61it/s]Extractor Estimating: 498it [05:06,  1.60it/s]Extractor Estimating: 499it [05:07,  1.61it/s]Extractor Estimating: 499it [05:07,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:47,438 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:47,444 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:47,445 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:47,445 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:47,445 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:02:48,022 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:02:48,023 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:02:48,608 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:02:49,659 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:02:49,659 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:52,526 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:52,530 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:52,530 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:52,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:02:52,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:02:53,155 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:02:53,156 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:02:53,740 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:02:53,893 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:02:53,893 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:18:28,156 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:18:28,174 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:18:28,174 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:18:28,174 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:18:28,175 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:18:28,721 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:18:28,722 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:18:29,087 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:18:30,247 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:18:30,247 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:18:33,455 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:18:33,513 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:18:33,513 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:18:33,513 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:18:33,513 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:18:34,507 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:18:34,508 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:18:35,206 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:18:35,484 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:18:35,484 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 9968 mean pseudo reward: 0.9614215256426462
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_1/dev.jsonl'}
train vocab size: 22030
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22130, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22130, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.079, loss:587.5068
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.074, loss:544.8200
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.084, loss:519.4691
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.078, loss:542.0901
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.087, loss:535.9549
>> valid entity prec:0.5846, rec:0.5697, f1:0.5771
>> valid relation prec:0.3346, rec:0.0715, f1:0.1179
>> valid relation with NER prec:0.3346, rec:0.0715, f1:0.1179
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.855, loss:524.3399
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 1.061, loss:549.2324
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 1.065, loss:550.7344
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.067, loss:509.1964
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.069, loss:529.7591
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5694, rec:0.6693, f1:0.6154
>> valid relation prec:0.2989, rec:0.0817, f1:0.1283
>> valid relation with NER prec:0.2989, rec:0.0817, f1:0.1283
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 268, avg_time 2.865, loss:539.7345
g_step 1200, step 368, avg_time 1.068, loss:529.0872
g_step 1300, step 52, avg_time 1.062, loss:528.4503
g_step 1400, step 152, avg_time 1.057, loss:504.1885
g_step 1500, step 252, avg_time 1.074, loss:515.8677
>> valid entity prec:0.5699, rec:0.5645, f1:0.5672
>> valid relation prec:0.3286, rec:0.0532, f1:0.0916
>> valid relation with NER prec:0.3286, rec:0.0532, f1:0.0916
g_step 1600, step 352, avg_time 2.851, loss:514.4305
g_step 1700, step 36, avg_time 1.055, loss:524.1239
g_step 1800, step 136, avg_time 1.077, loss:491.7833
g_step 1900, step 236, avg_time 1.062, loss:493.6733
g_step 2000, step 336, avg_time 1.074, loss:508.8100
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5429, rec:0.5650, f1:0.5537
>> valid relation prec:0.2604, rec:0.0678, f1:0.1076
>> valid relation with NER prec:0.2604, rec:0.0678, f1:0.1076
g_step 2100, step 20, avg_time 2.845, loss:484.8040
g_step 2200, step 120, avg_time 1.072, loss:471.5201
g_step 2300, step 220, avg_time 1.062, loss:469.4157
g_step 2400, step 320, avg_time 1.063, loss:499.2420
g_step 2500, step 4, avg_time 1.070, loss:458.8715
>> valid entity prec:0.5683, rec:0.5924, f1:0.5801
>> valid relation prec:0.2977, rec:0.0509, f1:0.0870
>> valid relation with NER prec:0.2977, rec:0.0509, f1:0.0870
g_step 2600, step 104, avg_time 2.851, loss:435.8869
g_step 2700, step 204, avg_time 1.066, loss:454.5697
g_step 2800, step 304, avg_time 1.071, loss:463.8892
g_step 2900, step 404, avg_time 1.064, loss:489.8836
g_step 3000, step 88, avg_time 1.066, loss:423.7447
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5942, rec:0.5551, f1:0.5740
>> valid relation prec:0.2806, rec:0.0605, f1:0.0996
>> valid relation with NER prec:0.2806, rec:0.0605, f1:0.0996
g_step 3100, step 188, avg_time 2.858, loss:447.0059
g_step 3200, step 288, avg_time 1.062, loss:441.4088
g_step 3300, step 388, avg_time 1.076, loss:454.3106
g_step 3400, step 72, avg_time 1.056, loss:419.8272
g_step 3500, step 172, avg_time 1.066, loss:434.3675
>> valid entity prec:0.5933, rec:0.5627, f1:0.5776
>> valid relation prec:0.2459, rec:0.0726, f1:0.1121
>> valid relation with NER prec:0.2459, rec:0.0726, f1:0.1121
g_step 3600, step 272, avg_time 2.852, loss:430.7962
g_step 3700, step 372, avg_time 1.075, loss:432.8241
g_step 3800, step 56, avg_time 1.069, loss:414.0610
g_step 3900, step 156, avg_time 1.069, loss:408.9105
g_step 4000, step 256, avg_time 1.053, loss:405.5409
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5969, rec:0.5410, f1:0.5676
>> valid relation prec:0.2023, rec:0.0509, f1:0.0814
>> valid relation with NER prec:0.2023, rec:0.0509, f1:0.0814
g_step 4100, step 356, avg_time 2.848, loss:424.0561
g_step 4200, step 40, avg_time 1.067, loss:393.4553
g_step 4300, step 140, avg_time 1.060, loss:387.0657
g_step 4400, step 240, avg_time 1.066, loss:379.0072
g_step 4500, step 340, avg_time 1.069, loss:419.3434
>> valid entity prec:0.5879, rec:0.5533, f1:0.5701
>> valid relation prec:0.2449, rec:0.0672, f1:0.1054
>> valid relation with NER prec:0.2449, rec:0.0672, f1:0.1054
g_step 4600, step 24, avg_time 2.856, loss:408.9252
g_step 4700, step 124, avg_time 1.066, loss:379.7618
g_step 4800, step 224, avg_time 1.064, loss:397.4051
g_step 4900, step 324, avg_time 1.077, loss:382.1126
g_step 5000, step 8, avg_time 1.057, loss:405.3987
learning rate was adjusted to 0.0008
>> valid entity prec:0.6140, rec:0.5180, f1:0.5619
>> valid relation prec:0.2330, rec:0.0435, f1:0.0733
>> valid relation with NER prec:0.2330, rec:0.0435, f1:0.0733
g_step 5100, step 108, avg_time 2.853, loss:376.4610
g_step 5200, step 208, avg_time 1.054, loss:375.0900
g_step 5300, step 308, avg_time 1.059, loss:372.0498
g_step 5400, step 408, avg_time 1.121, loss:374.6554
g_step 5500, step 92, avg_time 1.062, loss:360.0430
>> valid entity prec:0.5869, rec:0.5270, f1:0.5553
>> valid relation prec:0.2111, rec:0.0449, f1:0.0741
>> valid relation with NER prec:0.2111, rec:0.0449, f1:0.0741
g_step 5600, step 192, avg_time 2.832, loss:346.3210
g_step 5700, step 292, avg_time 1.065, loss:369.9138
g_step 5800, step 392, avg_time 1.073, loss:380.4419
g_step 5900, step 76, avg_time 1.051, loss:334.2258
g_step 6000, step 176, avg_time 1.062, loss:352.4449
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.6640, rec:0.4534, f1:0.5388
>> valid relation prec:0.2535, rec:0.0534, f1:0.0883
>> valid relation with NER prec:0.2535, rec:0.0534, f1:0.0883
g_step 6100, step 276, avg_time 2.832, loss:364.3416
g_step 6200, step 376, avg_time 1.067, loss:367.4749
g_step 6300, step 60, avg_time 1.068, loss:347.2098
g_step 6400, step 160, avg_time 1.070, loss:335.2233
g_step 6500, step 260, avg_time 1.077, loss:327.8153
>> valid entity prec:0.5948, rec:0.5571, f1:0.5753
>> valid relation prec:0.2333, rec:0.0524, f1:0.0856
>> valid relation with NER prec:0.2333, rec:0.0524, f1:0.0856
g_step 6600, step 360, avg_time 2.831, loss:349.0457
g_step 6700, step 44, avg_time 1.056, loss:344.7175
g_step 6800, step 144, avg_time 1.068, loss:336.1686
g_step 6900, step 244, avg_time 1.060, loss:330.8771
g_step 7000, step 344, avg_time 1.079, loss:349.3137
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.6597, rec:0.4621, f1:0.5435
>> valid relation prec:0.2990, rec:0.0549, f1:0.0928
>> valid relation with NER prec:0.2990, rec:0.0549, f1:0.0928
g_step 7100, step 28, avg_time 2.820, loss:330.8168
g_step 7200, step 128, avg_time 1.057, loss:319.4296
g_step 7300, step 228, avg_time 1.067, loss:312.7442
g_step 7400, step 328, avg_time 1.067, loss:337.9069
g_step 7500, step 12, avg_time 1.066, loss:321.4303
>> valid entity prec:0.6081, rec:0.4676, f1:0.5287
>> valid relation prec:0.2236, rec:0.0410, f1:0.0692
>> valid relation with NER prec:0.2236, rec:0.0410, f1:0.0692
g_step 7600, step 112, avg_time 2.836, loss:313.2755
g_step 7700, step 212, avg_time 1.074, loss:314.5524
g_step 7800, step 312, avg_time 1.067, loss:312.0787
g_step 7900, step 412, avg_time 1.062, loss:329.4627
g_step 8000, step 96, avg_time 1.076, loss:290.6779
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5951, rec:0.5382, f1:0.5652
>> valid relation prec:0.2478, rec:0.0462, f1:0.0778
>> valid relation with NER prec:0.2478, rec:0.0462, f1:0.0778
g_step 8100, step 196, avg_time 2.835, loss:314.6167
g_step 8200, step 296, avg_time 1.064, loss:307.9982
g_step 8300, step 396, avg_time 1.064, loss:317.0064
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_15_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'given name', 'participant in', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14118
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14218, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:05,  1.48it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.45it/s]Extractor Predicting: 12it [00:08,  1.48it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:09,  1.51it/s]Extractor Predicting: 16it [00:10,  1.51it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:15,  1.51it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:17,  1.54it/s]Extractor Predicting: 27it [00:17,  1.55it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:19,  1.53it/s]Extractor Predicting: 30it [00:19,  1.54it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:21,  1.51it/s]Extractor Predicting: 33it [00:21,  1.49it/s]Extractor Predicting: 34it [00:22,  1.48it/s]Extractor Predicting: 35it [00:23,  1.49it/s]Extractor Predicting: 36it [00:23,  1.50it/s]Extractor Predicting: 37it [00:24,  1.49it/s]Extractor Predicting: 38it [00:25,  1.49it/s]Extractor Predicting: 39it [00:25,  1.43it/s]Extractor Predicting: 40it [00:26,  1.44it/s]Extractor Predicting: 41it [00:27,  1.49it/s]Extractor Predicting: 42it [00:27,  1.56it/s]Extractor Predicting: 43it [00:28,  1.56it/s]Extractor Predicting: 44it [00:29,  1.51it/s]Extractor Predicting: 45it [00:29,  1.54it/s]Extractor Predicting: 46it [00:30,  1.55it/s]Extractor Predicting: 47it [00:31,  1.55it/s]Extractor Predicting: 48it [00:31,  1.57it/s]Extractor Predicting: 49it [00:32,  1.58it/s]Extractor Predicting: 50it [00:32,  1.57it/s]Extractor Predicting: 51it [00:33,  1.59it/s]Extractor Predicting: 52it [00:34,  1.61it/s]Extractor Predicting: 53it [00:34,  1.50it/s]Extractor Predicting: 54it [00:35,  1.57it/s]Extractor Predicting: 55it [00:36,  1.59it/s]Extractor Predicting: 56it [00:36,  1.59it/s]Extractor Predicting: 57it [00:37,  1.58it/s]Extractor Predicting: 58it [00:38,  1.57it/s]Extractor Predicting: 59it [00:38,  1.59it/s]Extractor Predicting: 60it [00:39,  1.59it/s]Extractor Predicting: 61it [00:39,  1.55it/s]Extractor Predicting: 62it [00:40,  1.57it/s]Extractor Predicting: 63it [00:41,  1.60it/s]Extractor Predicting: 64it [00:41,  1.64it/s]Extractor Predicting: 65it [00:42,  1.62it/s]Extractor Predicting: 66it [00:43,  1.57it/s]Extractor Predicting: 67it [00:43,  1.58it/s]Extractor Predicting: 68it [00:44,  1.56it/s]Extractor Predicting: 69it [00:44,  1.61it/s]Extractor Predicting: 70it [00:45,  1.60it/s]Extractor Predicting: 71it [00:46,  1.57it/s]Extractor Predicting: 72it [00:46,  1.59it/s]Extractor Predicting: 73it [00:47,  1.57it/s]Extractor Predicting: 74it [00:48,  1.59it/s]Extractor Predicting: 75it [00:48,  1.58it/s]Extractor Predicting: 76it [00:49,  1.56it/s]Extractor Predicting: 77it [00:50,  1.59it/s]Extractor Predicting: 78it [00:50,  1.59it/s]Extractor Predicting: 79it [00:51,  1.56it/s]Extractor Predicting: 80it [00:51,  1.56it/s]Extractor Predicting: 81it [00:52,  1.56it/s]Extractor Predicting: 82it [00:53,  1.53it/s]Extractor Predicting: 83it [00:53,  1.54it/s]Extractor Predicting: 84it [00:54,  1.53it/s]Extractor Predicting: 85it [00:55,  1.52it/s]Extractor Predicting: 86it [00:55,  1.53it/s]Extractor Predicting: 87it [00:56,  1.55it/s]Extractor Predicting: 88it [00:57,  1.55it/s]Extractor Predicting: 89it [00:57,  1.55it/s]Extractor Predicting: 90it [00:58,  1.39it/s]Extractor Predicting: 91it [00:59,  1.39it/s]Extractor Predicting: 92it [01:00,  1.43it/s]Extractor Predicting: 93it [01:00,  1.42it/s]Extractor Predicting: 94it [01:01,  1.43it/s]Extractor Predicting: 95it [01:02,  1.42it/s]Extractor Predicting: 96it [01:02,  1.43it/s]Extractor Predicting: 97it [01:03,  1.40it/s]Extractor Predicting: 98it [01:04,  1.40it/s]Extractor Predicting: 99it [01:05,  1.42it/s]Extractor Predicting: 100it [01:05,  1.44it/s]Extractor Predicting: 101it [01:06,  1.48it/s]Extractor Predicting: 102it [01:07,  1.47it/s]Extractor Predicting: 103it [01:07,  1.47it/s]Extractor Predicting: 104it [01:08,  1.45it/s]Extractor Predicting: 105it [01:09,  1.46it/s]Extractor Predicting: 106it [01:09,  1.47it/s]Extractor Predicting: 107it [01:10,  1.43it/s]Extractor Predicting: 108it [01:11,  1.44it/s]Extractor Predicting: 109it [01:11,  1.49it/s]Extractor Predicting: 110it [01:12,  1.47it/s]Extractor Predicting: 111it [01:13,  1.48it/s]Extractor Predicting: 112it [01:13,  1.48it/s]Extractor Predicting: 113it [01:14,  1.46it/s]Extractor Predicting: 114it [01:15,  1.44it/s]Extractor Predicting: 115it [01:15,  1.43it/s]Extractor Predicting: 116it [01:16,  1.44it/s]Extractor Predicting: 117it [01:17,  1.43it/s]Extractor Predicting: 118it [01:18,  1.44it/s]Extractor Predicting: 119it [01:18,  1.46it/s]Extractor Predicting: 120it [01:19,  1.47it/s]Extractor Predicting: 121it [01:20,  1.43it/s]Extractor Predicting: 122it [01:20,  1.48it/s]Extractor Predicting: 123it [01:21,  1.51it/s]Extractor Predicting: 124it [01:22,  1.47it/s]Extractor Predicting: 125it [01:22,  1.48it/s]Extractor Predicting: 126it [01:23,  1.47it/s]Extractor Predicting: 127it [01:24,  1.46it/s]Extractor Predicting: 128it [01:24,  1.43it/s]Extractor Predicting: 129it [01:25,  1.31it/s]Extractor Predicting: 130it [01:26,  1.34it/s]Extractor Predicting: 131it [01:27,  1.36it/s]Extractor Predicting: 132it [01:27,  1.38it/s]Extractor Predicting: 133it [01:28,  1.36it/s]Extractor Predicting: 134it [01:29,  1.35it/s]Extractor Predicting: 135it [01:30,  1.32it/s]Extractor Predicting: 136it [01:31,  1.32it/s]Extractor Predicting: 137it [01:31,  1.31it/s]Extractor Predicting: 138it [01:32,  1.31it/s]Extractor Predicting: 139it [01:33,  1.33it/s]Extractor Predicting: 140it [01:34,  1.35it/s]Extractor Predicting: 141it [01:34,  1.38it/s]Extractor Predicting: 142it [01:35,  1.36it/s]Extractor Predicting: 143it [01:36,  1.38it/s]Extractor Predicting: 144it [01:36,  1.36it/s]Extractor Predicting: 145it [01:37,  1.36it/s]Extractor Predicting: 146it [01:38,  1.33it/s]Extractor Predicting: 147it [01:39,  1.37it/s]Extractor Predicting: 148it [01:39,  1.38it/s]Extractor Predicting: 149it [01:40,  1.36it/s]Extractor Predicting: 150it [01:41,  1.37it/s]Extractor Predicting: 151it [01:42,  1.37it/s]Extractor Predicting: 152it [01:42,  1.38it/s]Extractor Predicting: 153it [01:43,  1.38it/s]Extractor Predicting: 154it [01:44,  1.35it/s]Extractor Predicting: 155it [01:44,  1.36it/s]Extractor Predicting: 156it [01:45,  1.31it/s]Extractor Predicting: 157it [01:46,  1.30it/s]Extractor Predicting: 158it [01:47,  1.30it/s]Extractor Predicting: 159it [01:48,  1.35it/s]Extractor Predicting: 160it [01:48,  1.36it/s]Extractor Predicting: 161it [01:49,  1.39it/s]Extractor Predicting: 162it [01:50,  1.39it/s]Extractor Predicting: 163it [01:50,  1.41it/s]Extractor Predicting: 164it [01:51,  1.44it/s]Extractor Predicting: 165it [01:52,  1.42it/s]Extractor Predicting: 166it [01:52,  1.45it/s]Extractor Predicting: 167it [01:53,  1.45it/s]Extractor Predicting: 168it [01:54,  1.45it/s]Extractor Predicting: 169it [01:54,  1.47it/s]Extractor Predicting: 170it [01:55,  1.44it/s]Extractor Predicting: 171it [01:56,  1.46it/s]Extractor Predicting: 172it [01:56,  1.48it/s]Extractor Predicting: 173it [01:57,  1.46it/s]Extractor Predicting: 174it [01:58,  1.47it/s]Extractor Predicting: 175it [01:59,  1.43it/s]Extractor Predicting: 176it [01:59,  1.43it/s]Extractor Predicting: 177it [02:00,  1.44it/s]Extractor Predicting: 178it [02:01,  1.44it/s]Extractor Predicting: 179it [02:01,  1.44it/s]Extractor Predicting: 180it [02:02,  1.44it/s]Extractor Predicting: 181it [02:03,  1.43it/s]Extractor Predicting: 182it [02:03,  1.43it/s]Extractor Predicting: 183it [02:04,  1.40it/s]Extractor Predicting: 184it [02:05,  1.42it/s]Extractor Predicting: 185it [02:05,  1.57it/s]Extractor Predicting: 185it [02:05,  1.47it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:21:00,304 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:21:00,334 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:21:00,334 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:21:00,334 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:21:00,334 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:21:01,124 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:21:01,125 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:21:01,738 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:21:02,822 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:21:02,822 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:21:05,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:21:05,861 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:21:05,861 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:21:05,861 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:21:05,861 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:21:06,605 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:21:06,606 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:21:07,225 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:21:07,421 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:21:07,421 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5263157894736842,
  "recall": 0.09351620947630923,
  "score": 0.15881418740074113,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_15_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'employer', 'field of work', 'languages spoken, written or signed', 'lyrics by', 'manufacturer', 'member of sports team', 'occupation', 'performer', 'residence', 'shares border with', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 31765
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31865, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.54it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:05,  1.51it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:11,  1.53it/s]Extractor Predicting: 18it [00:11,  1.49it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:13,  1.48it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.48it/s]Extractor Predicting: 24it [00:15,  1.50it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:17,  1.51it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:18,  1.50it/s]Extractor Predicting: 29it [00:19,  1.54it/s]Extractor Predicting: 30it [00:19,  1.62it/s]Extractor Predicting: 31it [00:20,  1.64it/s]Extractor Predicting: 32it [00:20,  1.61it/s]Extractor Predicting: 33it [00:21,  1.61it/s]Extractor Predicting: 34it [00:22,  1.62it/s]Extractor Predicting: 35it [00:22,  1.61it/s]Extractor Predicting: 36it [00:23,  1.46it/s]Extractor Predicting: 37it [00:24,  1.46it/s]Extractor Predicting: 38it [00:24,  1.49it/s]Extractor Predicting: 39it [00:25,  1.49it/s]Extractor Predicting: 40it [00:26,  1.51it/s]Extractor Predicting: 41it [00:26,  1.55it/s]Extractor Predicting: 42it [00:27,  1.53it/s]Extractor Predicting: 43it [00:28,  1.55it/s]Extractor Predicting: 44it [00:28,  1.58it/s]Extractor Predicting: 45it [00:29,  1.60it/s]Extractor Predicting: 46it [00:29,  1.63it/s]Extractor Predicting: 47it [00:30,  1.64it/s]Extractor Predicting: 48it [00:31,  1.61it/s]Extractor Predicting: 49it [00:31,  1.64it/s]Extractor Predicting: 50it [00:32,  1.61it/s]Extractor Predicting: 51it [00:33,  1.60it/s]Extractor Predicting: 52it [00:33,  1.59it/s]Extractor Predicting: 53it [00:34,  1.62it/s]Extractor Predicting: 54it [00:34,  1.56it/s]Extractor Predicting: 55it [00:35,  1.57it/s]Extractor Predicting: 56it [00:36,  1.55it/s]Extractor Predicting: 57it [00:36,  1.59it/s]Extractor Predicting: 58it [00:37,  1.66it/s]Extractor Predicting: 59it [00:38,  1.62it/s]Extractor Predicting: 60it [00:38,  1.58it/s]Extractor Predicting: 61it [00:39,  1.55it/s]Extractor Predicting: 62it [00:40,  1.54it/s]Extractor Predicting: 63it [00:40,  1.52it/s]Extractor Predicting: 64it [00:41,  1.51it/s]Extractor Predicting: 65it [00:42,  1.52it/s]Extractor Predicting: 66it [00:42,  1.53it/s]Extractor Predicting: 67it [00:43,  1.52it/s]Extractor Predicting: 68it [00:44,  1.50it/s]Extractor Predicting: 69it [00:44,  1.53it/s]Extractor Predicting: 70it [00:45,  1.53it/s]Extractor Predicting: 71it [00:46,  1.52it/s]Extractor Predicting: 72it [00:46,  1.49it/s]Extractor Predicting: 73it [00:47,  1.48it/s]Extractor Predicting: 74it [00:48,  1.47it/s]Extractor Predicting: 75it [00:48,  1.47it/s]Extractor Predicting: 76it [00:49,  1.45it/s]Extractor Predicting: 77it [00:50,  1.48it/s]Extractor Predicting: 78it [00:50,  1.53it/s]Extractor Predicting: 79it [00:51,  1.52it/s]Extractor Predicting: 80it [00:52,  1.54it/s]Extractor Predicting: 81it [00:52,  1.53it/s]Extractor Predicting: 82it [00:53,  1.52it/s]Extractor Predicting: 83it [00:53,  1.55it/s]Extractor Predicting: 84it [00:54,  1.54it/s]Extractor Predicting: 85it [00:55,  1.51it/s]Extractor Predicting: 86it [00:55,  1.52it/s]Extractor Predicting: 87it [00:56,  1.51it/s]Extractor Predicting: 88it [00:57,  1.51it/s]Extractor Predicting: 89it [00:57,  1.55it/s]Extractor Predicting: 90it [00:58,  1.54it/s]Extractor Predicting: 91it [00:59,  1.53it/s]Extractor Predicting: 92it [00:59,  1.50it/s]Extractor Predicting: 93it [01:00,  1.53it/s]Extractor Predicting: 94it [01:01,  1.55it/s]Extractor Predicting: 95it [01:01,  1.53it/s]Extractor Predicting: 96it [01:02,  1.54it/s]Extractor Predicting: 97it [01:03,  1.53it/s]Extractor Predicting: 98it [01:03,  1.48it/s]Extractor Predicting: 99it [01:04,  1.46it/s]Extractor Predicting: 100it [01:05,  1.43it/s]Extractor Predicting: 101it [01:06,  1.42it/s]Extractor Predicting: 102it [01:06,  1.44it/s]Extractor Predicting: 103it [01:07,  1.40it/s]Extractor Predicting: 104it [01:08,  1.47it/s]Extractor Predicting: 105it [01:08,  1.44it/s]Extractor Predicting: 106it [01:09,  1.48it/s]Extractor Predicting: 107it [01:10,  1.49it/s]Extractor Predicting: 108it [01:10,  1.50it/s]Extractor Predicting: 109it [01:11,  1.50it/s]Extractor Predicting: 110it [01:12,  1.47it/s]Extractor Predicting: 111it [01:12,  1.51it/s]Extractor Predicting: 112it [01:13,  1.52it/s]Extractor Predicting: 113it [01:14,  1.53it/s]Extractor Predicting: 114it [01:14,  1.53it/s]Extractor Predicting: 115it [01:15,  1.54it/s]Extractor Predicting: 116it [01:16,  1.53it/s]Extractor Predicting: 117it [01:16,  1.55it/s]Extractor Predicting: 118it [01:17,  1.59it/s]Extractor Predicting: 119it [01:17,  1.66it/s]Extractor Predicting: 120it [01:18,  1.59it/s]Extractor Predicting: 121it [01:19,  1.54it/s]Extractor Predicting: 122it [01:19,  1.55it/s]Extractor Predicting: 123it [01:20,  1.52it/s]Extractor Predicting: 124it [01:21,  1.53it/s]Extractor Predicting: 125it [01:21,  1.51it/s]Extractor Predicting: 126it [01:22,  1.49it/s]Extractor Predicting: 127it [01:23,  1.47it/s]Extractor Predicting: 128it [01:23,  1.50it/s]Extractor Predicting: 129it [01:24,  1.49it/s]Extractor Predicting: 130it [01:25,  1.50it/s]Extractor Predicting: 131it [01:25,  1.46it/s]Extractor Predicting: 132it [01:26,  1.43it/s]Extractor Predicting: 133it [01:27,  1.44it/s]Extractor Predicting: 134it [01:27,  1.48it/s]Extractor Predicting: 135it [01:28,  1.44it/s]Extractor Predicting: 136it [01:29,  1.45it/s]Extractor Predicting: 137it [01:30,  1.48it/s]Extractor Predicting: 138it [01:30,  1.47it/s]Extractor Predicting: 139it [01:31,  1.48it/s]Extractor Predicting: 140it [01:32,  1.46it/s]Extractor Predicting: 141it [01:32,  1.42it/s]Extractor Predicting: 142it [01:33,  1.44it/s]Extractor Predicting: 143it [01:34,  1.47it/s]Extractor Predicting: 144it [01:34,  1.49it/s]Extractor Predicting: 145it [01:35,  1.47it/s]Extractor Predicting: 146it [01:36,  1.45it/s]Extractor Predicting: 147it [01:36,  1.46it/s]Extractor Predicting: 148it [01:37,  1.45it/s]Extractor Predicting: 149it [01:38,  1.46it/s]Extractor Predicting: 150it [01:38,  1.48it/s]Extractor Predicting: 151it [01:39,  1.47it/s]Extractor Predicting: 152it [01:40,  1.51it/s]Extractor Predicting: 153it [01:40,  1.52it/s]Extractor Predicting: 154it [01:41,  1.51it/s]Extractor Predicting: 155it [01:42,  1.47it/s]Extractor Predicting: 156it [01:42,  1.47it/s]Extractor Predicting: 157it [01:43,  1.44it/s]Extractor Predicting: 158it [01:44,  1.46it/s]Extractor Predicting: 159it [01:45,  1.44it/s]Extractor Predicting: 160it [01:45,  1.46it/s]Extractor Predicting: 161it [01:46,  1.46it/s]Extractor Predicting: 162it [01:47,  1.47it/s]Extractor Predicting: 163it [01:47,  1.49it/s]Extractor Predicting: 164it [01:48,  1.49it/s]Extractor Predicting: 165it [01:49,  1.50it/s]Extractor Predicting: 166it [01:49,  1.49it/s]Extractor Predicting: 167it [01:50,  1.48it/s]Extractor Predicting: 168it [01:51,  1.50it/s]Extractor Predicting: 169it [01:51,  1.53it/s]Extractor Predicting: 170it [01:52,  1.51it/s]Extractor Predicting: 171it [01:53,  1.51it/s]Extractor Predicting: 172it [01:53,  1.50it/s]Extractor Predicting: 173it [01:54,  1.31it/s]Extractor Predicting: 174it [01:55,  1.36it/s]Extractor Predicting: 175it [01:56,  1.39it/s]Extractor Predicting: 176it [01:56,  1.42it/s]Extractor Predicting: 177it [01:57,  1.44it/s]Extractor Predicting: 178it [01:58,  1.43it/s]Extractor Predicting: 179it [01:58,  1.48it/s]Extractor Predicting: 180it [01:59,  1.51it/s]Extractor Predicting: 181it [02:00,  1.51it/s]Extractor Predicting: 182it [02:00,  1.50it/s]Extractor Predicting: 183it [02:01,  1.52it/s]Extractor Predicting: 184it [02:01,  1.51it/s]Extractor Predicting: 185it [02:02,  1.53it/s]Extractor Predicting: 186it [02:03,  1.51it/s]Extractor Predicting: 187it [02:03,  1.50it/s]Extractor Predicting: 188it [02:04,  1.50it/s]Extractor Predicting: 189it [02:05,  1.49it/s]Extractor Predicting: 190it [02:05,  1.50it/s]Extractor Predicting: 191it [02:06,  1.48it/s]Extractor Predicting: 192it [02:07,  1.50it/s]Extractor Predicting: 193it [02:07,  1.50it/s]Extractor Predicting: 194it [02:08,  1.50it/s]Extractor Predicting: 195it [02:09,  1.52it/s]Extractor Predicting: 196it [02:09,  1.53it/s]Extractor Predicting: 197it [02:10,  1.53it/s]Extractor Predicting: 198it [02:11,  1.49it/s]Extractor Predicting: 199it [02:11,  1.50it/s]Extractor Predicting: 200it [02:12,  1.50it/s]Extractor Predicting: 201it [02:13,  1.48it/s]Extractor Predicting: 202it [02:14,  1.45it/s]Extractor Predicting: 203it [02:14,  1.46it/s]Extractor Predicting: 204it [02:15,  1.46it/s]Extractor Predicting: 205it [02:16,  1.45it/s]Extractor Predicting: 206it [02:16,  1.49it/s]Extractor Predicting: 207it [02:17,  1.49it/s]Extractor Predicting: 208it [02:18,  1.51it/s]Extractor Predicting: 209it [02:18,  1.52it/s]Extractor Predicting: 210it [02:19,  1.53it/s]Extractor Predicting: 211it [02:20,  1.51it/s]Extractor Predicting: 212it [02:20,  1.55it/s]Extractor Predicting: 213it [02:21,  1.54it/s]Extractor Predicting: 214it [02:21,  1.53it/s]Extractor Predicting: 215it [02:22,  1.49it/s]Extractor Predicting: 216it [02:23,  1.47it/s]Extractor Predicting: 217it [02:24,  1.48it/s]Extractor Predicting: 218it [02:24,  1.49it/s]Extractor Predicting: 219it [02:25,  1.48it/s]Extractor Predicting: 220it [02:26,  1.48it/s]Extractor Predicting: 221it [02:26,  1.52it/s]Extractor Predicting: 222it [02:27,  1.50it/s]Extractor Predicting: 223it [02:27,  1.52it/s]Extractor Predicting: 224it [02:28,  1.52it/s]Extractor Predicting: 225it [02:29,  1.53it/s]Extractor Predicting: 226it [02:29,  1.52it/s]Extractor Predicting: 227it [02:30,  1.47it/s]Extractor Predicting: 228it [02:31,  1.49it/s]Extractor Predicting: 229it [02:32,  1.49it/s]Extractor Predicting: 230it [02:32,  1.51it/s]Extractor Predicting: 231it [02:33,  1.55it/s]Extractor Predicting: 232it [02:33,  1.54it/s]Extractor Predicting: 233it [02:34,  1.51it/s]Extractor Predicting: 234it [02:35,  1.55it/s]Extractor Predicting: 235it [02:35,  1.57it/s]Extractor Predicting: 236it [02:36,  1.54it/s]Extractor Predicting: 237it [02:37,  1.51it/s]Extractor Predicting: 238it [02:37,  1.52it/s]Extractor Predicting: 239it [02:38,  1.53it/s]Extractor Predicting: 240it [02:39,  1.55it/s]Extractor Predicting: 241it [02:39,  1.57it/s]Extractor Predicting: 242it [02:40,  1.53it/s]Extractor Predicting: 243it [02:41,  1.50it/s]Extractor Predicting: 244it [02:41,  1.53it/s]Extractor Predicting: 245it [02:42,  1.56it/s]Extractor Predicting: 246it [02:43,  1.54it/s]Extractor Predicting: 247it [02:43,  1.51it/s]Extractor Predicting: 248it [02:44,  1.54it/s]Extractor Predicting: 249it [02:44,  1.56it/s]Extractor Predicting: 250it [02:45,  1.55it/s]Extractor Predicting: 251it [02:46,  1.58it/s]Extractor Predicting: 252it [02:46,  1.59it/s]Extractor Predicting: 253it [02:47,  1.58it/s]Extractor Predicting: 254it [02:48,  1.59it/s]Extractor Predicting: 255it [02:48,  1.57it/s]Extractor Predicting: 256it [02:49,  1.56it/s]Extractor Predicting: 257it [02:50,  1.53it/s]Extractor Predicting: 258it [02:50,  1.54it/s]Extractor Predicting: 259it [02:51,  1.53it/s]Extractor Predicting: 260it [02:52,  1.55it/s]Extractor Predicting: 261it [02:52,  1.55it/s]Extractor Predicting: 262it [02:53,  1.57it/s]Extractor Predicting: 263it [02:53,  1.56it/s]Extractor Predicting: 264it [02:54,  1.54it/s]Extractor Predicting: 265it [02:55,  1.51it/s]Extractor Predicting: 266it [02:55,  1.54it/s]Extractor Predicting: 267it [02:56,  1.56it/s]Extractor Predicting: 268it [02:57,  1.55it/s]Extractor Predicting: 269it [02:57,  1.49it/s]Extractor Predicting: 270it [02:58,  1.55it/s]Extractor Predicting: 271it [02:59,  1.56it/s]Extractor Predicting: 272it [02:59,  1.53it/s]Extractor Predicting: 273it [03:00,  1.53it/s]Extractor Predicting: 274it [03:01,  1.52it/s]Extractor Predicting: 275it [03:01,  1.51it/s]Extractor Predicting: 276it [03:02,  1.52it/s]Extractor Predicting: 277it [03:03,  1.54it/s]Extractor Predicting: 278it [03:03,  1.50it/s]Extractor Predicting: 279it [03:04,  1.48it/s]Extractor Predicting: 280it [03:05,  1.48it/s]Extractor Predicting: 281it [03:05,  1.48it/s]Extractor Predicting: 282it [03:06,  1.48it/s]Extractor Predicting: 283it [03:07,  1.52it/s]Extractor Predicting: 284it [03:07,  1.45it/s]Extractor Predicting: 285it [03:08,  1.47it/s]Extractor Predicting: 286it [03:09,  1.49it/s]Extractor Predicting: 287it [03:09,  1.50it/s]Extractor Predicting: 288it [03:10,  1.48it/s]Extractor Predicting: 289it [03:11,  1.50it/s]Extractor Predicting: 290it [03:12,  1.42it/s]Extractor Predicting: 291it [03:12,  1.48it/s]Extractor Predicting: 292it [03:13,  1.45it/s]Extractor Predicting: 293it [03:13,  1.48it/s]Extractor Predicting: 294it [03:14,  1.52it/s]Extractor Predicting: 295it [03:15,  1.50it/s]Extractor Predicting: 296it [03:15,  1.53it/s]Extractor Predicting: 297it [03:16,  1.55it/s]Extractor Predicting: 298it [03:17,  1.57it/s]Extractor Predicting: 299it [03:17,  1.52it/s]Extractor Predicting: 300it [03:18,  1.52it/s]Extractor Predicting: 301it [03:19,  1.45it/s]Extractor Predicting: 302it [03:19,  1.46it/s]Extractor Predicting: 303it [03:20,  1.32it/s]Extractor Predicting: 304it [03:21,  1.34it/s]Extractor Predicting: 305it [03:22,  1.36it/s]Extractor Predicting: 306it [03:22,  1.40it/s]Extractor Predicting: 307it [03:23,  1.43it/s]Extractor Predicting: 308it [03:24,  1.48it/s]Extractor Predicting: 309it [03:24,  1.47it/s]Extractor Predicting: 310it [03:25,  1.45it/s]Extractor Predicting: 311it [03:26,  1.46it/s]Extractor Predicting: 312it [03:27,  1.45it/s]Extractor Predicting: 313it [03:27,  1.45it/s]Extractor Predicting: 314it [03:28,  1.42it/s]Extractor Predicting: 315it [03:29,  1.40it/s]Extractor Predicting: 316it [03:29,  1.44it/s]Extractor Predicting: 317it [03:30,  1.47it/s]Extractor Predicting: 318it [03:31,  1.51it/s]Extractor Predicting: 319it [03:31,  1.51it/s]Extractor Predicting: 320it [03:32,  1.55it/s]Extractor Predicting: 321it [03:33,  1.55it/s]Extractor Predicting: 322it [03:33,  1.54it/s]Extractor Predicting: 323it [03:34,  1.53it/s]Extractor Predicting: 324it [03:34,  1.56it/s]Extractor Predicting: 325it [03:35,  1.51it/s]Extractor Predicting: 326it [03:36,  1.51it/s]Extractor Predicting: 327it [03:37,  1.48it/s]Extractor Predicting: 328it [03:37,  1.45it/s]Extractor Predicting: 329it [03:38,  1.49it/s]Extractor Predicting: 330it [03:39,  1.52it/s]Extractor Predicting: 331it [03:39,  1.54it/s]Extractor Predicting: 332it [03:40,  1.55it/s]Extractor Predicting: 333it [03:40,  1.52it/s]Extractor Predicting: 334it [03:41,  1.51it/s]Extractor Predicting: 335it [03:42,  1.50it/s]Extractor Predicting: 336it [03:42,  1.50it/s]Extractor Predicting: 337it [03:43,  1.52it/s]Extractor Predicting: 338it [03:44,  1.54it/s]Extractor Predicting: 339it [03:44,  1.55it/s]Extractor Predicting: 340it [03:45,  1.57it/s]Extractor Predicting: 341it [03:46,  1.55it/s]Extractor Predicting: 342it [03:46,  1.56it/s]Extractor Predicting: 343it [03:47,  1.59it/s]Extractor Predicting: 344it [03:48,  1.57it/s]Extractor Predicting: 345it [03:48,  1.56it/s]Extractor Predicting: 346it [03:49,  1.50it/s]Extractor Predicting: 347it [03:50,  1.52it/s]Extractor Predicting: 348it [03:50,  1.52it/s]Extractor Predicting: 349it [03:51,  1.53it/s]Extractor Predicting: 350it [03:52,  1.54it/s]Extractor Predicting: 351it [03:52,  1.53it/s]Extractor Predicting: 352it [03:53,  1.57it/s]Extractor Predicting: 353it [03:53,  1.56it/s]Extractor Predicting: 354it [03:54,  1.56it/s]Extractor Predicting: 355it [03:55,  1.54it/s]Extractor Predicting: 356it [03:55,  1.53it/s]Extractor Predicting: 357it [03:56,  1.51it/s]Extractor Predicting: 358it [03:57,  1.53it/s]Extractor Predicting: 359it [03:57,  1.52it/s]Extractor Predicting: 360it [03:58,  1.53it/s]Extractor Predicting: 361it [03:59,  1.55it/s]Extractor Predicting: 362it [03:59,  1.53it/s]Extractor Predicting: 363it [04:00,  1.56it/s]Extractor Predicting: 364it [04:01,  1.55it/s]Extractor Predicting: 365it [04:01,  1.56it/s]Extractor Predicting: 366it [04:02,  1.53it/s]Extractor Predicting: 367it [04:03,  1.54it/s]Extractor Predicting: 368it [04:03,  1.52it/s]Extractor Predicting: 369it [04:04,  1.51it/s]Extractor Predicting: 370it [04:05,  1.55it/s]Extractor Predicting: 371it [04:05,  1.53it/s]Extractor Predicting: 372it [04:06,  1.56it/s]Extractor Predicting: 373it [04:06,  1.57it/s]Extractor Predicting: 374it [04:07,  1.57it/s]Extractor Predicting: 375it [04:08,  1.57it/s]Extractor Predicting: 376it [04:08,  1.53it/s]Extractor Predicting: 377it [04:09,  1.53it/s]Extractor Predicting: 378it [04:10,  1.53it/s]Extractor Predicting: 379it [04:10,  1.56it/s]Extractor Predicting: 380it [04:11,  1.55it/s]Extractor Predicting: 381it [04:12,  1.56it/s]Extractor Predicting: 382it [04:12,  1.54it/s]Extractor Predicting: 383it [04:13,  1.55it/s]Extractor Predicting: 384it [04:14,  1.52it/s]Extractor Predicting: 385it [04:14,  1.51it/s]Extractor Predicting: 386it [04:15,  1.52it/s]Extractor Predicting: 387it [04:16,  1.54it/s]Extractor Predicting: 388it [04:16,  1.50it/s]Extractor Predicting: 389it [04:17,  1.48it/s]Extractor Predicting: 390it [04:18,  1.43it/s]Extractor Predicting: 391it [04:18,  1.46it/s]Extractor Predicting: 392it [04:19,  1.46it/s]Extractor Predicting: 393it [04:20,  1.47it/s]Extractor Predicting: 394it [04:20,  1.51it/s]Extractor Predicting: 395it [04:21,  1.52it/s]Extractor Predicting: 396it [04:22,  1.52it/s]Extractor Predicting: 397it [04:22,  1.54it/s]Extractor Predicting: 398it [04:23,  1.54it/s]Extractor Predicting: 399it [04:23,  1.57it/s]Extractor Predicting: 400it [04:24,  1.56it/s]Extractor Predicting: 401it [04:25,  1.56it/s]Extractor Predicting: 402it [04:25,  1.56it/s]Extractor Predicting: 403it [04:26,  1.51it/s]Extractor Predicting: 404it [04:27,  1.52it/s]Extractor Predicting: 405it [04:27,  1.53it/s]Extractor Predicting: 406it [04:28,  1.53it/s]Extractor Predicting: 407it [04:29,  1.51it/s]Extractor Predicting: 408it [04:29,  1.50it/s]Extractor Predicting: 409it [04:30,  1.50it/s]Extractor Predicting: 410it [04:31,  1.53it/s]Extractor Predicting: 411it [04:31,  1.52it/s]Extractor Predicting: 412it [04:32,  1.51it/s]Extractor Predicting: 413it [04:33,  1.45it/s]Extractor Predicting: 414it [04:33,  1.48it/s]Extractor Predicting: 415it [04:34,  1.53it/s]Extractor Predicting: 416it [04:35,  1.54it/s]Extractor Predicting: 417it [04:35,  1.58it/s]Extractor Predicting: 418it [04:36,  1.54it/s]Extractor Predicting: 419it [04:37,  1.57it/s]Extractor Predicting: 420it [04:37,  1.55it/s]Extractor Predicting: 421it [04:38,  1.54it/s]Extractor Predicting: 422it [04:39,  1.56it/s]Extractor Predicting: 423it [04:39,  1.53it/s]Extractor Predicting: 424it [04:40,  1.54it/s]Extractor Predicting: 425it [04:40,  1.58it/s]Extractor Predicting: 426it [04:41,  1.58it/s]Extractor Predicting: 427it [04:42,  1.58it/s]Extractor Predicting: 428it [04:42,  1.51it/s]Extractor Predicting: 429it [04:43,  1.53it/s]Extractor Predicting: 430it [04:44,  1.57it/s]Extractor Predicting: 431it [04:44,  1.53it/s]Extractor Predicting: 432it [04:45,  1.56it/s]Extractor Predicting: 433it [04:46,  1.58it/s]Extractor Predicting: 434it [04:46,  1.59it/s]Extractor Predicting: 435it [04:47,  1.59it/s]Extractor Predicting: 436it [04:47,  1.60it/s]Extractor Predicting: 437it [04:48,  1.60it/s]Extractor Predicting: 438it [04:49,  1.60it/s]Extractor Predicting: 439it [04:49,  1.60it/s]Extractor Predicting: 440it [04:50,  1.56it/s]Extractor Predicting: 441it [04:51,  1.57it/s]Extractor Predicting: 442it [04:51,  1.55it/s]Extractor Predicting: 443it [04:52,  1.57it/s]Extractor Predicting: 444it [04:53,  1.57it/s]Extractor Predicting: 445it [04:53,  1.55it/s]Extractor Predicting: 446it [04:54,  1.50it/s]Extractor Predicting: 447it [04:55,  1.52it/s]Extractor Predicting: 448it [04:55,  1.55it/s]Extractor Predicting: 449it [04:56,  1.54it/s]Extractor Predicting: 450it [04:56,  1.56it/s]Extractor Predicting: 451it [04:57,  1.55it/s]Extractor Predicting: 452it [04:58,  1.56it/s]Extractor Predicting: 453it [04:59,  1.33it/s]Extractor Predicting: 454it [04:59,  1.36it/s]Extractor Predicting: 455it [05:00,  1.37it/s]Extractor Predicting: 456it [05:01,  1.37it/s]Extractor Predicting: 457it [05:02,  1.39it/s]Extractor Predicting: 458it [05:02,  1.41it/s]Extractor Predicting: 459it [05:03,  1.44it/s]Extractor Predicting: 460it [05:04,  1.46it/s]Extractor Predicting: 461it [05:04,  1.45it/s]Extractor Predicting: 462it [05:05,  1.43it/s]Extractor Predicting: 463it [05:06,  1.46it/s]Extractor Predicting: 464it [05:06,  1.44it/s]Extractor Predicting: 465it [05:07,  1.47it/s]Extractor Predicting: 466it [05:08,  1.44it/s]Extractor Predicting: 467it [05:08,  1.48it/s]Extractor Predicting: 468it [05:09,  1.44it/s]Extractor Predicting: 469it [05:10,  1.43it/s]Extractor Predicting: 470it [05:11,  1.45it/s]Extractor Predicting: 471it [05:11,  1.43it/s]Extractor Predicting: 472it [05:12,  1.41it/s]Extractor Predicting: 473it [05:13,  1.42it/s]Extractor Predicting: 474it [05:13,  1.44it/s]Extractor Predicting: 475it [05:14,  1.44it/s]Extractor Predicting: 476it [05:15,  1.42it/s]Extractor Predicting: 477it [05:15,  1.43it/s]Extractor Predicting: 478it [05:16,  1.45it/s]Extractor Predicting: 479it [05:17,  1.44it/s]Extractor Predicting: 480it [05:18,  1.46it/s]Extractor Predicting: 481it [05:18,  1.47it/s]Extractor Predicting: 482it [05:19,  1.47it/s]Extractor Predicting: 483it [05:20,  1.44it/s]Extractor Predicting: 484it [05:20,  1.42it/s]Extractor Predicting: 485it [05:21,  1.43it/s]Extractor Predicting: 486it [05:22,  1.47it/s]Extractor Predicting: 487it [05:22,  1.47it/s]Extractor Predicting: 488it [05:23,  1.47it/s]Extractor Predicting: 489it [05:24,  1.45it/s]Extractor Predicting: 490it [05:24,  1.46it/s]Extractor Predicting: 491it [05:25,  1.45it/s]Extractor Predicting: 492it [05:26,  1.42it/s]Extractor Predicting: 493it [05:27,  1.42it/s]Extractor Predicting: 494it [05:27,  1.47it/s]Extractor Predicting: 495it [05:28,  1.40it/s]Extractor Predicting: 496it [05:29,  1.43it/s]Extractor Predicting: 497it [05:29,  1.44it/s]Extractor Predicting: 498it [05:30,  1.48it/s]Extractor Predicting: 499it [05:31,  1.49it/s]Extractor Predicting: 500it [05:31,  1.43it/s]Extractor Predicting: 501it [05:32,  1.43it/s]Extractor Predicting: 502it [05:33,  1.41it/s]Extractor Predicting: 503it [05:33,  1.42it/s]Extractor Predicting: 504it [05:34,  1.43it/s]Extractor Predicting: 505it [05:35,  1.45it/s]Extractor Predicting: 506it [05:36,  1.43it/s]Extractor Predicting: 507it [05:36,  1.46it/s]Extractor Predicting: 508it [05:37,  1.48it/s]Extractor Predicting: 509it [05:37,  1.50it/s]Extractor Predicting: 510it [05:38,  1.51it/s]Extractor Predicting: 511it [05:39,  1.50it/s]Extractor Predicting: 512it [05:39,  1.50it/s]Extractor Predicting: 513it [05:40,  1.48it/s]Extractor Predicting: 514it [05:41,  1.49it/s]Extractor Predicting: 515it [05:42,  1.48it/s]Extractor Predicting: 516it [05:42,  1.46it/s]Extractor Predicting: 517it [05:43,  1.41it/s]Extractor Predicting: 518it [05:44,  1.42it/s]Extractor Predicting: 519it [05:44,  1.45it/s]Extractor Predicting: 520it [05:45,  1.50it/s]Extractor Predicting: 521it [05:46,  1.50it/s]Extractor Predicting: 522it [05:46,  1.49it/s]Extractor Predicting: 523it [05:47,  1.48it/s]Extractor Predicting: 524it [05:48,  1.48it/s]Extractor Predicting: 525it [05:48,  1.45it/s]Extractor Predicting: 526it [05:49,  1.41it/s]Extractor Predicting: 527it [05:50,  1.43it/s]Extractor Predicting: 528it [05:50,  1.46it/s]Extractor Predicting: 529it [05:51,  1.46it/s]Extractor Predicting: 530it [05:52,  1.43it/s]Extractor Predicting: 531it [05:53,  1.46it/s]Extractor Predicting: 532it [05:53,  1.42it/s]Extractor Predicting: 533it [05:54,  1.44it/s]Extractor Predicting: 534it [05:55,  1.45it/s]Extractor Predicting: 535it [05:55,  1.46it/s]Extractor Predicting: 536it [05:56,  1.42it/s]Extractor Predicting: 537it [05:57,  1.44it/s]Extractor Predicting: 538it [05:57,  1.44it/s]Extractor Predicting: 539it [05:58,  1.44it/s]Extractor Predicting: 540it [05:59,  1.43it/s]Extractor Predicting: 541it [06:00,  1.36it/s]Extractor Predicting: 542it [06:00,  1.38it/s]Extractor Predicting: 543it [06:01,  1.40it/s]Extractor Predicting: 544it [06:02,  1.43it/s]Extractor Predicting: 545it [06:02,  1.40it/s]Extractor Predicting: 546it [06:03,  1.42it/s]Extractor Predicting: 547it [06:04,  1.42it/s]Extractor Predicting: 548it [06:04,  1.46it/s]Extractor Predicting: 549it [06:05,  1.42it/s]Extractor Predicting: 550it [06:06,  1.42it/s]Extractor Predicting: 551it [06:07,  1.44it/s]Extractor Predicting: 552it [06:07,  1.43it/s]Extractor Predicting: 553it [06:08,  1.46it/s]Extractor Predicting: 554it [06:09,  1.46it/s]Extractor Predicting: 555it [06:09,  1.45it/s]Extractor Predicting: 556it [06:10,  1.46it/s]Extractor Predicting: 557it [06:11,  1.44it/s]Extractor Predicting: 558it [06:11,  1.43it/s]Extractor Predicting: 559it [06:12,  1.45it/s]Extractor Predicting: 560it [06:13,  1.48it/s]Extractor Predicting: 561it [06:13,  1.46it/s]Extractor Predicting: 562it [06:14,  1.48it/s]Extractor Predicting: 563it [06:15,  1.47it/s]Extractor Predicting: 564it [06:16,  1.45it/s]Extractor Predicting: 565it [06:16,  1.44it/s]Extractor Predicting: 566it [06:17,  1.44it/s]Extractor Predicting: 567it [06:18,  1.45it/s]Extractor Predicting: 568it [06:18,  1.51it/s]Extractor Predicting: 569it [06:19,  1.48it/s]Extractor Predicting: 570it [06:20,  1.46it/s]Extractor Predicting: 571it [06:20,  1.45it/s]Extractor Predicting: 572it [06:21,  1.42it/s]Extractor Predicting: 573it [06:22,  1.39it/s]Extractor Predicting: 574it [06:23,  1.37it/s]Extractor Predicting: 575it [06:23,  1.39it/s]Extractor Predicting: 576it [06:24,  1.42it/s]Extractor Predicting: 577it [06:24,  1.74it/s]Extractor Predicting: 577it [06:24,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:27:49,121 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:27:49,178 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:27:49,178 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:27:49,178 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:27:49,178 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:27:49,787 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:27:49,788 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:27:50,107 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:27:51,250 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:27:51,250 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:27:54,210 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:27:54,234 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:27:54,234 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:27:54,234 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:27:54,234 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:27:55,011 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:27:55,012 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:27:55,619 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:27:55,844 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:27:55,845 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2503288031565103,
  "recall": 0.041293028637547004,
  "score": 0.070892047923521,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_15_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'employer', 'field of work', 'languages spoken, written or signed', 'lyrics by', 'manufacturer', 'member of sports team', 'occupation', 'performer', 'residence', 'shares border with', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 11382
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11482, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.50it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.60it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:09,  1.58it/s]Extractor Predicting: 15it [00:09,  1.51it/s]Extractor Predicting: 16it [00:10,  1.50it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:11,  1.44it/s]Extractor Predicting: 19it [00:12,  1.45it/s]Extractor Predicting: 20it [00:13,  1.40it/s]Extractor Predicting: 21it [00:14,  1.43it/s]Extractor Predicting: 22it [00:14,  1.44it/s]Extractor Predicting: 23it [00:15,  1.44it/s]Extractor Predicting: 24it [00:16,  1.43it/s]Extractor Predicting: 25it [00:16,  1.37it/s]Extractor Predicting: 26it [00:17,  1.31it/s]Extractor Predicting: 27it [00:18,  1.34it/s]Extractor Predicting: 28it [00:19,  1.35it/s]Extractor Predicting: 29it [00:19,  1.33it/s]Extractor Predicting: 30it [00:20,  1.35it/s]Extractor Predicting: 31it [00:21,  1.40it/s]Extractor Predicting: 32it [00:21,  1.44it/s]Extractor Predicting: 33it [00:22,  1.43it/s]Extractor Predicting: 34it [00:23,  1.39it/s]Extractor Predicting: 35it [00:24,  1.45it/s]Extractor Predicting: 36it [00:24,  1.46it/s]Extractor Predicting: 37it [00:25,  1.46it/s]Extractor Predicting: 38it [00:26,  1.43it/s]Extractor Predicting: 39it [00:26,  1.37it/s]Extractor Predicting: 40it [00:27,  1.36it/s]Extractor Predicting: 41it [00:28,  1.38it/s]Extractor Predicting: 42it [00:29,  1.40it/s]Extractor Predicting: 43it [00:29,  1.40it/s]Extractor Predicting: 44it [00:30,  1.32it/s]Extractor Predicting: 45it [00:31,  1.34it/s]Extractor Predicting: 46it [00:32,  1.38it/s]Extractor Predicting: 47it [00:32,  1.40it/s]Extractor Predicting: 48it [00:33,  1.40it/s]Extractor Predicting: 49it [00:34,  1.40it/s]Extractor Predicting: 50it [00:34,  1.40it/s]Extractor Predicting: 51it [00:35,  1.42it/s]Extractor Predicting: 52it [00:36,  1.39it/s]Extractor Predicting: 53it [00:37,  1.41it/s]Extractor Predicting: 54it [00:37,  1.45it/s]Extractor Predicting: 55it [00:38,  1.48it/s]Extractor Predicting: 56it [00:38,  1.50it/s]Extractor Predicting: 57it [00:39,  1.50it/s]Extractor Predicting: 58it [00:40,  1.50it/s]Extractor Predicting: 59it [00:40,  1.51it/s]Extractor Predicting: 60it [00:41,  1.46it/s]Extractor Predicting: 61it [00:42,  1.50it/s]Extractor Predicting: 62it [00:43,  1.48it/s]Extractor Predicting: 63it [00:43,  1.45it/s]Extractor Predicting: 64it [00:44,  1.41it/s]Extractor Predicting: 65it [00:45,  1.41it/s]Extractor Predicting: 66it [00:45,  1.42it/s]Extractor Predicting: 67it [00:46,  1.41it/s]Extractor Predicting: 68it [00:47,  1.40it/s]Extractor Predicting: 69it [00:48,  1.40it/s]Extractor Predicting: 70it [00:48,  1.41it/s]Extractor Predicting: 71it [00:49,  1.43it/s]Extractor Predicting: 72it [00:50,  1.44it/s]Extractor Predicting: 73it [00:50,  1.45it/s]Extractor Predicting: 74it [00:51,  1.48it/s]Extractor Predicting: 75it [00:52,  1.48it/s]Extractor Predicting: 76it [00:52,  1.49it/s]Extractor Predicting: 77it [00:53,  1.50it/s]Extractor Predicting: 78it [00:54,  1.45it/s]Extractor Predicting: 79it [00:54,  1.49it/s]Extractor Predicting: 80it [00:55,  1.50it/s]Extractor Predicting: 81it [00:56,  1.51it/s]Extractor Predicting: 82it [00:56,  1.51it/s]Extractor Predicting: 83it [00:57,  1.48it/s]Extractor Predicting: 84it [00:58,  1.44it/s]Extractor Predicting: 85it [00:58,  1.48it/s]Extractor Predicting: 86it [00:59,  1.46it/s]Extractor Predicting: 87it [01:00,  1.45it/s]Extractor Predicting: 88it [01:00,  1.43it/s]Extractor Predicting: 89it [01:01,  1.40it/s]Extractor Predicting: 90it [01:02,  1.43it/s]Extractor Predicting: 91it [01:03,  1.41it/s]Extractor Predicting: 92it [01:03,  1.41it/s]Extractor Predicting: 93it [01:04,  1.40it/s]Extractor Predicting: 94it [01:05,  1.39it/s]Extractor Predicting: 95it [01:06,  1.37it/s]Extractor Predicting: 96it [01:06,  1.37it/s]Extractor Predicting: 97it [01:07,  1.39it/s]Extractor Predicting: 98it [01:08,  1.38it/s]Extractor Predicting: 99it [01:08,  1.39it/s]Extractor Predicting: 100it [01:09,  1.43it/s]Extractor Predicting: 101it [01:10,  1.43it/s]Extractor Predicting: 102it [01:11,  1.40it/s]Extractor Predicting: 103it [01:11,  1.37it/s]Extractor Predicting: 104it [01:12,  1.39it/s]Extractor Predicting: 105it [01:13,  1.41it/s]Extractor Predicting: 106it [01:13,  1.42it/s]Extractor Predicting: 107it [01:14,  1.38it/s]Extractor Predicting: 108it [01:15,  1.36it/s]Extractor Predicting: 109it [01:16,  1.23it/s]Extractor Predicting: 110it [01:17,  1.26it/s]Extractor Predicting: 110it [01:17,  1.43it/s]
[INFO|configuration_utils.py:515] 2023-08-29 12:29:16,273 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:29:16,274 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 12:29:16,312 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:29:16,313 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 12:29:16,330 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 12:29:29,486 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 12:29:29,533 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 12:29:29,807 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:29:29,808 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 12:29:29,917 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:29:29,975 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:29:29,976 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:29:29,976 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:29:29,976 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:29:29,976 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:29:29,976 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4652777777777778,
  "recall": 0.02168635701569833,
  "score": 0.04144116282665842,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 12:29:30,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:31,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:32,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:32,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:33,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:34,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:34,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:35,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:36,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:36,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:37,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:38,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:38,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:39,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:40,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:40,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:41,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:42,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:42,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:43,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:44,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:44,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:14<04:44, 15.00s/it][WARNING|generation_utils.py:914] 2023-08-29 12:29:45,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:46,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:46,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:47,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:48,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:48,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:49,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:50,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:50,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:51,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:52,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:52,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:53,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:54,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:54,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:55,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:55,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:56,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:57,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:57,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:58,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:29:59,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:29<04:22, 14.57s/it][WARNING|generation_utils.py:914] 2023-08-29 12:29:59,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:00,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:01,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:01,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:02,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:03,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:03,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:04,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:05,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:06,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:06,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:07,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:08,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:08,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:09,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:10,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:11,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:11,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:12,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:13,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:14,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:15,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:45<04:20, 15.30s/it][WARNING|generation_utils.py:914] 2023-08-29 12:30:15,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:16,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:17,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:17,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:18,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:18,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:19,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:20,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:20,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:21,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:21,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:22,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:23,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:23,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:24,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:25,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:25,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:26,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:26,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:27,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:57<03:43, 13.96s/it][WARNING|generation_utils.py:914] 2023-08-29 12:30:27,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:28,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:29,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:29,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:30,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:31,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:32,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:32,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:33,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:34,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:34,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:35,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:36,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:36,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:37,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:38,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:38,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:39,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:40,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:41,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:41,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:42,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:43,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:13<03:39, 14.65s/it][WARNING|generation_utils.py:914] 2023-08-29 12:30:43,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:44,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:45,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:45,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:46,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:47,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:48,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:48,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:49,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:50,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:51,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:51,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:52,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:53,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:53,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:54,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:55,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:56,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:57,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:57,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:27<03:25, 14.68s/it][WARNING|generation_utils.py:914] 2023-08-29 12:30:58,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:59,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:30:59,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:00,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:01,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:01,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:02,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:03,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:03,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:04,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:05,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:05,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:06,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:07,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:08,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:08,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:09,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:10,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:10,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:11,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:41<03:07, 14.39s/it][WARNING|generation_utils.py:914] 2023-08-29 12:31:12,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:12,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:13,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:14,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:15,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:15,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:16,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:17,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:17,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:18,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:19,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:20,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:20,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:21,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:22,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:22,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:23,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:24,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:24,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:25,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:55<02:50, 14.21s/it][WARNING|generation_utils.py:914] 2023-08-29 12:31:26,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:26,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:27,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:28,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:28,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:29,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:30,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:30,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:31,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:32,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:32,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:33,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:34,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:34,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:35,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:35,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:36,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:37,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:37,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:38,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:08<02:32, 13.87s/it][WARNING|generation_utils.py:914] 2023-08-29 12:31:39,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:40,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:40,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:41,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:42,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:43,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:43,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:44,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:45,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:46,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:46,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:47,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:48,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:49,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:49,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:50,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:51,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:51,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:52,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:53,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:54,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:24<02:24, 14.46s/it][WARNING|generation_utils.py:914] 2023-08-29 12:31:55,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:56,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:56,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:57,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:57,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:58,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:59,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:31:59,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:00,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:01,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:01,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:02,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:03,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:03,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:04,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:05,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:05,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:06,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:07,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:08,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:08,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:09,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:39<02:11, 14.66s/it][WARNING|generation_utils.py:914] 2023-08-29 12:32:10,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:10,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:11,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:12,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:13,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:13,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:14,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:15,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:15,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:16,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:17,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:17,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:18,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:19,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:19,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:20,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:21,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:22,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:22,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:23,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:24,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:54<01:57, 14.71s/it][WARNING|generation_utils.py:914] 2023-08-29 12:32:24,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:25,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:26,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:26,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:27,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:27,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:29,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:29,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:30,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:31,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:31,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:32,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:33,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:33,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:35,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:35,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:36,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:36,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:38,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:38,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:09<01:42, 14.68s/it][WARNING|generation_utils.py:914] 2023-08-29 12:32:39,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:40,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:40,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:41,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:42,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:42,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:43,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:43,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:44,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:44,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:45,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:46,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:46,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:47,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:47,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:48,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:48,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:49,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:50,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:20<01:21, 13.60s/it][WARNING|generation_utils.py:914] 2023-08-29 12:32:50,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:51,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:52,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:52,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:53,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:54,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:55,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:55,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:56,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:57,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:57,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:58,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:32:59,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:00,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:00,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:01,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:02,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:03,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:03,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:04,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:05,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:35<01:10, 14.10s/it][WARNING|generation_utils.py:914] 2023-08-29 12:33:05,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:06,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:07,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:07,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:08,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:09,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:09,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:10,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:11,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:11,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:12,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:13,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:13,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:14,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:15,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:15,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:16,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:17,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:18,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:18,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:49<00:55, 13.97s/it][WARNING|generation_utils.py:914] 2023-08-29 12:33:19,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:20,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:20,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:21,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:22,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:23,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:23,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:24,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:25,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:25,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:26,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:26,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:27,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:28,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:28,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:29,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:30,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:30,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:31,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:32,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:33,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:03<00:42, 14.03s/it][WARNING|generation_utils.py:914] 2023-08-29 12:33:33,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:34,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:35,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:35,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:36,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:37,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:38,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:38,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:39,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:40,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:41,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:41,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:42,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:42,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:43,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:44,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:44,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:45,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:46,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:46,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:47,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:17<00:28, 14.21s/it][WARNING|generation_utils.py:914] 2023-08-29 12:33:48,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:48,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:49,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:50,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:50,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:51,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:51,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:52,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:53,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:53,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:54,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:54,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:55,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:56,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:56,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:57,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:57,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:58,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:33:59,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:29<00:13, 13.31s/it][WARNING|generation_utils.py:914] 2023-08-29 12:33:59,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:00,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:01,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:02,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:03,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:03,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:04,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:05,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:06,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:06,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:07,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:08,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:09,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:10,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:11,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:12,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:13,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:13,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:14,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:15,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 12:34:16,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:46<00:00, 14.61s/it]Generating: 100%|██████████| 20/20 [04:46<00:00, 14.34s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:34:26,209 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:34:26,233 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:34:26,234 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:34:26,234 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:34:26,234 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:34:27,025 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:34:27,026 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:34:27,656 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:34:28,796 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:34:28,796 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:34:31,762 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:34:31,793 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:34:31,793 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:34:31,793 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:34:31,793 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:34:32,437 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:34:32,438 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:34:33,048 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:34:33,202 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:34:33,202 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : conflict .', 'success_rate': 0.8565340909090909, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : developer .', 'success_rate': 0.8835227272727273, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : given name .', 'success_rate': 0.8636363636363636, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 160, 'raw': 160}
{'target': 600, 'success': 191, 'raw': 192}
{'target': 600, 'success': 223, 'raw': 224}
{'target': 600, 'success': 255, 'raw': 256}
{'target': 600, 'success': 287, 'raw': 288}
{'target': 600, 'success': 319, 'raw': 320}
{'target': 600, 'success': 348, 'raw': 352}
{'target': 600, 'success': 379, 'raw': 384}
{'target': 600, 'success': 411, 'raw': 416}
{'target': 600, 'success': 442, 'raw': 448}
{'target': 600, 'success': 474, 'raw': 480}
{'target': 600, 'success': 504, 'raw': 512}
{'target': 600, 'success': 535, 'raw': 544}
{'target': 600, 'success': 567, 'raw': 576}
{'target': 600, 'success': 598, 'raw': 608}
{'target': 600, 'success': 630, 'raw': 640}
{'prompt': 'Relation : participant in .', 'success_rate': 0.984375, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : work location .', 'success_rate': 0.8274456521739131, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.95, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.9390625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : creator .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : employer .', 'success_rate': 0.940625, 'errors': {''}}
['Relation : field of work . Context : Later in 2008 , he moved to Paris , where he studied French philosophy with Pierre Bouillot , François Bouchard and Roger Pielkema , and became a leading critic of Bouchard s classic work . Head Entity : Jean Bouchard , Tail Entity : French philosophy .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9092261904761905, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.8636363636363636, 'errors': {'', "('Philippines', 'languages spoken, written or signed', '', 'It is also one of four official languages of the Philippines .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9285714285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 282, 'raw': 288}
{'target': 600, 'success': 313, 'raw': 320}
{'target': 600, 'success': 344, 'raw': 352}
{'target': 600, 'success': 376, 'raw': 384}
{'target': 600, 'success': 408, 'raw': 416}
{'target': 600, 'success': 438, 'raw': 448}
{'target': 600, 'success': 469, 'raw': 480}
{'target': 600, 'success': 501, 'raw': 512}
{'target': 600, 'success': 532, 'raw': 544}
{'target': 600, 'success': 563, 'raw': 576}
{'target': 600, 'success': 595, 'raw': 608}
{'target': 600, 'success': 624, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.975, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 159, 'raw': 160}
{'target': 600, 'success': 191, 'raw': 192}
{'target': 600, 'success': 223, 'raw': 224}
{'target': 600, 'success': 255, 'raw': 256}
{'target': 600, 'success': 287, 'raw': 288}
{'target': 600, 'success': 319, 'raw': 320}
{'target': 600, 'success': 351, 'raw': 352}
{'target': 600, 'success': 383, 'raw': 384}
{'target': 600, 'success': 414, 'raw': 416}
{'target': 600, 'success': 446, 'raw': 448}
{'target': 600, 'success': 478, 'raw': 480}
{'target': 600, 'success': 510, 'raw': 512}
{'target': 600, 'success': 542, 'raw': 544}
{'target': 600, 'success': 574, 'raw': 576}
{'target': 600, 'success': 606, 'raw': 608}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.9967105263157895, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.9122023809523809, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 466, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 590, 'raw': 608}
{'target': 600, 'success': 621, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.9703125, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : residence .', 'success_rate': 0.9196428571428571, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 160, 'raw': 160}
{'target': 600, 'success': 192, 'raw': 192}
{'target': 600, 'success': 224, 'raw': 224}
{'target': 600, 'success': 256, 'raw': 256}
{'target': 600, 'success': 288, 'raw': 288}
{'target': 600, 'success': 320, 'raw': 320}
{'target': 600, 'success': 352, 'raw': 352}
{'target': 600, 'success': 383, 'raw': 384}
{'target': 600, 'success': 415, 'raw': 416}
{'target': 600, 'success': 447, 'raw': 448}
{'target': 600, 'success': 479, 'raw': 480}
{'target': 600, 'success': 511, 'raw': 512}
{'target': 600, 'success': 543, 'raw': 544}
{'target': 600, 'success': 575, 'raw': 576}
{'target': 600, 'success': 607, 'raw': 608}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9983552631578947, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.9077380952380952, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/4_ext.jsonl'}}
estimate vocab size: 9335
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9435, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.57it/s]Extractor Estimating: 2it [00:01,  1.61it/s]Extractor Estimating: 3it [00:01,  1.61it/s]Extractor Estimating: 4it [00:02,  1.70it/s]Extractor Estimating: 5it [00:02,  1.77it/s]Extractor Estimating: 6it [00:03,  1.64it/s]Extractor Estimating: 7it [00:04,  1.65it/s]Extractor Estimating: 8it [00:04,  1.65it/s]Extractor Estimating: 9it [00:05,  1.61it/s]Extractor Estimating: 10it [00:06,  1.64it/s]Extractor Estimating: 11it [00:06,  1.69it/s]Extractor Estimating: 12it [00:07,  1.67it/s]Extractor Estimating: 13it [00:07,  1.72it/s]Extractor Estimating: 14it [00:08,  1.78it/s]Extractor Estimating: 15it [00:08,  1.74it/s]Extractor Estimating: 16it [00:09,  1.76it/s]Extractor Estimating: 17it [00:10,  1.76it/s]Extractor Estimating: 18it [00:10,  1.77it/s]Extractor Estimating: 19it [00:11,  1.75it/s]Extractor Estimating: 20it [00:11,  1.76it/s]Extractor Estimating: 21it [00:12,  1.72it/s]Extractor Estimating: 22it [00:12,  1.78it/s]Extractor Estimating: 23it [00:13,  1.79it/s]Extractor Estimating: 24it [00:14,  1.71it/s]Extractor Estimating: 25it [00:14,  1.69it/s]Extractor Estimating: 26it [00:15,  1.66it/s]Extractor Estimating: 27it [00:15,  1.63it/s]Extractor Estimating: 28it [00:16,  1.65it/s]Extractor Estimating: 29it [00:17,  1.68it/s]Extractor Estimating: 30it [00:17,  1.69it/s]Extractor Estimating: 31it [00:18,  1.72it/s]Extractor Estimating: 32it [00:18,  1.67it/s]Extractor Estimating: 33it [00:19,  1.64it/s]Extractor Estimating: 34it [00:20,  1.66it/s]Extractor Estimating: 35it [00:20,  1.73it/s]Extractor Estimating: 36it [00:21,  1.71it/s]Extractor Estimating: 37it [00:21,  1.70it/s]Extractor Estimating: 38it [00:22,  1.69it/s]Extractor Estimating: 39it [00:22,  1.69it/s]Extractor Estimating: 40it [00:23,  1.71it/s]Extractor Estimating: 41it [00:24,  1.73it/s]Extractor Estimating: 42it [00:24,  1.73it/s]Extractor Estimating: 43it [00:25,  1.69it/s]Extractor Estimating: 44it [00:25,  1.72it/s]Extractor Estimating: 45it [00:26,  1.72it/s]Extractor Estimating: 46it [00:27,  1.73it/s]Extractor Estimating: 47it [00:27,  1.70it/s]Extractor Estimating: 48it [00:28,  1.74it/s]Extractor Estimating: 49it [00:28,  1.73it/s]Extractor Estimating: 50it [00:29,  1.71it/s]Extractor Estimating: 51it [00:29,  1.67it/s]Extractor Estimating: 52it [00:30,  1.66it/s]Extractor Estimating: 53it [00:31,  1.65it/s]Extractor Estimating: 54it [00:31,  1.66it/s]Extractor Estimating: 55it [00:32,  1.67it/s]Extractor Estimating: 56it [00:32,  1.71it/s]Extractor Estimating: 57it [00:33,  1.71it/s]Extractor Estimating: 58it [00:34,  1.74it/s]Extractor Estimating: 59it [00:34,  1.69it/s]Extractor Estimating: 60it [00:35,  1.69it/s]Extractor Estimating: 61it [00:35,  1.68it/s]Extractor Estimating: 62it [00:36,  1.66it/s]Extractor Estimating: 63it [00:37,  1.63it/s]Extractor Estimating: 64it [00:37,  1.68it/s]Extractor Estimating: 65it [00:38,  1.65it/s]Extractor Estimating: 66it [00:38,  1.66it/s]Extractor Estimating: 67it [00:39,  1.61it/s]Extractor Estimating: 68it [00:40,  1.57it/s]Extractor Estimating: 69it [00:40,  1.65it/s]Extractor Estimating: 70it [00:41,  1.63it/s]Extractor Estimating: 71it [00:42,  1.58it/s]Extractor Estimating: 72it [00:42,  1.60it/s]Extractor Estimating: 73it [00:43,  1.59it/s]Extractor Estimating: 74it [00:44,  1.55it/s]Extractor Estimating: 75it [00:44,  1.60it/s]Extractor Estimating: 76it [00:45,  1.76it/s]Extractor Estimating: 77it [00:45,  1.85it/s]Extractor Estimating: 78it [00:46,  1.91it/s]Extractor Estimating: 79it [00:46,  2.00it/s]Extractor Estimating: 80it [00:47,  1.91it/s]Extractor Estimating: 81it [00:47,  1.97it/s]Extractor Estimating: 82it [00:47,  2.06it/s]Extractor Estimating: 83it [00:48,  2.04it/s]Extractor Estimating: 84it [00:48,  2.10it/s]Extractor Estimating: 85it [00:49,  2.03it/s]Extractor Estimating: 86it [00:49,  2.01it/s]Extractor Estimating: 87it [00:50,  2.01it/s]Extractor Estimating: 88it [00:50,  2.00it/s]Extractor Estimating: 89it [00:51,  1.97it/s]Extractor Estimating: 90it [00:51,  1.98it/s]Extractor Estimating: 91it [00:52,  1.91it/s]Extractor Estimating: 92it [00:53,  1.94it/s]Extractor Estimating: 93it [00:53,  1.97it/s]Extractor Estimating: 94it [00:54,  1.94it/s]Extractor Estimating: 95it [00:54,  2.00it/s]Extractor Estimating: 96it [00:55,  1.90it/s]Extractor Estimating: 97it [00:55,  1.99it/s]Extractor Estimating: 98it [00:56,  2.04it/s]Extractor Estimating: 99it [00:56,  2.07it/s]Extractor Estimating: 100it [00:57,  1.98it/s]Extractor Estimating: 101it [00:57,  1.87it/s]Extractor Estimating: 102it [00:58,  1.78it/s]Extractor Estimating: 103it [00:58,  1.71it/s]Extractor Estimating: 104it [00:59,  1.71it/s]Extractor Estimating: 105it [01:00,  1.66it/s]Extractor Estimating: 106it [01:00,  1.64it/s]Extractor Estimating: 107it [01:01,  1.42it/s]Extractor Estimating: 108it [01:02,  1.45it/s]Extractor Estimating: 109it [01:02,  1.49it/s]Extractor Estimating: 110it [01:03,  1.53it/s]Extractor Estimating: 111it [01:04,  1.55it/s]Extractor Estimating: 112it [01:04,  1.60it/s]Extractor Estimating: 113it [01:05,  1.64it/s]Extractor Estimating: 114it [01:05,  1.64it/s]Extractor Estimating: 115it [01:06,  1.59it/s]Extractor Estimating: 116it [01:07,  1.55it/s]Extractor Estimating: 117it [01:07,  1.57it/s]Extractor Estimating: 118it [01:08,  1.62it/s]Extractor Estimating: 119it [01:09,  1.63it/s]Extractor Estimating: 120it [01:09,  1.55it/s]Extractor Estimating: 121it [01:10,  1.59it/s]Extractor Estimating: 122it [01:11,  1.60it/s]Extractor Estimating: 123it [01:11,  1.66it/s]Extractor Estimating: 124it [01:12,  1.63it/s]Extractor Estimating: 125it [01:12,  1.59it/s]Extractor Estimating: 126it [01:13,  1.65it/s]Extractor Estimating: 127it [01:14,  1.64it/s]Extractor Estimating: 128it [01:14,  1.64it/s]Extractor Estimating: 129it [01:15,  1.65it/s]Extractor Estimating: 130it [01:15,  1.60it/s]Extractor Estimating: 131it [01:16,  1.63it/s]Extractor Estimating: 132it [01:17,  1.60it/s]Extractor Estimating: 133it [01:17,  1.64it/s]Extractor Estimating: 134it [01:18,  1.67it/s]Extractor Estimating: 135it [01:19,  1.59it/s]Extractor Estimating: 136it [01:19,  1.63it/s]Extractor Estimating: 137it [01:20,  1.64it/s]Extractor Estimating: 138it [01:20,  1.66it/s]Extractor Estimating: 139it [01:21,  1.60it/s]Extractor Estimating: 140it [01:22,  1.55it/s]Extractor Estimating: 141it [01:22,  1.57it/s]Extractor Estimating: 142it [01:23,  1.61it/s]Extractor Estimating: 143it [01:24,  1.60it/s]Extractor Estimating: 144it [01:24,  1.63it/s]Extractor Estimating: 145it [01:25,  1.52it/s]Extractor Estimating: 146it [01:26,  1.54it/s]Extractor Estimating: 147it [01:26,  1.53it/s]Extractor Estimating: 148it [01:27,  1.53it/s]Extractor Estimating: 149it [01:27,  1.57it/s]Extractor Estimating: 150it [01:28,  1.55it/s]Extractor Estimating: 151it [01:29,  1.59it/s]Extractor Estimating: 152it [01:29,  1.55it/s]Extractor Estimating: 153it [01:30,  1.57it/s]Extractor Estimating: 154it [01:31,  1.57it/s]Extractor Estimating: 155it [01:31,  1.51it/s]Extractor Estimating: 156it [01:32,  1.55it/s]Extractor Estimating: 157it [01:33,  1.54it/s]Extractor Estimating: 158it [01:33,  1.55it/s]Extractor Estimating: 159it [01:34,  1.54it/s]Extractor Estimating: 160it [01:35,  1.56it/s]Extractor Estimating: 161it [01:35,  1.58it/s]Extractor Estimating: 162it [01:36,  1.60it/s]Extractor Estimating: 163it [01:36,  1.60it/s]Extractor Estimating: 164it [01:37,  1.49it/s]Extractor Estimating: 165it [01:38,  1.52it/s]Extractor Estimating: 166it [01:38,  1.53it/s]Extractor Estimating: 167it [01:39,  1.53it/s]Extractor Estimating: 168it [01:40,  1.55it/s]Extractor Estimating: 169it [01:40,  1.49it/s]Extractor Estimating: 170it [01:41,  1.51it/s]Extractor Estimating: 171it [01:42,  1.54it/s]Extractor Estimating: 172it [01:42,  1.55it/s]Extractor Estimating: 173it [01:43,  1.54it/s]Extractor Estimating: 174it [01:44,  1.51it/s]Extractor Estimating: 175it [01:44,  1.54it/s]Extractor Estimating: 176it [01:45,  1.57it/s]Extractor Estimating: 177it [01:46,  1.59it/s]Extractor Estimating: 178it [01:46,  1.58it/s]Extractor Estimating: 179it [01:47,  1.58it/s]Extractor Estimating: 180it [01:47,  1.62it/s]Extractor Estimating: 181it [01:48,  1.62it/s]Extractor Estimating: 182it [01:49,  1.62it/s]Extractor Estimating: 183it [01:49,  1.64it/s]Extractor Estimating: 184it [01:50,  1.63it/s]Extractor Estimating: 185it [01:50,  1.60it/s]Extractor Estimating: 186it [01:51,  1.66it/s]Extractor Estimating: 187it [01:52,  1.66it/s]Extractor Estimating: 188it [01:52,  1.65it/s]Extractor Estimating: 189it [01:53,  1.63it/s]Extractor Estimating: 190it [01:54,  1.47it/s]Extractor Estimating: 191it [01:54,  1.50it/s]Extractor Estimating: 192it [01:55,  1.56it/s]Extractor Estimating: 193it [01:56,  1.57it/s]Extractor Estimating: 194it [01:56,  1.58it/s]Extractor Estimating: 195it [01:57,  1.62it/s]Extractor Estimating: 196it [01:57,  1.63it/s]Extractor Estimating: 197it [01:58,  1.64it/s]Extractor Estimating: 198it [01:59,  1.65it/s]Extractor Estimating: 199it [01:59,  1.62it/s]Extractor Estimating: 200it [02:00,  1.64it/s]Extractor Estimating: 201it [02:00,  1.67it/s]Extractor Estimating: 202it [02:01,  1.70it/s]Extractor Estimating: 203it [02:01,  1.72it/s]Extractor Estimating: 204it [02:02,  1.72it/s]Extractor Estimating: 205it [02:03,  1.69it/s]Extractor Estimating: 206it [02:03,  1.75it/s]Extractor Estimating: 207it [02:04,  1.74it/s]Extractor Estimating: 208it [02:04,  1.71it/s]Extractor Estimating: 209it [02:05,  1.75it/s]Extractor Estimating: 210it [02:05,  1.77it/s]Extractor Estimating: 211it [02:06,  1.79it/s]Extractor Estimating: 212it [02:07,  1.78it/s]Extractor Estimating: 213it [02:07,  1.78it/s]Extractor Estimating: 214it [02:08,  1.80it/s]Extractor Estimating: 215it [02:08,  1.79it/s]Extractor Estimating: 216it [02:09,  1.81it/s]Extractor Estimating: 217it [02:09,  1.78it/s]Extractor Estimating: 218it [02:10,  1.87it/s]Extractor Estimating: 219it [02:10,  1.85it/s]Extractor Estimating: 220it [02:11,  1.80it/s]Extractor Estimating: 221it [02:12,  1.82it/s]Extractor Estimating: 222it [02:12,  1.84it/s]Extractor Estimating: 223it [02:13,  1.82it/s]Extractor Estimating: 224it [02:13,  1.82it/s]Extractor Estimating: 225it [02:14,  1.79it/s]Extractor Estimating: 226it [02:14,  1.69it/s]Extractor Estimating: 227it [02:15,  1.70it/s]Extractor Estimating: 228it [02:16,  1.65it/s]Extractor Estimating: 229it [02:16,  1.56it/s]Extractor Estimating: 230it [02:17,  1.57it/s]Extractor Estimating: 231it [02:18,  1.60it/s]Extractor Estimating: 232it [02:18,  1.58it/s]Extractor Estimating: 233it [02:19,  1.60it/s]Extractor Estimating: 234it [02:20,  1.57it/s]Extractor Estimating: 235it [02:20,  1.58it/s]Extractor Estimating: 236it [02:21,  1.56it/s]Extractor Estimating: 237it [02:21,  1.59it/s]Extractor Estimating: 238it [02:22,  1.66it/s]Extractor Estimating: 239it [02:23,  1.64it/s]Extractor Estimating: 240it [02:23,  1.60it/s]Extractor Estimating: 241it [02:24,  1.56it/s]Extractor Estimating: 242it [02:25,  1.58it/s]Extractor Estimating: 243it [02:25,  1.59it/s]Extractor Estimating: 244it [02:26,  1.63it/s]Extractor Estimating: 245it [02:26,  1.63it/s]Extractor Estimating: 246it [02:27,  1.61it/s]Extractor Estimating: 247it [02:28,  1.59it/s]Extractor Estimating: 248it [02:28,  1.67it/s]Extractor Estimating: 249it [02:29,  1.62it/s]Extractor Estimating: 250it [02:29,  1.63it/s]Extractor Estimating: 251it [02:30,  1.63it/s]Extractor Estimating: 252it [02:31,  1.70it/s]Extractor Estimating: 253it [02:31,  1.74it/s]Extractor Estimating: 254it [02:32,  1.76it/s]Extractor Estimating: 255it [02:32,  1.84it/s]Extractor Estimating: 256it [02:33,  1.84it/s]Extractor Estimating: 257it [02:33,  1.86it/s]Extractor Estimating: 258it [02:34,  1.86it/s]Extractor Estimating: 259it [02:34,  1.90it/s]Extractor Estimating: 260it [02:35,  1.84it/s]Extractor Estimating: 261it [02:35,  1.86it/s]Extractor Estimating: 262it [02:36,  1.85it/s]Extractor Estimating: 263it [02:36,  1.83it/s]Extractor Estimating: 264it [02:37,  1.87it/s]Extractor Estimating: 265it [02:37,  1.95it/s]Extractor Estimating: 266it [02:38,  1.94it/s]Extractor Estimating: 267it [02:39,  1.84it/s]Extractor Estimating: 268it [02:39,  1.83it/s]Extractor Estimating: 269it [02:40,  1.75it/s]Extractor Estimating: 270it [02:40,  1.81it/s]Extractor Estimating: 271it [02:41,  1.86it/s]Extractor Estimating: 272it [02:41,  1.73it/s]Extractor Estimating: 273it [02:42,  1.77it/s]Extractor Estimating: 274it [02:42,  1.80it/s]Extractor Estimating: 275it [02:43,  1.80it/s]Extractor Estimating: 276it [02:44,  1.73it/s]Extractor Estimating: 277it [02:44,  1.66it/s]Extractor Estimating: 278it [02:45,  1.46it/s]Extractor Estimating: 279it [02:46,  1.46it/s]Extractor Estimating: 280it [02:47,  1.49it/s]Extractor Estimating: 281it [02:47,  1.55it/s]Extractor Estimating: 282it [02:48,  1.53it/s]Extractor Estimating: 283it [02:48,  1.54it/s]Extractor Estimating: 284it [02:49,  1.56it/s]Extractor Estimating: 285it [02:50,  1.50it/s]Extractor Estimating: 286it [02:50,  1.52it/s]Extractor Estimating: 287it [02:51,  1.51it/s]Extractor Estimating: 288it [02:52,  1.54it/s]Extractor Estimating: 289it [02:52,  1.52it/s]Extractor Estimating: 290it [02:53,  1.56it/s]Extractor Estimating: 291it [02:54,  1.61it/s]Extractor Estimating: 292it [02:54,  1.57it/s]Extractor Estimating: 293it [02:55,  1.54it/s]Extractor Estimating: 294it [02:56,  1.54it/s]Extractor Estimating: 295it [02:56,  1.58it/s]Extractor Estimating: 296it [02:57,  1.58it/s]Extractor Estimating: 297it [02:57,  1.60it/s]Extractor Estimating: 298it [02:58,  1.57it/s]Extractor Estimating: 299it [02:59,  1.60it/s]Extractor Estimating: 300it [02:59,  1.63it/s]Extractor Estimating: 301it [03:00,  1.72it/s]Extractor Estimating: 302it [03:00,  1.81it/s]Extractor Estimating: 303it [03:01,  1.96it/s]Extractor Estimating: 304it [03:01,  1.96it/s]Extractor Estimating: 305it [03:02,  1.98it/s]Extractor Estimating: 306it [03:02,  2.03it/s]Extractor Estimating: 307it [03:03,  1.95it/s]Extractor Estimating: 308it [03:03,  1.96it/s]Extractor Estimating: 309it [03:04,  1.93it/s]Extractor Estimating: 310it [03:04,  1.83it/s]Extractor Estimating: 311it [03:05,  1.91it/s]Extractor Estimating: 312it [03:05,  1.97it/s]Extractor Estimating: 313it [03:06,  2.06it/s]Extractor Estimating: 314it [03:06,  2.03it/s]Extractor Estimating: 315it [03:07,  2.13it/s]Extractor Estimating: 316it [03:07,  2.07it/s]Extractor Estimating: 317it [03:08,  2.04it/s]Extractor Estimating: 318it [03:08,  2.02it/s]Extractor Estimating: 319it [03:09,  2.00it/s]Extractor Estimating: 320it [03:09,  2.01it/s]Extractor Estimating: 321it [03:10,  2.01it/s]Extractor Estimating: 322it [03:10,  2.09it/s]Extractor Estimating: 323it [03:11,  2.12it/s]Extractor Estimating: 324it [03:11,  2.03it/s]Extractor Estimating: 325it [03:12,  1.98it/s]Extractor Estimating: 326it [03:12,  1.77it/s]Extractor Estimating: 327it [03:13,  1.68it/s]Extractor Estimating: 328it [03:14,  1.65it/s]Extractor Estimating: 329it [03:14,  1.61it/s]Extractor Estimating: 330it [03:15,  1.59it/s]Extractor Estimating: 331it [03:16,  1.53it/s]Extractor Estimating: 332it [03:16,  1.52it/s]Extractor Estimating: 333it [03:17,  1.53it/s]Extractor Estimating: 334it [03:18,  1.54it/s]Extractor Estimating: 335it [03:18,  1.54it/s]Extractor Estimating: 336it [03:19,  1.53it/s]Extractor Estimating: 337it [03:20,  1.53it/s]Extractor Estimating: 338it [03:20,  1.54it/s]Extractor Estimating: 339it [03:21,  1.53it/s]Extractor Estimating: 340it [03:21,  1.56it/s]Extractor Estimating: 341it [03:22,  1.52it/s]Extractor Estimating: 342it [03:23,  1.55it/s]Extractor Estimating: 343it [03:23,  1.57it/s]Extractor Estimating: 344it [03:24,  1.57it/s]Extractor Estimating: 345it [03:25,  1.58it/s]Extractor Estimating: 346it [03:25,  1.52it/s]Extractor Estimating: 347it [03:26,  1.53it/s]Extractor Estimating: 348it [03:27,  1.58it/s]Extractor Estimating: 349it [03:27,  1.57it/s]Extractor Estimating: 350it [03:28,  1.57it/s]Extractor Estimating: 351it [03:29,  1.52it/s]Extractor Estimating: 352it [03:29,  1.55it/s]Extractor Estimating: 353it [03:30,  1.53it/s]Extractor Estimating: 354it [03:31,  1.53it/s]Extractor Estimating: 355it [03:31,  1.56it/s]Extractor Estimating: 356it [03:32,  1.53it/s]Extractor Estimating: 357it [03:32,  1.58it/s]Extractor Estimating: 358it [03:33,  1.55it/s]Extractor Estimating: 359it [03:34,  1.59it/s]Extractor Estimating: 360it [03:34,  1.63it/s]Extractor Estimating: 361it [03:35,  1.61it/s]Extractor Estimating: 362it [03:36,  1.61it/s]Extractor Estimating: 363it [03:36,  1.59it/s]Extractor Estimating: 364it [03:37,  1.59it/s]Extractor Estimating: 365it [03:37,  1.56it/s]Extractor Estimating: 366it [03:38,  1.38it/s]Extractor Estimating: 367it [03:39,  1.39it/s]Extractor Estimating: 368it [03:40,  1.41it/s]Extractor Estimating: 369it [03:40,  1.46it/s]Extractor Estimating: 370it [03:41,  1.49it/s]Extractor Estimating: 371it [03:42,  1.52it/s]Extractor Estimating: 372it [03:42,  1.54it/s]Extractor Estimating: 373it [03:43,  1.54it/s]Extractor Estimating: 374it [03:44,  1.57it/s]Extractor Estimating: 375it [03:44,  1.62it/s]Extractor Estimating: 376it [03:45,  1.66it/s]Extractor Estimating: 377it [03:45,  1.64it/s]Extractor Estimating: 378it [03:46,  1.61it/s]Extractor Estimating: 379it [03:47,  1.68it/s]Extractor Estimating: 380it [03:47,  1.65it/s]Extractor Estimating: 381it [03:48,  1.68it/s]Extractor Estimating: 382it [03:48,  1.67it/s]Extractor Estimating: 383it [03:49,  1.66it/s]Extractor Estimating: 384it [03:50,  1.60it/s]Extractor Estimating: 385it [03:50,  1.61it/s]Extractor Estimating: 386it [03:51,  1.60it/s]Extractor Estimating: 387it [03:51,  1.63it/s]Extractor Estimating: 388it [03:52,  1.64it/s]Extractor Estimating: 389it [03:53,  1.64it/s]Extractor Estimating: 390it [03:53,  1.66it/s]Extractor Estimating: 391it [03:54,  1.67it/s]Extractor Estimating: 392it [03:54,  1.70it/s]Extractor Estimating: 393it [03:55,  1.68it/s]Extractor Estimating: 394it [03:56,  1.66it/s]Extractor Estimating: 395it [03:56,  1.64it/s]Extractor Estimating: 396it [03:57,  1.67it/s]Extractor Estimating: 397it [03:57,  1.70it/s]Extractor Estimating: 398it [03:58,  1.65it/s]Extractor Estimating: 399it [03:59,  1.69it/s]Extractor Estimating: 400it [03:59,  1.70it/s]Extractor Estimating: 401it [04:00,  1.61it/s]Extractor Estimating: 402it [04:01,  1.60it/s]Extractor Estimating: 403it [04:01,  1.61it/s]Extractor Estimating: 404it [04:02,  1.63it/s]Extractor Estimating: 405it [04:02,  1.64it/s]Extractor Estimating: 406it [04:03,  1.61it/s]Extractor Estimating: 407it [04:04,  1.67it/s]Extractor Estimating: 408it [04:04,  1.66it/s]Extractor Estimating: 409it [04:05,  1.65it/s]Extractor Estimating: 410it [04:05,  1.70it/s]Extractor Estimating: 411it [04:06,  1.71it/s]Extractor Estimating: 412it [04:06,  1.69it/s]Extractor Estimating: 413it [04:07,  1.67it/s]Extractor Estimating: 414it [04:08,  1.68it/s]Extractor Estimating: 415it [04:08,  1.67it/s]Extractor Estimating: 416it [04:09,  1.65it/s]Extractor Estimating: 417it [04:09,  1.68it/s]Extractor Estimating: 418it [04:10,  1.67it/s]Extractor Estimating: 419it [04:11,  1.65it/s]Extractor Estimating: 420it [04:11,  1.71it/s]Extractor Estimating: 421it [04:12,  1.66it/s]Extractor Estimating: 422it [04:13,  1.63it/s]Extractor Estimating: 423it [04:13,  1.63it/s]Extractor Estimating: 424it [04:14,  1.61it/s]Extractor Estimating: 425it [04:14,  1.62it/s]Extractor Estimating: 426it [04:15,  1.63it/s]Extractor Estimating: 427it [04:16,  1.63it/s]Extractor Estimating: 428it [04:16,  1.62it/s]Extractor Estimating: 429it [04:17,  1.60it/s]Extractor Estimating: 430it [04:18,  1.55it/s]Extractor Estimating: 431it [04:18,  1.59it/s]Extractor Estimating: 432it [04:19,  1.63it/s]Extractor Estimating: 433it [04:19,  1.61it/s]Extractor Estimating: 434it [04:20,  1.54it/s]Extractor Estimating: 435it [04:21,  1.62it/s]Extractor Estimating: 436it [04:21,  1.65it/s]Extractor Estimating: 437it [04:22,  1.75it/s]Extractor Estimating: 438it [04:22,  1.74it/s]Extractor Estimating: 439it [04:23,  1.71it/s]Extractor Estimating: 440it [04:24,  1.53it/s]Extractor Estimating: 441it [04:24,  1.55it/s]Extractor Estimating: 442it [04:25,  1.61it/s]Extractor Estimating: 443it [04:25,  1.65it/s]Extractor Estimating: 444it [04:26,  1.68it/s]Extractor Estimating: 445it [04:27,  1.67it/s]Extractor Estimating: 446it [04:27,  1.70it/s]Extractor Estimating: 447it [04:28,  1.74it/s]Extractor Estimating: 448it [04:28,  1.73it/s]Extractor Estimating: 449it [04:29,  1.70it/s]Extractor Estimating: 450it [04:30,  1.66it/s]Extractor Estimating: 451it [04:30,  1.56it/s]Extractor Estimating: 452it [04:31,  1.55it/s]Extractor Estimating: 453it [04:32,  1.49it/s]Extractor Estimating: 454it [04:32,  1.51it/s]Extractor Estimating: 455it [04:33,  1.52it/s]Extractor Estimating: 456it [04:34,  1.51it/s]Extractor Estimating: 457it [04:34,  1.49it/s]Extractor Estimating: 458it [04:35,  1.50it/s]Extractor Estimating: 459it [04:36,  1.51it/s]Extractor Estimating: 460it [04:36,  1.51it/s]Extractor Estimating: 461it [04:37,  1.51it/s]Extractor Estimating: 462it [04:38,  1.51it/s]Extractor Estimating: 463it [04:38,  1.51it/s]Extractor Estimating: 464it [04:39,  1.50it/s]Extractor Estimating: 465it [04:40,  1.52it/s]Extractor Estimating: 466it [04:40,  1.52it/s]Extractor Estimating: 467it [04:41,  1.49it/s]Extractor Estimating: 468it [04:42,  1.51it/s]Extractor Estimating: 469it [04:42,  1.53it/s]Extractor Estimating: 470it [04:43,  1.52it/s]Extractor Estimating: 471it [04:44,  1.57it/s]Extractor Estimating: 472it [04:44,  1.58it/s]Extractor Estimating: 473it [04:45,  1.61it/s]Extractor Estimating: 474it [04:45,  1.62it/s]Extractor Estimating: 475it [04:46,  1.67it/s]Extractor Estimating: 476it [04:46,  1.74it/s]Extractor Estimating: 477it [04:47,  1.84it/s]Extractor Estimating: 478it [04:47,  1.81it/s]Extractor Estimating: 479it [04:48,  1.69it/s]Extractor Estimating: 480it [04:49,  1.73it/s]Extractor Estimating: 481it [04:49,  1.72it/s]Extractor Estimating: 482it [04:50,  1.70it/s]Extractor Estimating: 483it [04:50,  1.73it/s]Extractor Estimating: 484it [04:51,  1.73it/s]Extractor Estimating: 485it [04:52,  1.72it/s]Extractor Estimating: 486it [04:52,  1.71it/s]Extractor Estimating: 487it [04:53,  1.65it/s]Extractor Estimating: 488it [04:53,  1.68it/s]Extractor Estimating: 489it [04:54,  1.69it/s]Extractor Estimating: 490it [04:55,  1.67it/s]Extractor Estimating: 491it [04:55,  1.70it/s]Extractor Estimating: 492it [04:56,  1.72it/s]Extractor Estimating: 493it [04:56,  1.71it/s]Extractor Estimating: 494it [04:57,  1.70it/s]Extractor Estimating: 495it [04:58,  1.71it/s]Extractor Estimating: 495it [04:58,  1.66it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:53,114 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:53,143 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:53,143 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:53,143 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:53,143 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:39:53,461 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:39:53,462 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:39:53,752 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:39:54,808 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:39:54,808 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:56,706 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:56,727 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:56,727 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:56,727 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:39:56,727 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:39:57,498 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:39:57,499 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:39:57,782 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:39:57,935 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:39:57,935 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
[INFO|training_args.py:725] 2023-08-29 15:51:33,198 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 15:51:33,932 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 9877 mean pseudo reward: 0.943482577560268
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_1/dev.jsonl'}
train vocab size: 17836
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17936, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=17936, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.036, loss:483.0540
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.062, loss:458.7818
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.049, loss:459.1819
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.046, loss:420.1658
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 88, avg_time 1.043, loss:420.1401
>> valid entity prec:0.5746, rec:0.6067, f1:0.5902
>> valid relation prec:0.3271, rec:0.0769, f1:0.1246
>> valid relation with NER prec:0.3271, rec:0.0769, f1:0.1246
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 188, avg_time 2.845, loss:419.4417
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 288, avg_time 1.046, loss:417.5286
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 388, avg_time 1.051, loss:438.1632
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 76, avg_time 1.040, loss:402.7748
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 176, avg_time 1.053, loss:413.2586
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5739, rec:0.5896, f1:0.5817
>> valid relation prec:0.3340, rec:0.0979, f1:0.1515
>> valid relation with NER prec:0.3340, rec:0.0979, f1:0.1515
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 276, avg_time 2.838, loss:414.5986
g_step 1200, step 376, avg_time 1.043, loss:427.6539
g_step 1300, step 64, avg_time 1.051, loss:390.9865
g_step 1400, step 164, avg_time 1.033, loss:378.5586
g_step 1500, step 264, avg_time 1.044, loss:405.4555
>> valid entity prec:0.6178, rec:0.5439, f1:0.5785
>> valid relation prec:0.3353, rec:0.0969, f1:0.1503
>> valid relation with NER prec:0.3353, rec:0.0969, f1:0.1503
g_step 1600, step 364, avg_time 2.839, loss:411.8132
g_step 1700, step 52, avg_time 1.053, loss:384.4202
g_step 1800, step 152, avg_time 1.053, loss:375.5227
g_step 1900, step 252, avg_time 1.052, loss:399.5346
g_step 2000, step 352, avg_time 1.051, loss:377.6130
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5652, rec:0.5770, f1:0.5710
>> valid relation prec:0.2224, rec:0.0707, f1:0.1073
>> valid relation with NER prec:0.2224, rec:0.0707, f1:0.1073
g_step 2100, step 40, avg_time 2.833, loss:375.6766
g_step 2200, step 140, avg_time 1.051, loss:346.5463
g_step 2300, step 240, avg_time 1.038, loss:372.7961
g_step 2400, step 340, avg_time 1.052, loss:371.8864
g_step 2500, step 28, avg_time 1.054, loss:372.2704
>> valid entity prec:0.5802, rec:0.4958, f1:0.5347
>> valid relation prec:0.2835, rec:0.0923, f1:0.1393
>> valid relation with NER prec:0.2835, rec:0.0923, f1:0.1393
g_step 2600, step 128, avg_time 2.820, loss:341.9971
g_step 2700, step 228, avg_time 1.048, loss:351.6605
g_step 2800, step 328, avg_time 1.056, loss:364.0337
g_step 2900, step 16, avg_time 1.045, loss:368.9568
g_step 3000, step 116, avg_time 1.056, loss:335.1009
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5736, rec:0.5528, f1:0.5630
>> valid relation prec:0.2719, rec:0.1104, f1:0.1571
>> valid relation with NER prec:0.2719, rec:0.1104, f1:0.1571
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 216, avg_time 2.842, loss:328.8339
g_step 3200, step 316, avg_time 1.028, loss:331.0885
g_step 3300, step 4, avg_time 1.035, loss:350.4543
g_step 3400, step 104, avg_time 1.057, loss:311.9790
g_step 3500, step 204, avg_time 1.046, loss:329.3112
>> valid entity prec:0.5906, rec:0.5161, f1:0.5509
>> valid relation prec:0.3207, rec:0.1056, f1:0.1589
>> valid relation with NER prec:0.3207, rec:0.1056, f1:0.1589
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3600, step 304, avg_time 2.828, loss:330.3620
g_step 3700, step 404, avg_time 1.050, loss:339.2551
g_step 3800, step 92, avg_time 1.052, loss:295.7538
g_step 3900, step 192, avg_time 1.051, loss:325.5680
g_step 4000, step 292, avg_time 1.053, loss:325.7103
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5559, rec:0.5325, f1:0.5439
>> valid relation prec:0.2360, rec:0.0850, f1:0.1250
>> valid relation with NER prec:0.2360, rec:0.0850, f1:0.1250
g_step 4100, step 392, avg_time 2.814, loss:337.4185
g_step 4200, step 80, avg_time 1.038, loss:309.5817
g_step 4300, step 180, avg_time 1.045, loss:302.1495
g_step 4400, step 280, avg_time 1.062, loss:317.0015
g_step 4500, step 380, avg_time 1.046, loss:331.3078
>> valid entity prec:0.6052, rec:0.5540, f1:0.5785
>> valid relation prec:0.2693, rec:0.1002, f1:0.1461
>> valid relation with NER prec:0.2693, rec:0.1002, f1:0.1461
g_step 4600, step 68, avg_time 2.836, loss:313.9899
g_step 4700, step 168, avg_time 1.075, loss:291.6034
g_step 4800, step 268, avg_time 1.047, loss:315.7942
g_step 4900, step 368, avg_time 1.051, loss:315.0546
g_step 5000, step 56, avg_time 1.044, loss:283.2967
learning rate was adjusted to 0.0008
>> valid entity prec:0.5738, rec:0.5689, f1:0.5713
>> valid relation prec:0.2616, rec:0.1009, f1:0.1456
>> valid relation with NER prec:0.2616, rec:0.1009, f1:0.1456
g_step 5100, step 156, avg_time 2.862, loss:289.5964
g_step 5200, step 256, avg_time 1.043, loss:305.0606
g_step 5300, step 356, avg_time 1.035, loss:302.8409
g_step 5400, step 44, avg_time 1.048, loss:289.5559
g_step 5500, step 144, avg_time 1.052, loss:274.1939
>> valid entity prec:0.5848, rec:0.4950, f1:0.5362
>> valid relation prec:0.2490, rec:0.0913, f1:0.1336
>> valid relation with NER prec:0.2490, rec:0.0913, f1:0.1336
g_step 5600, step 244, avg_time 2.811, loss:276.7273
g_step 5700, step 344, avg_time 1.046, loss:293.2309
g_step 5800, step 32, avg_time 1.036, loss:287.7633
g_step 5900, step 132, avg_time 1.044, loss:273.9251
g_step 6000, step 232, avg_time 1.047, loss:278.8101
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.6127, rec:0.4861, f1:0.5421
>> valid relation prec:0.2435, rec:0.0765, f1:0.1165
>> valid relation with NER prec:0.2435, rec:0.0765, f1:0.1165
g_step 6100, step 332, avg_time 2.823, loss:271.0969
g_step 6200, step 20, avg_time 1.046, loss:273.9652
g_step 6300, step 120, avg_time 1.051, loss:262.6000
g_step 6400, step 220, avg_time 1.046, loss:272.0112
g_step 6500, step 320, avg_time 1.045, loss:276.1012
>> valid entity prec:0.5429, rec:0.5601, f1:0.5514
>> valid relation prec:0.2205, rec:0.1052, f1:0.1425
>> valid relation with NER prec:0.2205, rec:0.1052, f1:0.1425
g_step 6600, step 8, avg_time 2.831, loss:268.6005
g_step 6700, step 108, avg_time 1.054, loss:259.9185
g_step 6800, step 208, avg_time 1.039, loss:268.5249
g_step 6900, step 308, avg_time 1.047, loss:265.4464
g_step 7000, step 408, avg_time 1.050, loss:271.6011
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5824, rec:0.5486, f1:0.5650
>> valid relation prec:0.2510, rec:0.1023, f1:0.1454
>> valid relation with NER prec:0.2510, rec:0.1023, f1:0.1454
g_step 7100, step 96, avg_time 2.828, loss:252.5680
g_step 7200, step 196, avg_time 1.042, loss:253.5266
g_step 7300, step 296, avg_time 1.056, loss:259.0240
g_step 7400, step 396, avg_time 1.044, loss:272.5438
g_step 7500, step 84, avg_time 1.043, loss:247.7307
>> valid entity prec:0.5800, rec:0.5277, f1:0.5526
>> valid relation prec:0.2513, rec:0.0894, f1:0.1319
>> valid relation with NER prec:0.2513, rec:0.0894, f1:0.1319
g_step 7600, step 184, avg_time 2.818, loss:257.7175
g_step 7700, step 284, avg_time 1.050, loss:262.1946
g_step 7800, step 384, avg_time 1.042, loss:248.1192
g_step 7900, step 72, avg_time 1.042, loss:242.0840
g_step 8000, step 172, avg_time 1.045, loss:248.4182
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5696, rec:0.5157, f1:0.5413
>> valid relation prec:0.2371, rec:0.0907, f1:0.1312
>> valid relation with NER prec:0.2371, rec:0.0907, f1:0.1312
g_step 8100, step 272, avg_time 2.825, loss:239.4424
g_step 8200, step 372, avg_time 1.062, loss:248.1582
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 15:51:33 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 15:51:33 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_15-51-33_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 15:51:35 - WARNING - datasets.builder -   Using custom data configuration default-2c8a6dab9f20d559
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-2c8a6dab9f20d559/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 15:51:43,858 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 15:51:43,859 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 15:51:43,860 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 15:51:43,861 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 15:51:44,278 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 15:51:44,526 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 15:51:44,527 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 15:51:44,527 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 15:51:44,527 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 15:51:44,527 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 15:51:44,527 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 15:51:45,993 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 15:51:49,289 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 15:51:49,370 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-2c8a6dab9f20d559/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:04,  1.84ba/s] 20%|██        | 2/10 [00:00<00:02,  2.93ba/s] 30%|███       | 3/10 [00:00<00:01,  3.62ba/s] 40%|████      | 4/10 [00:01<00:01,  4.08ba/s] 50%|█████     | 5/10 [00:01<00:01,  3.50ba/s] 60%|██████    | 6/10 [00:01<00:01,  3.91ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.23ba/s] 80%|████████  | 8/10 [00:02<00:00,  4.45ba/s] 90%|█████████ | 9/10 [00:02<00:00,  4.62ba/s]100%|██████████| 10/10 [00:02<00:00,  4.77ba/s]100%|██████████| 10/10 [00:02<00:00,  4.03ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.09ba/s] 40%|████      | 2/5 [00:00<00:00,  3.06ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.62ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.93ba/s]100%|██████████| 5/5 [00:01<00:00,  4.36ba/s]100%|██████████| 5/5 [00:01<00:00,  3.78ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:03,  2.80ba/s] 30%|███       | 3/10 [00:00<00:01,  6.30ba/s] 50%|█████     | 5/10 [00:00<00:00,  8.06ba/s] 70%|███████   | 7/10 [00:00<00:00,  9.01ba/s] 90%|█████████ | 9/10 [00:01<00:00,  9.62ba/s]100%|██████████| 10/10 [00:01<00:00,  8.47ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.83ba/s] 60%|██████    | 3/5 [00:00<00:00,  8.24ba/s]100%|██████████| 5/5 [00:00<00:00,  9.77ba/s]100%|██████████| 5/5 [00:00<00:00,  8.95ba/s]
[INFO|trainer.py:414] 2023-08-29 15:51:57,710 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 15:51:57,832 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 15:51:57,832 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 15:51:57,832 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 15:51:57,832 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 15:51:57,832 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 15:51:57,832 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 15:51:57,832 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:01<17:21,  1.34s/it]  0%|          | 2/780 [00:02<12:22,  1.05it/s]  0%|          | 3/780 [00:04<18:54,  1.46s/it]  1%|          | 4/780 [00:04<13:42,  1.06s/it]  1%|          | 5/780 [00:05<10:58,  1.18it/s]  1%|          | 6/780 [00:05<08:38,  1.49it/s]  1%|          | 7/780 [00:05<07:45,  1.66it/s]  1%|          | 8/780 [00:06<06:37,  1.94it/s]  1%|          | 9/780 [00:06<05:42,  2.25it/s]  1%|▏         | 10/780 [00:06<05:04,  2.53it/s]  1%|▏         | 11/780 [00:06<04:38,  2.76it/s]  2%|▏         | 12/780 [00:07<04:20,  2.95it/s]  2%|▏         | 13/780 [00:07<04:07,  3.09it/s]  2%|▏         | 14/780 [00:07<03:59,  3.20it/s]  2%|▏         | 15/780 [00:08<03:53,  3.28it/s]  2%|▏         | 16/780 [00:08<03:48,  3.34it/s]  2%|▏         | 17/780 [00:08<03:54,  3.26it/s]  2%|▏         | 18/780 [00:09<04:01,  3.16it/s]  2%|▏         | 19/780 [00:09<03:54,  3.25it/s]  3%|▎         | 20/780 [00:09<03:49,  3.32it/s]  3%|▎         | 21/780 [00:09<03:45,  3.37it/s]  3%|▎         | 22/780 [00:10<03:42,  3.40it/s]  3%|▎         | 23/780 [00:10<03:41,  3.43it/s]  3%|▎         | 24/780 [00:10<03:39,  3.44it/s]  3%|▎         | 25/780 [00:11<03:38,  3.46it/s]  3%|▎         | 26/780 [00:11<03:37,  3.47it/s]  3%|▎         | 27/780 [00:11<03:36,  3.47it/s]  4%|▎         | 28/780 [00:11<03:36,  3.47it/s]  4%|▎         | 29/780 [00:12<03:35,  3.48it/s]  4%|▍         | 30/780 [00:12<03:35,  3.48it/s]  4%|▍         | 31/780 [00:12<03:34,  3.48it/s]  4%|▍         | 32/780 [00:13<03:34,  3.48it/s]  4%|▍         | 33/780 [00:13<03:34,  3.48it/s]  4%|▍         | 34/780 [00:13<04:17,  2.90it/s]  4%|▍         | 35/780 [00:14<04:04,  3.05it/s]  5%|▍         | 36/780 [00:14<03:54,  3.17it/s]  5%|▍         | 37/780 [00:14<03:48,  3.26it/s]  5%|▍         | 38/780 [00:15<03:43,  3.32it/s]  5%|▌         | 39/780 [00:15<03:39,  3.37it/s]  5%|▌         | 40/780 [00:15<03:37,  3.40it/s]  5%|▌         | 41/780 [00:15<03:35,  3.43it/s]  5%|▌         | 42/780 [00:16<03:34,  3.44it/s]  6%|▌         | 43/780 [00:16<03:33,  3.46it/s]  6%|▌         | 44/780 [00:16<03:32,  3.47it/s]  6%|▌         | 45/780 [00:17<03:31,  3.47it/s]  6%|▌         | 46/780 [00:17<03:31,  3.48it/s]  6%|▌         | 47/780 [00:17<03:30,  3.48it/s]  6%|▌         | 48/780 [00:17<03:30,  3.48it/s]  6%|▋         | 49/780 [00:18<03:30,  3.48it/s]  6%|▋         | 50/780 [00:18<03:29,  3.48it/s]  7%|▋         | 51/780 [00:18<03:29,  3.48it/s]  7%|▋         | 52/780 [00:19<03:29,  3.48it/s]  7%|▋         | 53/780 [00:19<03:28,  3.48it/s]  7%|▋         | 54/780 [00:19<03:28,  3.48it/s]  7%|▋         | 55/780 [00:19<03:28,  3.48it/s]  7%|▋         | 56/780 [00:20<03:28,  3.48it/s]  7%|▋         | 57/780 [00:20<03:27,  3.48it/s]  7%|▋         | 58/780 [00:20<03:27,  3.48it/s]  8%|▊         | 59/780 [00:21<03:27,  3.48it/s]  8%|▊         | 60/780 [00:21<03:27,  3.48it/s]  8%|▊         | 61/780 [00:21<03:26,  3.48it/s]  8%|▊         | 62/780 [00:21<03:26,  3.48it/s]  8%|▊         | 63/780 [00:22<03:26,  3.48it/s]  8%|▊         | 64/780 [00:22<03:25,  3.48it/s]  8%|▊         | 65/780 [00:22<03:25,  3.48it/s]  8%|▊         | 66/780 [00:23<03:25,  3.47it/s]  9%|▊         | 67/780 [00:23<03:25,  3.48it/s]  9%|▊         | 68/780 [00:23<03:24,  3.48it/s]  9%|▉         | 69/780 [00:23<03:24,  3.48it/s]  9%|▉         | 70/780 [00:24<03:24,  3.48it/s]  9%|▉         | 71/780 [00:24<03:23,  3.48it/s]  9%|▉         | 72/780 [00:24<03:23,  3.48it/s]  9%|▉         | 73/780 [00:25<03:23,  3.48it/s]  9%|▉         | 74/780 [00:25<03:23,  3.48it/s] 10%|▉         | 75/780 [00:25<03:22,  3.48it/s] 10%|▉         | 76/780 [00:25<03:22,  3.48it/s] 10%|▉         | 77/780 [00:26<03:22,  3.47it/s] 10%|█         | 78/780 [00:26<03:22,  3.47it/s] 10%|█         | 79/780 [00:26<03:21,  3.48it/s] 10%|█         | 80/780 [00:27<03:21,  3.47it/s] 10%|█         | 81/780 [00:27<03:21,  3.48it/s] 11%|█         | 82/780 [00:27<03:20,  3.47it/s] 11%|█         | 83/780 [00:27<03:20,  3.48it/s] 11%|█         | 84/780 [00:28<03:20,  3.48it/s] 11%|█         | 85/780 [00:28<03:20,  3.47it/s] 11%|█         | 86/780 [00:28<03:19,  3.48it/s] 11%|█         | 87/780 [00:29<03:19,  3.48it/s] 11%|█▏        | 88/780 [00:29<03:19,  3.48it/s] 11%|█▏        | 89/780 [00:29<03:18,  3.48it/s] 12%|█▏        | 90/780 [00:29<03:18,  3.48it/s] 12%|█▏        | 91/780 [00:30<03:18,  3.48it/s] 12%|█▏        | 92/780 [00:30<03:17,  3.48it/s] 12%|█▏        | 93/780 [00:30<03:17,  3.47it/s] 12%|█▏        | 94/780 [00:31<03:17,  3.47it/s] 12%|█▏        | 95/780 [00:31<03:24,  3.35it/s] 12%|█▏        | 96/780 [00:31<03:22,  3.38it/s] 12%|█▏        | 97/780 [00:32<03:20,  3.41it/s] 13%|█▎        | 98/780 [00:32<03:19,  3.43it/s] 13%|█▎        | 99/780 [00:32<03:18,  3.44it/s] 13%|█▎        | 100/780 [00:32<03:17,  3.45it/s] 13%|█▎        | 101/780 [00:33<03:16,  3.46it/s] 13%|█▎        | 102/780 [00:33<03:16,  3.46it/s] 13%|█▎        | 103/780 [00:33<03:15,  3.46it/s] 13%|█▎        | 104/780 [00:34<03:15,  3.47it/s] 13%|█▎        | 105/780 [00:34<03:14,  3.47it/s] 14%|█▎        | 106/780 [00:34<03:14,  3.47it/s] 14%|█▎        | 107/780 [00:34<03:33,  3.15it/s] 14%|█▍        | 108/780 [00:35<03:27,  3.24it/s] 14%|█▍        | 109/780 [00:35<03:56,  2.84it/s] 14%|█▍        | 110/780 [00:36<04:12,  2.65it/s] 14%|█▍        | 111/780 [00:36<03:54,  2.85it/s] 14%|█▍        | 112/780 [00:36<03:55,  2.83it/s] 14%|█▍        | 113/780 [00:37<03:42,  2.99it/s] 15%|█▍        | 114/780 [00:37<03:33,  3.12it/s] 15%|█▍        | 115/780 [00:37<03:26,  3.22it/s] 15%|█▍        | 116/780 [00:38<03:37,  3.06it/s] 15%|█▌        | 117/780 [00:38<03:29,  3.17it/s] 15%|█▌        | 118/780 [00:38<03:23,  3.25it/s] 15%|█▌        | 119/780 [00:38<03:19,  3.32it/s] 15%|█▌        | 120/780 [00:39<03:16,  3.36it/s] 16%|█▌        | 121/780 [00:39<03:14,  3.40it/s] 16%|█▌        | 122/780 [00:39<03:12,  3.42it/s] 16%|█▌        | 123/780 [00:40<03:11,  3.43it/s] 16%|█▌        | 124/780 [00:40<03:10,  3.45it/s] 16%|█▌        | 125/780 [00:40<03:09,  3.45it/s] 16%|█▌        | 126/780 [00:40<03:08,  3.46it/s] 16%|█▋        | 127/780 [00:41<03:29,  3.12it/s] 16%|█▋        | 128/780 [00:41<03:43,  2.91it/s] 17%|█▋        | 129/780 [00:42<03:32,  3.06it/s] 17%|█▋        | 130/780 [00:42<03:25,  3.17it/s] 17%|█▋        | 131/780 [00:42<03:19,  3.25it/s] 17%|█▋        | 132/780 [00:42<03:15,  3.32it/s] 17%|█▋        | 133/780 [00:43<03:12,  3.36it/s] 17%|█▋        | 134/780 [00:43<03:10,  3.39it/s] 17%|█▋        | 135/780 [00:43<03:08,  3.42it/s] 17%|█▋        | 136/780 [00:44<03:07,  3.43it/s] 18%|█▊        | 137/780 [00:44<03:06,  3.44it/s] 18%|█▊        | 138/780 [00:44<03:18,  3.24it/s] 18%|█▊        | 139/780 [00:44<03:14,  3.30it/s] 18%|█▊        | 140/780 [00:45<03:10,  3.35it/s] 18%|█▊        | 141/780 [00:45<03:08,  3.39it/s] 18%|█▊        | 142/780 [00:45<03:07,  3.41it/s] 18%|█▊        | 143/780 [00:46<03:05,  3.43it/s] 18%|█▊        | 144/780 [00:46<03:05,  3.44it/s] 19%|█▊        | 145/780 [00:46<03:04,  3.45it/s] 19%|█▊        | 146/780 [00:46<03:03,  3.45it/s] 19%|█▉        | 147/780 [00:47<03:03,  3.46it/s] 19%|█▉        | 148/780 [00:47<03:02,  3.46it/s] 19%|█▉        | 149/780 [00:47<03:19,  3.16it/s] 19%|█▉        | 150/780 [00:48<03:13,  3.25it/s] 19%|█▉        | 151/780 [00:48<03:09,  3.31it/s] 19%|█▉        | 152/780 [00:48<03:07,  3.36it/s] 20%|█▉        | 153/780 [00:49<03:05,  3.39it/s] 20%|█▉        | 154/780 [00:49<03:03,  3.41it/s] 20%|█▉        | 155/780 [00:49<03:02,  3.43it/s] 20%|██        | 156/780 [00:49<03:01,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 15:52:47,830 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 15:52:47,830 >>   Num examples = 4812
[INFO|trainer.py:2145] 2023-08-29 15:52:47,830 >>   Batch size = 8

  0%|          | 0/602 [00:00<?, ?it/s][A
  1%|          | 6/602 [00:00<00:10, 57.89it/s][A
  2%|▏         | 12/602 [00:00<00:11, 50.83it/s][A
  3%|▎         | 18/602 [00:00<00:11, 48.97it/s][A
  4%|▍         | 23/602 [00:00<00:11, 48.31it/s][A
  5%|▍         | 28/602 [00:00<00:11, 47.85it/s][A
  5%|▌         | 33/602 [00:00<00:14, 39.13it/s][A
  6%|▋         | 38/602 [00:00<00:13, 41.32it/s][A
  7%|▋         | 43/602 [00:00<00:13, 42.94it/s][A
  8%|▊         | 48/602 [00:01<00:12, 44.06it/s][A
  9%|▉         | 53/602 [00:01<00:12, 44.92it/s][A
 10%|▉         | 58/602 [00:01<00:11, 45.53it/s][A
 10%|█         | 63/602 [00:01<00:11, 45.96it/s][A
 11%|█▏        | 68/602 [00:01<00:11, 46.21it/s][A
 12%|█▏        | 73/602 [00:01<00:11, 46.27it/s][A
 13%|█▎        | 78/602 [00:01<00:11, 46.35it/s][A
 14%|█▍        | 83/602 [00:01<00:11, 46.52it/s][A
 15%|█▍        | 88/602 [00:01<00:11, 46.67it/s][A
 15%|█▌        | 93/602 [00:02<00:10, 46.75it/s][A
 16%|█▋        | 98/602 [00:02<00:10, 46.77it/s][A
 17%|█▋        | 103/602 [00:02<00:10, 46.89it/s][A
 18%|█▊        | 108/602 [00:02<00:10, 46.98it/s][A
 19%|█▉        | 113/602 [00:02<00:10, 46.82it/s][A
 20%|█▉        | 118/602 [00:02<00:10, 46.78it/s][A
 20%|██        | 123/602 [00:02<00:10, 46.74it/s][A
 21%|██▏       | 128/602 [00:02<00:10, 46.70it/s][A
 22%|██▏       | 133/602 [00:02<00:10, 46.78it/s][A
 23%|██▎       | 138/602 [00:02<00:09, 46.86it/s][A
 24%|██▍       | 143/602 [00:03<00:09, 46.82it/s][A
 25%|██▍       | 148/602 [00:03<00:09, 46.84it/s][A
 25%|██▌       | 153/602 [00:03<00:09, 46.92it/s][A
 26%|██▌       | 158/602 [00:03<00:09, 46.94it/s][A
 27%|██▋       | 163/602 [00:03<00:09, 46.90it/s][A
 28%|██▊       | 168/602 [00:03<00:09, 46.78it/s][A
 29%|██▊       | 173/602 [00:03<00:10, 40.83it/s][A
 30%|██▉       | 178/602 [00:03<00:09, 42.49it/s][A
 30%|███       | 183/602 [00:04<00:09, 43.79it/s][A
 31%|███       | 188/602 [00:04<00:09, 44.68it/s][A
 32%|███▏      | 193/602 [00:04<00:09, 45.26it/s][A
 33%|███▎      | 198/602 [00:04<00:08, 45.80it/s][A
 34%|███▎      | 203/602 [00:04<00:08, 46.14it/s][A
 35%|███▍      | 208/602 [00:04<00:08, 46.32it/s][A
 35%|███▌      | 213/602 [00:04<00:08, 46.23it/s][A
 36%|███▌      | 218/602 [00:04<00:08, 46.27it/s][A
 37%|███▋      | 223/602 [00:04<00:08, 46.33it/s][A
 38%|███▊      | 228/602 [00:04<00:08, 46.47it/s][A
 39%|███▊      | 233/602 [00:05<00:07, 46.67it/s][A
 40%|███▉      | 238/602 [00:05<00:07, 46.70it/s][A
 40%|████      | 243/602 [00:05<00:07, 46.80it/s][A
 41%|████      | 248/602 [00:05<00:07, 46.84it/s][A
 42%|████▏     | 253/602 [00:05<00:07, 46.84it/s][A
 43%|████▎     | 258/602 [00:05<00:07, 46.69it/s][A
 44%|████▎     | 263/602 [00:05<00:07, 46.63it/s][A
 45%|████▍     | 268/602 [00:05<00:07, 46.50it/s][A
 45%|████▌     | 273/602 [00:05<00:07, 46.67it/s][A
 46%|████▌     | 278/602 [00:06<00:06, 46.68it/s][A
 47%|████▋     | 283/602 [00:06<00:06, 46.82it/s][A
 48%|████▊     | 288/602 [00:06<00:06, 46.74it/s][A
 49%|████▊     | 293/602 [00:06<00:06, 46.82it/s][A
 50%|████▉     | 298/602 [00:06<00:06, 46.84it/s][A
 50%|█████     | 303/602 [00:06<00:06, 46.75it/s][A
 51%|█████     | 308/602 [00:06<00:06, 46.62it/s][A
 52%|█████▏    | 313/602 [00:06<00:07, 40.13it/s][A
 53%|█████▎    | 318/602 [00:06<00:06, 41.76it/s][A
 54%|█████▎    | 323/602 [00:07<00:06, 43.21it/s][A
 54%|█████▍    | 328/602 [00:07<00:06, 44.27it/s][A
 55%|█████▌    | 333/602 [00:07<00:05, 45.06it/s][A
 56%|█████▌    | 338/602 [00:07<00:05, 45.57it/s][A
 57%|█████▋    | 343/602 [00:07<00:05, 46.02it/s][A
 58%|█████▊    | 348/602 [00:07<00:05, 46.28it/s][A
 59%|█████▊    | 353/602 [00:07<00:05, 46.23it/s][A
 59%|█████▉    | 358/602 [00:07<00:05, 46.25it/s][A
 60%|██████    | 363/602 [00:07<00:05, 46.31it/s][A
 61%|██████    | 368/602 [00:08<00:05, 46.46it/s][A
 62%|██████▏   | 373/602 [00:08<00:04, 46.66it/s][A
 63%|██████▎   | 378/602 [00:08<00:04, 46.72it/s][A
 64%|██████▎   | 383/602 [00:08<00:04, 46.78it/s][A
 64%|██████▍   | 388/602 [00:08<00:04, 46.78it/s][A
 65%|██████▌   | 393/602 [00:08<00:04, 46.70it/s][A
 66%|██████▌   | 398/602 [00:08<00:04, 46.71it/s][A
 67%|██████▋   | 403/602 [00:08<00:04, 46.55it/s][A
 68%|██████▊   | 408/602 [00:08<00:04, 46.60it/s][A
 69%|██████▊   | 413/602 [00:08<00:04, 46.52it/s][A
 69%|██████▉   | 418/602 [00:09<00:03, 46.59it/s][A
 70%|███████   | 423/602 [00:09<00:03, 46.73it/s][A
 71%|███████   | 428/602 [00:09<00:03, 46.88it/s][A
 72%|███████▏  | 433/602 [00:09<00:03, 46.77it/s][A
 73%|███████▎  | 438/602 [00:09<00:03, 46.73it/s][A
 74%|███████▎  | 443/602 [00:09<00:03, 46.73it/s][A
 74%|███████▍  | 448/602 [00:09<00:03, 46.58it/s][A
 75%|███████▌  | 453/602 [00:09<00:03, 42.92it/s][A
 76%|███████▌  | 458/602 [00:09<00:03, 43.96it/s][A
 77%|███████▋  | 463/602 [00:10<00:03, 44.70it/s][A
 78%|███████▊  | 468/602 [00:10<00:02, 45.33it/s][A
 79%|███████▊  | 473/602 [00:10<00:02, 45.75it/s][A
 79%|███████▉  | 478/602 [00:10<00:02, 46.02it/s][A
 80%|████████  | 483/602 [00:10<00:02, 46.25it/s][A
 81%|████████  | 488/602 [00:10<00:02, 46.38it/s][A
 82%|████████▏ | 493/602 [00:10<00:02, 46.43it/s][A
 83%|████████▎ | 498/602 [00:10<00:02, 46.60it/s][A
 84%|████████▎ | 503/602 [00:10<00:02, 46.61it/s][A
 84%|████████▍ | 508/602 [00:11<00:02, 46.64it/s][A
 85%|████████▌ | 513/602 [00:11<00:01, 46.66it/s][A
 86%|████████▌ | 518/602 [00:11<00:01, 46.62it/s][A
 87%|████████▋ | 523/602 [00:11<00:01, 46.69it/s][A
 88%|████████▊ | 528/602 [00:11<00:01, 46.77it/s][A
 89%|████████▊ | 533/602 [00:11<00:01, 46.70it/s][A
 89%|████████▉ | 538/602 [00:11<00:01, 46.68it/s][A
 90%|█████████ | 543/602 [00:11<00:01, 46.71it/s][A
 91%|█████████ | 548/602 [00:11<00:01, 46.68it/s][A
 92%|█████████▏| 553/602 [00:12<00:01, 46.73it/s][A
 93%|█████████▎| 558/602 [00:12<00:00, 46.72it/s][A
 94%|█████████▎| 563/602 [00:12<00:00, 46.68it/s][A
 94%|█████████▍| 568/602 [00:12<00:00, 46.51it/s][A
 95%|█████████▌| 573/602 [00:12<00:00, 46.76it/s][A
 96%|█████████▌| 578/602 [00:12<00:00, 46.72it/s][A
 97%|█████████▋| 583/602 [00:12<00:00, 46.73it/s][A
 98%|█████████▊| 588/602 [00:12<00:00, 46.79it/s][A
 99%|█████████▊| 593/602 [00:12<00:00, 42.82it/s][A
 99%|█████████▉| 598/602 [00:13<00:00, 43.98it/s][A                                                 
                                                 [A 20%|██        | 156/780 [01:03<03:01,  3.44it/s]
100%|██████████| 602/602 [00:13<00:00, 43.98it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 15:53:01,466 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 15:53:01,816 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 15:53:06,728 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 15:53:07,099 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 15:53:07,282 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:20<1:38:13,  9.46s/it] 20%|██        | 158/780 [01:21<1:09:41,  6.72s/it] 20%|██        | 159/780 [01:21<49:36,  4.79s/it]   21%|██        | 160/780 [01:21<35:33,  3.44s/it] 21%|██        | 161/780 [01:22<25:44,  2.49s/it] 21%|██        | 162/780 [01:22<18:52,  1.83s/it] 21%|██        | 163/780 [01:22<14:04,  1.37s/it] 21%|██        | 164/780 [01:22<10:43,  1.04s/it] 21%|██        | 165/780 [01:23<08:22,  1.22it/s] 21%|██▏       | 166/780 [01:23<06:44,  1.52it/s] 21%|██▏       | 167/780 [01:23<05:35,  1.83it/s] 22%|██▏       | 168/780 [01:24<04:47,  2.13it/s] 22%|██▏       | 169/780 [01:24<04:25,  2.30it/s] 22%|██▏       | 170/780 [01:24<03:58,  2.56it/s] 22%|██▏       | 171/780 [01:24<03:39,  2.78it/s] 22%|██▏       | 172/780 [01:25<03:25,  2.96it/s] 22%|██▏       | 173/780 [01:25<03:16,  3.10it/s] 22%|██▏       | 174/780 [01:25<03:09,  3.20it/s] 22%|██▏       | 175/780 [01:26<03:04,  3.28it/s] 23%|██▎       | 176/780 [01:26<03:01,  3.33it/s] 23%|██▎       | 177/780 [01:26<02:58,  3.37it/s] 23%|██▎       | 178/780 [01:26<02:56,  3.40it/s] 23%|██▎       | 179/780 [01:27<02:55,  3.43it/s] 23%|██▎       | 180/780 [01:27<03:08,  3.19it/s] 23%|██▎       | 181/780 [01:27<03:03,  3.27it/s] 23%|██▎       | 182/780 [01:28<02:59,  3.33it/s] 23%|██▎       | 183/780 [01:28<02:57,  3.37it/s] 24%|██▎       | 184/780 [01:28<02:55,  3.40it/s] 24%|██▎       | 185/780 [01:29<02:53,  3.42it/s] 24%|██▍       | 186/780 [01:29<02:52,  3.44it/s] 24%|██▍       | 187/780 [01:29<02:52,  3.45it/s] 24%|██▍       | 188/780 [01:29<02:51,  3.46it/s] 24%|██▍       | 189/780 [01:30<02:50,  3.46it/s] 24%|██▍       | 190/780 [01:30<02:50,  3.47it/s] 24%|██▍       | 191/780 [01:30<02:56,  3.33it/s] 25%|██▍       | 192/780 [01:31<02:54,  3.37it/s] 25%|██▍       | 193/780 [01:31<02:52,  3.40it/s] 25%|██▍       | 194/780 [01:31<02:57,  3.31it/s] 25%|██▌       | 195/780 [01:32<02:54,  3.35it/s] 25%|██▌       | 196/780 [01:32<02:52,  3.39it/s] 25%|██▌       | 197/780 [01:32<02:50,  3.41it/s] 25%|██▌       | 198/780 [01:32<02:49,  3.43it/s] 26%|██▌       | 199/780 [01:33<02:48,  3.44it/s] 26%|██▌       | 200/780 [01:33<02:47,  3.45it/s] 26%|██▌       | 201/780 [01:33<02:47,  3.46it/s] 26%|██▌       | 202/780 [01:34<03:04,  3.13it/s] 26%|██▌       | 203/780 [01:34<02:58,  3.22it/s] 26%|██▌       | 204/780 [01:34<02:54,  3.29it/s] 26%|██▋       | 205/780 [01:34<02:51,  3.34it/s] 26%|██▋       | 206/780 [01:35<02:49,  3.38it/s] 27%|██▋       | 207/780 [01:35<03:00,  3.18it/s] 27%|██▋       | 208/780 [01:35<03:07,  3.05it/s] 27%|██▋       | 209/780 [01:36<03:19,  2.86it/s] 27%|██▋       | 210/780 [01:36<03:09,  3.01it/s] 27%|██▋       | 211/780 [01:37<03:16,  2.90it/s] 27%|██▋       | 212/780 [01:37<03:20,  2.84it/s] 27%|██▋       | 213/780 [01:37<03:08,  3.00it/s] 27%|██▋       | 214/780 [01:38<03:00,  3.13it/s] 28%|██▊       | 215/780 [01:38<02:55,  3.23it/s] 28%|██▊       | 216/780 [01:38<02:51,  3.30it/s] 28%|██▊       | 217/780 [01:38<02:48,  3.35it/s] 28%|██▊       | 218/780 [01:39<02:46,  3.38it/s] 28%|██▊       | 219/780 [01:39<02:44,  3.41it/s] 28%|██▊       | 220/780 [01:39<02:43,  3.43it/s] 28%|██▊       | 221/780 [01:40<02:42,  3.44it/s] 28%|██▊       | 222/780 [01:40<02:41,  3.45it/s] 29%|██▊       | 223/780 [01:40<02:53,  3.22it/s] 29%|██▊       | 224/780 [01:40<02:49,  3.29it/s] 29%|██▉       | 225/780 [01:41<02:46,  3.34it/s] 29%|██▉       | 226/780 [01:41<02:43,  3.38it/s] 29%|██▉       | 227/780 [01:41<02:42,  3.41it/s] 29%|██▉       | 228/780 [01:42<02:41,  3.42it/s] 29%|██▉       | 229/780 [01:42<02:40,  3.44it/s] 29%|██▉       | 230/780 [01:42<02:39,  3.45it/s] 30%|██▉       | 231/780 [01:42<02:38,  3.46it/s] 30%|██▉       | 232/780 [01:43<02:38,  3.46it/s] 30%|██▉       | 233/780 [01:43<02:38,  3.46it/s] 30%|███       | 234/780 [01:43<02:37,  3.46it/s] 30%|███       | 235/780 [01:44<02:37,  3.47it/s] 30%|███       | 236/780 [01:44<02:36,  3.47it/s] 30%|███       | 237/780 [01:44<02:36,  3.47it/s] 31%|███       | 238/780 [01:44<02:36,  3.47it/s] 31%|███       | 239/780 [01:45<02:35,  3.47it/s] 31%|███       | 240/780 [01:45<02:35,  3.47it/s] 31%|███       | 241/780 [01:45<02:35,  3.47it/s] 31%|███       | 242/780 [01:46<02:42,  3.30it/s] 31%|███       | 243/780 [01:46<02:40,  3.35it/s] 31%|███▏      | 244/780 [01:46<02:38,  3.39it/s] 31%|███▏      | 245/780 [01:47<02:36,  3.41it/s] 32%|███▏      | 246/780 [01:47<02:35,  3.43it/s] 32%|███▏      | 247/780 [01:47<02:34,  3.44it/s] 32%|███▏      | 248/780 [01:47<02:34,  3.45it/s] 32%|███▏      | 249/780 [01:48<02:33,  3.46it/s] 32%|███▏      | 250/780 [01:48<02:33,  3.46it/s] 32%|███▏      | 251/780 [01:48<02:32,  3.46it/s] 32%|███▏      | 252/780 [01:49<02:32,  3.47it/s] 32%|███▏      | 253/780 [01:49<02:38,  3.33it/s] 33%|███▎      | 254/780 [01:49<02:35,  3.37it/s] 33%|███▎      | 255/780 [01:49<02:34,  3.40it/s] 33%|███▎      | 256/780 [01:50<02:33,  3.42it/s] 33%|███▎      | 257/780 [01:50<02:32,  3.44it/s] 33%|███▎      | 258/780 [01:50<02:31,  3.45it/s] 33%|███▎      | 259/780 [01:51<02:30,  3.45it/s] 33%|███▎      | 260/780 [01:51<02:30,  3.46it/s] 33%|███▎      | 261/780 [01:51<02:29,  3.46it/s] 34%|███▎      | 262/780 [01:51<02:29,  3.46it/s] 34%|███▎      | 263/780 [01:52<02:29,  3.46it/s] 34%|███▍      | 264/780 [01:52<02:34,  3.34it/s] 34%|███▍      | 265/780 [01:52<02:32,  3.38it/s] 34%|███▍      | 266/780 [01:53<02:30,  3.41it/s] 34%|███▍      | 267/780 [01:53<02:29,  3.42it/s] 34%|███▍      | 268/780 [01:53<02:28,  3.44it/s] 34%|███▍      | 269/780 [01:54<02:28,  3.44it/s] 35%|███▍      | 270/780 [01:54<02:27,  3.45it/s] 35%|███▍      | 271/780 [01:54<02:27,  3.46it/s] 35%|███▍      | 272/780 [01:54<02:26,  3.46it/s] 35%|███▌      | 273/780 [01:55<02:26,  3.46it/s] 35%|███▌      | 274/780 [01:55<02:26,  3.46it/s] 35%|███▌      | 275/780 [01:55<02:34,  3.27it/s] 35%|███▌      | 276/780 [01:56<02:31,  3.33it/s] 36%|███▌      | 277/780 [01:56<02:29,  3.37it/s] 36%|███▌      | 278/780 [01:56<02:27,  3.40it/s] 36%|███▌      | 279/780 [01:56<02:26,  3.42it/s] 36%|███▌      | 280/780 [01:57<02:25,  3.43it/s] 36%|███▌      | 281/780 [01:57<02:24,  3.44it/s] 36%|███▌      | 282/780 [01:57<02:24,  3.45it/s] 36%|███▋      | 283/780 [01:58<02:23,  3.45it/s] 36%|███▋      | 284/780 [01:58<02:23,  3.45it/s] 37%|███▋      | 285/780 [01:58<02:23,  3.46it/s] 37%|███▋      | 286/780 [01:59<02:30,  3.29it/s] 37%|███▋      | 287/780 [01:59<02:27,  3.34it/s] 37%|███▋      | 288/780 [01:59<02:25,  3.37it/s] 37%|███▋      | 289/780 [01:59<02:24,  3.40it/s] 37%|███▋      | 290/780 [02:00<02:23,  3.42it/s] 37%|███▋      | 291/780 [02:00<02:22,  3.43it/s] 37%|███▋      | 292/780 [02:00<02:21,  3.44it/s] 38%|███▊      | 293/780 [02:01<02:21,  3.45it/s] 38%|███▊      | 294/780 [02:01<02:20,  3.45it/s] 38%|███▊      | 295/780 [02:01<02:20,  3.45it/s] 38%|███▊      | 296/780 [02:01<02:20,  3.46it/s] 38%|███▊      | 297/780 [02:02<02:25,  3.33it/s] 38%|███▊      | 298/780 [02:02<02:23,  3.37it/s] 38%|███▊      | 299/780 [02:02<02:21,  3.40it/s] 38%|███▊      | 300/780 [02:03<02:20,  3.42it/s] 39%|███▊      | 301/780 [02:03<02:19,  3.43it/s] 39%|███▊      | 302/780 [02:03<02:19,  3.44it/s] 39%|███▉      | 303/780 [02:04<02:18,  3.45it/s] 39%|███▉      | 304/780 [02:04<02:17,  3.45it/s] 39%|███▉      | 305/780 [02:04<02:17,  3.45it/s] 39%|███▉      | 306/780 [02:04<02:17,  3.46it/s] 39%|███▉      | 307/780 [02:05<02:16,  3.46it/s] 39%|███▉      | 308/780 [02:05<02:24,  3.26it/s] 40%|███▉      | 309/780 [02:05<02:22,  3.32it/s] 40%|███▉      | 310/780 [02:06<02:19,  3.36it/s] 40%|███▉      | 311/780 [02:06<02:18,  3.39it/s] 40%|████      | 312/780 [02:06<02:17,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 15:54:04,551 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 15:54:04,552 >>   Num examples = 4812
[INFO|trainer.py:2145] 2023-08-29 15:54:04,552 >>   Batch size = 8
{'eval_loss': 1.0098013877868652, 'eval_runtime': 13.1339, 'eval_samples_per_second': 366.38, 'eval_steps_per_second': 45.836, 'epoch': 1.0}

  0%|          | 0/602 [00:00<?, ?it/s][A
  1%|          | 6/602 [00:00<00:10, 57.18it/s][A
  2%|▏         | 12/602 [00:00<00:11, 50.60it/s][A
  3%|▎         | 18/602 [00:00<00:11, 48.77it/s][A
  4%|▍         | 23/602 [00:00<00:12, 48.12it/s][A
  5%|▍         | 28/602 [00:00<00:12, 47.67it/s][A
  5%|▌         | 33/602 [00:00<00:12, 47.37it/s][A
  6%|▋         | 38/602 [00:00<00:11, 47.06it/s][A
  7%|▋         | 43/602 [00:00<00:11, 46.69it/s][A
  8%|▊         | 48/602 [00:01<00:11, 46.59it/s][A
  9%|▉         | 53/602 [00:01<00:11, 46.76it/s][A
 10%|▉         | 58/602 [00:01<00:11, 46.80it/s][A
 10%|█         | 63/602 [00:01<00:11, 46.81it/s][A
 11%|█▏        | 68/602 [00:01<00:11, 46.87it/s][A
 12%|█▏        | 73/602 [00:01<00:12, 43.01it/s][A
 13%|█▎        | 78/602 [00:01<00:11, 44.18it/s][A
 14%|█▍        | 83/602 [00:01<00:11, 44.92it/s][A
 15%|█▍        | 88/602 [00:01<00:11, 45.50it/s][A
 15%|█▌        | 93/602 [00:01<00:11, 45.92it/s][A
 16%|█▋        | 98/602 [00:02<00:10, 46.15it/s][A
 17%|█▋        | 103/602 [00:02<00:10, 46.33it/s][A
 18%|█▊        | 108/602 [00:02<00:10, 46.47it/s][A
 19%|█▉        | 113/602 [00:02<00:10, 46.27it/s][A
 20%|█▉        | 118/602 [00:02<00:10, 46.31it/s][A
 20%|██        | 123/602 [00:02<00:10, 46.46it/s][A
 21%|██▏       | 128/602 [00:02<00:10, 46.54it/s][A
 22%|██▏       | 133/602 [00:02<00:10, 46.60it/s][A
 23%|██▎       | 138/602 [00:02<00:09, 46.68it/s][A
 24%|██▍       | 143/602 [00:03<00:09, 46.64it/s][A
 25%|██▍       | 148/602 [00:03<00:09, 46.61it/s][A
 25%|██▌       | 153/602 [00:03<00:09, 46.71it/s][A
 26%|██▌       | 158/602 [00:03<00:09, 46.71it/s][A
 27%|██▋       | 163/602 [00:03<00:09, 46.71it/s][A
 28%|██▊       | 168/602 [00:03<00:09, 46.74it/s][A
 29%|██▊       | 173/602 [00:03<00:09, 46.69it/s][A
 30%|██▉       | 178/602 [00:03<00:09, 46.64it/s][A
 30%|███       | 183/602 [00:03<00:08, 46.68it/s][A
 31%|███       | 188/602 [00:04<00:08, 46.69it/s][A
 32%|███▏      | 193/602 [00:04<00:08, 46.70it/s][A
 33%|███▎      | 198/602 [00:04<00:08, 46.61it/s][A
 34%|███▎      | 203/602 [00:04<00:08, 46.63it/s][A
 35%|███▍      | 208/602 [00:04<00:08, 46.67it/s][A
 35%|███▌      | 213/602 [00:04<00:09, 42.85it/s][A
 36%|███▌      | 218/602 [00:04<00:08, 43.90it/s][A
 37%|███▋      | 223/602 [00:04<00:08, 44.69it/s][A
 38%|███▊      | 228/602 [00:04<00:08, 45.33it/s][A
 39%|███▊      | 233/602 [00:05<00:08, 45.67it/s][A
 40%|███▉      | 238/602 [00:05<00:07, 46.02it/s][A
 40%|████      | 243/602 [00:05<00:07, 46.24it/s][A
 41%|████      | 248/602 [00:05<00:07, 46.33it/s][A
 42%|████▏     | 253/602 [00:05<00:07, 46.39it/s][A
 43%|████▎     | 258/602 [00:05<00:07, 46.44it/s][A
 44%|████▎     | 263/602 [00:05<00:07, 46.53it/s][A
 45%|████▍     | 268/602 [00:05<00:07, 46.60it/s][A
 45%|████▌     | 273/602 [00:05<00:07, 46.67it/s][A
 46%|████▌     | 278/602 [00:05<00:06, 46.69it/s][A
 47%|████▋     | 283/602 [00:06<00:06, 46.71it/s][A
 48%|████▊     | 288/602 [00:06<00:06, 46.72it/s][A
 49%|████▊     | 293/602 [00:06<00:06, 46.68it/s][A
 50%|████▉     | 298/602 [00:06<00:06, 46.67it/s][A
 50%|█████     | 303/602 [00:06<00:06, 46.63it/s][A
 51%|█████     | 308/602 [00:06<00:06, 46.69it/s][A
 52%|█████▏    | 313/602 [00:06<00:06, 46.69it/s][A
 53%|█████▎    | 318/602 [00:06<00:07, 36.32it/s][A
 53%|█████▎    | 322/602 [00:07<00:08, 32.27it/s][A
 54%|█████▍    | 326/602 [00:07<00:08, 30.83it/s][A
 55%|█████▍    | 331/602 [00:07<00:07, 34.66it/s][A
 56%|█████▌    | 336/602 [00:07<00:07, 37.72it/s][A
 57%|█████▋    | 341/602 [00:07<00:06, 40.14it/s][A
 57%|█████▋    | 346/602 [00:07<00:06, 41.97it/s][A
 58%|█████▊    | 351/602 [00:07<00:05, 43.31it/s][A
 59%|█████▉    | 356/602 [00:07<00:05, 44.31it/s][A
 60%|█████▉    | 361/602 [00:08<00:05, 45.06it/s][A
 61%|██████    | 366/602 [00:08<00:05, 45.45it/s][A
 62%|██████▏   | 371/602 [00:08<00:05, 45.74it/s][A
 62%|██████▏   | 376/602 [00:08<00:04, 46.12it/s][A
 63%|██████▎   | 381/602 [00:08<00:04, 46.27it/s][A
 64%|██████▍   | 386/602 [00:08<00:04, 46.37it/s][A
 65%|██████▍   | 391/602 [00:08<00:04, 46.49it/s][A
 66%|██████▌   | 396/602 [00:08<00:04, 46.53it/s][A
 67%|██████▋   | 401/602 [00:08<00:04, 46.58it/s][A
 67%|██████▋   | 406/602 [00:08<00:04, 46.60it/s][A
 68%|██████▊   | 411/602 [00:09<00:04, 46.61it/s][A
 69%|██████▉   | 416/602 [00:09<00:03, 46.59it/s][A
 70%|██████▉   | 421/602 [00:09<00:03, 46.64it/s][A
 71%|███████   | 426/602 [00:09<00:03, 46.62it/s][A
 72%|███████▏  | 431/602 [00:09<00:03, 46.72it/s][A
 72%|███████▏  | 436/602 [00:09<00:03, 46.74it/s][A
 73%|███████▎  | 441/602 [00:09<00:03, 46.63it/s][A
 74%|███████▍  | 446/602 [00:09<00:03, 41.72it/s][A
 75%|███████▍  | 451/602 [00:09<00:03, 43.11it/s][A
 76%|███████▌  | 456/602 [00:10<00:03, 44.13it/s][A
 77%|███████▋  | 461/602 [00:10<00:03, 44.86it/s][A
 77%|███████▋  | 466/602 [00:10<00:02, 45.43it/s][A
 78%|███████▊  | 471/602 [00:10<00:02, 45.85it/s][A
 79%|███████▉  | 476/602 [00:10<00:02, 46.09it/s][A
 80%|███████▉  | 481/602 [00:10<00:02, 46.27it/s][A
 81%|████████  | 486/602 [00:10<00:02, 46.28it/s][A
 82%|████████▏ | 491/602 [00:10<00:02, 46.35it/s][A
 82%|████████▏ | 496/602 [00:10<00:02, 46.47it/s][A
 83%|████████▎ | 501/602 [00:11<00:02, 46.53it/s][A
 84%|████████▍ | 506/602 [00:11<00:02, 46.61it/s][A
 85%|████████▍ | 511/602 [00:11<00:01, 46.60it/s][A
 86%|████████▌ | 516/602 [00:11<00:01, 46.65it/s][A
 87%|████████▋ | 521/602 [00:11<00:01, 46.67it/s][A
 87%|████████▋ | 526/602 [00:11<00:01, 46.69it/s][A
 88%|████████▊ | 531/602 [00:11<00:01, 46.50it/s][A
 89%|████████▉ | 536/602 [00:11<00:01, 40.03it/s][A
 90%|████████▉ | 541/602 [00:11<00:01, 41.83it/s][A
 91%|█████████ | 546/602 [00:12<00:01, 43.23it/s][A
 92%|█████████▏| 551/602 [00:12<00:01, 44.25it/s][A
 92%|█████████▏| 556/602 [00:12<00:01, 45.01it/s][A
 93%|█████████▎| 561/602 [00:12<00:00, 45.44it/s][A
 94%|█████████▍| 566/602 [00:12<00:00, 45.77it/s][A
 95%|█████████▍| 571/602 [00:12<00:00, 46.11it/s][A
 96%|█████████▌| 576/602 [00:12<00:00, 46.03it/s][A
 97%|█████████▋| 581/602 [00:12<00:00, 46.02it/s][A
 97%|█████████▋| 586/602 [00:13<00:00, 38.42it/s][A
 98%|█████████▊| 591/602 [00:13<00:00, 40.60it/s][A
 99%|█████████▉| 596/602 [00:13<00:00, 42.24it/s][A
100%|█████████▉| 601/602 [00:13<00:00, 43.53it/s][A
                                                 [A                                                 
100%|██████████| 602/602 [00:13<00:00, 43.53it/s][A 40%|████      | 312/780 [02:20<02:17,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 15:54:18,475 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 15:54:18,851 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 15:54:24,247 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 15:54:24,500 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 15:54:24,675 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:40<1:19:52, 10.26s/it] 40%|████      | 314/780 [02:40<56:40,  7.30s/it]   40%|████      | 315/780 [02:40<40:15,  5.19s/it] 41%|████      | 316/780 [02:41<28:47,  3.72s/it] 41%|████      | 317/780 [02:41<20:46,  2.69s/it] 41%|████      | 318/780 [02:41<15:10,  1.97s/it] 41%|████      | 319/780 [02:42<11:15,  1.47s/it] 41%|████      | 320/780 [02:42<08:31,  1.11s/it] 41%|████      | 321/780 [02:42<06:36,  1.16it/s] 41%|████▏     | 322/780 [02:42<05:16,  1.45it/s] 41%|████▏     | 323/780 [02:43<04:20,  1.75it/s] 42%|████▏     | 324/780 [02:43<03:41,  2.06it/s] 42%|████▏     | 325/780 [02:43<03:28,  2.19it/s] 42%|████▏     | 326/780 [02:44<03:04,  2.46it/s] 42%|████▏     | 327/780 [02:44<02:47,  2.70it/s] 42%|████▏     | 328/780 [02:44<02:36,  2.89it/s] 42%|████▏     | 329/780 [02:44<02:28,  3.05it/s] 42%|████▏     | 330/780 [02:45<02:22,  3.16it/s] 42%|████▏     | 331/780 [02:45<02:18,  3.25it/s] 43%|████▎     | 332/780 [02:45<02:15,  3.32it/s] 43%|████▎     | 333/780 [02:46<02:12,  3.36it/s] 43%|████▎     | 334/780 [02:46<02:11,  3.40it/s] 43%|████▎     | 335/780 [02:46<02:10,  3.41it/s] 43%|████▎     | 336/780 [02:47<02:09,  3.43it/s] 43%|████▎     | 337/780 [02:47<02:08,  3.45it/s] 43%|████▎     | 338/780 [02:47<02:26,  3.02it/s] 43%|████▎     | 339/780 [02:48<02:20,  3.15it/s] 44%|████▎     | 340/780 [02:48<02:15,  3.24it/s] 44%|████▎     | 341/780 [02:48<02:12,  3.30it/s] 44%|████▍     | 342/780 [02:48<02:10,  3.36it/s] 44%|████▍     | 343/780 [02:49<02:08,  3.39it/s] 44%|████▍     | 344/780 [02:49<02:07,  3.42it/s] 44%|████▍     | 345/780 [02:49<02:06,  3.44it/s] 44%|████▍     | 346/780 [02:50<02:05,  3.45it/s] 44%|████▍     | 347/780 [02:50<02:05,  3.46it/s] 45%|████▍     | 348/780 [02:50<02:22,  3.03it/s] 45%|████▍     | 349/780 [02:51<02:16,  3.15it/s] 45%|████▍     | 350/780 [02:51<02:12,  3.24it/s] 45%|████▌     | 351/780 [02:51<02:09,  3.31it/s] 45%|████▌     | 352/780 [02:51<02:07,  3.36it/s] 45%|████▌     | 353/780 [02:52<02:05,  3.39it/s] 45%|████▌     | 354/780 [02:52<02:04,  3.41it/s] 46%|████▌     | 355/780 [02:52<02:03,  3.43it/s] 46%|████▌     | 356/780 [02:53<02:03,  3.45it/s] 46%|████▌     | 357/780 [02:53<02:02,  3.45it/s] 46%|████▌     | 358/780 [02:53<02:11,  3.22it/s] 46%|████▌     | 359/780 [02:53<02:07,  3.29it/s] 46%|████▌     | 360/780 [02:54<02:05,  3.34it/s] 46%|████▋     | 361/780 [02:54<02:03,  3.38it/s] 46%|████▋     | 362/780 [02:54<02:02,  3.40it/s] 47%|████▋     | 363/780 [02:55<02:01,  3.42it/s] 47%|████▋     | 364/780 [02:55<02:01,  3.43it/s] 47%|████▋     | 365/780 [02:55<02:00,  3.44it/s] 47%|████▋     | 366/780 [02:55<01:59,  3.45it/s] 47%|████▋     | 367/780 [02:56<01:59,  3.46it/s] 47%|████▋     | 368/780 [02:56<01:59,  3.46it/s] 47%|████▋     | 369/780 [02:56<02:07,  3.23it/s] 47%|████▋     | 370/780 [02:57<02:04,  3.30it/s] 48%|████▊     | 371/780 [02:57<02:02,  3.35it/s] 48%|████▊     | 372/780 [02:57<02:00,  3.39it/s] 48%|████▊     | 373/780 [02:58<01:59,  3.41it/s] 48%|████▊     | 374/780 [02:58<01:58,  3.43it/s] 48%|████▊     | 375/780 [02:58<01:57,  3.44it/s] 48%|████▊     | 376/780 [02:58<01:57,  3.45it/s] 48%|████▊     | 377/780 [02:59<01:56,  3.46it/s] 48%|████▊     | 378/780 [02:59<01:56,  3.46it/s] 49%|████▊     | 379/780 [02:59<01:55,  3.46it/s] 49%|████▊     | 380/780 [03:00<02:08,  3.11it/s] 49%|████▉     | 381/780 [03:00<02:04,  3.21it/s] 49%|████▉     | 382/780 [03:00<02:01,  3.28it/s] 49%|████▉     | 383/780 [03:01<01:58,  3.34it/s] 49%|████▉     | 384/780 [03:01<01:57,  3.37it/s] 49%|████▉     | 385/780 [03:01<01:56,  3.40it/s] 49%|████▉     | 386/780 [03:01<01:55,  3.42it/s] 50%|████▉     | 387/780 [03:02<01:54,  3.44it/s] 50%|████▉     | 388/780 [03:02<01:53,  3.45it/s] 50%|████▉     | 389/780 [03:02<01:53,  3.45it/s] 50%|█████     | 390/780 [03:03<01:52,  3.46it/s] 50%|█████     | 391/780 [03:03<02:00,  3.22it/s] 50%|█████     | 392/780 [03:03<01:57,  3.29it/s] 50%|█████     | 393/780 [03:04<01:55,  3.34it/s] 51%|█████     | 394/780 [03:04<01:54,  3.38it/s] 51%|█████     | 395/780 [03:04<01:52,  3.41it/s] 51%|█████     | 396/780 [03:04<01:52,  3.43it/s] 51%|█████     | 397/780 [03:05<01:51,  3.44it/s] 51%|█████     | 398/780 [03:05<01:50,  3.45it/s] 51%|█████     | 399/780 [03:05<01:50,  3.45it/s] 51%|█████▏    | 400/780 [03:06<01:50,  3.45it/s] 51%|█████▏    | 401/780 [03:06<01:49,  3.46it/s] 52%|█████▏    | 402/780 [03:06<01:53,  3.33it/s] 52%|█████▏    | 403/780 [03:06<01:51,  3.37it/s] 52%|█████▏    | 404/780 [03:07<01:50,  3.40it/s] 52%|█████▏    | 405/780 [03:07<01:49,  3.42it/s] 52%|█████▏    | 406/780 [03:07<01:48,  3.43it/s] 52%|█████▏    | 407/780 [03:08<01:48,  3.44it/s] 52%|█████▏    | 408/780 [03:08<01:47,  3.45it/s] 52%|█████▏    | 409/780 [03:08<01:47,  3.46it/s] 53%|█████▎    | 410/780 [03:08<01:46,  3.46it/s] 53%|█████▎    | 411/780 [03:09<01:46,  3.46it/s] 53%|█████▎    | 412/780 [03:09<01:46,  3.46it/s] 53%|█████▎    | 413/780 [03:09<01:54,  3.20it/s] 53%|█████▎    | 414/780 [03:10<01:51,  3.28it/s] 53%|█████▎    | 415/780 [03:10<01:49,  3.33it/s] 53%|█████▎    | 416/780 [03:10<01:48,  3.37it/s] 53%|█████▎    | 417/780 [03:11<01:46,  3.40it/s] 54%|█████▎    | 418/780 [03:11<01:45,  3.42it/s] 54%|█████▎    | 419/780 [03:11<01:45,  3.43it/s] 54%|█████▍    | 420/780 [03:11<01:44,  3.44it/s] 54%|█████▍    | 421/780 [03:12<01:44,  3.45it/s] 54%|█████▍    | 422/780 [03:12<01:43,  3.45it/s] 54%|█████▍    | 423/780 [03:12<01:43,  3.46it/s] 54%|█████▍    | 424/780 [03:13<01:49,  3.26it/s] 54%|█████▍    | 425/780 [03:13<01:47,  3.32it/s] 55%|█████▍    | 426/780 [03:13<01:57,  3.01it/s] 55%|█████▍    | 427/780 [03:14<01:52,  3.13it/s] 55%|█████▍    | 428/780 [03:14<01:49,  3.22it/s] 55%|█████▌    | 429/780 [03:14<01:46,  3.29it/s] 55%|█████▌    | 430/780 [03:14<01:44,  3.34it/s] 55%|█████▌    | 431/780 [03:15<01:43,  3.38it/s] 55%|█████▌    | 432/780 [03:15<01:42,  3.41it/s] 56%|█████▌    | 433/780 [03:15<01:41,  3.42it/s] 56%|█████▌    | 434/780 [03:16<01:40,  3.44it/s] 56%|█████▌    | 435/780 [03:16<01:40,  3.45it/s] 56%|█████▌    | 436/780 [03:16<01:39,  3.45it/s] 56%|█████▌    | 437/780 [03:17<01:39,  3.46it/s] 56%|█████▌    | 438/780 [03:17<01:38,  3.46it/s] 56%|█████▋    | 439/780 [03:17<01:38,  3.46it/s] 56%|█████▋    | 440/780 [03:17<01:38,  3.46it/s] 57%|█████▋    | 441/780 [03:18<01:45,  3.20it/s] 57%|█████▋    | 442/780 [03:18<01:48,  3.13it/s] 57%|█████▋    | 443/780 [03:18<01:44,  3.21it/s] 57%|█████▋    | 444/780 [03:19<01:42,  3.28it/s] 57%|█████▋    | 445/780 [03:19<01:40,  3.34it/s] 57%|█████▋    | 446/780 [03:19<01:39,  3.37it/s] 57%|█████▋    | 447/780 [03:20<01:37,  3.40it/s] 57%|█████▋    | 448/780 [03:20<01:37,  3.42it/s] 58%|█████▊    | 449/780 [03:20<01:36,  3.43it/s] 58%|█████▊    | 450/780 [03:20<01:35,  3.44it/s] 58%|█████▊    | 451/780 [03:21<01:40,  3.27it/s] 58%|█████▊    | 452/780 [03:21<01:38,  3.32it/s] 58%|█████▊    | 453/780 [03:21<01:37,  3.36it/s] 58%|█████▊    | 454/780 [03:22<01:36,  3.39it/s] 58%|█████▊    | 455/780 [03:22<01:35,  3.41it/s] 58%|█████▊    | 456/780 [03:22<01:34,  3.43it/s] 59%|█████▊    | 457/780 [03:22<01:33,  3.44it/s] 59%|█████▊    | 458/780 [03:23<01:33,  3.45it/s] 59%|█████▉    | 459/780 [03:23<01:33,  3.45it/s] 59%|█████▉    | 460/780 [03:23<01:32,  3.45it/s] 59%|█████▉    | 461/780 [03:24<01:32,  3.45it/s] 59%|█████▉    | 462/780 [03:24<01:38,  3.24it/s] 59%|█████▉    | 463/780 [03:24<01:35,  3.30it/s] 59%|█████▉    | 464/780 [03:25<01:34,  3.35it/s] 60%|█████▉    | 465/780 [03:25<01:33,  3.38it/s] 60%|█████▉    | 466/780 [03:25<01:32,  3.41it/s] 60%|█████▉    | 467/780 [03:25<01:31,  3.42it/s] 60%|██████    | 468/780 [03:26<01:30,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 15:55:24,089 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 15:55:24,089 >>   Num examples = 4812
[INFO|trainer.py:2145] 2023-08-29 15:55:24,089 >>   Batch size = 8
{'eval_loss': 1.0272631645202637, 'eval_runtime': 13.3935, 'eval_samples_per_second': 359.279, 'eval_steps_per_second': 44.947, 'epoch': 2.0}

  0%|          | 0/602 [00:00<?, ?it/s][A
  1%|          | 6/602 [00:00<00:10, 57.05it/s][A
  2%|▏         | 12/602 [00:00<00:11, 50.53it/s][A
  3%|▎         | 18/602 [00:00<00:11, 48.92it/s][A
  4%|▍         | 23/602 [00:00<00:12, 48.18it/s][A
  5%|▍         | 28/602 [00:00<00:12, 47.63it/s][A
  5%|▌         | 33/602 [00:00<00:12, 47.25it/s][A
  6%|▋         | 38/602 [00:00<00:11, 47.00it/s][A
  7%|▋         | 43/602 [00:00<00:11, 46.65it/s][A
  8%|▊         | 48/602 [00:01<00:13, 41.76it/s][A
  9%|▉         | 53/602 [00:01<00:12, 43.22it/s][A
 10%|▉         | 58/602 [00:01<00:12, 44.26it/s][A
 10%|█         | 63/602 [00:01<00:11, 44.96it/s][A
 11%|█▏        | 68/602 [00:01<00:11, 45.57it/s][A
 12%|█▏        | 73/602 [00:01<00:11, 46.04it/s][A
 13%|█▎        | 78/602 [00:01<00:11, 46.35it/s][A
 14%|█▍        | 83/602 [00:01<00:11, 46.48it/s][A
 15%|█▍        | 88/602 [00:01<00:11, 46.07it/s][A
 15%|█▌        | 93/602 [00:02<00:11, 46.15it/s][A
 16%|█▋        | 98/602 [00:02<00:10, 46.19it/s][A
 17%|█▋        | 103/602 [00:02<00:10, 46.38it/s][A
 18%|█▊        | 108/602 [00:02<00:10, 46.50it/s][A
 19%|█▉        | 113/602 [00:02<00:10, 46.53it/s][A
 20%|█▉        | 118/602 [00:02<00:10, 46.56it/s][A
 20%|██        | 123/602 [00:02<00:10, 46.68it/s][A
 21%|██▏       | 128/602 [00:02<00:10, 46.63it/s][A
 22%|██▏       | 133/602 [00:02<00:10, 46.71it/s][A
 23%|██▎       | 138/602 [00:02<00:09, 46.75it/s][A
 24%|██▍       | 143/602 [00:03<00:09, 46.60it/s][A
 25%|██▍       | 148/602 [00:03<00:09, 46.63it/s][A
 25%|██▌       | 153/602 [00:03<00:09, 46.73it/s][A
 26%|██▌       | 158/602 [00:03<00:09, 46.57it/s][A
 27%|██▋       | 163/602 [00:03<00:09, 46.64it/s][A
 28%|██▊       | 168/602 [00:03<00:09, 46.72it/s][A
 29%|██▊       | 173/602 [00:03<00:09, 46.67it/s][A
 30%|██▉       | 178/602 [00:03<00:09, 46.72it/s][A
 30%|███       | 183/602 [00:03<00:08, 46.73it/s][A
 31%|███       | 188/602 [00:04<00:09, 42.70it/s][A
 32%|███▏      | 193/602 [00:04<00:09, 43.90it/s][A
 33%|███▎      | 198/602 [00:04<00:09, 44.76it/s][A
 34%|███▎      | 203/602 [00:04<00:08, 45.26it/s][A
 35%|███▍      | 208/602 [00:04<00:08, 45.55it/s][A
 35%|███▌      | 213/602 [00:04<00:08, 45.99it/s][A
 36%|███▌      | 218/602 [00:04<00:08, 46.24it/s][A
 37%|███▋      | 223/602 [00:04<00:08, 46.44it/s][A
 38%|███▊      | 228/602 [00:04<00:08, 46.35it/s][A
 39%|███▊      | 233/602 [00:05<00:07, 46.33it/s][A
 40%|███▉      | 238/602 [00:05<00:07, 46.46it/s][A
 40%|████      | 243/602 [00:05<00:07, 46.46it/s][A
 41%|████      | 248/602 [00:05<00:07, 46.55it/s][A
 42%|████▏     | 253/602 [00:05<00:07, 46.65it/s][A
 43%|████▎     | 258/602 [00:05<00:07, 46.65it/s][A
 44%|████▎     | 263/602 [00:05<00:07, 46.70it/s][A
 45%|████▍     | 268/602 [00:05<00:07, 46.66it/s][A
 45%|████▌     | 273/602 [00:05<00:07, 46.60it/s][A
 46%|████▌     | 278/602 [00:06<00:06, 46.61it/s][A
 47%|████▋     | 283/602 [00:06<00:06, 46.70it/s][A
 48%|████▊     | 288/602 [00:06<00:06, 46.61it/s][A
 49%|████▊     | 293/602 [00:06<00:06, 46.64it/s][A
 50%|████▉     | 298/602 [00:06<00:06, 46.69it/s][A
 50%|█████     | 303/602 [00:06<00:06, 46.57it/s][A
 51%|█████     | 308/602 [00:06<00:06, 46.69it/s][A
 52%|█████▏    | 313/602 [00:06<00:06, 46.67it/s][A
 53%|█████▎    | 318/602 [00:06<00:06, 46.51it/s][A
 54%|█████▎    | 323/602 [00:07<00:05, 46.62it/s][A
 54%|█████▍    | 328/602 [00:07<00:06, 41.60it/s][A
 55%|█████▌    | 333/602 [00:07<00:06, 43.02it/s][A
 56%|█████▌    | 338/602 [00:07<00:05, 44.13it/s][A
 57%|█████▋    | 343/602 [00:07<00:05, 44.85it/s][A
 58%|█████▊    | 348/602 [00:07<00:05, 45.47it/s][A
 59%|█████▊    | 353/602 [00:07<00:05, 45.90it/s][A
 59%|█████▉    | 358/602 [00:07<00:05, 46.14it/s][A
 60%|██████    | 363/602 [00:07<00:05, 46.33it/s][A
 61%|██████    | 368/602 [00:07<00:05, 46.06it/s][A
 62%|██████▏   | 373/602 [00:08<00:04, 46.20it/s][A
 63%|██████▎   | 378/602 [00:08<00:04, 46.36it/s][A
 64%|██████▎   | 383/602 [00:08<00:04, 46.48it/s][A
 64%|██████▍   | 388/602 [00:08<00:04, 46.51it/s][A
 65%|██████▌   | 393/602 [00:08<00:04, 46.60it/s][A
 66%|██████▌   | 398/602 [00:08<00:04, 46.67it/s][A
 67%|██████▋   | 403/602 [00:08<00:04, 46.72it/s][A
 68%|██████▊   | 408/602 [00:08<00:04, 46.74it/s][A
 69%|██████▊   | 413/602 [00:08<00:04, 46.59it/s][A
 69%|██████▉   | 418/602 [00:09<00:03, 46.46it/s][A
 70%|███████   | 423/602 [00:09<00:03, 46.45it/s][A
 71%|███████   | 428/602 [00:09<00:03, 46.59it/s][A
 72%|███████▏  | 433/602 [00:09<00:03, 46.61it/s][A
 73%|███████▎  | 438/602 [00:09<00:03, 46.61it/s][A
 74%|███████▎  | 443/602 [00:09<00:03, 46.73it/s][A
 74%|███████▍  | 448/602 [00:09<00:03, 46.67it/s][A
 75%|███████▌  | 453/602 [00:09<00:03, 46.63it/s][A
 76%|███████▌  | 458/602 [00:09<00:03, 46.64it/s][A
 77%|███████▋  | 463/602 [00:10<00:02, 46.50it/s][A
 78%|███████▊  | 468/602 [00:10<00:03, 38.89it/s][A
 79%|███████▊  | 473/602 [00:10<00:03, 40.94it/s][A
 79%|███████▉  | 478/602 [00:10<00:02, 42.48it/s][A
 80%|████████  | 483/602 [00:10<00:02, 43.73it/s][A
 81%|████████  | 488/602 [00:10<00:02, 44.57it/s][A
 82%|████████▏ | 493/602 [00:10<00:02, 45.24it/s][A
 83%|████████▎ | 498/602 [00:10<00:02, 45.68it/s][A
 84%|████████▎ | 503/602 [00:10<00:02, 45.96it/s][A
 84%|████████▍ | 508/602 [00:11<00:02, 45.90it/s][A
 85%|████████▌ | 513/602 [00:11<00:01, 46.06it/s][A
 86%|████████▌ | 518/602 [00:11<00:01, 46.26it/s][A
 87%|████████▋ | 523/602 [00:11<00:01, 46.33it/s][A
 88%|████████▊ | 528/602 [00:11<00:01, 46.48it/s][A
 89%|████████▊ | 533/602 [00:11<00:01, 36.51it/s][A
 89%|████████▉ | 538/602 [00:11<00:01, 38.98it/s][A
 90%|█████████ | 543/602 [00:11<00:01, 40.98it/s][A
 91%|█████████ | 548/602 [00:12<00:01, 42.54it/s][A
 92%|█████████▏| 553/602 [00:12<00:01, 43.75it/s][A
 93%|█████████▎| 558/602 [00:12<00:00, 44.64it/s][A
 94%|█████████▎| 563/602 [00:12<00:00, 45.25it/s][A
 94%|█████████▍| 568/602 [00:12<00:00, 45.60it/s][A
 95%|█████████▌| 573/602 [00:13<00:01, 20.43it/s][A
 96%|█████████▌| 578/602 [00:13<00:00, 24.62it/s][A
 97%|█████████▋| 582/602 [00:13<00:00, 24.37it/s][A
 98%|█████████▊| 587/602 [00:13<00:00, 28.65it/s][A
 98%|█████████▊| 592/602 [00:13<00:00, 32.50it/s][A
 99%|█████████▉| 597/602 [00:13<00:00, 35.80it/s][A
100%|██████████| 602/602 [00:13<00:00, 38.59it/s][A
                                                 [A                                                 
100%|██████████| 602/602 [00:13<00:00, 38.59it/s][A 60%|██████    | 468/780 [03:40<01:30,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 15:55:38,672 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 15:55:39,191 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 15:55:45,345 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 15:55:46,320 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 15:55:46,565 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [04:02<56:43, 10.94s/it] 60%|██████    | 470/780 [04:02<40:07,  7.77s/it] 60%|██████    | 471/780 [04:02<28:26,  5.52s/it] 61%|██████    | 472/780 [04:02<20:17,  3.95s/it] 61%|██████    | 473/780 [04:03<14:35,  2.85s/it] 61%|██████    | 474/780 [04:03<10:37,  2.08s/it] 61%|██████    | 475/780 [04:03<07:51,  1.54s/it] 61%|██████    | 476/780 [04:04<05:54,  1.17s/it] 61%|██████    | 477/780 [04:04<04:33,  1.11it/s] 61%|██████▏   | 478/780 [04:04<03:36,  1.39it/s] 61%|██████▏   | 479/780 [04:04<02:57,  1.70it/s] 62%|██████▏   | 480/780 [04:05<02:29,  2.00it/s] 62%|██████▏   | 481/780 [04:05<02:12,  2.26it/s] 62%|██████▏   | 482/780 [04:05<01:58,  2.52it/s] 62%|██████▏   | 483/780 [04:06<01:47,  2.75it/s] 62%|██████▏   | 484/780 [04:06<01:40,  2.93it/s] 62%|██████▏   | 485/780 [04:06<01:35,  3.08it/s] 62%|██████▏   | 486/780 [04:06<01:32,  3.19it/s] 62%|██████▏   | 487/780 [04:07<01:29,  3.27it/s] 63%|██████▎   | 488/780 [04:07<01:27,  3.33it/s] 63%|██████▎   | 489/780 [04:07<01:26,  3.37it/s] 63%|██████▎   | 490/780 [04:08<01:25,  3.40it/s] 63%|██████▎   | 491/780 [04:08<01:24,  3.42it/s] 63%|██████▎   | 492/780 [04:08<01:29,  3.21it/s] 63%|██████▎   | 493/780 [04:09<01:27,  3.29it/s] 63%|██████▎   | 494/780 [04:09<01:25,  3.34it/s] 63%|██████▎   | 495/780 [04:09<01:24,  3.38it/s] 64%|██████▎   | 496/780 [04:09<01:23,  3.40it/s] 64%|██████▎   | 497/780 [04:10<01:22,  3.42it/s] 64%|██████▍   | 498/780 [04:10<01:21,  3.44it/s] 64%|██████▍   | 499/780 [04:10<01:21,  3.45it/s] 64%|██████▍   | 500/780 [04:11<01:21,  3.46it/s]                                                  64%|██████▍   | 500/780 [04:11<01:21,  3.46it/s] 64%|██████▍   | 501/780 [04:11<01:20,  3.46it/s] 64%|██████▍   | 502/780 [04:11<01:20,  3.46it/s] 64%|██████▍   | 503/780 [04:12<01:26,  3.21it/s] 65%|██████▍   | 504/780 [04:12<01:24,  3.28it/s] 65%|██████▍   | 505/780 [04:12<01:22,  3.34it/s] 65%|██████▍   | 506/780 [04:12<01:21,  3.38it/s] 65%|██████▌   | 507/780 [04:13<01:20,  3.40it/s] 65%|██████▌   | 508/780 [04:13<01:19,  3.42it/s] 65%|██████▌   | 509/780 [04:13<01:18,  3.44it/s] 65%|██████▌   | 510/780 [04:14<01:18,  3.45it/s] 66%|██████▌   | 511/780 [04:14<01:17,  3.45it/s] 66%|██████▌   | 512/780 [04:14<01:17,  3.46it/s] 66%|██████▌   | 513/780 [04:14<01:17,  3.46it/s] 66%|██████▌   | 514/780 [04:15<01:21,  3.28it/s] 66%|██████▌   | 515/780 [04:15<01:19,  3.33it/s] 66%|██████▌   | 516/780 [04:15<01:18,  3.37it/s] 66%|██████▋   | 517/780 [04:16<01:17,  3.40it/s] 66%|██████▋   | 518/780 [04:16<01:16,  3.42it/s] 67%|██████▋   | 519/780 [04:16<01:15,  3.43it/s] 67%|██████▋   | 520/780 [04:16<01:15,  3.45it/s] 67%|██████▋   | 521/780 [04:17<01:15,  3.45it/s] 67%|██████▋   | 522/780 [04:17<01:14,  3.46it/s] 67%|██████▋   | 523/780 [04:17<01:14,  3.46it/s] 67%|██████▋   | 524/780 [04:18<01:13,  3.46it/s] 67%|██████▋   | 525/780 [04:18<01:13,  3.47it/s] 67%|██████▋   | 526/780 [04:18<01:13,  3.47it/s] 68%|██████▊   | 527/780 [04:18<01:12,  3.47it/s] 68%|██████▊   | 528/780 [04:19<01:12,  3.46it/s] 68%|██████▊   | 529/780 [04:19<01:12,  3.46it/s] 68%|██████▊   | 530/780 [04:19<01:16,  3.26it/s] 68%|██████▊   | 531/780 [04:20<01:15,  3.32it/s] 68%|██████▊   | 532/780 [04:20<01:13,  3.36it/s] 68%|██████▊   | 533/780 [04:20<01:12,  3.39it/s] 68%|██████▊   | 534/780 [04:21<01:12,  3.42it/s] 69%|██████▊   | 535/780 [04:21<01:11,  3.43it/s] 69%|██████▊   | 536/780 [04:21<01:10,  3.44it/s] 69%|██████▉   | 537/780 [04:21<01:10,  3.45it/s] 69%|██████▉   | 538/780 [04:22<01:10,  3.45it/s] 69%|██████▉   | 539/780 [04:22<01:09,  3.46it/s] 69%|██████▉   | 540/780 [04:22<01:09,  3.46it/s] 69%|██████▉   | 541/780 [04:23<01:11,  3.36it/s] 69%|██████▉   | 542/780 [04:23<01:10,  3.39it/s] 70%|██████▉   | 543/780 [04:23<01:09,  3.41it/s] 70%|██████▉   | 544/780 [04:23<01:08,  3.43it/s] 70%|██████▉   | 545/780 [04:24<01:08,  3.44it/s] 70%|███████   | 546/780 [04:24<01:07,  3.45it/s] 70%|███████   | 547/780 [04:24<01:07,  3.45it/s] 70%|███████   | 548/780 [04:25<01:07,  3.46it/s] 70%|███████   | 549/780 [04:25<01:06,  3.46it/s] 71%|███████   | 550/780 [04:25<01:06,  3.46it/s] 71%|███████   | 551/780 [04:26<01:06,  3.46it/s] 71%|███████   | 552/780 [04:26<01:10,  3.22it/s] 71%|███████   | 553/780 [04:26<01:09,  3.29it/s] 71%|███████   | 554/780 [04:26<01:07,  3.34it/s] 71%|███████   | 555/780 [04:27<01:06,  3.38it/s] 71%|███████▏  | 556/780 [04:27<01:05,  3.41it/s] 71%|███████▏  | 557/780 [04:27<01:05,  3.42it/s] 72%|███████▏  | 558/780 [04:28<01:04,  3.44it/s] 72%|███████▏  | 559/780 [04:28<01:04,  3.45it/s] 72%|███████▏  | 560/780 [04:28<01:03,  3.45it/s] 72%|███████▏  | 561/780 [04:28<01:03,  3.46it/s] 72%|███████▏  | 562/780 [04:29<01:03,  3.46it/s] 72%|███████▏  | 563/780 [04:29<01:06,  3.25it/s] 72%|███████▏  | 564/780 [04:29<01:05,  3.31it/s] 72%|███████▏  | 565/780 [04:30<01:04,  3.36it/s] 73%|███████▎  | 566/780 [04:30<01:03,  3.39it/s] 73%|███████▎  | 567/780 [04:30<01:02,  3.41it/s] 73%|███████▎  | 568/780 [04:31<01:01,  3.43it/s] 73%|███████▎  | 569/780 [04:31<01:01,  3.44it/s] 73%|███████▎  | 570/780 [04:31<01:00,  3.45it/s] 73%|███████▎  | 571/780 [04:31<01:00,  3.45it/s] 73%|███████▎  | 572/780 [04:32<01:00,  3.46it/s] 73%|███████▎  | 573/780 [04:32<00:59,  3.46it/s] 74%|███████▎  | 574/780 [04:32<01:03,  3.25it/s] 74%|███████▎  | 575/780 [04:33<01:01,  3.32it/s] 74%|███████▍  | 576/780 [04:33<01:00,  3.36it/s] 74%|███████▍  | 577/780 [04:33<00:59,  3.39it/s] 74%|███████▍  | 578/780 [04:33<00:59,  3.41it/s] 74%|███████▍  | 579/780 [04:34<00:58,  3.43it/s] 74%|███████▍  | 580/780 [04:34<00:58,  3.44it/s] 74%|███████▍  | 581/780 [04:34<00:57,  3.45it/s] 75%|███████▍  | 582/780 [04:35<00:57,  3.45it/s] 75%|███████▍  | 583/780 [04:35<00:56,  3.46it/s] 75%|███████▍  | 584/780 [04:35<00:56,  3.46it/s] 75%|███████▌  | 585/780 [04:36<01:04,  3.02it/s] 75%|███████▌  | 586/780 [04:36<01:01,  3.14it/s] 75%|███████▌  | 587/780 [04:36<00:59,  3.23it/s] 75%|███████▌  | 588/780 [04:37<00:58,  3.30it/s] 76%|███████▌  | 589/780 [04:37<00:57,  3.35it/s] 76%|███████▌  | 590/780 [04:37<00:56,  3.38it/s] 76%|███████▌  | 591/780 [04:37<00:55,  3.41it/s] 76%|███████▌  | 592/780 [04:38<00:54,  3.42it/s] 76%|███████▌  | 593/780 [04:38<00:54,  3.44it/s] 76%|███████▌  | 594/780 [04:38<00:53,  3.45it/s] 76%|███████▋  | 595/780 [04:39<01:08,  2.70it/s] 76%|███████▋  | 596/780 [04:39<01:03,  2.89it/s] 77%|███████▋  | 597/780 [04:39<01:00,  3.04it/s] 77%|███████▋  | 598/780 [04:40<00:57,  3.16it/s] 77%|███████▋  | 599/780 [04:40<00:55,  3.25it/s] 77%|███████▋  | 600/780 [04:40<00:54,  3.31it/s] 77%|███████▋  | 601/780 [04:41<00:53,  3.35it/s] 77%|███████▋  | 602/780 [04:41<00:52,  3.39it/s] 77%|███████▋  | 603/780 [04:41<00:51,  3.41it/s] 77%|███████▋  | 604/780 [04:41<00:51,  3.43it/s] 78%|███████▊  | 605/780 [04:42<00:57,  3.04it/s] 78%|███████▊  | 606/780 [04:42<00:55,  3.16it/s] 78%|███████▊  | 607/780 [04:42<00:53,  3.24it/s] 78%|███████▊  | 608/780 [04:43<00:51,  3.31it/s] 78%|███████▊  | 609/780 [04:43<00:50,  3.35it/s] 78%|███████▊  | 610/780 [04:43<00:50,  3.39it/s] 78%|███████▊  | 611/780 [04:44<00:49,  3.41it/s] 78%|███████▊  | 612/780 [04:44<00:49,  3.42it/s] 79%|███████▊  | 613/780 [04:44<00:48,  3.44it/s] 79%|███████▊  | 614/780 [04:44<00:48,  3.44it/s] 79%|███████▉  | 615/780 [04:45<00:51,  3.20it/s] 79%|███████▉  | 616/780 [04:45<00:50,  3.27it/s] 79%|███████▉  | 617/780 [04:45<00:49,  3.33it/s] 79%|███████▉  | 618/780 [04:46<00:48,  3.37it/s] 79%|███████▉  | 619/780 [04:46<00:47,  3.39it/s] 79%|███████▉  | 620/780 [04:46<00:46,  3.41it/s] 80%|███████▉  | 621/780 [04:47<00:46,  3.43it/s] 80%|███████▉  | 622/780 [04:47<00:49,  3.18it/s] 80%|███████▉  | 623/780 [04:47<00:48,  3.26it/s] 80%|████████  | 624/780 [04:47<00:47,  3.32it/s][INFO|trainer.py:2140] 2023-08-29 15:56:45,846 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 15:56:45,846 >>   Num examples = 4812
[INFO|trainer.py:2145] 2023-08-29 15:56:45,846 >>   Batch size = 8
{'eval_loss': 1.043225884437561, 'eval_runtime': 13.7469, 'eval_samples_per_second': 350.042, 'eval_steps_per_second': 43.792, 'epoch': 3.0}
{'loss': 0.3713, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/602 [00:00<?, ?it/s][A
  1%|          | 6/602 [00:00<00:10, 57.09it/s][A
  2%|▏         | 12/602 [00:00<00:11, 50.61it/s][A
  3%|▎         | 18/602 [00:00<00:11, 48.79it/s][A
  4%|▍         | 23/602 [00:00<00:12, 48.11it/s][A
  5%|▍         | 28/602 [00:00<00:12, 47.70it/s][A
  5%|▌         | 33/602 [00:00<00:12, 47.28it/s][A
  6%|▋         | 38/602 [00:00<00:11, 47.05it/s][A
  7%|▋         | 43/602 [00:00<00:12, 46.54it/s][A
  8%|▊         | 48/602 [00:01<00:11, 46.70it/s][A
  9%|▉         | 53/602 [00:01<00:11, 46.82it/s][A
 10%|▉         | 58/602 [00:01<00:11, 46.72it/s][A
 10%|█         | 63/602 [00:01<00:11, 46.83it/s][A
 11%|█▏        | 68/602 [00:01<00:11, 46.80it/s][A
 12%|█▏        | 73/602 [00:01<00:11, 46.75it/s][A
 13%|█▎        | 78/602 [00:01<00:11, 46.78it/s][A
 14%|█▍        | 83/602 [00:01<00:11, 46.70it/s][A
 15%|█▍        | 88/602 [00:01<00:11, 46.57it/s][A
 15%|█▌        | 93/602 [00:02<00:14, 35.75it/s][A
 16%|█▌        | 97/602 [00:02<00:14, 35.33it/s][A
 17%|█▋        | 102/602 [00:02<00:13, 38.28it/s][A
 18%|█▊        | 107/602 [00:02<00:12, 40.56it/s][A
 19%|█▊        | 112/602 [00:02<00:11, 42.30it/s][A
 19%|█▉        | 117/602 [00:02<00:18, 25.60it/s][A
 20%|██        | 122/602 [00:02<00:16, 29.60it/s][A
 21%|██        | 127/602 [00:03<00:14, 33.28it/s][A
 22%|██▏       | 132/602 [00:03<00:12, 36.49it/s][A
 23%|██▎       | 137/602 [00:03<00:11, 39.08it/s][A
 24%|██▎       | 142/602 [00:03<00:11, 41.14it/s][A
 24%|██▍       | 147/602 [00:03<00:10, 42.67it/s][A
 25%|██▌       | 152/602 [00:03<00:10, 43.73it/s][A
 26%|██▌       | 157/602 [00:03<00:10, 44.48it/s][A
 27%|██▋       | 162/602 [00:03<00:09, 45.18it/s][A
 28%|██▊       | 167/602 [00:03<00:09, 45.65it/s][A
 29%|██▊       | 172/602 [00:04<00:09, 45.97it/s][A
 29%|██▉       | 177/602 [00:04<00:09, 46.23it/s][A
 30%|███       | 182/602 [00:04<00:09, 46.33it/s][A
 31%|███       | 187/602 [00:04<00:08, 46.51it/s][A
 32%|███▏      | 192/602 [00:04<00:08, 46.63it/s][A
 33%|███▎      | 197/602 [00:04<00:08, 46.65it/s][A
 34%|███▎      | 202/602 [00:04<00:08, 46.70it/s][A
 34%|███▍      | 207/602 [00:04<00:08, 46.62it/s][A
 35%|███▌      | 212/602 [00:04<00:08, 46.56it/s][A
 36%|███▌      | 217/602 [00:05<00:08, 46.69it/s][A
 37%|███▋      | 222/602 [00:05<00:08, 46.73it/s][A
 38%|███▊      | 227/602 [00:05<00:09, 39.85it/s][A
 39%|███▊      | 232/602 [00:05<00:08, 41.70it/s][A
 39%|███▉      | 237/602 [00:05<00:08, 43.12it/s][A
 40%|████      | 242/602 [00:05<00:08, 44.19it/s][A
 41%|████      | 247/602 [00:05<00:07, 44.95it/s][A
 42%|████▏     | 252/602 [00:05<00:07, 45.47it/s][A
 43%|████▎     | 257/602 [00:05<00:07, 45.79it/s][A
 44%|████▎     | 262/602 [00:06<00:07, 46.10it/s][A
 44%|████▍     | 267/602 [00:06<00:07, 46.22it/s][A
 45%|████▌     | 272/602 [00:06<00:07, 46.39it/s][A
 46%|████▌     | 277/602 [00:06<00:06, 46.44it/s][A
 47%|████▋     | 282/602 [00:06<00:06, 46.46it/s][A
 48%|████▊     | 287/602 [00:06<00:06, 46.20it/s][A
 49%|████▊     | 292/602 [00:06<00:06, 46.59it/s][A
 49%|████▉     | 297/602 [00:06<00:06, 46.68it/s][A
 50%|█████     | 302/602 [00:06<00:06, 46.66it/s][A
 51%|█████     | 307/602 [00:07<00:06, 46.67it/s][A
 52%|█████▏    | 312/602 [00:07<00:06, 46.61it/s][A
 53%|█████▎    | 317/602 [00:07<00:06, 46.76it/s][A
 53%|█████▎    | 322/602 [00:07<00:05, 46.77it/s][A
 54%|█████▍    | 327/602 [00:07<00:05, 46.79it/s][A
 55%|█████▌    | 332/602 [00:07<00:05, 46.78it/s][A
 56%|█████▌    | 337/602 [00:07<00:05, 46.77it/s][A
 57%|█████▋    | 342/602 [00:07<00:05, 46.73it/s][A
 58%|█████▊    | 347/602 [00:07<00:05, 46.79it/s][A
 58%|█████▊    | 352/602 [00:07<00:05, 46.71it/s][A
 59%|█████▉    | 357/602 [00:08<00:05, 46.70it/s][A
 60%|██████    | 362/602 [00:08<00:05, 46.61it/s][A
 61%|██████    | 367/602 [00:08<00:05, 40.75it/s][A
 62%|██████▏   | 372/602 [00:08<00:05, 42.32it/s][A
 63%|██████▎   | 377/602 [00:08<00:05, 43.56it/s][A
 63%|██████▎   | 382/602 [00:08<00:04, 44.51it/s][A
 64%|██████▍   | 387/602 [00:08<00:04, 45.15it/s][A
 65%|██████▌   | 392/602 [00:08<00:04, 45.60it/s][A
 66%|██████▌   | 397/602 [00:08<00:04, 45.91it/s][A
 67%|██████▋   | 402/602 [00:09<00:04, 46.19it/s][A
 68%|██████▊   | 407/602 [00:09<00:04, 46.21it/s][A
 68%|██████▊   | 412/602 [00:09<00:04, 46.23it/s][A
 69%|██████▉   | 417/602 [00:09<00:03, 46.39it/s][A
 70%|███████   | 422/602 [00:09<00:03, 46.44it/s][A
 71%|███████   | 427/602 [00:09<00:03, 46.44it/s][A
 72%|███████▏  | 432/602 [00:09<00:03, 46.52it/s][A
 73%|███████▎  | 437/602 [00:09<00:03, 46.45it/s][A
 73%|███████▎  | 442/602 [00:09<00:03, 46.46it/s][A
 74%|███████▍  | 447/602 [00:10<00:03, 46.42it/s][A
 75%|███████▌  | 452/602 [00:10<00:03, 46.35it/s][A
 76%|███████▌  | 457/602 [00:10<00:03, 46.37it/s][A
 77%|███████▋  | 462/602 [00:10<00:03, 46.44it/s][A
 78%|███████▊  | 467/602 [00:10<00:02, 46.50it/s][A
 78%|███████▊  | 472/602 [00:10<00:02, 46.62it/s][A
 79%|███████▉  | 477/602 [00:10<00:02, 46.68it/s][A
 80%|████████  | 482/602 [00:10<00:02, 46.66it/s][A
 81%|████████  | 487/602 [00:10<00:02, 46.65it/s][A
 82%|████████▏ | 492/602 [00:11<00:02, 46.65it/s][A
 83%|████████▎ | 497/602 [00:11<00:02, 46.60it/s][A
 83%|████████▎ | 502/602 [00:11<00:02, 46.57it/s][A
 84%|████████▍ | 507/602 [00:11<00:02, 40.02it/s][A
 85%|████████▌ | 512/602 [00:11<00:02, 41.86it/s][A
 86%|████████▌ | 517/602 [00:11<00:01, 43.20it/s][A
 87%|████████▋ | 522/602 [00:11<00:01, 44.24it/s][A
 88%|████████▊ | 527/602 [00:11<00:01, 44.96it/s][A
 88%|████████▊ | 532/602 [00:11<00:01, 45.52it/s][A
 89%|████████▉ | 537/602 [00:12<00:01, 45.91it/s][A
 90%|█████████ | 542/602 [00:12<00:01, 46.21it/s][A
 91%|█████████ | 547/602 [00:12<00:01, 38.79it/s][A
 92%|█████████▏| 552/602 [00:12<00:01, 41.03it/s][A
 93%|█████████▎| 557/602 [00:12<00:01, 42.65it/s][A
 93%|█████████▎| 562/602 [00:12<00:00, 43.76it/s][A
 94%|█████████▍| 567/602 [00:12<00:00, 44.62it/s][A
 95%|█████████▌| 572/602 [00:12<00:00, 45.27it/s][A
 96%|█████████▌| 577/602 [00:12<00:00, 45.70it/s][A
 97%|█████████▋| 582/602 [00:13<00:00, 46.06it/s][A
 98%|█████████▊| 587/602 [00:13<00:00, 46.20it/s][A
 98%|█████████▊| 592/602 [00:13<00:00, 46.06it/s][A
 99%|█████████▉| 597/602 [00:13<00:00, 46.22it/s][A
100%|██████████| 602/602 [00:13<00:00, 46.29it/s][A
                                                 [A                                                 
100%|██████████| 602/602 [00:13<00:00, 46.29it/s][A 80%|████████  | 624/780 [05:01<00:47,  3.32it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 15:57:00,010 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 15:57:00,489 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 15:57:07,365 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 15:57:07,573 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 15:57:07,658 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [05:19<25:19,  9.80s/it] 80%|████████  | 626/780 [05:20<17:49,  6.95s/it] 80%|████████  | 627/780 [05:20<12:37,  4.95s/it] 81%|████████  | 628/780 [05:20<08:59,  3.55s/it] 81%|████████  | 629/780 [05:21<06:30,  2.59s/it] 81%|████████  | 630/780 [05:21<04:44,  1.90s/it] 81%|████████  | 631/780 [05:21<03:30,  1.42s/it] 81%|████████  | 632/780 [05:22<02:39,  1.08s/it] 81%|████████  | 633/780 [05:22<02:03,  1.19it/s] 81%|████████▏ | 634/780 [05:22<01:38,  1.48it/s] 81%|████████▏ | 635/780 [05:22<01:21,  1.79it/s] 82%|████████▏ | 636/780 [05:23<01:08,  2.09it/s] 82%|████████▏ | 637/780 [05:23<01:00,  2.38it/s] 82%|████████▏ | 638/780 [05:23<00:54,  2.63it/s] 82%|████████▏ | 639/780 [05:24<00:49,  2.83it/s] 82%|████████▏ | 640/780 [05:24<00:48,  2.89it/s] 82%|████████▏ | 641/780 [05:24<00:45,  3.04it/s] 82%|████████▏ | 642/780 [05:24<00:43,  3.16it/s] 82%|████████▏ | 643/780 [05:25<00:42,  3.25it/s] 83%|████████▎ | 644/780 [05:25<00:41,  3.31it/s] 83%|████████▎ | 645/780 [05:25<00:40,  3.36it/s] 83%|████████▎ | 646/780 [05:26<00:39,  3.39it/s] 83%|████████▎ | 647/780 [05:26<00:38,  3.42it/s] 83%|████████▎ | 648/780 [05:26<00:38,  3.43it/s] 83%|████████▎ | 649/780 [05:26<00:38,  3.44it/s] 83%|████████▎ | 650/780 [05:27<00:37,  3.45it/s] 83%|████████▎ | 651/780 [05:27<00:39,  3.29it/s] 84%|████████▎ | 652/780 [05:27<00:38,  3.34it/s] 84%|████████▎ | 653/780 [05:28<00:37,  3.38it/s] 84%|████████▍ | 654/780 [05:28<00:36,  3.41it/s] 84%|████████▍ | 655/780 [05:28<00:36,  3.43it/s] 84%|████████▍ | 656/780 [05:29<00:36,  3.44it/s] 84%|████████▍ | 657/780 [05:29<00:35,  3.45it/s] 84%|████████▍ | 658/780 [05:29<00:35,  3.46it/s] 84%|████████▍ | 659/780 [05:29<00:34,  3.46it/s] 85%|████████▍ | 660/780 [05:30<00:34,  3.47it/s] 85%|████████▍ | 661/780 [05:30<00:34,  3.47it/s] 85%|████████▍ | 662/780 [05:30<00:35,  3.29it/s] 85%|████████▌ | 663/780 [05:31<00:35,  3.34it/s] 85%|████████▌ | 664/780 [05:31<00:34,  3.38it/s] 85%|████████▌ | 665/780 [05:31<00:33,  3.41it/s] 85%|████████▌ | 666/780 [05:31<00:33,  3.42it/s] 86%|████████▌ | 667/780 [05:32<00:32,  3.44it/s] 86%|████████▌ | 668/780 [05:32<00:32,  3.45it/s] 86%|████████▌ | 669/780 [05:32<00:32,  3.45it/s] 86%|████████▌ | 670/780 [05:33<00:31,  3.46it/s] 86%|████████▌ | 671/780 [05:33<00:31,  3.46it/s] 86%|████████▌ | 672/780 [05:33<00:31,  3.47it/s] 86%|████████▋ | 673/780 [05:34<00:32,  3.30it/s] 86%|████████▋ | 674/780 [05:34<00:31,  3.35it/s] 87%|████████▋ | 675/780 [05:34<00:31,  3.38it/s] 87%|████████▋ | 676/780 [05:34<00:30,  3.41it/s] 87%|████████▋ | 677/780 [05:35<00:30,  3.43it/s] 87%|████████▋ | 678/780 [05:35<00:29,  3.44it/s] 87%|████████▋ | 679/780 [05:35<00:29,  3.45it/s] 87%|████████▋ | 680/780 [05:36<00:28,  3.45it/s] 87%|████████▋ | 681/780 [05:36<00:28,  3.46it/s] 87%|████████▋ | 682/780 [05:36<00:28,  3.46it/s] 88%|████████▊ | 683/780 [05:36<00:28,  3.46it/s] 88%|████████▊ | 684/780 [05:37<00:29,  3.30it/s] 88%|████████▊ | 685/780 [05:37<00:28,  3.35it/s] 88%|████████▊ | 686/780 [05:37<00:27,  3.39it/s] 88%|████████▊ | 687/780 [05:38<00:27,  3.41it/s] 88%|████████▊ | 688/780 [05:38<00:26,  3.43it/s] 88%|████████▊ | 689/780 [05:38<00:26,  3.44it/s] 88%|████████▊ | 690/780 [05:38<00:26,  3.45it/s] 89%|████████▊ | 691/780 [05:39<00:31,  2.87it/s] 89%|████████▊ | 692/780 [05:39<00:29,  3.02it/s] 89%|████████▉ | 693/780 [05:40<00:27,  3.14it/s] 89%|████████▉ | 694/780 [05:40<00:29,  2.93it/s] 89%|████████▉ | 695/780 [05:40<00:27,  3.07it/s] 89%|████████▉ | 696/780 [05:40<00:26,  3.18it/s] 89%|████████▉ | 697/780 [05:41<00:25,  3.26it/s] 89%|████████▉ | 698/780 [05:41<00:24,  3.32it/s] 90%|████████▉ | 699/780 [05:41<00:24,  3.36it/s] 90%|████████▉ | 700/780 [05:42<00:23,  3.39it/s] 90%|████████▉ | 701/780 [05:42<00:23,  3.42it/s] 90%|█████████ | 702/780 [05:42<00:22,  3.43it/s] 90%|█████████ | 703/780 [05:43<00:22,  3.44it/s] 90%|█████████ | 704/780 [05:43<00:22,  3.45it/s] 90%|█████████ | 705/780 [05:43<00:24,  3.09it/s] 91%|█████████ | 706/780 [05:43<00:23,  3.20it/s] 91%|█████████ | 707/780 [05:44<00:22,  3.27it/s] 91%|█████████ | 708/780 [05:44<00:21,  3.33it/s] 91%|█████████ | 709/780 [05:44<00:21,  3.37it/s] 91%|█████████ | 710/780 [05:45<00:20,  3.40it/s] 91%|█████████ | 711/780 [05:45<00:20,  3.42it/s] 91%|█████████▏| 712/780 [05:45<00:19,  3.43it/s] 91%|█████████▏| 713/780 [05:46<00:19,  3.44it/s] 92%|█████████▏| 714/780 [05:46<00:19,  3.45it/s] 92%|█████████▏| 715/780 [05:46<00:18,  3.46it/s] 92%|█████████▏| 716/780 [05:46<00:20,  3.13it/s] 92%|█████████▏| 717/780 [05:47<00:19,  3.22it/s] 92%|█████████▏| 718/780 [05:47<00:18,  3.29it/s] 92%|█████████▏| 719/780 [05:47<00:18,  3.34it/s] 92%|█████████▏| 720/780 [05:48<00:17,  3.38it/s] 92%|█████████▏| 721/780 [05:48<00:17,  3.41it/s] 93%|█████████▎| 722/780 [05:48<00:16,  3.42it/s] 93%|█████████▎| 723/780 [05:48<00:16,  3.44it/s] 93%|█████████▎| 724/780 [05:49<00:16,  3.45it/s] 93%|█████████▎| 725/780 [05:49<00:15,  3.45it/s] 93%|█████████▎| 726/780 [05:49<00:15,  3.46it/s] 93%|█████████▎| 727/780 [05:50<00:15,  3.46it/s] 93%|█████████▎| 728/780 [05:50<00:15,  3.46it/s] 93%|█████████▎| 729/780 [05:50<00:14,  3.46it/s] 94%|█████████▎| 730/780 [05:51<00:14,  3.47it/s] 94%|█████████▎| 731/780 [05:51<00:14,  3.47it/s] 94%|█████████▍| 732/780 [05:51<00:13,  3.47it/s] 94%|█████████▍| 733/780 [05:51<00:14,  3.30it/s] 94%|█████████▍| 734/780 [05:52<00:13,  3.35it/s] 94%|█████████▍| 735/780 [05:52<00:13,  3.38it/s] 94%|█████████▍| 736/780 [05:52<00:12,  3.41it/s] 94%|█████████▍| 737/780 [05:53<00:12,  3.43it/s] 95%|█████████▍| 738/780 [05:53<00:12,  3.44it/s] 95%|█████████▍| 739/780 [05:53<00:11,  3.45it/s] 95%|█████████▍| 740/780 [05:53<00:11,  3.45it/s] 95%|█████████▌| 741/780 [05:54<00:11,  3.46it/s] 95%|█████████▌| 742/780 [05:54<00:10,  3.46it/s] 95%|█████████▌| 743/780 [05:54<00:10,  3.46it/s] 95%|█████████▌| 744/780 [05:55<00:10,  3.34it/s] 96%|█████████▌| 745/780 [05:55<00:10,  3.37it/s] 96%|█████████▌| 746/780 [05:55<00:10,  3.40it/s] 96%|█████████▌| 747/780 [05:56<00:09,  3.42it/s] 96%|█████████▌| 748/780 [05:56<00:09,  3.43it/s] 96%|█████████▌| 749/780 [05:56<00:09,  3.44it/s] 96%|█████████▌| 750/780 [05:56<00:08,  3.45it/s] 96%|█████████▋| 751/780 [05:57<00:08,  3.45it/s] 96%|█████████▋| 752/780 [05:57<00:08,  3.46it/s] 97%|█████████▋| 753/780 [05:57<00:07,  3.46it/s] 97%|█████████▋| 754/780 [05:58<00:07,  3.46it/s] 97%|█████████▋| 755/780 [05:58<00:07,  3.32it/s] 97%|█████████▋| 756/780 [05:58<00:07,  3.36it/s] 97%|█████████▋| 757/780 [05:58<00:06,  3.39it/s] 97%|█████████▋| 758/780 [05:59<00:06,  3.41it/s] 97%|█████████▋| 759/780 [05:59<00:06,  3.43it/s] 97%|█████████▋| 760/780 [05:59<00:05,  3.44it/s] 98%|█████████▊| 761/780 [06:00<00:05,  3.44it/s] 98%|█████████▊| 762/780 [06:00<00:05,  3.44it/s] 98%|█████████▊| 763/780 [06:00<00:04,  3.45it/s] 98%|█████████▊| 764/780 [06:00<00:04,  3.45it/s] 98%|█████████▊| 765/780 [06:01<00:04,  3.45it/s] 98%|█████████▊| 766/780 [06:01<00:04,  3.26it/s] 98%|█████████▊| 767/780 [06:01<00:03,  3.32it/s] 98%|█████████▊| 768/780 [06:02<00:03,  3.36it/s] 99%|█████████▊| 769/780 [06:02<00:03,  3.39it/s] 99%|█████████▊| 770/780 [06:02<00:02,  3.41it/s] 99%|█████████▉| 771/780 [06:03<00:02,  3.42it/s] 99%|█████████▉| 772/780 [06:03<00:02,  3.43it/s] 99%|█████████▉| 773/780 [06:03<00:02,  3.44it/s] 99%|█████████▉| 774/780 [06:03<00:01,  3.45it/s] 99%|█████████▉| 775/780 [06:04<00:01,  3.45it/s] 99%|█████████▉| 776/780 [06:04<00:01,  3.45it/s]100%|█████████▉| 777/780 [06:04<00:00,  3.24it/s]100%|█████████▉| 778/780 [06:05<00:00,  3.30it/s]100%|█████████▉| 779/780 [06:05<00:00,  3.35it/s]100%|██████████| 780/780 [06:05<00:00,  3.38it/s][INFO|trainer.py:2140] 2023-08-29 15:58:03,539 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 15:58:03,539 >>   Num examples = 4812
[INFO|trainer.py:2145] 2023-08-29 15:58:03,539 >>   Batch size = 8
{'eval_loss': 1.0526942014694214, 'eval_runtime': 13.562, 'eval_samples_per_second': 354.816, 'eval_steps_per_second': 44.389, 'epoch': 4.0}

  0%|          | 0/602 [00:00<?, ?it/s][A
  1%|          | 6/602 [00:00<00:10, 57.27it/s][A
  2%|▏         | 12/602 [00:00<00:11, 50.43it/s][A
  3%|▎         | 18/602 [00:00<00:11, 48.79it/s][A
  4%|▍         | 23/602 [00:00<00:12, 47.94it/s][A
  5%|▍         | 28/602 [00:00<00:12, 47.59it/s][A
  5%|▌         | 33/602 [00:00<00:12, 47.37it/s][A
  6%|▋         | 38/602 [00:00<00:11, 47.10it/s][A
  7%|▋         | 43/602 [00:00<00:11, 46.84it/s][A
  8%|▊         | 48/602 [00:01<00:11, 46.68it/s][A
  9%|▉         | 53/602 [00:01<00:11, 46.66it/s][A
 10%|▉         | 58/602 [00:01<00:11, 46.65it/s][A
 10%|█         | 63/602 [00:01<00:11, 46.73it/s][A
 11%|█▏        | 68/602 [00:01<00:11, 46.78it/s][A
 12%|█▏        | 73/602 [00:01<00:11, 46.71it/s][A
 13%|█▎        | 78/602 [00:01<00:11, 46.76it/s][A
 14%|█▍        | 83/602 [00:01<00:11, 46.69it/s][A
 15%|█▍        | 88/602 [00:01<00:11, 46.58it/s][A
 15%|█▌        | 93/602 [00:02<00:12, 41.80it/s][A
 16%|█▋        | 98/602 [00:02<00:11, 43.27it/s][A
 17%|█▋        | 103/602 [00:02<00:11, 44.30it/s][A
 18%|█▊        | 108/602 [00:02<00:10, 45.02it/s][A
 19%|█▉        | 113/602 [00:02<00:10, 45.54it/s][A
 20%|█▉        | 118/602 [00:02<00:10, 45.82it/s][A
 20%|██        | 123/602 [00:02<00:10, 46.17it/s][A
 21%|██▏       | 128/602 [00:02<00:10, 46.38it/s][A
 22%|██▏       | 133/602 [00:02<00:10, 46.23it/s][A
 23%|██▎       | 138/602 [00:02<00:10, 46.31it/s][A
 24%|██▍       | 143/602 [00:03<00:09, 46.28it/s][A
 25%|██▍       | 148/602 [00:03<00:09, 46.43it/s][A
 25%|██▌       | 153/602 [00:03<00:09, 46.61it/s][A
 26%|██▌       | 158/602 [00:03<00:09, 46.70it/s][A
 27%|██▋       | 163/602 [00:03<00:09, 46.74it/s][A
 28%|██▊       | 168/602 [00:03<00:09, 46.73it/s][A
 29%|██▊       | 173/602 [00:03<00:09, 46.70it/s][A
 30%|██▉       | 178/602 [00:03<00:09, 46.62it/s][A
 30%|███       | 183/602 [00:03<00:09, 46.54it/s][A
 31%|███       | 188/602 [00:04<00:08, 46.45it/s][A
 32%|███▏      | 193/602 [00:04<00:08, 46.58it/s][A
 33%|███▎      | 198/602 [00:04<00:08, 46.57it/s][A
 34%|███▎      | 203/602 [00:04<00:08, 46.62it/s][A
 35%|███▍      | 208/602 [00:04<00:08, 46.72it/s][A
 35%|███▌      | 213/602 [00:04<00:08, 46.76it/s][A
 36%|███▌      | 218/602 [00:04<00:08, 46.75it/s][A
 37%|███▋      | 223/602 [00:04<00:08, 46.65it/s][A
 38%|███▊      | 228/602 [00:04<00:08, 46.61it/s][A
 39%|███▊      | 233/602 [00:05<00:08, 44.19it/s][A
 40%|███▉      | 238/602 [00:05<00:08, 44.87it/s][A
 40%|████      | 243/602 [00:05<00:07, 45.48it/s][A
 41%|████      | 248/602 [00:05<00:08, 41.63it/s][A
 42%|████▏     | 253/602 [00:05<00:08, 39.20it/s][A
 43%|████▎     | 258/602 [00:05<00:11, 29.08it/s][A
 44%|████▎     | 263/602 [00:05<00:10, 32.84it/s][A
 45%|████▍     | 268/602 [00:06<00:09, 36.08it/s][A
 45%|████▌     | 273/602 [00:06<00:08, 38.75it/s][A
 46%|████▌     | 278/602 [00:06<00:07, 40.88it/s][A
 47%|████▋     | 283/602 [00:06<00:07, 42.43it/s][A
 48%|████▊     | 288/602 [00:06<00:07, 43.61it/s][A
 49%|████▊     | 293/602 [00:06<00:06, 44.59it/s][A
 50%|████▉     | 298/602 [00:06<00:06, 45.09it/s][A
 50%|█████     | 303/602 [00:06<00:06, 45.41it/s][A
 51%|█████     | 308/602 [00:06<00:06, 45.68it/s][A
 52%|█████▏    | 313/602 [00:06<00:06, 45.85it/s][A
 53%|█████▎    | 318/602 [00:07<00:06, 46.21it/s][A
 54%|█████▎    | 323/602 [00:07<00:06, 46.27it/s][A
 54%|█████▍    | 328/602 [00:07<00:05, 46.45it/s][A
 55%|█████▌    | 333/602 [00:07<00:05, 46.60it/s][A
 56%|█████▌    | 338/602 [00:07<00:05, 46.66it/s][A
 57%|█████▋    | 343/602 [00:07<00:05, 46.57it/s][A
 58%|█████▊    | 348/602 [00:07<00:05, 46.49it/s][A
 59%|█████▊    | 353/602 [00:07<00:05, 46.49it/s][A
 59%|█████▉    | 358/602 [00:08<00:05, 46.50it/s][A
 60%|██████    | 363/602 [00:08<00:06, 38.40it/s][A
 61%|██████    | 368/602 [00:08<00:05, 40.50it/s][A
 62%|██████▏   | 373/602 [00:08<00:05, 42.13it/s][A
 63%|██████▎   | 378/602 [00:08<00:05, 43.45it/s][A
 64%|██████▎   | 383/602 [00:08<00:04, 44.38it/s][A
 64%|██████▍   | 388/602 [00:08<00:04, 45.09it/s][A
 65%|██████▌   | 393/602 [00:08<00:04, 45.58it/s][A
 66%|██████▌   | 398/602 [00:08<00:04, 45.91it/s][A
 67%|██████▋   | 403/602 [00:08<00:04, 46.04it/s][A
 68%|██████▊   | 408/602 [00:09<00:04, 46.14it/s][A
 69%|██████▊   | 413/602 [00:09<00:04, 46.28it/s][A
 69%|██████▉   | 418/602 [00:09<00:03, 46.47it/s][A
 70%|███████   | 423/602 [00:09<00:03, 46.44it/s][A
 71%|███████   | 428/602 [00:09<00:03, 46.47it/s][A
 72%|███████▏  | 433/602 [00:09<00:03, 46.49it/s][A
 73%|███████▎  | 438/602 [00:09<00:03, 46.56it/s][A
 74%|███████▎  | 443/602 [00:09<00:03, 46.64it/s][A
 74%|███████▍  | 448/602 [00:09<00:03, 46.66it/s][A
 75%|███████▌  | 453/602 [00:10<00:03, 46.66it/s][A
 76%|███████▌  | 458/602 [00:10<00:03, 46.65it/s][A
 77%|███████▋  | 463/602 [00:10<00:02, 46.64it/s][A
 78%|███████▊  | 468/602 [00:10<00:02, 46.67it/s][A
 79%|███████▊  | 473/602 [00:10<00:02, 46.62it/s][A
 79%|███████▉  | 478/602 [00:10<00:02, 46.64it/s][A
 80%|████████  | 483/602 [00:10<00:02, 46.63it/s][A
 81%|████████  | 488/602 [00:10<00:02, 46.56it/s][A
 82%|████████▏ | 493/602 [00:10<00:02, 46.66it/s][A
 83%|████████▎ | 498/602 [00:11<00:02, 46.68it/s][A
 84%|████████▎ | 503/602 [00:11<00:02, 40.92it/s][A
 84%|████████▍ | 508/602 [00:11<00:02, 42.53it/s][A
 85%|████████▌ | 513/602 [00:11<00:02, 43.73it/s][A
 86%|████████▌ | 518/602 [00:11<00:01, 44.61it/s][A
 87%|████████▋ | 523/602 [00:11<00:01, 45.13it/s][A
 88%|████████▊ | 528/602 [00:11<00:01, 45.57it/s][A
 89%|████████▊ | 533/602 [00:11<00:01, 45.87it/s][A
 89%|████████▉ | 538/602 [00:11<00:01, 46.16it/s][A
 90%|█████████ | 543/602 [00:12<00:01, 46.14it/s][A
 91%|█████████ | 548/602 [00:12<00:01, 46.28it/s][A
 92%|█████████▏| 553/602 [00:12<00:01, 46.37it/s][A
 93%|█████████▎| 558/602 [00:12<00:00, 46.47it/s][A
 94%|█████████▎| 563/602 [00:12<00:00, 40.56it/s][A
 94%|█████████▍| 568/602 [00:12<00:00, 42.77it/s][A
 95%|█████████▌| 573/602 [00:12<00:00, 43.90it/s][A
 96%|█████████▌| 578/602 [00:12<00:00, 44.68it/s][A
 97%|█████████▋| 583/602 [00:12<00:00, 45.25it/s][A
 98%|█████████▊| 588/602 [00:13<00:00, 45.70it/s][A
 99%|█████████▊| 593/602 [00:13<00:00, 45.95it/s][A
 99%|█████████▉| 598/602 [00:13<00:00, 46.26it/s][A
                                                 [A                                                 
100%|██████████| 602/602 [00:13<00:00, 46.26it/s][A100%|██████████| 780/780 [06:19<00:00,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 15:58:17,279 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 15:58:17,579 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 15:58:22,097 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 15:58:22,364 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 15:58:22,492 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 15:58:35,418 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 15:58:35,455 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156 (score: 1.0098013877868652).
                                                 100%|██████████| 780/780 [06:58<00:00,  3.38it/s]100%|██████████| 780/780 [06:58<00:00,  1.86it/s]
[INFO|trainer.py:1894] 2023-08-29 15:58:56,558 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 15:58:56,895 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 15:59:02,098 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 15:59:02,375 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 15:59:02,554 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 15:59:03,339 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:59:03,339 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:59:03,339 >>   train_loss               =      0.364
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:59:03,339 >>   train_runtime            = 0:06:58.52
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:59:03,340 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:59:03,340 >>   train_samples_per_second =    119.467
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:59:03,340 >>   train_steps_per_second   =      1.864
{'eval_loss': 1.060807704925537, 'eval_runtime': 13.3748, 'eval_samples_per_second': 359.782, 'eval_steps_per_second': 45.01, 'epoch': 5.0}
{'train_runtime': 418.5247, 'train_samples_per_second': 119.467, 'train_steps_per_second': 1.864, 'train_loss': 0.36399395282451924, 'epoch': 5.0}
08/29/2023 15:59:03 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 15:59:03,761 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 15:59:03,761 >>   Num examples = 4812
[INFO|trainer.py:2145] 2023-08-29 15:59:03,761 >>   Batch size = 8
  0%|          | 0/602 [00:00<?, ?it/s]  1%|          | 6/602 [00:00<00:10, 58.94it/s]  2%|▏         | 12/602 [00:00<00:11, 51.53it/s]  3%|▎         | 18/602 [00:00<00:11, 49.53it/s]  4%|▍         | 23/602 [00:00<00:11, 48.48it/s]  5%|▍         | 28/602 [00:00<00:11, 48.16it/s]  5%|▌         | 33/602 [00:00<00:11, 48.00it/s]  6%|▋         | 38/602 [00:00<00:11, 47.83it/s]  7%|▋         | 43/602 [00:00<00:11, 47.69it/s]  8%|▊         | 48/602 [00:00<00:11, 47.53it/s]  9%|▉         | 53/602 [00:01<00:11, 47.37it/s] 10%|▉         | 58/602 [00:01<00:11, 47.40it/s] 10%|█         | 63/602 [00:01<00:11, 47.47it/s] 11%|█▏        | 68/602 [00:01<00:11, 47.52it/s] 12%|█▏        | 73/602 [00:01<00:12, 42.29it/s] 13%|█▎        | 78/602 [00:01<00:11, 43.82it/s] 14%|█▍        | 83/602 [00:01<00:11, 44.80it/s] 15%|█▍        | 88/602 [00:01<00:11, 45.63it/s] 15%|█▌        | 93/602 [00:01<00:11, 46.18it/s] 16%|█▋        | 98/602 [00:02<00:10, 46.46it/s] 17%|█▋        | 103/602 [00:02<00:10, 46.77it/s] 18%|█▊        | 108/602 [00:02<00:10, 46.99it/s] 19%|█▉        | 113/602 [00:02<00:10, 47.06it/s] 20%|█▉        | 118/602 [00:02<00:10, 47.08it/s] 20%|██        | 123/602 [00:02<00:10, 47.13it/s] 21%|██▏       | 128/602 [00:02<00:10, 47.17it/s] 22%|██▏       | 133/602 [00:02<00:09, 47.31it/s] 23%|██▎       | 138/602 [00:02<00:09, 47.39it/s] 24%|██▍       | 143/602 [00:03<00:09, 47.45it/s] 25%|██▍       | 148/602 [00:03<00:09, 47.44it/s] 25%|██▌       | 153/602 [00:03<00:09, 47.42it/s] 26%|██▌       | 158/602 [00:03<00:09, 47.40it/s] 27%|██▋       | 163/602 [00:03<00:09, 47.21it/s] 28%|██▊       | 168/602 [00:03<00:09, 47.33it/s] 29%|██▊       | 173/602 [00:03<00:09, 47.28it/s] 30%|██▉       | 178/602 [00:03<00:08, 47.21it/s] 30%|███       | 183/602 [00:03<00:08, 46.83it/s] 31%|███       | 188/602 [00:03<00:08, 47.25it/s] 32%|███▏      | 193/602 [00:04<00:08, 47.34it/s] 33%|███▎      | 198/602 [00:04<00:08, 47.32it/s] 34%|███▎      | 203/602 [00:04<00:08, 47.35it/s] 35%|███▍      | 208/602 [00:04<00:08, 47.34it/s] 35%|███▌      | 213/602 [00:04<00:08, 47.37it/s] 36%|███▌      | 218/602 [00:04<00:08, 43.31it/s] 37%|███▋      | 223/602 [00:04<00:08, 44.33it/s] 38%|███▊      | 228/602 [00:04<00:08, 45.26it/s] 39%|███▊      | 233/602 [00:04<00:08, 45.91it/s] 40%|███▉      | 238/602 [00:05<00:07, 46.36it/s] 40%|████      | 243/602 [00:05<00:11, 31.51it/s] 41%|████      | 247/602 [00:05<00:13, 25.41it/s] 42%|████▏     | 253/602 [00:05<00:11, 30.73it/s] 43%|████▎     | 258/602 [00:05<00:10, 34.30it/s] 44%|████▎     | 263/602 [00:05<00:09, 37.39it/s] 45%|████▍     | 268/602 [00:06<00:08, 39.88it/s] 45%|████▌     | 273/602 [00:06<00:07, 41.83it/s] 46%|████▌     | 278/602 [00:06<00:07, 43.40it/s] 47%|████▋     | 283/602 [00:06<00:07, 44.53it/s] 48%|████▊     | 288/602 [00:06<00:06, 45.39it/s] 49%|████▊     | 293/602 [00:06<00:06, 45.72it/s] 50%|████▉     | 298/602 [00:06<00:06, 46.11it/s] 50%|█████     | 303/602 [00:06<00:06, 46.46it/s] 51%|█████     | 308/602 [00:06<00:06, 46.75it/s] 52%|█████▏    | 313/602 [00:06<00:06, 47.00it/s] 53%|█████▎    | 318/602 [00:07<00:06, 47.16it/s] 54%|█████▎    | 323/602 [00:07<00:05, 47.19it/s] 54%|█████▍    | 328/602 [00:07<00:05, 47.31it/s] 55%|█████▌    | 333/602 [00:07<00:05, 47.28it/s] 56%|█████▌    | 338/602 [00:07<00:05, 47.12it/s] 57%|█████▋    | 343/602 [00:07<00:06, 42.37it/s] 58%|█████▊    | 348/602 [00:07<00:05, 43.69it/s] 59%|█████▊    | 353/602 [00:07<00:05, 44.65it/s] 59%|█████▉    | 358/602 [00:07<00:05, 45.48it/s] 60%|██████    | 363/602 [00:08<00:05, 46.02it/s] 61%|██████    | 368/602 [00:08<00:05, 46.40it/s] 62%|██████▏   | 373/602 [00:08<00:04, 46.71it/s] 63%|██████▎   | 378/602 [00:08<00:04, 46.89it/s] 64%|██████▎   | 383/602 [00:08<00:04, 46.95it/s] 64%|██████▍   | 388/602 [00:08<00:04, 47.03it/s] 65%|██████▌   | 393/602 [00:08<00:04, 47.01it/s] 66%|██████▌   | 398/602 [00:08<00:04, 47.16it/s] 67%|██████▋   | 403/602 [00:08<00:04, 47.15it/s] 68%|██████▊   | 408/602 [00:09<00:04, 47.19it/s] 69%|██████▊   | 413/602 [00:09<00:03, 47.26it/s] 69%|██████▉   | 418/602 [00:09<00:03, 47.27it/s] 70%|███████   | 423/602 [00:09<00:03, 47.26it/s] 71%|███████   | 428/602 [00:09<00:03, 47.24it/s] 72%|███████▏  | 433/602 [00:09<00:03, 47.28it/s] 73%|███████▎  | 438/602 [00:09<00:03, 47.32it/s] 74%|███████▎  | 443/602 [00:09<00:03, 47.25it/s] 74%|███████▍  | 448/602 [00:09<00:03, 47.21it/s] 75%|███████▌  | 453/602 [00:09<00:03, 47.17it/s] 76%|███████▌  | 458/602 [00:10<00:03, 47.20it/s] 77%|███████▋  | 463/602 [00:10<00:02, 47.23it/s] 78%|███████▊  | 468/602 [00:10<00:02, 47.26it/s] 79%|███████▊  | 473/602 [00:10<00:02, 47.25it/s] 79%|███████▉  | 478/602 [00:10<00:02, 47.27it/s] 80%|████████  | 483/602 [00:10<00:02, 41.73it/s] 81%|████████  | 488/602 [00:10<00:02, 43.05it/s] 82%|████████▏ | 493/602 [00:10<00:02, 44.29it/s] 83%|████████▎ | 498/602 [00:10<00:02, 45.06it/s] 84%|████████▎ | 503/602 [00:11<00:02, 45.69it/s] 84%|████████▍ | 508/602 [00:11<00:02, 46.12it/s] 85%|████████▌ | 513/602 [00:11<00:01, 46.46it/s] 86%|████████▌ | 518/602 [00:11<00:01, 46.66it/s] 87%|████████▋ | 523/602 [00:11<00:01, 46.70it/s] 88%|████████▊ | 528/602 [00:11<00:01, 46.84it/s] 89%|████████▊ | 533/602 [00:11<00:01, 46.93it/s] 89%|████████▉ | 538/602 [00:11<00:01, 46.93it/s] 90%|█████████ | 543/602 [00:11<00:01, 47.03it/s] 91%|█████████ | 548/602 [00:12<00:01, 47.01it/s] 92%|█████████▏| 553/602 [00:12<00:01, 47.04it/s] 93%|█████████▎| 558/602 [00:12<00:01, 39.90it/s] 94%|█████████▎| 563/602 [00:12<00:00, 41.89it/s] 94%|█████████▍| 568/602 [00:12<00:00, 43.35it/s] 95%|█████████▌| 573/602 [00:12<00:00, 44.48it/s] 96%|█████████▌| 578/602 [00:12<00:00, 45.28it/s] 97%|█████████▋| 583/602 [00:12<00:00, 45.83it/s] 98%|█████████▊| 588/602 [00:12<00:00, 46.15it/s] 99%|█████████▊| 593/602 [00:13<00:00, 46.46it/s] 99%|█████████▉| 598/602 [00:13<00:00, 46.57it/s]100%|██████████| 602/602 [00:13<00:00, 45.38it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 15:59:17,051 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:59:17,051 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:59:17,051 >>   eval_loss               =     1.0098
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:59:17,051 >>   eval_runtime            = 0:00:13.28
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:59:17,051 >>   eval_samples            =       4812
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:59:17,051 >>   eval_samples_per_second =    362.093
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:59:17,051 >>   eval_steps_per_second   =     45.299
[INFO|trainer_pt_utils.py:913] 2023-08-29 15:59:17,051 >>   perplexity              =     2.7451
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:59:30,987 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:59:31,044 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:59:31,045 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:59:31,045 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:59:31,045 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 15:59:32,054 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 15:59:32,056 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 15:59:32,797 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 15:59:33,939 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 15:59:33,939 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:59:37,244 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:59:37,287 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:59:37,287 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:59:37,287 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:59:37,287 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 15:59:38,275 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 15:59:38,276 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 15:59:38,930 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 15:59:39,197 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 15:59:39,197 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-624
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-780
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/generator/iter5/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_15_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'given name', 'participant in', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14118
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14218, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.45it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:04,  1.49it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:06,  1.46it/s]Extractor Predicting: 11it [00:07,  1.41it/s]Extractor Predicting: 12it [00:08,  1.45it/s]Extractor Predicting: 13it [00:08,  1.39it/s]Extractor Predicting: 14it [00:09,  1.43it/s]Extractor Predicting: 15it [00:10,  1.47it/s]Extractor Predicting: 16it [00:10,  1.43it/s]Extractor Predicting: 17it [00:11,  1.44it/s]Extractor Predicting: 18it [00:12,  1.45it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.52it/s]Extractor Predicting: 21it [00:14,  1.46it/s]Extractor Predicting: 22it [00:15,  1.47it/s]Extractor Predicting: 23it [00:15,  1.48it/s]Extractor Predicting: 24it [00:16,  1.51it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:17,  1.51it/s]Extractor Predicting: 27it [00:18,  1.53it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.52it/s]Extractor Predicting: 30it [00:20,  1.54it/s]Extractor Predicting: 31it [00:20,  1.50it/s]Extractor Predicting: 32it [00:21,  1.51it/s]Extractor Predicting: 33it [00:22,  1.50it/s]Extractor Predicting: 34it [00:22,  1.48it/s]Extractor Predicting: 35it [00:23,  1.50it/s]Extractor Predicting: 36it [00:24,  1.50it/s]Extractor Predicting: 37it [00:24,  1.50it/s]Extractor Predicting: 38it [00:25,  1.50it/s]Extractor Predicting: 39it [00:26,  1.44it/s]Extractor Predicting: 40it [00:27,  1.45it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.58it/s]Extractor Predicting: 43it [00:28,  1.57it/s]Extractor Predicting: 44it [00:29,  1.52it/s]Extractor Predicting: 45it [00:30,  1.55it/s]Extractor Predicting: 46it [00:30,  1.56it/s]Extractor Predicting: 47it [00:31,  1.56it/s]Extractor Predicting: 48it [00:32,  1.57it/s]Extractor Predicting: 49it [00:32,  1.58it/s]Extractor Predicting: 50it [00:33,  1.57it/s]Extractor Predicting: 51it [00:33,  1.61it/s]Extractor Predicting: 52it [00:34,  1.62it/s]Extractor Predicting: 53it [00:35,  1.62it/s]Extractor Predicting: 54it [00:35,  1.66it/s]Extractor Predicting: 55it [00:36,  1.67it/s]Extractor Predicting: 56it [00:36,  1.66it/s]Extractor Predicting: 57it [00:37,  1.60it/s]Extractor Predicting: 58it [00:38,  1.58it/s]Extractor Predicting: 59it [00:38,  1.60it/s]Extractor Predicting: 60it [00:39,  1.60it/s]Extractor Predicting: 61it [00:40,  1.60it/s]Extractor Predicting: 62it [00:40,  1.59it/s]Extractor Predicting: 63it [00:41,  1.61it/s]Extractor Predicting: 64it [00:41,  1.65it/s]Extractor Predicting: 65it [00:42,  1.63it/s]Extractor Predicting: 66it [00:43,  1.61it/s]Extractor Predicting: 67it [00:43,  1.58it/s]Extractor Predicting: 68it [00:44,  1.56it/s]Extractor Predicting: 69it [00:45,  1.61it/s]Extractor Predicting: 70it [00:45,  1.61it/s]Extractor Predicting: 71it [00:46,  1.58it/s]Extractor Predicting: 72it [00:47,  1.56it/s]Extractor Predicting: 73it [00:47,  1.56it/s]Extractor Predicting: 74it [00:48,  1.58it/s]Extractor Predicting: 75it [00:48,  1.58it/s]Extractor Predicting: 76it [00:49,  1.58it/s]Extractor Predicting: 77it [00:50,  1.59it/s]Extractor Predicting: 78it [00:50,  1.59it/s]Extractor Predicting: 79it [00:51,  1.60it/s]Extractor Predicting: 80it [00:52,  1.58it/s]Extractor Predicting: 81it [00:52,  1.58it/s]Extractor Predicting: 82it [00:53,  1.51it/s]Extractor Predicting: 83it [00:54,  1.53it/s]Extractor Predicting: 84it [00:54,  1.56it/s]Extractor Predicting: 85it [00:55,  1.54it/s]Extractor Predicting: 86it [00:55,  1.54it/s]Extractor Predicting: 87it [00:56,  1.52it/s]Extractor Predicting: 88it [00:57,  1.52it/s]Extractor Predicting: 89it [00:57,  1.56it/s]Extractor Predicting: 90it [00:58,  1.39it/s]Extractor Predicting: 91it [00:59,  1.39it/s]Extractor Predicting: 92it [01:00,  1.44it/s]Extractor Predicting: 93it [01:00,  1.44it/s]Extractor Predicting: 94it [01:01,  1.42it/s]Extractor Predicting: 95it [01:02,  1.42it/s]Extractor Predicting: 96it [01:02,  1.44it/s]Extractor Predicting: 97it [01:03,  1.41it/s]Extractor Predicting: 98it [01:04,  1.42it/s]Extractor Predicting: 99it [01:05,  1.43it/s]Extractor Predicting: 100it [01:05,  1.46it/s]Extractor Predicting: 101it [01:06,  1.36it/s]Extractor Predicting: 102it [01:07,  1.39it/s]Extractor Predicting: 103it [01:07,  1.41it/s]Extractor Predicting: 104it [01:08,  1.41it/s]Extractor Predicting: 105it [01:09,  1.43it/s]Extractor Predicting: 106it [01:10,  1.45it/s]Extractor Predicting: 107it [01:10,  1.42it/s]Extractor Predicting: 108it [01:11,  1.44it/s]Extractor Predicting: 109it [01:12,  1.47it/s]Extractor Predicting: 110it [01:12,  1.46it/s]Extractor Predicting: 111it [01:13,  1.46it/s]Extractor Predicting: 112it [01:14,  1.48it/s]Extractor Predicting: 113it [01:14,  1.46it/s]Extractor Predicting: 114it [01:15,  1.44it/s]Extractor Predicting: 115it [01:16,  1.43it/s]Extractor Predicting: 116it [01:16,  1.44it/s]Extractor Predicting: 117it [01:17,  1.42it/s]Extractor Predicting: 118it [01:18,  1.44it/s]Extractor Predicting: 119it [01:19,  1.42it/s]Extractor Predicting: 120it [01:19,  1.44it/s]Extractor Predicting: 121it [01:20,  1.42it/s]Extractor Predicting: 122it [01:21,  1.47it/s]Extractor Predicting: 123it [01:21,  1.51it/s]Extractor Predicting: 124it [01:22,  1.44it/s]Extractor Predicting: 125it [01:23,  1.46it/s]Extractor Predicting: 126it [01:23,  1.45it/s]Extractor Predicting: 127it [01:24,  1.45it/s]Extractor Predicting: 128it [01:25,  1.43it/s]Extractor Predicting: 129it [01:25,  1.44it/s]Extractor Predicting: 130it [01:26,  1.43it/s]Extractor Predicting: 131it [01:27,  1.44it/s]Extractor Predicting: 132it [01:28,  1.45it/s]Extractor Predicting: 133it [01:28,  1.41it/s]Extractor Predicting: 134it [01:29,  1.39it/s]Extractor Predicting: 135it [01:30,  1.36it/s]Extractor Predicting: 136it [01:31,  1.36it/s]Extractor Predicting: 137it [01:31,  1.33it/s]Extractor Predicting: 138it [01:32,  1.31it/s]Extractor Predicting: 139it [01:33,  1.34it/s]Extractor Predicting: 140it [01:34,  1.36it/s]Extractor Predicting: 141it [01:34,  1.39it/s]Extractor Predicting: 142it [01:35,  1.39it/s]Extractor Predicting: 143it [01:36,  1.39it/s]Extractor Predicting: 144it [01:36,  1.37it/s]Extractor Predicting: 145it [01:37,  1.38it/s]Extractor Predicting: 146it [01:38,  1.34it/s]Extractor Predicting: 147it [01:39,  1.38it/s]Extractor Predicting: 148it [01:39,  1.37it/s]Extractor Predicting: 149it [01:40,  1.36it/s]Extractor Predicting: 150it [01:41,  1.37it/s]Extractor Predicting: 151it [01:41,  1.37it/s]Extractor Predicting: 152it [01:42,  1.38it/s]Extractor Predicting: 153it [01:43,  1.35it/s]Extractor Predicting: 154it [01:44,  1.34it/s]Extractor Predicting: 155it [01:44,  1.36it/s]Extractor Predicting: 156it [01:45,  1.34it/s]Extractor Predicting: 157it [01:46,  1.30it/s]Extractor Predicting: 158it [01:47,  1.30it/s]Extractor Predicting: 159it [01:48,  1.35it/s]Extractor Predicting: 160it [01:48,  1.39it/s]Extractor Predicting: 161it [01:49,  1.41it/s]Extractor Predicting: 162it [01:50,  1.39it/s]Extractor Predicting: 163it [01:50,  1.41it/s]Extractor Predicting: 164it [01:51,  1.44it/s]Extractor Predicting: 165it [01:52,  1.45it/s]Extractor Predicting: 166it [01:52,  1.48it/s]Extractor Predicting: 167it [01:53,  1.45it/s]Extractor Predicting: 168it [01:54,  1.45it/s]Extractor Predicting: 169it [01:54,  1.47it/s]Extractor Predicting: 170it [01:55,  1.48it/s]Extractor Predicting: 171it [01:56,  1.48it/s]Extractor Predicting: 172it [01:56,  1.46it/s]Extractor Predicting: 173it [01:57,  1.32it/s]Extractor Predicting: 174it [01:58,  1.37it/s]Extractor Predicting: 175it [01:59,  1.38it/s]Extractor Predicting: 176it [01:59,  1.36it/s]Extractor Predicting: 177it [02:00,  1.39it/s]Extractor Predicting: 178it [02:01,  1.41it/s]Extractor Predicting: 179it [02:02,  1.41it/s]Extractor Predicting: 180it [02:02,  1.43it/s]Extractor Predicting: 181it [02:03,  1.38it/s]Extractor Predicting: 182it [02:04,  1.39it/s]Extractor Predicting: 183it [02:04,  1.38it/s]Extractor Predicting: 184it [02:05,  1.41it/s]Extractor Predicting: 185it [02:06,  1.57it/s]Extractor Predicting: 185it [02:06,  1.47it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:02:09,721 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:02:09,783 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:02:09,783 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:02:09,783 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:02:09,783 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 16:02:10,891 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 16:02:10,893 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 16:02:11,583 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 16:02:12,749 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 16:02:12,821 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:02:16,072 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:02:16,115 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:02:16,115 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:02:16,115 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:02:16,115 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 16:02:17,159 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 16:02:17,160 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 16:02:17,873 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 16:02:18,139 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 16:02:18,139 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5716272600834492,
  "recall": 0.08541147132169576,
  "score": 0.14861688663894412,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_15_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'employer', 'field of work', 'languages spoken, written or signed', 'lyrics by', 'manufacturer', 'member of sports team', 'occupation', 'performer', 'residence', 'shares border with', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 31765
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31865, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:03,  1.49it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:05,  1.50it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:07,  1.51it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.50it/s]Extractor Predicting: 17it [00:11,  1.51it/s]Extractor Predicting: 18it [00:11,  1.48it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:14,  1.45it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.47it/s]Extractor Predicting: 24it [00:15,  1.50it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:17,  1.45it/s]Extractor Predicting: 27it [00:18,  1.48it/s]Extractor Predicting: 28it [00:18,  1.48it/s]Extractor Predicting: 29it [00:19,  1.53it/s]Extractor Predicting: 30it [00:19,  1.61it/s]Extractor Predicting: 31it [00:20,  1.63it/s]Extractor Predicting: 32it [00:21,  1.64it/s]Extractor Predicting: 33it [00:21,  1.62it/s]Extractor Predicting: 34it [00:22,  1.63it/s]Extractor Predicting: 35it [00:22,  1.61it/s]Extractor Predicting: 36it [00:23,  1.55it/s]Extractor Predicting: 37it [00:24,  1.55it/s]Extractor Predicting: 38it [00:24,  1.57it/s]Extractor Predicting: 39it [00:25,  1.54it/s]Extractor Predicting: 40it [00:26,  1.55it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:27,  1.55it/s]Extractor Predicting: 43it [00:28,  1.57it/s]Extractor Predicting: 44it [00:28,  1.59it/s]Extractor Predicting: 45it [00:29,  1.61it/s]Extractor Predicting: 46it [00:29,  1.62it/s]Extractor Predicting: 47it [00:30,  1.65it/s]Extractor Predicting: 48it [00:31,  1.61it/s]Extractor Predicting: 49it [00:31,  1.64it/s]Extractor Predicting: 50it [00:32,  1.61it/s]Extractor Predicting: 51it [00:33,  1.59it/s]Extractor Predicting: 52it [00:33,  1.60it/s]Extractor Predicting: 53it [00:34,  1.62it/s]Extractor Predicting: 54it [00:34,  1.57it/s]Extractor Predicting: 55it [00:35,  1.57it/s]Extractor Predicting: 56it [00:36,  1.53it/s]Extractor Predicting: 57it [00:36,  1.60it/s]Extractor Predicting: 58it [00:37,  1.66it/s]Extractor Predicting: 59it [00:38,  1.62it/s]Extractor Predicting: 60it [00:38,  1.59it/s]Extractor Predicting: 61it [00:39,  1.54it/s]Extractor Predicting: 62it [00:40,  1.54it/s]Extractor Predicting: 63it [00:40,  1.52it/s]Extractor Predicting: 64it [00:41,  1.52it/s]Extractor Predicting: 65it [00:42,  1.54it/s]Extractor Predicting: 66it [00:42,  1.53it/s]Extractor Predicting: 67it [00:43,  1.53it/s]Extractor Predicting: 68it [00:44,  1.51it/s]Extractor Predicting: 69it [00:44,  1.54it/s]Extractor Predicting: 70it [00:45,  1.55it/s]Extractor Predicting: 71it [00:45,  1.52it/s]Extractor Predicting: 72it [00:46,  1.49it/s]Extractor Predicting: 73it [00:47,  1.48it/s]Extractor Predicting: 74it [00:48,  1.47it/s]Extractor Predicting: 75it [00:48,  1.48it/s]Extractor Predicting: 76it [00:49,  1.45it/s]Extractor Predicting: 77it [00:50,  1.48it/s]Extractor Predicting: 78it [00:50,  1.53it/s]Extractor Predicting: 79it [00:51,  1.52it/s]Extractor Predicting: 80it [00:51,  1.57it/s]Extractor Predicting: 81it [00:52,  1.55it/s]Extractor Predicting: 82it [00:53,  1.34it/s]Extractor Predicting: 83it [00:54,  1.40it/s]Extractor Predicting: 84it [00:54,  1.43it/s]Extractor Predicting: 85it [00:55,  1.45it/s]Extractor Predicting: 86it [00:56,  1.48it/s]Extractor Predicting: 87it [00:56,  1.47it/s]Extractor Predicting: 88it [00:57,  1.46it/s]Extractor Predicting: 89it [00:58,  1.51it/s]Extractor Predicting: 90it [00:58,  1.52it/s]Extractor Predicting: 91it [00:59,  1.51it/s]Extractor Predicting: 92it [01:00,  1.48it/s]Extractor Predicting: 93it [01:00,  1.49it/s]Extractor Predicting: 94it [01:01,  1.52it/s]Extractor Predicting: 95it [01:02,  1.52it/s]Extractor Predicting: 96it [01:02,  1.53it/s]Extractor Predicting: 97it [01:03,  1.52it/s]Extractor Predicting: 98it [01:04,  1.45it/s]Extractor Predicting: 99it [01:04,  1.43it/s]Extractor Predicting: 100it [01:05,  1.42it/s]Extractor Predicting: 101it [01:06,  1.42it/s]Extractor Predicting: 102it [01:07,  1.43it/s]Extractor Predicting: 103it [01:07,  1.39it/s]Extractor Predicting: 104it [01:08,  1.46it/s]Extractor Predicting: 105it [01:09,  1.44it/s]Extractor Predicting: 106it [01:09,  1.48it/s]Extractor Predicting: 107it [01:10,  1.49it/s]Extractor Predicting: 108it [01:11,  1.49it/s]Extractor Predicting: 109it [01:11,  1.49it/s]Extractor Predicting: 110it [01:12,  1.48it/s]Extractor Predicting: 111it [01:13,  1.52it/s]Extractor Predicting: 112it [01:13,  1.53it/s]Extractor Predicting: 113it [01:14,  1.52it/s]Extractor Predicting: 114it [01:15,  1.53it/s]Extractor Predicting: 115it [01:15,  1.54it/s]Extractor Predicting: 116it [01:16,  1.54it/s]Extractor Predicting: 117it [01:16,  1.55it/s]Extractor Predicting: 118it [01:17,  1.52it/s]Extractor Predicting: 119it [01:18,  1.60it/s]Extractor Predicting: 120it [01:18,  1.56it/s]Extractor Predicting: 121it [01:19,  1.54it/s]Extractor Predicting: 122it [01:20,  1.56it/s]Extractor Predicting: 123it [01:20,  1.49it/s]Extractor Predicting: 124it [01:21,  1.49it/s]Extractor Predicting: 125it [01:22,  1.49it/s]Extractor Predicting: 126it [01:22,  1.50it/s]Extractor Predicting: 127it [01:23,  1.48it/s]Extractor Predicting: 128it [01:24,  1.50it/s]Extractor Predicting: 129it [01:24,  1.47it/s]Extractor Predicting: 130it [01:25,  1.48it/s]Extractor Predicting: 131it [01:26,  1.48it/s]Extractor Predicting: 132it [01:27,  1.43it/s]Extractor Predicting: 133it [01:27,  1.45it/s]Extractor Predicting: 134it [01:28,  1.46it/s]Extractor Predicting: 135it [01:29,  1.42it/s]Extractor Predicting: 136it [01:29,  1.46it/s]Extractor Predicting: 137it [01:30,  1.49it/s]Extractor Predicting: 138it [01:31,  1.47it/s]Extractor Predicting: 139it [01:31,  1.47it/s]Extractor Predicting: 140it [01:32,  1.45it/s]Extractor Predicting: 141it [01:33,  1.43it/s]Extractor Predicting: 142it [01:33,  1.45it/s]Extractor Predicting: 143it [01:34,  1.48it/s]Extractor Predicting: 144it [01:35,  1.47it/s]Extractor Predicting: 145it [01:35,  1.46it/s]Extractor Predicting: 146it [01:36,  1.46it/s]Extractor Predicting: 147it [01:37,  1.46it/s]Extractor Predicting: 148it [01:38,  1.46it/s]Extractor Predicting: 149it [01:38,  1.44it/s]Extractor Predicting: 150it [01:39,  1.46it/s]Extractor Predicting: 151it [01:40,  1.47it/s]Extractor Predicting: 152it [01:40,  1.51it/s]Extractor Predicting: 153it [01:41,  1.51it/s]Extractor Predicting: 154it [01:42,  1.49it/s]Extractor Predicting: 155it [01:42,  1.46it/s]Extractor Predicting: 156it [01:43,  1.47it/s]Extractor Predicting: 157it [01:44,  1.45it/s]Extractor Predicting: 158it [01:44,  1.46it/s]Extractor Predicting: 159it [01:45,  1.43it/s]Extractor Predicting: 160it [01:46,  1.46it/s]Extractor Predicting: 161it [01:46,  1.46it/s]Extractor Predicting: 162it [01:47,  1.49it/s]Extractor Predicting: 163it [01:48,  1.51it/s]Extractor Predicting: 164it [01:48,  1.48it/s]Extractor Predicting: 165it [01:49,  1.50it/s]Extractor Predicting: 166it [01:50,  1.49it/s]Extractor Predicting: 167it [01:50,  1.49it/s]Extractor Predicting: 168it [01:51,  1.51it/s]Extractor Predicting: 169it [01:52,  1.46it/s]Extractor Predicting: 170it [01:52,  1.46it/s]Extractor Predicting: 171it [01:53,  1.48it/s]Extractor Predicting: 172it [01:54,  1.50it/s]Extractor Predicting: 173it [01:54,  1.50it/s]Extractor Predicting: 174it [01:55,  1.49it/s]Extractor Predicting: 175it [01:56,  1.48it/s]Extractor Predicting: 176it [01:56,  1.51it/s]Extractor Predicting: 177it [01:57,  1.51it/s]Extractor Predicting: 178it [01:58,  1.49it/s]Extractor Predicting: 179it [01:58,  1.49it/s]Extractor Predicting: 180it [01:59,  1.52it/s]Extractor Predicting: 181it [02:00,  1.54it/s]Extractor Predicting: 182it [02:00,  1.53it/s]Extractor Predicting: 183it [02:01,  1.54it/s]Extractor Predicting: 184it [02:02,  1.50it/s]Extractor Predicting: 185it [02:02,  1.53it/s]Extractor Predicting: 186it [02:03,  1.53it/s]Extractor Predicting: 187it [02:04,  1.52it/s]Extractor Predicting: 188it [02:04,  1.52it/s]Extractor Predicting: 189it [02:05,  1.50it/s]Extractor Predicting: 190it [02:06,  1.51it/s]Extractor Predicting: 191it [02:06,  1.50it/s]Extractor Predicting: 192it [02:07,  1.52it/s]Extractor Predicting: 193it [02:08,  1.52it/s]Extractor Predicting: 194it [02:08,  1.49it/s]Extractor Predicting: 195it [02:09,  1.52it/s]Extractor Predicting: 196it [02:10,  1.56it/s]Extractor Predicting: 197it [02:10,  1.55it/s]Extractor Predicting: 198it [02:11,  1.51it/s]Extractor Predicting: 199it [02:12,  1.49it/s]Extractor Predicting: 200it [02:12,  1.50it/s]Extractor Predicting: 201it [02:13,  1.50it/s]Extractor Predicting: 202it [02:14,  1.47it/s]Extractor Predicting: 203it [02:14,  1.47it/s]Extractor Predicting: 204it [02:15,  1.46it/s]Extractor Predicting: 205it [02:16,  1.27it/s]Extractor Predicting: 206it [02:17,  1.35it/s]Extractor Predicting: 207it [02:17,  1.40it/s]Extractor Predicting: 208it [02:18,  1.40it/s]Extractor Predicting: 209it [02:19,  1.44it/s]Extractor Predicting: 210it [02:19,  1.47it/s]Extractor Predicting: 211it [02:20,  1.47it/s]Extractor Predicting: 212it [02:21,  1.53it/s]Extractor Predicting: 213it [02:21,  1.46it/s]Extractor Predicting: 214it [02:22,  1.48it/s]Extractor Predicting: 215it [02:23,  1.45it/s]Extractor Predicting: 216it [02:23,  1.44it/s]Extractor Predicting: 217it [02:24,  1.49it/s]Extractor Predicting: 218it [02:25,  1.49it/s]Extractor Predicting: 219it [02:25,  1.47it/s]Extractor Predicting: 220it [02:26,  1.44it/s]Extractor Predicting: 221it [02:27,  1.49it/s]Extractor Predicting: 222it [02:27,  1.49it/s]Extractor Predicting: 223it [02:28,  1.52it/s]Extractor Predicting: 224it [02:29,  1.52it/s]Extractor Predicting: 225it [02:29,  1.49it/s]Extractor Predicting: 226it [02:30,  1.48it/s]Extractor Predicting: 227it [02:31,  1.48it/s]Extractor Predicting: 228it [02:31,  1.50it/s]Extractor Predicting: 229it [02:32,  1.51it/s]Extractor Predicting: 230it [02:33,  1.48it/s]Extractor Predicting: 231it [02:33,  1.53it/s]Extractor Predicting: 232it [02:34,  1.55it/s]Extractor Predicting: 233it [02:35,  1.53it/s]Extractor Predicting: 234it [02:35,  1.56it/s]Extractor Predicting: 235it [02:36,  1.55it/s]Extractor Predicting: 236it [02:37,  1.52it/s]Extractor Predicting: 237it [02:37,  1.53it/s]Extractor Predicting: 238it [02:38,  1.53it/s]Extractor Predicting: 239it [02:39,  1.53it/s]Extractor Predicting: 240it [02:39,  1.53it/s]Extractor Predicting: 241it [02:40,  1.55it/s]Extractor Predicting: 242it [02:41,  1.52it/s]Extractor Predicting: 243it [02:41,  1.50it/s]Extractor Predicting: 244it [02:42,  1.53it/s]Extractor Predicting: 245it [02:43,  1.26it/s]Extractor Predicting: 246it [02:44,  1.32it/s]Extractor Predicting: 247it [02:44,  1.38it/s]Extractor Predicting: 248it [02:45,  1.44it/s]Extractor Predicting: 249it [02:46,  1.47it/s]Extractor Predicting: 250it [02:46,  1.48it/s]Extractor Predicting: 251it [02:47,  1.53it/s]Extractor Predicting: 252it [02:47,  1.55it/s]Extractor Predicting: 253it [02:48,  1.55it/s]Extractor Predicting: 254it [02:49,  1.56it/s]Extractor Predicting: 255it [02:49,  1.55it/s]Extractor Predicting: 256it [02:50,  1.55it/s]Extractor Predicting: 257it [02:51,  1.52it/s]Extractor Predicting: 258it [02:51,  1.53it/s]Extractor Predicting: 259it [02:52,  1.51it/s]Extractor Predicting: 260it [02:53,  1.53it/s]Extractor Predicting: 261it [02:53,  1.54it/s]Extractor Predicting: 262it [02:54,  1.56it/s]Extractor Predicting: 263it [02:55,  1.56it/s]Extractor Predicting: 264it [02:55,  1.55it/s]Extractor Predicting: 265it [02:56,  1.51it/s]Extractor Predicting: 266it [02:57,  1.52it/s]Extractor Predicting: 267it [02:57,  1.55it/s]Extractor Predicting: 268it [02:58,  1.54it/s]Extractor Predicting: 269it [02:59,  1.50it/s]Extractor Predicting: 270it [02:59,  1.56it/s]Extractor Predicting: 271it [03:00,  1.54it/s]Extractor Predicting: 272it [03:01,  1.52it/s]Extractor Predicting: 273it [03:01,  1.52it/s]Extractor Predicting: 274it [03:02,  1.52it/s]Extractor Predicting: 275it [03:03,  1.52it/s]Extractor Predicting: 276it [03:03,  1.49it/s]Extractor Predicting: 277it [03:04,  1.53it/s]Extractor Predicting: 278it [03:05,  1.50it/s]Extractor Predicting: 279it [03:05,  1.50it/s]Extractor Predicting: 280it [03:06,  1.50it/s]Extractor Predicting: 281it [03:07,  1.48it/s]Extractor Predicting: 282it [03:07,  1.50it/s]Extractor Predicting: 283it [03:08,  1.54it/s]Extractor Predicting: 284it [03:09,  1.49it/s]Extractor Predicting: 285it [03:09,  1.51it/s]Extractor Predicting: 286it [03:10,  1.50it/s]Extractor Predicting: 287it [03:10,  1.52it/s]Extractor Predicting: 288it [03:11,  1.51it/s]Extractor Predicting: 289it [03:12,  1.53it/s]Extractor Predicting: 290it [03:13,  1.45it/s]Extractor Predicting: 291it [03:13,  1.47it/s]Extractor Predicting: 292it [03:14,  1.46it/s]Extractor Predicting: 293it [03:15,  1.50it/s]Extractor Predicting: 294it [03:15,  1.54it/s]Extractor Predicting: 295it [03:16,  1.53it/s]Extractor Predicting: 296it [03:16,  1.55it/s]Extractor Predicting: 297it [03:17,  1.57it/s]Extractor Predicting: 298it [03:18,  1.59it/s]Extractor Predicting: 299it [03:18,  1.55it/s]Extractor Predicting: 300it [03:19,  1.56it/s]Extractor Predicting: 301it [03:20,  1.48it/s]Extractor Predicting: 302it [03:20,  1.49it/s]Extractor Predicting: 303it [03:21,  1.54it/s]Extractor Predicting: 304it [03:22,  1.52it/s]Extractor Predicting: 305it [03:22,  1.51it/s]Extractor Predicting: 306it [03:23,  1.50it/s]Extractor Predicting: 307it [03:24,  1.51it/s]Extractor Predicting: 308it [03:24,  1.55it/s]Extractor Predicting: 309it [03:25,  1.53it/s]Extractor Predicting: 310it [03:26,  1.54it/s]Extractor Predicting: 311it [03:26,  1.52it/s]Extractor Predicting: 312it [03:27,  1.46it/s]Extractor Predicting: 313it [03:28,  1.46it/s]Extractor Predicting: 314it [03:28,  1.43it/s]Extractor Predicting: 315it [03:29,  1.43it/s]Extractor Predicting: 316it [03:30,  1.47it/s]Extractor Predicting: 317it [03:30,  1.48it/s]Extractor Predicting: 318it [03:31,  1.53it/s]Extractor Predicting: 319it [03:32,  1.51it/s]Extractor Predicting: 320it [03:32,  1.58it/s]Extractor Predicting: 321it [03:33,  1.57it/s]Extractor Predicting: 322it [03:34,  1.53it/s]Extractor Predicting: 323it [03:34,  1.53it/s]Extractor Predicting: 324it [03:35,  1.55it/s]Extractor Predicting: 325it [03:36,  1.54it/s]Extractor Predicting: 326it [03:36,  1.53it/s]Extractor Predicting: 327it [03:37,  1.47it/s]Extractor Predicting: 328it [03:38,  1.45it/s]Extractor Predicting: 329it [03:38,  1.49it/s]Extractor Predicting: 330it [03:39,  1.53it/s]Extractor Predicting: 331it [03:40,  1.55it/s]Extractor Predicting: 332it [03:40,  1.52it/s]Extractor Predicting: 333it [03:41,  1.51it/s]Extractor Predicting: 334it [03:42,  1.50it/s]Extractor Predicting: 335it [03:42,  1.51it/s]Extractor Predicting: 336it [03:43,  1.51it/s]Extractor Predicting: 337it [03:44,  1.51it/s]Extractor Predicting: 338it [03:44,  1.54it/s]Extractor Predicting: 339it [03:45,  1.55it/s]Extractor Predicting: 340it [03:45,  1.59it/s]Extractor Predicting: 341it [03:46,  1.56it/s]Extractor Predicting: 342it [03:47,  1.35it/s]Extractor Predicting: 343it [03:48,  1.43it/s]Extractor Predicting: 344it [03:48,  1.45it/s]Extractor Predicting: 345it [03:49,  1.48it/s]Extractor Predicting: 346it [03:50,  1.45it/s]Extractor Predicting: 347it [03:50,  1.47it/s]Extractor Predicting: 348it [03:51,  1.48it/s]Extractor Predicting: 349it [03:52,  1.49it/s]Extractor Predicting: 350it [03:52,  1.52it/s]Extractor Predicting: 351it [03:53,  1.53it/s]Extractor Predicting: 352it [03:54,  1.53it/s]Extractor Predicting: 353it [03:54,  1.53it/s]Extractor Predicting: 354it [03:55,  1.54it/s]Extractor Predicting: 355it [03:56,  1.52it/s]Extractor Predicting: 356it [03:56,  1.53it/s]Extractor Predicting: 357it [03:57,  1.51it/s]Extractor Predicting: 358it [03:58,  1.52it/s]Extractor Predicting: 359it [03:58,  1.49it/s]Extractor Predicting: 360it [03:59,  1.51it/s]Extractor Predicting: 361it [03:59,  1.53it/s]Extractor Predicting: 362it [04:00,  1.53it/s]Extractor Predicting: 363it [04:01,  1.55it/s]Extractor Predicting: 364it [04:01,  1.51it/s]Extractor Predicting: 365it [04:02,  1.53it/s]Extractor Predicting: 366it [04:03,  1.53it/s]Extractor Predicting: 367it [04:03,  1.54it/s]Extractor Predicting: 368it [04:04,  1.51it/s]Extractor Predicting: 369it [04:05,  1.49it/s]Extractor Predicting: 370it [04:05,  1.54it/s]Extractor Predicting: 371it [04:06,  1.54it/s]Extractor Predicting: 372it [04:07,  1.56it/s]Extractor Predicting: 373it [04:07,  1.58it/s]Extractor Predicting: 374it [04:08,  1.55it/s]Extractor Predicting: 375it [04:09,  1.56it/s]Extractor Predicting: 376it [04:09,  1.54it/s]Extractor Predicting: 377it [04:10,  1.53it/s]Extractor Predicting: 378it [04:11,  1.53it/s]Extractor Predicting: 379it [04:11,  1.54it/s]Extractor Predicting: 380it [04:12,  1.55it/s]Extractor Predicting: 381it [04:12,  1.58it/s]Extractor Predicting: 382it [04:13,  1.55it/s]Extractor Predicting: 383it [04:14,  1.57it/s]Extractor Predicting: 384it [04:14,  1.53it/s]Extractor Predicting: 385it [04:15,  1.51it/s]Extractor Predicting: 386it [04:16,  1.54it/s]Extractor Predicting: 387it [04:16,  1.55it/s]Extractor Predicting: 388it [04:17,  1.51it/s]Extractor Predicting: 389it [04:18,  1.47it/s]Extractor Predicting: 390it [04:19,  1.44it/s]Extractor Predicting: 391it [04:19,  1.46it/s]Extractor Predicting: 392it [04:20,  1.47it/s]Extractor Predicting: 393it [04:21,  1.49it/s]Extractor Predicting: 394it [04:21,  1.49it/s]Extractor Predicting: 395it [04:22,  1.50it/s]Extractor Predicting: 396it [04:22,  1.52it/s]Extractor Predicting: 397it [04:23,  1.54it/s]Extractor Predicting: 398it [04:24,  1.57it/s]Extractor Predicting: 399it [04:24,  1.55it/s]Extractor Predicting: 400it [04:25,  1.56it/s]Extractor Predicting: 401it [04:26,  1.56it/s]Extractor Predicting: 402it [04:26,  1.56it/s]Extractor Predicting: 403it [04:27,  1.54it/s]Extractor Predicting: 404it [04:28,  1.53it/s]Extractor Predicting: 405it [04:28,  1.54it/s]Extractor Predicting: 406it [04:29,  1.51it/s]Extractor Predicting: 407it [04:30,  1.49it/s]Extractor Predicting: 408it [04:30,  1.51it/s]Extractor Predicting: 409it [04:31,  1.51it/s]Extractor Predicting: 410it [04:32,  1.54it/s]Extractor Predicting: 411it [04:32,  1.51it/s]Extractor Predicting: 412it [04:33,  1.51it/s]Extractor Predicting: 413it [04:34,  1.47it/s]Extractor Predicting: 414it [04:34,  1.51it/s]Extractor Predicting: 415it [04:35,  1.56it/s]Extractor Predicting: 416it [04:36,  1.54it/s]Extractor Predicting: 417it [04:36,  1.58it/s]Extractor Predicting: 418it [04:37,  1.59it/s]Extractor Predicting: 419it [04:37,  1.62it/s]Extractor Predicting: 420it [04:38,  1.59it/s]Extractor Predicting: 421it [04:39,  1.53it/s]Extractor Predicting: 422it [04:39,  1.56it/s]Extractor Predicting: 423it [04:40,  1.55it/s]Extractor Predicting: 424it [04:41,  1.56it/s]Extractor Predicting: 425it [04:41,  1.60it/s]Extractor Predicting: 426it [04:42,  1.57it/s]Extractor Predicting: 427it [04:42,  1.58it/s]Extractor Predicting: 428it [04:43,  1.55it/s]Extractor Predicting: 429it [04:44,  1.56it/s]Extractor Predicting: 430it [04:44,  1.60it/s]Extractor Predicting: 431it [04:45,  1.53it/s]Extractor Predicting: 432it [04:46,  1.57it/s]Extractor Predicting: 433it [04:46,  1.61it/s]Extractor Predicting: 434it [04:47,  1.61it/s]Extractor Predicting: 435it [04:48,  1.61it/s]Extractor Predicting: 436it [04:48,  1.58it/s]Extractor Predicting: 437it [04:49,  1.59it/s]Extractor Predicting: 438it [04:49,  1.60it/s]Extractor Predicting: 439it [04:50,  1.60it/s]Extractor Predicting: 440it [04:51,  1.57it/s]Extractor Predicting: 441it [04:51,  1.55it/s]Extractor Predicting: 442it [04:52,  1.54it/s]Extractor Predicting: 443it [04:53,  1.56it/s]Extractor Predicting: 444it [04:53,  1.58it/s]Extractor Predicting: 445it [04:54,  1.55it/s]Extractor Predicting: 446it [04:55,  1.50it/s]Extractor Predicting: 447it [04:55,  1.53it/s]Extractor Predicting: 448it [04:56,  1.56it/s]Extractor Predicting: 449it [04:57,  1.57it/s]Extractor Predicting: 450it [04:57,  1.59it/s]Extractor Predicting: 451it [04:58,  1.60it/s]Extractor Predicting: 452it [04:58,  1.59it/s]Extractor Predicting: 453it [04:59,  1.54it/s]Extractor Predicting: 454it [05:00,  1.52it/s]Extractor Predicting: 455it [05:00,  1.49it/s]Extractor Predicting: 456it [05:01,  1.47it/s]Extractor Predicting: 457it [05:02,  1.46it/s]Extractor Predicting: 458it [05:03,  1.45it/s]Extractor Predicting: 459it [05:03,  1.48it/s]Extractor Predicting: 460it [05:04,  1.49it/s]Extractor Predicting: 461it [05:05,  1.49it/s]Extractor Predicting: 462it [05:05,  1.46it/s]Extractor Predicting: 463it [05:06,  1.47it/s]Extractor Predicting: 464it [05:07,  1.47it/s]Extractor Predicting: 465it [05:07,  1.48it/s]Extractor Predicting: 466it [05:08,  1.49it/s]Extractor Predicting: 467it [05:09,  1.53it/s]Extractor Predicting: 468it [05:09,  1.46it/s]Extractor Predicting: 469it [05:10,  1.45it/s]Extractor Predicting: 470it [05:11,  1.47it/s]Extractor Predicting: 471it [05:11,  1.47it/s]Extractor Predicting: 472it [05:12,  1.45it/s]Extractor Predicting: 473it [05:13,  1.43it/s]Extractor Predicting: 474it [05:13,  1.45it/s]Extractor Predicting: 475it [05:14,  1.45it/s]Extractor Predicting: 476it [05:15,  1.44it/s]Extractor Predicting: 477it [05:15,  1.45it/s]Extractor Predicting: 478it [05:17,  1.26it/s]Extractor Predicting: 479it [05:17,  1.31it/s]Extractor Predicting: 480it [05:18,  1.36it/s]Extractor Predicting: 481it [05:19,  1.39it/s]Extractor Predicting: 482it [05:19,  1.38it/s]Extractor Predicting: 483it [05:20,  1.38it/s]Extractor Predicting: 484it [05:21,  1.38it/s]Extractor Predicting: 485it [05:21,  1.41it/s]Extractor Predicting: 486it [05:22,  1.45it/s]Extractor Predicting: 487it [05:23,  1.42it/s]Extractor Predicting: 488it [05:23,  1.43it/s]Extractor Predicting: 489it [05:24,  1.42it/s]Extractor Predicting: 490it [05:25,  1.45it/s]Extractor Predicting: 491it [05:26,  1.44it/s]Extractor Predicting: 492it [05:26,  1.38it/s]Extractor Predicting: 493it [05:27,  1.39it/s]Extractor Predicting: 494it [05:28,  1.45it/s]Extractor Predicting: 495it [05:28,  1.42it/s]Extractor Predicting: 496it [05:29,  1.43it/s]Extractor Predicting: 497it [05:30,  1.39it/s]Extractor Predicting: 498it [05:31,  1.44it/s]Extractor Predicting: 499it [05:31,  1.45it/s]Extractor Predicting: 500it [05:32,  1.44it/s]Extractor Predicting: 501it [05:33,  1.43it/s]Extractor Predicting: 502it [05:33,  1.39it/s]Extractor Predicting: 503it [05:34,  1.40it/s]Extractor Predicting: 504it [05:35,  1.42it/s]Extractor Predicting: 505it [05:35,  1.46it/s]Extractor Predicting: 506it [05:36,  1.45it/s]Extractor Predicting: 507it [05:37,  1.45it/s]Extractor Predicting: 508it [05:37,  1.48it/s]Extractor Predicting: 509it [05:38,  1.51it/s]Extractor Predicting: 510it [05:39,  1.55it/s]Extractor Predicting: 511it [05:39,  1.51it/s]Extractor Predicting: 512it [05:40,  1.49it/s]Extractor Predicting: 513it [05:41,  1.48it/s]Extractor Predicting: 514it [05:41,  1.49it/s]Extractor Predicting: 515it [05:42,  1.49it/s]Extractor Predicting: 516it [05:43,  1.46it/s]Extractor Predicting: 517it [05:44,  1.39it/s]Extractor Predicting: 518it [05:44,  1.40it/s]Extractor Predicting: 519it [05:45,  1.44it/s]Extractor Predicting: 520it [05:46,  1.50it/s]Extractor Predicting: 521it [05:46,  1.51it/s]Extractor Predicting: 522it [05:47,  1.48it/s]Extractor Predicting: 523it [05:48,  1.47it/s]Extractor Predicting: 524it [05:48,  1.48it/s]Extractor Predicting: 525it [05:49,  1.48it/s]Extractor Predicting: 526it [05:50,  1.44it/s]Extractor Predicting: 527it [05:50,  1.43it/s]Extractor Predicting: 528it [05:51,  1.46it/s]Extractor Predicting: 529it [05:52,  1.47it/s]Extractor Predicting: 530it [05:52,  1.46it/s]Extractor Predicting: 531it [05:53,  1.49it/s]Extractor Predicting: 532it [05:54,  1.42it/s]Extractor Predicting: 533it [05:54,  1.44it/s]Extractor Predicting: 534it [05:55,  1.45it/s]Extractor Predicting: 535it [05:56,  1.48it/s]Extractor Predicting: 536it [05:57,  1.45it/s]Extractor Predicting: 537it [05:57,  1.44it/s]Extractor Predicting: 538it [05:58,  1.44it/s]Extractor Predicting: 539it [05:59,  1.44it/s]Extractor Predicting: 540it [05:59,  1.45it/s]Extractor Predicting: 541it [06:00,  1.37it/s]Extractor Predicting: 542it [06:01,  1.37it/s]Extractor Predicting: 543it [06:02,  1.40it/s]Extractor Predicting: 544it [06:02,  1.43it/s]Extractor Predicting: 545it [06:03,  1.41it/s]Extractor Predicting: 546it [06:04,  1.42it/s]Extractor Predicting: 547it [06:04,  1.41it/s]Extractor Predicting: 548it [06:05,  1.45it/s]Extractor Predicting: 549it [06:06,  1.41it/s]Extractor Predicting: 550it [06:06,  1.43it/s]Extractor Predicting: 551it [06:07,  1.44it/s]Extractor Predicting: 552it [06:08,  1.42it/s]Extractor Predicting: 553it [06:08,  1.46it/s]Extractor Predicting: 554it [06:09,  1.46it/s]Extractor Predicting: 555it [06:10,  1.46it/s]Extractor Predicting: 556it [06:11,  1.47it/s]Extractor Predicting: 557it [06:11,  1.42it/s]Extractor Predicting: 558it [06:12,  1.42it/s]Extractor Predicting: 559it [06:13,  1.45it/s]Extractor Predicting: 560it [06:13,  1.48it/s]Extractor Predicting: 561it [06:14,  1.47it/s]Extractor Predicting: 562it [06:15,  1.46it/s]Extractor Predicting: 563it [06:15,  1.46it/s]Extractor Predicting: 564it [06:16,  1.45it/s]Extractor Predicting: 565it [06:17,  1.45it/s]Extractor Predicting: 566it [06:17,  1.45it/s]Extractor Predicting: 567it [06:18,  1.43it/s]Extractor Predicting: 568it [06:19,  1.49it/s]Extractor Predicting: 569it [06:19,  1.47it/s]Extractor Predicting: 570it [06:20,  1.45it/s]Extractor Predicting: 571it [06:21,  1.44it/s]Extractor Predicting: 572it [06:22,  1.40it/s]Extractor Predicting: 573it [06:22,  1.37it/s]Extractor Predicting: 574it [06:23,  1.37it/s]Extractor Predicting: 575it [06:24,  1.39it/s]Extractor Predicting: 576it [06:24,  1.42it/s]Extractor Predicting: 577it [06:25,  1.68it/s]Extractor Predicting: 577it [06:25,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:09:10,838 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:09:10,901 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:09:10,901 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:09:10,901 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:09:10,901 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 16:09:11,943 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 16:09:11,944 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 16:09:12,619 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 16:09:13,766 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 16:09:13,766 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:09:17,217 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:09:17,260 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:09:17,261 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:09:17,261 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 16:09:17,261 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 16:09:18,161 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 16:09:18,162 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 16:09:18,822 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 16:09:19,074 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 16:09:19,074 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.23672316384180792,
  "recall": 0.060601677755279144,
  "score": 0.09649930907415938,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_15_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'employer', 'field of work', 'languages spoken, written or signed', 'lyrics by', 'manufacturer', 'member of sports team', 'occupation', 'performer', 'residence', 'shares border with', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 11382
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11482, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.60it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:09,  1.43it/s]Extractor Predicting: 16it [00:10,  1.45it/s]Extractor Predicting: 17it [00:11,  1.45it/s]Extractor Predicting: 18it [00:12,  1.42it/s]Extractor Predicting: 19it [00:12,  1.41it/s]Extractor Predicting: 20it [00:13,  1.39it/s]Extractor Predicting: 21it [00:14,  1.42it/s]Extractor Predicting: 22it [00:14,  1.43it/s]Extractor Predicting: 23it [00:15,  1.43it/s]Extractor Predicting: 24it [00:16,  1.38it/s]Extractor Predicting: 25it [00:17,  1.37it/s]Extractor Predicting: 26it [00:17,  1.40it/s]Extractor Predicting: 27it [00:18,  1.40it/s]Extractor Predicting: 28it [00:19,  1.41it/s]Extractor Predicting: 29it [00:19,  1.37it/s]Extractor Predicting: 30it [00:20,  1.38it/s]Extractor Predicting: 31it [00:21,  1.43it/s]Extractor Predicting: 32it [00:21,  1.46it/s]Extractor Predicting: 33it [00:22,  1.45it/s]Extractor Predicting: 34it [00:23,  1.40it/s]Extractor Predicting: 35it [00:24,  1.46it/s]Extractor Predicting: 36it [00:24,  1.48it/s]Extractor Predicting: 37it [00:25,  1.47it/s]Extractor Predicting: 38it [00:26,  1.44it/s]Extractor Predicting: 39it [00:26,  1.35it/s]Extractor Predicting: 40it [00:27,  1.34it/s]Extractor Predicting: 41it [00:28,  1.37it/s]Extractor Predicting: 42it [00:29,  1.39it/s]Extractor Predicting: 43it [00:29,  1.37it/s]Extractor Predicting: 44it [00:30,  1.32it/s]Extractor Predicting: 45it [00:31,  1.34it/s]Extractor Predicting: 46it [00:32,  1.39it/s]Extractor Predicting: 47it [00:32,  1.42it/s]Extractor Predicting: 48it [00:33,  1.37it/s]Extractor Predicting: 49it [00:34,  1.40it/s]Extractor Predicting: 50it [00:34,  1.41it/s]Extractor Predicting: 51it [00:35,  1.44it/s]Extractor Predicting: 52it [00:36,  1.42it/s]Extractor Predicting: 53it [00:36,  1.43it/s]Extractor Predicting: 54it [00:37,  1.47it/s]Extractor Predicting: 55it [00:38,  1.49it/s]Extractor Predicting: 56it [00:38,  1.51it/s]Extractor Predicting: 57it [00:39,  1.46it/s]Extractor Predicting: 58it [00:40,  1.49it/s]Extractor Predicting: 59it [00:40,  1.50it/s]Extractor Predicting: 60it [00:41,  1.46it/s]Extractor Predicting: 61it [00:42,  1.50it/s]Extractor Predicting: 62it [00:43,  1.43it/s]Extractor Predicting: 63it [00:43,  1.43it/s]Extractor Predicting: 64it [00:44,  1.40it/s]Extractor Predicting: 65it [00:45,  1.41it/s]Extractor Predicting: 66it [00:45,  1.43it/s]Extractor Predicting: 67it [00:46,  1.40it/s]Extractor Predicting: 68it [00:47,  1.43it/s]Extractor Predicting: 69it [00:47,  1.42it/s]Extractor Predicting: 70it [00:48,  1.43it/s]Extractor Predicting: 71it [00:49,  1.45it/s]Extractor Predicting: 72it [00:50,  1.46it/s]Extractor Predicting: 73it [00:50,  1.48it/s]Extractor Predicting: 74it [00:51,  1.52it/s]Extractor Predicting: 75it [00:51,  1.52it/s]Extractor Predicting: 76it [00:52,  1.52it/s]Extractor Predicting: 77it [00:53,  1.51it/s]Extractor Predicting: 78it [00:53,  1.51it/s]Extractor Predicting: 79it [00:54,  1.55it/s]Extractor Predicting: 80it [00:55,  1.56it/s]Extractor Predicting: 81it [00:55,  1.58it/s]Extractor Predicting: 82it [00:56,  1.53it/s]Extractor Predicting: 83it [00:57,  1.38it/s]Extractor Predicting: 84it [00:58,  1.38it/s]Extractor Predicting: 85it [00:58,  1.45it/s]Extractor Predicting: 86it [00:59,  1.46it/s]Extractor Predicting: 87it [01:00,  1.40it/s]Extractor Predicting: 88it [01:00,  1.41it/s]Extractor Predicting: 89it [01:01,  1.39it/s]Extractor Predicting: 90it [01:02,  1.42it/s]Extractor Predicting: 91it [01:02,  1.41it/s]Extractor Predicting: 92it [01:03,  1.37it/s]Extractor Predicting: 93it [01:04,  1.38it/s]Extractor Predicting: 94it [01:05,  1.38it/s]Extractor Predicting: 95it [01:05,  1.37it/s]Extractor Predicting: 96it [01:06,  1.36it/s]Extractor Predicting: 97it [01:07,  1.38it/s]Extractor Predicting: 98it [01:08,  1.38it/s]Extractor Predicting: 99it [01:08,  1.39it/s]Extractor Predicting: 100it [01:09,  1.42it/s]Extractor Predicting: 101it [01:10,  1.42it/s]Extractor Predicting: 102it [01:11,  1.36it/s]Extractor Predicting: 103it [01:11,  1.35it/s]Extractor Predicting: 104it [01:12,  1.37it/s]Extractor Predicting: 105it [01:13,  1.40it/s]Extractor Predicting: 106it [01:13,  1.41it/s]Extractor Predicting: 107it [01:14,  1.35it/s]Extractor Predicting: 108it [01:15,  1.34it/s]Extractor Predicting: 109it [01:16,  1.33it/s]Extractor Predicting: 110it [01:16,  1.35it/s]Extractor Predicting: 110it [01:16,  1.43it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.42533081285444235,
  "recall": 0.03641365916815019,
  "score": 0.0670840787119857,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_15_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/', 'labels': ['composer', 'country of citizenship', 'creator', 'employer', 'field of work', 'languages spoken, written or signed', 'lyrics by', 'manufacturer', 'member of sports team', 'occupation', 'performer', 'residence', 'shares border with', 'sports discipline competed in', 'twinned administrative body'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_1/extractor/iter1/results_single_is_eval_True_limit5000.json'
